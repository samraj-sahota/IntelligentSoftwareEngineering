Repository,Number,Body,class,Title,Combined_Text
incubator-mxnet,9216,"description 1. batchnorm loses little precision 2. output var batchnorm may wrong environment info os arch linux 4.14.8 mxnet 1.0.0 1.0.1 latest version, cpu version build config make j8 use opencv 1 use blas openblas hi, there. converted resnet model caffe https github.com kaiminghe deep residual networks resnet model mxnet https github.com wkcn resnet v1 mx . found output results caffe mxnet different. reason computations caffe mxnet different. batchnorm caffe, output . batchnorm mxnet, output , . think method mxnet lose little precision bring higher performance reduce times division . time, found batchnorm may wrong. invstd , namely multiplicative inverse standard deviation. think variance. steps reproduce testing code https github.com wkcn test mxnet bn . compare three outputs numpy compute manally caffe mxnet first column , second column . tried solve change batchnorm implement mxnet, output modified batchnorm cpu code https github.com wkcn incubator mxnet commit 5ecd4882bc043cf059e962f7ce488270bafa07c7 .",1,loss precision batchnorm output var may wrong,"loss precision batchnorm output var may wrong description 1. batchnorm loses little precision 2. output var batchnorm may wrong environment info os arch linux 4.14.8 mxnet 1.0.0 1.0.1 latest version, cpu version build config make j8 use opencv 1 use blas openblas hi, there. converted resnet model caffe https github.com kaiminghe deep residual networks resnet model mxnet https github.com wkcn resnet v1 mx . found output results caffe mxnet different. reason computations caffe mxnet different. batchnorm caffe, output . batchnorm mxnet, output , . think method mxnet lose little precision bring higher performance reduce times division . time, found batchnorm may wrong. invstd , namely multiplicative inverse standard deviation. think variance. steps reproduce testing code https github.com wkcn test mxnet bn . compare three outputs numpy compute manally caffe mxnet first column , second column . tried solve change batchnorm implement mxnet, output modified batchnorm cpu code https github.com wkcn incubator mxnet commit 5ecd4882bc043cf059e962f7ce488270bafa07c7 ."
incubator-mxnet,10881,"using mx.sym.dot operator keras heavily. observe cpu performance suspiciously slower. profiling rnn lstm example, observation shown below. dot operator contributing 90 computation time. performance implication mx.sym.dot operator cpu? using mxnet mkl dnn build, operator using gemm operations hood? ! image https user images.githubusercontent.com 3403674 39853665 655d9708 53d8 11e8 8f34 58de7cbafd0e.png anirudh2290 zheng da suggestions comments?",1,mx.sym.dot performance cpu,"mx.sym.dot performance cpu using mx.sym.dot operator keras heavily. observe cpu performance suspiciously slower. profiling rnn lstm example, observation shown below. dot operator contributing 90 computation time. performance implication mx.sym.dot operator cpu? using mxnet mkl dnn build, operator using gemm operations hood? ! image https user images.githubusercontent.com 3403674 39853665 655d9708 53d8 11e8 8f34 58de7cbafd0e.png anirudh2290 zheng da suggestions comments?"
incubator-mxnet,10368,"train part cost 0.01 second, asscalar operation cost seconds sometimes 10. install newest version mxnet pip, happened cpu gpu context both. anyone idea fixing this? thank much.",1,asscalar slow,"asscalar slow train part cost 0.01 second, asscalar operation cost seconds sometimes 10. install newest version mxnet pip, happened cpu gpu context both. anyone idea fixing this? thank much."
incubator-mxnet,3591,quite deep net trained 4 gpus works fine. try run predict slow. 1 cpu anything 16 gpus nothing. looks like model loaded gpu memory actually think never unloaded training. sure happen automatically not. force predict run gpu?,1,r prediction running cpu sow,r prediction running cpu sow quite deep net trained 4 gpus works fine. try run predict slow. 1 cpu anything 16 gpus nothing. looks like model loaded gpu memory actually think never unloaded training. sure happen automatically not. force predict run gpu?
incubator-mxnet,13449,seems https github.com apache incubator mxnet pull 12380 causes significant performance regression spmv. causes 3 times slow p3.16x. main reason pr causes small number omp threads perform computation. minimal code reproducing bug. seems problem occurs model initialized multiple gpus. use code run code. csr file downloaded,1,significant performance regression spmv,significant performance regression spmv seems https github.com apache incubator mxnet pull 12380 causes significant performance regression spmv. causes 3 times slow p3.16x. main reason pr causes small number omp threads perform computation. minimal code reproducing bug. seems problem occurs model initialized multiple gpus. use code run code. csr file downloaded
incubator-mxnet,7820,"changed model called pvanet caffe https github.com sanghoon pva faster rcnn https github.com sanghoon pva faster rcnn , implementation article author https arxiv.org abs 1608.08021 https arxiv.org abs 1608.08021 , problem accuracy low author's done 1 implement classification net https github.com qingzhouzhen incubator mxnet blob pvanet e2e example image classification symbols pvanet.py https github.com qingzhouzhen incubator mxnet blob pvanet e2e example image classification symbols pvanet.py 2 feed classification faster r cnn module https github.com qingzhouzhen incubator mxnet blob pvanet e2e example rcnn rcnn symbol symbol pvanet.py https github.com qingzhouzhen incubator mxnet blob pvanet e2e example rcnn rcnn symbol symbol pvanet.py job done atapt faster r cnn, replace vgg pre trained model pvanet pre trained model, details see 7786 firstly classification net pvanet's accuracy 64.0 70.6 article secondly detection net pvanet rpn rcnn 's map 59.37 82.5 article checked net structure carefully make author's implementation caffe, anything wrong?",1,low accuracy pvanet,"low accuracy pvanet changed model called pvanet caffe https github.com sanghoon pva faster rcnn https github.com sanghoon pva faster rcnn , implementation article author https arxiv.org abs 1608.08021 https arxiv.org abs 1608.08021 , problem accuracy low author's done 1 implement classification net https github.com qingzhouzhen incubator mxnet blob pvanet e2e example image classification symbols pvanet.py https github.com qingzhouzhen incubator mxnet blob pvanet e2e example image classification symbols pvanet.py 2 feed classification faster r cnn module https github.com qingzhouzhen incubator mxnet blob pvanet e2e example rcnn rcnn symbol symbol pvanet.py https github.com qingzhouzhen incubator mxnet blob pvanet e2e example rcnn rcnn symbol symbol pvanet.py job done atapt faster r cnn, replace vgg pre trained model pvanet pre trained model, details see 7786 firstly classification net pvanet's accuracy 64.0 70.6 article secondly detection net pvanet rpn rcnn 's map 59.37 82.5 article checked net structure carefully make author's implementation caffe, anything wrong?"
incubator-mxnet,269,"hi use alexnet.py example imagenet training configure unchanged,but 20 rounds ,the accuracy 0.438870,much worse result shown docs 81 . dataset imagenet. wonder difference comes from. help? thx.",1,training accuracy,"training accuracy hi use alexnet.py example imagenet training configure unchanged,but 20 rounds ,the accuracy 0.438870,much worse result shown docs 81 . dataset imagenet. wonder difference comes from. help? thx."
incubator-mxnet,13454,"description pr 11001 introduced checked symbolblock.imports makes loading large graph takes long time used instantaneous issue code checking symbol row sparse, removing check allows load large model instantaneously again. environment info required python info version 3.6.5 compiler gcc 4.2.1 compatible apple llvm 9.0.0 clang 900.0.39.2 build 'default', 'jun 17 2018 12 26 58' arch '64bit', '' pip info version 18.0 directory usr local lib python3.6 site packages pip mxnet info usr local lib python3.6 site packages requests init .py 80 requestsdependencywarning urllib3 1.24 chardet 3.0.4 match supported version! requestsdependencywarning version 1.3.0 directory usr local lib python3.6 site packages mxnet commit hash b3be92f4a48bce62a5a8424271871c2f81c8f7f1 system info platform darwin 16.7.0 x86 64 i386 64bit system darwin node 186590d6796f.ant.amazon.com release 16.7.0 version darwin kernel version 16.7.0 wed oct 10 20 06 00 pdt 2018 root xnu 3789.73.24 1 release x86 64 hardware info machine x86 64 processor i386 b'machdep.cpu.extfeatures syscall xd 1gbpage em64t lahf lzcnt prefetchw rdtscp tsci' b'machdep.cpu.leaf7 features smep erms rdwrfsgs tsc thread offset bmi1 avx2 bmi2 invpcid smap rdseed adx ipt fpu csds' b'machdep.cpu.features fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clfsh ds acpi mmx fxsr sse sse2 ss htt tm pbe sse3 pclmulqdq dtes64 mon dscpl vmx est tm2 ssse3 fma cx16 tpr pdcm sse4.1 sse4.2 x2apic movbe popcnt aes pcid xsave osxsave seglim64 tsctmr avx1.0 rdrand f16c' b'machdep.cpu.brand string intel r core tm i7 5557u cpu 3.10ghz' network test setting timeout 10 timing mxnet https github.com apache incubator mxnet, dns 0.0543 sec, load 1.2793 sec. timing gluon tutorial en http gluon.mxnet.io, dns 0.0628 sec, load 0.9555 sec. timing gluon tutorial cn https zh.gluon.ai, dns 0.0802 sec, load 0.8386 sec. timing fashionmnist https apache mxnet.s3 accelerate.dualstack.amazonaws.com gluon dataset fashion mnist train labels idx1 ubyte.gz, dns 0.0073 sec, load 1.2169 sec. timing pypi https pypi.python.org pypi pip, dns 0.0521 sec, load 1.1819 sec. timing conda https repo.continuum.io pkgs free , dns 0.0419 sec, load 0.3027 sec. minimum reproducible example using code, please provide short script reproduces error. otherwise, please provide link existing example. steps reproduce paste commands ran produced error. 1. download model attached script load model loading issue.zip https github.com apache incubator mxnet files 2629332 model loading issue.zip 2. python slow model loading.py tried solve it? 1. comment check symbol row sparse",1,model loading became slow 11001,"model loading became slow 11001 description pr 11001 introduced checked symbolblock.imports makes loading large graph takes long time used instantaneous issue code checking symbol row sparse, removing check allows load large model instantaneously again. environment info required python info version 3.6.5 compiler gcc 4.2.1 compatible apple llvm 9.0.0 clang 900.0.39.2 build 'default', 'jun 17 2018 12 26 58' arch '64bit', '' pip info version 18.0 directory usr local lib python3.6 site packages pip mxnet info usr local lib python3.6 site packages requests init .py 80 requestsdependencywarning urllib3 1.24 chardet 3.0.4 match supported version! requestsdependencywarning version 1.3.0 directory usr local lib python3.6 site packages mxnet commit hash b3be92f4a48bce62a5a8424271871c2f81c8f7f1 system info platform darwin 16.7.0 x86 64 i386 64bit system darwin node 186590d6796f.ant.amazon.com release 16.7.0 version darwin kernel version 16.7.0 wed oct 10 20 06 00 pdt 2018 root xnu 3789.73.24 1 release x86 64 hardware info machine x86 64 processor i386 b'machdep.cpu.extfeatures syscall xd 1gbpage em64t lahf lzcnt prefetchw rdtscp tsci' b'machdep.cpu.leaf7 features smep erms rdwrfsgs tsc thread offset bmi1 avx2 bmi2 invpcid smap rdseed adx ipt fpu csds' b'machdep.cpu.features fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clfsh ds acpi mmx fxsr sse sse2 ss htt tm pbe sse3 pclmulqdq dtes64 mon dscpl vmx est tm2 ssse3 fma cx16 tpr pdcm sse4.1 sse4.2 x2apic movbe popcnt aes pcid xsave osxsave seglim64 tsctmr avx1.0 rdrand f16c' b'machdep.cpu.brand string intel r core tm i7 5557u cpu 3.10ghz' network test setting timeout 10 timing mxnet https github.com apache incubator mxnet, dns 0.0543 sec, load 1.2793 sec. timing gluon tutorial en http gluon.mxnet.io, dns 0.0628 sec, load 0.9555 sec. timing gluon tutorial cn https zh.gluon.ai, dns 0.0802 sec, load 0.8386 sec. timing fashionmnist https apache mxnet.s3 accelerate.dualstack.amazonaws.com gluon dataset fashion mnist train labels idx1 ubyte.gz, dns 0.0073 sec, load 1.2169 sec. timing pypi https pypi.python.org pypi pip, dns 0.0521 sec, load 1.1819 sec. timing conda https repo.continuum.io pkgs free , dns 0.0419 sec, load 0.3027 sec. minimum reproducible example using code, please provide short script reproduces error. otherwise, please provide link existing example. steps reproduce paste commands ran produced error. 1. download model attached script load model loading issue.zip https github.com apache incubator mxnet files 2629332 model loading issue.zip 2. python slow model loading.py tried solve it? 1. comment check symbol row sparse"
incubator-mxnet,8335,"testing mxnet win10 ubuntu 16.04 long time. found performance windows lower linux 15 20 gpu cpu contexts. wonder makes performance gap hope someone solve problem. config like following mxnet ubuntu mkl blas cuda8 cudnn6 opencv 2.4.13 jemalloc, gperftools building makefile mxnet windows mkl blas cuda8 cudnn6 opencv 2.4.13 jemalloc, gperftools building cmake",1,performance mxnet windows lower linux 15 20,"performance mxnet windows lower linux 15 20 testing mxnet win10 ubuntu 16.04 long time. found performance windows lower linux 15 20 gpu cpu contexts. wonder makes performance gap hope someone solve problem. config like following mxnet ubuntu mkl blas cuda8 cudnn6 opencv 2.4.13 jemalloc, gperftools building makefile mxnet windows mkl blas cuda8 cudnn6 opencv 2.4.13 jemalloc, gperftools building cmake"
incubator-mxnet,11919,description trained squeezenet model hyper parameters dataset p3.8xlarge p3.16xlarge ami got 3 lower accuracies p3.16xlarge. used batch size per gpu effective batch size 2x p3.16xlarge due 2x number gpus. environment info required p3.8xlarge p3.16xlarge package used python minimum reproducible example used model definition training script gluoncv https gluon cv.mxnet.io build examples classification dive deep imagenet.html sphx glr build examples classification dive deep imagenet py steps reproduce 1. p3.8xlarge 2. p3.16xlarge,1,accuracy changes number gpus,accuracy changes number gpus description trained squeezenet model hyper parameters dataset p3.8xlarge p3.16xlarge ami got 3 lower accuracies p3.16xlarge. used batch size per gpu effective batch size 2x p3.16xlarge due 2x number gpus. environment info required p3.8xlarge p3.16xlarge package used python minimum reproducible example used model definition training script gluoncv https gluon cv.mxnet.io build examples classification dive deep imagenet.html sphx glr build examples classification dive deep imagenet py steps reproduce 1. p3.8xlarge 2. p3.16xlarge
incubator-mxnet,913,"hi, test ubuntu 14.04 gtx 970 2015 12 13 18 43 42,924 node 0 start training gpu 0 2015 12 13 18 44 41,062 node 0 iter 0 batch 50 speed 395.17 samples sec 2015 12 13 18 44 57,556 node 0 iter 0 batch 100 speed 388.03 samples sec 2015 12 13 18 45 14,045 node 0 iter 0 batch 150 speed 388.13 samples sec 2015 12 13 18 45 30,548 node 0 iter 0 batch 200 speed 387.82 samples sec said gtx 980 842 img sec. nearly 2x difference. ideas?",1,speed 2x difference gtx 970 gtx 980,"speed 2x difference gtx 970 gtx 980 hi, test ubuntu 14.04 gtx 970 2015 12 13 18 43 42,924 node 0 start training gpu 0 2015 12 13 18 44 41,062 node 0 iter 0 batch 50 speed 395.17 samples sec 2015 12 13 18 44 57,556 node 0 iter 0 batch 100 speed 388.03 samples sec 2015 12 13 18 45 14,045 node 0 iter 0 batch 150 speed 388.13 samples sec 2015 12 13 18 45 30,548 node 0 iter 0 batch 200 speed 387.82 samples sec said gtx 980 842 img sec. nearly 2x difference. ideas?"
incubator-mxnet,6802,"cross entropy loss classification task somehow periodic iteration, period exactly epoch iterations data set. loss lowest end every epoch increases rapidly relatively beginning next epoch.... phenomenon appears different networks training different data sets experiments , think might something wrong training procedure rather model data sets. potential mistake probably made use metric use dataiter use optimizer anyone encountered similar problem? codes",1,periodic loss value,"periodic loss value cross entropy loss classification task somehow periodic iteration, period exactly epoch iterations data set. loss lowest end every epoch increases rapidly relatively beginning next epoch.... phenomenon appears different networks training different data sets experiments , think might something wrong training procedure rather model data sets. potential mistake probably made use metric use dataiter use optimizer anyone encountered similar problem? codes"
incubator-mxnet,8978,"description pretrained models seem working well gluon, specifically datasets build dataloader imagerecords imagefolders. example, load imagenet validation dataset feed alexnet downloaded gluon model zoo 12 accuracy shows issue probably transforms used train model exactly align transforms presented gluon tutorial. would nice example showing properly using new gluon functions added. environment info required python 3.6",1,low accuracy using pretrained model,"low accuracy using pretrained model description pretrained models seem working well gluon, specifically datasets build dataloader imagerecords imagefolders. example, load imagenet validation dataset feed alexnet downloaded gluon model zoo 12 accuracy shows issue probably transforms used train model exactly align transforms presented gluon tutorial. would nice example showing properly using new gluon functions added. environment info required python 3.6"
incubator-mxnet,13593,"mxnet low cpu usage running cpu operations multiple process scenarios. specifically, mxnet computation subprocess, mxnet use 1 2 cpus job. issue shows different behavior different variants mxnet see different machines issue critical slows multiprocess object detection data loading gluoncv significantly, making faster rcnn training gluoncv unusable. tested 20181207 version, versions e.g., 1.3.1 show similar problems. code reproduce issue filename detailed experiments run main process ! image https user images.githubusercontent.com 7865903 49704337 a3807000 fbc6 11e8 9118 0c7034e52cf9.png working fine mxnet variants gpu cpu . run two subproceses p3.16x ! image https user images.githubusercontent.com 7865903 49704395 420cd100 fbc7 11e8 9607 a0c907b2057a.png uses 2 cpus per subprocess. p3.16x ! image https user images.githubusercontent.com 7865903 49704444 14745780 fbc8 11e8 8754 81b90af4f876.png here. uses 2 cpus per subprocess. cpu machine c5.18x ! image https user images.githubusercontent.com 7865903 49704457 3d94e800 fbc8 11e8 8831 9136465fad1f.png even worse. uses 1.5 cpus per subprocess. however, vanilla cpu version c5.18x ! image https user images.githubusercontent.com 7865903 49704510 e2afc080 fbc8 11e8 8548 2505a7070205.png working better. least, uses 5 cpus per subprocess. weirdly, still vanilla cpu version gpu machine p3.16x ! image https user images.githubusercontent.com 7865903 49704532 1a1e6d00 fbc9 11e8 8009 95519fd9f1ef.png working worse, i.e., 2 cpus per subprocesses. problem seems relevant mxnet manage thread per subprocess. main process instead subprocess ! image https user images.githubusercontent.com 7865903 49704599 d11ae880 fbc9 11e8 8460 e7d1e53abb13.png everything working fine.",1,low cpu usage mxnet subprocesses,"low cpu usage mxnet subprocesses mxnet low cpu usage running cpu operations multiple process scenarios. specifically, mxnet computation subprocess, mxnet use 1 2 cpus job. issue shows different behavior different variants mxnet see different machines issue critical slows multiprocess object detection data loading gluoncv significantly, making faster rcnn training gluoncv unusable. tested 20181207 version, versions e.g., 1.3.1 show similar problems. code reproduce issue filename detailed experiments run main process ! image https user images.githubusercontent.com 7865903 49704337 a3807000 fbc6 11e8 9118 0c7034e52cf9.png working fine mxnet variants gpu cpu . run two subproceses p3.16x ! image https user images.githubusercontent.com 7865903 49704395 420cd100 fbc7 11e8 9607 a0c907b2057a.png uses 2 cpus per subprocess. p3.16x ! image https user images.githubusercontent.com 7865903 49704444 14745780 fbc8 11e8 8754 81b90af4f876.png here. uses 2 cpus per subprocess. cpu machine c5.18x ! image https user images.githubusercontent.com 7865903 49704457 3d94e800 fbc8 11e8 8831 9136465fad1f.png even worse. uses 1.5 cpus per subprocess. however, vanilla cpu version c5.18x ! image https user images.githubusercontent.com 7865903 49704510 e2afc080 fbc8 11e8 8548 2505a7070205.png working better. least, uses 5 cpus per subprocess. weirdly, still vanilla cpu version gpu machine p3.16x ! image https user images.githubusercontent.com 7865903 49704532 1a1e6d00 fbc9 11e8 8009 95519fd9f1ef.png working worse, i.e., 2 cpus per subprocesses. problem seems relevant mxnet manage thread per subprocess. main process instead subprocess ! image https user images.githubusercontent.com 7865903 49704599 d11ae880 fbc9 11e8 8460 e7d1e53abb13.png everything working fine."
incubator-mxnet,2041,"hi, using mxnet train 11 class image classifier. observing weird behavior training accuracy increasing slowly went upto 39 next epoch went 9 stays close 9 rest training. restarted training saved model 39 training accuracy keeping parameter . training accuracy increasing again. reason ? able understand . getting difficult train model way requires see training accuracy values constantly.",1,sudden drop accuracy training deep neural net,"sudden drop accuracy training deep neural net hi, using mxnet train 11 class image classifier. observing weird behavior training accuracy increasing slowly went upto 39 next epoch went 9 stays close 9 rest training. restarted training saved model 39 training accuracy keeping parameter . training accuracy increasing again. reason ? able understand . getting difficult train model way requires see training accuracy values constantly."
incubator-mxnet,7582,"bugs installation issues, please provide following information. information provide, likely people able help you. environment info operating system ubuntu 16.04 package used python r scala julia python mxnet version 0.11 python version distribution 2.7 running vgg fcn model keras tensorflow gluon. set batch size 32 keras tensorflow, set batch size 20 gluon. last block model gluon block 7 self.net nn.sequential prefix 'net' self.net.add nn.conv3d 256, kernel size 3, padding 1, activation 'relu' self.net.add nn.conv3d 4096, kernel size 1, padding 0, activation 'relu' self.net.add nn.conv3d 4096, kernel size 1, padding 0, activation 'relu' self.net.add nn.conv3d max, kernel size 1, activation 'sigmoid' corresponding part keras tf x conv3d 256, 3, 3, 3 , padding 'same', activation 'relu', kernel initializer 'normal', name 'rpn conv1' base layers x conv3d 4096, 1, 1, 1 , padding 'same', activation 'relu', kernel initializer 'normal', name 'rpn fc1' x x conv3d 4096, 1, 1, 1 , padding 'same', activation 'relu', kernel initializer 'normal', name 'rpn fc2' x x dist conv3d max, 1, 1, 1 , activation 'sigmoid', kernel initializer 'uniform', name 'rpn class' x",1,gluon gpu memory efficiency,"gluon gpu memory efficiency bugs installation issues, please provide following information. information provide, likely people able help you. environment info operating system ubuntu 16.04 package used python r scala julia python mxnet version 0.11 python version distribution 2.7 running vgg fcn model keras tensorflow gluon. set batch size 32 keras tensorflow, set batch size 20 gluon. last block model gluon block 7 self.net nn.sequential prefix 'net' self.net.add nn.conv3d 256, kernel size 3, padding 1, activation 'relu' self.net.add nn.conv3d 4096, kernel size 1, padding 0, activation 'relu' self.net.add nn.conv3d 4096, kernel size 1, padding 0, activation 'relu' self.net.add nn.conv3d max, kernel size 1, activation 'sigmoid' corresponding part keras tf x conv3d 256, 3, 3, 3 , padding 'same', activation 'relu', kernel initializer 'normal', name 'rpn conv1' base layers x conv3d 4096, 1, 1, 1 , padding 'same', activation 'relu', kernel initializer 'normal', name 'rpn fc1' x x conv3d 4096, 1, 1, 1 , padding 'same', activation 'relu', kernel initializer 'normal', name 'rpn fc2' x x dist conv3d max, 1, 1, 1 , activation 'sigmoid', kernel initializer 'uniform', name 'rpn class' x"
incubator-mxnet,5158,"environment info operating system centos 7.0 package used python r scala julia python mxnet version 0.93 python version distribution 2.7 testing distributed train multi machine mxnet. compile mxnet parameter use dist kvstore 1 , successful run train mlp mnist. found speed distributed training unusually slow compared speed stand alone training. stand alone commands speed info root start arguments namespace batch size 64, disp batches 100, gpus none, kv store 'device', load epoch none, lr 0.05, lr factor 0.1, lr step epochs '10', model prefix none, mom 0.9, monitor 0, network 'mlp', num classes 10, num epochs 20, num examples 60000, num layers none, optimizer 'sgd', test io 0, top k 0, wd 0.0001 info root epoch 0 batch 100 speed 21875.54 samples sec train accuracy 0.771040 info root epoch 0 batch 200 speed 21260.40 samples sec train accuracy 0.913906 info root epoch 0 batch 300 speed 21302.58 samples sec train accuracy 0.933750 info root epoch 0 batch 400 speed 21216.02 samples sec train accuracy 0.936875 info root epoch 0 batch 500 speed 22835.21 samples sec train accuracy 0.933594 info root epoch 0 batch 600 speed 21612.71 samples sec train accuracy 0.947500 info root epoch 0 batch 700 speed 23362.35 samples sec train accuracy 0.954375 info root epoch 0 batch 800 speed 21683.75 samples sec train accuracy 0.954219 info root epoch 0 batch 900 speed 21656.14 samples sec train accuracy 0.955781 info root epoch 0 train accuracy 0.958615 info root epoch 0 time cost 2.804 info root epoch 0 validation accuracy 0.957803 command speed distributed training local machine using two workers info root start arguments namespace batch size 64, disp batches 100, gpus none, kv store 'dist sync', load epoch none, lr 0.05, lr factor 0.1, lr step epochs '10', model prefix none, mom 0.9, monitor 0, network 'mlp', num classes 10, num epochs 20, num examples 60000, num layers none, optimizer 'sgd', test io 0, top k 0, wd 0.0001 info root start arguments namespace batch size 64, disp batches 100, gpus none, kv store 'dist sync', load epoch none, lr 0.05, lr factor 0.1, lr step epochs '10', model prefix none, mom 0.9, monitor 0, network 'mlp', num classes 10, num epochs 20, num examples 60000, num layers none, optimizer 'sgd', test io 0, top k 0, wd 0.0001 info root epoch 0 batch 100 speed 2580.90 samples sec train accuracy 0.104425 info root epoch 0 batch 100 speed 2574.24 samples sec train accuracy 0.096689 info root epoch 0 batch 200 speed 4408.62 samples sec train accuracy 0.093594 info root epoch 0 batch 200 speed 4399.50 samples sec train accuracy 0.099531 info root epoch 0 batch 300 speed 3343.81 samples sec train accuracy 0.095469 info root epoch 0 batch 300 speed 3342.38 samples sec train accuracy 0.097344 info root epoch 0 batch 400 speed 3247.69 samples sec train accuracy 0.096250 info root epoch 0 batch 400 speed 3245.97 samples sec train accuracy 0.104531 info root epoch 0 batch 500 speed 3364.46 samples sec train accuracy 0.102188 info root epoch 0 batch 500 speed 3364.97 samples sec train accuracy 0.098437 info root epoch 0 batch 600 speed 3729.89 samples sec train accuracy 0.097500 info root epoch 0 batch 600 speed 3732.76 samples sec train accuracy 0.097812 info root epoch 0 batch 700 speed 5105.08 samples sec train accuracy 0.087969 info root epoch 0 batch 700 speed 5097.00 samples sec train accuracy 0.104375 info root epoch 0 batch 800 speed 3931.05 samples sec train accuracy 0.099062 info root epoch 0 batch 800 speed 3930.63 samples sec train accuracy 0.101406 info root epoch 0 batch 900 speed 3763.65 samples sec train accuracy 0.098906 info root epoch 0 batch 900 speed 3758.21 samples sec train accuracy 0.099531 info root epoch 0 train accuracy 0.104307 info root epoch 0 time cost 16.560 info root epoch 0 train accuracy 0.099662 info root epoch 0 time cost 16.584 info root epoch 0 validation accuracy 0.098029 info root epoch 0 validation accuracy 0.098029 command speed distributed training two computers hardware configration info root start arguments namespace batch size 64, disp batches 100, gpus none, kv store 'dist sync', load epoch none, lr 0.05, lr factor 0.1, lr step epochs '10', model prefix none, mom 0.9, monitor 0, network 'mlp', num classes 10, num epochs 20, num examples 60000, num layers none, optimizer 'sgd', test io 0, top k 0, wd 0.0001 info root start arguments namespace batch size 64, disp batches 100, gpus none, kv store 'dist sync', load epoch none, lr 0.05, lr factor 0.1, lr step epochs '10', model prefix none, mom 0.9, monitor 0, network 'mlp', num classes 10, num epochs 20, num examples 60000, num layers none, optimizer 'sgd', test io 0, top k 0, wd 0.0001 info root epoch 0 batch 100 speed 614.03 samples sec train accuracy 0.097463 info root epoch 0 batch 100 speed 613.66 samples sec train accuracy 0.096225 info root epoch 0 batch 200 speed 577.24 samples sec train accuracy 0.097812 info root epoch 0 batch 200 speed 576.24 samples sec train accuracy 0.096250 info root epoch 0 batch 300 speed 609.78 samples sec train accuracy 0.094531 info root epoch 0 batch 300 speed 610.73 samples sec train accuracy 0.098125 info root epoch 0 batch 400 speed 624.24 samples sec train accuracy 0.100000 info root epoch 0 batch 400 speed 623.74 samples sec train accuracy 0.100625 info root epoch 0 batch 500 speed 642.81 samples sec train accuracy 0.096719 info root epoch 0 batch 500 speed 643.28 samples sec train accuracy 0.099531 info root epoch 0 batch 600 speed 613.65 samples sec train accuracy 0.097187 info root epoch 0 batch 600 speed 613.63 samples sec train accuracy 0.096406 info root epoch 0 batch 700 speed 626.13 samples sec train accuracy 0.106875 info root epoch 0 batch 700 speed 626.15 samples sec train accuracy 0.096562 info root epoch 0 batch 800 speed 621.21 samples sec train accuracy 0.098437 info root epoch 0 batch 800 speed 620.83 samples sec train accuracy 0.099687 info root epoch 0 batch 900 speed 589.75 samples sec train accuracy 0.098281 info root epoch 0 batch 900 speed 589.72 samples sec train accuracy 0.104844 info root epoch 0 train accuracy 0.101351 info root epoch 0 time cost 98.129 info root epoch 0 train accuracy 0.097551 info root epoch 0 time cost 98.144 info root epoch 0 validation accuracy 0.098029 info root epoch 0 validation accuracy 0.098029 info root epoch 1 batch 100 speed 639.28 samples sec train accuracy 0.097308 info root epoch 1 batch 100 speed 640.32 samples sec train accuracy 0.096844 know speed slow distributed training , someone help us? thanks",1,distributed training speed extremely slow,"distributed training speed extremely slow environment info operating system centos 7.0 package used python r scala julia python mxnet version 0.93 python version distribution 2.7 testing distributed train multi machine mxnet. compile mxnet parameter use dist kvstore 1 , successful run train mlp mnist. found speed distributed training unusually slow compared speed stand alone training. stand alone commands speed info root start arguments namespace batch size 64, disp batches 100, gpus none, kv store 'device', load epoch none, lr 0.05, lr factor 0.1, lr step epochs '10', model prefix none, mom 0.9, monitor 0, network 'mlp', num classes 10, num epochs 20, num examples 60000, num layers none, optimizer 'sgd', test io 0, top k 0, wd 0.0001 info root epoch 0 batch 100 speed 21875.54 samples sec train accuracy 0.771040 info root epoch 0 batch 200 speed 21260.40 samples sec train accuracy 0.913906 info root epoch 0 batch 300 speed 21302.58 samples sec train accuracy 0.933750 info root epoch 0 batch 400 speed 21216.02 samples sec train accuracy 0.936875 info root epoch 0 batch 500 speed 22835.21 samples sec train accuracy 0.933594 info root epoch 0 batch 600 speed 21612.71 samples sec train accuracy 0.947500 info root epoch 0 batch 700 speed 23362.35 samples sec train accuracy 0.954375 info root epoch 0 batch 800 speed 21683.75 samples sec train accuracy 0.954219 info root epoch 0 batch 900 speed 21656.14 samples sec train accuracy 0.955781 info root epoch 0 train accuracy 0.958615 info root epoch 0 time cost 2.804 info root epoch 0 validation accuracy 0.957803 command speed distributed training local machine using two workers info root start arguments namespace batch size 64, disp batches 100, gpus none, kv store 'dist sync', load epoch none, lr 0.05, lr factor 0.1, lr step epochs '10', model prefix none, mom 0.9, monitor 0, network 'mlp', num classes 10, num epochs 20, num examples 60000, num layers none, optimizer 'sgd', test io 0, top k 0, wd 0.0001 info root start arguments namespace batch size 64, disp batches 100, gpus none, kv store 'dist sync', load epoch none, lr 0.05, lr factor 0.1, lr step epochs '10', model prefix none, mom 0.9, monitor 0, network 'mlp', num classes 10, num epochs 20, num examples 60000, num layers none, optimizer 'sgd', test io 0, top k 0, wd 0.0001 info root epoch 0 batch 100 speed 2580.90 samples sec train accuracy 0.104425 info root epoch 0 batch 100 speed 2574.24 samples sec train accuracy 0.096689 info root epoch 0 batch 200 speed 4408.62 samples sec train accuracy 0.093594 info root epoch 0 batch 200 speed 4399.50 samples sec train accuracy 0.099531 info root epoch 0 batch 300 speed 3343.81 samples sec train accuracy 0.095469 info root epoch 0 batch 300 speed 3342.38 samples sec train accuracy 0.097344 info root epoch 0 batch 400 speed 3247.69 samples sec train accuracy 0.096250 info root epoch 0 batch 400 speed 3245.97 samples sec train accuracy 0.104531 info root epoch 0 batch 500 speed 3364.46 samples sec train accuracy 0.102188 info root epoch 0 batch 500 speed 3364.97 samples sec train accuracy 0.098437 info root epoch 0 batch 600 speed 3729.89 samples sec train accuracy 0.097500 info root epoch 0 batch 600 speed 3732.76 samples sec train accuracy 0.097812 info root epoch 0 batch 700 speed 5105.08 samples sec train accuracy 0.087969 info root epoch 0 batch 700 speed 5097.00 samples sec train accuracy 0.104375 info root epoch 0 batch 800 speed 3931.05 samples sec train accuracy 0.099062 info root epoch 0 batch 800 speed 3930.63 samples sec train accuracy 0.101406 info root epoch 0 batch 900 speed 3763.65 samples sec train accuracy 0.098906 info root epoch 0 batch 900 speed 3758.21 samples sec train accuracy 0.099531 info root epoch 0 train accuracy 0.104307 info root epoch 0 time cost 16.560 info root epoch 0 train accuracy 0.099662 info root epoch 0 time cost 16.584 info root epoch 0 validation accuracy 0.098029 info root epoch 0 validation accuracy 0.098029 command speed distributed training two computers hardware configration info root start arguments namespace batch size 64, disp batches 100, gpus none, kv store 'dist sync', load epoch none, lr 0.05, lr factor 0.1, lr step epochs '10', model prefix none, mom 0.9, monitor 0, network 'mlp', num classes 10, num epochs 20, num examples 60000, num layers none, optimizer 'sgd', test io 0, top k 0, wd 0.0001 info root start arguments namespace batch size 64, disp batches 100, gpus none, kv store 'dist sync', load epoch none, lr 0.05, lr factor 0.1, lr step epochs '10', model prefix none, mom 0.9, monitor 0, network 'mlp', num classes 10, num epochs 20, num examples 60000, num layers none, optimizer 'sgd', test io 0, top k 0, wd 0.0001 info root epoch 0 batch 100 speed 614.03 samples sec train accuracy 0.097463 info root epoch 0 batch 100 speed 613.66 samples sec train accuracy 0.096225 info root epoch 0 batch 200 speed 577.24 samples sec train accuracy 0.097812 info root epoch 0 batch 200 speed 576.24 samples sec train accuracy 0.096250 info root epoch 0 batch 300 speed 609.78 samples sec train accuracy 0.094531 info root epoch 0 batch 300 speed 610.73 samples sec train accuracy 0.098125 info root epoch 0 batch 400 speed 624.24 samples sec train accuracy 0.100000 info root epoch 0 batch 400 speed 623.74 samples sec train accuracy 0.100625 info root epoch 0 batch 500 speed 642.81 samples sec train accuracy 0.096719 info root epoch 0 batch 500 speed 643.28 samples sec train accuracy 0.099531 info root epoch 0 batch 600 speed 613.65 samples sec train accuracy 0.097187 info root epoch 0 batch 600 speed 613.63 samples sec train accuracy 0.096406 info root epoch 0 batch 700 speed 626.13 samples sec train accuracy 0.106875 info root epoch 0 batch 700 speed 626.15 samples sec train accuracy 0.096562 info root epoch 0 batch 800 speed 621.21 samples sec train accuracy 0.098437 info root epoch 0 batch 800 speed 620.83 samples sec train accuracy 0.099687 info root epoch 0 batch 900 speed 589.75 samples sec train accuracy 0.098281 info root epoch 0 batch 900 speed 589.72 samples sec train accuracy 0.104844 info root epoch 0 train accuracy 0.101351 info root epoch 0 time cost 98.129 info root epoch 0 train accuracy 0.097551 info root epoch 0 time cost 98.144 info root epoch 0 validation accuracy 0.098029 info root epoch 0 validation accuracy 0.098029 info root epoch 1 batch 100 speed 639.28 samples sec train accuracy 0.097308 info root epoch 1 batch 100 speed 640.32 samples sec train accuracy 0.096844 know speed slow distributed training , someone help us? thanks"
incubator-mxnet,938,"new mxnet. set tested package three different systems, cpu linux vm, cpu windows server gpu based windows pc. sample using example notebooks cifar10 recipe.ipynb comparing performance across three systems, noticed initial training 12 cifar10 recipe.ipynb , train accuracy validation accuracy got around 0.4 0.5, pretty far 65 accuracy documentation stated. here's example output test gpu based machine. causing difference test one document using code test data set? also documentation says model able achieve 65 accuracy testset not, try times , try times mean? thanks",1,training accuracy issue,"training accuracy issue new mxnet. set tested package three different systems, cpu linux vm, cpu windows server gpu based windows pc. sample using example notebooks cifar10 recipe.ipynb comparing performance across three systems, noticed initial training 12 cifar10 recipe.ipynb , train accuracy validation accuracy got around 0.4 0.5, pretty far 65 accuracy documentation stated. here's example output test gpu based machine. causing difference test one document using code test data set? also documentation says model able achieve 65 accuracy testset not, try times , try times mean? thanks"
incubator-mxnet,5035,"using bucketing module expect memory usage using normal module unrolled largest bucket size. however observe unusually high gpu memory usage mxnet using multiple buckets. reproduced observed lstm bucketing.py example latest mxnet commit examples rnn lstm bucketing.py change using multiple buckets see line 49 , overall memory usage 1419mb. changing line 49 use single bucket e.g. 60 , overall memory usage 1185mb. noted initial memory usage bucketing 1185mb , couple batches memory usage increases. suspect due bucketingmodule binding another sub module new bucket size given data iterator memory sharing across modules working properly. model difference 300 mb, observed much higher differences practice, making difficult train reasonably sized model bucketing. note default bucket key course largest bucket.",1,high memory usage bucketing,"high memory usage bucketing using bucketing module expect memory usage using normal module unrolled largest bucket size. however observe unusually high gpu memory usage mxnet using multiple buckets. reproduced observed lstm bucketing.py example latest mxnet commit examples rnn lstm bucketing.py change using multiple buckets see line 49 , overall memory usage 1419mb. changing line 49 use single bucket e.g. 60 , overall memory usage 1185mb. noted initial memory usage bucketing 1185mb , couple batches memory usage increases. suspect due bucketingmodule binding another sub module new bucket size given data iterator memory sharing across modules working properly. model difference 300 mb, observed much higher differences practice, making difficult train reasonably sized model bucketing. note default bucket key course largest bucket."
incubator-mxnet,15148,"description mxnet consumes nearly 2gb cpu ram even loading relatively small model e.g. resnet 18 directed gpu . understand, real need use much cpu memory model running gpu. issue extremely prohibitive trying run multiple processes mxnet machine, imo gives significant disadvantage compared frameworks used ai production systems. environment info package used python r scala julia using python3 build info mxnet installed using pip3 steps reproduce paste commands ran produced error. 1. run following code 2. check process memory run top, shift sort processes memory usage 3. memory usage 1.5 2gb ram",1,large cpu ram memory consumption 1gb,"large cpu ram memory consumption 1gb description mxnet consumes nearly 2gb cpu ram even loading relatively small model e.g. resnet 18 directed gpu . understand, real need use much cpu memory model running gpu. issue extremely prohibitive trying run multiple processes mxnet machine, imo gives significant disadvantage compared frameworks used ai production systems. environment info package used python r scala julia using python3 build info mxnet installed using pip3 steps reproduce paste commands ran produced error. 1. run following code 2. check process memory run top, shift sort processes memory usage 3. memory usage 1.5 2gb ram"
incubator-mxnet,16687,"description 16602 merge, java scala ssd gpu inference latency increased 20x 70ms 1400ms steps reproduce paste commands ran produced error. patriczhao zhennanqin thanks advance happy halloween!",1,performance regression scala java ssd inference,"performance regression scala java ssd inference description 16602 merge, java scala ssd gpu inference latency increased 20x 70ms 1400ms steps reproduce paste commands ran produced error. patriczhao zhennanqin thanks advance happy halloween!"
incubator-mxnet,3378,"trying inception bn full dataset. near 1400w 5190 classes . python train imagenet.py network inception bn full batch size 100 lr 0.05 lr factor 0.9 gpus 2,3 num epoch 60 data dir data5 rd xiajizhong parts 17kclasses train dataset small rec val dataset small rec val num examples 14040000 and, use latest mxnet get bad validation accuracy. 2016 09 24 02 49 24,212 node 0 epoch 0 batch 140400 speed 47.30 samples sec train accuracy 0.546400 2016 09 24 02 49 24,213 node 0 epoch 0 batch 140400 speed 47.30 samples sec train top k accuracy 5 0.787400 2016 09 24 02 49 24,213 node 0 epoch 0 batch 140400 speed 47.30 samples sec train top k accuracy 10 0.842400 2016 09 24 02 49 24,213 node 0 epoch 0 batch 140400 speed 47.30 samples sec train top k accuracy 20 0.884800 2016 09 24 02 49 25,074 node 0 update 140401 change learning rate 4.50000e 02 2016 09 24 02 49 31,389 node 0 epoch 0 resetting data iterator 2016 09 24 02 49 31,390 node 0 epoch 0 time cost 301141.343 2016 09 24 02 49 31,645 node 0 saved checkpoint model inception bn full vdian 0001.params 2016 09 24 05 15 15,472 node 0 epoch 0 validation accuracy 0.142758 2016 09 24 05 15 15,472 node 0 epoch 0 validation top k accuracy 5 0.280551 2016 09 24 05 15 15,472 node 0 epoch 0 validation top k accuracy 10 0.350004 2016 09 24 05 15 15,472 node 0 epoch 0 validation top k accuracy 20 0.418618 but, experiment well old version mxnet, validation accuracy also high near top1 50 , one epoch. wondering happened? clear different changed new version mxnet. caused pooling layer behavior changed? old version, result below, 2016 06 10 10 26 15,591 node 0 epoch 0 batch 140350 speed 50.40 samples sec train accuracy 0.406846 2016 06 10 10 27 53,871 node 0 epoch 0 batch 140400 speed 50.88 samples sec train accuracy 0.406867 2016 06 10 10 27 53,875 node 0 update 140401 change learning rate 4.05000e 01 2016 06 10 10 27 59,645 node 0 epoch 0 resetting data iterator 2016 06 10 10 27 59,647 node 0 epoch 0 train accuracy 0.406869 2016 06 10 10 27 59,647 node 0 epoch 0 time cost 258948.773 2016 06 10 10 28 01,107 node 0 saved checkpoint model vdian5190 0 0001.params 2016 06 10 12 50 10,700 node 0 epoch 0 validation accuracy 0.457665",1,"train accuracy high, low validation accuracy","train accuracy high, low validation accuracy trying inception bn full dataset. near 1400w 5190 classes . python train imagenet.py network inception bn full batch size 100 lr 0.05 lr factor 0.9 gpus 2,3 num epoch 60 data dir data5 rd xiajizhong parts 17kclasses train dataset small rec val dataset small rec val num examples 14040000 and, use latest mxnet get bad validation accuracy. 2016 09 24 02 49 24,212 node 0 epoch 0 batch 140400 speed 47.30 samples sec train accuracy 0.546400 2016 09 24 02 49 24,213 node 0 epoch 0 batch 140400 speed 47.30 samples sec train top k accuracy 5 0.787400 2016 09 24 02 49 24,213 node 0 epoch 0 batch 140400 speed 47.30 samples sec train top k accuracy 10 0.842400 2016 09 24 02 49 24,213 node 0 epoch 0 batch 140400 speed 47.30 samples sec train top k accuracy 20 0.884800 2016 09 24 02 49 25,074 node 0 update 140401 change learning rate 4.50000e 02 2016 09 24 02 49 31,389 node 0 epoch 0 resetting data iterator 2016 09 24 02 49 31,390 node 0 epoch 0 time cost 301141.343 2016 09 24 02 49 31,645 node 0 saved checkpoint model inception bn full vdian 0001.params 2016 09 24 05 15 15,472 node 0 epoch 0 validation accuracy 0.142758 2016 09 24 05 15 15,472 node 0 epoch 0 validation top k accuracy 5 0.280551 2016 09 24 05 15 15,472 node 0 epoch 0 validation top k accuracy 10 0.350004 2016 09 24 05 15 15,472 node 0 epoch 0 validation top k accuracy 20 0.418618 but, experiment well old version mxnet, validation accuracy also high near top1 50 , one epoch. wondering happened? clear different changed new version mxnet. caused pooling layer behavior changed? old version, result below, 2016 06 10 10 26 15,591 node 0 epoch 0 batch 140350 speed 50.40 samples sec train accuracy 0.406846 2016 06 10 10 27 53,871 node 0 epoch 0 batch 140400 speed 50.88 samples sec train accuracy 0.406867 2016 06 10 10 27 53,875 node 0 update 140401 change learning rate 4.05000e 01 2016 06 10 10 27 59,645 node 0 epoch 0 resetting data iterator 2016 06 10 10 27 59,647 node 0 epoch 0 train accuracy 0.406869 2016 06 10 10 27 59,647 node 0 epoch 0 time cost 258948.773 2016 06 10 10 28 01,107 node 0 saved checkpoint model vdian5190 0 0001.params 2016 06 10 12 50 10,700 node 0 epoch 0 validation accuracy 0.457665"
incubator-mxnet,9396,"hi, updated mxnet today 0.10.0 1.0.0 order use new features. versions installed pip like . however, detailed benchmark test, observed significant speed drop running resnet inference especially batch size small. result resnet152 like network json file downloaded http data.mxnet.io models imagenet resnet 152 layers resnet 152 symbol.json ps. noticed batch size small i.e. batch size 1 , gpu usage 95 100 mxnet 0.10.0 80 83 mxnet 1.0.0 means gpu fully utilized all. software env ubuntu 16.04, python 3.5, cuda 8.0, cudnn 5.1. gpu gtx 1080 ti. also test server titan xp got similar result. speed test script pasted",1,inference speed drop updating mxnet 0.10.0 1.0.0,"inference speed drop updating mxnet 0.10.0 1.0.0 hi, updated mxnet today 0.10.0 1.0.0 order use new features. versions installed pip like . however, detailed benchmark test, observed significant speed drop running resnet inference especially batch size small. result resnet152 like network json file downloaded http data.mxnet.io models imagenet resnet 152 layers resnet 152 symbol.json ps. noticed batch size small i.e. batch size 1 , gpu usage 95 100 mxnet 0.10.0 80 83 mxnet 1.0.0 means gpu fully utilized all. software env ubuntu 16.04, python 3.5, cuda 8.0, cudnn 5.1. gpu gtx 1080 ti. also test server titan xp got similar result. speed test script pasted"
incubator-mxnet,11575,"hi, trying train model following code. classes predict 0 1. results . 2018 07 06 00 54 38,926 epoch 0 batch 100 speed 20.37 samples sec accuracy 0.544554 rmse 0.591505 mae 0.500000 2018 07 06 00 54 43,694 epoch 0 batch 200 speed 20.97 samples sec accuracy 0.470000 rmse 0.586368 mae 0.500000 2018 07 06 00 54 48,509 epoch 0 batch 300 speed 20.77 samples sec accuracy 0.520000 rmse 0.587208 mae 0.500000 2018 07 06 00 54 53,286 epoch 0 batch 400 speed 20.93 samples sec accuracy 0.560000 rmse 0.602946 mae 0.500000 2018 07 06 00 54 58,057 epoch 0 batch 500 speed 20.96 samples sec accuracy 0.460000 rmse 0.576723 mae 0.500000 2018 07 06 00 55 02,886 epoch 0 batch 600 speed 20.71 samples sec accuracy 0.490000 rmse 0.577645 mae 0.500000 2018 07 06 00 55 07,703 epoch 0 batch 700 speed 20.76 samples sec accuracy 0.600000 rmse 0.585552 mae 0.500000 2018 07 06 00 55 12,453 epoch 0 batch 800 speed 21.05 samples sec accuracy 0.560000 rmse 0.585788 mae 0.500000 2018 07 06 00 55 17,236 epoch 0 batch 900 speed 20.91 samples sec accuracy 0.500000 rmse 0.567332 mae 0.500000 2018 07 06 00 55 21,993 epoch 0 batch 1000 speed 21.02 samples sec accuracy 0.590000 rmse 0.580251 mae 0.500000 2018 07 06 00 55 26,776 epoch 0 batch 1100 speed 20.91 samples sec accuracy 0.550000 rmse 0.564997 mae 0.500000 2018 07 06 00 55 31,532 epoch 0 batch 1200 speed 21.02 samples sec accuracy 0.620000 rmse 0.564062 mae 0.500000 2018 07 06 00 55 36,281 epoch 0 batch 1300 speed 21.06 samples sec accuracy 0.650000 rmse 0.566788 mae 0.500000 2018 07 06 00 55 41,062 epoch 0 batch 1400 speed 20.92 samples sec accuracy 0.590000 rmse 0.574353 mae 0.500000 2018 07 06 00 55 45,845 epoch 0 batch 1500 speed 20.91 samples sec accuracy 0.690000 rmse 0.569736 mae 0.500000 2018 07 06 00 55 50,623 epoch 0 batch 1600 speed 20.93 samples sec accuracy 0.700000 rmse 0.579918 mae 0.500000 2018 07 06 00 55 55,407 epoch 0 batch 1700 speed 20.90 samples sec accuracy 0.730000 rmse 0.585157 mae 0.500000 2018 07 06 00 56 00,170 epoch 0 batch 1800 speed 20.99 samples sec accuracy 0.810000 rmse 0.590722 mae 0.500000 2018 07 06 00 56 04,944 epoch 0 batch 1900 speed 20.95 samples sec accuracy 0.860000 rmse 0.601612 mae 0.500000 2018 07 06 00 56 09,704 epoch 0 batch 2000 speed 21.01 samples sec accuracy 0.800000 rmse 0.593426 mae 0.500000 2018 07 06 00 56 14,460 epoch 0 batch 2100 speed 21.02 samples sec accuracy 0.860000 rmse 0.618266 mae 0.500000 2018 07 06 00 56 19,215 epoch 0 batch 2200 speed 21.03 samples sec accuracy 0.760000 rmse 0.605838 mae 0.500000 2018 07 06 00 56 23,997 epoch 0 batch 2300 speed 20.92 samples sec accuracy 0.840000 rmse 0.616089 mae 0.500000 2018 07 06 00 56 28,773 epoch 0 batch 2400 speed 20.94 samples sec accuracy 0.850000 rmse 0.620063 mae 0.500000 2018 07 06 00 56 33,562 epoch 0 batch 2500 speed 20.88 samples sec accuracy 0.820000 rmse 0.608637 mae 0.500000 2018 07 06 00 56 38,401 epoch 0 batch 2600 speed 20.67 samples sec accuracy 0.880000 rmse 0.631159 mae 0.500000 2018 07 06 00 56 43,179 epoch 0 batch 2700 speed 20.93 samples sec accuracy 0.850000 rmse 0.624896 mae 0.500000 2018 07 06 00 56 47,975 epoch 0 batch 2800 speed 20.85 samples sec accuracy 0.830000 rmse 0.631906 mae 0.500000 2018 07 06 00 56 52,744 epoch 0 batch 2900 speed 20.97 samples sec accuracy 0.890000 rmse 0.634412 mae 0.500000 2018 07 06 00 56 57,510 epoch 0 batch 3000 speed 20.98 samples sec accuracy 0.810000 rmse 0.628204 mae 0.500000 2018 07 06 00 57 02,293 epoch 0 batch 3100 speed 20.91 samples sec accuracy 0.900000 rmse 0.648476 mae 0.500000 2018 07 06 00 57 07,066 epoch 0 batch 3200 speed 20.95 samples sec accuracy 0.920000 rmse 0.642261 mae 0.500000 result looks strange me. 1 mae always same. 2 accuracy almost optimal near 1 3 rmse worse random 3 things happen time?",1,inconsistent results mae acc rmse,"inconsistent results mae acc rmse hi, trying train model following code. classes predict 0 1. results . 2018 07 06 00 54 38,926 epoch 0 batch 100 speed 20.37 samples sec accuracy 0.544554 rmse 0.591505 mae 0.500000 2018 07 06 00 54 43,694 epoch 0 batch 200 speed 20.97 samples sec accuracy 0.470000 rmse 0.586368 mae 0.500000 2018 07 06 00 54 48,509 epoch 0 batch 300 speed 20.77 samples sec accuracy 0.520000 rmse 0.587208 mae 0.500000 2018 07 06 00 54 53,286 epoch 0 batch 400 speed 20.93 samples sec accuracy 0.560000 rmse 0.602946 mae 0.500000 2018 07 06 00 54 58,057 epoch 0 batch 500 speed 20.96 samples sec accuracy 0.460000 rmse 0.576723 mae 0.500000 2018 07 06 00 55 02,886 epoch 0 batch 600 speed 20.71 samples sec accuracy 0.490000 rmse 0.577645 mae 0.500000 2018 07 06 00 55 07,703 epoch 0 batch 700 speed 20.76 samples sec accuracy 0.600000 rmse 0.585552 mae 0.500000 2018 07 06 00 55 12,453 epoch 0 batch 800 speed 21.05 samples sec accuracy 0.560000 rmse 0.585788 mae 0.500000 2018 07 06 00 55 17,236 epoch 0 batch 900 speed 20.91 samples sec accuracy 0.500000 rmse 0.567332 mae 0.500000 2018 07 06 00 55 21,993 epoch 0 batch 1000 speed 21.02 samples sec accuracy 0.590000 rmse 0.580251 mae 0.500000 2018 07 06 00 55 26,776 epoch 0 batch 1100 speed 20.91 samples sec accuracy 0.550000 rmse 0.564997 mae 0.500000 2018 07 06 00 55 31,532 epoch 0 batch 1200 speed 21.02 samples sec accuracy 0.620000 rmse 0.564062 mae 0.500000 2018 07 06 00 55 36,281 epoch 0 batch 1300 speed 21.06 samples sec accuracy 0.650000 rmse 0.566788 mae 0.500000 2018 07 06 00 55 41,062 epoch 0 batch 1400 speed 20.92 samples sec accuracy 0.590000 rmse 0.574353 mae 0.500000 2018 07 06 00 55 45,845 epoch 0 batch 1500 speed 20.91 samples sec accuracy 0.690000 rmse 0.569736 mae 0.500000 2018 07 06 00 55 50,623 epoch 0 batch 1600 speed 20.93 samples sec accuracy 0.700000 rmse 0.579918 mae 0.500000 2018 07 06 00 55 55,407 epoch 0 batch 1700 speed 20.90 samples sec accuracy 0.730000 rmse 0.585157 mae 0.500000 2018 07 06 00 56 00,170 epoch 0 batch 1800 speed 20.99 samples sec accuracy 0.810000 rmse 0.590722 mae 0.500000 2018 07 06 00 56 04,944 epoch 0 batch 1900 speed 20.95 samples sec accuracy 0.860000 rmse 0.601612 mae 0.500000 2018 07 06 00 56 09,704 epoch 0 batch 2000 speed 21.01 samples sec accuracy 0.800000 rmse 0.593426 mae 0.500000 2018 07 06 00 56 14,460 epoch 0 batch 2100 speed 21.02 samples sec accuracy 0.860000 rmse 0.618266 mae 0.500000 2018 07 06 00 56 19,215 epoch 0 batch 2200 speed 21.03 samples sec accuracy 0.760000 rmse 0.605838 mae 0.500000 2018 07 06 00 56 23,997 epoch 0 batch 2300 speed 20.92 samples sec accuracy 0.840000 rmse 0.616089 mae 0.500000 2018 07 06 00 56 28,773 epoch 0 batch 2400 speed 20.94 samples sec accuracy 0.850000 rmse 0.620063 mae 0.500000 2018 07 06 00 56 33,562 epoch 0 batch 2500 speed 20.88 samples sec accuracy 0.820000 rmse 0.608637 mae 0.500000 2018 07 06 00 56 38,401 epoch 0 batch 2600 speed 20.67 samples sec accuracy 0.880000 rmse 0.631159 mae 0.500000 2018 07 06 00 56 43,179 epoch 0 batch 2700 speed 20.93 samples sec accuracy 0.850000 rmse 0.624896 mae 0.500000 2018 07 06 00 56 47,975 epoch 0 batch 2800 speed 20.85 samples sec accuracy 0.830000 rmse 0.631906 mae 0.500000 2018 07 06 00 56 52,744 epoch 0 batch 2900 speed 20.97 samples sec accuracy 0.890000 rmse 0.634412 mae 0.500000 2018 07 06 00 56 57,510 epoch 0 batch 3000 speed 20.98 samples sec accuracy 0.810000 rmse 0.628204 mae 0.500000 2018 07 06 00 57 02,293 epoch 0 batch 3100 speed 20.91 samples sec accuracy 0.900000 rmse 0.648476 mae 0.500000 2018 07 06 00 57 07,066 epoch 0 batch 3200 speed 20.95 samples sec accuracy 0.920000 rmse 0.642261 mae 0.500000 result looks strange me. 1 mae always same. 2 accuracy almost optimal near 1 3 rmse worse random 3 things happen time?"
incubator-mxnet,951,used two machine every machine one gpu use alexnet run multile machine.the result saids slower one machine use one gpu. net km. multi machine limited net?,1,run two machine two gpu slower one machine one gpu,run two machine two gpu slower one machine one gpu used two machine every machine one gpu use alexnet run multile machine.the result saids slower one machine use one gpu. net km. multi machine limited net?
incubator-mxnet,16568,"looking metrics seems like behave slightly different way depending sum batch inputs. example, accuracy calculate mean single batches, sums evaluates single mean get method https github.com apache incubator mxnet blob master python mxnet metric.py l507 contrary, rmse evaluates single rmse batch, average get method. https github.com apache incubator mxnet blob master python mxnet metric.py l1273 leads discrepancy metrics example using metrics non linear f.i. mae, mse, rmse, pcc metric samples ! mean metric batch samples .",1,"different uniform behavior rmse,mse,mae","different uniform behavior rmse,mse,mae looking metrics seems like behave slightly different way depending sum batch inputs. example, accuracy calculate mean single batches, sums evaluates single mean get method https github.com apache incubator mxnet blob master python mxnet metric.py l507 contrary, rmse evaluates single rmse batch, average get method. https github.com apache incubator mxnet blob master python mxnet metric.py l1273 leads discrepancy metrics example using metrics non linear f.i. mae, mse, rmse, pcc metric samples ! mean metric batch samples ."
incubator-mxnet,10167,"here's interesting example hybridblock sometimes much slower block. define identity block using two strategies compare running time. result find case, using block better choice using hybridblock.",1,hybridblock slower block,"hybridblock slower block here's interesting example hybridblock sometimes much slower block. define identity block using two strategies compare running time. result find case, using block better choice using hybridblock."
incubator-mxnet,11192,"mx.sym.batchnorm operator https mxnet.incubator.apache.org api python symbol symbol.html mxnet.symbol.batchnorm considerably slower gpus channels last axis 1 . understanding that, channels first axis 1 operator calls cudnn implementation channels last axis 1 mxnet implementation.",1,batchnorm operator gpus slow channels last,"batchnorm operator gpus slow channels last mx.sym.batchnorm operator https mxnet.incubator.apache.org api python symbol symbol.html mxnet.symbol.batchnorm considerably slower gpus channels last axis 1 . understanding that, channels first axis 1 operator calls cudnn implementation channels last axis 1 mxnet implementation."
incubator-mxnet,5052,"example code run simple classification regression examples mxnet running r. examples first solve cpu, gpu. classification example get 32x speed using gpu, however speed running regression example much less 3x . details given below, main questions would 1. reason speed much less significant regression example? 2. people mxnet get similar comparable results me? alternatively benchmark run compare benchmark performances? output get example code details set environment info operating system ubuntu 16.04 r details",1,"mxnet r, gpu speed much less regression example classification","mxnet r, gpu speed much less regression example classification example code run simple classification regression examples mxnet running r. examples first solve cpu, gpu. classification example get 32x speed using gpu, however speed running regression example much less 3x . details given below, main questions would 1. reason speed much less significant regression example? 2. people mxnet get similar comparable results me? alternatively benchmark run compare benchmark performances? output get example code details set environment info operating system ubuntu 16.04 r details"
incubator-mxnet,8126,"environment info operating system ubuntu 16.04 compiler g package used python r scala julia c mxnet version 0.11 installed source mxnet commit hash branch 0.11.0 error message error training accuracy always remains zero. problem code constructing neural network symbol training procedure correct, i.e. gradient updates correctly specified? minimum reproducible example 1. defined siamese net like architecture final layer using code provided output code always something like tried solve it? 1. tried different variations training data network always predicting zero output.",1,able train neural network xor added,"able train neural network xor added environment info operating system ubuntu 16.04 compiler g package used python r scala julia c mxnet version 0.11 installed source mxnet commit hash branch 0.11.0 error message error training accuracy always remains zero. problem code constructing neural network symbol training procedure correct, i.e. gradient updates correctly specified? minimum reproducible example 1. defined siamese net like architecture final layer using code provided output code always something like tried solve it? 1. tried different variations training data network always predicting zero output."
incubator-mxnet,7615,"environment info operating system ubuntu 16.04 mxnet version 0.11.0 installed source no, using pip install mxnet cu80 python version distribution python27 run demo https github.com zackchase mxnet straight dope blob master p06 c03 object detection.ipynb try modify use multi gpu version follows try train single gpu multi gpu respectively results 165 epochs scratch 1. single gpu ! http ww1.sinaimg.cn mw690 9ddd8b3bly1fiw9iwppcsj20rs0rsapr.jpg 2. multi gpu ! http ww1.sinaimg.cn mw690 9ddd8b3bly1fiw9kgp9loj20rs0rs7k0.jpg resonable? familiar new grammar sure wether made mistale.",1,accuracy sacrificed multi gpu using gluon,"accuracy sacrificed multi gpu using gluon environment info operating system ubuntu 16.04 mxnet version 0.11.0 installed source no, using pip install mxnet cu80 python version distribution python27 run demo https github.com zackchase mxnet straight dope blob master p06 c03 object detection.ipynb try modify use multi gpu version follows try train single gpu multi gpu respectively results 165 epochs scratch 1. single gpu ! http ww1.sinaimg.cn mw690 9ddd8b3bly1fiw9iwppcsj20rs0rsapr.jpg 2. multi gpu ! http ww1.sinaimg.cn mw690 9ddd8b3bly1fiw9kgp9loj20rs0rs7k0.jpg resonable? familiar new grammar sure wether made mistale."
incubator-mxnet,17086,"description rnn op gradient computation cpu broken source built mxnet. running language model training script ec2 instance. tested script latest source built mxnet. training, ran following log log message loss value change more. use mxnet build pip installation . log normal. gradient warning loss keeps changing reproduce training script found . reproduce log message, ran script following command tried solve it? problem occurred computing gradients https github.com dmlc gluon nlp blob master scripts language model word language model.py l381 gradient values order . normally gradient value within . thanks leezu , found error introduced mkldnn option. use mxnet built source mkldnn turned on, i.e., , gradient error appears whereas problem gone . therefore issue traced back mkldnn. zixuanweeei ciyongch pengzhao intel environment environment specs found",1,mkldnn rnn op gradient computation broken,"mkldnn rnn op gradient computation broken description rnn op gradient computation cpu broken source built mxnet. running language model training script ec2 instance. tested script latest source built mxnet. training, ran following log log message loss value change more. use mxnet build pip installation . log normal. gradient warning loss keeps changing reproduce training script found . reproduce log message, ran script following command tried solve it? problem occurred computing gradients https github.com dmlc gluon nlp blob master scripts language model word language model.py l381 gradient values order . normally gradient value within . thanks leezu , found error introduced mkldnn option. use mxnet built source mkldnn turned on, i.e., , gradient error appears whereas problem gone . therefore issue traced back mkldnn. zixuanweeei ciyongch pengzhao intel environment environment specs found"
incubator-mxnet,6974,"want get network output numpy array, achieved method. found slow since copies elements. computation takes ms, call takes 100 ms. obtain numpy array efficiently? also tried directly manipulate mxnet ndarray. used , since multi dimension indexing supported . methods support slicing contiguous region. makes big trouble me. btw, run program cpu arrays size 2000x6.",1,ndarray.asnumpy slow,"ndarray.asnumpy slow want get network output numpy array, achieved method. found slow since copies elements. computation takes ms, call takes 100 ms. obtain numpy array efficiently? also tried directly manipulate mxnet ndarray. used , since multi dimension indexing supported . methods support slicing contiguous region. makes big trouble me. btw, run program cpu arrays size 2000x6."
incubator-mxnet,9026,"test py aws ec2 p3.2xlarge gpu v100 , ami ami 77eb3a0f, python version 2.7. .py follow host, win10, mx0.12, gpu 940m, got near 110 samples seconds default params, surprisingly, p3.2xlarge, got 170 samples seconds. detail, , found volatile gpu utile always near 0 , t0 4 . why??? got custom dataiter?",1,slow mxnet0.12 even nvidia v100 gpu?,"slow mxnet0.12 even nvidia v100 gpu? test py aws ec2 p3.2xlarge gpu v100 , ami ami 77eb3a0f, python version 2.7. .py follow host, win10, mx0.12, gpu 940m, got near 110 samples seconds default params, surprisingly, p3.2xlarge, got 170 samples seconds. detail, , found volatile gpu utile always near 0 , t0 4 . why??? got custom dataiter?"
incubator-mxnet,4163,"using python version mxnet 64 bit ubuntu 16.04 os. changed fully connected layers vgg 16 network wanted fine tune it. first used 'fixed param names' mx.mod.module class seemed working well. accuracy getting better better, 75 accuracy 10th epoch. saved checkpoint 10th epoch, changed module removing 'fixed param names' parameters, set 'lr mult' 0 calling 'opt.set lr mult ', finally loaded checkpoint file continue training process. however, accuracy rapidly dropped 50 2 classes included training set. here's code segment using 'fixed param names' net, arg params, aux params mx.model.load checkpoint '.. model vgg16', 0 name list k k arg params 'fc' k mod mx.module.module net, context ctx, work load list wl, fixed param names name list here's codes 'set lr mult' method opt mx.optimizer.adam learning rate 0.001 mult dict k 0.0 k arg params 'fc' k opt.set lr mult mult dict mod.init optimizer optimizer opt understand 'fixed param names' parameter related 'grad req' calling executor, memory allocated gradients way. 'lr mult' case, gradients calculated, added weights learning rate 0. guess make difference memory occupation computation speed. result same, weights cases. different?? maybe wrong understanding matters. could someone help that? way, cannot understand difference 'fixed param names' 'blockgrad' operator. guess save memory cosuming, 'blockgrad' cut backpropagation completely. guess could implemented 'fixed param names' specifying layers blocking node, right? furthermore, wanted freeze middle part network keep rest parts trainable trainable parts frozen parts trainable parts deviation , guess 'blockgrad' would help, 'fixed param names' would function. results training correct, guess must something wrong understanding. would someone please help me? thx.",1,'fixed param names' 'lr mult' behave differently,"'fixed param names' 'lr mult' behave differently using python version mxnet 64 bit ubuntu 16.04 os. changed fully connected layers vgg 16 network wanted fine tune it. first used 'fixed param names' mx.mod.module class seemed working well. accuracy getting better better, 75 accuracy 10th epoch. saved checkpoint 10th epoch, changed module removing 'fixed param names' parameters, set 'lr mult' 0 calling 'opt.set lr mult ', finally loaded checkpoint file continue training process. however, accuracy rapidly dropped 50 2 classes included training set. here's code segment using 'fixed param names' net, arg params, aux params mx.model.load checkpoint '.. model vgg16', 0 name list k k arg params 'fc' k mod mx.module.module net, context ctx, work load list wl, fixed param names name list here's codes 'set lr mult' method opt mx.optimizer.adam learning rate 0.001 mult dict k 0.0 k arg params 'fc' k opt.set lr mult mult dict mod.init optimizer optimizer opt understand 'fixed param names' parameter related 'grad req' calling executor, memory allocated gradients way. 'lr mult' case, gradients calculated, added weights learning rate 0. guess make difference memory occupation computation speed. result same, weights cases. different?? maybe wrong understanding matters. could someone help that? way, cannot understand difference 'fixed param names' 'blockgrad' operator. guess save memory cosuming, 'blockgrad' cut backpropagation completely. guess could implemented 'fixed param names' specifying layers blocking node, right? furthermore, wanted freeze middle part network keep rest parts trainable trainable parts frozen parts trainable parts deviation , guess 'blockgrad' would help, 'fixed param names' would function. results training correct, guess must something wrong understanding. would someone please help me? thx."
incubator-mxnet,1092,run examples image classification train mnist.py single gpu double gpus. expectation accuracies slightly different. around 0.1 different . though big issue still confused completely equal theory. explain cause slight difference? thank u!,1,accuracy train mnist.py different single gpu multi gpus,accuracy train mnist.py different single gpu multi gpus run examples image classification train mnist.py single gpu double gpus. expectation accuracies slightly different. around 0.1 different . though big issue still confused completely equal theory. explain cause slight difference? thank u!
incubator-mxnet,12612,"use dcgan.py train dataset mnist, 25 epochs netds loss little high, looks like converge netd loss netg loss ! g loss https user images.githubusercontent.com 28485566 45806511 d582e400 bcf3 11e8 9972 462bece22ccb.png",1,cannot converge use dcgan mnist cifar10 dataset?,"cannot converge use dcgan mnist cifar10 dataset? use dcgan.py train dataset mnist, 25 epochs netds loss little high, looks like converge netd loss netg loss ! g loss https user images.githubusercontent.com 28485566 45806511 d582e400 bcf3 11e8 9972 462bece22ccb.png"
incubator-mxnet,6975,"environment info operating system ubuntu14.04 package used python r scala julia python mxnet version 0.10.1 installed source install source using python package, please provide python version distribution python 2.7 problem message 1. train model distribute computers mxnet 0.10.1, find slower train mxnet 0.9.4. 2. use profile analyze program. mxnet 0.10.1, find grad arrays pushed kv store backward layers compute finished. compute can't cover time data communicate. 3. mxnet 0.9.4 grad array convolution layer push kv store directly layers' backward compute. meet problem resolve it?",1,problem distribute training,"problem distribute training environment info operating system ubuntu14.04 package used python r scala julia python mxnet version 0.10.1 installed source install source using python package, please provide python version distribution python 2.7 problem message 1. train model distribute computers mxnet 0.10.1, find slower train mxnet 0.9.4. 2. use profile analyze program. mxnet 0.10.1, find grad arrays pushed kv store backward layers compute finished. compute can't cover time data communicate. 3. mxnet 0.9.4 grad array convolution layer push kv store directly layers' backward compute. meet problem resolve it?"
incubator-mxnet,3656,"tried learn mxnet data, achieved 50 . arch rly need mxnet, question wrong?",1,accuracy level 50,"accuracy level 50 tried learn mxnet data, achieved 50 . arch rly need mxnet, question wrong?"
incubator-mxnet,1228,"hey quite new mxnet, followed installation instructions succeeded installing windows 8.1 64 bit, ran train mnist.py network lenet without problem, quite slow accuracy end good around 99.2, run network lenet gpus 0 use gpu definitely lot faster accuracy never gets 10 terrible, must something wrong theoretically accuracy right? installed cuda 7.5 also extracted cuddn v3 indicated, everything runs without problem except accuracy terrible, running laptop nvidia 660m graphics card, compute capability 3.0. running file get train accuracy 0.098825",1,windows gpu accuracy extremely bad,"windows gpu accuracy extremely bad hey quite new mxnet, followed installation instructions succeeded installing windows 8.1 64 bit, ran train mnist.py network lenet without problem, quite slow accuracy end good around 99.2, run network lenet gpus 0 use gpu definitely lot faster accuracy never gets 10 terrible, must something wrong theoretically accuracy right? installed cuda 7.5 also extracted cuddn v3 indicated, everything runs without problem except accuracy terrible, running laptop nvidia 660m graphics card, compute capability 3.0. running file get train accuracy 0.098825"
incubator-mxnet,11469,"description there'a big performance regression augmentation recordio pipeline slowing 5000 samples sec 3000 samples sec resnet50 imagenet . linked pr https github.com apache incubator mxnet pull 11027 pr tries problematic, get 5k samples sec older commit d37f3a3e63cef5a79c3e673cec30e70f8bf83b3e pr may24. form got merged there's big slowdown. environment info package used python r scala julia python 3 build info pip nightly mxnet cu90 1.3.0b20180627 , well built source master commit pr got merged mxnet commit hash n build config tried without use libjpeg turbo, using increases speed bit 3500 , still much slower before. also enabled use cuda, use cudnn steps reproduce tried solve it? tried profile see might wrong tool perf. looks like opencv causing wait reason. please see figure 3 1. here's perf summary ! image https user images.githubusercontent.com 3457240 42059154 67db4e46 7ad7 11e8 875d 2ce64984cdfc.png 2. perf summary may 24 commit ! image https user images.githubusercontent.com 3457240 42059170 71a59904 7ad7 11e8 83da e7701e8dc69a.png 3. call graph using perf ! image https user images.githubusercontent.com 3457240 42059189 7e1170aa 7ad7 11e8 8f8a aab7dbfddd2b.png hetong007 piiswrong ideas?",1,performance regression augmentation,"performance regression augmentation description there'a big performance regression augmentation recordio pipeline slowing 5000 samples sec 3000 samples sec resnet50 imagenet . linked pr https github.com apache incubator mxnet pull 11027 pr tries problematic, get 5k samples sec older commit d37f3a3e63cef5a79c3e673cec30e70f8bf83b3e pr may24. form got merged there's big slowdown. environment info package used python r scala julia python 3 build info pip nightly mxnet cu90 1.3.0b20180627 , well built source master commit pr got merged mxnet commit hash n build config tried without use libjpeg turbo, using increases speed bit 3500 , still much slower before. also enabled use cuda, use cudnn steps reproduce tried solve it? tried profile see might wrong tool perf. looks like opencv causing wait reason. please see figure 3 1. here's perf summary ! image https user images.githubusercontent.com 3457240 42059154 67db4e46 7ad7 11e8 875d 2ce64984cdfc.png 2. perf summary may 24 commit ! image https user images.githubusercontent.com 3457240 42059170 71a59904 7ad7 11e8 83da e7701e8dc69a.png 3. call graph using perf ! image https user images.githubusercontent.com 3457240 42059189 7e1170aa 7ad7 11e8 8f8a aab7dbfddd2b.png hetong007 piiswrong ideas?"
incubator-mxnet,5074,"bugs installation issues, please provide following information. information provide, likely people able help you. environment info operating system ubuntu 14.04.5 compiler gcc 4.8.4 package used python r scala julia python mxnet version installed source installed git clone https github.com dmlc mxnet.git mxnet recursive mxnet commit hash using python package, please provide python version distribution python 2.7.13 anaconda 4.3.0 using r package, please provide r error message please paste full error message, including stack trace. minimum reproducible example using code, please provide short script reproduces error. steps reproduce running standard examples, please provide commands run lead error. 1. 2. 3. tried solve it? 1. 2. 3. build environment nvidia 375.26, cuda 8.0, gcc 4.8.4, ubundu 14.04.5, cudnn 5.1 use 4 pascal gpus titan x retrain resnet 50 model load epoch 90 imagenet'12 dataset. speed slow. furthermore, accuracy also wrong. following picture shows two procedures. two weird accuracy. mxnet latest version know why. change pascal gpu, use 4 m40 gpu issue old mxnet version, please see second picture . ! image https cloud.githubusercontent.com assets 3366247 23129516 f19b3afe f7bd 11e6 9f29 e089c7888c68.png obviously, abnormal phenomenon. normally, results shown following ! image https cloud.githubusercontent.com assets 3366247 23129668 89631b9a f7be 11e6 8c94 3951d057b8f8.png",1,training speed slow performance weird 4 titan x gpu ?,"training speed slow performance weird 4 titan x gpu ? bugs installation issues, please provide following information. information provide, likely people able help you. environment info operating system ubuntu 14.04.5 compiler gcc 4.8.4 package used python r scala julia python mxnet version installed source installed git clone https github.com dmlc mxnet.git mxnet recursive mxnet commit hash using python package, please provide python version distribution python 2.7.13 anaconda 4.3.0 using r package, please provide r error message please paste full error message, including stack trace. minimum reproducible example using code, please provide short script reproduces error. steps reproduce running standard examples, please provide commands run lead error. 1. 2. 3. tried solve it? 1. 2. 3. build environment nvidia 375.26, cuda 8.0, gcc 4.8.4, ubundu 14.04.5, cudnn 5.1 use 4 pascal gpus titan x retrain resnet 50 model load epoch 90 imagenet'12 dataset. speed slow. furthermore, accuracy also wrong. following picture shows two procedures. two weird accuracy. mxnet latest version know why. change pascal gpu, use 4 m40 gpu issue old mxnet version, please see second picture . ! image https cloud.githubusercontent.com assets 3366247 23129516 f19b3afe f7bd 11e6 9f29 e089c7888c68.png obviously, abnormal phenomenon. normally, results shown following ! image https cloud.githubusercontent.com assets 3366247 23129668 89631b9a f7be 11e6 8c94 3951d057b8f8.png"
incubator-mxnet,9171,"description mxnet using fusedrnncell bidirectional flag turned true, lead hanging i.e. infinite pause without progress error crash training run. details running single training run sequence sequence model using bucketingmodule. iam using encoder decoder network. using fusedrnncell bidirectional flag turned encoder unfused rnncell decoder. gpu utilization 15000mb 16000mb. cpu utilization 95 . batch training, forward pass backward pass. 5 15 epochs, training run gets stuck forward pass one mini batches. forward pass complete. errors thrown anything crash. gpu cpu utilization remains identically same. tried ablation many many things training run architecture, data, code etc . conclusion specifically using fusedrnncell bidirectional flag turned true causes problem. package used python environment info python info version 3.5.2 compiler gcc 5.4.0 20160609 build 'default', 'nov 23 2017 16 37 01' arch '64bit', 'elf' pip info version 9.0.1 directory usr local lib python3.5 dist packages pip mxnet info version 1.0.0 directory usr local lib python3.5 dist packages mxnet commit hash 25720d0e3c29232a37e2650f3ba3a2454f9367bb system info platform linux 4.4.0 1039 aws x86 64 ubuntu 16.04 xenial system linux node ip 172 31 85 194 release 4.4.0 1039 aws version 48 ubuntu smp wed oct 11 15 15 01 utc 2017 hardware info machine x86 64 processor x86 64 architecture x86 64 cpu op mode 32 bit, 64 bit byte order little endian cpu 64 line cpu list 0 63 thread per core 2 core per socket 16 socket 2 numa node 2 vendor id genuineintel cpu family 6 model 79 model name intel r xeon r cpu e5 2686 v4 2.30ghz stepping 1 cpu mhz 1200.582 cpu max mhz 3000.0000 cpu min mhz 1200.0000 bogomips 4600.09 hypervisor vendor xen virtualization type full l1d cache 32k l1i cache 32k l2 cache 256k l3 cache 46080k numa node0 cpu 0 15,32 47 numa node1 cpu 16 31,48 63 flags fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx pdpe1gb rdtscp lm constant tsc arch perfmon rep good nopl xtopology nonstop tsc aperfmperf eagerfpu pni pclmulqdq monitor est ssse3 fma cx16 pcid sse4 1 sse4 2 x2apic movbe popcnt tsc deadline timer aes xsave avx f16c rdrand hypervisor lahf lm abm 3dnowprefetch fsgsbase bmi1 hle avx2 smep bmi2 erms invpcid rtm rdseed adx xsaveopt ida network test setting timeout 10 timing conda https repo.continuum.io pkgs free , dns 0.0300 sec, load 0.0514 sec. timing gluon tutorial en http gluon.mxnet.io, dns 0.1141 sec, load 0.1956 sec. timing mxnet https github.com apache incubator mxnet, dns 0.0016 sec, load 0.4062 sec. timing gluon tutorial cn https zh.gluon.ai, dns 0.1799 sec, load 0.3847 sec. timing pypi https pypi.python.org pypi pip, dns 0.0046 sec, load 0.0126 sec. timing fashionmnist https apache mxnet.s3 accelerate.dualstack.amazonaws.com gluon dataset fashion mnist train labels idx1 ubyte.gz, dns 0.0154 sec, load 0.1567 sec.",1,"mxnet using fusedrnncell bidirectional flag turned true, lead hanging training run.","mxnet using fusedrnncell bidirectional flag turned true, lead hanging training run. description mxnet using fusedrnncell bidirectional flag turned true, lead hanging i.e. infinite pause without progress error crash training run. details running single training run sequence sequence model using bucketingmodule. iam using encoder decoder network. using fusedrnncell bidirectional flag turned encoder unfused rnncell decoder. gpu utilization 15000mb 16000mb. cpu utilization 95 . batch training, forward pass backward pass. 5 15 epochs, training run gets stuck forward pass one mini batches. forward pass complete. errors thrown anything crash. gpu cpu utilization remains identically same. tried ablation many many things training run architecture, data, code etc . conclusion specifically using fusedrnncell bidirectional flag turned true causes problem. package used python environment info python info version 3.5.2 compiler gcc 5.4.0 20160609 build 'default', 'nov 23 2017 16 37 01' arch '64bit', 'elf' pip info version 9.0.1 directory usr local lib python3.5 dist packages pip mxnet info version 1.0.0 directory usr local lib python3.5 dist packages mxnet commit hash 25720d0e3c29232a37e2650f3ba3a2454f9367bb system info platform linux 4.4.0 1039 aws x86 64 ubuntu 16.04 xenial system linux node ip 172 31 85 194 release 4.4.0 1039 aws version 48 ubuntu smp wed oct 11 15 15 01 utc 2017 hardware info machine x86 64 processor x86 64 architecture x86 64 cpu op mode 32 bit, 64 bit byte order little endian cpu 64 line cpu list 0 63 thread per core 2 core per socket 16 socket 2 numa node 2 vendor id genuineintel cpu family 6 model 79 model name intel r xeon r cpu e5 2686 v4 2.30ghz stepping 1 cpu mhz 1200.582 cpu max mhz 3000.0000 cpu min mhz 1200.0000 bogomips 4600.09 hypervisor vendor xen virtualization type full l1d cache 32k l1i cache 32k l2 cache 256k l3 cache 46080k numa node0 cpu 0 15,32 47 numa node1 cpu 16 31,48 63 flags fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx pdpe1gb rdtscp lm constant tsc arch perfmon rep good nopl xtopology nonstop tsc aperfmperf eagerfpu pni pclmulqdq monitor est ssse3 fma cx16 pcid sse4 1 sse4 2 x2apic movbe popcnt tsc deadline timer aes xsave avx f16c rdrand hypervisor lahf lm abm 3dnowprefetch fsgsbase bmi1 hle avx2 smep bmi2 erms invpcid rtm rdseed adx xsaveopt ida network test setting timeout 10 timing conda https repo.continuum.io pkgs free , dns 0.0300 sec, load 0.0514 sec. timing gluon tutorial en http gluon.mxnet.io, dns 0.1141 sec, load 0.1956 sec. timing mxnet https github.com apache incubator mxnet, dns 0.0016 sec, load 0.4062 sec. timing gluon tutorial cn https zh.gluon.ai, dns 0.1799 sec, load 0.3847 sec. timing pypi https pypi.python.org pypi pip, dns 0.0046 sec, load 0.0126 sec. timing fashionmnist https apache mxnet.s3 accelerate.dualstack.amazonaws.com gluon dataset fashion mnist train labels idx1 ubyte.gz, dns 0.0154 sec, load 0.1567 sec."
incubator-mxnet,12891,"hi, tried mxnet mkldnn avx 512 capable windows machine, performance terrible. noticed mkldnn pulled mxnet quite old, older version mkldnn enable openmp windows omp support added commit https github.com intel mkl dnn pull 260 . plan upgrade mkldnn version?",1,mkldnn performance windows upgrading mkldnn submodule?,"mkldnn performance windows upgrading mkldnn submodule? hi, tried mxnet mkldnn avx 512 capable windows machine, performance terrible. noticed mkldnn pulled mxnet quite old, older version mkldnn enable openmp windows omp support added commit https github.com intel mkl dnn pull 260 . plan upgrade mkldnn version?"
incubator-mxnet,16220,"description works slow imperative execution gpu x3 slower relu . details environment info required using python build info required built source n error message running gluoncv resnet18 v2 imagenet imperative, activation throughput 900 samples sec. x3 slower compared imperative, relu activation original version throughput 3000 samples sec. hybrid, relu activation original version throughput 3000 samples sec. hybrid, activation throughput 3000 samples sec. minimum reproducible example steps reproduce 1. start aws p3.8xlarge deep learning ami ubuntu version 24.1 machine 2. activate mxnet env 3. install gluoncv pip install gluoncv 4. download train imagenet.py gluoncv https gluon cv.mxnet.io downloads 3bb06a6d6d085b1bb501b30aaf6c21c5 train imagenet.py source https gluon cv.mxnet.io model zoo classification.html imagenet 5. modify line 257 https github.com dmlc gluon cv blob 745ed855d769534eb2e23f0c136cd5f1bc9b60b7 gluoncv model zoo resnet.py l257 home ubuntu anaconda3 envs mxnet p36 lib python3.6 site packages gluoncv model zoo resnet.py , replace 6. run tried solve it? n might related 11683",1,ndarray.clip works slow imperative execution gpu.,"ndarray.clip works slow imperative execution gpu. description works slow imperative execution gpu x3 slower relu . details environment info required using python build info required built source n error message running gluoncv resnet18 v2 imagenet imperative, activation throughput 900 samples sec. x3 slower compared imperative, relu activation original version throughput 3000 samples sec. hybrid, relu activation original version throughput 3000 samples sec. hybrid, activation throughput 3000 samples sec. minimum reproducible example steps reproduce 1. start aws p3.8xlarge deep learning ami ubuntu version 24.1 machine 2. activate mxnet env 3. install gluoncv pip install gluoncv 4. download train imagenet.py gluoncv https gluon cv.mxnet.io downloads 3bb06a6d6d085b1bb501b30aaf6c21c5 train imagenet.py source https gluon cv.mxnet.io model zoo classification.html imagenet 5. modify line 257 https github.com dmlc gluon cv blob 745ed855d769534eb2e23f0c136cd5f1bc9b60b7 gluoncv model zoo resnet.py l257 home ubuntu anaconda3 envs mxnet p36 lib python3.6 site packages gluoncv model zoo resnet.py , replace 6. run tried solve it? n might related 11683"
incubator-mxnet,12255,"import mxnet 8 processes simultaneously, cpu resources used program stagnates almost 5 minutes. works fine mxnet1.1 failed mxnet1.2 mxnet1.3 following sample code solution mxnet1.2 mxnet1.3? thanks.",1,pretty high cpu load import mxnet,"pretty high cpu load import mxnet import mxnet 8 processes simultaneously, cpu resources used program stagnates almost 5 minutes. works fine mxnet1.1 failed mxnet1.2 mxnet1.3 following sample code solution mxnet1.2 mxnet1.3? thanks."
incubator-mxnet,3039,"seems mx.nd.zeros runs pretty slow gpu, rather cpu. tested 980ti aws g2.8xlarge instance k520 gpu . cuda 7.0 cudnn r5. python codes results commented",1,mx.nd.zeros slow gpu,"mx.nd.zeros slow gpu seems mx.nd.zeros runs pretty slow gpu, rather cpu. tested 980ti aws g2.8xlarge instance k520 gpu . cuda 7.0 cudnn r5. python codes results commented"
incubator-mxnet,1889,"alright, running box cifar10 image training script 8 gpu system. seems like getting best performance using 4 gpus graph attached. expected expected possibly need additional configuration ? thanks ! ! mxnet cifar10 gpus https cloud.githubusercontent.com assets 442121 14619220 4a43764e 056b 11e6 9fc1 f8259789ca7f.png",1,benchmarking 8 gpu system,"benchmarking 8 gpu system alright, running box cifar10 image training script 8 gpu system. seems like getting best performance using 4 gpus graph attached. expected expected possibly need additional configuration ? thanks ! ! mxnet cifar10 gpus https cloud.githubusercontent.com assets 442121 14619220 4a43764e 056b 11e6 9fc1 f8259789ca7f.png"
incubator-mxnet,4195,"hi everyone, want use lstm cnn implement text recognition image. example recognize 'good' text image bounding box. use warp ctc loss function find softmax always same. environment info operating system ubuntu 16.04 compiler gcc 5.4 package used python r scala julia python mxnet commit hash 32cb6bc python version distribution python 2.7.11 anaconda 4.0.0 x86 64 error message following output final fullyconnected layer pred array 0.01282051, 0.01282051, 0.01282051, ..., 0.01282051, 0.01282051, 0.01282051 , 0.01282051, 0.01282051, 0.01282051, ..., 0.01282051, 0.01282051, 0.01282051 , 0.01282051, 0.01282051, 0.01282051, ..., 0.01282051, 0.01282051, 0.01282051 , ..., 0.01282051, 0.01282051, 0.01282051, ..., 0.01282051, 0.01282051, 0.01282051 , 0.01282051, 0.01282051, 0.01282051, ..., 0.01282051, 0.01282051, 0.01282051 , 0.01282051, 0.01282051, 0.01282051, ..., 0.01282051, 0.01282051, 0.01282051 , dtype float32 0.01282051 1 78 78 different chars classify. dataiter based bucket io.py rnn example dir, lstm part based warp ctc example. minimum reproducible example 1.my network structure 2.metric function 3.dataiter tried solve it? 1.i try use bi lstm get output problem 2.do mxnet provide api get middle layers' output fitting model?",1,problem bucketing lstm cnn text recognition,"problem bucketing lstm cnn text recognition hi everyone, want use lstm cnn implement text recognition image. example recognize 'good' text image bounding box. use warp ctc loss function find softmax always same. environment info operating system ubuntu 16.04 compiler gcc 5.4 package used python r scala julia python mxnet commit hash 32cb6bc python version distribution python 2.7.11 anaconda 4.0.0 x86 64 error message following output final fullyconnected layer pred array 0.01282051, 0.01282051, 0.01282051, ..., 0.01282051, 0.01282051, 0.01282051 , 0.01282051, 0.01282051, 0.01282051, ..., 0.01282051, 0.01282051, 0.01282051 , 0.01282051, 0.01282051, 0.01282051, ..., 0.01282051, 0.01282051, 0.01282051 , ..., 0.01282051, 0.01282051, 0.01282051, ..., 0.01282051, 0.01282051, 0.01282051 , 0.01282051, 0.01282051, 0.01282051, ..., 0.01282051, 0.01282051, 0.01282051 , 0.01282051, 0.01282051, 0.01282051, ..., 0.01282051, 0.01282051, 0.01282051 , dtype float32 0.01282051 1 78 78 different chars classify. dataiter based bucket io.py rnn example dir, lstm part based warp ctc example. minimum reproducible example 1.my network structure 2.metric function 3.dataiter tried solve it? 1.i try use bi lstm get output problem 2.do mxnet provide api get middle layers' output fitting model?"
incubator-mxnet,2147,"fine tuning model based inception bn using image dataset 10 classes case multi class problem . randomly split whole dataset training test subsets. training phase, observe training accuracy high i.e., 90 , validation accuracy rather low i.e., 20 . also tried use model predict tens test images. seems prediction scores 10 classes roughly test image, i.e., test images predicted class. double checked files generated. good. also, fine tuning, simply copied inception bn symbol file new one modified last fully connected layer name renamed. things look pretty good me.. similar fine tuning , i.e., need rename fully connected layer reset . anyone encounter problem before? thanks.",1,high training accuracy low validation accuracy?,"high training accuracy low validation accuracy? fine tuning model based inception bn using image dataset 10 classes case multi class problem . randomly split whole dataset training test subsets. training phase, observe training accuracy high i.e., 90 , validation accuracy rather low i.e., 20 . also tried use model predict tens test images. seems prediction scores 10 classes roughly test image, i.e., test images predicted class. double checked files generated. good. also, fine tuning, simply copied inception bn symbol file new one modified last fully connected layer name renamed. things look pretty good me.. similar fine tuning , i.e., need rename fully connected layer reset . anyone encounter problem before? thanks."
incubator-mxnet,14821,"hello, get problem. run dir , always gets stuck many hours. linux distro version lsb version core 4.1 amd64 core 4.1 noarch distributor id centos description centos linux release 7.2.1511 core release 7.2.1511 codename core envirs gpu type tesla v100 p100 nvidia driver version nvidia smi 410.48 384.81 cuda version 9.2 9.0 cudnn version 7.5.0 7.3.1 mxnet using pip mxnet cu92 mxnet cu90 anyone give advises?",1,always getting stuck many hours run python lstm ocr train.py dir example ctc,"always getting stuck many hours run python lstm ocr train.py dir example ctc hello, get problem. run dir , always gets stuck many hours. linux distro version lsb version core 4.1 amd64 core 4.1 noarch distributor id centos description centos linux release 7.2.1511 core release 7.2.1511 codename core envirs gpu type tesla v100 p100 nvidia driver version nvidia smi 410.48 384.81 cuda version 9.2 9.0 cudnn version 7.5.0 7.3.1 mxnet using pip mxnet cu92 mxnet cu90 anyone give advises?"
incubator-mxnet,2277,"find tasks, using multi gpu slower single gpu. may main reason this? example. try use mxnet solve image caption problem. find two strange results 1. using large batch size 100 much slower small batch size 4 2. using 4 gpu much slower use 1 gpu. tasks, find similar phenomenon.",1,running multi gpu slower single gpu,"running multi gpu slower single gpu find tasks, using multi gpu slower single gpu. may main reason this? example. try use mxnet solve image caption problem. find two strange results 1. using large batch size 100 much slower small batch size 4 2. using 4 gpu much slower use 1 gpu. tasks, find similar phenomenon."
incubator-mxnet,1767,"hi all, tried predict using libmxnet predict.so libmxnet.so find libmxnet predict.so much slower libmxnet.so. use inception bn 224 224 input image, run cpu 0 share predict.py script. using libmxnet.so. predict image cost time elapsed 0.445577. using libmxnet predict.so predict image cost time elapsed 2.475879. 6 times slower libmxnet.so can't figure wrong libmxnet predict.so , anybody ideas ? thank advance! haria",1,performance issue libmxnet predict.so,"performance issue libmxnet predict.so hi all, tried predict using libmxnet predict.so libmxnet.so find libmxnet predict.so much slower libmxnet.so. use inception bn 224 224 input image, run cpu 0 share predict.py script. using libmxnet.so. predict image cost time elapsed 0.445577. using libmxnet predict.so predict image cost time elapsed 2.475879. 6 times slower libmxnet.so can't figure wrong libmxnet predict.so , anybody ideas ? thank advance! haria"
incubator-mxnet,6888,"input sparse feature, total parameters 1.3g , speed distributed trainning slow.so mxnet can't support large parameters?",1,distributed mxnet slow parameter size increased,"distributed mxnet slow parameter size increased input sparse feature, total parameters 1.3g , speed distributed trainning slow.so mxnet can't support large parameters?"
incubator-mxnet,8681,"training machine p100. begining, training speed 400images secod. decreases gradually 130 images second 100 epoch. others user pytorch machine meet problem. anyone knows possibile reasons?",1,training p100 gets slower slower,"training p100 gets slower slower training machine p100. begining, training speed 400images secod. decreases gradually 130 images second 100 epoch. others user pytorch machine meet problem. anyone knows possibile reasons?"
incubator-mxnet,1516,"tried use adam replace sgd. nothing replaced optimiser, found speed dropped much, confusing. using sgd, got 0.6 sample per sec use large input fcn . using adam, got 0.4 sample per sec. know makes sense, since complexity adam seems high caused much difference speed. idea?",1,adam much slower sgd?,"adam much slower sgd? tried use adam replace sgd. nothing replaced optimiser, found speed dropped much, confusing. using sgd, got 0.6 sample per sec use large input fcn . using adam, got 0.4 sample per sec. know makes sense, since complexity adam seems high caused much difference speed. idea?"
incubator-mxnet,6892,"environment info ubuntu 16.04, gcc 5.4.0, mxnet 0.9.5, python 2.7 error message like train model without metric, use code follows. find memory usage keeps increasing full. define class named 'nometric' follows use metric. memory still keeps increasing, try luckily, memory usage increase anymore. want know must call asnumpy ? latest find whether use not, memory keeps increasing. similarly, use multi process read data queue acceleration, problem also appear. latest version mxnet fix bug.",1,stop metric made increasing memory usage,"stop metric made increasing memory usage environment info ubuntu 16.04, gcc 5.4.0, mxnet 0.9.5, python 2.7 error message like train model without metric, use code follows. find memory usage keeps increasing full. define class named 'nometric' follows use metric. memory still keeps increasing, try luckily, memory usage increase anymore. want know must call asnumpy ? latest find whether use not, memory keeps increasing. similarly, use multi process read data queue acceleration, problem also appear. latest version mxnet fix bug."
incubator-mxnet,10095,"profiler shows, nn.batchnorm axis 1 cause 10x lower speed application nn.batchnorm axis 1 . however, often need nn.batchnorm axis 1 nn.dense flatten false .",1,batchnorm axis 1 slower axis 1,"batchnorm axis 1 slower axis 1 profiler shows, nn.batchnorm axis 1 cause 10x lower speed application nn.batchnorm axis 1 . however, often need nn.batchnorm axis 1 nn.dense flatten false ."
incubator-mxnet,1138,"train accuracy continues increase validation accuracy. observed log. normal miss something? using following settings python example image classification train imagenet.py network inception bn data dir media ssd imagenet mxnet batch size 128 num examples 1281167 model prefix imagenet kv store local allreduce device lr factor 0.9 2016 01 01 21 43 39,380 node 0 update 48956 change learning rate 5.90490e 03 2016 01 01 21 43 49,904 node 0 epoch 4 train accuracy 0.548824 2016 01 01 21 43 49,904 node 0 epoch 4 time cost 20670.702 2016 01 01 21 46 44,073 node 0 epoch 4 validation accuracy 0.406630 2016 01 02 03 30 58,979 node 0 update 58747 change learning rate 5.31441e 03 2016 01 02 03 31 11,608 node 0 epoch 5 train accuracy 0.577686 2016 01 02 03 31 11,608 node 0 epoch 5 time cost 20667.375 2016 01 02 03 34 05,323 node 0 epoch 5 validation accuracy 0.404347 2016 01 02 09 18 14,071 node 0 update 68538 change learning rate 4.78297e 03 2016 01 02 09 18 28,803 node 0 epoch 6 train accuracy 0.600828 2016 01 02 09 18 28,803 node 0 epoch 6 time cost 20663.319 2016 01 02 09 21 23,119 node 0 epoch 6 validation accuracy 0.380495 thanks great tool guys developed",1,validation accuracy imagenet inception bn starts drop 5 epochs,"validation accuracy imagenet inception bn starts drop 5 epochs train accuracy continues increase validation accuracy. observed log. normal miss something? using following settings python example image classification train imagenet.py network inception bn data dir media ssd imagenet mxnet batch size 128 num examples 1281167 model prefix imagenet kv store local allreduce device lr factor 0.9 2016 01 01 21 43 39,380 node 0 update 48956 change learning rate 5.90490e 03 2016 01 01 21 43 49,904 node 0 epoch 4 train accuracy 0.548824 2016 01 01 21 43 49,904 node 0 epoch 4 time cost 20670.702 2016 01 01 21 46 44,073 node 0 epoch 4 validation accuracy 0.406630 2016 01 02 03 30 58,979 node 0 update 58747 change learning rate 5.31441e 03 2016 01 02 03 31 11,608 node 0 epoch 5 train accuracy 0.577686 2016 01 02 03 31 11,608 node 0 epoch 5 time cost 20667.375 2016 01 02 03 34 05,323 node 0 epoch 5 validation accuracy 0.404347 2016 01 02 09 18 14,071 node 0 update 68538 change learning rate 4.78297e 03 2016 01 02 09 18 28,803 node 0 epoch 6 train accuracy 0.600828 2016 01 02 09 18 28,803 node 0 epoch 6 time cost 20663.319 2016 01 02 09 21 23,119 node 0 epoch 6 validation accuracy 0.380495 thanks great tool guys developed"
incubator-mxnet,11763,"description run voc dataset. handle data read like fllow ! image https user images.githubusercontent.com 3112825 42734787 4d3dd150 887c 11e8 8b9a 0bf9409406f5.png ! image https user images.githubusercontent.com 3112825 42734807 7c3bbbb6 887c 11e8 9179 abeb78933e8c.png environment info required steps reproduce write vocdetection, code foohttp gluon cv.mxnet.io build examples datasets index.html. transforms replace vocdetection . tried solve it? 1. add display info check vocdetection, ok produce batch images merge labels bbox cls label . 2. add display info seem empty dict run hold never restore run. ! image https user images.githubusercontent.com 3112825 42734875 a82954da 887d 11e8 8004 e46abac52a11.png",1,"train ssd, hold read data","train ssd, hold read data description run voc dataset. handle data read like fllow ! image https user images.githubusercontent.com 3112825 42734787 4d3dd150 887c 11e8 8b9a 0bf9409406f5.png ! image https user images.githubusercontent.com 3112825 42734807 7c3bbbb6 887c 11e8 9179 abeb78933e8c.png environment info required steps reproduce write vocdetection, code foohttp gluon cv.mxnet.io build examples datasets index.html. transforms replace vocdetection . tried solve it? 1. add display info check vocdetection, ok produce batch images merge labels bbox cls label . 2. add display info seem empty dict run hold never restore run. ! image https user images.githubusercontent.com 3112825 42734875 a82954da 887d 11e8 8004 e46abac52a11.png"
incubator-mxnet,5747,"piiswrong questions , data shuffled every epoch? speed using multiple different jobs. launch second training job, uses different csv file, training speed drops, normal? why?",1,questions csviter,"questions csviter piiswrong questions , data shuffled every epoch? speed using multiple different jobs. launch second training job, uses different csv file, training speed drops, normal? why?"
incubator-mxnet,14838,recently found performance regression training imagenet resnet50v1 upgrading cudnn 7.3.1 7.5.0 speed droped 5800 images 4800 images environment aws dlami ami id ami 2dcceb57 available us east command code https github.com rahul003 deep learning benchmark mirror blob master mxnet benchmark train imagenet.py nightly pip packages impacted building cuddn 7.5.0. stable version mxnet pip packages impacted. using issue keep track everyone updated. cc szha dickjc123 stu1130 pinaraws,1,regression cudnn upgrade 7.3.1 7.5.0,regression cudnn upgrade 7.3.1 7.5.0 recently found performance regression training imagenet resnet50v1 upgrading cudnn 7.3.1 7.5.0 speed droped 5800 images 4800 images environment aws dlami ami id ami 2dcceb57 available us east command code https github.com rahul003 deep learning benchmark mirror blob master mxnet benchmark train imagenet.py nightly pip packages impacted building cuddn 7.5.0. stable version mxnet pip packages impacted. using issue keep track everyone updated. cc szha dickjc123 stu1130 pinaraws
incubator-mxnet,14073,"currently, given fp16 inputs, nd.layernorm sym.layernorm perform reduction fp16, losses precision. reduction done fp32 instead. sxjscience",1,fp16 support layernorm,"fp16 support layernorm currently, given fp16 inputs, nd.layernorm sym.layernorm perform reduction fp16, losses precision. reduction done fp32 instead. sxjscience"
incubator-mxnet,16891,description change upgraded mkldnn 1.0 caused performance images sec drop 200 points. error message put performance images sec training dropped 1300 images sec. prior change throughput range 1500 1530 images sec. reproduce attached gzip file contains training script trains resnet18 v2 network cifar10 dataset. image classification.tar.gz https github.com apache incubator mxnet files 3881313 image classification.tar.gz numbers measured c5.18xlarge ubuntu instance. steps reproduce paste commands ran produced error. 1. build install mxnet mkl pip wheel contains changes test machine. 2. unzip attached gzip file test machine. 3. install psutil gluoncv export kmp affinity anf omp num threads variable below. 4. run following command start training. sample output looks like below. environment 1. c5.18xlarge 2. ubuntu 14.04 lts,1,upgrading mkldnn 1.0 causes performance regression.,upgrading mkldnn 1.0 causes performance regression. description change upgraded mkldnn 1.0 caused performance images sec drop 200 points. error message put performance images sec training dropped 1300 images sec. prior change throughput range 1500 1530 images sec. reproduce attached gzip file contains training script trains resnet18 v2 network cifar10 dataset. image classification.tar.gz https github.com apache incubator mxnet files 3881313 image classification.tar.gz numbers measured c5.18xlarge ubuntu instance. steps reproduce paste commands ran produced error. 1. build install mxnet mkl pip wheel contains changes test machine. 2. unzip attached gzip file test machine. 3. install psutil gluoncv export kmp affinity anf omp num threads variable below. 4. run following command start training. sample output looks like below. environment 1. c5.18xlarge 2. ubuntu 14.04 lts
incubator-mxnet,3325,"training model mpi cluster 10 machines. training dataset large, distributing whole machine requires much time space. manually split 10 non overlap text file . use make 10 imagerecordio dataset. training multi machine, machine download use source training iterator. forget change parameters training imagerecorditer. change validation dataset. distribute complete machine. believe training procedure roughly use one . result disappointing. training accuracy curve good, even slightly better previous. however validation curve much worse. ! image https cloud.githubusercontent.com assets 3807357 18622871 2e7c70e8 7e68 11e6 9d02 0ef70b3edfcf.png blue curves training val curves use one complete red curves using splited .... mistake may have?",1,low validation accuracy split training data manually training multi machine,"low validation accuracy split training data manually training multi machine training model mpi cluster 10 machines. training dataset large, distributing whole machine requires much time space. manually split 10 non overlap text file . use make 10 imagerecordio dataset. training multi machine, machine download use source training iterator. forget change parameters training imagerecorditer. change validation dataset. distribute complete machine. believe training procedure roughly use one . result disappointing. training accuracy curve good, even slightly better previous. however validation curve much worse. ! image https cloud.githubusercontent.com assets 3807357 18622871 2e7c70e8 7e68 11e6 9d02 0ef70b3edfcf.png blue curves training val curves use one complete red curves using splited .... mistake may have?"
incubator-mxnet,10839,"instead using cached dockerfiles, make run downloads installs everything scratch. ensure third party dependencies actually still valid. see https github.com apache incubator mxnet issues 10837 example",0,execute runs ci without cache part nightly,"execute runs ci without cache part nightly instead using cached dockerfiles, make run downloads installs everything scratch. ensure third party dependencies actually still valid. see https github.com apache incubator mxnet issues 10837 example"
incubator-mxnet,9335,"want use sequence mini batches compare performance different optimizers. used mxnet.random.seed 1 . however, works initializer. mxnet.gluon.data.dataloader shuffle true, needs extra import random random.seed 1",0,mxnet.random.seed work mxnet.gluon.data.dataloader shuffle true,"mxnet.random.seed work mxnet.gluon.data.dataloader shuffle true want use sequence mini batches compare performance different optimizers. used mxnet.random.seed 1 . however, works initializer. mxnet.gluon.data.dataloader shuffle true, needs extra import random random.seed 1"
incubator-mxnet,14704,docstrings still improved better user experience. adding parameter descriptions examples docstrings. example modopts mapkvstoreoptimizersgdreset optimizertruerescalegradidx2nameforce initfalse list namespaces still need improved x x x,0,clojure documentation improve docstrings,clojure documentation improve docstrings docstrings still improved better user experience. adding parameter descriptions examples docstrings. example modopts mapkvstoreoptimizersgdreset optimizertruerescalegradidx2nameforce initfalse list namespaces still need improved x x x
incubator-mxnet,16431,"thanks nswamy inputs design discussions related project frankfliu explaining requirements use case customer perspective. problem statement one big un catered use cases mxnet loading model able run parallel inference model multiple threads sharing parameters. multiple user requests 1 https github.com apache incubator mxnet issues 3946 . also lot confusion around current state mxnet respect thread safety. doc attempts address three things 1. tries clarify current state mxnet respect thread safety. 2. tries give idea benefits expect adding feature. 3. attempts solve problem parallel inference providing multi threaded inference api c apis frontend apis cpp python , current state mxnet thread safety mxnet dependency engine thread safety examining mxnet dependency engine code, looks like designed thread safe. tried push convolution op multiple threads mxnet engine, see issues thread safety. used cpp package same. script provided https github.com anirudh2290 mxnet tree multithreaded inference poc cpp package example multithreading engine push mxnet op.cpp script pushes convolution op engine multiple threads. verify correctness op script https github.com anirudh2290 mxnet tree multithreaded inference poc test cached op ts check.py mxnet graph executor thread safety removed naiveengine restriction c predict api tried run multi threaded inference c predict api using threadedengine commenting check https github.com anirudh2290 mxnet tree multithreaded inference poc src c api c predict api.cc running example program core dumps memory leaks graph executor bind. shows graph executor thread safe. cached op gluon backend thread safety try create cached op main thread spawn multiple threads invoke cached op inside threads. script https github.com anirudh2290 mxnet tree multithreaded inference poc cpp package example multithreading engine push cached op.cpp multiple failures seen run one dmlc threadlocalstore 2 https github.com dmlc dmlc core issues 571 , mxplanmemory, retrieving forward ref count attribute. errors race condition w.r.t reading writing shared states cachedop. proposed solution additions prioritized 1.6 proposing add minimal thread safe cached op inference following 1. similar cached op, except supports inference use cases. 2. support inlining, dynamic shapes, bulking, static alloc. 3. use static thread local variables graphinfo maintains fwd graph state, buff maintains ndarray states op states. scope additional optimization w.r.t separation buffers inputs params 4. addition means instantiate one thread safe cached op per process. frontend api symbolblockthreadsafe needs singleton limitation. c api changes prioritized 1.6 adding new thread safe flag mxcreatecachedopex. set true create thread safe cached op instead cached op. add similar thread safe flag flags invoke free invoke thread safe cached op versions instead default versions. please see poc details 1. thread safe cached op code https github.com anirudh2290 mxnet tree multithreaded inference poc src imperative cached op threadsafe.h , https github.com anirudh2290 mxnet tree multithreaded inference poc src imperative cached op threadsafe.cc 2. example code invoking cached op inference multiple threads https github.com anirudh2290 mxnet tree multithreaded inference poc cpp package example multithreading engine push cached op.cpp use cases tested 1. create cached op single op convolution main thread. spawn additional threads invoke cached op thread. 2. create cached op full model resnet 18 main thread. spawn additional threads invoke cached op thread. cpp frontend changes priority 1.6 1. add singleton symbolblock threadsafe version imports api like python, targeted inference. 2. params loaded using ndarray module. 3. initially one context supported extended multi context. 4. forward call invoke cachedop passing input ndarrays param ndarrays. 5. initially sparse storage types supported casting supported. 6. added contrib api. access2rohit helping cpp api changes. python frontend changes lower priority, post 1.6 1. add symbolblock threadsafe version, singleton inheriting symbolblock imports forward api. 2. poc https github.com anirudh2290 mxnet tree multithreaded inference poc python mxnet gluon contrib block.py example call https github.com anirudh2290 mxnet tree multithreaded inference poc test symbolblock cached op ts.py 3. poc currently functioning hangs randomly. could waitforvar waitforall thread safety issues cross device copy thread safety issues issues usage python thread local. requires investigation. existing issues 1. dmlc core threadlocalstore issue 2 https github.com dmlc dmlc core issues 571 . reverting back mx thread local fixes issue need explore additional downsides reverting back. high priority 1.6 addressed https github.com apache incubator mxnet pull 16526 2. waitforvar waitforall thread safe. high priority 1.6 . 3 https github.com apache incubator mxnet issues 16434 3. python api issues mentioned above. lower priority, post 1.6 . expected benefits one big benefit able run inference model shared params multiple threads. current approach use multiprocessing library import mxnet process. saves lot memory footprint improves throughput inference single machine. obtain numbers wrote multiprocessing script python load model run inference multiple processes. please see python script https github.com anirudh2290 mxnet tree multithreaded inference poc test symbolblock cached op ts.py runs memory 12 parallel inferences. running model inference cpp, please see example https github.com anirudh2290 mxnet tree multithreaded inference poc cpp package example multithreading engine push cached op full model.cpp able run 960 parallel inferences though increased latency higher number parallel inferences. model coverage models tested mkldnn cudnn cudnn resnet 18 yes yes yes work progress list models added list. supported 1.6 ? since, new interface many things go wrong, starting small incrementally add support. lot features may work requires effort verification feasible 1.6. 1. operators tested existing model coverage supported. operators stateful operators, custom operators supported. 2. dense storage types supported currently. 3. multi gpu inference supported currently. 4. instantiating multiple instances symbolblockthreadsafe supported. run parallel inference one model per process. 5. dynamic shapes supported. 6. static alloc static shape supported. 7. bulking ops supported. 8. inference use cases, backward pass training use cases supported. 9. graph rewrites subgraph api currently supported. 10. python frontend changes references 1. https github.com apache incubator mxnet issues 3946 2. https github.com dmlc dmlc core issues 571 3. https github.com apache incubator mxnet issues 16434",0,rfc mxnet multithreaded inference interface,"rfc mxnet multithreaded inference interface thanks nswamy inputs design discussions related project frankfliu explaining requirements use case customer perspective. problem statement one big un catered use cases mxnet loading model able run parallel inference model multiple threads sharing parameters. multiple user requests 1 https github.com apache incubator mxnet issues 3946 . also lot confusion around current state mxnet respect thread safety. doc attempts address three things 1. tries clarify current state mxnet respect thread safety. 2. tries give idea benefits expect adding feature. 3. attempts solve problem parallel inference providing multi threaded inference api c apis frontend apis cpp python , current state mxnet thread safety mxnet dependency engine thread safety examining mxnet dependency engine code, looks like designed thread safe. tried push convolution op multiple threads mxnet engine, see issues thread safety. used cpp package same. script provided https github.com anirudh2290 mxnet tree multithreaded inference poc cpp package example multithreading engine push mxnet op.cpp script pushes convolution op engine multiple threads. verify correctness op script https github.com anirudh2290 mxnet tree multithreaded inference poc test cached op ts check.py mxnet graph executor thread safety removed naiveengine restriction c predict api tried run multi threaded inference c predict api using threadedengine commenting check https github.com anirudh2290 mxnet tree multithreaded inference poc src c api c predict api.cc running example program core dumps memory leaks graph executor bind. shows graph executor thread safe. cached op gluon backend thread safety try create cached op main thread spawn multiple threads invoke cached op inside threads. script https github.com anirudh2290 mxnet tree multithreaded inference poc cpp package example multithreading engine push cached op.cpp multiple failures seen run one dmlc threadlocalstore 2 https github.com dmlc dmlc core issues 571 , mxplanmemory, retrieving forward ref count attribute. errors race condition w.r.t reading writing shared states cachedop. proposed solution additions prioritized 1.6 proposing add minimal thread safe cached op inference following 1. similar cached op, except supports inference use cases. 2. support inlining, dynamic shapes, bulking, static alloc. 3. use static thread local variables graphinfo maintains fwd graph state, buff maintains ndarray states op states. scope additional optimization w.r.t separation buffers inputs params 4. addition means instantiate one thread safe cached op per process. frontend api symbolblockthreadsafe needs singleton limitation. c api changes prioritized 1.6 adding new thread safe flag mxcreatecachedopex. set true create thread safe cached op instead cached op. add similar thread safe flag flags invoke free invoke thread safe cached op versions instead default versions. please see poc details 1. thread safe cached op code https github.com anirudh2290 mxnet tree multithreaded inference poc src imperative cached op threadsafe.h , https github.com anirudh2290 mxnet tree multithreaded inference poc src imperative cached op threadsafe.cc 2. example code invoking cached op inference multiple threads https github.com anirudh2290 mxnet tree multithreaded inference poc cpp package example multithreading engine push cached op.cpp use cases tested 1. create cached op single op convolution main thread. spawn additional threads invoke cached op thread. 2. create cached op full model resnet 18 main thread. spawn additional threads invoke cached op thread. cpp frontend changes priority 1.6 1. add singleton symbolblock threadsafe version imports api like python, targeted inference. 2. params loaded using ndarray module. 3. initially one context supported extended multi context. 4. forward call invoke cachedop passing input ndarrays param ndarrays. 5. initially sparse storage types supported casting supported. 6. added contrib api. access2rohit helping cpp api changes. python frontend changes lower priority, post 1.6 1. add symbolblock threadsafe version, singleton inheriting symbolblock imports forward api. 2. poc https github.com anirudh2290 mxnet tree multithreaded inference poc python mxnet gluon contrib block.py example call https github.com anirudh2290 mxnet tree multithreaded inference poc test symbolblock cached op ts.py 3. poc currently functioning hangs randomly. could waitforvar waitforall thread safety issues cross device copy thread safety issues issues usage python thread local. requires investigation. existing issues 1. dmlc core threadlocalstore issue 2 https github.com dmlc dmlc core issues 571 . reverting back mx thread local fixes issue need explore additional downsides reverting back. high priority 1.6 addressed https github.com apache incubator mxnet pull 16526 2. waitforvar waitforall thread safe. high priority 1.6 . 3 https github.com apache incubator mxnet issues 16434 3. python api issues mentioned above. lower priority, post 1.6 . expected benefits one big benefit able run inference model shared params multiple threads. current approach use multiprocessing library import mxnet process. saves lot memory footprint improves throughput inference single machine. obtain numbers wrote multiprocessing script python load model run inference multiple processes. please see python script https github.com anirudh2290 mxnet tree multithreaded inference poc test symbolblock cached op ts.py runs memory 12 parallel inferences. running model inference cpp, please see example https github.com anirudh2290 mxnet tree multithreaded inference poc cpp package example multithreading engine push cached op full model.cpp able run 960 parallel inferences though increased latency higher number parallel inferences. model coverage models tested mkldnn cudnn cudnn resnet 18 yes yes yes work progress list models added list. supported 1.6 ? since, new interface many things go wrong, starting small incrementally add support. lot features may work requires effort verification feasible 1.6. 1. operators tested existing model coverage supported. operators stateful operators, custom operators supported. 2. dense storage types supported currently. 3. multi gpu inference supported currently. 4. instantiating multiple instances symbolblockthreadsafe supported. run parallel inference one model per process. 5. dynamic shapes supported. 6. static alloc static shape supported. 7. bulking ops supported. 8. inference use cases, backward pass training use cases supported. 9. graph rewrites subgraph api currently supported. 10. python frontend changes references 1. https github.com apache incubator mxnet issues 3946 2. https github.com dmlc dmlc core issues 571 3. https github.com apache incubator mxnet issues 16434"
incubator-mxnet,7445,"piiswrong, szha cudnn 7 supports ctc loss, perhaps discard current gpu implementation contrib.ctc loss adapted warpctc implementation use cudnn gpu? main reasons 1 requires maintenance effort ensure gpu implementation works new gpu architectures, requiring careful updating dependencies like modern gpu . 2 users still reporting problems memset issues using warpctc plugin 6121 think maintenance effort worthwhile almost every single user training cuda cudnn. thoughts?",0,using cudnn ctc loss,"using cudnn ctc loss piiswrong, szha cudnn 7 supports ctc loss, perhaps discard current gpu implementation contrib.ctc loss adapted warpctc implementation use cudnn gpu? main reasons 1 requires maintenance effort ensure gpu implementation works new gpu architectures, requiring careful updating dependencies like modern gpu . 2 users still reporting problems memset issues using warpctc plugin 6121 think maintenance effort worthwhile almost every single user training cuda cudnn. thoughts?"
incubator-mxnet,9937,"system ubuntu 16.04. run code got error g std c 11 c dmshadow force stream wall wsign compare o3 home duoduo github mxnet mshadow home duoduo github mxnet dmlc core include fpic home duoduo github mxnet nnvm include iinclude funroll loops wno unused variable wno unused parameter wno unknown pragmas wno unused local typedefs msse3 usr local cuda include dmshadow use cblas 1 dmshadow use mkl 0 dmshadow rabit ps 0 dmshadow dist ps 0 dmshadow use pascal 0 dmxnet use opencv 1 usr local include opencv usr local include fopenmp dmshadow use cudnn 1 home duoduo github mxnet cub dmxnet use nvrtc 0 mmd c src operator nn softmax.cc build src operator nn softmax.o g std c 11 c dmshadow force stream wall wsign compare o3 home duoduo github mxnet mshadow home duoduo github mxnet dmlc core include fpic home duoduo github mxnet nnvm include iinclude funroll loops wno unused variable wno unused parameter wno unknown pragmas wno unused local typedefs msse3 usr local cuda include dmshadow use cblas 1 dmshadow use mkl 0 dmshadow rabit ps 0 dmshadow dist ps 0 dmshadow use pascal 0 dmxnet use opencv 1 usr local include opencv usr local include fopenmp dmshadow use cudnn 1 home duoduo github mxnet cub dmxnet use nvrtc 0 mmd c src operator mkl mkl memory.cc build src operator mkl mkl memory.o g std c 11 c dmshadow force stream wall wsign compare o3 home duoduo github mxnet mshadow home duoduo github mxnet dmlc core include fpic home duoduo github mxnet nnvm include iinclude funroll loops wno unused variable wno unused parameter wno unknown pragmas wno unused local typedefs msse3 usr local cuda include dmshadow use cblas 1 dmshadow use mkl 0 dmshadow rabit ps 0 dmshadow dist ps 0 dmshadow use pascal 0 dmxnet use opencv 1 usr local include opencv usr local include fopenmp dmshadow use cudnn 1 home duoduo github mxnet cub dmxnet use nvrtc 0 mmd c src operator tensor elemwise binary broadcast op basic.cc build src operator tensor elemwise binary broadcast op basic.o g std c 11 c dmshadow force stream wall wsign compare o3 home duoduo github mxnet mshadow home duoduo github mxnet dmlc core include fpic home duoduo github mxnet nnvm include iinclude funroll loops wno unused variable wno unused parameter wno unknown pragmas wno unused local typedefs msse3 usr local cuda include dmshadow use cblas 1 dmshadow use mkl 0 dmshadow rabit ps 0 dmshadow dist ps 0 dmshadow use pascal 0 dmxnet use opencv 1 usr local include opencv usr local include fopenmp dmshadow use cudnn 1 home duoduo github mxnet cub dmxnet use nvrtc 0 mmd c src operator tensor elemwise binary op extended.cc build src operator tensor elemwise binary op extended.o file included home duoduo github mxnet mshadow mshadow tensor.h 16 0, include mxnet base.h 13, src operator nn . .. mxnet op.h 10, src operator nn . softmax inl.h 11, src operator nn softmax.cc 6 home duoduo github mxnet mshadow mshadow . base.h 136 23 fatal error cblas.h file directory compilation terminated. makefile 207 recipe target 'build src operator nn softmax.o' failed make build src operator nn softmax.o error 1 make waiting unfinished jobs.... file included home duoduo github mxnet mshadow mshadow tensor.h 16 0, include mxnet . base.h 13, include mxnet operator.h 19, src operator mkl .. operator common.h 13, src operator mkl mkl memory.cc 22 home duoduo github mxnet mshadow mshadow . base.h 136 23 fatal error cblas.h file directory compilation terminated. makefile 207 recipe target 'build src operator mkl mkl memory.o' failed make build src operator mkl mkl memory.o error 1 file included home duoduo github mxnet mshadow mshadow tensor.h 16 0, include mxnet . base.h 13, include mxnet operator util.h 24, src operator tensor . elemwise unary op.h 9, src operator tensor elemwise binary broadcast op basic.cc 6 home duoduo github mxnet mshadow mshadow . base.h 136 23 fatal error cblas.h file directory compilation terminated. makefile 207 recipe target 'build src operator tensor elemwise binary broadcast op basic.o' failed make build src operator tensor elemwise binary broadcast op basic.o error 1 file included home duoduo github mxnet mshadow mshadow tensor.h 16 0, include mxnet . base.h 13, include mxnet operator util.h 24, src operator tensor . elemwise unary op.h 9, src operator tensor elemwise binary op extended.cc 6 home duoduo github mxnet mshadow mshadow . base.h 136 23 fatal error cblas.h file directory compilation terminated. makefile 207 recipe target 'build src operator tensor elemwise binary op extended.o' failed make build src operator tensor elemwise binary op extended.o error 1 solve problem",0,fatal error cblas.h file directory,"fatal error cblas.h file directory system ubuntu 16.04. run code got error g std c 11 c dmshadow force stream wall wsign compare o3 home duoduo github mxnet mshadow home duoduo github mxnet dmlc core include fpic home duoduo github mxnet nnvm include iinclude funroll loops wno unused variable wno unused parameter wno unknown pragmas wno unused local typedefs msse3 usr local cuda include dmshadow use cblas 1 dmshadow use mkl 0 dmshadow rabit ps 0 dmshadow dist ps 0 dmshadow use pascal 0 dmxnet use opencv 1 usr local include opencv usr local include fopenmp dmshadow use cudnn 1 home duoduo github mxnet cub dmxnet use nvrtc 0 mmd c src operator nn softmax.cc build src operator nn softmax.o g std c 11 c dmshadow force stream wall wsign compare o3 home duoduo github mxnet mshadow home duoduo github mxnet dmlc core include fpic home duoduo github mxnet nnvm include iinclude funroll loops wno unused variable wno unused parameter wno unknown pragmas wno unused local typedefs msse3 usr local cuda include dmshadow use cblas 1 dmshadow use mkl 0 dmshadow rabit ps 0 dmshadow dist ps 0 dmshadow use pascal 0 dmxnet use opencv 1 usr local include opencv usr local include fopenmp dmshadow use cudnn 1 home duoduo github mxnet cub dmxnet use nvrtc 0 mmd c src operator mkl mkl memory.cc build src operator mkl mkl memory.o g std c 11 c dmshadow force stream wall wsign compare o3 home duoduo github mxnet mshadow home duoduo github mxnet dmlc core include fpic home duoduo github mxnet nnvm include iinclude funroll loops wno unused variable wno unused parameter wno unknown pragmas wno unused local typedefs msse3 usr local cuda include dmshadow use cblas 1 dmshadow use mkl 0 dmshadow rabit ps 0 dmshadow dist ps 0 dmshadow use pascal 0 dmxnet use opencv 1 usr local include opencv usr local include fopenmp dmshadow use cudnn 1 home duoduo github mxnet cub dmxnet use nvrtc 0 mmd c src operator tensor elemwise binary broadcast op basic.cc build src operator tensor elemwise binary broadcast op basic.o g std c 11 c dmshadow force stream wall wsign compare o3 home duoduo github mxnet mshadow home duoduo github mxnet dmlc core include fpic home duoduo github mxnet nnvm include iinclude funroll loops wno unused variable wno unused parameter wno unknown pragmas wno unused local typedefs msse3 usr local cuda include dmshadow use cblas 1 dmshadow use mkl 0 dmshadow rabit ps 0 dmshadow dist ps 0 dmshadow use pascal 0 dmxnet use opencv 1 usr local include opencv usr local include fopenmp dmshadow use cudnn 1 home duoduo github mxnet cub dmxnet use nvrtc 0 mmd c src operator tensor elemwise binary op extended.cc build src operator tensor elemwise binary op extended.o file included home duoduo github mxnet mshadow mshadow tensor.h 16 0, include mxnet base.h 13, src operator nn . .. mxnet op.h 10, src operator nn . softmax inl.h 11, src operator nn softmax.cc 6 home duoduo github mxnet mshadow mshadow . base.h 136 23 fatal error cblas.h file directory compilation terminated. makefile 207 recipe target 'build src operator nn softmax.o' failed make build src operator nn softmax.o error 1 make waiting unfinished jobs.... file included home duoduo github mxnet mshadow mshadow tensor.h 16 0, include mxnet . base.h 13, include mxnet operator.h 19, src operator mkl .. operator common.h 13, src operator mkl mkl memory.cc 22 home duoduo github mxnet mshadow mshadow . base.h 136 23 fatal error cblas.h file directory compilation terminated. makefile 207 recipe target 'build src operator mkl mkl memory.o' failed make build src operator mkl mkl memory.o error 1 file included home duoduo github mxnet mshadow mshadow tensor.h 16 0, include mxnet . base.h 13, include mxnet operator util.h 24, src operator tensor . elemwise unary op.h 9, src operator tensor elemwise binary broadcast op basic.cc 6 home duoduo github mxnet mshadow mshadow . base.h 136 23 fatal error cblas.h file directory compilation terminated. makefile 207 recipe target 'build src operator tensor elemwise binary broadcast op basic.o' failed make build src operator tensor elemwise binary broadcast op basic.o error 1 file included home duoduo github mxnet mshadow mshadow tensor.h 16 0, include mxnet . base.h 13, include mxnet operator util.h 24, src operator tensor . elemwise unary op.h 9, src operator tensor elemwise binary op extended.cc 6 home duoduo github mxnet mshadow mshadow . base.h 136 23 fatal error cblas.h file directory compilation terminated. makefile 207 recipe target 'build src operator tensor elemwise binary op extended.o' failed make build src operator tensor elemwise binary op extended.o error 1 solve problem"
incubator-mxnet,13743,"windows gpu stage ci, different tests fail. fails commit passes updating readme https github.com apache incubator mxnet pull 13595 http jenkins.mxnet ci.amazon ml.com blue organizations jenkins mxnet validation 2fwindows gpu detail pr 13595 9 pipeline http jenkins.mxnet ci.amazon ml.com blue organizations jenkins mxnet validation 2fwindows gpu detail pr 13681 37 pipeline seems occur windows gpu build. q discussion, please start discussion thread https discuss.mxnet.io description brief description problem 2 sentences. environment info required python diagnose.py package used python r scala julia using scala user, please provide 1. java version 2. maven version 3. scala runtime applicable r user, please provide r build info required built source compiler gcc clang mingw visual studio mxnet commit hash paste output here. build config paste content config.mk, build command. error message paste complete error message, including stack trace. minimum reproducible example using code, please provide short script reproduces error. otherwise, please provide link existing example. steps reproduce paste commands ran produced error. 1. 2. tried solve it? 1. 2.",0,flaky test windows gpu ci,"flaky test windows gpu ci windows gpu stage ci, different tests fail. fails commit passes updating readme https github.com apache incubator mxnet pull 13595 http jenkins.mxnet ci.amazon ml.com blue organizations jenkins mxnet validation 2fwindows gpu detail pr 13595 9 pipeline http jenkins.mxnet ci.amazon ml.com blue organizations jenkins mxnet validation 2fwindows gpu detail pr 13681 37 pipeline seems occur windows gpu build. q discussion, please start discussion thread https discuss.mxnet.io description brief description problem 2 sentences. environment info required python diagnose.py package used python r scala julia using scala user, please provide 1. java version 2. maven version 3. scala runtime applicable r user, please provide r build info required built source compiler gcc clang mingw visual studio mxnet commit hash paste output here. build config paste content config.mk, build command. error message paste complete error message, including stack trace. minimum reproducible example using code, please provide short script reproduces error. otherwise, please provide link existing example. steps reproduce paste commands ran produced error. 1. 2. tried solve it? 1. 2."
incubator-mxnet,11843,description mxnet coreml package tools coreml ci new releases breaks package directly affects customers. see issue 10349 https github.com apache incubator mxnet issues 10349 . add tests package ci system. environment info required package used python r scala julia python build info required built source compiler gcc clang mingw visual studio g mxnet commit hash e4134c8270c1b944278b1e0331313074b1d97cc0,0,ci tools coreml package,ci tools coreml package description mxnet coreml package tools coreml ci new releases breaks package directly affects customers. see issue 10349 https github.com apache incubator mxnet issues 10349 . add tests package ci system. environment info required package used python r scala julia python build info required built source compiler gcc clang mingw visual studio g mxnet commit hash e4134c8270c1b944278b1e0331313074b1d97cc0
incubator-mxnet,14484,"description training fcn model gluon cv 2 gpus encounter different perhaps related issues depending kind kvstore use 'local' 'device' . think gluon cv issue. test script included. environment info required package used python r scala julia python error message kvstore 'local' kvstore 'device' error, process hangs trying push kvstore . example script includes debug code narrow process hangs. note specific layer stops varies. minimum reproducible example steps reproduce 1. run script, setting kvstore type either . tried solve it? 1. disabling gc beginning epoch enabling end, seemed work one similar seeming issue, made difference me. note still get result using sub classed version gluon.trainer.",0,odd behaviour 'device' kvstore cuda illegal memory access errors,"odd behaviour 'device' kvstore cuda illegal memory access errors description training fcn model gluon cv 2 gpus encounter different perhaps related issues depending kind kvstore use 'local' 'device' . think gluon cv issue. test script included. environment info required package used python r scala julia python error message kvstore 'local' kvstore 'device' error, process hangs trying push kvstore . example script includes debug code narrow process hangs. note specific layer stops varies. minimum reproducible example steps reproduce 1. run script, setting kvstore type either . tried solve it? 1. disabling gc beginning epoch enabling end, seemed work one similar seeming issue, made difference me. note still get result using sub classed version gluon.trainer."
incubator-mxnet,14981,description test large tensor gpu step failing see http jenkins.mxnet ci.amazon ml.com blue organizations jenkins nightlytestsforbinaries detail master 312 pipeline 144 full log,0,ci nightlytestsforbinaries test large tensor gpu failing,ci nightlytestsforbinaries test large tensor gpu failing description test large tensor gpu step failing see http jenkins.mxnet ci.amazon ml.com blue organizations jenkins nightlytestsforbinaries detail master 312 pipeline 144 full log
incubator-mxnet,4105,"want create custom iterator r. initial example use mnist data. would like iterator gets input raw data, would matrix 60000 examples times 784 image expanded vector whose output, every iteration, would matrix operation inside iterator following know way reimplement operator r. know done python c , sure r. interface something like help appreciated piiswrong thirdwing",0,r data augmentation basic mnist example,"r data augmentation basic mnist example want create custom iterator r. initial example use mnist data. would like iterator gets input raw data, would matrix 60000 examples times 784 image expanded vector whose output, every iteration, would matrix operation inside iterator following know way reimplement operator r. know done python c , sure r. interface something like help appreciated piiswrong thirdwing"
incubator-mxnet,10101,"dear all, would useful one could add nn layers gluon custom model inside list, similar , something like think many use cases, one important one indexing neuroevolution problems, i.e. using variable architecture specified set layers. thank much great work put gluon mxnet.",0,gluon feature request proper registration initialization layers inside list container custom hybrid blocks,"gluon feature request proper registration initialization layers inside list container custom hybrid blocks dear all, would useful one could add nn layers gluon custom model inside list, similar , something like think many use cases, one important one indexing neuroevolution problems, i.e. using variable architecture specified set layers. thank much great work put gluon mxnet."
incubator-mxnet,6918,"bugs installation issues, please provide following information. information provide, likely people able help you. environment info operating system centos release 6.9 final compiler gcc version 4.9.2 20150212 red hat 4.9.2 6 gcc package used python r scala julia r mxnet version 0.10 installed source mxnet commit hash using python package, please provide python version distribution using r package, please provide 3.4 r sessioninfo package null r version 3.4.0 2017 04 21 platform x86 64 redhat linux gnu 64 bit running centos release 6.9 final matrix products default blas usr lib64 r lib librblas.so lapack usr lib64 r lib librlapack.so locale 1 lc ctype en us.utf 8 lc numeric c 3 lc time en us.utf 8 lc collate en us.utf 8 5 lc monetary en us.utf 8 lc messages en us.utf 8 7 lc paper en us.utf 8 lc name c 9 lc address c lc telephone c 11 lc measurement en us.utf 8 lc identification c attached base packages 1 stats graphics grdevices utils datasets methods base loaded via namespace attached 1 drat 0.1.2 compiler 3.4.0 tools 3.4.0 error message please paste full error message, including stack trace. warning message package mxnet available r version 3.4.0 minimum reproducible example using code, please provide short script reproduces error. steps reproduce running standard examples, please provide commands run lead error. 1. r console entered following commands install.packages drat , repos https cran.rstudio.com drat addrepo dmlc install.packages mxnet 2. 3. tried solve it? 1. attempted install source received following error message running make rpkg command mkdir p r package inst mkdir p r package inst libs cp rf lib libmxnet.so r package inst libs mkdir p r package inst include cp rf include r package inst include cp rf dmlc core include r package inst include cp rf nnvm include r package inst include echo import rcpp r package namespace echo import methods r package namespace r cmd install r package make 1 entering directory all'. make 1 leaving directory root mxnet r package src' man pages found package mxnet error package namespace load failed mxnet .onload failed loadnamespace 'mxnet', details call dyn.load file, dllpath dllpath, error unable load shared object ' usr lib64 r library mxnet libs libmxnet.so' libopenblas.so.0 cannot open shared object file file directory error loading failed execution halted 2. 3.",0,mxnet r 3.4 installation centos release 6.9 final,"mxnet r 3.4 installation centos release 6.9 final bugs installation issues, please provide following information. information provide, likely people able help you. environment info operating system centos release 6.9 final compiler gcc version 4.9.2 20150212 red hat 4.9.2 6 gcc package used python r scala julia r mxnet version 0.10 installed source mxnet commit hash using python package, please provide python version distribution using r package, please provide 3.4 r sessioninfo package null r version 3.4.0 2017 04 21 platform x86 64 redhat linux gnu 64 bit running centos release 6.9 final matrix products default blas usr lib64 r lib librblas.so lapack usr lib64 r lib librlapack.so locale 1 lc ctype en us.utf 8 lc numeric c 3 lc time en us.utf 8 lc collate en us.utf 8 5 lc monetary en us.utf 8 lc messages en us.utf 8 7 lc paper en us.utf 8 lc name c 9 lc address c lc telephone c 11 lc measurement en us.utf 8 lc identification c attached base packages 1 stats graphics grdevices utils datasets methods base loaded via namespace attached 1 drat 0.1.2 compiler 3.4.0 tools 3.4.0 error message please paste full error message, including stack trace. warning message package mxnet available r version 3.4.0 minimum reproducible example using code, please provide short script reproduces error. steps reproduce running standard examples, please provide commands run lead error. 1. r console entered following commands install.packages drat , repos https cran.rstudio.com drat addrepo dmlc install.packages mxnet 2. 3. tried solve it? 1. attempted install source received following error message running make rpkg command mkdir p r package inst mkdir p r package inst libs cp rf lib libmxnet.so r package inst libs mkdir p r package inst include cp rf include r package inst include cp rf dmlc core include r package inst include cp rf nnvm include r package inst include echo import rcpp r package namespace echo import methods r package namespace r cmd install r package make 1 entering directory all'. make 1 leaving directory root mxnet r package src' man pages found package mxnet error package namespace load failed mxnet .onload failed loadnamespace 'mxnet', details call dyn.load file, dllpath dllpath, error unable load shared object ' usr lib64 r library mxnet libs libmxnet.so' libopenblas.so.0 cannot open shared object file file directory error loading failed execution halted 2. 3."
incubator-mxnet,7272,https builds.apache.org blue organizations jenkins incubator mxnet detail master 83 pipeline output amalgamation running shell script tests ci build ci build.sh cpu make c amalgamation use blas openblas min 1 workspace home jenkins jenkins slave workspace amalgamation ci docker extra params command make c amalgamation use blas openblas min 1 container type cpu build tag jenkins incubator mxnet master 83 node name mxnet3 docker container name mx ci.cpu pre command tests ci build user building container mx ci.cpu sending build context docker daemon 59.39 kb step 1 9 ubuntu 14.04 4a2820e686c4 step 2 9 copy install ubuntu install core.sh install using cache a2648bbfb7e4 step 3 9 run install ubuntu install core.sh using cache bd54bb963df0 step 4 9 copy install ubuntu install python.sh install using cache b79f713d5145 step 5 9 run install ubuntu install python.sh using cache 570842ae2af2 step 6 9 copy install ubuntu install scala.sh install using cache 6f8e5f2011ba step 7 9 run install ubuntu install scala.sh using cache e2b49cb08c67 step 8 9 copy install ubuntu install r.sh install using cache f5f990f7f4ac step 9 9 run install ubuntu install r.sh using cache d795cd130ad6 successfully built d795cd130ad6 running 'make c amalgamation use blas openblas min 1' inside mx ci.cpu... adding group workspace amalgamation' g std c 11 wno unknown pragmas wall dmshadow use cblas 0 ddisable openmp 1 dmshadow use cuda 0 dmshadow use mkl 0 dmshadow rabit ps 0 dmshadow dist ps 0 ddmlc log stack trace 0 dmshadow force stream dmxnet use opencv 0 dmxnet predict 1 fpic mxnet predict all.o c mxnet predict all.cc mxnet predict all.cc 42 37 fatal error io threaded input split.h file directory include compilation terminated. make leaving directory workspace amalgamation' make mxnet predict all.o error 1,0,jenkins incubator mxnet pipeline broken master,jenkins incubator mxnet pipeline broken master https builds.apache.org blue organizations jenkins incubator mxnet detail master 83 pipeline output amalgamation running shell script tests ci build ci build.sh cpu make c amalgamation use blas openblas min 1 workspace home jenkins jenkins slave workspace amalgamation ci docker extra params command make c amalgamation use blas openblas min 1 container type cpu build tag jenkins incubator mxnet master 83 node name mxnet3 docker container name mx ci.cpu pre command tests ci build user building container mx ci.cpu sending build context docker daemon 59.39 kb step 1 9 ubuntu 14.04 4a2820e686c4 step 2 9 copy install ubuntu install core.sh install using cache a2648bbfb7e4 step 3 9 run install ubuntu install core.sh using cache bd54bb963df0 step 4 9 copy install ubuntu install python.sh install using cache b79f713d5145 step 5 9 run install ubuntu install python.sh using cache 570842ae2af2 step 6 9 copy install ubuntu install scala.sh install using cache 6f8e5f2011ba step 7 9 run install ubuntu install scala.sh using cache e2b49cb08c67 step 8 9 copy install ubuntu install r.sh install using cache f5f990f7f4ac step 9 9 run install ubuntu install r.sh using cache d795cd130ad6 successfully built d795cd130ad6 running 'make c amalgamation use blas openblas min 1' inside mx ci.cpu... adding group workspace amalgamation' g std c 11 wno unknown pragmas wall dmshadow use cblas 0 ddisable openmp 1 dmshadow use cuda 0 dmshadow use mkl 0 dmshadow rabit ps 0 dmshadow dist ps 0 ddmlc log stack trace 0 dmshadow force stream dmxnet use opencv 0 dmxnet predict 1 fpic mxnet predict all.o c mxnet predict all.cc mxnet predict all.cc 42 37 fatal error io threaded input split.h file directory include compilation terminated. make leaving directory workspace amalgamation' make mxnet predict all.o error 1
incubator-mxnet,3622,"problems use exts parametes im2rec.py. right use default value,but error use like exts '.jpg','.jpeg' . why?",0,use parameters im2rec.py?,"use parameters im2rec.py? problems use exts parametes im2rec.py. right use default value,but error use like exts '.jpg','.jpeg' . why?"
incubator-mxnet,1049,occur error using mean.img as.array mx.nd.load inception mean 224.nd mean img error mx.nd.load inception mean 224.nd mean img object type 'externalptr' subsettable could fix it?,0,mean.img error,mean.img error occur error using mean.img as.array mx.nd.load inception mean 224.nd mean img error mx.nd.load inception mean 224.nd mean img object type 'externalptr' subsettable could fix it?
incubator-mxnet,8983,"description debug conv stop breakpoint. environment info required os ubuntu 16.04 xenial build info required built source gcc mxnet 1.0.0 cpu build config debug 1 cuda 0 cudnn 0 cuda path none make j nproc error message gdb args python debug conv.py gnu gdb ubuntu 7.11.1 0ubuntu1 16.5 7.11.1 copyright c 2016 free software foundation, inc. license gplv3 gnu gpl version 3 later free software free change redistribute it. warranty, extent permitted law. type show copying show warranty details. gdb configured x86 64 linux gnu . type show configuration configuration details. bug reporting instructions, please see . find gdb manual documentation resources online . help, type help . type apropos word search commands related word reading symbols python...done. gdb b incubator mxnet src operator convolution v1 inl.h 130 source file named incubator mxnet src operator convolution v1 inl.h. make breakpoint pending future shared library load? n n gdb run starting program home hl anaconda2 bin python debug conv.py thread debugging using libthread db enabled using host libthread db library lib x86 64 linux gnu libthread db.so.1 . new thread 0x7fffe57a4700 lwp 12340 new thread 0x7fffe4fa3700 lwp 12341 new thread 0x7fffe47a2700 lwp 12342 new thread 0x7fffe3fa1700 lwp 12343 new thread 0x7fffe37a0700 lwp 12344 thread 0x7fffe37a0700 lwp 12344 exited thread 0x7fffe3fa1700 lwp 12343 exited thread 0x7fffe47a2700 lwp 12342 exited thread 0x7fffe4fa3700 lwp 12341 exited new thread 0x7fffe47a2700 lwp 12345 new thread 0x7fffe37a0700 lwp 12346 new thread 0x7fffe4fa3700 lwp 12347 new thread 0x7fffe3fa1700 lwp 12348 home hl anaconda2 lib python2.7 site packages urllib3 contrib pyopenssl.py 46 deprecationwarning openssl.rand deprecated use os.urandom instead import openssl.ssl 11 18 59 src engine engine.cc 55 mxnet start using engine naiveengine 0.00020103 0.00926307 0.00926307 0.00926307 0.00431658 0.0041606 0.01661832 0.01661832 0.01661832 0.02024863 0.0041606 0.01661832 0.01661832 0.01661832 0.02024863 0.0041606 0.01661832 0.01661832 0.01661832 0.02024863 0.00537269 0.01894068 0.01894068 0.01894068 0.02670578 thread 0x7fffe3fa1700 lwp 12348 exited thread 0x7fffe4fa3700 lwp 12347 exited thread 0x7fffe47a2700 lwp 12345 exited thread 0x7fffe57a4700 lwp 12340 exited thread 0x7ffff7fcb700 lwp 12334 exited inferior 1 process 12334 exited normally gdb b incubator mxnet src operator convolution v1 inl.h 130 breakpoint 1 0x7ffff0b8a90f incubator mxnet src operator convolution v1 inl.h 130. 3 locations gdb run starting program home hl anaconda2 bin python debug conv.py thread debugging using libthread db enabled using host libthread db library lib x86 64 linux gnu libthread db.so.1 . new thread 0x7fffe57a4700 lwp 12354 new thread 0x7fffe4fa3700 lwp 12355 new thread 0x7fffe47a2700 lwp 12356 new thread 0x7fffe3fa1700 lwp 12357 new thread 0x7fffe37a0700 lwp 12358 thread 0x7fffe37a0700 lwp 12358 exited thread 0x7fffe3fa1700 lwp 12357 exited thread 0x7fffe47a2700 lwp 12356 exited thread 0x7fffe4fa3700 lwp 12355 exited new thread 0x7fffe47a2700 lwp 12359 ........................",0,python howto debug conv work?,"python howto debug conv work? description debug conv stop breakpoint. environment info required os ubuntu 16.04 xenial build info required built source gcc mxnet 1.0.0 cpu build config debug 1 cuda 0 cudnn 0 cuda path none make j nproc error message gdb args python debug conv.py gnu gdb ubuntu 7.11.1 0ubuntu1 16.5 7.11.1 copyright c 2016 free software foundation, inc. license gplv3 gnu gpl version 3 later free software free change redistribute it. warranty, extent permitted law. type show copying show warranty details. gdb configured x86 64 linux gnu . type show configuration configuration details. bug reporting instructions, please see . find gdb manual documentation resources online . help, type help . type apropos word search commands related word reading symbols python...done. gdb b incubator mxnet src operator convolution v1 inl.h 130 source file named incubator mxnet src operator convolution v1 inl.h. make breakpoint pending future shared library load? n n gdb run starting program home hl anaconda2 bin python debug conv.py thread debugging using libthread db enabled using host libthread db library lib x86 64 linux gnu libthread db.so.1 . new thread 0x7fffe57a4700 lwp 12340 new thread 0x7fffe4fa3700 lwp 12341 new thread 0x7fffe47a2700 lwp 12342 new thread 0x7fffe3fa1700 lwp 12343 new thread 0x7fffe37a0700 lwp 12344 thread 0x7fffe37a0700 lwp 12344 exited thread 0x7fffe3fa1700 lwp 12343 exited thread 0x7fffe47a2700 lwp 12342 exited thread 0x7fffe4fa3700 lwp 12341 exited new thread 0x7fffe47a2700 lwp 12345 new thread 0x7fffe37a0700 lwp 12346 new thread 0x7fffe4fa3700 lwp 12347 new thread 0x7fffe3fa1700 lwp 12348 home hl anaconda2 lib python2.7 site packages urllib3 contrib pyopenssl.py 46 deprecationwarning openssl.rand deprecated use os.urandom instead import openssl.ssl 11 18 59 src engine engine.cc 55 mxnet start using engine naiveengine 0.00020103 0.00926307 0.00926307 0.00926307 0.00431658 0.0041606 0.01661832 0.01661832 0.01661832 0.02024863 0.0041606 0.01661832 0.01661832 0.01661832 0.02024863 0.0041606 0.01661832 0.01661832 0.01661832 0.02024863 0.00537269 0.01894068 0.01894068 0.01894068 0.02670578 thread 0x7fffe3fa1700 lwp 12348 exited thread 0x7fffe4fa3700 lwp 12347 exited thread 0x7fffe47a2700 lwp 12345 exited thread 0x7fffe57a4700 lwp 12340 exited thread 0x7ffff7fcb700 lwp 12334 exited inferior 1 process 12334 exited normally gdb b incubator mxnet src operator convolution v1 inl.h 130 breakpoint 1 0x7ffff0b8a90f incubator mxnet src operator convolution v1 inl.h 130. 3 locations gdb run starting program home hl anaconda2 bin python debug conv.py thread debugging using libthread db enabled using host libthread db library lib x86 64 linux gnu libthread db.so.1 . new thread 0x7fffe57a4700 lwp 12354 new thread 0x7fffe4fa3700 lwp 12355 new thread 0x7fffe47a2700 lwp 12356 new thread 0x7fffe3fa1700 lwp 12357 new thread 0x7fffe37a0700 lwp 12358 thread 0x7fffe37a0700 lwp 12358 exited thread 0x7fffe3fa1700 lwp 12357 exited thread 0x7fffe47a2700 lwp 12356 exited thread 0x7fffe4fa3700 lwp 12355 exited new thread 0x7fffe47a2700 lwp 12359 ........................"
incubator-mxnet,11703,"unit test title using fixed seed mask flakiness. suggested action 1. evaluate whether test flaky without fixed seed. not, remove seed. else move 2 2. test flaky, determine whether actual uncaught edge case. so, fix operator. else move 3 3. numerical instability inevitable, adjust tolerance level appropriately.",0,test loss.test triplet loss fixed seed mask flakiness,"test loss.test triplet loss fixed seed mask flakiness unit test title using fixed seed mask flakiness. suggested action 1. evaluate whether test flaky without fixed seed. not, remove seed. else move 2 2. test flaky, determine whether actual uncaught edge case. so, fix operator. else move 3 3. numerical instability inevitable, adjust tolerance level appropriately."
incubator-mxnet,6810,"using mxnet transfer learning python code, encounters error. checked data batch generated dataiter, seems ok. error traceback recent call last file users nali pycharmprojects mxnet ir test.py , line 104, test train file users nali pycharmprojects mxnet ir test.py , line 100, test train epoch end callback mx.callback.do checkpoint models ir blur file library python 2.7 site packages mxnet model.py , line 826, fit sym gen self.sym gen file library python 2.7 site packages mxnet model.py , line 237, train multi device executor manager.load data batch data batch file library python 2.7 site packages mxnet executor manager.py , line 412, load data batch self.curr execgrp.load data batch data batch file library python 2.7 site packages mxnet executor manager.py , line 259, load data batch load data data batch, self.data arrays file library python 2.7 site packages mxnet executor manager.py , line 95, load data load general batch.data, targets attributeerror 'nonetype' object attribute 'data' code optimizer mx.optimizer.sgd momentum 0.99 lr scheduler lr scheduler model mx.model.feedforward allow extra params true, ctx dev, symbol network, num epoch 200, learning rate 0.1 1e 2, wd 0.0001, initializer mx.init.load . resnet resnet 18 0000.params , default init mx.init.xavier rnd type gaussian , factor type , magnitude 2 , optimizer optimizer model.fit x train set, eval metric auc , kvstore 'local allreduce device', batch end callback mx.callback.speedometer batch size, 10 , epoch end callback mx.callback.do checkpoint models ir blur",0,'nonetype' object attribute 'data',"'nonetype' object attribute 'data' using mxnet transfer learning python code, encounters error. checked data batch generated dataiter, seems ok. error traceback recent call last file users nali pycharmprojects mxnet ir test.py , line 104, test train file users nali pycharmprojects mxnet ir test.py , line 100, test train epoch end callback mx.callback.do checkpoint models ir blur file library python 2.7 site packages mxnet model.py , line 826, fit sym gen self.sym gen file library python 2.7 site packages mxnet model.py , line 237, train multi device executor manager.load data batch data batch file library python 2.7 site packages mxnet executor manager.py , line 412, load data batch self.curr execgrp.load data batch data batch file library python 2.7 site packages mxnet executor manager.py , line 259, load data batch load data data batch, self.data arrays file library python 2.7 site packages mxnet executor manager.py , line 95, load data load general batch.data, targets attributeerror 'nonetype' object attribute 'data' code optimizer mx.optimizer.sgd momentum 0.99 lr scheduler lr scheduler model mx.model.feedforward allow extra params true, ctx dev, symbol network, num epoch 200, learning rate 0.1 1e 2, wd 0.0001, initializer mx.init.load . resnet resnet 18 0000.params , default init mx.init.xavier rnd type gaussian , factor type , magnitude 2 , optimizer optimizer model.fit x train set, eval metric auc , kvstore 'local allreduce device', batch end callback mx.callback.speedometer batch size, 10 , epoch end callback mx.callback.do checkpoint models ir blur"
incubator-mxnet,15658,"use control profiler's behaviors, i.e. profile. two parameters control whether profile operators called two modes respectively. however, seems currently functioning correctly. screenshot below, simple scrip symbolic mode imperative mode. set see events . ! screen shot 2019 07 25 10 55 47 https user images.githubusercontent.com 16669457 61897220 78376080 aecb 11e9 82dd 68cff9b83c6d.png however, set . event gone, still event, expected behavior. ! screen shot 2019 07 25 10 56 16 https user images.githubusercontent.com 16669457 61897214 753c7000 aecb 11e9 89a6 e876b0b6ccb3.png mxnet version 1.4 1.6 last night build tested far 1.4 python version 2 3",0,profile symbolic flag working profiler,"profile symbolic flag working profiler use control profiler's behaviors, i.e. profile. two parameters control whether profile operators called two modes respectively. however, seems currently functioning correctly. screenshot below, simple scrip symbolic mode imperative mode. set see events . ! screen shot 2019 07 25 10 55 47 https user images.githubusercontent.com 16669457 61897220 78376080 aecb 11e9 82dd 68cff9b83c6d.png however, set . event gone, still event, expected behavior. ! screen shot 2019 07 25 10 56 16 https user images.githubusercontent.com 16669457 61897214 753c7000 aecb 11e9 89a6 e876b0b6ccb3.png mxnet version 1.4 1.6 last night build tested far 1.4 python version 2 3"
incubator-mxnet,8016,"hi. ran mxnet installation gpu validation issue nvidia tx1 ubuntu 16.04 . built mxnet source according device jetson tx2 gpu directions https mxnet.incubator.apache.org get started install.html validate mxnet installation url cpu validation code works fine, gpu errors see attached log file import mxnet mx mx.nd.ones 2, 3 , mx.gpu b 2 1 b.asnumpy array 3., 3., 3. , 3., 3., 3. , dtype float32 bugs installation issues, please provide following information. information provide, likely people able help you. environment info operating system ubuntu 16.04 compiler g ubuntu linaro 5.4.0 6ubuntu1 16.04.4 5.4.0 20160609 package used python r scala julia mxnet version mxnet 0.11.1 installed source git clone https github.com dmlc mxnet.git recursive mxnet commit hash ebf1bf9d3a6cef335a5b2c2a37175e9e91f5b546 using python package, please provide pip 9.0.1 python version distribution python 2.7.12 default, nov 19 2016, 06 48 10 using r package, please provide r error message please paste full error message, including stack trace. 19 27 21 home nvidia mxnet dmlc core include dmlc . logging.h 308 19 27 21 home nvidia mxnet mshadow mshadow . . . . cuda tensor gpu inl.cuh 110 check failed err cudasuccess 8 vs. 0 name mapplankernel errstr invalid device function see attached log file minimum reproducible example using code, please provide short script reproduces error. import mxnet mx mx.nd.ones 2, 3 , mx.gpu b 2 1 b.asnumpy array 3., 3., 3. , 3., 3., 3. , dtype float32 steps reproduce running standard examples, please provide commands run lead error. 1. 2. 3. tried solve it? 1. reinstalled 2. 3. mxnet out.log https github.com apache incubator mxnet files 1327872 mxnet out.log",0,mxnet installation gpu validation issue error,"mxnet installation gpu validation issue error hi. ran mxnet installation gpu validation issue nvidia tx1 ubuntu 16.04 . built mxnet source according device jetson tx2 gpu directions https mxnet.incubator.apache.org get started install.html validate mxnet installation url cpu validation code works fine, gpu errors see attached log file import mxnet mx mx.nd.ones 2, 3 , mx.gpu b 2 1 b.asnumpy array 3., 3., 3. , 3., 3., 3. , dtype float32 bugs installation issues, please provide following information. information provide, likely people able help you. environment info operating system ubuntu 16.04 compiler g ubuntu linaro 5.4.0 6ubuntu1 16.04.4 5.4.0 20160609 package used python r scala julia mxnet version mxnet 0.11.1 installed source git clone https github.com dmlc mxnet.git recursive mxnet commit hash ebf1bf9d3a6cef335a5b2c2a37175e9e91f5b546 using python package, please provide pip 9.0.1 python version distribution python 2.7.12 default, nov 19 2016, 06 48 10 using r package, please provide r error message please paste full error message, including stack trace. 19 27 21 home nvidia mxnet dmlc core include dmlc . logging.h 308 19 27 21 home nvidia mxnet mshadow mshadow . . . . cuda tensor gpu inl.cuh 110 check failed err cudasuccess 8 vs. 0 name mapplankernel errstr invalid device function see attached log file minimum reproducible example using code, please provide short script reproduces error. import mxnet mx mx.nd.ones 2, 3 , mx.gpu b 2 1 b.asnumpy array 3., 3., 3. , 3., 3., 3. , dtype float32 steps reproduce running standard examples, please provide commands run lead error. 1. 2. 3. tried solve it? 1. reinstalled 2. 3. mxnet out.log https github.com apache incubator mxnet files 1327872 mxnet out.log"
incubator-mxnet,3395,"hi mxnet users recently downloaded latest mxnet. tried train ilsvrc2012 data mxnet, could train alexnet inception bn network. use inception bn network, .. .. tools launch.py h hosts n 2 launcher ssh python train imagenet.py network inception bn gpus 0,1,2,3 batch size 144 num epochs 1 lr 0.05 lr factor 0.94 data shape 256 data dir .. .. data ilsvrc12 kv store dist sync following error 10 45 00 cm shared scratch rengan dl mxnet dmlc core include dmlc logging.h 235 10 45 00 src operator . cudnn convolution inl.h 113 check failed cudnnconvolutionforward dnn handle , alpha, desc , data ptr data offset g, filter desc , wmat ptr weight offset g, conv desc , algo , workspace.dptr , forward workspace byte , beta, desc , ptr offset g cudnn status success 10 45 00 cm shared scratch rengan dl mxnet dmlc core include dmlc logging.h 235 10 45 00 src engine . threaded engine.h 306 10 45 00 src operator . cudnn convolution inl.h 113 check failed cudnnconvolutionforward dnn handle , alpha, desc , data ptr data offset g, filter desc , wmat ptr weight offset g, conv desc , algo , workspace.dptr , forward workspace byte , beta, desc , ptr offset g cudnn status success fatal error occurred asynchronous engine operation. know caused error, try set environment variable mxnet engine type naiveengine run debugger i.e. gdb . force operations synchronous backtrace give series calls lead error. remember set mxnet engine type back empty debugging. alexnet error inception bn. anyone know got errors? thanks. regards, rengan",0,checked failed cuudnnconvolutionforward,"checked failed cuudnnconvolutionforward hi mxnet users recently downloaded latest mxnet. tried train ilsvrc2012 data mxnet, could train alexnet inception bn network. use inception bn network, .. .. tools launch.py h hosts n 2 launcher ssh python train imagenet.py network inception bn gpus 0,1,2,3 batch size 144 num epochs 1 lr 0.05 lr factor 0.94 data shape 256 data dir .. .. data ilsvrc12 kv store dist sync following error 10 45 00 cm shared scratch rengan dl mxnet dmlc core include dmlc logging.h 235 10 45 00 src operator . cudnn convolution inl.h 113 check failed cudnnconvolutionforward dnn handle , alpha, desc , data ptr data offset g, filter desc , wmat ptr weight offset g, conv desc , algo , workspace.dptr , forward workspace byte , beta, desc , ptr offset g cudnn status success 10 45 00 cm shared scratch rengan dl mxnet dmlc core include dmlc logging.h 235 10 45 00 src engine . threaded engine.h 306 10 45 00 src operator . cudnn convolution inl.h 113 check failed cudnnconvolutionforward dnn handle , alpha, desc , data ptr data offset g, filter desc , wmat ptr weight offset g, conv desc , algo , workspace.dptr , forward workspace byte , beta, desc , ptr offset g cudnn status success fatal error occurred asynchronous engine operation. know caused error, try set environment variable mxnet engine type naiveengine run debugger i.e. gdb . force operations synchronous backtrace give series calls lead error. remember set mxnet engine type back empty debugging. alexnet error inception bn. anyone know got errors? thanks. regards, rengan"
incubator-mxnet,15376,"description trying fix https github.com mli new docs pull 123 found html code render fine markdown file, get converted building site. however, video html codes worked fine. there's warning error. skips lines code. seems like bug new site's build. refer commit see difference work, compared work. https github.com mli new docs pull 123 commits 58b88f83b91c31f2fa31a5810f9563ed581a498c",0,beta website render html tags images,"beta website render html tags images description trying fix https github.com mli new docs pull 123 found html code render fine markdown file, get converted building site. however, video html codes worked fine. there's warning error. skips lines code. seems like bug new site's build. refer commit see difference work, compared work. https github.com mli new docs pull 123 commits 58b88f83b91c31f2fa31a5810f9563ed581a498c"
incubator-mxnet,3647,"http mxnet.io api python symbol.html mxnet.symbol.sequencemask said sequence length false, example batch assumed max sequence length, operator becomes identity operator. first think sequence length parameter mentioned use sequence length. document means op works set use sequence length true, otherwise identity op. therefore need parameter? true default?",0,question sequencemask op,"question sequencemask op http mxnet.io api python symbol.html mxnet.symbol.sequencemask said sequence length false, example batch assumed max sequence length, operator becomes identity operator. first think sequence length parameter mentioned use sequence length. document means op works set use sequence length true, otherwise identity op. therefore need parameter? true default?"
incubator-mxnet,14712,"code report cudnn bug, also tried nvidia docker official also work too.",0,conv3dtranspose work ubuntu.,"conv3dtranspose work ubuntu. code report cudnn bug, also tried nvidia docker official also work too."
incubator-mxnet,3814,"try install mxnet scala package following installation guide. building shared library, execute command make scalapkg. got error failed execute goal net.alchim31.maven scala maven plugin 3.2.2 compile default project mxnet init 2.11 execution default goal net.alchim31.maven scala maven plugin 3.2.2 compile failed. compilefailed anybody knows solove it? thank much.",0,installation error scala package,"installation error scala package try install mxnet scala package following installation guide. building shared library, execute command make scalapkg. got error failed execute goal net.alchim31.maven scala maven plugin 3.2.2 compile default project mxnet init 2.11 execution default goal net.alchim31.maven scala maven plugin 3.2.2 compile failed. compilefailed anybody knows solove it? thank much."
incubator-mxnet,7415,"hi, tried converting caffe model. convert symbol ran successfully, convert model crashed. know model works fine caffe. know details model. environment info operating system ubuntu 16.04 package used python r scala julia python mxnet commit hash 89e3ee3ea7c223db8c65ddd8c94c6e787d7c52df using python package, please provide python version distribution python 2.7 error message please paste full error message, including stack trace. converting layer fc6, wmat shape 22440, 512 fc6 bias found arg shape dic. skipping layer softmaxloss1 type softmaxwithloss skipping layer center loss 1 type centerloss traceback recent call last file convert model.py , line 220, main file convert model.py , line 216, main convert model args.prototxt, args.caffemodel, args.save model name file convert model.py , line 198, convert model assert len layer blobs 0 assertionerror steps reproduce 1. run tools caffe converter python convert model.py proto caffemodel filename",0,caffe converter convert model.py fail,"caffe converter convert model.py fail hi, tried converting caffe model. convert symbol ran successfully, convert model crashed. know model works fine caffe. know details model. environment info operating system ubuntu 16.04 package used python r scala julia python mxnet commit hash 89e3ee3ea7c223db8c65ddd8c94c6e787d7c52df using python package, please provide python version distribution python 2.7 error message please paste full error message, including stack trace. converting layer fc6, wmat shape 22440, 512 fc6 bias found arg shape dic. skipping layer softmaxloss1 type softmaxwithloss skipping layer center loss 1 type centerloss traceback recent call last file convert model.py , line 220, main file convert model.py , line 216, main convert model args.prototxt, args.caffemodel, args.save model name file convert model.py , line 198, convert model assert len layer blobs 0 assertionerror steps reproduce 1. run tools caffe converter python convert model.py proto caffemodel filename"
incubator-mxnet,11700,"unit test title using fixed seed mask flakiness. suggested action 1. evaluate whether test flaky without fixed seed. not, remove seed. else move 2 2. test flaky, determine whether actual uncaught edge case. so, fix operator. else move 3 3. numerical instability inevitable, adjust tolerance level appropriately.",0,test loss.test sample weight loss fixed seed mask flakiness,"test loss.test sample weight loss fixed seed mask flakiness unit test title using fixed seed mask flakiness. suggested action 1. evaluate whether test flaky without fixed seed. not, remove seed. else move 2 2. test flaky, determine whether actual uncaught edge case. so, fix operator. else move 3 3. numerical instability inevitable, adjust tolerance level appropriately."
incubator-mxnet,12949,description sphinx throwing errors modules. error,0,text contrib module docs errors,text contrib module docs errors description sphinx throwing errors modules. error
incubator-mxnet,14437,"note providing complete information concise form best way get help. issue template serves checklist essential information technical issues bug reports. non technical issues feature requests, feel free present information believe best form. q discussion, please start discussion thread https discuss.mxnet.io description following git clone recursive https github.com apache incubator mxnet.git cd incubator mxnet make j4 error message ! image https user images.githubusercontent.com 48436808 54449650 98bb1580 4789 11e9 8eda 33ad937ee9ec.png someone tell solve issue.",0,compile error,"compile error note providing complete information concise form best way get help. issue template serves checklist essential information technical issues bug reports. non technical issues feature requests, feel free present information believe best form. q discussion, please start discussion thread https discuss.mxnet.io description following git clone recursive https github.com apache incubator mxnet.git cd incubator mxnet make j4 error message ! image https user images.githubusercontent.com 48436808 54449650 98bb1580 4789 11e9 8eda 33ad937ee9ec.png someone tell solve issue."
incubator-mxnet,12087,"causes error recent master. code worked days ago. guess 11370 brokes it. according api doc, types used dataset. 11370 temporal think would good idea avoid breaking existing code. zhreshold",0,python list gluon dataset,"python list gluon dataset causes error recent master. code worked days ago. guess 11370 brokes it. according api doc, types used dataset. 11370 temporal think would good idea avoid breaking existing code. zhreshold"
incubator-mxnet,16560,"description use large tensor, easy crash mxnet kernel. using following python code reproduce error looks like int32 overflow shape.size. easy way fix out? way found compile mxnet use int64 tensor size on, slower default one. environment info required mxnet 1.5.1 pip3 install package used python r scala julia python error message",0,easy crash mxnet tensor goes larger,"easy crash mxnet tensor goes larger description use large tensor, easy crash mxnet kernel. using following python code reproduce error looks like int32 overflow shape.size. easy way fix out? way found compile mxnet use int64 tensor size on, slower default one. environment info required mxnet 1.5.1 pip3 install package used python r scala julia python error message"
incubator-mxnet,15194,"assume, total idiot indeed, feel like one, least like bloody beginner... whole neural net installation gpu total nightmare. came failing get tensorflow run. seems like disaster mxnet. people, awesome tools free use hobbyist like me, everybody professional developer! keep learning course, hurdle high moment. whole installation routine windows 10 version 10.0.18362.145 64bit clean install r 3.6.0 uninstalled installed r 3.5.2 later r studio 1.2.1335 rtools35 nvidia display driver 26.21.14.3086 gtx 1060 3gb cuda 9.0.176 win10 update 9.0.176.1 9.0.176.2 9.0.176.3 cudnn 9.0 windows10 x64 v7.1.zip unzip cudnn64 7.dll c program files nvidia gpu computing toolkit cuda v9.0 bin anaconda3 2019.03 windows x86 64 downgrade anaconda3 5.2.0 windows x86 64 problems tensorflow upgrade back later anaconda3 2019.03 windows x86 64 r studio, called calling get error package namespace load failed mxnet .onload failed loadnamespace 'mxnet', details call indl x, as.logical local , as.logical , error unable load shared object 'd benutzer myname documents r win library 3.5 mxnet libs x64 libmxnet.dll' loadlibrary failure die angegebene prozedur wurde nicht gefunden. german specified procedure found older version dumb end user like get running copying pasting lines r code r terminal execute these?",0,get smoothest installation experience r gpu version ?,"get smoothest installation experience r gpu version ? assume, total idiot indeed, feel like one, least like bloody beginner... whole neural net installation gpu total nightmare. came failing get tensorflow run. seems like disaster mxnet. people, awesome tools free use hobbyist like me, everybody professional developer! keep learning course, hurdle high moment. whole installation routine windows 10 version 10.0.18362.145 64bit clean install r 3.6.0 uninstalled installed r 3.5.2 later r studio 1.2.1335 rtools35 nvidia display driver 26.21.14.3086 gtx 1060 3gb cuda 9.0.176 win10 update 9.0.176.1 9.0.176.2 9.0.176.3 cudnn 9.0 windows10 x64 v7.1.zip unzip cudnn64 7.dll c program files nvidia gpu computing toolkit cuda v9.0 bin anaconda3 2019.03 windows x86 64 downgrade anaconda3 5.2.0 windows x86 64 problems tensorflow upgrade back later anaconda3 2019.03 windows x86 64 r studio, called calling get error package namespace load failed mxnet .onload failed loadnamespace 'mxnet', details call indl x, as.logical local , as.logical , error unable load shared object 'd benutzer myname documents r win library 3.5 mxnet libs x64 libmxnet.dll' loadlibrary failure die angegebene prozedur wurde nicht gefunden. german specified procedure found older version dumb end user like get running copying pasting lines r code r terminal execute these?"
incubator-mxnet,1325,"trouble compiling mxnet package r gpu support enabled ubuntu machine aws instance. like many others before, get error run gpu algorithms r. however, know sure installed cuda correctly compiled mxnet settings config.mk file sits mxnet root compilation errors well recompiled r package something else missing here?",0,compiling mxnet gpu support r,"compiling mxnet gpu support r trouble compiling mxnet package r gpu support enabled ubuntu machine aws instance. like many others before, get error run gpu algorithms r. however, know sure installed cuda correctly compiled mxnet settings config.mk file sits mxnet root compilation errors well recompiled r package something else missing here?"
incubator-mxnet,1272,"way could get position max pooling operations. example, 1,2 , 3,4 4 , could get 0, 0 , 0, 1 , thanks",0,max pooling index,"max pooling index way could get position max pooling operations. example, 1,2 , 3,4 4 , could get 0, 0 , 0, 1 , thanks"
incubator-mxnet,10694,"description importing onnx model mxnet errors out. model fer emotion recognition https github.com onnx models tree master emotion ferplus may issue onnx model itself, onnx model zoo assume tested regularly. reproduce download onnx model listed working directory, add signature.json synset.txt use new mxnet onnx contrib api import onnx model mxnet expected result emotion detection sym params returned actual result errors environment info required file users hag anaconda envs mms onnx talk 3 6 lib python3.6 site packages mxnet contrib onnx import import model.py , line 53, import model sym, arg params, aux params graph.from onnx model proto.graph file users hag anaconda envs mms onnx talk 3 6 lib python3.6 site packages mxnet contrib onnx import import onnx.py , line 114, onnx mxnet sym self. convert operator node name, op name, onnx attr, inputs file users hag anaconda envs mms onnx talk 3 6 lib python3.6 site packages mxnet contrib onnx import import onnx.py , line 58, convert operator op name, new attrs, inputs convert map op name attrs, inputs, self file users hag anaconda envs mms onnx talk 3 6 lib python3.6 site packages mxnet contrib onnx import op translations.py , line 216, conv new attrs translation utils. fix channels 'convolution', new attrs, inputs, cls file users hag anaconda envs mms onnx talk 3 6 lib python3.6 site packages mxnet contrib onnx import translation utils.py , line 172, fix channels raise valueerror unable get channels units attr onnx graph. valueerror unable get channels units attr onnx graph.",0,importing onnx model model zoo mxnet errors,"importing onnx model model zoo mxnet errors description importing onnx model mxnet errors out. model fer emotion recognition https github.com onnx models tree master emotion ferplus may issue onnx model itself, onnx model zoo assume tested regularly. reproduce download onnx model listed working directory, add signature.json synset.txt use new mxnet onnx contrib api import onnx model mxnet expected result emotion detection sym params returned actual result errors environment info required file users hag anaconda envs mms onnx talk 3 6 lib python3.6 site packages mxnet contrib onnx import import model.py , line 53, import model sym, arg params, aux params graph.from onnx model proto.graph file users hag anaconda envs mms onnx talk 3 6 lib python3.6 site packages mxnet contrib onnx import import onnx.py , line 114, onnx mxnet sym self. convert operator node name, op name, onnx attr, inputs file users hag anaconda envs mms onnx talk 3 6 lib python3.6 site packages mxnet contrib onnx import import onnx.py , line 58, convert operator op name, new attrs, inputs convert map op name attrs, inputs, self file users hag anaconda envs mms onnx talk 3 6 lib python3.6 site packages mxnet contrib onnx import op translations.py , line 216, conv new attrs translation utils. fix channels 'convolution', new attrs, inputs, cls file users hag anaconda envs mms onnx talk 3 6 lib python3.6 site packages mxnet contrib onnx import translation utils.py , line 172, fix channels raise valueerror unable get channels units attr onnx graph. valueerror unable get channels units attr onnx graph."
incubator-mxnet,14022,"note providing complete information concise form best way get help. issue template serves checklist essential information technical issues bug reports. non technical issues feature requests, feel free present information believe best form. q discussion, please start discussion thread https discuss.mxnet.io description got segmentation fault 11 error ran following command, python imagenet inference.py symbol file . model imagenet1k inception bn quantized 5batches naive symbol.json param file . model imagenet1k inception bn quantized 0000.params rgb mean 123.68,116.779,103.939 num skipped batches 50 batch size 64 num inference batches 500 dataset . data val 256 q90.rec ctx cpu data nthreads 1 please note model quantized using imagenet gen qsym.py instead imagenet gen qsym mkldnn.py environment info required python diagnose.py package used python r scala julia using python scala user, please provide 1. java version 2. maven version 3. scala runtime applicable r user, please provide r build info required built source compiler gcc clang mingw visual studio mxnet commit hash paste output here. build config paste content config.mk, build command. error message paste complete error message, including stack trace. info logger batch size 64 inference info logger rgb mean 123.68,116.779,103.939 info logger rgb std 1,1,1 info logger label name softmax label info logger input data shape 3, 224, 224 info logger dataset inference . data val 256 q90.rec 14 01 27 src io iter image recordio 2.cc 170 imagerecordioparser2 . data val 256 q90.rec, use 1 threads decoding.. info logger loading symbol file work projects qat incubator mxnet master example quantization . model imagenet1k inception bn quantized 5batches naive symbol.json info logger loading params file work projects qat incubator mxnet master example quantization . model imagenet1k inception bn quantized 0000.params info logger skipping first 50 batches info logger running model . model imagenet1k inception bn quantized 5batches naive symbol.json inference 14 01 34 src executor attach op execs pass.cc 335 neither fcompute fcomputeex registered contrib quantized pooling 14 01 34 src executor attach op execs pass.cc 335 neither fcompute fcomputeex registered contrib quantized conv 14 01 34 src executor attach op execs pass.cc 335 neither fcompute fcomputeex registered contrib quantized conv 14 01 34 src executor attach op execs pass.cc 335 neither fcompute fcomputeex registered contrib quantized pooling 14 01 34 src executor attach op execs pass.cc 335 neither fcompute fcomputeex registered contrib quantized conv 14 01 34 src executor attach op execs pass.cc 335 neither fcompute fcomputeex registered contrib quantized conv 14 01 34 src executor attach op execs pass.cc 335 neither fcompute fcomputeex registered contrib quantized conv 14 01 34 src executor attach op execs pass.cc 335 neither fcompute fcomputeex registered contrib quantized conv 14 01 34 src executor attach op execs pass.cc 335 neither fcompute fcomputeex registered contrib quantized conv 14 01 34 src executor attach op execs pass.cc 335 neither fcompute fcomputeex registered contrib quantized conv 14 01 34 src executor attach op execs pass.cc 335 neither fcompute fcomputeex registered contrib quantized pooling 14 01 34 src executor attach op execs pass.cc 335 neither fcompute fcomputeex registered contrib quantized conv 14 01 34 src executor attach op execs pass.cc 335 neither fcompute fcomputeex registered contrib quantized conv 14 01 34 src executor attach op execs pass.cc 335 neither fcompute fcomputeex registered contrib quantized conv 14 01 34 src executor attach op execs pass.cc 335 neither fcompute fcomputeex registered contrib quantized conv 14 01 34 src executor attach op execs pass.cc 335 neither fcompute fcomputeex registered contrib quantized conv 14 01 34 src executor attach op execs pass.cc 335 neither fcompute fcomputeex registered contrib quantized conv 14 01 34 src executor attach op execs pass.cc 335 neither fcompute fcomputeex registered contrib quantized conv 14 01 34 src executor attach op execs pass.cc 335 neither fcompute fcomputeex registered contrib quantized pooling 14 01 34 src executor attach op execs pass.cc 335 neither fcompute fcomputeex registered contrib quantized conv 14 01 34 src executor attach op execs pass.cc 335 neither fcompute fcomputeex registered contrib quantized conv 14 01 34 src executor attach op execs pass.cc 335 neither fcompute fcomputeex registered contrib quantized conv 14 01 34 src executor attach op execs pass.cc 335 neither fcompute fcomputeex registered contrib quantized conv 14 01 34 src executor attach op execs pass.cc 335 neither fcompute fcomputeex registered contrib quantized conv 14 01 34 src executor attach op execs pass.cc 335 neither fcompute fcomputeex registered contrib quantized conv 14 01 34 src executor attach op execs pass.cc 335 neither fcompute fcomputeex registered contrib quantized pooling 14 01 34 src executor attach op execs pass.cc 335 neither fcompute fcomputeex registered contrib quantized conv 14 01 34 src executor attach op execs pass.cc 335 neither fcompute fcomputeex registered contrib quantized conv 14 01 34 src executor attach op execs pass.cc 335 neither fcompute fcomputeex registered contrib quantized conv 14 01 34 src executor attach op execs pass.cc 335 neither fcompute fcomputeex registered contrib quantized conv 14 01 34 src executor attach op execs pass.cc 335 neither fcompute fcomputeex registered contrib quantized conv 14 01 34 src executor attach op execs pass.cc 335 neither fcompute fcomputeex registered contrib quantized conv 14 01 34 src executor attach op execs pass.cc 335 neither fcompute fcomputeex registered contrib quantized pooling 14 01 34 src executor attach op execs pass.cc 335 neither fcompute fcomputeex registered contrib quantized conv 14 01 34 src executor attach op execs pass.cc 335 neither fcompute fcomputeex registered contrib quantized conv 14 01 34 src executor attach op execs pass.cc 335 neither fcompute fcomputeex registered contrib quantized conv 14 01 34 src executor attach op execs pass.cc 335 neither fcompute fcomputeex registered contrib quantized conv 14 01 34 src executor attach op execs pass.cc 335 neither fcompute fcomputeex registered contrib quantized conv 14 01 34 src executor attach op execs pass.cc 335 neither fcompute fcomputeex registered contrib quantized conv 14 01 34 src executor attach op execs pass.cc 335 neither fcompute fcomputeex registered contrib quantized conv 14 01 34 src executor attach op execs pass.cc 335 neither fcompute fcomputeex registered contrib quantized pooling 14 01 34 src executor attach op execs pass.cc 335 neither fcompute fcomputeex registered contrib quantized conv 14 01 34 src executor attach op execs pass.cc 335 neither fcompute fcomputeex registered contrib quantized conv 14 01 34 src executor attach op execs pass.cc 335 neither fcompute fcomputeex registered contrib quantized conv 14 01 34 src executor attach op execs pass.cc 335 neither fcompute fcomputeex registered contrib quantized conv 14 01 34 src executor attach op execs pass.cc 335 neither fcompute fcomputeex registered contrib quantized conv 14 01 34 src executor attach op execs pass.cc 335 neither fcompute fcomputeex registered contrib quantized conv 14 01 34 src executor attach op execs pass.cc 335 neither fcompute fcomputeex registered contrib quantized conv 14 01 34 src executor attach op execs pass.cc 335 neither fcompute fcomputeex registered contrib quantized pooling 14 01 34 src executor attach op execs pass.cc 335 neither fcompute fcomputeex registered contrib quantized conv 14 01 34 src executor attach op execs pass.cc 335 neither fcompute fcomputeex registered contrib quantized conv 14 01 34 src executor attach op execs pass.cc 335 neither fcompute fcomputeex registered contrib quantized conv 14 01 34 src executor attach op execs pass.cc 335 neither fcompute fcomputeex registered contrib quantized conv 14 01 34 src executor attach op execs pass.cc 335 neither fcompute fcomputeex registered contrib quantized conv 14 01 34 src executor attach op execs pass.cc 335 neither fcompute fcomputeex registered contrib quantized conv 14 01 34 src executor attach op execs pass.cc 335 neither fcompute fcomputeex registered contrib quantized conv 14 01 34 src executor attach op execs pass.cc 335 neither fcompute fcomputeex registered contrib quantized pooling 14 01 34 src executor attach op execs pass.cc 335 neither fcompute fcomputeex registered contrib quantized conv 14 01 34 src executor attach op execs pass.cc 335 neither fcompute fcomputeex registered contrib quantized conv 14 01 34 src executor attach op execs pass.cc 335 neither fcompute fcomputeex registered contrib quantized conv 14 01 34 src executor attach op execs pass.cc 335 neither fcompute fcomputeex registered contrib quantized conv 14 01 34 src executor attach op execs pass.cc 335 neither fcompute fcomputeex registered contrib quantized conv 14 01 34 src executor attach op execs pass.cc 335 neither fcompute fcomputeex registered contrib quantized conv 14 01 34 src executor attach op execs pass.cc 335 neither fcompute fcomputeex registered contrib quantized pooling 14 01 34 src executor attach op execs pass.cc 335 neither fcompute fcomputeex registered contrib quantized conv 14 01 34 src executor attach op execs pass.cc 335 neither fcompute fcomputeex registered contrib quantized conv 14 01 34 src executor attach op execs pass.cc 335 neither fcompute fcomputeex registered contrib quantized conv 14 01 34 src executor attach op execs pass.cc 335 neither fcompute fcomputeex registered contrib quantized conv 14 01 34 src executor attach op execs pass.cc 335 neither fcompute fcomputeex registered contrib quantized conv 14 01 34 src executor attach op execs pass.cc 335 neither fcompute fcomputeex registered contrib quantized conv 14 01 34 src executor attach op execs pass.cc 335 neither fcompute fcomputeex registered contrib quantized pooling 14 01 34 src executor attach op execs pass.cc 335 neither fcompute fcomputeex registered contrib quantized conv 14 01 34 src executor attach op execs pass.cc 335 neither fcompute fcomputeex registered contrib quantized conv 14 01 34 src executor attach op execs pass.cc 335 neither fcompute fcomputeex registered contrib quantized conv 14 01 34 src executor attach op execs pass.cc 335 neither fcompute fcomputeex registered contrib quantized conv 14 01 34 src executor attach op execs pass.cc 335 neither fcompute fcomputeex registered contrib quantized conv 14 01 34 src executor attach op execs pass.cc 335 neither fcompute fcomputeex registered contrib quantized conv 14 01 34 src executor attach op execs pass.cc 335 neither fcompute fcomputeex registered contrib quantized conv 14 01 34 src executor attach op execs pass.cc 335 neither fcompute fcomputeex registered contrib quantized pooling 14 01 34 src executor attach op execs pass.cc 335 neither fcompute fcomputeex registered contrib quantized conv 14 01 34 src executor attach op execs pass.cc 335 neither fcompute fcomputeex registered contrib quantized pooling 14 01 34 src executor attach op execs pass.cc 335 neither fcompute fcomputeex registered contrib quantized fully connected segmentation fault 11 stack trace returned 10 entries bt 0 work anaconda3 envs py3 lib python3.7 site packages mxnet libmxnet.so 0x381822 0x7fa65e7af822 bt 1 work anaconda3 envs py3 lib python3.7 site packages mxnet libmxnet.so 0x368d87e 0x7fa661abb87e bt 2 lib x86 64 linux gnu libc.so.6 0x3ef20 0x7fa6c57fbf20 bt 3 work anaconda3 envs py3 lib python3.7 site packages mxnet libmxnet.so 0x2f3bcbe 0x7fa661369cbe bt 4 work anaconda3 envs py3 lib python3.7 site packages mxnet libmxnet.so 0x2f3c661 0x7fa66136a661 bt 5 work anaconda3 envs py3 lib python3.7 site packages mxnet libmxnet.so 0x2f5c3d6 0x7fa66138a3d6 bt 6 work anaconda3 envs py3 lib python3.7 site packages mxnet libmxnet.so 0x2f63574 0x7fa661391574 bt 7 work anaconda3 envs py3 lib python3.7 site packages mxnet libmxnet.so 0x2f63c64 0x7fa661391c64 bt 8 work anaconda3 envs py3 lib python3.7 site packages mxnet libmxnet.so mxexecutorsimplebind 0x2260 0x7fa6612ed940 bt 9 work anaconda3 envs py3 lib python3.7 lib dynload .. .. libffi.so.6 ffi call unix64 0x4c 0x7fa6c3f63ec0 minimum reproducible example using code, please provide short script reproduces error. otherwise, please provide link existing example. https github.com apache incubator mxnet tree master example quantization steps reproduce paste commands ran produced error. 1. python imagenet gen qsym.py model imagenet1k inception bn num calib batches 5 calib mode naive note command worked fine 2. python imagenet inference.py symbol file . model imagenet1k inception bn quantized 5batches naive symbol.json param file . model imagenet1k inception bn quantized 0000.params rgb mean 123.68,116.779,103.939 num skipped batches 50 batch size 64 num inference batches 500 dataset . data val 256 q90.rec ctx cpu data nthreads 1 note command stopped returned error tried solve it? 1. 2.",0,segmentation fault inference quantization,"segmentation fault inference quantization note providing complete information concise form best way get help. issue template serves checklist essential information technical issues bug reports. non technical issues feature requests, feel free present information believe best form. q discussion, please start discussion thread https discuss.mxnet.io description got segmentation fault 11 error ran following command, python imagenet inference.py symbol file . model imagenet1k inception bn quantized 5batches naive symbol.json param file . model imagenet1k inception bn quantized 0000.params rgb mean 123.68,116.779,103.939 num skipped batches 50 batch size 64 num inference batches 500 dataset . data val 256 q90.rec ctx cpu data nthreads 1 please note model quantized using imagenet gen qsym.py instead imagenet gen qsym mkldnn.py environment info required python diagnose.py package used python r scala julia using python scala user, please provide 1. java version 2. maven version 3. scala runtime applicable r user, please provide r build info required built source compiler gcc clang mingw visual studio mxnet commit hash paste output here. build config paste content config.mk, build command. error message paste complete error message, including stack trace. info logger batch size 64 inference info logger rgb mean 123.68,116.779,103.939 info logger rgb std 1,1,1 info logger label name softmax label info logger input data shape 3, 224, 224 info logger dataset inference . data val 256 q90.rec 14 01 27 src io iter image recordio 2.cc 170 imagerecordioparser2 . data val 256 q90.rec, use 1 threads decoding.. info logger loading symbol file work projects qat incubator mxnet master example quantization . model imagenet1k inception bn quantized 5batches naive symbol.json info logger loading params file work projects qat incubator mxnet master example quantization . model imagenet1k inception bn quantized 0000.params info logger skipping first 50 batches info logger running model . model imagenet1k inception bn quantized 5batches naive symbol.json inference 14 01 34 src executor attach op execs pass.cc 335 neither fcompute fcomputeex registered contrib quantized pooling 14 01 34 src executor attach op execs pass.cc 335 neither fcompute fcomputeex registered contrib quantized conv 14 01 34 src executor attach op execs pass.cc 335 neither fcompute fcomputeex registered contrib quantized conv 14 01 34 src executor attach op execs pass.cc 335 neither fcompute fcomputeex registered contrib quantized pooling 14 01 34 src executor attach op execs pass.cc 335 neither fcompute fcomputeex registered contrib quantized conv 14 01 34 src executor attach op execs pass.cc 335 neither fcompute fcomputeex registered contrib quantized conv 14 01 34 src executor attach op execs pass.cc 335 neither fcompute fcomputeex registered contrib quantized conv 14 01 34 src executor attach op execs pass.cc 335 neither fcompute fcomputeex registered contrib quantized conv 14 01 34 src executor attach op execs pass.cc 335 neither fcompute fcomputeex registered contrib quantized conv 14 01 34 src executor attach op execs pass.cc 335 neither fcompute fcomputeex registered contrib quantized conv 14 01 34 src executor attach op execs pass.cc 335 neither fcompute fcomputeex registered contrib quantized pooling 14 01 34 src executor attach op execs pass.cc 335 neither fcompute fcomputeex registered contrib quantized conv 14 01 34 src executor attach op execs pass.cc 335 neither fcompute fcomputeex registered contrib quantized conv 14 01 34 src executor attach op execs pass.cc 335 neither fcompute fcomputeex registered contrib quantized conv 14 01 34 src executor attach op execs pass.cc 335 neither fcompute fcomputeex registered contrib quantized conv 14 01 34 src executor attach op execs pass.cc 335 neither fcompute fcomputeex registered contrib quantized conv 14 01 34 src executor attach op execs pass.cc 335 neither fcompute fcomputeex registered contrib quantized conv 14 01 34 src executor attach op execs pass.cc 335 neither fcompute fcomputeex registered contrib quantized conv 14 01 34 src executor attach op execs pass.cc 335 neither fcompute fcomputeex registered contrib quantized pooling 14 01 34 src executor attach op execs pass.cc 335 neither fcompute fcomputeex registered contrib quantized conv 14 01 34 src executor attach op execs pass.cc 335 neither fcompute fcomputeex registered contrib quantized conv 14 01 34 src executor attach op execs pass.cc 335 neither fcompute fcomputeex registered contrib quantized conv 14 01 34 src executor attach op execs pass.cc 335 neither fcompute fcomputeex registered contrib quantized conv 14 01 34 src executor attach op execs pass.cc 335 neither fcompute fcomputeex registered contrib quantized conv 14 01 34 src executor attach op execs pass.cc 335 neither fcompute fcomputeex registered contrib quantized conv 14 01 34 src executor attach op execs pass.cc 335 neither fcompute fcomputeex registered contrib quantized pooling 14 01 34 src executor attach op execs pass.cc 335 neither fcompute fcomputeex registered contrib quantized conv 14 01 34 src executor attach op execs pass.cc 335 neither fcompute fcomputeex registered contrib quantized conv 14 01 34 src executor attach op execs pass.cc 335 neither fcompute fcomputeex registered contrib quantized conv 14 01 34 src executor attach op execs pass.cc 335 neither fcompute fcomputeex registered contrib quantized conv 14 01 34 src executor attach op execs pass.cc 335 neither fcompute fcomputeex registered contrib quantized conv 14 01 34 src executor attach op execs pass.cc 335 neither fcompute fcomputeex registered contrib quantized conv 14 01 34 src executor attach op execs pass.cc 335 neither fcompute fcomputeex registered contrib quantized pooling 14 01 34 src executor attach op execs pass.cc 335 neither fcompute fcomputeex registered contrib quantized conv 14 01 34 src executor attach op execs pass.cc 335 neither fcompute fcomputeex registered contrib quantized conv 14 01 34 src executor attach op execs pass.cc 335 neither fcompute fcomputeex registered contrib quantized conv 14 01 34 src executor attach op execs pass.cc 335 neither fcompute fcomputeex registered contrib quantized conv 14 01 34 src executor attach op execs pass.cc 335 neither fcompute fcomputeex registered contrib quantized conv 14 01 34 src executor attach op execs pass.cc 335 neither fcompute fcomputeex registered contrib quantized conv 14 01 34 src executor attach op execs pass.cc 335 neither fcompute fcomputeex registered contrib quantized conv 14 01 34 src executor attach op execs pass.cc 335 neither fcompute fcomputeex registered contrib quantized pooling 14 01 34 src executor attach op execs pass.cc 335 neither fcompute fcomputeex registered contrib quantized conv 14 01 34 src executor attach op execs pass.cc 335 neither fcompute fcomputeex registered contrib quantized conv 14 01 34 src executor attach op execs pass.cc 335 neither fcompute fcomputeex registered contrib quantized conv 14 01 34 src executor attach op execs pass.cc 335 neither fcompute fcomputeex registered contrib quantized conv 14 01 34 src executor attach op execs pass.cc 335 neither fcompute fcomputeex registered contrib quantized conv 14 01 34 src executor attach op execs pass.cc 335 neither fcompute fcomputeex registered contrib quantized conv 14 01 34 src executor attach op execs pass.cc 335 neither fcompute fcomputeex registered contrib quantized conv 14 01 34 src executor attach op execs pass.cc 335 neither fcompute fcomputeex registered contrib quantized pooling 14 01 34 src executor attach op execs pass.cc 335 neither fcompute fcomputeex registered contrib quantized conv 14 01 34 src executor attach op execs pass.cc 335 neither fcompute fcomputeex registered contrib quantized conv 14 01 34 src executor attach op execs pass.cc 335 neither fcompute fcomputeex registered contrib quantized conv 14 01 34 src executor attach op execs pass.cc 335 neither fcompute fcomputeex registered contrib quantized conv 14 01 34 src executor attach op execs pass.cc 335 neither fcompute fcomputeex registered contrib quantized conv 14 01 34 src executor attach op execs pass.cc 335 neither fcompute fcomputeex registered contrib quantized conv 14 01 34 src executor attach op execs pass.cc 335 neither fcompute fcomputeex registered contrib quantized conv 14 01 34 src executor attach op execs pass.cc 335 neither fcompute fcomputeex registered contrib quantized pooling 14 01 34 src executor attach op execs pass.cc 335 neither fcompute fcomputeex registered contrib quantized conv 14 01 34 src executor attach op execs pass.cc 335 neither fcompute fcomputeex registered contrib quantized conv 14 01 34 src executor attach op execs pass.cc 335 neither fcompute fcomputeex registered contrib quantized conv 14 01 34 src executor attach op execs pass.cc 335 neither fcompute fcomputeex registered contrib quantized conv 14 01 34 src executor attach op execs pass.cc 335 neither fcompute fcomputeex registered contrib quantized conv 14 01 34 src executor attach op execs pass.cc 335 neither fcompute fcomputeex registered contrib quantized conv 14 01 34 src executor attach op execs pass.cc 335 neither fcompute fcomputeex registered contrib quantized pooling 14 01 34 src executor attach op execs pass.cc 335 neither fcompute fcomputeex registered contrib quantized conv 14 01 34 src executor attach op execs pass.cc 335 neither fcompute fcomputeex registered contrib quantized conv 14 01 34 src executor attach op execs pass.cc 335 neither fcompute fcomputeex registered contrib quantized conv 14 01 34 src executor attach op execs pass.cc 335 neither fcompute fcomputeex registered contrib quantized conv 14 01 34 src executor attach op execs pass.cc 335 neither fcompute fcomputeex registered contrib quantized conv 14 01 34 src executor attach op execs pass.cc 335 neither fcompute fcomputeex registered contrib quantized conv 14 01 34 src executor attach op execs pass.cc 335 neither fcompute fcomputeex registered contrib quantized pooling 14 01 34 src executor attach op execs pass.cc 335 neither fcompute fcomputeex registered contrib quantized conv 14 01 34 src executor attach op execs pass.cc 335 neither fcompute fcomputeex registered contrib quantized conv 14 01 34 src executor attach op execs pass.cc 335 neither fcompute fcomputeex registered contrib quantized conv 14 01 34 src executor attach op execs pass.cc 335 neither fcompute fcomputeex registered contrib quantized conv 14 01 34 src executor attach op execs pass.cc 335 neither fcompute fcomputeex registered contrib quantized conv 14 01 34 src executor attach op execs pass.cc 335 neither fcompute fcomputeex registered contrib quantized conv 14 01 34 src executor attach op execs pass.cc 335 neither fcompute fcomputeex registered contrib quantized conv 14 01 34 src executor attach op execs pass.cc 335 neither fcompute fcomputeex registered contrib quantized pooling 14 01 34 src executor attach op execs pass.cc 335 neither fcompute fcomputeex registered contrib quantized conv 14 01 34 src executor attach op execs pass.cc 335 neither fcompute fcomputeex registered contrib quantized pooling 14 01 34 src executor attach op execs pass.cc 335 neither fcompute fcomputeex registered contrib quantized fully connected segmentation fault 11 stack trace returned 10 entries bt 0 work anaconda3 envs py3 lib python3.7 site packages mxnet libmxnet.so 0x381822 0x7fa65e7af822 bt 1 work anaconda3 envs py3 lib python3.7 site packages mxnet libmxnet.so 0x368d87e 0x7fa661abb87e bt 2 lib x86 64 linux gnu libc.so.6 0x3ef20 0x7fa6c57fbf20 bt 3 work anaconda3 envs py3 lib python3.7 site packages mxnet libmxnet.so 0x2f3bcbe 0x7fa661369cbe bt 4 work anaconda3 envs py3 lib python3.7 site packages mxnet libmxnet.so 0x2f3c661 0x7fa66136a661 bt 5 work anaconda3 envs py3 lib python3.7 site packages mxnet libmxnet.so 0x2f5c3d6 0x7fa66138a3d6 bt 6 work anaconda3 envs py3 lib python3.7 site packages mxnet libmxnet.so 0x2f63574 0x7fa661391574 bt 7 work anaconda3 envs py3 lib python3.7 site packages mxnet libmxnet.so 0x2f63c64 0x7fa661391c64 bt 8 work anaconda3 envs py3 lib python3.7 site packages mxnet libmxnet.so mxexecutorsimplebind 0x2260 0x7fa6612ed940 bt 9 work anaconda3 envs py3 lib python3.7 lib dynload .. .. libffi.so.6 ffi call unix64 0x4c 0x7fa6c3f63ec0 minimum reproducible example using code, please provide short script reproduces error. otherwise, please provide link existing example. https github.com apache incubator mxnet tree master example quantization steps reproduce paste commands ran produced error. 1. python imagenet gen qsym.py model imagenet1k inception bn num calib batches 5 calib mode naive note command worked fine 2. python imagenet inference.py symbol file . model imagenet1k inception bn quantized 5batches naive symbol.json param file . model imagenet1k inception bn quantized 0000.params rgb mean 123.68,116.779,103.939 num skipped batches 50 batch size 64 num inference batches 500 dataset . data val 256 q90.rec ctx cpu data nthreads 1 note command stopped returned error tried solve it? 1. 2."
incubator-mxnet,4697,"environment info operating system debian testing compiler package used python r scala julia scala 2.11, spark 2.0.1 mxnet version installed source master 2017 01 16 mxnet commit hash error message 2017 01 17 16 20 38,890 error executor.executor logging.scala logerror 91 exception task 0.0 stage 1.0 tid 1 java.io.notserializableexception ml.dmlc.mxnet.symbol java.io.objectoutputstream.writeobject0 objectoutputstream.java 1184 java.io.objectoutputstream.defaultwritefields objectoutputstream.java 1548 java.io.objectoutputstream.writeserialdata objectoutputstream.java 1509 java.io.objectoutputstream.writeordinaryobject objectoutputstream.java 1432 java.io.objectoutputstream.writeobject0 objectoutputstream.java 1178 java.io.objectoutputstream.writeobject objectoutputstream.java 348 ml.dmlc.mxnet.javaserializer.serialize serializer.scala 48 ml.dmlc.mxnet.kvstore.setoptimizer kvstore.scala 187 ml.dmlc.mxnet.model anonfun trainmultidevice 4.apply model.scala 259 ml.dmlc.mxnet.model anonfun trainmultidevice 4.apply model.scala 259 scala.option.foreach option.scala 257 ml.dmlc.mxnet.model .trainmultidevice model.scala 259 ml.dmlc.mxnet.feedforward.fit feedforward.scala 347 ml.dmlc.mxnet.feedforward.fit feedforward.scala 286 ml.dmlc.mxnet.feedforward.fit feedforward.scala 294 ml.dmlc.mxnet.feedforward.fit feedforward.scala 300 ml.dmlc.mxnet.spark.mxnet anonfun 1.apply mxnet.scala 168 ml.dmlc.mxnet.spark.mxnet anonfun 1.apply mxnet.scala 127 org.apache.spark.rdd.rdd anonfun mappartitions 1 anonfun apply 23.apply rdd.scala 785 org.apache.spark.rdd.rdd anonfun mappartitions 1 anonfun apply 23.apply rdd.scala 785 org.apache.spark.rdd.mappartitionsrdd.compute mappartitionsrdd.scala 38 org.apache.spark.rdd.rdd.computeorreadcheckpoint rdd.scala 319 org.apache.spark.rdd.rdd anonfun 8.apply rdd.scala 332 org.apache.spark.rdd.rdd anonfun 8.apply rdd.scala 330 org.apache.spark.storage.blockmanager anonfun doputiterator 1.apply blockmanager.scala 935 org.apache.spark.storage.blockmanager anonfun doputiterator 1.apply blockmanager.scala 926 org.apache.spark.storage.blockmanager.doput blockmanager.scala 866 org.apache.spark.storage.blockmanager.doputiterator blockmanager.scala 926 org.apache.spark.storage.blockmanager.getorelseupdate blockmanager.scala 670 org.apache.spark.rdd.rdd.getorcompute rdd.scala 330 org.apache.spark.rdd.rdd.iterator rdd.scala 281 org.apache.spark.scheduler.resulttask.runtask resulttask.scala 70 org.apache.spark.scheduler.task.run task.scala 86 org.apache.spark.executor.executor taskrunner.run executor.scala 274 java.util.concurrent.threadpoolexecutor.runworker threadpoolexecutor.java 1142 java.util.concurrent.threadpoolexecutor worker.run threadpoolexecutor.java 617 java.lang.thread.run thread.java 745 minimum reproducible example steps reproduce run example tried solve it? seems commit scala bucketing api support https github.com dmlc mxnet commit 4d9ac5b77d3a5ad7b34d69f972386b9ae36d68c6 adds causes problem. 1. since symbol used right now, remove temporarily 2. convert symbol json",0,scala symbol serializable,"scala symbol serializable environment info operating system debian testing compiler package used python r scala julia scala 2.11, spark 2.0.1 mxnet version installed source master 2017 01 16 mxnet commit hash error message 2017 01 17 16 20 38,890 error executor.executor logging.scala logerror 91 exception task 0.0 stage 1.0 tid 1 java.io.notserializableexception ml.dmlc.mxnet.symbol java.io.objectoutputstream.writeobject0 objectoutputstream.java 1184 java.io.objectoutputstream.defaultwritefields objectoutputstream.java 1548 java.io.objectoutputstream.writeserialdata objectoutputstream.java 1509 java.io.objectoutputstream.writeordinaryobject objectoutputstream.java 1432 java.io.objectoutputstream.writeobject0 objectoutputstream.java 1178 java.io.objectoutputstream.writeobject objectoutputstream.java 348 ml.dmlc.mxnet.javaserializer.serialize serializer.scala 48 ml.dmlc.mxnet.kvstore.setoptimizer kvstore.scala 187 ml.dmlc.mxnet.model anonfun trainmultidevice 4.apply model.scala 259 ml.dmlc.mxnet.model anonfun trainmultidevice 4.apply model.scala 259 scala.option.foreach option.scala 257 ml.dmlc.mxnet.model .trainmultidevice model.scala 259 ml.dmlc.mxnet.feedforward.fit feedforward.scala 347 ml.dmlc.mxnet.feedforward.fit feedforward.scala 286 ml.dmlc.mxnet.feedforward.fit feedforward.scala 294 ml.dmlc.mxnet.feedforward.fit feedforward.scala 300 ml.dmlc.mxnet.spark.mxnet anonfun 1.apply mxnet.scala 168 ml.dmlc.mxnet.spark.mxnet anonfun 1.apply mxnet.scala 127 org.apache.spark.rdd.rdd anonfun mappartitions 1 anonfun apply 23.apply rdd.scala 785 org.apache.spark.rdd.rdd anonfun mappartitions 1 anonfun apply 23.apply rdd.scala 785 org.apache.spark.rdd.mappartitionsrdd.compute mappartitionsrdd.scala 38 org.apache.spark.rdd.rdd.computeorreadcheckpoint rdd.scala 319 org.apache.spark.rdd.rdd anonfun 8.apply rdd.scala 332 org.apache.spark.rdd.rdd anonfun 8.apply rdd.scala 330 org.apache.spark.storage.blockmanager anonfun doputiterator 1.apply blockmanager.scala 935 org.apache.spark.storage.blockmanager anonfun doputiterator 1.apply blockmanager.scala 926 org.apache.spark.storage.blockmanager.doput blockmanager.scala 866 org.apache.spark.storage.blockmanager.doputiterator blockmanager.scala 926 org.apache.spark.storage.blockmanager.getorelseupdate blockmanager.scala 670 org.apache.spark.rdd.rdd.getorcompute rdd.scala 330 org.apache.spark.rdd.rdd.iterator rdd.scala 281 org.apache.spark.scheduler.resulttask.runtask resulttask.scala 70 org.apache.spark.scheduler.task.run task.scala 86 org.apache.spark.executor.executor taskrunner.run executor.scala 274 java.util.concurrent.threadpoolexecutor.runworker threadpoolexecutor.java 1142 java.util.concurrent.threadpoolexecutor worker.run threadpoolexecutor.java 617 java.lang.thread.run thread.java 745 minimum reproducible example steps reproduce run example tried solve it? seems commit scala bucketing api support https github.com dmlc mxnet commit 4d9ac5b77d3a5ad7b34d69f972386b9ae36d68c6 adds causes problem. 1. since symbol used right now, remove temporarily 2. convert symbol json"
incubator-mxnet,9279,set export mxnet backward mirror 1 use gluon environment work gluon? thanks lot,0,gluon use sublinear memory,gluon use sublinear memory set export mxnet backward mirror 1 use gluon environment work gluon? thanks lot
incubator-mxnet,887,"dear all, pulled code today fail build successfully built . error get pkg config cflags opencvpkg config cflags opencvpkg config cflags opencv",0,latest pull today dec 9 fails build mac,"latest pull today dec 9 fails build mac dear all, pulled code today fail build successfully built . error get pkg config cflags opencvpkg config cflags opencvpkg config cflags opencv"
incubator-mxnet,7386,"need define function .h file gpu version only, implenent .cu file, declaration .h implement .cu file problem force declare function cpu version implement it, compile failed need function gpu, could get ride problem?",0,could define function gpu only?,"could define function gpu only? need define function .h file gpu version only, implenent .cu file, declaration .h implement .cu file problem force declare function cpu version implement it, compile failed need function gpu, could get ride problem?"
incubator-mxnet,3146,want construct network need spatial pyramid pooling spp spatial pyramid pooling deep convolutional network visual recognition operation. mxnet ready made spp operation?,0,mxnet ready made spp operation?,mxnet ready made spp operation? want construct network need spatial pyramid pooling spp spatial pyramid pooling deep convolutional network visual recognition operation. mxnet ready made spp operation?
incubator-mxnet,5093,"bugs installation issues, please provide following information. information provide, likely people able help you. environment info operating system windows 10 64 bit compiler vs2015 c package used python r scala julia mxnet version latest nightly windows build 20170221, cuda 8.0.61, cudnn 8.0v5.1 installed source mxnet commit hash using python package, please provide python version distribution using r package, please provide r error message please paste full error message, including stack trace. mlp.obj error lnk2001 unresolved external symbol imp mxoptimizerfree mlp.obj error lnk2001 unresolved external symbol imp mxoptimizerfindcreator mlp.obj error lnk2001 unresolved external symbol imp mxoptimizerupdate error lnk2001 unresolved external symbol imp mxoptimizercreateoptimizer minimum reproducible example using code, please provide short script reproduces error. steps reproduce running standard examples, please provide commands run lead error. 1. compiled mlp.cpp sample c vs2015 solution. worked earlier install mxnet based version mxnet.cpp cudnnv3.1, cuda 7.5 vs2013. entire mxnet version separate folder structure . 2. 3. tried solve it? 1. tried find symbols cannot linked defined. can't find source code github. 2. googled solution 3.",0,linker error building mlp.cpp vs2015,"linker error building mlp.cpp vs2015 bugs installation issues, please provide following information. information provide, likely people able help you. environment info operating system windows 10 64 bit compiler vs2015 c package used python r scala julia mxnet version latest nightly windows build 20170221, cuda 8.0.61, cudnn 8.0v5.1 installed source mxnet commit hash using python package, please provide python version distribution using r package, please provide r error message please paste full error message, including stack trace. mlp.obj error lnk2001 unresolved external symbol imp mxoptimizerfree mlp.obj error lnk2001 unresolved external symbol imp mxoptimizerfindcreator mlp.obj error lnk2001 unresolved external symbol imp mxoptimizerupdate error lnk2001 unresolved external symbol imp mxoptimizercreateoptimizer minimum reproducible example using code, please provide short script reproduces error. steps reproduce running standard examples, please provide commands run lead error. 1. compiled mlp.cpp sample c vs2015 solution. worked earlier install mxnet based version mxnet.cpp cudnnv3.1, cuda 7.5 vs2013. entire mxnet version separate folder structure . 2. 3. tried solve it? 1. tried find symbols cannot linked defined. can't find source code github. 2. googled solution 3."
incubator-mxnet,9357,figured split layer 8 gpus using group2ctx great feature. wonder group2ctx support split layer multi machines eg. 16 gpus two machine easy modifications? thanks. tqchen piiswrong mli,0,group2ctx used multi machine model parallel situation?,group2ctx used multi machine model parallel situation? figured split layer 8 gpus using group2ctx great feature. wonder group2ctx support split layer multi machines eg. 16 gpus two machine easy modifications? thanks. tqchen piiswrong mli
incubator-mxnet,5037,"hi, attempting load multiple ark files training, noticed numpy.dataiter takes single numpy feature matrix single numpy label vector, possible load multiple data sets?? figure attempted concantataneted ark files created single numpy matrix vector features matrix ended error even though shapes include mxnet . tensor blob.h 742 check failed shape .size shape.size tblob.get shape new old shape match total elements traceback recent call last file am.py , line 52, train mx.io.ndarrayiter featuremat, label targetmat, batch size 100 file usr local lib python2.7 site packages mxnet 0.7.0 py2.7.egg mxnet io.py , line 420, init self.data init data data, allow empty false, default name 'data' file usr local lib python2.7 site packages mxnet 0.7.0 py2.7.egg mxnet io.py , line 391, init data ndarray numpy.ndarray typeerror invalid type '' data, ndarray numpy.ndarray shapes though 39673722, 340 featuremat 39673722, targetvec",0,loading multiple files training,"loading multiple files training hi, attempting load multiple ark files training, noticed numpy.dataiter takes single numpy feature matrix single numpy label vector, possible load multiple data sets?? figure attempted concantataneted ark files created single numpy matrix vector features matrix ended error even though shapes include mxnet . tensor blob.h 742 check failed shape .size shape.size tblob.get shape new old shape match total elements traceback recent call last file am.py , line 52, train mx.io.ndarrayiter featuremat, label targetmat, batch size 100 file usr local lib python2.7 site packages mxnet 0.7.0 py2.7.egg mxnet io.py , line 420, init self.data init data data, allow empty false, default name 'data' file usr local lib python2.7 site packages mxnet 0.7.0 py2.7.egg mxnet io.py , line 391, init data ndarray numpy.ndarray typeerror invalid type '' data, ndarray numpy.ndarray shapes though 39673722, 340 featuremat 39673722, targetvec"
incubator-mxnet,7560,"mydata2.zip https github.com apache incubator mxnet files 1242794 mydata2.zip hi, model layer would like train, use weights bias parameter create new layer dataneurona read.csv datos proyecto mydata2.csv ,sep , , header dataneurona as.matrix dataneurona duplico la ultima fila para luego hacer el test con ella dataneurona rbind dataneurona,dataneurona length dataneurona ,1 , numtest 2 train.ind c 41 length dataneurona ,1 numtest train.x dataneurona train.ind, length dataneurona 1, train.y dataneurona train.ind,length dataneurona 1, test.x dataneurona c 1 41,train.ind , length dataneurona 1, test.y dataneurona c 1 41,train.ind ,length dataneurona 1, data mx.symbol.variable data w mx.symbol.variable 'myweight' fc1 mx.symbol.fullyconnected data, name fc1 , num hidden 50, weights ww act1 mx.symbol.activation fc1,name sigmoid1 ,act type sigmoid fc2 mx.symbol.fullyconnected act1,name fc2 ,num hidden 1 lro mx.symbol.linearregressionoutput data fc2, grad.scale 1 mx.set.seed 0 train iter mx.io.arrayiter data train.x , label train.y model mx.model.feedforward.create symbol lro, x train iter, ctx mx.cpu 2 , num.round 50, optimizer initialize mx.init.uniform 0.7 , array.batch.size 3, learning.rate 0.01, momentum 0.9, eval.metric mx.metric.mse problem find way create layer later weights parameter, tried 1 model arg.params fc1 weight fc1 mx.symbol.fullyconnected data, name fc1 , num hidden 25, weights model arg.params fc1 weight 2 model arg.params fc1 weight fc1 mx.symbol.fullyconnected data, name fc1 , num hidden 25, weights model arg.params fc1 weight, bias model arg.params fc1 bias 3 model arg.params fc1 weight fc1 mx.symbol.fullyconnected data, name fc1 , num hidden 25, weights model arg.params fc1 weight , bias model arg.params fc1 bias 4 model.extract model, weights quite understand wrong .... greeting",0,creating layer layer network,"creating layer layer network mydata2.zip https github.com apache incubator mxnet files 1242794 mydata2.zip hi, model layer would like train, use weights bias parameter create new layer dataneurona read.csv datos proyecto mydata2.csv ,sep , , header dataneurona as.matrix dataneurona duplico la ultima fila para luego hacer el test con ella dataneurona rbind dataneurona,dataneurona length dataneurona ,1 , numtest 2 train.ind c 41 length dataneurona ,1 numtest train.x dataneurona train.ind, length dataneurona 1, train.y dataneurona train.ind,length dataneurona 1, test.x dataneurona c 1 41,train.ind , length dataneurona 1, test.y dataneurona c 1 41,train.ind ,length dataneurona 1, data mx.symbol.variable data w mx.symbol.variable 'myweight' fc1 mx.symbol.fullyconnected data, name fc1 , num hidden 50, weights ww act1 mx.symbol.activation fc1,name sigmoid1 ,act type sigmoid fc2 mx.symbol.fullyconnected act1,name fc2 ,num hidden 1 lro mx.symbol.linearregressionoutput data fc2, grad.scale 1 mx.set.seed 0 train iter mx.io.arrayiter data train.x , label train.y model mx.model.feedforward.create symbol lro, x train iter, ctx mx.cpu 2 , num.round 50, optimizer initialize mx.init.uniform 0.7 , array.batch.size 3, learning.rate 0.01, momentum 0.9, eval.metric mx.metric.mse problem find way create layer later weights parameter, tried 1 model arg.params fc1 weight fc1 mx.symbol.fullyconnected data, name fc1 , num hidden 25, weights model arg.params fc1 weight 2 model arg.params fc1 weight fc1 mx.symbol.fullyconnected data, name fc1 , num hidden 25, weights model arg.params fc1 weight, bias model arg.params fc1 bias 3 model arg.params fc1 weight fc1 mx.symbol.fullyconnected data, name fc1 , num hidden 25, weights model arg.params fc1 weight , bias model arg.params fc1 bias 4 model.extract model, weights quite understand wrong .... greeting"
incubator-mxnet,890,"fatal error occurred asynchronous engine operation. know caused error, try set environment variable mxnet engine typeto naiveengine run debugger i.e. gdb . force operations synchronous backtrace give series calls lead error. remember set mxnet engine type back empty debugging. terminate called throwing instance 'dmlc error' 15 04 06 src engine . threaded engine.h 295 15 04 06 src operator . convolution inl.h 258 check failed param .workspace required size minimum workspace size 666989568 bytes given 536870912 bytes",0,error check failed param .workspace required size,"error check failed param .workspace required size fatal error occurred asynchronous engine operation. know caused error, try set environment variable mxnet engine typeto naiveengine run debugger i.e. gdb . force operations synchronous backtrace give series calls lead error. remember set mxnet engine type back empty debugging. terminate called throwing instance 'dmlc error' 15 04 06 src engine . threaded engine.h 295 15 04 06 src operator . convolution inl.h 258 check failed param .workspace required size minimum workspace size 666989568 bytes given 536870912 bytes"
incubator-mxnet,4223,want print values specified locations tensor forward backward op function call implement function,0,print values tensor,print values tensor want print values specified locations tensor forward backward op function call implement function
incubator-mxnet,8821,"use mxnet 0.12 python2.7 ,win10. example https github.com apache incubator mxnet blob b5648a43955f7d05c0e53c1ab61a58bd402b4416 example multi task example multi task.py want test use multi label accuracy example, got error importerror module named get data googled, found module relevant one related mnist example, find anymore. could someone fix it, provided module?",0,find module called get data mxnet 0.12,"find module called get data mxnet 0.12 use mxnet 0.12 python2.7 ,win10. example https github.com apache incubator mxnet blob b5648a43955f7d05c0e53c1ab61a58bd402b4416 example multi task example multi task.py want test use multi label accuracy example, got error importerror module named get data googled, found module relevant one related mnist example, find anymore. could someone fix it, provided module?"
incubator-mxnet,9993,"hi, sorry,there problems cmake command use git clone recursive https github.com apache incubator mxnet.git cd incubator mxnet mkdir build release cd build release cmake .. .. make j8 cmake .. .. output log c compiler identification gnu 5.4.0 cxx compiler identification gnu 5.4.0 check working c compiler usr bin cc check working c compiler usr bin cc works detecting c compiler abi info detecting c compiler abi info done detecting c compile features detecting c compile features done check working cxx compiler usr bin c check working cxx compiler usr bin c works detecting cxx compiler abi info detecting cxx compiler abi info done detecting cxx compile features detecting cxx compile features done cmake version '3.5.1' using generator 'unix makefiles' performing test support cxx11 performing test support cxx11 success performing test support cxx0x performing test support cxx0x success performing test support msse2 performing test support msse2 success cmake build type unset, defaulting release detecting intel r mkl trying mklml intel detecting intel r mkl trying mklml detecting intel r mkl trying mkl rt cmake warning 3rdparty mkldnn cmake mkl.cmake 177 message intel r mkl found. performance features may available. please run scripts prepare mkl.sh download minimal set libraries get full version https software.intel.com en us intel mkl call stack recent call first 3rdparty mkldnn cmake openmp.cmake 24 include 3rdparty mkldnn cmakelists.txt 57 include try openmp c flag fopenmp performing test openmp flag detected performing test openmp flag detected success try openmp cxx flag fopenmp performing test openmp flag detected performing test openmp flag detected success found openmp fopenmp could find doxygen missing doxygen executable vtune profiling environment unset looking pthread.h looking pthread.h found looking pthread create looking pthread create found found threads true found cuda usr local cuda 8.0 found version 8.0 found openblas libraries usr lib libopenblas.so found openblas include usr include cuda detected 8.0 found cudnn include usr local cuda 8.0 include, library usr local cuda 8.0 lib64 libcudnn.so running gpu architecture autodetection found cuda arch 5.2 5.2 5.2 5.2 added cuda nvcc flags sm 52 could find gperftools missing gperftools libraries gperftools include dir found pkgconfig usr bin pkg config found version 0.29.1 could find jemalloc missing jemalloc library jemalloc include dir opencv libs opencv core opencv highgui opencv imgproc opencv imgcodecs opencv found usr local share opencv performing test libomp std cpp11 flag performing test libomp std cpp11 flag success performing test libomp fno exceptions flag performing test libomp fno exceptions flag success performing test libomp fno rtti flag performing test libomp fno rtti flag success performing test libomp x cpp flag performing test libomp x cpp flag success performing test libomp werror flag performing test libomp werror flag success performing test libomp wno unused function flag performing test libomp wno unused function flag success performing test libomp wno unused local typedef flag performing test libomp wno unused local typedef flag failed performing test libomp wno unused value flag performing test libomp wno unused value flag success performing test libomp wno unused variable flag performing test libomp wno unused variable flag success performing test libomp wno switch flag performing test libomp wno switch flag success performing test libomp wno covered switch default flag performing test libomp wno covered switch default flag failed performing test libomp wno deprecated register flag performing test libomp wno deprecated register flag failed performing test libomp wno sign compare flag performing test libomp wno sign compare flag success performing test libomp wno gnu anonymous struct flag performing test libomp wno gnu anonymous struct flag failed performing test libomp wno unknown pragmas flag performing test libomp wno unknown pragmas flag success performing test libomp wno missing field initializers flag performing test libomp wno missing field initializers flag success performing test libomp wno missing braces flag performing test libomp wno missing braces flag success performing test libomp wno comment flag performing test libomp wno comment flag success performing test libomp wno self assign flag performing test libomp wno self assign flag failed performing test libomp wno vla extension flag performing test libomp wno vla extension flag failed performing test libomp wno format pedantic flag performing test libomp wno format pedantic flag failed performing test libomp msse2 flag performing test libomp msse2 flag success performing test libomp ftls model flag performing test libomp ftls model flag success performing test libomp mmic flag performing test libomp mmic flag failed performing test libomp m32 flag performing test libomp m32 flag failed performing test libomp x flag performing test libomp x flag success performing test libomp warn shared textrel flag performing test libomp warn shared textrel flag success performing test libomp needed flag performing test libomp needed flag success performing test libomp version script flag performing test libomp version script flag success performing test libomp static libgcc flag performing test libomp static libgcc flag success performing test libomp z noexecstack flag performing test libomp z noexecstack flag success performing test libomp fini flag performing test libomp fini flag success found perl usr bin perl found version 5.22.1 performing test libomp version symbols performing test libomp version symbols success performing test libomp builtin frame address performing test libomp builtin frame address success performing test libomp weak attribute performing test libomp weak attribute success looking include files windows.h, psapi.h looking include files windows.h, psapi.h found looking enumprocessmodules psapi looking enumprocessmodules psapi found libomp operating system linux libomp target architecture x86 64 libomp build type release libomp openmp version 50 libomp library kind shared libomp library type normal libomp fortran modules false libomp build 20140926 libomp use stats gathering false libomp use debugger support false libomp use itt notify true libomp use ompt support false libomp use adaptive locks true libomp use quad precision true libomp use tsan support false libomp use hwloc library false found pythoninterp usr bin python found version 2.7.12 looking sqrt looking sqrt found looking atomic load 1 looking atomic load 1 found looking atomic load 1 atomic looking atomic load 1 atomic found libomp cannot find llvm lit. libomp please put llvm lit path, set libomp llvm lit executable full path point openmp llvm tools dir directory cmake warning 3rdparty openmp runtime cmake libomputils.cmake 21 message libomp check libomp target available! call stack recent call first 3rdparty openmp runtime test cmakelists.txt 62 libomp warning say could find jemalloc missing jemalloc library jemalloc include dir found gtest gtest found cudnn include usr local cuda 8.0 include, library usr local cuda 8.0 lib64 libcudnn.so called add library library mxnet without source files. typically indicates problem cmakelists.txt file configuring done generating done build files written home jacky4323 test bmxnet incubator mxnet build release make j8 error 56 linking cxx executable benchdnn 57 linking cxx executable test convolution forward u8s8s32 57 built target benchdnn 57 built target test convolution forward u8s8s32 57 linking cxx executable test convolution relu forward f32 57 built target test convolution relu forward f32 cmakefiles makefile2 139 recipe target 'cmakefiles mxnet static.dir all' failed make 1 cmakefiles mxnet static.dir error 2 makefile 138 recipe target 'all' failed make error 2",0,cmake cannot build mxnet,"cmake cannot build mxnet hi, sorry,there problems cmake command use git clone recursive https github.com apache incubator mxnet.git cd incubator mxnet mkdir build release cd build release cmake .. .. make j8 cmake .. .. output log c compiler identification gnu 5.4.0 cxx compiler identification gnu 5.4.0 check working c compiler usr bin cc check working c compiler usr bin cc works detecting c compiler abi info detecting c compiler abi info done detecting c compile features detecting c compile features done check working cxx compiler usr bin c check working cxx compiler usr bin c works detecting cxx compiler abi info detecting cxx compiler abi info done detecting cxx compile features detecting cxx compile features done cmake version '3.5.1' using generator 'unix makefiles' performing test support cxx11 performing test support cxx11 success performing test support cxx0x performing test support cxx0x success performing test support msse2 performing test support msse2 success cmake build type unset, defaulting release detecting intel r mkl trying mklml intel detecting intel r mkl trying mklml detecting intel r mkl trying mkl rt cmake warning 3rdparty mkldnn cmake mkl.cmake 177 message intel r mkl found. performance features may available. please run scripts prepare mkl.sh download minimal set libraries get full version https software.intel.com en us intel mkl call stack recent call first 3rdparty mkldnn cmake openmp.cmake 24 include 3rdparty mkldnn cmakelists.txt 57 include try openmp c flag fopenmp performing test openmp flag detected performing test openmp flag detected success try openmp cxx flag fopenmp performing test openmp flag detected performing test openmp flag detected success found openmp fopenmp could find doxygen missing doxygen executable vtune profiling environment unset looking pthread.h looking pthread.h found looking pthread create looking pthread create found found threads true found cuda usr local cuda 8.0 found version 8.0 found openblas libraries usr lib libopenblas.so found openblas include usr include cuda detected 8.0 found cudnn include usr local cuda 8.0 include, library usr local cuda 8.0 lib64 libcudnn.so running gpu architecture autodetection found cuda arch 5.2 5.2 5.2 5.2 added cuda nvcc flags sm 52 could find gperftools missing gperftools libraries gperftools include dir found pkgconfig usr bin pkg config found version 0.29.1 could find jemalloc missing jemalloc library jemalloc include dir opencv libs opencv core opencv highgui opencv imgproc opencv imgcodecs opencv found usr local share opencv performing test libomp std cpp11 flag performing test libomp std cpp11 flag success performing test libomp fno exceptions flag performing test libomp fno exceptions flag success performing test libomp fno rtti flag performing test libomp fno rtti flag success performing test libomp x cpp flag performing test libomp x cpp flag success performing test libomp werror flag performing test libomp werror flag success performing test libomp wno unused function flag performing test libomp wno unused function flag success performing test libomp wno unused local typedef flag performing test libomp wno unused local typedef flag failed performing test libomp wno unused value flag performing test libomp wno unused value flag success performing test libomp wno unused variable flag performing test libomp wno unused variable flag success performing test libomp wno switch flag performing test libomp wno switch flag success performing test libomp wno covered switch default flag performing test libomp wno covered switch default flag failed performing test libomp wno deprecated register flag performing test libomp wno deprecated register flag failed performing test libomp wno sign compare flag performing test libomp wno sign compare flag success performing test libomp wno gnu anonymous struct flag performing test libomp wno gnu anonymous struct flag failed performing test libomp wno unknown pragmas flag performing test libomp wno unknown pragmas flag success performing test libomp wno missing field initializers flag performing test libomp wno missing field initializers flag success performing test libomp wno missing braces flag performing test libomp wno missing braces flag success performing test libomp wno comment flag performing test libomp wno comment flag success performing test libomp wno self assign flag performing test libomp wno self assign flag failed performing test libomp wno vla extension flag performing test libomp wno vla extension flag failed performing test libomp wno format pedantic flag performing test libomp wno format pedantic flag failed performing test libomp msse2 flag performing test libomp msse2 flag success performing test libomp ftls model flag performing test libomp ftls model flag success performing test libomp mmic flag performing test libomp mmic flag failed performing test libomp m32 flag performing test libomp m32 flag failed performing test libomp x flag performing test libomp x flag success performing test libomp warn shared textrel flag performing test libomp warn shared textrel flag success performing test libomp needed flag performing test libomp needed flag success performing test libomp version script flag performing test libomp version script flag success performing test libomp static libgcc flag performing test libomp static libgcc flag success performing test libomp z noexecstack flag performing test libomp z noexecstack flag success performing test libomp fini flag performing test libomp fini flag success found perl usr bin perl found version 5.22.1 performing test libomp version symbols performing test libomp version symbols success performing test libomp builtin frame address performing test libomp builtin frame address success performing test libomp weak attribute performing test libomp weak attribute success looking include files windows.h, psapi.h looking include files windows.h, psapi.h found looking enumprocessmodules psapi looking enumprocessmodules psapi found libomp operating system linux libomp target architecture x86 64 libomp build type release libomp openmp version 50 libomp library kind shared libomp library type normal libomp fortran modules false libomp build 20140926 libomp use stats gathering false libomp use debugger support false libomp use itt notify true libomp use ompt support false libomp use adaptive locks true libomp use quad precision true libomp use tsan support false libomp use hwloc library false found pythoninterp usr bin python found version 2.7.12 looking sqrt looking sqrt found looking atomic load 1 looking atomic load 1 found looking atomic load 1 atomic looking atomic load 1 atomic found libomp cannot find llvm lit. libomp please put llvm lit path, set libomp llvm lit executable full path point openmp llvm tools dir directory cmake warning 3rdparty openmp runtime cmake libomputils.cmake 21 message libomp check libomp target available! call stack recent call first 3rdparty openmp runtime test cmakelists.txt 62 libomp warning say could find jemalloc missing jemalloc library jemalloc include dir found gtest gtest found cudnn include usr local cuda 8.0 include, library usr local cuda 8.0 lib64 libcudnn.so called add library library mxnet without source files. typically indicates problem cmakelists.txt file configuring done generating done build files written home jacky4323 test bmxnet incubator mxnet build release make j8 error 56 linking cxx executable benchdnn 57 linking cxx executable test convolution forward u8s8s32 57 built target benchdnn 57 built target test convolution forward u8s8s32 57 linking cxx executable test convolution relu forward f32 57 built target test convolution relu forward f32 cmakefiles makefile2 139 recipe target 'cmakefiles mxnet static.dir all' failed make 1 cmakefiles mxnet static.dir error 2 makefile 138 recipe target 'all' failed make error 2"
incubator-mxnet,10279,http jenkins.mxnet ci.amazon ml.com blue organizations jenkins incubator mxnet detail master 546 pipeline https issues.apache.org jira browse mxnet 237,0,failing test operator gpu.test sparse quadratic function test flip,failing test operator gpu.test sparse quadratic function test flip http jenkins.mxnet ci.amazon ml.com blue organizations jenkins incubator mxnet detail master 546 pipeline https issues.apache.org jira browse mxnet 237
incubator-mxnet,4155,2 symbol.cc 2 g opensource mxnet build release libmxnet.lib g opensource mxnet build release libmxnet.exp 2 2 g opensource mxnet src engine threaded engine.cc 298 fatal error c1001 2 f dd vctools compiler utc src p2 ehexcept.c 956 2 2 visual c 2 2 link fatal error lnk1257,0,"error occured complie mxnet vs2013, could tell fix","error occured complie mxnet vs2013, could tell fix 2 symbol.cc 2 g opensource mxnet build release libmxnet.lib g opensource mxnet build release libmxnet.exp 2 2 g opensource mxnet src engine threaded engine.cc 298 fatal error c1001 2 f dd vctools compiler utc src p2 ehexcept.c 956 2 2 visual c 2 2 link fatal error lnk1257"
incubator-mxnet,14844,"description tried build android armv7 using ci build.py failed. runned error showed follows environment info required os ubuntu 18.04 64bit lts mxnet 1.3.1 python 3.6 gpu nvidia gtx1060 6g cpu i7 8750h 16g memory tried solve it? 1. changed mxnetci dockcross address previously 2. set argument 'mxnetcipinned' 3. created restarted docker service 4. used proxy shadowsocks proxychains4 . seems nothing network connection? complete building log wondering anyone reproduce error, something wrong network connection e.g. gfw ?",0,ci build.py failed build mxnet android armv7,"ci build.py failed build mxnet android armv7 description tried build android armv7 using ci build.py failed. runned error showed follows environment info required os ubuntu 18.04 64bit lts mxnet 1.3.1 python 3.6 gpu nvidia gtx1060 6g cpu i7 8750h 16g memory tried solve it? 1. changed mxnetci dockcross address previously 2. set argument 'mxnetcipinned' 3. created restarted docker service 4. used proxy shadowsocks proxychains4 . seems nothing network connection? complete building log wondering anyone reproduce error, something wrong network connection e.g. gfw ?"
incubator-mxnet,9823,"using mxnet cuda9 cudnn7 distributed training enabled. however, run rcnn code example, got following error traceback recent call last file train end2end.py , line 199, main file train end2end.py , line 196, main lr args.lr, lr step args.lr step file train end2end.py , line 158, train net arg params arg params, aux params aux params, begin epoch begin epoch, num epoch end epoch file libs incubator mxnet python mxnet module base module.py , line 496, fit self.update metric eval metric, data batch.label file mx rcnn rcnn core module.py , line 227, update metric self. curr module.update metric eval metric, labels file libs incubator mxnet python mxnet module module.py , line 749, update metric self. exec group.update metric eval metric, labels file libs incubator mxnet python mxnet module executor group.py , line 616, update metric eval metric.update dict labels , preds file libs incubator mxnet python mxnet metric.py , line 280, update dict metric.update dict labels, preds file libs incubator mxnet python mxnet metric.py , line 108, update dict self.update label, pred file mx rcnn rcnn core metric.py , line 51, update pred label mx.ndarray.argmax channel pred .asnumpy .astype 'int32' file libs incubator mxnet python mxnet ndarray ndarray.py , line 1801, asnumpy ctypes.c size data.size file libs incubator mxnet python mxnet base.py , line 148, check call raise mxneterror py str lib.mxgetlasterror mxnet.base.mxneterror 17 08 44 src operator nn . cudnn cudnn softmax activation inl.h 154 check failed e cudnn status success 3 vs. 0 cudnn cudnn status bad param stack trace returned 10 entries bt 0 libs incubator mxnet python mxnet .. .. lib libmxnet.so dmlc stacktrace 0x3d 0x2adc0c3395cd bt 1 libs incubator mxnet python mxnet .. .. lib libmxnet.so dmlc logmessagefatal logmessagefatal 0x18 0x2adc0c339a58 bt 2 libs incubator mxnet python mxnet .. .. lib libmxnet.so mxnet op cudnnsoftmaxactivationop backward mxnet opcontext const , mxnet tblob const , mxnet tblob const , mxnet opreqtype const , mxnet tblob const 0x10b9 0x2adc0f5c7669 bt 3 libs incubator mxnet python mxnet .. .. lib libmxnet.so void mxnet op softmaxactivationgradcompute nnvm nodeattrs const , mxnet opcontext const , std vector const , std vector const , std vector const 0xd4c 0x2adc0f5c2eac bt 4 libs incubator mxnet python mxnet .. .. lib libmxnet.so mxnet exec fcomputeexecutor run mxnet runcontext, bool 0x50 0x2adc0ec4cc40 bt 5 libs incubator mxnet python mxnet .. .. lib libmxnet.so 0x3284653 0x2adc0ec54653 bt 6 libs incubator mxnet python mxnet .. .. lib libmxnet.so mxnet engine threadedengine executeoprblock mxnet runcontext, mxnet engine oprblock 0x2c4 0x2adc0ec2fcd4 bt 7 libs incubator mxnet python mxnet .. .. lib libmxnet.so void mxnet engine threadedengineperdevice gpuworker mxnet context, bool, mxnet engine threadedengineperdevice threadworkerblock , std shared ptr const 0x103 0x2adc0ec34253 bt 8 libs incubator mxnet python mxnet .. .. lib libmxnet.so std function handler , mxnet engine threadedengineperdevice pushtoexecute mxnet engine oprblock , bool lambda 3 operator const lambda std shared ptr 1 invoke std data const , std shared ptr 0x3e 0x2adc0ec3448e bt 9 libs incubator mxnet python mxnet .. .. lib libmxnet.so std thread impl std shared ptr run 0x3b 0x2adc0ec2e36b anyone help it? thanks much!",0,rcnn example fails using latest mxnet,"rcnn example fails using latest mxnet using mxnet cuda9 cudnn7 distributed training enabled. however, run rcnn code example, got following error traceback recent call last file train end2end.py , line 199, main file train end2end.py , line 196, main lr args.lr, lr step args.lr step file train end2end.py , line 158, train net arg params arg params, aux params aux params, begin epoch begin epoch, num epoch end epoch file libs incubator mxnet python mxnet module base module.py , line 496, fit self.update metric eval metric, data batch.label file mx rcnn rcnn core module.py , line 227, update metric self. curr module.update metric eval metric, labels file libs incubator mxnet python mxnet module module.py , line 749, update metric self. exec group.update metric eval metric, labels file libs incubator mxnet python mxnet module executor group.py , line 616, update metric eval metric.update dict labels , preds file libs incubator mxnet python mxnet metric.py , line 280, update dict metric.update dict labels, preds file libs incubator mxnet python mxnet metric.py , line 108, update dict self.update label, pred file mx rcnn rcnn core metric.py , line 51, update pred label mx.ndarray.argmax channel pred .asnumpy .astype 'int32' file libs incubator mxnet python mxnet ndarray ndarray.py , line 1801, asnumpy ctypes.c size data.size file libs incubator mxnet python mxnet base.py , line 148, check call raise mxneterror py str lib.mxgetlasterror mxnet.base.mxneterror 17 08 44 src operator nn . cudnn cudnn softmax activation inl.h 154 check failed e cudnn status success 3 vs. 0 cudnn cudnn status bad param stack trace returned 10 entries bt 0 libs incubator mxnet python mxnet .. .. lib libmxnet.so dmlc stacktrace 0x3d 0x2adc0c3395cd bt 1 libs incubator mxnet python mxnet .. .. lib libmxnet.so dmlc logmessagefatal logmessagefatal 0x18 0x2adc0c339a58 bt 2 libs incubator mxnet python mxnet .. .. lib libmxnet.so mxnet op cudnnsoftmaxactivationop backward mxnet opcontext const , mxnet tblob const , mxnet tblob const , mxnet opreqtype const , mxnet tblob const 0x10b9 0x2adc0f5c7669 bt 3 libs incubator mxnet python mxnet .. .. lib libmxnet.so void mxnet op softmaxactivationgradcompute nnvm nodeattrs const , mxnet opcontext const , std vector const , std vector const , std vector const 0xd4c 0x2adc0f5c2eac bt 4 libs incubator mxnet python mxnet .. .. lib libmxnet.so mxnet exec fcomputeexecutor run mxnet runcontext, bool 0x50 0x2adc0ec4cc40 bt 5 libs incubator mxnet python mxnet .. .. lib libmxnet.so 0x3284653 0x2adc0ec54653 bt 6 libs incubator mxnet python mxnet .. .. lib libmxnet.so mxnet engine threadedengine executeoprblock mxnet runcontext, mxnet engine oprblock 0x2c4 0x2adc0ec2fcd4 bt 7 libs incubator mxnet python mxnet .. .. lib libmxnet.so void mxnet engine threadedengineperdevice gpuworker mxnet context, bool, mxnet engine threadedengineperdevice threadworkerblock , std shared ptr const 0x103 0x2adc0ec34253 bt 8 libs incubator mxnet python mxnet .. .. lib libmxnet.so std function handler , mxnet engine threadedengineperdevice pushtoexecute mxnet engine oprblock , bool lambda 3 operator const lambda std shared ptr 1 invoke std data const , std shared ptr 0x3e 0x2adc0ec3448e bt 9 libs incubator mxnet python mxnet .. .. lib libmxnet.so std thread impl std shared ptr run 0x3b 0x2adc0ec2e36b anyone help it? thanks much!"
incubator-mxnet,8898,"class prelu gluon.hybridblock def init self, args super prelu, self . init args self.name scope self.alpha self.params.get 'alpha', shape 1, , init mx.init.zero self.act gluon.nn.activation activation 'relu' def hybrid forward self, f, x, args, kwargs pos self.act x neg f.negative self.alpha.data self.act f.negative x return pos neg custom block works non hybridized models error , would nice see default activation option, something similar gluon.nn.activation activation 'prelu' plans going forward? assertionerror argument data must symbol instances, got 0.",0,prelu gluon,"prelu gluon class prelu gluon.hybridblock def init self, args super prelu, self . init args self.name scope self.alpha self.params.get 'alpha', shape 1, , init mx.init.zero self.act gluon.nn.activation activation 'relu' def hybrid forward self, f, x, args, kwargs pos self.act x neg f.negative self.alpha.data self.act f.negative x return pos neg custom block works non hybridized models error , would nice see default activation option, something similar gluon.nn.activation activation 'prelu' plans going forward? assertionerror argument data must symbol instances, got 0."
incubator-mxnet,9117,following snippet gives think argsort result ready used index matrices conversion. please also note argsort returns ndaarray floats ints one would expect indexes.,0,argsort produces ndarray cannot used indexing ndarray,argsort produces ndarray cannot used indexing ndarray following snippet gives think argsort result ready used index matrices conversion. please also note argsort returns ndaarray floats ints one would expect indexes.
incubator-mxnet,1102,create new layer python refered example numpy softmax.py https github.com dmlc mxnet blob master example numpy ops numpy softmax.py. fit process good. tried load model json file segmentation fault. known caused serialization deserialization native python object. make sure load model correctly?,0,segmentation fault loading saved numpy op model,segmentation fault loading saved numpy op model create new layer python refered example numpy softmax.py https github.com dmlc mxnet blob master example numpy ops numpy softmax.py. fit process good. tried load model json file segmentation fault. known caused serialization deserialization native python object. make sure load model correctly?
incubator-mxnet,3568,"know vague question, recently pulled latest code models runs half fast worse training code. last version came early march. wondering something training changed could explain happened. using convolutional, pooling, batch normalization, dense layers relu activations. thanks!",0,slowness new code,"slowness new code know vague question, recently pulled latest code models runs half fast worse training code. last version came early march. wondering something training changed could explain happened. using convolutional, pooling, batch normalization, dense layers relu activations. thanks!"
incubator-mxnet,3723,"would like train example code train cifar10 resnet.py two local workers, existed model average implementation directly use? example, cntk kind feature write corresponding update rules kv store? thanks lot!",0,mxnet provide model average distributed training?,"mxnet provide model average distributed training? would like train example code train cifar10 resnet.py two local workers, existed model average implementation directly use? example, cntk kind feature write corresponding update rules kv store? thanks lot!"
incubator-mxnet,2018,see papers ocr without segmentation using cnn lstm. mxnet support work?,0,example ocr use cnn rnn,example ocr use cnn rnn see papers ocr without segmentation using cnn lstm. mxnet support work?
incubator-mxnet,16996,"description input data smaller batch size, sometime errors reproduce using batch size 20 succeeds, larger sizes fail. steps reproduce using 1.6.0 branch",0,mx.io.ndarrayiter pad size large,"mx.io.ndarrayiter pad size large description input data smaller batch size, sometime errors reproduce using batch size 20 succeeds, larger sizes fail. steps reproduce using 1.6.0 branch"
incubator-mxnet,7826,"bugs installation issues, please provide following information. information provide, likely people able help you. environment info operating system windows compiler package used python r scala julia python mxnet version master installed source mxnet commit hash using python package, please provide python version distribution using r package, please provide r error message please paste full error message, including stack trace. minimum reproducible example using code, please provide short script reproduces error. steps reproduce running standard examples, please provide commands run lead error. 1. nosetests tests python unittest test io.py 2. 3. tried solve it? 1. 2. 3.",0,enable test csviter fails intermittently windows,"enable test csviter fails intermittently windows bugs installation issues, please provide following information. information provide, likely people able help you. environment info operating system windows compiler package used python r scala julia python mxnet version master installed source mxnet commit hash using python package, please provide python version distribution using r package, please provide r error message please paste full error message, including stack trace. minimum reproducible example using code, please provide short script reproduces error. steps reproduce running standard examples, please provide commands run lead error. 1. nosetests tests python unittest test io.py 2. 3. tried solve it? 1. 2. 3."
incubator-mxnet,1801,"dear all, title indicats would like customise flow gradients. example, updating parameters, want update according gradient ignoring . notice might done via utilising array, anyone give specific instruction? level acceptable python c . thanks advance.",0,customise control towards gradient flow,"customise control towards gradient flow dear all, title indicats would like customise flow gradients. example, updating parameters, want update according gradient ignoring . notice might done via utilising array, anyone give specific instruction? level acceptable python c . thanks advance."
incubator-mxnet,902,"want init fc layer weights alone , ?",0,initialize weights,"initialize weights want init fc layer weights alone , ?"
incubator-mxnet,3198,"lacking operators major disadvantage mxnet comparing frameworks. current multidimensional array interface follows convention. see tutorial http nbviewer.jupyter.org github dmlc mxnet notebooks blob master python basic ndarray.ipynb quick overview. going add rest numpy operators mxnet, also fix existing operators different according ones numpy. grabbed numpy's routines page https docs.scipy.org doc numpy reference routines.html . comparison mxnet's operators listed following issues. operator, 4 states v already done, consistent numpy p partially done. part fixed comments x exists, need add mxnet exists, support short time ? sure whether x , leave discussion use separate issues track operator category. schedule survey current status, operators added fixed. examples codes implement new operators assign jobs contributors also related tasks better operator documents better unittest improve neural network related operators",0,check list operators,"check list operators lacking operators major disadvantage mxnet comparing frameworks. current multidimensional array interface follows convention. see tutorial http nbviewer.jupyter.org github dmlc mxnet notebooks blob master python basic ndarray.ipynb quick overview. going add rest numpy operators mxnet, also fix existing operators different according ones numpy. grabbed numpy's routines page https docs.scipy.org doc numpy reference routines.html . comparison mxnet's operators listed following issues. operator, 4 states v already done, consistent numpy p partially done. part fixed comments x exists, need add mxnet exists, support short time ? sure whether x , leave discussion use separate issues track operator category. schedule survey current status, operators added fixed. examples codes implement new operators assign jobs contributors also related tasks better operator documents better unittest improve neural network related operators"
incubator-mxnet,3915,"changes made api pages, example, changing link content left sidebar, makes auto module index.js failed. entries symbol creation api reference child level more.",0,auto module index.js work properly,"auto module index.js work properly changes made api pages, example, changing link content left sidebar, makes auto module index.js failed. entries symbol creation api reference child level more."
incubator-mxnet,2727,"tmatas sure using im2rec.py, example, file 'im2rec.py' folder 'd test' two class trainning images two classification folders 'd test train1' 'd test train2', respectively, image file jpeg format. example, create rec file mx.io.imagerecorditer im2rec.py? thanks help much.",0,using im2rec.py,"using im2rec.py tmatas sure using im2rec.py, example, file 'im2rec.py' folder 'd test' two class trainning images two classification folders 'd test train1' 'd test train2', respectively, image file jpeg format. example, create rec file mx.io.imagerecorditer im2rec.py? thanks help much."
incubator-mxnet,13441,"would nice add spec validations random namespace https github.com apache incubator mxnet blob master contrib clojure package src org apache clojure mxnet random.clj user calls function incorrect arguments, guide correct form. using function https github.com apache incubator mxnet blob master contrib clojure package src org apache clojure mxnet module.clj l186 correct specs would great addition project. addition unit tests test failing case spec exception thrown would great",0,clojure add spec validations random namespace,"clojure add spec validations random namespace would nice add spec validations random namespace https github.com apache incubator mxnet blob master contrib clojure package src org apache clojure mxnet random.clj user calls function incorrect arguments, guide correct form. using function https github.com apache incubator mxnet blob master contrib clojure package src org apache clojure mxnet module.clj l186 correct specs would great addition project. addition unit tests test failing case spec exception thrown would great"
incubator-mxnet,14895,"page https github.com apache incubator mxnet blob master docs install build source.md click ubuntu hyperlink, broken https github.com apache incubator mxnet blob master docs install ubuntu setup.html",0,doc build source link ubuntu broken,"doc build source link ubuntu broken page https github.com apache incubator mxnet blob master docs install build source.md click ubuntu hyperlink, broken https github.com apache incubator mxnet blob master docs install ubuntu setup.html"
incubator-mxnet,6912,tools project?,0,sorry find tools?,sorry find tools? tools project?
incubator-mxnet,12030,"example code something like above. resnet 50 initialized initializer. however, outputs two softmax different begining, something likes 27.x vs 21.x strange think.",0,happen group two model together train it?,"happen group two model together train it? example code something like above. resnet 50 initialized initializer. however, outputs two softmax different begining, something likes 27.x vs 21.x strange think."
incubator-mxnet,16932,"description sometimes users may expect gluon hybridization magically things currently cannot do. example https github.com apache incubator mxnet issues 16926, user expects control flow incorporated hybridized graph. instead silently ignoring control flow picking branch selected first call hybrid network, error warn user.",0,detect unsupported usage gluon hybridization,"detect unsupported usage gluon hybridization description sometimes users may expect gluon hybridization magically things currently cannot do. example https github.com apache incubator mxnet issues 16926, user expects control flow incorporated hybridized graph. instead silently ignoring control flow picking branch selected first call hybrid network, error warn user."
incubator-mxnet,15482,"description use mx2onnx onnx mxnet.export model transfer mxnet symbol onnx . moving mean moving var param batchnorm params. environment info required package used python r scala julia usining python build info required built source compiler gcc mxnet commit hash da4b2a82511df build config ifndef cc export cc gcc endif ifndef cxx export cxx g endif ifndef nvcc export nvcc nvcc endif whether compile options mxnet developer dev 0 whether compile debug debug 0 whether turn segfault signal handler log stack trace use signal handler additional link flags want add add ldflags additional compile flags want add add cflags matrix computation libraries cpu gpu whether use cuda compile use cuda 1 add path cuda library link compile flag already add environment variable, leave none use cuda path usr local cuda use cuda path none whether enable cuda runtime compilation enable cuda rtc 1 whether use cudnn r3 library use cudnn 1 whether use nvtx profiling use nvtx 0 whether use nccl library use nccl 0 add path nccl library use nccl path none whether use opencv compilation disable it, however, able use imbin iterator use opencv 1 add opencv include path, directory exists use opencv inc path none add opencv shared library path, shared library exists use opencv lib path none whether use libjpeg turbo image decode without opencv wrapper use libjpeg turbo 0 add path libjpeg turbo library use libjpeg turbo path none use openmp parallelization use openmp 1 whether use mkl dnn library 0 disabled, 1 enabled use mkldnn defined, mkl dnn enabled default x86 linux. disable explicity use mkldnn 0 use mkldnn 0 whether use nnpack library use nnpack 0 choose version blas want use mkl, blas, atlas, openblas default use atlas linux apple osx uname shell uname ifeq uname , darwin use blas apple else use blas atlas endif whether use lapack compilation effective compiled blas versions openblas apple atlas mkl use lapack 1 path lapack library case non standard installation use lapack path add path intel library, may need mkl, add path environment variable use intel path none use mkl blas, choose static link automatically allow python wrapper ifeq use blas , mkl use static mkl 1 else use static mkl none endif settings power arm arch arch shell uname ifneq , filter arch , armv6l armv7l powerpc64le ppc64le aarch64 use sse 0 use f16c 0 else use sse 1 endif f16c instruction support faster arithmetic fp16 cpu distributed training fp16, helps even training gpus left empty, checks cpu support turns on. cross compilation, please check support f16c target device turn necessary. use f16c distributed computing whether enable multi machine supporting use dist kvstore 0 whether allow read write hdfs directly. yes, hadoop required use hdfs 0 path libjvm.so. required use hdfs 1 libjvm java home jre lib amd64 server whether allow read write aws s3 directly. yes, libcurl4 openssl dev required, installed ubuntu sudo apt get install libcurl4 openssl dev use s3 0 performance settings use operator tuning use operator tuning 1 use gperftools found disable 8968 use gperftools 0 path gperftools tcmalloc library case non standard installation use gperftools path link gperftools statically use gperftools static use jemalloc found, using gperftools use jemalloc 1 path jemalloc library case non standard installation use jemalloc path link jemalloc statically use jemalloc static additional operators path folders containing projects specific operators want put src operators extra operators features create c interface package use cpp package 0 use int64 type represent total number elements tensor cause performance degradation reported issue 14496 set 1 large tensor tensor size greater int32 max i.e. 2147483647 note size dimension still bounded int32 max use int64 tensor size 0 python executable. needed cython target python python plugins whether use caffe integration. requires installing caffe. also need add caffe path build lib ld library path caffe path home caffe mxnet plugins plugin caffe caffe.mk warpctc path home warp ctc warpctc path home deep warp ctc mxnet plugins plugin warpctc warpctc.mk whether use sframe integration. requires build sframe git github.com dato code sframe.git sframe path home sframe mxnet plugins plugin sframe plugin.mk error message info root converting idx 0, op null, name data info root converting idx 1, op null, name first 3x3 conv conv2d weight info root converting idx 2, op convolution, name first 3x3 conv conv2d info root converting idx 3, op null, name first 3x3 conv batchnorm gamma info root converting idx 4, op null, name first 3x3 conv batchnorm beta info root converting idx 5, op null, name first 3x3 conv batchnorm moving mean traceback recent call last file home deep workssd arm tvm app tune relay mobile gpu.py , line 484, tune evaluate tuning option file home deep workssd arm tvm app tune relay mobile gpu.py , line 436, tune evaluate net, params, input shape, get network network, batch size 1 file home deep workssd arm tvm app tune relay mobile gpu.py , line 93, get network return get network lpr mb2 name,batch size file home deep workssd arm tvm app tune relay mobile gpu.py , line 143, get network lpr mb2 test onnx file home deep workssd arm tvm app tune relay mobile gpu.py , line 135, test onnx converted model path onnx mxnet.export model mx sym, args, input shape , np.float32, onnx file, true file home deep workssd mxnet incubator mxnet python mxnet contrib onnx mx2onnx export model.py , line 87, export model verbose verbose file home deep workssd mxnet incubator mxnet python mxnet contrib onnx mx2onnx export onnx.py , line 256, create onnx graph proto idx idx file home deep workssd mxnet incubator mxnet python mxnet contrib onnx mx2onnx export onnx.py , line 92, convert layer return convert func node, kwargs file home deep workssd mxnet incubator mxnet python mxnet contrib onnx mx2onnx op translations.py , line 170, convert weights inputs np arr weights name keyerror 'first 3x3 conv batchnorm moving mean' error sys.excepthook traceback recent call last file usr lib python3 dist packages apport python hook.py , line 63, apport excepthook apport.fileutils import likely packaged, get recent crashes file usr lib python3 dist packages apport init .py , line 5, apport.report import report file usr lib python3 dist packages apport report.py , line 30, import apport.fileutils file usr lib python3 dist packages apport fileutils.py , line 23, apport.packaging impl import impl packaging file usr lib python3 dist packages apport packaging impl.py , line 23, import apt file usr lib python3 dist packages apt init .py , line 23, import apt pkg modulenotfounderror module named 'apt pkg' original exception traceback recent call last file home deep workssd arm tvm app tune relay mobile gpu.py , line 484, tune evaluate tuning option file home deep workssd arm tvm app tune relay mobile gpu.py , line 436, tune evaluate net, params, input shape, get network network, batch size 1 file home deep workssd arm tvm app tune relay mobile gpu.py , line 93, get network return get network lpr mb2 name,batch size file home deep workssd arm tvm app tune relay mobile gpu.py , line 143, get network lpr mb2 test onnx file home deep workssd arm tvm app tune relay mobile gpu.py , line 135, test onnx converted model path onnx mxnet.export model mx sym, args, input shape , np.float32, onnx file, true file home deep workssd mxnet incubator mxnet python mxnet contrib onnx mx2onnx export model.py , line 87, export model verbose verbose file home deep workssd mxnet incubator mxnet python mxnet contrib onnx mx2onnx export onnx.py , line 256, create onnx graph proto idx idx file home deep workssd mxnet incubator mxnet python mxnet contrib onnx mx2onnx export onnx.py , line 92, convert layer return convert func node, kwargs file home deep workssd mxnet incubator mxnet python mxnet contrib onnx mx2onnx op translations.py , line 170, convert weights inputs np arr weights name keyerror 'first 3x3 conv batchnorm moving mean' minimum reproducible example steps reproduce paste commands ran produced error. 1.python3 tran2onnx.py 2. tried solve it? 1.by debugging ,the moving mean moving var batchnorm params ,so converter treat input real. 2. code process moving mean moving var batchnorm indepently.",0,mx2onnx error batchnorm,"mx2onnx error batchnorm description use mx2onnx onnx mxnet.export model transfer mxnet symbol onnx . moving mean moving var param batchnorm params. environment info required package used python r scala julia usining python build info required built source compiler gcc mxnet commit hash da4b2a82511df build config ifndef cc export cc gcc endif ifndef cxx export cxx g endif ifndef nvcc export nvcc nvcc endif whether compile options mxnet developer dev 0 whether compile debug debug 0 whether turn segfault signal handler log stack trace use signal handler additional link flags want add add ldflags additional compile flags want add add cflags matrix computation libraries cpu gpu whether use cuda compile use cuda 1 add path cuda library link compile flag already add environment variable, leave none use cuda path usr local cuda use cuda path none whether enable cuda runtime compilation enable cuda rtc 1 whether use cudnn r3 library use cudnn 1 whether use nvtx profiling use nvtx 0 whether use nccl library use nccl 0 add path nccl library use nccl path none whether use opencv compilation disable it, however, able use imbin iterator use opencv 1 add opencv include path, directory exists use opencv inc path none add opencv shared library path, shared library exists use opencv lib path none whether use libjpeg turbo image decode without opencv wrapper use libjpeg turbo 0 add path libjpeg turbo library use libjpeg turbo path none use openmp parallelization use openmp 1 whether use mkl dnn library 0 disabled, 1 enabled use mkldnn defined, mkl dnn enabled default x86 linux. disable explicity use mkldnn 0 use mkldnn 0 whether use nnpack library use nnpack 0 choose version blas want use mkl, blas, atlas, openblas default use atlas linux apple osx uname shell uname ifeq uname , darwin use blas apple else use blas atlas endif whether use lapack compilation effective compiled blas versions openblas apple atlas mkl use lapack 1 path lapack library case non standard installation use lapack path add path intel library, may need mkl, add path environment variable use intel path none use mkl blas, choose static link automatically allow python wrapper ifeq use blas , mkl use static mkl 1 else use static mkl none endif settings power arm arch arch shell uname ifneq , filter arch , armv6l armv7l powerpc64le ppc64le aarch64 use sse 0 use f16c 0 else use sse 1 endif f16c instruction support faster arithmetic fp16 cpu distributed training fp16, helps even training gpus left empty, checks cpu support turns on. cross compilation, please check support f16c target device turn necessary. use f16c distributed computing whether enable multi machine supporting use dist kvstore 0 whether allow read write hdfs directly. yes, hadoop required use hdfs 0 path libjvm.so. required use hdfs 1 libjvm java home jre lib amd64 server whether allow read write aws s3 directly. yes, libcurl4 openssl dev required, installed ubuntu sudo apt get install libcurl4 openssl dev use s3 0 performance settings use operator tuning use operator tuning 1 use gperftools found disable 8968 use gperftools 0 path gperftools tcmalloc library case non standard installation use gperftools path link gperftools statically use gperftools static use jemalloc found, using gperftools use jemalloc 1 path jemalloc library case non standard installation use jemalloc path link jemalloc statically use jemalloc static additional operators path folders containing projects specific operators want put src operators extra operators features create c interface package use cpp package 0 use int64 type represent total number elements tensor cause performance degradation reported issue 14496 set 1 large tensor tensor size greater int32 max i.e. 2147483647 note size dimension still bounded int32 max use int64 tensor size 0 python executable. needed cython target python python plugins whether use caffe integration. requires installing caffe. also need add caffe path build lib ld library path caffe path home caffe mxnet plugins plugin caffe caffe.mk warpctc path home warp ctc warpctc path home deep warp ctc mxnet plugins plugin warpctc warpctc.mk whether use sframe integration. requires build sframe git github.com dato code sframe.git sframe path home sframe mxnet plugins plugin sframe plugin.mk error message info root converting idx 0, op null, name data info root converting idx 1, op null, name first 3x3 conv conv2d weight info root converting idx 2, op convolution, name first 3x3 conv conv2d info root converting idx 3, op null, name first 3x3 conv batchnorm gamma info root converting idx 4, op null, name first 3x3 conv batchnorm beta info root converting idx 5, op null, name first 3x3 conv batchnorm moving mean traceback recent call last file home deep workssd arm tvm app tune relay mobile gpu.py , line 484, tune evaluate tuning option file home deep workssd arm tvm app tune relay mobile gpu.py , line 436, tune evaluate net, params, input shape, get network network, batch size 1 file home deep workssd arm tvm app tune relay mobile gpu.py , line 93, get network return get network lpr mb2 name,batch size file home deep workssd arm tvm app tune relay mobile gpu.py , line 143, get network lpr mb2 test onnx file home deep workssd arm tvm app tune relay mobile gpu.py , line 135, test onnx converted model path onnx mxnet.export model mx sym, args, input shape , np.float32, onnx file, true file home deep workssd mxnet incubator mxnet python mxnet contrib onnx mx2onnx export model.py , line 87, export model verbose verbose file home deep workssd mxnet incubator mxnet python mxnet contrib onnx mx2onnx export onnx.py , line 256, create onnx graph proto idx idx file home deep workssd mxnet incubator mxnet python mxnet contrib onnx mx2onnx export onnx.py , line 92, convert layer return convert func node, kwargs file home deep workssd mxnet incubator mxnet python mxnet contrib onnx mx2onnx op translations.py , line 170, convert weights inputs np arr weights name keyerror 'first 3x3 conv batchnorm moving mean' error sys.excepthook traceback recent call last file usr lib python3 dist packages apport python hook.py , line 63, apport excepthook apport.fileutils import likely packaged, get recent crashes file usr lib python3 dist packages apport init .py , line 5, apport.report import report file usr lib python3 dist packages apport report.py , line 30, import apport.fileutils file usr lib python3 dist packages apport fileutils.py , line 23, apport.packaging impl import impl packaging file usr lib python3 dist packages apport packaging impl.py , line 23, import apt file usr lib python3 dist packages apt init .py , line 23, import apt pkg modulenotfounderror module named 'apt pkg' original exception traceback recent call last file home deep workssd arm tvm app tune relay mobile gpu.py , line 484, tune evaluate tuning option file home deep workssd arm tvm app tune relay mobile gpu.py , line 436, tune evaluate net, params, input shape, get network network, batch size 1 file home deep workssd arm tvm app tune relay mobile gpu.py , line 93, get network return get network lpr mb2 name,batch size file home deep workssd arm tvm app tune relay mobile gpu.py , line 143, get network lpr mb2 test onnx file home deep workssd arm tvm app tune relay mobile gpu.py , line 135, test onnx converted model path onnx mxnet.export model mx sym, args, input shape , np.float32, onnx file, true file home deep workssd mxnet incubator mxnet python mxnet contrib onnx mx2onnx export model.py , line 87, export model verbose verbose file home deep workssd mxnet incubator mxnet python mxnet contrib onnx mx2onnx export onnx.py , line 256, create onnx graph proto idx idx file home deep workssd mxnet incubator mxnet python mxnet contrib onnx mx2onnx export onnx.py , line 92, convert layer return convert func node, kwargs file home deep workssd mxnet incubator mxnet python mxnet contrib onnx mx2onnx op translations.py , line 170, convert weights inputs np arr weights name keyerror 'first 3x3 conv batchnorm moving mean' minimum reproducible example steps reproduce paste commands ran produced error. 1.python3 tran2onnx.py 2. tried solve it? 1.by debugging ,the moving mean moving var batchnorm params ,so converter treat input real. 2. code process moving mean moving var batchnorm indepently."
incubator-mxnet,6736,"call , try concatenate outputs different devices along major axis. major axis computed based initializer . recommended way set symbol? simply pass constructing symbol? attribute set automatically module binding? setting necessary, otherwise , leading trying concatenate along fail batch size divisible number devices symbol outputs shape 1, batch size per device, x . i.e. case 3 devices batch size 128, concatenating along fail.",0,dataparallelexecutorgroup layout handling symbols,"dataparallelexecutorgroup layout handling symbols call , try concatenate outputs different devices along major axis. major axis computed based initializer . recommended way set symbol? simply pass constructing symbol? attribute set automatically module binding? setting necessary, otherwise , leading trying concatenate along fail batch size divisible number devices symbol outputs shape 1, batch size per device, x . i.e. case 3 devices batch size 128, concatenating along fail."
incubator-mxnet,1246,"find mxnet add torch support, question. piiswrong",0,torch backend torch backend efficient original mxnet's op?,"torch backend torch backend efficient original mxnet's op? find mxnet add torch support, question. piiswrong"
incubator-mxnet,9853,http jenkins.mxnet ci.amazon ml.com blue organizations jenkins incubator mxnet detail master 397 pipeline,0,flaky test operator.test binary op,flaky test operator.test binary op http jenkins.mxnet ci.amazon ml.com blue organizations jenkins incubator mxnet detail master 397 pipeline
incubator-mxnet,9044,"description able built libmxnet.so, successfully build scala package, run gan mnist.sh fails. environment info required change diagnose.py 113 except ioerror e since filenotfounderror python 3. package used python r scala julia scala scala user, please provide 1. java version 2. maven version build info required built source compiler gcc clang mingw visual studio mxnet commit hash 2700ddbbeef212879802f7f0c0812192ec5c2b77 build config error message minimum reproducible example https github.com apache incubator mxnet blob master scala package examples scripts run gan mnist.sh steps reproduce 1. make j4 2. make scalapkg 3. make scalatest 4. make scalainstall 5. get mnist data 6. update run gan mnist.sh cpu 7. bash run gan mnist.sh 1 .. .. data .. .. tmp tried solve it? 1. check links libmxnet.so 2. check built openblas https github.com xianyi openblas, seem issue since cblas function 3. looked possibly related issues, like 2184, closed placed todo 3084 javelinjs closed without resolved.",0,scala run gan mnist.sh fails,"scala run gan mnist.sh fails description able built libmxnet.so, successfully build scala package, run gan mnist.sh fails. environment info required change diagnose.py 113 except ioerror e since filenotfounderror python 3. package used python r scala julia scala scala user, please provide 1. java version 2. maven version build info required built source compiler gcc clang mingw visual studio mxnet commit hash 2700ddbbeef212879802f7f0c0812192ec5c2b77 build config error message minimum reproducible example https github.com apache incubator mxnet blob master scala package examples scripts run gan mnist.sh steps reproduce 1. make j4 2. make scalapkg 3. make scalatest 4. make scalainstall 5. get mnist data 6. update run gan mnist.sh cpu 7. bash run gan mnist.sh 1 .. .. data .. .. tmp tried solve it? 1. check links libmxnet.so 2. check built openblas https github.com xianyi openblas, seem issue since cblas function 3. looked possibly related issues, like 2184, closed placed todo 3084 javelinjs closed without resolved."
incubator-mxnet,12839,"mentioned 12822 something readme clojure package intended fix it. however, read text bit detail, broader questions might good discuss advance come random change. main issue current version someone familiar package architecture starting clojure that's currently , follow instructions really clue doing. nagging questions want use prebuilt jars, exactly do? fact instructions need follow cloning repo running source rather confusing. feel instructions prebuilt jars source entirely separate. maybe also good know would possible scenarios exist. guess would say 1 everything prebuilt jars, 2 deps prebuilt jars, clojure package source, 3 infinity possible combinations prebuilt jars manual builds deps, notably scala package mxnet core. prebuilt jars what? certainly much mention, guess, biggest ones, scala package base mxnet package would worth mentioning. instructions end? found surprising scroll past examples find header build mxnet source , source mentioned earlier already. two things connected? one way make clear two entirely separate sets instructions there? make headers method 1 use prebuilt jars method 2 build source . prebuilt scala package working 12822, workaround place . build source still use prebuilt jar mxnet core? mentioned above, really familiar build infrastructure clojure package architecture, exactly know write instructions. equipped extra knowledge, certainly could.",0,clojure readme getting started entirely trivial follow,"clojure readme getting started entirely trivial follow mentioned 12822 something readme clojure package intended fix it. however, read text bit detail, broader questions might good discuss advance come random change. main issue current version someone familiar package architecture starting clojure that's currently , follow instructions really clue doing. nagging questions want use prebuilt jars, exactly do? fact instructions need follow cloning repo running source rather confusing. feel instructions prebuilt jars source entirely separate. maybe also good know would possible scenarios exist. guess would say 1 everything prebuilt jars, 2 deps prebuilt jars, clojure package source, 3 infinity possible combinations prebuilt jars manual builds deps, notably scala package mxnet core. prebuilt jars what? certainly much mention, guess, biggest ones, scala package base mxnet package would worth mentioning. instructions end? found surprising scroll past examples find header build mxnet source , source mentioned earlier already. two things connected? one way make clear two entirely separate sets instructions there? make headers method 1 use prebuilt jars method 2 build source . prebuilt scala package working 12822, workaround place . build source still use prebuilt jar mxnet core? mentioned above, really familiar build infrastructure clojure package architecture, exactly know write instructions. equipped extra knowledge, certainly could."
incubator-mxnet,7110,"wondering already easy use mxnet.metric loss function place defining using gluon loss. instance, would prefer use mx.metric perplexity. currently, defining loss function calls perplexity class, seems like feature would good gluon.",0,gluon using mxnet evaluation metrics gluon,"gluon using mxnet evaluation metrics gluon wondering already easy use mxnet.metric loss function place defining using gluon loss. instance, would prefer use mx.metric perplexity. currently, defining loss function calls perplexity class, seems like feature would good gluon."
incubator-mxnet,9068,"win10, mxnet 0.12,python 2.7 official example warpctc, trying use sym.contrib.ctc loss instead baidu warpctc, windows. modified . noitice modified. running , get error said wonder fix it.",0,use sym.contrib.ctc loss instead baidu warpctc,"use sym.contrib.ctc loss instead baidu warpctc win10, mxnet 0.12,python 2.7 official example warpctc, trying use sym.contrib.ctc loss instead baidu warpctc, windows. modified . noitice modified. running , get error said wonder fix it."
incubator-mxnet,7865,"bugs installation issues, please provide following information. information provide, likely people able help you. environment info operating system windows 10 compiler vs2015 package used python r scala julia mxnet version 0.905 installed source mxnet commit hash using python package, please provide python version distribution using r package, please provide r error message loaded ndarrays map loadtomap device context 'gpu'. minimum reproducible example use following code read params file params map ndarray loadtomap filenamepars args map.clear auxs map.clear int pos 0 auto params map.begin ! params map.end const string key first ndarray val second pos key.find arg ! string npos args map key.substr 4 val pos key.find aux ! string npos auxs map key.substr 4 val steps reproduce running standard examples, please provide commands run lead error. 1. 2. 3. tried solve it? 1. 2. 3.",0,load params loadtomap c api set device contect gpu?,"load params loadtomap c api set device contect gpu? bugs installation issues, please provide following information. information provide, likely people able help you. environment info operating system windows 10 compiler vs2015 package used python r scala julia mxnet version 0.905 installed source mxnet commit hash using python package, please provide python version distribution using r package, please provide r error message loaded ndarrays map loadtomap device context 'gpu'. minimum reproducible example use following code read params file params map ndarray loadtomap filenamepars args map.clear auxs map.clear int pos 0 auto params map.begin ! params map.end const string key first ndarray val second pos key.find arg ! string npos args map key.substr 4 val pos key.find aux ! string npos auxs map key.substr 4 val steps reproduce running standard examples, please provide commands run lead error. 1. 2. 3. tried solve it? 1. 2. 3."
incubator-mxnet,14484,"description training fcn model gluon cv 2 gpus encounter different perhaps related issues depending kind kvstore use 'local' 'device' . think gluon cv issue. test script included. environment info required package used python r scala julia python error message kvstore 'local' kvstore 'device' error, process hangs trying push kvstore . example script includes debug code narrow process hangs. note specific layer stops varies. minimum reproducible example steps reproduce 1. run script, setting kvstore type either . tried solve it? 1. disabling gc beginning epoch enabling end, seemed work one similar seeming issue, made difference me. note still get result using sub classed version gluon.trainer.",0,odd behaviour 'device' kvstore cuda illegal memory access errors,"odd behaviour 'device' kvstore cuda illegal memory access errors description training fcn model gluon cv 2 gpus encounter different perhaps related issues depending kind kvstore use 'local' 'device' . think gluon cv issue. test script included. environment info required package used python r scala julia python error message kvstore 'local' kvstore 'device' error, process hangs trying push kvstore . example script includes debug code narrow process hangs. note specific layer stops varies. minimum reproducible example steps reproduce 1. run script, setting kvstore type either . tried solve it? 1. disabling gc beginning epoch enabling end, seemed work one similar seeming issue, made difference me. note still get result using sub classed version gluon.trainer."
incubator-mxnet,11120,http jenkins.mxnet ci.amazon ml.com blue organizations jenkins incubator mxnet detail pr 10656 7 pipeline possibly related hardcoding ports.,0,address already use tutorial test,address already use tutorial test http jenkins.mxnet ci.amazon ml.com blue organizations jenkins incubator mxnet detail pr 10656 7 pipeline possibly related hardcoding ports.
incubator-mxnet,4841,"trying reproduce faster rcnn tutorial http mxnet.io tutorials computer vision detection.html running python train alternate.py gpus 0 yields error traceback recent call last file train alternate.py , line 7, rcnn.tools.train rpn import train rpn file mnt gelu mxnet example rcnn rcnn tools train rpn.py , line 7, ..symbol import file mnt gelu mxnet example rcnn rcnn symbol init .py , line 1, symbol vgg import file mnt gelu mxnet example rcnn rcnn symbol symbol vgg.py , line 2, import proposal file mnt gelu mxnet example rcnn rcnn symbol proposal.py , line 11, rcnn.processing.bbox transform import bbox pred, clip boxes file mnt gelu mxnet example rcnn rcnn processing bbox transform.py , line 2, ..cython.bbox import bbox overlaps cython importerror module named bbox tried searching bbox module could find anywhere. mxnet compiled ubuntu 16 cuda cdnn support",0,module named bbox running faster cnn exampl,"module named bbox running faster cnn exampl trying reproduce faster rcnn tutorial http mxnet.io tutorials computer vision detection.html running python train alternate.py gpus 0 yields error traceback recent call last file train alternate.py , line 7, rcnn.tools.train rpn import train rpn file mnt gelu mxnet example rcnn rcnn tools train rpn.py , line 7, ..symbol import file mnt gelu mxnet example rcnn rcnn symbol init .py , line 1, symbol vgg import file mnt gelu mxnet example rcnn rcnn symbol symbol vgg.py , line 2, import proposal file mnt gelu mxnet example rcnn rcnn symbol proposal.py , line 11, rcnn.processing.bbox transform import bbox pred, clip boxes file mnt gelu mxnet example rcnn rcnn processing bbox transform.py , line 2, ..cython.bbox import bbox overlaps cython importerror module named bbox tried searching bbox module could find anywhere. mxnet compiled ubuntu 16 cuda cdnn support"
incubator-mxnet,7491,"found gpudevicestorage used, however cpudevicestorage used, that?",0,gpudevicestorage used? why?,"gpudevicestorage used? why? found gpudevicestorage used, however cpudevicestorage used, that?"
incubator-mxnet,3517,found faster rcnn supported mxnet. train multi gpu multi machine ?,0,train faster rcnn multi gpu multi machine,train faster rcnn multi gpu multi machine found faster rcnn supported mxnet. train multi gpu multi machine ?
incubator-mxnet,3058,"output loss accuray others log info. tutorial demo? search fo long time ,no harvest. tks.",0,output loss,"output loss output loss accuray others log info. tutorial demo? search fo long time ,no harvest. tks."
incubator-mxnet,2171,"hi, trying implement sequence sequence model using mxnet. seems argmax api mxnet symbol http mxnet.readthedocs.io en latest packages python symbol.html . implement decoder part emitting word softmax? suggestions? thanks.",0,symbol argmax api absence,"symbol argmax api absence hi, trying implement sequence sequence model using mxnet. seems argmax api mxnet symbol http mxnet.readthedocs.io en latest packages python symbol.html . implement decoder part emitting word softmax? suggestions? thanks."
incubator-mxnet,10217,description building opencv cmake causes link errors ubuntu installed opencv environment info required package used n build info required built source compiler gcc clang mingw visual studio gcc mxnet commit hash 09281c76bc215823bc74b34f9ac65d906b741377 build config error message link error tiffreadrgbastrip libtiff 4.0' usr local lib libopencv imgcodecs.so.3.2.0 undefined reference tiffwriteencodedstrip libtiff 4.0' usr local lib libopencv imgcodecs.so.3.2.0 undefined reference tiffwritescanline libtiff 4.0' usr local lib libopencv imgcodecs.so.3.2.0 undefined reference tiffnumberofstrips libtiff 4.0' usr local lib libopencv imgcodecs.so.3.2.0 undefined reference tiffreadencodedtile libtiff 4.0' usr local lib libopencv imgcodecs.so.3.2.0 undefined reference tiffclose libtiff 4.0' usr local lib libopencv imgcodecs.so.3.2.0 undefined reference tiffopen libtiff 4.0' usr local lib libopencv imgcodecs.so.3.2.0 undefined reference tiffsetfield libtiff 4.0' usr local lib libopencv imgcodecs.so.3.2.0 undefined reference tiffseterrorhandler libtiff 4.0' collect2 error ld returned 1 exit status ninja build stopped subcommand failed. minimum reproducible example n steps reproduce 1. cmake dcmake build type release duse gperftools duse dist kvstore duse sse duse mkldnn ..,0,building opencv causes link errors,building opencv causes link errors description building opencv cmake causes link errors ubuntu installed opencv environment info required package used n build info required built source compiler gcc clang mingw visual studio gcc mxnet commit hash 09281c76bc215823bc74b34f9ac65d906b741377 build config error message link error tiffreadrgbastrip libtiff 4.0' usr local lib libopencv imgcodecs.so.3.2.0 undefined reference tiffwriteencodedstrip libtiff 4.0' usr local lib libopencv imgcodecs.so.3.2.0 undefined reference tiffwritescanline libtiff 4.0' usr local lib libopencv imgcodecs.so.3.2.0 undefined reference tiffnumberofstrips libtiff 4.0' usr local lib libopencv imgcodecs.so.3.2.0 undefined reference tiffreadencodedtile libtiff 4.0' usr local lib libopencv imgcodecs.so.3.2.0 undefined reference tiffclose libtiff 4.0' usr local lib libopencv imgcodecs.so.3.2.0 undefined reference tiffopen libtiff 4.0' usr local lib libopencv imgcodecs.so.3.2.0 undefined reference tiffsetfield libtiff 4.0' usr local lib libopencv imgcodecs.so.3.2.0 undefined reference tiffseterrorhandler libtiff 4.0' collect2 error ld returned 1 exit status ninja build stopped subcommand failed. minimum reproducible example n steps reproduce 1. cmake dcmake build type release duse gperftools duse dist kvstore duse sse duse mkldnn ..
incubator-mxnet,14177,"hi trying optimize models hybridize accidentally misspelled argument method. got quite interesting message using minimal example although find message quite useful, also found quite surprising since documentation https mxnet.incubator.apache.org versions master api python gluon gluon.html?highlight hybrid mxnet.gluon.block.hybridize mentions . sure effect parameters changing appropriate. hoping someone could tell suggest documentation updated describe undocumented keyword arguments.",0,insufficient documentation .hybridize,"insufficient documentation .hybridize hi trying optimize models hybridize accidentally misspelled argument method. got quite interesting message using minimal example although find message quite useful, also found quite surprising since documentation https mxnet.incubator.apache.org versions master api python gluon gluon.html?highlight hybrid mxnet.gluon.block.hybridize mentions . sure effect parameters changing appropriate. hoping someone could tell suggest documentation updated describe undocumented keyword arguments."
incubator-mxnet,14755,"description build issue happened mxnet github repo linux using command make j use opencv 1 use mkldnn 1 use blas mkl test build okay test removed. taolv pengzhao intel environment info required gcc 4.8.5 centos 7.5 package used python r scala julia python build info required built source compiler gcc clang mingw visual studio gcc4.8.5 mxnet commit hash paste output here. build config paste content config.mk, build command. error message paste complete error message, including stack trace. tried solve it? zachgk build issue likely introduced commit https github.com apache incubator mxnet commit 391a1be260eb75b437ebced6743647b8e9df7802 commit https github.com apache incubator mxnet commit dc48cd2a5a6460171bf9b842453866e731e6ff7d seems changing head 3rdpary googletest eb9225c, switch 3rdpary googletest previous head ec44c6c , make j use opencv 1 use mkldnn 1 use blas mkl test succeed.",0,build linux fail test option used,"build linux fail test option used description build issue happened mxnet github repo linux using command make j use opencv 1 use mkldnn 1 use blas mkl test build okay test removed. taolv pengzhao intel environment info required gcc 4.8.5 centos 7.5 package used python r scala julia python build info required built source compiler gcc clang mingw visual studio gcc4.8.5 mxnet commit hash paste output here. build config paste content config.mk, build command. error message paste complete error message, including stack trace. tried solve it? zachgk build issue likely introduced commit https github.com apache incubator mxnet commit 391a1be260eb75b437ebced6743647b8e9df7802 commit https github.com apache incubator mxnet commit dc48cd2a5a6460171bf9b842453866e731e6ff7d seems changing head 3rdpary googletest eb9225c, switch 3rdpary googletest previous head ec44c6c , make j use opencv 1 use mkldnn 1 use blas mkl test succeed."
incubator-mxnet,1712,"hi, wondering metrics evaluated end training epoch validation afterwards. mean metric value per batch? so, least validation robust evaluate metric based prediction values entire eval dataset? one example think validation dataset sparse labels. batches, certain labels may appear, breaking specific metric functions. thanks!",0,train validation metric evaluation,"train validation metric evaluation hi, wondering metrics evaluated end training epoch validation afterwards. mean metric value per batch? so, least validation robust evaluate metric based prediction values entire eval dataset? one example think validation dataset sparse labels. batches, certain labels may appear, breaking specific metric functions. thanks!"
incubator-mxnet,14955,"gluon function calculate total params network tool calculate network flops g . need this? model flops could straightly measure network inference speed. research needed. ops widely used many networks like conv, pooling, fc, bn , relu, softmax. ops looping many times different shape feature maps outpus, hard calculate manually. hard calculating ops actually necessary discussions https discuss.gluon.ai topic 10228 8 . partition demand. 1. widely used many networks commonly used. urgent conv2d 3d maxpool2d 3d avgpool 2d 3d globalavgpool2d 3d fc relu, leakyrelu, prelu, tanh, sigmoid bn softmax rnn basic rnn matrix multiplication, complicated know, may need add anything misses. 2. used somewhere may common. urgent dropout conv1d, maxpool1d, avgpool1d globalavgpool1d convtranspose1d 2d 3d gan may use this, gan care flops upsampling bilinear nearest instancenorm, layernorm l2normalization 3. may use little times model, flops may based implement. hard calculate manually. may implement. roipooling atomic level operation see anyone calculate them. 4. need implement loss functions. additional interface added make users manually defined blocks. welcome add anything missing. welcome suggest anything wrong.",0,feature request calculate network calculations tools gluon.,"feature request calculate network calculations tools gluon. gluon function calculate total params network tool calculate network flops g . need this? model flops could straightly measure network inference speed. research needed. ops widely used many networks like conv, pooling, fc, bn , relu, softmax. ops looping many times different shape feature maps outpus, hard calculate manually. hard calculating ops actually necessary discussions https discuss.gluon.ai topic 10228 8 . partition demand. 1. widely used many networks commonly used. urgent conv2d 3d maxpool2d 3d avgpool 2d 3d globalavgpool2d 3d fc relu, leakyrelu, prelu, tanh, sigmoid bn softmax rnn basic rnn matrix multiplication, complicated know, may need add anything misses. 2. used somewhere may common. urgent dropout conv1d, maxpool1d, avgpool1d globalavgpool1d convtranspose1d 2d 3d gan may use this, gan care flops upsampling bilinear nearest instancenorm, layernorm l2normalization 3. may use little times model, flops may based implement. hard calculate manually. may implement. roipooling atomic level operation see anyone calculate them. 4. need implement loss functions. additional interface added make users manually defined blocks. welcome add anything missing. welcome suggest anything wrong."
incubator-mxnet,9342,"issue . include dmlc . optional.h 67 29 error 'class dmlc optional' member named 'val' std swap val, other.val modify?",0,problem process cross compiling,"problem process cross compiling issue . include dmlc . optional.h 67 29 error 'class dmlc optional' member named 'val' std swap val, other.val modify?"
incubator-mxnet,945,"hi, want perform multi label image classification dataset mxnet. questions bother me. prepare recordio database index file? according extension mutliple labels single image https github.com dmlc mxnet blob a6b4baf9824bb3f0bfb8ec804d333913b3bbc0c8 doc python io.md , index file fixed width label field. labels always width common occasion. labels single image treated vector like 1,0,0,...,1,...0,...,1 ? prepare binary vector label image data? loss function assign training phase? deal multi label? implement like loss function? evaluation criteria choose test val phase? accuracy metric single label. thanks!",0,questions multi label image classification mxnet.,"questions multi label image classification mxnet. hi, want perform multi label image classification dataset mxnet. questions bother me. prepare recordio database index file? according extension mutliple labels single image https github.com dmlc mxnet blob a6b4baf9824bb3f0bfb8ec804d333913b3bbc0c8 doc python io.md , index file fixed width label field. labels always width common occasion. labels single image treated vector like 1,0,0,...,1,...0,...,1 ? prepare binary vector label image data? loss function assign training phase? deal multi label? implement like loss function? evaluation criteria choose test val phase? accuracy metric single label. thanks!"
incubator-mxnet,16424,"description trained nas searched shufflenet related model https github.com canyonwind mxnet single path one shot nas , contains rare operators like channel shuffle, hard swish, hard sigmoid, etc.. runs fine gpu raw cpu backend failed val acc 0.0 mkl cpu backend. environment info required background shufflenet related model built layers ops common ops conv, bn, activation 'relu' concat concat shuffle channel slice reshape swapaxes reshape slice hard swish plusscalar clip divscalar mul hard sigmoid plusscalar clip divscalar global average pooling pool shuffle channel implemented ! alt text https github.com canyonwind mxnet single path one shot nas blob master images channel shuffle split.png?raw true error message model planned quantized using mxnet 1.6.0 master quantization tool https github.com apache incubator mxnet tree master example quantization . error occurs trying use mkl backend run raw model quantization well quantized model. raw model using imagenet inference.py https github.com canyonwind mxnet single path one shot nas blob master quantization imagenet inference.py mxnet cpu mkl , works fine raw model using model code https github.com canyonwind mxnet single path one shot nas blob master quantization imagenet inference.py mxnet mkl, failed quantized model interestingly, mxnet mkl, quantization process works smoothly generates quantized model. using imagenet inference.py https github.com canyonwind mxnet single path one shot nas blob master quantization imagenet inference.py verify quantized model's performance, failed like raw model quantization. steps reproduce 1. clone code model put 2. reproduce mxnet cpu without mkl 3. reproduce mxnet mkl failed validation accuracy",0,channel shuffle hard swish hard sigmoid running mkl cpu backend failed,"channel shuffle hard swish hard sigmoid running mkl cpu backend failed description trained nas searched shufflenet related model https github.com canyonwind mxnet single path one shot nas , contains rare operators like channel shuffle, hard swish, hard sigmoid, etc.. runs fine gpu raw cpu backend failed val acc 0.0 mkl cpu backend. environment info required background shufflenet related model built layers ops common ops conv, bn, activation 'relu' concat concat shuffle channel slice reshape swapaxes reshape slice hard swish plusscalar clip divscalar mul hard sigmoid plusscalar clip divscalar global average pooling pool shuffle channel implemented ! alt text https github.com canyonwind mxnet single path one shot nas blob master images channel shuffle split.png?raw true error message model planned quantized using mxnet 1.6.0 master quantization tool https github.com apache incubator mxnet tree master example quantization . error occurs trying use mkl backend run raw model quantization well quantized model. raw model using imagenet inference.py https github.com canyonwind mxnet single path one shot nas blob master quantization imagenet inference.py mxnet cpu mkl , works fine raw model using model code https github.com canyonwind mxnet single path one shot nas blob master quantization imagenet inference.py mxnet mkl, failed quantized model interestingly, mxnet mkl, quantization process works smoothly generates quantized model. using imagenet inference.py https github.com canyonwind mxnet single path one shot nas blob master quantization imagenet inference.py verify quantized model's performance, failed like raw model quantization. steps reproduce 1. clone code model put 2. reproduce mxnet cpu without mkl 3. reproduce mxnet mkl failed validation accuracy"
incubator-mxnet,13592,consider following using mxnet release version 1.3.1 osx returns array. however throws exception may related issue 8048.,0,transpose mxnet backward mirror throws exception,transpose mxnet backward mirror throws exception consider following using mxnet release version 1.3.1 osx returns array. however throws exception may related issue 8048.
incubator-mxnet,13438,"description getenv calls libc threadsafe according https rachelbythebay.com w 2017 01 30 env https github.com xianyi openblas issues 716 indirect calls dmlc getenv across mxnet codebase, https github.com apache incubator mxnet blob 266de6bef4da5769431557288d41fab2a02e52ca src engine threaded engine perdevice.cc l79 https github.com apache incubator mxnet blob 5a83b6b563211f430688e41eab4752c6de4ecf22 src executor graph executor.cc l1194 error message lib64 libc.so.6 0x35250 0x7fdc5b99b250 lib64 libc.so.6 getenv 0xad 0x7fdc5b99e0cd opt amazon lib libmxnet.so zn4dmlc6getenvibeet pkcs1 0x1b 0x7fdc4a1bedab opt amazon lib libmxnet.so zn5mxnet4exec13graphexecutor10initopsegsev 0x1cb 0x7fdc4a1b6e0b opt amazon lib libmxnet.so zn5mxnet4exec13graphexecutor15finishinitgraphen4nnvm6symbolens2 5graphepns 8executorerkst13unordered mapins2 9nodeentryens 7ndarrayens2 13nodeentryhashens2 14nodeentryequalesaist4pairiks8 s9 eee 0x71b 0x7fdc4a1b760b opt amazon lib libmxnet.so zn5mxnet4exec13graphexecutor4initen4nnvm6symbolerkns 7contexterkst3mapisss4 st4lessissesaist4pairiksss4 eeerkst6vectoris4 sais4 eesl sl rkst13unordered mapissns2 6tshapeest4hashissest8equal toissesaisa isb sn eeerksm issisp sr saisa isb ieees11 rksh ins 9opreqtype esais12 eerkst13unordered setisssp sr saisseepsh ins 7ndarrayesais1c ees1f s1f psm isss1c sp sr saisa isb s1c eeepns 8executorerksm ins2 9nodeentryes1c ns2 13nodeentryhashens2 14nodeentryequalesaisa iks1m s1c eee 0x75d 0x7fdc4a1b958d opt amazon lib libmxnet.so zn5mxnet8executor10simplebinden4nnvm6symbolerkns 7contexterkst3mapisss3 st4lessissesaist4pairiksss3 eeerkst6vectoris3 sais3 eesk sk rkst13unordered mapissns1 6tshapeest4hashissest8equal toissesais9 isa sm eeerksl issiso sq sais9 isa ieees10 rksg ins 9opreqtypeesai s11 eerkst13unordered setissso sq saisseepsg ins 7ndarrayesais1b ees1e s1e psl isss1b sq sais9 isa s1b eeeps0 0x1a6 0x7fdc4a1b9f46 opt amazon lib libmxnet.so mxexecutorsimplebind 0x1e38 0x7fdc4a1031a8 opt amazon python2.7 lib python2.7 lib dynload ctypes.so ffi call unix64 0x4c 0x7fdc5af9b858 minimum reproducible example todo steps reproduce todo",0,libc getenv threadsafe,"libc getenv threadsafe description getenv calls libc threadsafe according https rachelbythebay.com w 2017 01 30 env https github.com xianyi openblas issues 716 indirect calls dmlc getenv across mxnet codebase, https github.com apache incubator mxnet blob 266de6bef4da5769431557288d41fab2a02e52ca src engine threaded engine perdevice.cc l79 https github.com apache incubator mxnet blob 5a83b6b563211f430688e41eab4752c6de4ecf22 src executor graph executor.cc l1194 error message lib64 libc.so.6 0x35250 0x7fdc5b99b250 lib64 libc.so.6 getenv 0xad 0x7fdc5b99e0cd opt amazon lib libmxnet.so zn4dmlc6getenvibeet pkcs1 0x1b 0x7fdc4a1bedab opt amazon lib libmxnet.so zn5mxnet4exec13graphexecutor10initopsegsev 0x1cb 0x7fdc4a1b6e0b opt amazon lib libmxnet.so zn5mxnet4exec13graphexecutor15finishinitgraphen4nnvm6symbolens2 5graphepns 8executorerkst13unordered mapins2 9nodeentryens 7ndarrayens2 13nodeentryhashens2 14nodeentryequalesaist4pairiks8 s9 eee 0x71b 0x7fdc4a1b760b opt amazon lib libmxnet.so zn5mxnet4exec13graphexecutor4initen4nnvm6symbolerkns 7contexterkst3mapisss4 st4lessissesaist4pairiksss4 eeerkst6vectoris4 sais4 eesl sl rkst13unordered mapissns2 6tshapeest4hashissest8equal toissesaisa isb sn eeerksm issisp sr saisa isb ieees11 rksh ins 9opreqtype esais12 eerkst13unordered setisssp sr saisseepsh ins 7ndarrayesais1c ees1f s1f psm isss1c sp sr saisa isb s1c eeepns 8executorerksm ins2 9nodeentryes1c ns2 13nodeentryhashens2 14nodeentryequalesaisa iks1m s1c eee 0x75d 0x7fdc4a1b958d opt amazon lib libmxnet.so zn5mxnet8executor10simplebinden4nnvm6symbolerkns 7contexterkst3mapisss3 st4lessissesaist4pairiksss3 eeerkst6vectoris3 sais3 eesk sk rkst13unordered mapissns1 6tshapeest4hashissest8equal toissesais9 isa sm eeerksl issiso sq sais9 isa ieees10 rksg ins 9opreqtypeesai s11 eerkst13unordered setissso sq saisseepsg ins 7ndarrayesais1b ees1e s1e psl isss1b sq sais9 isa s1b eeeps0 0x1a6 0x7fdc4a1b9f46 opt amazon lib libmxnet.so mxexecutorsimplebind 0x1e38 0x7fdc4a1031a8 opt amazon python2.7 lib python2.7 lib dynload ctypes.so ffi call unix64 0x4c 0x7fdc5af9b858 minimum reproducible example todo steps reproduce todo"
incubator-mxnet,3836,"c winpython python 2.7.10.amd64 python.exe mycoding deeplearning test example mxnet example rcnn train end2end.py root path h data faster rcnn devkit path h data faster rcnn vocdevkit frequent 50 kv store local prefix h data faster rcnn model faster rcnn pretrained h data faster rcnn model vgg16 load epoch 1 info root namespace devkit path 'h data faster rcnn vocdevkit', factor step 50000, frequent 50, gpu ids '1,0', image set 'trainval', kv store 'local', load epoch 1, lr 0.001, mom 0.9, monitor false, flip false, num classes 21, num epoch 10, prefix 'h data faster rcnn model faster rcnn', pretrained 'h data faster rcnn model vgg16', resume false, root path 'h data faster rcnn', test image set 'test', wd 0.0005, work load list none, year '2007' info root train faster rcnn approximate joint end2end 'providing maximum shape', 'data', 2, 3, 1000, 1000 , 'label', 1l, 34596l , 'bbox target', 1l, 36l, 62l, 62l , 'bbox inside weight', 1l, 36l, 62l, 62l , 'bbox outside weight', 1l, 36l, 62l, 62l , 'gt boxes', 2, 500 voc 2007 trainval gt roidb loaded h data faster rcnn cache voc 2007 trainval gt roidb.pkl append flipped images roidb prepare roidb traceback recent call last file mycoding deeplearning test example mxnet example rcnn train end2end.py , line 178, args.work load list, args.resume, args.no flip, args.factor step file mycoding deeplearning test example mxnet example rcnn train end2end.py , line 125, end2end train arg params args, aux params auxs, begin epoch begin epoch, num epoch num epoch file c winpython python 2.7.10.amd64 lib site packages mxnet 0.7.0 py2.7.egg mxnet module base module.py , line 338, fit training true, force rebind force rebind file mycoding deeplearning test example mxnet example rcnn rcnn module.py , line 137, bind force rebind false, shared module none file c winpython python 2.7.10.amd64 lib site packages mxnet 0.7.0 py2.7.egg mxnet module module.py , line 282, bind grad req grad req, input types input types file c winpython python 2.7.10.amd64 lib site packages mxnet 0.7.0 py2.7.egg mxnet module executor group.py , line 170, init self.label layouts self.decide slices label shapes file c winpython python 2.7.10.amd64 lib site packages mxnet 0.7.0 py2.7.egg mxnet module executor group.py , line 196, decide slices shape name, shape assertionerror data must batch size batch size 2, label shape 1l, 34596l process finished exit code 1 training end2end",0,"train faster rcnn use two gpus, error occured like","train faster rcnn use two gpus, error occured like c winpython python 2.7.10.amd64 python.exe mycoding deeplearning test example mxnet example rcnn train end2end.py root path h data faster rcnn devkit path h data faster rcnn vocdevkit frequent 50 kv store local prefix h data faster rcnn model faster rcnn pretrained h data faster rcnn model vgg16 load epoch 1 info root namespace devkit path 'h data faster rcnn vocdevkit', factor step 50000, frequent 50, gpu ids '1,0', image set 'trainval', kv store 'local', load epoch 1, lr 0.001, mom 0.9, monitor false, flip false, num classes 21, num epoch 10, prefix 'h data faster rcnn model faster rcnn', pretrained 'h data faster rcnn model vgg16', resume false, root path 'h data faster rcnn', test image set 'test', wd 0.0005, work load list none, year '2007' info root train faster rcnn approximate joint end2end 'providing maximum shape', 'data', 2, 3, 1000, 1000 , 'label', 1l, 34596l , 'bbox target', 1l, 36l, 62l, 62l , 'bbox inside weight', 1l, 36l, 62l, 62l , 'bbox outside weight', 1l, 36l, 62l, 62l , 'gt boxes', 2, 500 voc 2007 trainval gt roidb loaded h data faster rcnn cache voc 2007 trainval gt roidb.pkl append flipped images roidb prepare roidb traceback recent call last file mycoding deeplearning test example mxnet example rcnn train end2end.py , line 178, args.work load list, args.resume, args.no flip, args.factor step file mycoding deeplearning test example mxnet example rcnn train end2end.py , line 125, end2end train arg params args, aux params auxs, begin epoch begin epoch, num epoch num epoch file c winpython python 2.7.10.amd64 lib site packages mxnet 0.7.0 py2.7.egg mxnet module base module.py , line 338, fit training true, force rebind force rebind file mycoding deeplearning test example mxnet example rcnn rcnn module.py , line 137, bind force rebind false, shared module none file c winpython python 2.7.10.amd64 lib site packages mxnet 0.7.0 py2.7.egg mxnet module module.py , line 282, bind grad req grad req, input types input types file c winpython python 2.7.10.amd64 lib site packages mxnet 0.7.0 py2.7.egg mxnet module executor group.py , line 170, init self.label layouts self.decide slices label shapes file c winpython python 2.7.10.amd64 lib site packages mxnet 0.7.0 py2.7.egg mxnet module executor group.py , line 196, decide slices shape name, shape assertionerror data must batch size batch size 2, label shape 1l, 34596l process finished exit code 1 training end2end"
incubator-mxnet,9520,"note providing complete information concise form best way get help. issue template serves checklist essential information technical issues bug reports. non technical issues feature requests, feel free present information believe best form. q discussion, please start discussion thread https discuss.mxnet.io description image augumention crash set mxnet cpu worker nthreads bigger 3 environment info required python info 'version ', '2.7.6' 'compiler ', 'gcc 4.8.4' 'build ', 'default', 'oct 26 2016 20 30 19' 'arch ', '64bit', 'elf' pip info 'version ', '9.0.1' 'directory ', ' usr local lib python2.7 dist packages pip' mxnet info 'version ', '0.12.1' 'directory ', ' usr local lib python2.7 dist packages mxnet' 'commit hash ', 'e0c7906693f0c79b0ce34a4d777c26a6bf1903c1' system info 'platform ', 'linux 4.4.0 64 generic x86 64 ubuntu 14.04 trusty' 'system ', 'linux' 'node ', 'meter' 'release ', '4.4.0 64 generic' 'version ', ' 85 14.04.1 ubuntu smp mon feb 20 12 10 54 utc 2017' hardware info 'machine ', 'x86 64' 'processor ', 'x86 64' architecture x86 64 cpu op mode 32 bit, 64 bit byte order little endian cpu 8 line cpu list 0 7 thread per core 2 core per socket 4 socket 1 numa node 1 vendor id genuineintel cpu family 6 model 94 stepping 3 cpu mhz 4200.000 bogomips 8016.71 virtualization vt x l1d cache 32k l1i cache 32k l2 cache 256k l3 cache 8192k numa node0 cpu 0 7 network test setting timeout 10 timing mxnet https github.com apache incubator mxnet, dns 0.0072 sec, load 1.6354 sec. timing pypi https pypi.python.org pypi pip, dns 0.0060 sec, load 0.5680 sec. timing fashionmnist https apache mxnet.s3 accelerate.dualstack.amazonaws.com gluon dataset fashion mnist train labels idx1 ubyte.gz, dns 0.1315 sec, load 0.9401 sec. timing conda https repo.continuum.io pkgs free , dns 0.1766 sec, load 1.0127 sec. timing gluon tutorial en http gluon.mxnet.io, dns 0.0067 sec, load 0.3688 sec. error open gluon tutorial cn https zh.gluon.ai, , dns finished 0.277799129486 sec. package used python r scala julia python error message blas program terminated. tried allocate many memory regions. minimum reproducible example",0,image augumention crash set mxnet cpu worker nthreads bigger 3,"image augumention crash set mxnet cpu worker nthreads bigger 3 note providing complete information concise form best way get help. issue template serves checklist essential information technical issues bug reports. non technical issues feature requests, feel free present information believe best form. q discussion, please start discussion thread https discuss.mxnet.io description image augumention crash set mxnet cpu worker nthreads bigger 3 environment info required python info 'version ', '2.7.6' 'compiler ', 'gcc 4.8.4' 'build ', 'default', 'oct 26 2016 20 30 19' 'arch ', '64bit', 'elf' pip info 'version ', '9.0.1' 'directory ', ' usr local lib python2.7 dist packages pip' mxnet info 'version ', '0.12.1' 'directory ', ' usr local lib python2.7 dist packages mxnet' 'commit hash ', 'e0c7906693f0c79b0ce34a4d777c26a6bf1903c1' system info 'platform ', 'linux 4.4.0 64 generic x86 64 ubuntu 14.04 trusty' 'system ', 'linux' 'node ', 'meter' 'release ', '4.4.0 64 generic' 'version ', ' 85 14.04.1 ubuntu smp mon feb 20 12 10 54 utc 2017' hardware info 'machine ', 'x86 64' 'processor ', 'x86 64' architecture x86 64 cpu op mode 32 bit, 64 bit byte order little endian cpu 8 line cpu list 0 7 thread per core 2 core per socket 4 socket 1 numa node 1 vendor id genuineintel cpu family 6 model 94 stepping 3 cpu mhz 4200.000 bogomips 8016.71 virtualization vt x l1d cache 32k l1i cache 32k l2 cache 256k l3 cache 8192k numa node0 cpu 0 7 network test setting timeout 10 timing mxnet https github.com apache incubator mxnet, dns 0.0072 sec, load 1.6354 sec. timing pypi https pypi.python.org pypi pip, dns 0.0060 sec, load 0.5680 sec. timing fashionmnist https apache mxnet.s3 accelerate.dualstack.amazonaws.com gluon dataset fashion mnist train labels idx1 ubyte.gz, dns 0.1315 sec, load 0.9401 sec. timing conda https repo.continuum.io pkgs free , dns 0.1766 sec, load 1.0127 sec. timing gluon tutorial en http gluon.mxnet.io, dns 0.0067 sec, load 0.3688 sec. error open gluon tutorial cn https zh.gluon.ai, , dns finished 0.277799129486 sec. package used python r scala julia python error message blas program terminated. tried allocate many memory regions. minimum reproducible example"
incubator-mxnet,13303,"description mxnet cpp package cross compilation fails oserror wrong elf class elfclass32 environment info required host system ubuntu 18.04.1 lts 4.15.0 38 generic x86 64 target system arm cortex a9 cpu build info required built source compiler gcc clang mingw visual studio arm buildroot linux gnueabihf gcc 7.3.0 mxnet commit hash 85fefa8c7291b556057ae685a5883c3f6a175c18 build config dbuild cpp examples duse mkldnn dthreads pthread arg duse opencv duse openmp duse cuda duse cudnn duse sse duse cpp package duse libjpeg turbo denable cuda rtc error message mxnet 1.3.0 building .... 86 building c object cmakefiles mxnet.dir dummy.c.o 93 built target mxnet unit tests 93 linking cxx shared library libmxnet.so 93 built target mxnet scanning dependencies target cpp package op h running opwrappergenerator.py traceback recent call last file opwrappergenerator.py , line 428, raise e oserror buildroot output build mxnet 1.3.0 libmxnet.so wrong elf class elfclass32 cpp package cmakefiles cpp package op h.dir build.make 58 recipe target 'cpp package cmakefiles cpp package op h' failed make 4 cpp package cmakefiles cpp package op h error 1 cmakefiles makefile2 541 recipe target 'cpp package cmakefiles cpp package op h.dir all' failed make 3 cpp package cmakefiles cpp package op h.dir error 2 minimum reproducible example steps reproduce 1. configure buildroot target system 2. add mxnet cmake buildroot package mxnet version 1.3.0 mxnet site https github.com apache incubator mxnet.git mxnet site method git mxnet git submodules yes mxnet install staging yes mxnet conf opts dbuild cpp examples duse mkldnn dthreads pthread arg duse opencv duse openmp duse cuda duse cudnn duse sse duse cpp package duse libjpeg turbo denable cuda rtc eval cmake package 3. compile package tried solve it? 1. checked opwrappergenerator.py source code. tries invoke python host system x86 64 perform cdll.libmxnet cdll.loadlibrary sys.argv 1 library built target . eventually host python know nothing arm library format.",0,mxnet cpp package cross compilation fails oserror wrong elf class elfclass32,"mxnet cpp package cross compilation fails oserror wrong elf class elfclass32 description mxnet cpp package cross compilation fails oserror wrong elf class elfclass32 environment info required host system ubuntu 18.04.1 lts 4.15.0 38 generic x86 64 target system arm cortex a9 cpu build info required built source compiler gcc clang mingw visual studio arm buildroot linux gnueabihf gcc 7.3.0 mxnet commit hash 85fefa8c7291b556057ae685a5883c3f6a175c18 build config dbuild cpp examples duse mkldnn dthreads pthread arg duse opencv duse openmp duse cuda duse cudnn duse sse duse cpp package duse libjpeg turbo denable cuda rtc error message mxnet 1.3.0 building .... 86 building c object cmakefiles mxnet.dir dummy.c.o 93 built target mxnet unit tests 93 linking cxx shared library libmxnet.so 93 built target mxnet scanning dependencies target cpp package op h running opwrappergenerator.py traceback recent call last file opwrappergenerator.py , line 428, raise e oserror buildroot output build mxnet 1.3.0 libmxnet.so wrong elf class elfclass32 cpp package cmakefiles cpp package op h.dir build.make 58 recipe target 'cpp package cmakefiles cpp package op h' failed make 4 cpp package cmakefiles cpp package op h error 1 cmakefiles makefile2 541 recipe target 'cpp package cmakefiles cpp package op h.dir all' failed make 3 cpp package cmakefiles cpp package op h.dir error 2 minimum reproducible example steps reproduce 1. configure buildroot target system 2. add mxnet cmake buildroot package mxnet version 1.3.0 mxnet site https github.com apache incubator mxnet.git mxnet site method git mxnet git submodules yes mxnet install staging yes mxnet conf opts dbuild cpp examples duse mkldnn dthreads pthread arg duse opencv duse openmp duse cuda duse cudnn duse sse duse cpp package duse libjpeg turbo denable cuda rtc eval cmake package 3. compile package tried solve it? 1. checked opwrappergenerator.py source code. tries invoke python host system x86 64 perform cdll.libmxnet cdll.loadlibrary sys.argv 1 library built target . eventually host python know nothing arm library format."
incubator-mxnet,6759,set mxnet cpu worker nthreads r,0,set mxnet cpu worker nthreads r,set mxnet cpu worker nthreads r set mxnet cpu worker nthreads r
incubator-mxnet,15138,"current tutorial developing operators mxnet, https mxnet.incubator.apache.org versions master faq add op backend.html talk fstatefulcompute fstatefulcomputeex, useful many cudnn ops. add section two interfaces, states kept persistent cached op. zheng da",0,tutorial developing advanced operators,"tutorial developing advanced operators current tutorial developing operators mxnet, https mxnet.incubator.apache.org versions master faq add op backend.html talk fstatefulcompute fstatefulcomputeex, useful many cudnn ops. add section two interfaces, states kept persistent cached op. zheng da"
incubator-mxnet,15919,"https github.com apache incubator mxnet blob master python mxnet gluon trainer.py l265 l275 update kvstore true, trainer.set learning rate modify learning rate remote parameter server",0,trainer.set learning rate broken distributed kvstore used,"trainer.set learning rate broken distributed kvstore used https github.com apache incubator mxnet blob master python mxnet gluon trainer.py l265 l275 update kvstore true, trainer.set learning rate modify learning rate remote parameter server"
incubator-mxnet,16508,"https github.com apache incubator mxnet tree master example distributed training horovod examples follow mpirun np 8 h server1 4,server2 4 bind none map slot x nccl debug info mca pml ob1 mca btl openib python train.py train.py means one gluon mnist.pymodule mnist.pyresnet50 imagenet.py ? use gluon mnist.py instead train.py?",0,distributed training using mxnet horovod,"distributed training using mxnet horovod https github.com apache incubator mxnet tree master example distributed training horovod examples follow mpirun np 8 h server1 4,server2 4 bind none map slot x nccl debug info mca pml ob1 mca btl openib python train.py train.py means one gluon mnist.pymodule mnist.pyresnet50 imagenet.py ? use gluon mnist.py instead train.py?"
incubator-mxnet,13439,build failure error http jenkins.mxnet ci.amazon ml.com blue organizations jenkins incubator mxnet detail master 2038 pipeline,0,test failure r cpu,test failure r cpu build failure error http jenkins.mxnet ci.amazon ml.com blue organizations jenkins incubator mxnet detail master 2038 pipeline
incubator-mxnet,14203,"os win10 64bits compiler vc2015 64 bits cuda cuda92 commit hash bada8a1961f0da7f01cd9e61d03f280c48083f1b bug 1 openmp required int loop line 515, line 580, line 607 mxnet op.h change line 729 utils.h change line 315, 203, 218, 252 broad cast reduce inl.h line 430 indexing op.cc bug 2 std min cannot find overload version line 340, line 463 convolution v1 inl.h change bug 3 c1002 link time code generation finding way fix it, link https social.msdn.microsoft.com forums sqlserver en us d2c4bb60 e558 4dc6 a0ba 47611d45bc86 c1002 compiler heap space pass 2?forum vcgeneral suggest link time code generation set profile guided optimization optimization ltcg pgoptimize instead blank. bug 4 uncheck build cpp examples cmake gui, cpp examples still include project files.",0,bug cannot compile mxnet windows,"bug cannot compile mxnet windows os win10 64bits compiler vc2015 64 bits cuda cuda92 commit hash bada8a1961f0da7f01cd9e61d03f280c48083f1b bug 1 openmp required int loop line 515, line 580, line 607 mxnet op.h change line 729 utils.h change line 315, 203, 218, 252 broad cast reduce inl.h line 430 indexing op.cc bug 2 std min cannot find overload version line 340, line 463 convolution v1 inl.h change bug 3 c1002 link time code generation finding way fix it, link https social.msdn.microsoft.com forums sqlserver en us d2c4bb60 e558 4dc6 a0ba 47611d45bc86 c1002 compiler heap space pass 2?forum vcgeneral suggest link time code generation set profile guided optimization optimization ltcg pgoptimize instead blank. bug 4 uncheck build cpp examples cmake gui, cpp examples still include project files."
incubator-mxnet,6874,"environment info operating system error raised ubuntu 14.04, windows 10 mac os x el capitan 10.11.6 compiler gcc visual studio 2013 clang omp package used python mxnet version 0.10.1 installed source python version distribution 2.7.6 installed apt get ubuntu. 2.7.12 python.org windows mac os. error message mac os minimum reproducible example code runs fine mxnet 0.9.5, 0.10.1 steps reproduce run python code above.",0,mx.symbol.softmax cross entropy raises error enough argument call operator softmax cross entropy,"mx.symbol.softmax cross entropy raises error enough argument call operator softmax cross entropy environment info operating system error raised ubuntu 14.04, windows 10 mac os x el capitan 10.11.6 compiler gcc visual studio 2013 clang omp package used python mxnet version 0.10.1 installed source python version distribution 2.7.6 installed apt get ubuntu. 2.7.12 python.org windows mac os. error message mac os minimum reproducible example code runs fine mxnet 0.9.5, 0.10.1 steps reproduce run python code above."
incubator-mxnet,16960,"description conversion fails large matrices. error message traceback recent call last file , line 1, file home ec2 user .local lib python3.6 site packages mxnet ndarray utils.py , line 146, array return array source array, ctx ctx, dtype dtype file home ec2 user .local lib python3.6 site packages mxnet ndarray ndarray.py , line 2505, array arr source array file home ec2 user .local lib python3.6 site packages mxnet ndarray ndarray.py , line 449, setitem self. set nd basic indexing key, value file home ec2 user .local lib python3.6 site packages mxnet ndarray ndarray.py , line 715, set nd basic indexing self. sync copyfrom value file home ec2 user .local lib python3.6 site packages mxnet ndarray ndarray.py , line 881, sync copyfrom ctypes.c size source array.size file home ec2 user .local lib python3.6 site packages mxnet base.py , line 253, check call raise mxneterror py str lib.mxgetlasterror mxnet.base.mxneterror 19 09 04 src ndarray ndarray function.cc 51 check failed size size 294967296 vs. 4000000000 copying size mismatch, 18446744072529682432 bytes, 16000000000 bytes. stack trace bt 0 home ec2 user .local lib python3.6 site packages mxnet libmxnet.so 0x2795cb 0x7efff4d2c5cb bt 1 home ec2 user .local lib python3.6 site packages mxnet libmxnet.so 0x259399b 0x7efff704699b bt 2 home ec2 user .local lib python3.6 site packages mxnet libmxnet.so mxnet ndarray synccopyfromcpu void const , unsigned long const 0x284 0x7efff6fe5934 bt 3 home ec2 user .local lib python3.6 site packages mxnet libmxnet.so mxndarraysynccopyfromcpu 0x2b 0x7efff6d8ebcb bt 4 usr lib64 libffi.so.6 ffi call unix64 0x4c 0x7f0059bbacec bt 5 usr lib64 libffi.so.6 ffi call 0x1f5 0x7f0059bba615 bt 6 usr lib64 python3.6 lib dynload ctypes.cpython 36m x86 64 linux gnu.so ctypes callproc 0x2a0 0x7f0059dcd290 bt 7 usr lib64 python3.6 lib dynload ctypes.cpython 36m x86 64 linux gnu.so 0x9586 0x7f0059dc6586 bt 8 usr lib64 libpython3.6m.so.1.0 pyobject fastcalldict 0x90 0x7f0061bce7e0 steps reproduce nd.array np.random.randn 5000000,800 paste commands ran produced error. tried solve it? 1. workaround convert smaller chunks concatenate ndarray create final mxnet",0,large numpy array mxnet ndarray conversion,"large numpy array mxnet ndarray conversion description conversion fails large matrices. error message traceback recent call last file , line 1, file home ec2 user .local lib python3.6 site packages mxnet ndarray utils.py , line 146, array return array source array, ctx ctx, dtype dtype file home ec2 user .local lib python3.6 site packages mxnet ndarray ndarray.py , line 2505, array arr source array file home ec2 user .local lib python3.6 site packages mxnet ndarray ndarray.py , line 449, setitem self. set nd basic indexing key, value file home ec2 user .local lib python3.6 site packages mxnet ndarray ndarray.py , line 715, set nd basic indexing self. sync copyfrom value file home ec2 user .local lib python3.6 site packages mxnet ndarray ndarray.py , line 881, sync copyfrom ctypes.c size source array.size file home ec2 user .local lib python3.6 site packages mxnet base.py , line 253, check call raise mxneterror py str lib.mxgetlasterror mxnet.base.mxneterror 19 09 04 src ndarray ndarray function.cc 51 check failed size size 294967296 vs. 4000000000 copying size mismatch, 18446744072529682432 bytes, 16000000000 bytes. stack trace bt 0 home ec2 user .local lib python3.6 site packages mxnet libmxnet.so 0x2795cb 0x7efff4d2c5cb bt 1 home ec2 user .local lib python3.6 site packages mxnet libmxnet.so 0x259399b 0x7efff704699b bt 2 home ec2 user .local lib python3.6 site packages mxnet libmxnet.so mxnet ndarray synccopyfromcpu void const , unsigned long const 0x284 0x7efff6fe5934 bt 3 home ec2 user .local lib python3.6 site packages mxnet libmxnet.so mxndarraysynccopyfromcpu 0x2b 0x7efff6d8ebcb bt 4 usr lib64 libffi.so.6 ffi call unix64 0x4c 0x7f0059bbacec bt 5 usr lib64 libffi.so.6 ffi call 0x1f5 0x7f0059bba615 bt 6 usr lib64 python3.6 lib dynload ctypes.cpython 36m x86 64 linux gnu.so ctypes callproc 0x2a0 0x7f0059dcd290 bt 7 usr lib64 python3.6 lib dynload ctypes.cpython 36m x86 64 linux gnu.so 0x9586 0x7f0059dc6586 bt 8 usr lib64 libpython3.6m.so.1.0 pyobject fastcalldict 0x90 0x7f0061bce7e0 steps reproduce nd.array np.random.randn 5000000,800 paste commands ran produced error. tried solve it? 1. workaround convert smaller chunks concatenate ndarray create final mxnet"
incubator-mxnet,13199,description brief description problem 2 sentences. environment info required package used python r scala julia using python error message minimum reproducible example steps reproduce paste commands ran produced error. 1. 2. tried solve it? forward starting thread.,0,namescope none hybridize multi threading environment. attributeerror 'nonetype' object attribute ' exit ',namescope none hybridize multi threading environment. attributeerror 'nonetype' object attribute ' exit ' description brief description problem 2 sentences. environment info required package used python r scala julia using python error message minimum reproducible example steps reproduce paste commands ran produced error. 1. 2. tried solve it? forward starting thread.
incubator-mxnet,9381,looking build status commit mxnet master noticed perl test failed weeks ago. already fixed? sergeykolychev http jenkins.mxnet ci.amazon ml.com blue organizations jenkins incubator mxnet detail master 74 pipeline,0,flaky? perl test failure,flaky? perl test failure looking build status commit mxnet master noticed perl test failed weeks ago. already fixed? sergeykolychev http jenkins.mxnet ci.amazon ml.com blue organizations jenkins incubator mxnet detail master 74 pipeline
incubator-mxnet,16214,"test sync batchnorm behaves differently different number gpu devices machine. fails p3.8xlarge num devices 1 2, test passes.",0,test sync batchnorm failure p3.8xlarge,"test sync batchnorm failure p3.8xlarge test sync batchnorm behaves differently different number gpu devices machine. fails p3.8xlarge num devices 1 2, test passes."
incubator-mxnet,14681,"currently, best ? way load huge dataset mxnet implement dataloader. however, need ability easily random access th example, becauce index need overwrite. huge data huge save disk usually stored remote side example hdfs several small datapart. datapart easily load memory contains maybe millions examples. random access difficult need ability random access different datapart datapart saved hdfs. think good dataloader situation 1. first random choose datapart 2. shuffle small datapart 3. train",0,suggestion different random dataloader,"suggestion different random dataloader currently, best ? way load huge dataset mxnet implement dataloader. however, need ability easily random access th example, becauce index need overwrite. huge data huge save disk usually stored remote side example hdfs several small datapart. datapart easily load memory contains maybe millions examples. random access difficult need ability random access different datapart datapart saved hdfs. think good dataloader situation 1. first random choose datapart 2. shuffle small datapart 3. train"
incubator-mxnet,15112,"pyinstaller bundles python application dependencies single package. support windows linux mac os. publish app rapidly platforms pyinstaller. however, mxnet cannot packaged dependency. think make mxnet compatible pyinstaller. p.s directly packaged pyinstaller. tested windows.",0,feature request make mxnet available published pyinstaller,"feature request make mxnet available published pyinstaller pyinstaller bundles python application dependencies single package. support windows linux mac os. publish app rapidly platforms pyinstaller. however, mxnet cannot packaged dependency. think make mxnet compatible pyinstaller. p.s directly packaged pyinstaller. tested windows."
incubator-mxnet,13838,brew updated opencv get opencv4 compatible current master. current install docs osx say use prereq impact new users trying mxnet. particularly problematic way brew install 3.4 version details https github.com homebrew homebrew core issues 35869 currently pr works address compatibility opencv 4.0 https github.com apache incubator mxnet pull 13559. issue raise awareness impacting project now.,0,osx brew install opencv breaks master,osx brew install opencv breaks master brew updated opencv get opencv4 compatible current master. current install docs osx say use prereq impact new users trying mxnet. particularly problematic way brew install 3.4 version details https github.com homebrew homebrew core issues 35869 currently pr works address compatibility opencv 4.0 https github.com apache incubator mxnet pull 13559. issue raise awareness impacting project now.
incubator-mxnet,14366,appeared https github.com apache incubator mxnet pull 14321 see logs http jenkins.mxnet ci.amazon ml.com blue organizations jenkins mxnet validation 2fcentos gpu detail pr 14321 6 pipeline http jenkins.mxnet ci.amazon ml.com blue organizations jenkins mxnet validation 2funix gpu detail pr 14321 5 pipeline,0,flaky test test random seed,flaky test test random seed appeared https github.com apache incubator mxnet pull 14321 see logs http jenkins.mxnet ci.amazon ml.com blue organizations jenkins mxnet validation 2fcentos gpu detail pr 14321 6 pipeline http jenkins.mxnet ci.amazon ml.com blue organizations jenkins mxnet validation 2funix gpu detail pr 14321 5 pipeline
incubator-mxnet,9436,"description output topk operator positional 0s 1s. would standard numpy ndarray dl framework, return values boolean true false . similarly, since mxnet ndarray wants closer behavior numpy ndarray operations, would good support boolean types output conditional operators. functionality also help apache mxnet backend keras2",0,support boolean outputs topk operator,"support boolean outputs topk operator description output topk operator positional 0s 1s. would standard numpy ndarray dl framework, return values boolean true false . similarly, since mxnet ndarray wants closer behavior numpy ndarray operations, would good support boolean types output conditional operators. functionality also help apache mxnet backend keras2"
incubator-mxnet,17033,"description clear concise description bug is. hello, using cython generate dynamic library files mxnet project. run project dymanic library files, problem attributeerror 'nonetype' object attribute 'mxndarrayfree' sometimes occurs, exactly problem always come up. strange problem comes code executed. cython version 0.29.14 python version 3.5.2 mxnet version 1.5.1.post0 error message paste complete error message, including stack trace. exception ignored traceback recent call last file usr local lib python3.5 dist packages mxnet ctypes ndarray.py , line 51, del attributeerror 'nonetype' object attribute 'mxndarrayfree' exception ignored traceback recent call last file usr local lib python3.5 dist packages mxnet ctypes ndarray.py , line 51, del attributeerror 'nonetype' object attribute 'mxndarrayfree' exception ignored traceback recent call last file usr local lib python3.5 dist packages mxnet ctypes ndarray.py , line 51, del attributeerror 'nonetype' object attribute 'mxndarrayfree' reproduce developed code, please provide short script reproduces error. existing examples, please provide link. steps reproduce paste commands ran produced error. 1. 2. tried solve it? 1. 2. environment recommend using script collecting diagnositc information. run following command paste outputs",0,attributeerror 'nonetype' object attribute 'mxndarrayfree',"attributeerror 'nonetype' object attribute 'mxndarrayfree' description clear concise description bug is. hello, using cython generate dynamic library files mxnet project. run project dymanic library files, problem attributeerror 'nonetype' object attribute 'mxndarrayfree' sometimes occurs, exactly problem always come up. strange problem comes code executed. cython version 0.29.14 python version 3.5.2 mxnet version 1.5.1.post0 error message paste complete error message, including stack trace. exception ignored traceback recent call last file usr local lib python3.5 dist packages mxnet ctypes ndarray.py , line 51, del attributeerror 'nonetype' object attribute 'mxndarrayfree' exception ignored traceback recent call last file usr local lib python3.5 dist packages mxnet ctypes ndarray.py , line 51, del attributeerror 'nonetype' object attribute 'mxndarrayfree' exception ignored traceback recent call last file usr local lib python3.5 dist packages mxnet ctypes ndarray.py , line 51, del attributeerror 'nonetype' object attribute 'mxndarrayfree' reproduce developed code, please provide short script reproduces error. existing examples, please provide link. steps reproduce paste commands ran produced error. 1. 2. tried solve it? 1. 2. environment recommend using script collecting diagnositc information. run following command paste outputs"
incubator-mxnet,12062,"description dmlc ps root uri ip hostname , use hostname instead ip , reports bind failed src van.cc environment info required package used python r scala julia python build info required built source gcc mxnet commit hash 3df9bf802021d5aa67c609c6736acee94aaf3a48 build config doc https mxnet.apache.org install index.html?platform linux language python processor cpu error message paste complete error message, including stack trace. 17 46 11 home tusimple incubator mxnet dmlc core include dmlc . logging.h 308 17 46 11 src van.cc 76 check failed node .port ! 1 bind failed stack trace returned 10 entries bt 0 home tusimple incubator mxnet example image classification mxnet libmxnet.so zn4dmlc15logmessagefatald1ev 0x3c 0x7f1a283f624c bt 1 home tusimple incubator mxnet example image classification mxnet libmxnet.so zn2ps3van5startev 0x91f 0x7f1a2af45b8f bt 2 home tusimple incubator mxnet example image classification mxnet libmxnet.so zn2ps6zmqvan5startev 0x4a 0x7f1a2af504fa bt 3 home tusimple incubator mxnet example image classification mxnet libmxnet.so zn2ps10postoffice5startepkcb 0x1e9 0x7f1a2af42119 bt 4 home tusimple incubator mxnet example image classification mxnet libmxnet.so zn5mxnet7kvstore11kvstoredist9runservererkst8functionifvirknst7 cxx1112basic stringicst11char traitsicesaiceeeee 0x1c5 0x7f1a2aee1c35 bt 5 home tusimple incubator mxnet example image classification mxnet libmxnet.so mxkvstorerunserver 0x4b 0x7f1a2ae629db bt 6 usr lib x86 64 linux gnu libffi.so.6 ffi call unix64 0x4c 0x7f1a41450e40 bt 7 usr lib x86 64 linux gnu libffi.so.6 ffi call 0x2eb 0x7f1a414508ab bt 8 usr lib python2.7 lib dynload ctypes.x86 64 linux gnu.so ctypes callproc 0x48f 0x7f1a416603df bt 9 usr lib python2.7 lib dynload ctypes.x86 64 linux gnu.so 0x11d82 0x7f1a41664d82 traceback recent call last file train mnist.py , line 25, common import find mxnet, fit file home tusimple incubator mxnet example image classification common find mxnet.py , line 20, import mxnet mx file home tusimple incubator mxnet example image classification mxnet init .py , line 56, . import kvstore server file home tusimple incubator mxnet example image classification mxnet kvstore server.py , line 85, init kvstore server module file home tusimple incubator mxnet example image classification mxnet kvstore server.py , line 82, init kvstore server module server.run file home tusimple incubator mxnet example image classification mxnet kvstore server.py , line 73, run check call lib.mxkvstorerunserver self.handle, ctrl proto self. controller , none file home tusimple incubator mxnet example image classification mxnet base.py , line 146, check call raise mxneterror py str lib.mxgetlasterror mxnet.base.mxneterror 17 46 11 src van.cc 76 check failed node .port ! 1 bind failed stack trace returned 10 entries bt 0 home tusimple incubator mxnet example image classification mxnet libmxnet.so zn4dmlc15logmessagefatald1ev 0x3c 0x7f1a283f624c bt 1 home tusimple incubator mxnet example image classification mxnet libmxnet.so zn2ps3van5startev 0x91f 0x7f1a2af45b8f bt 2 home tusimple incubator mxnet example image classification mxnet libmxnet.so zn2ps6zmqvan5startev 0x4a 0x7f1a2af504fa bt 3 home tusimple incubator mxnet example image classification mxnet libmxnet.so zn2ps10postoffice5startepkcb 0x1e9 0x7f1a2af42119 bt 4 home tusimple incubator mxnet example image classification mxnet libmxnet.so zn5mxnet7kvstore11kvstoredist9runservererkst8functionifvirknst7 cxx1112basic stringicst11char traitsicesaiceeeee 0x1c5 0x7f1a2aee1c35 bt 5 home tusimple incubator mxnet example image classification mxnet libmxnet.so mxkvstorerunserver 0x4b 0x7f1a2ae629db bt 6 usr lib x86 64 linux gnu libffi.so.6 ffi call unix64 0x4c 0x7f1a41450e40 bt 7 usr lib x86 64 linux gnu libffi.so.6 ffi call 0x2eb 0x7f1a414508ab bt 8 usr lib python2.7 lib dynload ctypes.x86 64 linux gnu.so ctypes callproc 0x48f 0x7f1a416603df bt 9 usr lib python2.7 lib dynload ctypes.x86 64 linux gnu.so 0x11d82 0x7f1a41664d82 minimum reproducible example 1 scheduler 1 server 1 worker steps reproduce paste commands ran produced error. 1.export dmlc ps root uri tusimple system product name export dmlc role scheduler export dmlc ps root port 9001 export dmlc num worker 1 export dmlc num server 1 2.python train mnist.py tried solve it? 1.i replaced dmlc ps root uri ip works well",0,dmlc ps root uri using hostname failed distributed training,"dmlc ps root uri using hostname failed distributed training description dmlc ps root uri ip hostname , use hostname instead ip , reports bind failed src van.cc environment info required package used python r scala julia python build info required built source gcc mxnet commit hash 3df9bf802021d5aa67c609c6736acee94aaf3a48 build config doc https mxnet.apache.org install index.html?platform linux language python processor cpu error message paste complete error message, including stack trace. 17 46 11 home tusimple incubator mxnet dmlc core include dmlc . logging.h 308 17 46 11 src van.cc 76 check failed node .port ! 1 bind failed stack trace returned 10 entries bt 0 home tusimple incubator mxnet example image classification mxnet libmxnet.so zn4dmlc15logmessagefatald1ev 0x3c 0x7f1a283f624c bt 1 home tusimple incubator mxnet example image classification mxnet libmxnet.so zn2ps3van5startev 0x91f 0x7f1a2af45b8f bt 2 home tusimple incubator mxnet example image classification mxnet libmxnet.so zn2ps6zmqvan5startev 0x4a 0x7f1a2af504fa bt 3 home tusimple incubator mxnet example image classification mxnet libmxnet.so zn2ps10postoffice5startepkcb 0x1e9 0x7f1a2af42119 bt 4 home tusimple incubator mxnet example image classification mxnet libmxnet.so zn5mxnet7kvstore11kvstoredist9runservererkst8functionifvirknst7 cxx1112basic stringicst11char traitsicesaiceeeee 0x1c5 0x7f1a2aee1c35 bt 5 home tusimple incubator mxnet example image classification mxnet libmxnet.so mxkvstorerunserver 0x4b 0x7f1a2ae629db bt 6 usr lib x86 64 linux gnu libffi.so.6 ffi call unix64 0x4c 0x7f1a41450e40 bt 7 usr lib x86 64 linux gnu libffi.so.6 ffi call 0x2eb 0x7f1a414508ab bt 8 usr lib python2.7 lib dynload ctypes.x86 64 linux gnu.so ctypes callproc 0x48f 0x7f1a416603df bt 9 usr lib python2.7 lib dynload ctypes.x86 64 linux gnu.so 0x11d82 0x7f1a41664d82 traceback recent call last file train mnist.py , line 25, common import find mxnet, fit file home tusimple incubator mxnet example image classification common find mxnet.py , line 20, import mxnet mx file home tusimple incubator mxnet example image classification mxnet init .py , line 56, . import kvstore server file home tusimple incubator mxnet example image classification mxnet kvstore server.py , line 85, init kvstore server module file home tusimple incubator mxnet example image classification mxnet kvstore server.py , line 82, init kvstore server module server.run file home tusimple incubator mxnet example image classification mxnet kvstore server.py , line 73, run check call lib.mxkvstorerunserver self.handle, ctrl proto self. controller , none file home tusimple incubator mxnet example image classification mxnet base.py , line 146, check call raise mxneterror py str lib.mxgetlasterror mxnet.base.mxneterror 17 46 11 src van.cc 76 check failed node .port ! 1 bind failed stack trace returned 10 entries bt 0 home tusimple incubator mxnet example image classification mxnet libmxnet.so zn4dmlc15logmessagefatald1ev 0x3c 0x7f1a283f624c bt 1 home tusimple incubator mxnet example image classification mxnet libmxnet.so zn2ps3van5startev 0x91f 0x7f1a2af45b8f bt 2 home tusimple incubator mxnet example image classification mxnet libmxnet.so zn2ps6zmqvan5startev 0x4a 0x7f1a2af504fa bt 3 home tusimple incubator mxnet example image classification mxnet libmxnet.so zn2ps10postoffice5startepkcb 0x1e9 0x7f1a2af42119 bt 4 home tusimple incubator mxnet example image classification mxnet libmxnet.so zn5mxnet7kvstore11kvstoredist9runservererkst8functionifvirknst7 cxx1112basic stringicst11char traitsicesaiceeeee 0x1c5 0x7f1a2aee1c35 bt 5 home tusimple incubator mxnet example image classification mxnet libmxnet.so mxkvstorerunserver 0x4b 0x7f1a2ae629db bt 6 usr lib x86 64 linux gnu libffi.so.6 ffi call unix64 0x4c 0x7f1a41450e40 bt 7 usr lib x86 64 linux gnu libffi.so.6 ffi call 0x2eb 0x7f1a414508ab bt 8 usr lib python2.7 lib dynload ctypes.x86 64 linux gnu.so ctypes callproc 0x48f 0x7f1a416603df bt 9 usr lib python2.7 lib dynload ctypes.x86 64 linux gnu.so 0x11d82 0x7f1a41664d82 minimum reproducible example 1 scheduler 1 server 1 worker steps reproduce paste commands ran produced error. 1.export dmlc ps root uri tusimple system product name export dmlc role scheduler export dmlc ps root port 9001 export dmlc num worker 1 export dmlc num server 1 2.python train mnist.py tried solve it? 1.i replaced dmlc ps root uri ip works well"
incubator-mxnet,7080,"bugs installation issues, please provide following information. information provide, likely people able help you. environment info operating system aws deep learning ami package used python r scala julia python installed source mxnet commit hash 8c81ee48c197dd66276fa8d4008cbad0dcd2c8fb using python package, please provide python version distribution python 2.7 error message please paste full error message, including stack trace. minimum reproducible example using code, please provide short script reproduces error. steps reproduce running standard examples, please provide commands run lead error. 1. run following code tried solve it? 1. due recent change using src operator elemwise op common.h backward pass reduces number copies. however, failed copy gradient across devices. could probably solved registering attribute inplace updates.",0,simple bind elemwise add group2ctx fails,"simple bind elemwise add group2ctx fails bugs installation issues, please provide following information. information provide, likely people able help you. environment info operating system aws deep learning ami package used python r scala julia python installed source mxnet commit hash 8c81ee48c197dd66276fa8d4008cbad0dcd2c8fb using python package, please provide python version distribution python 2.7 error message please paste full error message, including stack trace. minimum reproducible example using code, please provide short script reproduces error. steps reproduce running standard examples, please provide commands run lead error. 1. run following code tried solve it? 1. due recent change using src operator elemwise op common.h backward pass reduces number copies. however, failed copy gradient across devices. could probably solved registering attribute inplace updates."
incubator-mxnet,10968,"seems mistakes ndarray save dmlc stream strm file ndarray.cc, line 1587 1651 ndarray load dmlc stream strm file ndarray.cc, line 1700 1781 . issue may cause cuda invalid device ordinal following two steps 1. save ndarray gpu 1 machine two gpu card 2. load ndarray another machine one gpu card program. find problem function ndarray save context ndarray saved stream function ndarray load data loaded memory converted context saved ndarray save. context saved ndarray save? improve portability saved params, think better remove operations avoid load ndarray specific device initializing.",0,discussion ndarray discuss ndarray save ndarray load,"discussion ndarray discuss ndarray save ndarray load seems mistakes ndarray save dmlc stream strm file ndarray.cc, line 1587 1651 ndarray load dmlc stream strm file ndarray.cc, line 1700 1781 . issue may cause cuda invalid device ordinal following two steps 1. save ndarray gpu 1 machine two gpu card 2. load ndarray another machine one gpu card program. find problem function ndarray save context ndarray saved stream function ndarray load data loaded memory converted context saved ndarray save. context saved ndarray save? improve portability saved params, think better remove operations avoid load ndarray specific device initializing."
incubator-mxnet,16319,description sphinx builds python api docs following warning reproduce run problem following steps. 1. run lite binary build. 2. run python api docs build,0,python docs multiple broken cross links tutorials,python docs multiple broken cross links tutorials description sphinx builds python api docs following warning reproduce run problem following steps. 1. run lite binary build. 2. run python api docs build
incubator-mxnet,16187,"description symbol.contrib.cond operator support custom operator execution. environment info required using pyton build info required built source n error message minimum reproducible example steps reproduce run code tried solve it? 1. replace custom operator operator built operator works see comment hybrid forward suspect something custom operators executed imperatively ? might related 12154 , 11641 16182 . sure custom operator implementation missing something, attached example simple identity custom operator work .",0,symbol.contrib.cond support custom operator execution,"symbol.contrib.cond support custom operator execution description symbol.contrib.cond operator support custom operator execution. environment info required using pyton build info required built source n error message minimum reproducible example steps reproduce run code tried solve it? 1. replace custom operator operator built operator works see comment hybrid forward suspect something custom operators executed imperatively ? might related 12154 , 11641 16182 . sure custom operator implementation missing something, attached example simple identity custom operator work ."
incubator-mxnet,16333,"description using official mxnet docker image cloud server without docker local server, inference result different. input exactly same. environment info required docker cloud server cuda9.0, cuda drive 418 host machine , mxnet 1.4.0 local server cuda9.0, cuda drive 396, mxnet 1.3.0",0,mxnet docker gives different inference output,"mxnet docker gives different inference output description using official mxnet docker image cloud server without docker local server, inference result different. input exactly same. environment info required docker cloud server cuda9.0, cuda drive 418 host machine , mxnet 1.4.0 local server cuda9.0, cuda drive 396, mxnet 1.3.0"
incubator-mxnet,7933,"successfully converted squeezenet resnet50 models examples coreml using mxnet coreml. however, converting model fine tuning using data, predictions seemingly random. model fine tuned using finetune.py examples. model performs well prior conversion coreml. conversion coreml, model predicts probabilities regardless image. pre trained model using fine tuning imagenet11k places resnet50 model. tried 1. subtracting channel biases performed fine tuning. pre processing arguments ' image input names data , red bias 123.68, blue bias 103.939, green bias 116.779 ' 2. subtracting channel biases scaling 1 255 pre processing arguments ' image input names data , red bias 123.68, blue bias 103.939, green bias 116.779, image scale 0.00392156862 ' 3. subtracting scaled channel biases unsure coreml performed scaling pre processing arguments ' image input names data , red bias 0.485019, blue bias 0.407603, green bias 0.457956, image scale 0.00392156862 ' 4. scaling biasing channels anyone successfully converted model fine tuning using different data set? ideas would greatly appreciated. fairly certain there's something simple overlooking... also examined converted model using model pb2 make sure preprocessing flags respected, appear print model.neuralnetworkclassifier.preprocessing featurename data scaler channelscale 0.00380000006407 bluebias 103.939 greenbias 116.779 redbias 123.68 here's entire cmd line mxnet coreml converter.py model prefix 'imagenet11k places resnet 50' epoch 47 input shape ' data 3,224,224 ' mode classifier class labels myclass labels.txt output file mxnetimagenet11kplaces50resnet.mlmodel pre processing arguments ' image input names data , red bias 123.68, blue bias 103.939, green bias 116.779, image scale 0.00392156862 '",0,coreml conversion finetuned model,"coreml conversion finetuned model successfully converted squeezenet resnet50 models examples coreml using mxnet coreml. however, converting model fine tuning using data, predictions seemingly random. model fine tuned using finetune.py examples. model performs well prior conversion coreml. conversion coreml, model predicts probabilities regardless image. pre trained model using fine tuning imagenet11k places resnet50 model. tried 1. subtracting channel biases performed fine tuning. pre processing arguments ' image input names data , red bias 123.68, blue bias 103.939, green bias 116.779 ' 2. subtracting channel biases scaling 1 255 pre processing arguments ' image input names data , red bias 123.68, blue bias 103.939, green bias 116.779, image scale 0.00392156862 ' 3. subtracting scaled channel biases unsure coreml performed scaling pre processing arguments ' image input names data , red bias 0.485019, blue bias 0.407603, green bias 0.457956, image scale 0.00392156862 ' 4. scaling biasing channels anyone successfully converted model fine tuning using different data set? ideas would greatly appreciated. fairly certain there's something simple overlooking... also examined converted model using model pb2 make sure preprocessing flags respected, appear print model.neuralnetworkclassifier.preprocessing featurename data scaler channelscale 0.00380000006407 bluebias 103.939 greenbias 116.779 redbias 123.68 here's entire cmd line mxnet coreml converter.py model prefix 'imagenet11k places resnet 50' epoch 47 input shape ' data 3,224,224 ' mode classifier class labels myclass labels.txt output file mxnetimagenet11kplaces50resnet.mlmodel pre processing arguments ' image input names data , red bias 123.68, blue bias 103.939, green bias 116.779, image scale 0.00392156862 '"
incubator-mxnet,16188,"description symbol.contrib.cond operator support build operators round, floor ceil probably . environment info required using pyton build info required built source n error message minimum reproducible example steps reproduce 1. run code above. hybrid forward lines comment working uncomment lines see examples tried solve it? n might related 12154 , 11641 , 16182 16187 . keep issues separate sure cause same.",0,symbol.contrib.cond support built operators,"symbol.contrib.cond support built operators description symbol.contrib.cond operator support build operators round, floor ceil probably . environment info required using pyton build info required built source n error message minimum reproducible example steps reproduce 1. run code above. hybrid forward lines comment working uncomment lines see examples tried solve it? n might related 12154 , 11641 , 16182 16187 . keep issues separate sure cause same."
incubator-mxnet,12126,platform independent api without using nvidia smi querying gpu memory. c implementation provided sbodenstein pr https github.com apache incubator mxnet pull 12083 pullrequestreview 145347358,0,feature request utility api querying gpu memory,feature request utility api querying gpu memory platform independent api without using nvidia smi querying gpu memory. c implementation provided sbodenstein pr https github.com apache incubator mxnet pull 12083 pullrequestreview 145347358
incubator-mxnet,16674,"description http jenkins.mxnet ci.amazon ml.com blue organizations jenkins mxnet validation 2funix cpu detail pr 16671 2 pipeline error message reproduce developed code, please provide short script reproduces error. existing examples, please provide link. steps reproduce paste commands ran produced error. 1. 2. tried solve it? 1. 2. environment recommend using script collecting diagnositc information. run following command paste outputs",0,garbage ctx.dev mask seen clojure cpu integration ci run,"garbage ctx.dev mask seen clojure cpu integration ci run description http jenkins.mxnet ci.amazon ml.com blue organizations jenkins mxnet validation 2funix cpu detail pr 16671 2 pipeline error message reproduce developed code, please provide short script reproduces error. existing examples, please provide link. steps reproduce paste commands ran produced error. 1. 2. tried solve it? 1. 2. environment recommend using script collecting diagnositc information. run following command paste outputs"
incubator-mxnet,10549,"description like build mxnet scala windows source. build libmxnet.dll successfully, instructions build scala package windows. instruction page https mxnet.incubator.apache.org install windows setup.html install mxnet package scala refers make only. could provide instructions relevant windows mxnet scala.dll? environment info required windows 10. mxnet 1.1.0 build info required built source compiler gcc clang mingw visual studio visual studio 14 2015 win64 mxnet commit hash 07a83a0325a3d782513a04f47d711710972cb144 build config mxnet option use cpp package build c package steps reproduce 1. build libmxnet.dll 2. try build scala package?? tried solve it? 1. tried compile dll files scala package native src failed resolve dependencies correctly",0,scala package 1.1.0 build instruction windows vs2015,"scala package 1.1.0 build instruction windows vs2015 description like build mxnet scala windows source. build libmxnet.dll successfully, instructions build scala package windows. instruction page https mxnet.incubator.apache.org install windows setup.html install mxnet package scala refers make only. could provide instructions relevant windows mxnet scala.dll? environment info required windows 10. mxnet 1.1.0 build info required built source compiler gcc clang mingw visual studio visual studio 14 2015 win64 mxnet commit hash 07a83a0325a3d782513a04f47d711710972cb144 build config mxnet option use cpp package build c package steps reproduce 1. build libmxnet.dll 2. try build scala package?? tried solve it? 1. tried compile dll files scala package native src failed resolve dependencies correctly"
incubator-mxnet,12683,"note providing complete information concise form best way get help. issue template serves checklist essential information technical issues bug reports. non technical issues feature requests, feel free present information believe best form. q discussion, please start discussion thread https discuss.mxnet.io description unit tests pulling data data.dmlc.ml randomly failing environment info required jenkins http jenkins.mxnet ci.amazon ml.com blue organizations jenkins incubator mxnet detail master 1676 pipeline",0,tests still pulling data data.dmlc.ml randomly failing,"tests still pulling data data.dmlc.ml randomly failing note providing complete information concise form best way get help. issue template serves checklist essential information technical issues bug reports. non technical issues feature requests, feel free present information believe best form. q discussion, please start discussion thread https discuss.mxnet.io description unit tests pulling data data.dmlc.ml randomly failing environment info required jenkins http jenkins.mxnet ci.amazon ml.com blue organizations jenkins incubator mxnet detail master 1676 pipeline"
incubator-mxnet,13100,"description hybridizing creates superfluous backward ops conditions. bad, ops may consequently trigger storage fallback. example below, gradient requested. still generated consequently storage fallback occurs. note example works without storage fallback imperative symbolic api imperative, remove hybridize. symbolic, see https github.com apache incubator mxnet blob master example sparse factorization machine model.py l39 environment info required mxnet floatnp.floatingnp.float64 np.dtype float .type steps reproduce paste commands ran produced error. 1. run code example. note storage fallback occurs. 2. remove line. note storage fallback disappears.",0,hybridization generates superfluous backward ops may induce storage fallbacks,"hybridization generates superfluous backward ops may induce storage fallbacks description hybridizing creates superfluous backward ops conditions. bad, ops may consequently trigger storage fallback. example below, gradient requested. still generated consequently storage fallback occurs. note example works without storage fallback imperative symbolic api imperative, remove hybridize. symbolic, see https github.com apache incubator mxnet blob master example sparse factorization machine model.py l39 environment info required mxnet floatnp.floatingnp.float64 np.dtype float .type steps reproduce paste commands ran produced error. 1. run code example. note storage fallback occurs. 2. remove line. note storage fallback disappears."
incubator-mxnet,11255,"note providing complete information concise form best way get help. issue template serves checklist essential information technical issues bug reports. non technical issues feature requests, feel free present information believe best form. q discussion, please start discussion thread https discuss.mxnet.io description installed anaconda3 installed mxnet cu90 pip training speed slow find ,the data reading speed much slow , reading image data decoding binary data imdecode , gpu usage 30 , guess that, speed bottleneck speed data reading. multi gpu single gpu slow environment info required ubuntu16.04 p100 python diagnose.py package used python r scala julia using anaconda3 python3.6 mxnet 1.2.0 scala user, please provide 1. java version 2. maven version 3. scala runtime applicable r user, please provide r build info required built source compiler gcc clang mingw visual studio mxnet commit hash paste output here. build config paste content config.mk, build command. pip install mxnet cu90 error message paste complete error message, including stack trace. error, speed slow, gpu usage low ! default https user images.githubusercontent.com 12196464 41327683 f5f6506e 6ef6 11e8 9650 cd2add427809.png traiing slow ! default https user images.githubusercontent.com 12196464 41327726 1cb087f6 6ef7 11e8 86dd 42978e275253.png minimum reproducible example using code, please provide short script reproduces error. otherwise, please provide link existing example. steps reproduce paste commands ran produced error. 1.training mxnet p100 2. tried solve it? 1. installed python3, python2, pytthon2 encoutered problem 2.",0,p100 mxnet slow,"p100 mxnet slow note providing complete information concise form best way get help. issue template serves checklist essential information technical issues bug reports. non technical issues feature requests, feel free present information believe best form. q discussion, please start discussion thread https discuss.mxnet.io description installed anaconda3 installed mxnet cu90 pip training speed slow find ,the data reading speed much slow , reading image data decoding binary data imdecode , gpu usage 30 , guess that, speed bottleneck speed data reading. multi gpu single gpu slow environment info required ubuntu16.04 p100 python diagnose.py package used python r scala julia using anaconda3 python3.6 mxnet 1.2.0 scala user, please provide 1. java version 2. maven version 3. scala runtime applicable r user, please provide r build info required built source compiler gcc clang mingw visual studio mxnet commit hash paste output here. build config paste content config.mk, build command. pip install mxnet cu90 error message paste complete error message, including stack trace. error, speed slow, gpu usage low ! default https user images.githubusercontent.com 12196464 41327683 f5f6506e 6ef6 11e8 9650 cd2add427809.png traiing slow ! default https user images.githubusercontent.com 12196464 41327726 1cb087f6 6ef7 11e8 86dd 42978e275253.png minimum reproducible example using code, please provide short script reproduces error. otherwise, please provide link existing example. steps reproduce paste commands ran produced error. 1.training mxnet p100 2. tried solve it? 1. installed python3, python2, pytthon2 encoutered problem 2."
incubator-mxnet,13853,"opening issue behalf adwivedi discussion forum https discuss.mxnet.io error running mxnet spark examples test cases 2720 . quotes thread... trying run mxnet distributed mode using spark implemented https github.com apache incubator mxnet tree master scala package spark able run examples tests. commented tests file https github.com apache incubator mxnet blob master scala package spark src test scala org apache mxnet spark mxnetgeneralsuite.scala keep running following error. get error try run examples repo. seen error generally mismatch scala versions api, case using using pom file project including external libraries. also looked pom file found lib might mismatch case, libraries pom 2.11 version building source, running vm parameter let find native library.",0,error running mxnet spark examples test cases,"error running mxnet spark examples test cases opening issue behalf adwivedi discussion forum https discuss.mxnet.io error running mxnet spark examples test cases 2720 . quotes thread... trying run mxnet distributed mode using spark implemented https github.com apache incubator mxnet tree master scala package spark able run examples tests. commented tests file https github.com apache incubator mxnet blob master scala package spark src test scala org apache mxnet spark mxnetgeneralsuite.scala keep running following error. get error try run examples repo. seen error generally mismatch scala versions api, case using using pom file project including external libraries. also looked pom file found lib might mismatch case, libraries pom 2.11 version building source, running vm parameter let find native library."
incubator-mxnet,9763,"hi everyone, working c project needs use mxnet cnn inference. set compiled mxnet source code myself. c program, include first got errors like caused https github.com apache incubator mxnet blob master cpp package include mxnet cpp base.h l31 includes inside it. thus, add lot dirs avoid similar errors want ask possible make mxnet c api easier use? instance, mxnet cpp package folder contains header files necessary c development libraries necessary c development i.e. libmxnet.so",0,many header files need included using c api,"many header files need included using c api hi everyone, working c project needs use mxnet cnn inference. set compiled mxnet source code myself. c program, include first got errors like caused https github.com apache incubator mxnet blob master cpp package include mxnet cpp base.h l31 includes inside it. thus, add lot dirs avoid similar errors want ask possible make mxnet c api easier use? instance, mxnet cpp package folder contains header files necessary c development libraries necessary c development i.e. libmxnet.so"
incubator-mxnet,15165,looks like file cannot found building latest trunk,0,cannot open include file 'cub cub.cuh' file directory,cannot open include file 'cub cub.cuh' file directory looks like file cannot found building latest trunk
incubator-mxnet,10668,used use plugin like restart builds using keyword trigger like . could handy. marcoabreu plugin https plugins.jenkins.io github pr comment build,0,feature request jenkins build restartable comments pr,feature request jenkins build restartable comments pr used use plugin like restart builds using keyword trigger like . could handy. marcoabreu plugin https plugins.jenkins.io github pr comment build
incubator-mxnet,16793,"description current design deepnumpy's random module follows native numpy terms interpretation parameter . specifically, indicates final output size sampling operation. parameter tensors, narrower smaller , automatically broadcast output's shape. however, mechanism makes i.i.d sampling little bit tricky, example problem would arise symbolic model, shape cannot obtained frontend. solution following function could resolve issue. modified https github.com apache incubator mxnet blob master src operator numpy random dist common.h l143 notice function could stay same. modified sampling method able produce following result",0,numpy wip rfc sample n op deepnumpy,"numpy wip rfc sample n op deepnumpy description current design deepnumpy's random module follows native numpy terms interpretation parameter . specifically, indicates final output size sampling operation. parameter tensors, narrower smaller , automatically broadcast output's shape. however, mechanism makes i.i.d sampling little bit tricky, example problem would arise symbolic model, shape cannot obtained frontend. solution following function could resolve issue. modified https github.com apache incubator mxnet blob master src operator numpy random dist common.h l143 notice function could stay same. modified sampling method able produce following result"
incubator-mxnet,12894,"description training ssd networks leakyrelu rrelu activation causes training crash. tried different networks vgg16 reduced.py well. always crashes environment info required package used python r scala julia python build info required built source compiler gcc clang mingw visual studio gcc mxnet commit hash 74638105f5480349cf57cda40a37475d626dbf41 build config make j4 use opencv 1 use blas openblas use cuda 1 use cuda path usr local cuda use cudnn 1 error message minimum reproducible example vgg16 reduced.py example ssd symbol, make following changes shown below. replacing leakyrelu activations positions also causes training crash steps reproduce python train.py gpus 0,1 batch size 32 pretrained '' tried solve it? replace leakyrelu rrelu activations get around issue.",0,training crash ssd leakyrelu rrelu,"training crash ssd leakyrelu rrelu description training ssd networks leakyrelu rrelu activation causes training crash. tried different networks vgg16 reduced.py well. always crashes environment info required package used python r scala julia python build info required built source compiler gcc clang mingw visual studio gcc mxnet commit hash 74638105f5480349cf57cda40a37475d626dbf41 build config make j4 use opencv 1 use blas openblas use cuda 1 use cuda path usr local cuda use cudnn 1 error message minimum reproducible example vgg16 reduced.py example ssd symbol, make following changes shown below. replacing leakyrelu activations positions also causes training crash steps reproduce python train.py gpus 0,1 batch size 32 pretrained '' tried solve it? replace leakyrelu rrelu activations get around issue."
incubator-mxnet,9574,description running mobilenet environment info required build info required built source clang mxnet commit hash 20253d5ce821ac012e2483b5dfb15bb5b7202f6d update test gluon model zoo.py 9539 minimum reproducible example run mobilenet.py steps reproduce tried solve it? 1. try run massif see memory going,0,large host memory usage using gpu mobilenets,large host memory usage using gpu mobilenets description running mobilenet environment info required build info required built source clang mxnet commit hash 20253d5ce821ac012e2483b5dfb15bb5b7202f6d update test gluon model zoo.py 9539 minimum reproducible example run mobilenet.py steps reproduce tried solve it? 1. try run massif see memory going
incubator-mxnet,10207,"description 1. khatri rao prematurely added mx.nd mx.sym namespace instead contrib namespace happened https github.com apache incubator mxnet pull 7781 files r176510540. 2. khatri rao gpu version. given pr merged last year, feature made way releases already. resolve, need one following 1. mark released op deprecated move back contrib next major version. 2. move forward one pass vetting api design cc piiswrong , 2 implement gpu version. cc cswiercz",0,mxnet.ndarray.khatri rao needs gpu version,"mxnet.ndarray.khatri rao needs gpu version description 1. khatri rao prematurely added mx.nd mx.sym namespace instead contrib namespace happened https github.com apache incubator mxnet pull 7781 files r176510540. 2. khatri rao gpu version. given pr merged last year, feature made way releases already. resolve, need one following 1. mark released op deprecated move back contrib next major version. 2. move forward one pass vetting api design cc piiswrong , 2 implement gpu version. cc cswiercz"
incubator-mxnet,5677,"constant initializer overrides init weight method. consequently, good way use existing initializers initialize values bias terms, always set zero. example code true uniform, normal, argument could made typically uniform normal used, biases set zero, current design results unexpected behavior. apply constant, uniform, normal, initializer variable, expectation actually initialize variable way! digging source code apparent expectation violated even made apparent documentation, think that's right design anyway. functions work would expect name. someone wants treat bias terms uniquely, use mixed initializer explicitly state difference. or, initing bias zero common enough, allow mode flag constructor initializers, still explicit, simpler using mixed. complex initialization strategies, makes sense variable dependent.",0,constant initializer works keys ending weight,"constant initializer works keys ending weight constant initializer overrides init weight method. consequently, good way use existing initializers initialize values bias terms, always set zero. example code true uniform, normal, argument could made typically uniform normal used, biases set zero, current design results unexpected behavior. apply constant, uniform, normal, initializer variable, expectation actually initialize variable way! digging source code apparent expectation violated even made apparent documentation, think that's right design anyway. functions work would expect name. someone wants treat bias terms uniquely, use mixed initializer explicitly state difference. or, initing bias zero common enough, allow mode flag constructor initializers, still explicit, simpler using mixed. complex initialization strategies, makes sense variable dependent."
incubator-mxnet,16868,"description checking validity parameters crucial many operators, especially distribution related ops. see references implementations torch tensorflow implement operator called , takes boolean tensor error message input checks elements true, not, raises exception given message. return scalar tensor none elements false. however, op fails symbolic mode, output op neither returned users used input ops, causing engine completely ignore check op. short, exception raised. leezu provides workaround like approach works well, exception got thrown expected. however, method convenient buried function called inside hybrid forward, e.g case, becomes quite difficult manually turn tensor return value. another solution could op control flow, unfortunately, seems maintenance. believe, simplest way kinds mechanism force mxnet evaluate particular op. open solutions . would great implement feature using existing infrastructure references https pytorch.org docs stable modules torch distributions constraints.html constraint https github.com tensorflow probability blob 84cdabcdea19ce20d044fb7809b5742b953c9688 tensorflow probability python internal assert util.py",0,numpy infrastructure implementing constraint check,"numpy infrastructure implementing constraint check description checking validity parameters crucial many operators, especially distribution related ops. see references implementations torch tensorflow implement operator called , takes boolean tensor error message input checks elements true, not, raises exception given message. return scalar tensor none elements false. however, op fails symbolic mode, output op neither returned users used input ops, causing engine completely ignore check op. short, exception raised. leezu provides workaround like approach works well, exception got thrown expected. however, method convenient buried function called inside hybrid forward, e.g case, becomes quite difficult manually turn tensor return value. another solution could op control flow, unfortunately, seems maintenance. believe, simplest way kinds mechanism force mxnet evaluate particular op. open solutions . would great implement feature using existing infrastructure references https pytorch.org docs stable modules torch distributions constraints.html constraint https github.com tensorflow probability blob 84cdabcdea19ce20d044fb7809b5742b953c9688 tensorflow probability python internal assert util.py"
incubator-mxnet,14916,"hi, there! seems current mxnet could convert basic cnn models like alexnet, resnet pytorch simply shape onnx defined tensor rather attribute mentioned https github.com apache incubator mxnet issues 13395 issuecomment 443304545 . specific plans solving problem ? notice error could even occur dynamic reshape. simple script produce like error message",0,failed convert pytorch networks torch.view mxnet onnx,"failed convert pytorch networks torch.view mxnet onnx hi, there! seems current mxnet could convert basic cnn models like alexnet, resnet pytorch simply shape onnx defined tensor rather attribute mentioned https github.com apache incubator mxnet issues 13395 issuecomment 443304545 . specific plans solving problem ? notice error could even occur dynamic reshape. simple script produce like error message"
incubator-mxnet,12736,"description liblapack found trying build mxnet wheel ci build.py jetson framework host computer. installed wheel jetson see would happen lapack supported functions returned errors, build correctly, expected. environment info required running build.py ubuntu 16.04 info docker version 18.06.1 ce 1. download diagnosis script https raw.githubusercontent.com apache incubator mxnet master tools diagnose.py 2. run script using paste output here. sudo python diagnose.py python info 'version ', '2.7.12' 'compiler ', 'gcc 5.4.0 20160609' 'build ', 'default', 'dec 4 2017 14 50 18' 'arch ', '64bit', 'elf' pip info 'version ', '18.0' 'directory ', ' usr local lib python2.7 dist packages pip' mxnet info mxnet installed. system info 'platform ', 'linux 4.15.0 34 generic x86 64 ubuntu 16.04 xenial' 'system ', 'linux' 'node ', 'cfa system product name' 'release ', '4.15.0 34 generic' 'version ', ' 37 16.04.1 ubuntu smp tue aug 28 10 44 06 utc 2018' hardware info 'machine ', 'x86 64' 'processor ', 'x86 64' architecture x86 64 cpu op mode 32 bit, 64 bit byte order little endian cpu 8 line cpu list 0 7 thread per core 2 core per socket 4 socket 1 numa node 1 vendor id genuineintel cpu family 6 model 158 model name intel r core tm i7 7700k cpu 4.20ghz stepping 9 cpu mhz 4297.817 cpu max mhz 4500.0000 cpu min mhz 800.0000 bogomips 8400.00 virtualization vt x l1d cache 32k l1i cache 32k l2 cache 256k l3 cache 8192k numa node0 cpu 0 7 flags fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant tsc art arch perfmon pebs bts rep good nopl xtopology nonstop tsc cpuid aperfmperf tsc known freq pni pclmulqdq dtes64 monitor ds cpl vmx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid sse4 1 sse4 2 x2apic movbe popcnt tsc deadline timer aes xsave avx f16c rdrand lahf lm abm 3dnowprefetch cpuid fault invpcid single pti ssbd ibrs ibpb stibp tpr shadow vnmi flexpriority ept vpid fsgsbase tsc adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx rdseed adx smap clflushopt intel pt xsaveopt xsavec xgetbv1 xsaves dtherm ida arat pln pts hwp hwp notify hwp act window hwp epp flush l1d network test setting timeout 10 timing mxnet https github.com apache incubator mxnet, dns 0.0063 sec, load 0.4813 sec. timing pypi https pypi.python.org pypi pip, dns 0.0086 sec, load 0.3699 sec. timing fashionmnist https apache mxnet.s3 accelerate.dualstack.amazonaws.com gluon dataset fashion mnist train labels idx1 ubyte.gz, dns 0.1589 sec, load 0.9135 sec. timing conda https repo.continuum.io pkgs free , dns 0.0258 sec, load 0.0875 sec. timing gluon tutorial en http gluon.mxnet.io, dns 0.0778 sec, load 0.6933 sec. timing gluon tutorial cn https zh.gluon.ai, dns 0.1163 sec, load 0.6141 sec. package used python r scala julia python 2 listed build info required built source cloned repository 10 3 2018 mxnet commit hash bcd24f85457821f9c0ce17d60e545a252a87a5ae build config standard config.mk, copied crosscompile.jetson.mk error message including preceding output build.py 2018 10 04 13 04 06,562 executing equivalent docker run cap add sys ptrace rm shm size 500m v home cfa mxnet 1.3 work mxnet v home cfa mxnet 1.3 build work build v tmp ci ccache work ccache u 0 0 e ccache maxsize 500g e ccache tempdir tmp ccache e ccache dir work ccache e ccache logfile tmp ccache.log ti mxnetci build.jetson work mxnet ci docker runtime functions.sh build jetson build.py 2018 10 04 13 04 07,016 started container f3dad0aa9232 nose coverage arguments ' coverage cover inclusive cover xml cover branches cover package mxnet' set x pushd . work mxnet work mxnet cp make crosscompile.jetson.mk . config.mk nproc make j8 makefile 180 use lapack disabled libraries found minimum reproducible example using code, please provide short script reproduces error. otherwise, please provide link existing example. steps reproduce 1. cloned repo mxnet 1.3 2. cd mxnet 1.3 3. sudo time ci build.py p jetson tried solve it? 1. tried symbolically linking liblapack.so liblapack.a files main mxnet 1.3 folder host computer, tried hard copying well. enable lapack ci build.py method make .whl file install jetson tx2?",0,"running build.py generate .whl jetson, lapack lib found","running build.py generate .whl jetson, lapack lib found description liblapack found trying build mxnet wheel ci build.py jetson framework host computer. installed wheel jetson see would happen lapack supported functions returned errors, build correctly, expected. environment info required running build.py ubuntu 16.04 info docker version 18.06.1 ce 1. download diagnosis script https raw.githubusercontent.com apache incubator mxnet master tools diagnose.py 2. run script using paste output here. sudo python diagnose.py python info 'version ', '2.7.12' 'compiler ', 'gcc 5.4.0 20160609' 'build ', 'default', 'dec 4 2017 14 50 18' 'arch ', '64bit', 'elf' pip info 'version ', '18.0' 'directory ', ' usr local lib python2.7 dist packages pip' mxnet info mxnet installed. system info 'platform ', 'linux 4.15.0 34 generic x86 64 ubuntu 16.04 xenial' 'system ', 'linux' 'node ', 'cfa system product name' 'release ', '4.15.0 34 generic' 'version ', ' 37 16.04.1 ubuntu smp tue aug 28 10 44 06 utc 2018' hardware info 'machine ', 'x86 64' 'processor ', 'x86 64' architecture x86 64 cpu op mode 32 bit, 64 bit byte order little endian cpu 8 line cpu list 0 7 thread per core 2 core per socket 4 socket 1 numa node 1 vendor id genuineintel cpu family 6 model 158 model name intel r core tm i7 7700k cpu 4.20ghz stepping 9 cpu mhz 4297.817 cpu max mhz 4500.0000 cpu min mhz 800.0000 bogomips 8400.00 virtualization vt x l1d cache 32k l1i cache 32k l2 cache 256k l3 cache 8192k numa node0 cpu 0 7 flags fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant tsc art arch perfmon pebs bts rep good nopl xtopology nonstop tsc cpuid aperfmperf tsc known freq pni pclmulqdq dtes64 monitor ds cpl vmx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid sse4 1 sse4 2 x2apic movbe popcnt tsc deadline timer aes xsave avx f16c rdrand lahf lm abm 3dnowprefetch cpuid fault invpcid single pti ssbd ibrs ibpb stibp tpr shadow vnmi flexpriority ept vpid fsgsbase tsc adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx rdseed adx smap clflushopt intel pt xsaveopt xsavec xgetbv1 xsaves dtherm ida arat pln pts hwp hwp notify hwp act window hwp epp flush l1d network test setting timeout 10 timing mxnet https github.com apache incubator mxnet, dns 0.0063 sec, load 0.4813 sec. timing pypi https pypi.python.org pypi pip, dns 0.0086 sec, load 0.3699 sec. timing fashionmnist https apache mxnet.s3 accelerate.dualstack.amazonaws.com gluon dataset fashion mnist train labels idx1 ubyte.gz, dns 0.1589 sec, load 0.9135 sec. timing conda https repo.continuum.io pkgs free , dns 0.0258 sec, load 0.0875 sec. timing gluon tutorial en http gluon.mxnet.io, dns 0.0778 sec, load 0.6933 sec. timing gluon tutorial cn https zh.gluon.ai, dns 0.1163 sec, load 0.6141 sec. package used python r scala julia python 2 listed build info required built source cloned repository 10 3 2018 mxnet commit hash bcd24f85457821f9c0ce17d60e545a252a87a5ae build config standard config.mk, copied crosscompile.jetson.mk error message including preceding output build.py 2018 10 04 13 04 06,562 executing equivalent docker run cap add sys ptrace rm shm size 500m v home cfa mxnet 1.3 work mxnet v home cfa mxnet 1.3 build work build v tmp ci ccache work ccache u 0 0 e ccache maxsize 500g e ccache tempdir tmp ccache e ccache dir work ccache e ccache logfile tmp ccache.log ti mxnetci build.jetson work mxnet ci docker runtime functions.sh build jetson build.py 2018 10 04 13 04 07,016 started container f3dad0aa9232 nose coverage arguments ' coverage cover inclusive cover xml cover branches cover package mxnet' set x pushd . work mxnet work mxnet cp make crosscompile.jetson.mk . config.mk nproc make j8 makefile 180 use lapack disabled libraries found minimum reproducible example using code, please provide short script reproduces error. otherwise, please provide link existing example. steps reproduce 1. cloned repo mxnet 1.3 2. cd mxnet 1.3 3. sudo time ci build.py p jetson tried solve it? 1. tried symbolically linking liblapack.so liblapack.a files main mxnet 1.3 folder host computer, tried hard copying well. enable lapack ci build.py method make .whl file install jetson tx2?"
incubator-mxnet,14349,"note providing complete information concise form best way get help. issue template serves checklist essential information technical issues bug reports. non technical issues feature requests, feel free present information believe best form. q discussion, please start discussion thread https discuss.mxnet.io description brief description problem 2 sentences. installed mxnet using pip install mxnet 1.3.1. using numpy 1.14.3. import mxnet fails module found environment info required pip freeze mxnet 1.3.1 numpy 1.14.3 python diagnose.py package used python r scala julia using python scala user, please provide 1. java version 2. maven version 3. scala runtime applicable r user, please provide r build info required built source compiler gcc clang mingw visual studio mxnet commit hash paste output here. build config paste content config.mk, build command. error message paste complete error message, including stack trace. import mxnet mx file c users achatterjee appdata local programs python python36 lib site packages mxnet init .py , line 24, .context import context, current context, cpu, gpu, cpu pinned file c users achatterjee appdata local programs python python36 lib site packages mxnet context.py , line 24, .base import classproperty, metaclass, mxclasspropertymetaclass file c users achatterjee appdata local programs python python36 lib site packages mxnet base.py , line 213, lib load lib file c users achatterjee appdata local programs python python36 lib site packages mxnet base.py , line 204, load lib lib ctypes.cdll lib path 0 , ctypes.rtld local file c users achatterjee appdata local programs python python36 lib ctypes init .py , line 348, init self. handle dlopen self. name, mode oserror winerror 126 specified module could found minimum reproducible example using code, please provide short script reproduces error. otherwise, please provide link existing example. steps reproduce paste commands ran produced error. 1. import mxnet mx 2. tried solve it? 1. 2.",0,installed mxnet using pip install mxnet 1.3.1. using numpy 1.14.3. import mxnet fails,"installed mxnet using pip install mxnet 1.3.1. using numpy 1.14.3. import mxnet fails note providing complete information concise form best way get help. issue template serves checklist essential information technical issues bug reports. non technical issues feature requests, feel free present information believe best form. q discussion, please start discussion thread https discuss.mxnet.io description brief description problem 2 sentences. installed mxnet using pip install mxnet 1.3.1. using numpy 1.14.3. import mxnet fails module found environment info required pip freeze mxnet 1.3.1 numpy 1.14.3 python diagnose.py package used python r scala julia using python scala user, please provide 1. java version 2. maven version 3. scala runtime applicable r user, please provide r build info required built source compiler gcc clang mingw visual studio mxnet commit hash paste output here. build config paste content config.mk, build command. error message paste complete error message, including stack trace. import mxnet mx file c users achatterjee appdata local programs python python36 lib site packages mxnet init .py , line 24, .context import context, current context, cpu, gpu, cpu pinned file c users achatterjee appdata local programs python python36 lib site packages mxnet context.py , line 24, .base import classproperty, metaclass, mxclasspropertymetaclass file c users achatterjee appdata local programs python python36 lib site packages mxnet base.py , line 213, lib load lib file c users achatterjee appdata local programs python python36 lib site packages mxnet base.py , line 204, load lib lib ctypes.cdll lib path 0 , ctypes.rtld local file c users achatterjee appdata local programs python python36 lib ctypes init .py , line 348, init self. handle dlopen self. name, mode oserror winerror 126 specified module could found minimum reproducible example using code, please provide short script reproduces error. otherwise, please provide link existing example. steps reproduce paste commands ran produced error. 1. import mxnet mx 2. tried solve it? 1. 2."
incubator-mxnet,9755,"core api's io https github.com apache incubator mxnet tree master src io updated incude ndarrayiter https github.com apache incubator mxnet blob master python mxnet io.py l544 , forces developers languages write equivalants. core api like , , , etc, would called languages language's mxdataiter wrapper. first introduced numpyiter https github.com apache incubator mxnet blob f6327ecf86d8a169ec1277c1e7809ed14f89b0c0 python mxnet io.py l83 sneakerkg 2015, evolved general numeric vector iterator useful languages python, r, defined implement r ndarrayiter https github.com apache incubator mxnet blob master r package src io.h l92 seen comments would make sense core, called r's mxdataiter wrapper https github.com apache incubator mxnet blob master r package src io.h l53 . could also accessed scala's mxdataiter wrapper https github.com apache incubator mxnet blob master scala package core src main scala ml dmlc mxnet io mxdataiter.scala . instead, code test scala ndarrayiter https github.com apache incubator mxnet blob master scala package core src main scala ml dmlc mxnet io ndarrayiter.scala . c api even mention c mxdataiter wrapper https github.com apache incubator mxnet blob master cpp package include mxnet cpp io.h .",0,feature request move ndarrayiter core api,"feature request move ndarrayiter core api core api's io https github.com apache incubator mxnet tree master src io updated incude ndarrayiter https github.com apache incubator mxnet blob master python mxnet io.py l544 , forces developers languages write equivalants. core api like , , , etc, would called languages language's mxdataiter wrapper. first introduced numpyiter https github.com apache incubator mxnet blob f6327ecf86d8a169ec1277c1e7809ed14f89b0c0 python mxnet io.py l83 sneakerkg 2015, evolved general numeric vector iterator useful languages python, r, defined implement r ndarrayiter https github.com apache incubator mxnet blob master r package src io.h l92 seen comments would make sense core, called r's mxdataiter wrapper https github.com apache incubator mxnet blob master r package src io.h l53 . could also accessed scala's mxdataiter wrapper https github.com apache incubator mxnet blob master scala package core src main scala ml dmlc mxnet io mxdataiter.scala . instead, code test scala ndarrayiter https github.com apache incubator mxnet blob master scala package core src main scala ml dmlc mxnet io ndarrayiter.scala . c api even mention c mxdataiter wrapper https github.com apache incubator mxnet blob master cpp package include mxnet cpp io.h ."
incubator-mxnet,16656,nfo 56358 2019 10 28 08 26 33 main crack segmentation.py 382 segmentation crack... usr local lib python2.7 dist packages mxnet 1.3.0 py2.7.egg mxnet module base module.py 66 userwarning data provided label shapes match names specified label names vs. 'softmax label' warnings.warn msg error usr bin python' free invalid pointer 0x00007f9a484890a0 backtrace lib x86 64 linux gnu libc.so.6 0x777e5 0x7f9a764bc7e5 lib x86 64 linux gnu libc.so.6 0x8037a 0x7f9a764c537a lib x86 64 linux gnu libc.so.6 cfree 0x4c 0x7f9a764c953c usr local lib python2.7 dist packages mxnet 1.3.0 py2.7.egg mxnet libmxnet.so mxexecutorreshape 0x1de7 0x7f99c3a2d5e7 usr lib x86 64 linux gnu libffi.so.6 ffi call unix64 0x4c 0x7f9a0cfcfe40 usr lib x86 64 linux gnu libffi.so.6 ffi call 0x2eb 0x7f9a0cfcf8ab usr lib python2.7 lib dynload ctypes.x86 64 linux gnu.so ctypes callproc 0x48f 0x7f9a0d1df3df usr lib python2.7 lib dynload ctypes.x86 64 linux gnu.so 0x11d82 0x7f9a0d1e3d82 usr bin python pyeval evalframeex 0x578d 0x4c166d usr bin python pyeval evalcodeex 0x306 0x4b9b66 usr bin python 0x4d57a3 usr bin python pyobject call 0x3e 0x4a587e usr bin python pyeval evalframeex 0x263e 0x4be51e usr bin python pyeval evalcodeex 0x306 0x4b9b66 usr bin python pyeval evalframeex 0x58e6 0x4c17c6 usr bin python pyeval evalframeex 0x553f 0x4c141f usr bin python pyeval evalcodeex 0x306 0x4b9b66 usr bin python pyeval evalframeex 0x58e6 0x4c17c6 usr bin python pyeval evalcodeex 0x306 0x4b9b66 usr bin python pyeval evalframeex 0x58e6 0x4c17c6 usr bin python pyeval evalframeex 0x553f 0x4c141f usr bin python pyeval evalframeex 0x553f 0x4c141f usr bin python pyeval evalcodeex 0x306 0x4b9b66 usr bin python 0x4eb69f usr bin python pyrun fileexflags 0x82 0x4e58f2 usr bin python 0x54aae7 usr bin python pyeval evalframeex 0x5f3e 0x4c1e1e usr bin python pyeval evalcodeex 0x306 0x4b9b66 usr bin python pyeval evalframeex 0x58e6 0x4c17c6 usr bin python pyeval evalframeex 0x553f 0x4c141f usr bin python pyeval evalcodeex 0x306 0x4b9b66 usr bin python 0x4eb69f usr bin python pyrun fileexflags 0x82 0x4e58f2 usr bin python pyrun simplefileexflags 0x186 0x4e41a6 usr bin python py main 0x54e 0x4938ce lib x86 64 linux gnu libc.so.6 libc start main 0xf0 0x7f9a76465830 usr bin python start 0x29 0x493299 memory map 00400000 006de000 r xp 00000000 08 21 62529595 usr bin python2.7 008dd000 008de000 r p 002dd000 08 21 62529595 usr bin python2.7 008de000 00955000 rw p 002de000 08 21 62529595 usr bin python2.7 00955000 00978000 rw p 00000000 00 00 0 00fae000 2d86a000 rw p 00000000 00 00 0 heap 200000000 200200000 rw 00000000 00 06 815 dev nvidiactl 200200000 200400000 p 00000000 00 00 0 200400000 200404000 rw 00000000 00 06 815 dev nvidiactl 200404000 200600000 p 00000000 00 00 0 200600000 200a00000 rw 00000000 00 06 815 dev nvidiactl 200a00000 201800000 p 00000000 00 00 0 201800000 201804000 rw 00000000 00 06 815 dev nvidiactl 201804000 201a00000 p 00000000 00 00 0 201a00000 201e00000 rw 00000000 00 06 815 dev nvidiactl 201e00000 202c00000 p 00000000 00 00 0 202c00000 202c04000 rw 00000000 00 06 815 dev nvidiactl 202c04000 202e00000 p 00000000 00 00 0 202e00000 203200000 rw 00000000 00 06 815 dev nvidiactl 203200000 204000000 p 00000000 00 00 0 204000000 204004000 rw 00000000 00 06 815 dev nvidiactl 204004000 204200000 p 00000000 00 00 0 204200000 204600000 rw 00000000 00 06 815 dev nvidiactl 204600000 205400000 p 00000000 00 00 0 205400000 205404000 rw 00000000 00 06 815 dev nvidiactl 205404000 205600000 p 00000000 00 00 0 205600000 205a00000 rw 00000000 00 06 815 dev nvidiactl 205a00000 206800000 p 00000000 00 00 0 206800000 206804000 rw 00000000 00 06 815 dev nvidiactl 206804000 206a00000 p 00000000 00 00 0 206a00000 206e00000 rw 00000000 00 06 815 dev nvidiactl 206e00000 207c00000 p 00000000 00 00 0 207c00000 207c04000 rw 00000000 00 06 815 dev nvidiactl 207c04000 207e00000 p 00000000 00 00 0 207e00000 208200000 rw 00000000 00 06 815 dev nvidiactl 208200000 209000000 p 00000000 00 00 0 209000000 209004000 rw 00000000 00 06 815 dev nvidiactl 209004000 209200000 p 00000000 00 00 0 209200000 209600000 rw 00000000 00 06 815 dev nvidiactl 209600000 20a400000 p 00000000 00 00 0 20a400000 20a404000 rw 00000000 00 06 815 dev nvidiactl 20a404000 20a600000 p 00000000 00 00 0 20a600000 20aa00000 rw 00000000 00 06 815 dev nvidiactl 20aa00000 20aa04000 rw 00000000 00 06 815 dev nvidiactl 20aa04000 20ac00000 p 00000000 00 00 0 20ac00000 20b000000 rw 00000000 00 06 815 dev nvidiactl 20b000000 20b004000 rw 00000000 00 06 815 dev nvidiactl 20b004000 20b200000 p 00000000 00 00 0 20b200000 20b600000 rw 00000000 00 06 815 dev nvidiactl 20b600000 20b604000 rw 00000000 00 06 815 dev nvidiactl 20b604000 20b800000 p 00000000 00 00 0 20b800000 20bc00000 rw 00000000 00 06 815 dev nvidiactl 20bc00000 20bc04000 rw 00000000 00 06 815 dev nvidiactl 20bc04000 20be00000 p 00000000 00 00 0 20be00000 20c200000 rw 00000000 00 06 815 dev nvidiactl 20c200000 20c204000 rw 00000000 00 06 815 dev nvidiactl 20c204000 20c400000 p 00000000 00 00 0 20c400000 20c800000 rw 00000000 00 06 815 dev nvidiactl 20c800000 20c804000 rw 00000000 00 06 815 dev nvidiactl 20c804000 20ca00000 p 00000000 00 00 0 20ca00000 20ce00000 rw 00000000 00 06 815 dev nvidiactl 20ce00000 20ce04000 rw 00000000 00 06 815 dev nvidiactl 20ce04000 20d000000 p 00000000 00 00 0 20d000000 20d400000 rw 00000000 00 06 815 dev nvidiactl 20d400000 20d600000 p 00000000 00 00 0 20d600000 20d800000 rw 00000000 00 06 815 dev nvidiactl 20d800000 c00200000 p 00000000 00 00 0 10000000000 10b04000000 p 00000000 00 00 0 7f9823829000 7f9834000000 rw p 00000000 00 00 0 7f9834000000 7f9834022000 rw p 00000000 00 00 0 7f9834022000 7f9838000000 p 00000000 00 00 0 7f9838000000 7f983a08a000 rw p 00000000 00 00 0 7f983a08a000 7f983c000000 p 00000000 00 00 0 7f983c000000 7f983c022000 rw p 00000000 00 00 0 7f983c022000 7f9840000000 p 00000000 00 00 0 7f9842807000 7f9849009000 rw p 00000000 00 00 0 7f984d7fe000 7f9854000000 rw p 00000000 00 00 0 7f9854000000 7f98569b9000 rw p 00000000 00 00 0 7f98569b9000 7f9858000000 p 00000000 00 00 0 7f985c000000 7f985db22000 rw p 00000000 00 00 0 7f985db22000 7f9860000000 p 00000000 00 00 0 7f9864000000 7f9866f05000 rw p 00000000 00 00 0 7f9866f05000 7f9868000000 p 00000000 00 00 0 7f9868000000 7f986be05000 rw p 00000000 00 00 0 7f986be05000 7f986c000000 p 00000000 00 00 0 7f986c000000 7f986ff25000 rw p 00000000 00 00 0 7f986ff25000 7f9870000000 p 00000000 00 00 0 7f9870200000 7f9870800000 p 00000000 00 00 0 7f98709fd000 7f98709fe000 p 00000000 00 00 0 7f98709fe000 7f98711fe000 rw p 00000000 00 00 0 7f98711fe000 7f98711ff000 p 00000000 00 00 0 7f98711ff000 7f98719ff000 rw p 00000000 00 00 0 7f98721fd000 7f98721fe000 p 00000000 00 00 0 7f98721fe000 7f98729fe000 rw p 00000000 00 00 0 7f98729fe000 7f98729ff000 p 00000000 00 00 0 7f98729ff000 7f98731ff000 rw p 00000000 00 00 0 7f98731ff000 7f9873200000 p 00000000 00 00 0 7f9873200000 7f9873a00000 rw p 00000000 00 00 0 7f9873a00000 7f9874000000 p 00000000 00 00 0 7f9874000000 7f9877da7000 rw p 00000000 00 00 0 7f9877da7000 7f9878000000 p 00000000 00 00 0 7f9878200000 7f9878400000 p 00000000 00 00 0 7f98785ff000 7f9878600000 p 00000000 00 00 0 7f9878600000 7f9878e00000 rw p 00000000 00 00 0 7f9878e00000 7f9894000000 p 00000000 00 00 0 7f9894000000 7f9898091000 rw p 00000000 00 00 0 7f9898091000 7f989c000000 p 00000000 00 00 0 7f989c000000 7f989ffff000 rw p 00000000 00 00 0 7f989ffff000 7f98a0000000 p 00000000 00 00 0 7f98a0000000 7f98a3f49000 rw p 00000000 00 00 0 7f98a3f49000 7f98a4000000 p 00000000 00 00 0 7f98a4000000 7f98a7fce000 rw p 00000000 00 00 0 7f98a7fce000 7f98a8000000 p 00000000 00 00 0 7f98a8000000 7f98abfee000 rw p 00000000 00 00 0 7f98abfee000 7f98ac000000 p 00000000 00 00 0 7f98ac000000 7f98aff87000 rw p 00000000 00 00 0 7f98aff87000 7f98b0000000 p 00000000 00 00 0 7f98b0000000 7f98b4000000 rw p 00000000 00 00 0 7f98b4000000 7f98b7efc000 rw p 00000000 00 00 0 7f98b7efc000 7f98b8000000 p 00000000 00 00 0 7f98b8000000 7f98bc000000 rw p 00000000 00 00 0 7f98bc000000 7f98bd0a3000 rw p 00000000 00 00 0 7f98bd0a3000 7f98c0000000 p 00000000 00 00 0 7f98c0000000 7f98c4000000 rw p 00000000 00 00 0 7f98c4000000 7f98c7ffe000 rw p 00000000 00 00 0 7f98c7ffe000 7f98c8000000 p 00000000 00 00 0 7f98c8000000 7f98cbffd000 rw p 00000000 00 00 0 7f98cbffd000 7f98cc000000 p 00000000 00 00 0 7f98cc000000 7f98d0000000 rw p 00000000 00 00 0 7f98d0000000 7f98d8000000 p 00000000 00 00 0 7f98d8000000 7f98d8021000 rw p 00000000 00 00 0 7f98d8021000 7f98dc000000 p 00000000 00 00 0 7f98dc200000 7f98e0000000 p 00000000 00 00 0 7f98e0000000 7f98e0021000 rw p 00000000 00 00 0 7f98e0021000 7f98e4000000 p 00000000 00 00 0 7f98e4200000 7f98e4e00000 p 00000000 00 00 0 7f98e4ffe000 7f98eaffe000 p 00000000 00 00 0 7f98eaffe000 7f98f4000000 rw p 00000000 00 00 0 7f98f4000000 7f98f4022000 rw p 00000000 00 00 0 7f98f4022000 7f98f8000000 p 00000000 00 00 0 7f98f8000000 7f98f8022000 rw p 00000000 00 00 0 7f98f8022000 7f98fc000000 p 00000000 00 00 0 7f98fc000000 7f98fc022000 rw p 00000000 00 00 0 7f98fc022000 7f9900000000 p 00000000 00 00 0 7f9900200000 7f9900800000 p 00000000 00 00 0 7f99009fc000 7f9904000000 rw p 00000000 00 00 0 7f9904000000 7f9904022000 rw p 00000000 00 00 0 7f9904022000 7f9908000000 p 00000000 00 00 0 7f9908248000 7f9909d4b000 rw p 00000000 00 00 0 7f9909d4b000 7f9909d54000 r xp 00000000 08 21 62787974 usr lib python2.7 lib dynload bz2.x86 64 linux gnu.so 7f9909d54000 7f9909f53000 p 00009000 08 21 62787974 usr lib python2.7 lib dynload bz2.x86 64 linux gnu.so 7f9909f53000 7f9909f54000 r p 00008000 08 21 62787974 usr lib python2.7 lib dynload bz2.x86 64 linux gnu.so 7f9909f54000 7f9909f56000 rw p 00009000 08 21 62787974 usr lib python2.7 lib dynload bz2.x86 64 linux gnu.so 7f9909f56000 7f9909f57000 p 00000000 00 00 0 7f9909f57000 7f990a757000 rw p 00000000 00 00 0 7f990a757000 7f990a767000 r xp 00000000 08 21 2233737 usr lib python2.7 dist packages h5py h5l.x86 64 linux gnu.so 7f990a767000 7f990a966000 p 00010000 08 21 2233737 usr lib python2.7 dist packages h5py h5l.x86 64 linux gnu.so 7f990a966000 7f990a967000 r p 0000f000 08 21 2233737 usr lib python2.7 dist packages h5py h5l.x86 64 linux gnu.so 7f990a967000 7f990a96a000 rw p 00010000 08 21 2233737 usr lib python2.7 dist packages h5py h5l.x86 64 linux gnu.so 7f990a96a000 7f990a97b000 r xp 00000000 08 21 2233672 usr lib python2.7 dist packages h5py h5o.x86 64 linux gnu.so 7f990a97b000 7f990ab7a000 p 00011000 08 21 2233672 usr lib python2.7 dist packages h5py h5o.x86 64 linux gnu.so 7f990ab7a000 7f990ab7b000 r p 00010000 08 21 2233672 usr lib python2.7 dist packages h5py h5o.x86 64 linux gnu.so 7f990ab7b000 7f990ab7e000 rw p 00011000 08 21 2233672 usr lib python2.7 dist packages h5py h5o.x86 64 linux gnu.so 7f990ab7e000 7f990ab7f000 rw p 00000000 00 00 0 7f990ab7f000 7f990ab83000 r xp 00000000 08 21 2233671 usr lib python2.7 dist packages h5py h5fd.x86 64 linux gnu.so 7f990ab83000 7f990ad82000 p 00004000 08 21 2233671 usr lib python2.7 dist packages h5py h5fd.x86 64 linux gnu.so 7f990ad82000 7f990ad83000 r p 00003000 08 21 2233671 usr lib python2.7 dist packages h5py h5fd.x86 64 linux gnu.so 7f990ad83000 7f990ad84000 rw p 00004000 08 21 2233671 usr lib python2.7 dist packages h5py h5fd.x86 64 linux gnu.so 7f990ad84000 7f990ad8f000 r xp 00000000 08 21 2233717 usr lib python2.7 dist packages h5py h5i.x86 64 linux gnu.so 7f990ad8f000 7f990af8e000 p 0000b000 08 21 2233717 usr lib python2.7 dist packages h5py h5i.x86 64 linux gnu.so 7f990af8e000 7f990af8f000 r p 0000a000 08 21 2233717 usr lib python2.7 dist packages h5py h5i.x86 64 linux gnu.so 7f990af8f000 7f990af91000 rw p 0000b000 08 21 2233717 usr lib python2.7 dist packages h5py h5i.x86 64 linux gnu.so 7f990af91000 7f990afa7000 r xp 00000000 08 21 2233684 usr lib python2.7 dist packages h5py h5g.x86 64 linux gnu.so 7f990afa7000 7f990b1a6000 p 00016000 08 21 2233684 usr lib python2.7 dist packages h5py h5g.x86 64 linux gnu.so 7f990b1a6000 7f990b1a7000 r p 00015000 08 21 2233684 usr lib python2.7 dist packages h5py h5g.x86 64 linux gnu.so 7f990b1a7000 7f990b1aa000 rw p 00016000 08 21 2233684 usr lib python2.7 dist packages h5py h5g.x86 64 linux gnu.so 7f990b1aa000 7f990b1bf000 r xp 00000000 08 21 2233735 usr lib python2.7 dist packages h5py h5f.x86 64 linux gnu.so 7f990b1bf000 7f990b3be000 p 00015000 08 21 2233735 usr lib python2.7 dist packages h5py h5f.x86 64 linux gnu.so 7f990b3be000 7f990b3bf000 r p 00014000 08 21 2233735 usr lib python2.7 dist packages h5py h5f.x86 64 linux gnu.so 7f990b3bf000 7f990b3c2000 rw p 00015000 08 21 2233735 usr lib python2.7 dist packages h5py h5f.x86 64 linux gnu.so 7f990b3c2000 7f990b3c3000 rw p 00000000 00 00 0 7f990b3c3000 7f990b3d0000 r xp 00000000 08 21 2233673 usr lib python2.7 dist packages h5py h5ds.x86 64 linux gnu.so 7f990b3d0000 7f990b5cf000 p 0000d000 08 21 2233673 usr lib python2.7 dist packages h5py h5ds.x86 64 linux gnu.so 7f990b5cf000 7f990b5d0000 r p 0000c000 08 21 2233673 usr lib python2.7 dist packages h5py h5ds.x86 64 linux gnu.so 7f990b5d0000 7f990b5d2000 rw p 0000d000 08 21 2233673 usr lib python2.7 dist packages h5py h5ds.x86 64 linux gnu.so 7f990b5d2000 7f990b5e6000 r xp 00000000 08 21 2233716 usr lib python2.7 dist packages h5py h5d.x86 64 linux gnu.so 7f990b5e6000 7f990b7e5000 p 00014000 08 21 2233716 usr lib python2.7 dist packages h5py h5d.x86 64 linux gnu.so 7f990b7e5000 7f990b7e6000 r p 00013000 08 21 2233716 usr lib python2.7 dist packages h5py h5d.x86 64 linux gnu.so 7f990b7e6000 7f990b7e9000 rw p 00014000 08 21 2233716 usr lib python2.7 dist packages h5py h5d.x86 64 linux gnu.so 7f990b7e9000 7f990b7f0000 r xp 00000000 08 21 2233682 usr lib python2.7 dist packages h5py proxy.x86 64 linux gnu.so 7f990b7f0000 7f990b9ef000 p 00007000 08 21 2233682 usr lib python2.7 dist packages h5py proxy.x86 64 linux gnu.so 7f990b9ef000 7f990b9f0000 r p 00006000 08 21 2233682 usr lib python2.7 dist packages h5py proxy.x86 64 linux gnu.so 7f990b9f0000 7f990b9f1000 rw p 00007000 08 21 2233682 usr lib python2.7 dist packages h5py proxy.x86 64 linux gnu.so 7f990b9f1000 7f990b9fc000 r xp 00000000 08 21 2233676 usr lib python2.7 dist packages h5py h5ac.x86 64 linux gnu.so 7f990b9fc000 7f990bbfb000 p 0000b000 08 21 2233676 usr lib python2.7 dist packages h5py h5ac.x86 64 linux gnu.so 7f990bbfb000 7f990bbfc000 r p 0000a000 08 21 2233676 usr lib python2.7 dist packages h5py h5ac.x86 64 linux gnu.so 7f990bbfc000 7f990bbfd000 rw p 0000b000 08 21 2233676 usr lib python2.7 dist packages h5py h5ac.x86 64 linux gnu.so 7f990bbfd000 7f990bc25000 r xp 00000000 08 21 2233678 usr lib python2.7 dist packages h5py h5p.x86 64 linux gnu.so 7f990bc25000 7f990be25000 p 00028000 08 21 2233678 usr lib python2.7 dist packages h5py h5p.x86 64 linux gnu.so 7f990be25000 7f990be26000 r p 00028000 08 21 2233678 usr lib python2.7 dist packages h5py h5p.x86 64 linux gnu.so 7f990be26000 7f990be2e000 rw p 00029000 08 21 2233678 usr lib python2.7 dist packages h5py h5p.x86 64 linux gnu.so 7f990be2e000 7f990be2f000 rw p 00000000 00 00 0 7f990be2f000 7f990be43000 r xp 00000000 08 21 2233718 usr lib python2.7 dist packages h5py h5s.x86 64 linux gnu.so 7f990be43000 7f990c042000 p 00014000 08 21 2233718 usr lib python2.7 dist packages h5py h5s.x86 64 linux gnu.so 7f990c042000 7f990c043000 r p 00013000 08 21 2233718 usr lib python2.7 dist packages h5py h5s.x86 64 linux gnu.so 7f990c043000 7f990c046000 rw p 00014000 08 21 2233718 usr lib python2.7 dist packages h5py h5s.x86 64 linux gnu.so 7f990c046000 7f990c047000 rw p 00000000 00 00 0 7f990c047000 7f990c05c000 r xp 00000000 08 21 2233681 usr lib python2.7 dist packages h5py h5a.x86 64 linux gnu.so 7f990c05c000 7f990c25c000 p 00015000 08 21 2233681 usr lib python2.7 dist packages h5py h5a.x86 64 linux gnu.so 7f990c25c000 7f990c25d000 r p 00015000 08 21 2233681 usr lib python2.7 dist packages h5py h5a.x86 64 linux gnu.so 7f990c25d000 7f990c260000 rw p 00016000 08 21 2233681 usr lib python2.7 dist packages h5py h5a.x86 64 linux gnu.so 7f990c260000 7f990c267000 r xp 00000000 08 21 2233721 usr lib python2.7 dist packages h5py h5z.x86 64 linux gnu.so 7f990c267000 7f990c466000 p 00007000 08 21 2233721 usr lib python2.7 dist packages h5py h5z.x86 64 linux gnu.so 7f990c466000 7f990c467000 r p 00006000 08 21 2233721 usr lib python2.7 dist packages h5py h5z.x86 64 linux gnu.so 7f990c467000 7f990c468000 rw p 00007000 08 21 2233721 usr lib python2.7 dist packages h5py h5z.x86 64 linux gnu.so 7f990c468000 7f990c469000 rw p 00000000 00 00 0 7f990c469000 7f990c47a000 r xp 00000000 08 21 2233675 usr lib python2.7 dist packages h5py h5.x86 64 linux gnu.so 7f990c47a000 7f990c679000 p 00011000 08 21 2233675 usr lib python2.7 dist packages h5py h5.x86 64 linux gnu.so 7f990c679000 7f990c67a000 r p 00010000 08 21 2233675 usr lib python2.7 dist packages h5py h5.x86 64 linux gnu.so 7f990c67a000 7f990c67c000 rw p 00011000 08 21 2233675 usr lib python2.7 dist packages h5py h5.x86 64 linux gnu.so 7f990c67c000 7f990c67d000 rw p 00000000 00 00 0 7f990c67d000 7f990c686000 r xp 00000000 08 21 2233720 usr lib python2.7 dist packages h5py utils.x86 64 linux gnu.so 7f990c686000 7f990c885000 p 00009000 08 21 2233720 usr lib python2.7 dist packages h5py utils.x86 64 linux gnu.so 7f990c885000 7f990c886000 r p 00008000 08 21 2233720 usr lib python2.7 dist packages h5py utils.x86 64 linux gnu.so 7f990c886000 7f990c887000 rw p 00009000 08 21 2233720 usr lib python2.7 dist packages h5py utils.x86 64 linux gnu.so 7f990c887000 7f990c8cb000 r xp 00000000 08 21 2233734 usr lib python2.7 dist packages h5py h5t.x86 64 linux gnu.so 7f990c8cb000 7f990caca000 p 00044000 08 21 2233734 usr lib python2.7 dist packages h5py h5t.x86 64 linux gnu.so 7f990caca000 7f990cacb000 r p 00043000 08 21 2233734 usr lib python2.7 dist packages h5py h5t.x86 64 linux gnu.so 7f990cacb000 7f990cad5000 rw p 00044000 08 21 2233734 usr lib python2.7 dist packages h5py h5t.x86 64 linux gnu.so 7f990cad5000 7f990caf3000 r xp 00000000 08 21 1053210 usr lib x86 64 linux gnu libhdf5 serial hl.so.10.0.2 7f990caf3000 7f990ccf2000 p 0001e000 08 21 1053210 usr lib x86 64 linux gnu libhdf5 serial hl.so.10.0.2 7f990ccf2000 7f990ccf3000 r p 0001d000 08 21 1053210 usr lib x86 64 linux gnu libhdf5 serial hl.so.10.0.2 7f990ccf3000 7f990ccf4000 rw p 0001e000 08 21 1053210 usr lib x86 64 linux gnu libhdf5 serial hl.so.10.0.2 7f990ccf4000 7f990ccf5000 rw p 00000000 00 00 0 7f990ccf5000 7f990cd1c000 r xp 00000000 08 21 2233674 usr lib python2.7 dist packages h5py defs.x86 64 linux gnu.so 7f990cd1c000 7f990cf1c000 p 00027000 08 21 2233674 usr lib python2.7 dist packages h5py defs.x86 64 linux gnu.so 7f990cf1c000 7f990cf1d000 r p 00027000 08 21 2233674 usr lib python2.7 dist packages h5py defs.x86 64 linux gnu.so 7f990cf1d000 7f990cf1f000 rw p 00028000 08 21 2233674 usr lib python2.7 dist packages h5py defs.x86 64 linux gnu.so 7f990cf1f000 7f990cf35000 r xp 00000000 08 21 2233736 usr lib python2.7 dist packages h5py objects.x86 64 linux gnu.so 7f990cf35000 7f990d134000 p 00016000 08 21 2233736 usr lib python2.7 dist packages h5py objects.x86 64 linux gnu.so 7f990d134000 7f990d135000 r p 00015000 08 21 2233736 usr lib python2.7 dist packages h5py objects.x86 64 linux gnu.so 7f990d135000 7f990d138000 rw p 00016000 08 21 2233736 usr lib python2.7 dist packages h5py objects.x86 64 linux gnu.so 7f990d138000 7f990d142000 r xp 00000000 08 21 2233715 usr lib python2.7 dist packages h5py h5r.x86 64 linux gnu.so 7f990d142000 7f990d341000 p 0000a000 08 21 2233715 usr lib python2.7 dist packages h5py h5r.x86 64 linux gnu.so 7f990d341000 7f990d342000 r p 00009000 08 21 2233715 usr lib python2.7 dist packages h5py h5r.x86 64 linux gnu.so 7f990d342000 7f990d344000 rw p 0000a000 08 21 2233715 usr lib python2.7 dist packages h5py h5r.x86 64 linux gnu.so 7f990d344000 7f990d351000 r xp 00000000 08 21 2233683 usr lib python2.7 dist packages h5py conv.x86 64 linux gnu.so 7f990d351000 7f990d550000 p 0000d000 08 21 2233683 usr lib python2.7 dist packages h5py conv.x86 64 linux gnu.so 7f990d550000 7f990d551000 r p 0000c000 08 21 2233683 usr lib python2.7 dist packages h5py conv.x86 64 linux gnu.so 7f990d551000 7f990d552000 rw p 0000d000 08 21 2233683 usr lib python2.7 dist packages h5py conv.x86 64 linux gnu.so 7f990d552000 7f990d559000 r xp 00000000 08 21 1053203 usr lib x86 64 linux gnu libaec.so.0.0.3 7f990d559000 7f990d758000 p 00007000 08 21 1053203 usr lib x86 64 linux gnu libaec.so.0.0.3 7f990d758000 7f990d759000 r p 00006000 08 21 1053203 usr lib x86 64 linux gnu libaec.so.0.0.3 7f990d759000 7f990d75a000 rw p 00007000 08 21 1053203 usr lib x86 64 linux gnu libaec.so.0.0.3 7f990d75a000 7f990d75c000 r xp 00000000 08 21 1053205 usr lib x86 64 linux gnu libsz.so.2.0.1 7f990d75c000 7f990d95b000 p 00002000 08 21 1053205 usr lib x86 64 linux gnu libsz.so.2.0.1 7f990d95b000 7f990d95c000 r p 00001000 08 21 1053205 usr lib x86 64 linux gnu libsz.so.2.0.1 7f990d95c000 7f990d95d000 rw p 00002000 08 21 1053205 usr lib x86 64 linux gnu libsz.so.2.0.1 7f990d95d000 7f990dbf1000 r xp 00000000 08 21 1053208 usr lib x86 64 linux gnu libhdf5 serial.so.10.1.0 7f990dbf1000 7f990ddf0000 p 00294000 08 21 1053208 usr lib x86 64 linux gnu libhdf5 serial.so.10.1.0 7f990ddf0000 7f990ddf5000 r p 00293000 08 21 1053208 usr lib x86 64 linux gnu libhdf5 serial.so.10.1.0 7f990ddf5000 7f990ddfa000 rw p 00298000 08 21 1053208 usr lib x86 64 linux gnu libhdf5 serial.so.10.1.0 7f990ddfa000 7f990ddfb000 rw p 00000000 00 00 0 7f990ddfb000 7f990de03000 r xp 00000000 08 21 2233738 usr lib python2.7 dist packages h5py errors.x86 64 linux gnu.so 7f990de03000 7f990e002000 p 00008000 08 21 2233738 usr lib python2.7 dist packages h5py errors.x86 64 linux gnu.so 7f990e002000 7f990e003000 r p 00007000 08 21 2233738 usr lib python2.7 dist packages h5py errors.x86 64 linux gnu.so 7f990e003000 7f990e004000 rw p 00008000 08 21 2233738 usr lib python2.7 dist packages h5py errors.x86 64 linux gnu.so 7f990e004000 7f993a004000 rw p 00000000 00 00 0 7f993a004000 7f993a005000 p 00000000 00 00 0 7f993a005000 7f993a805000 rw p 00000000 00 00 0 7f993a805000 7f993a806000 p 00000000 00 00 0 7f993a806000 7f993b006000 rw p 00000000 00 00 0 7f993b006000 7f993b007000 p 00000000 00 00 0 7f993b007000 7f993b807000 rw p 00000000 00 00 0 7f993b807000 7f993b808000 p 00000000 00 00 0 7f993b808000 7f993c008000 rw p 00000000 00 00 0 7f993c400000 7f993c600000 p 00000000 00 00 0 7f993c60a000 7f993f80b000 rw p 00000000 00 00 0 7f993fc00000 7f993fe00000 p 00000000 00 00 0 7f993fe00000 7f9940000000 rw 00000000 00 05 206094999 dev zero deleted 7f994000c000 7f994200c000 rw p 00000000 00 00 0 7f9942400000 7f9942600000 p 00000000 00 00 0 7f994260d000 7f994580f000 rw p 00000000 00 00 0 7f9945c00000 7f9946000000 p 00000000 00 00 0 7f9946010000 7f9948010000 rw p 00000000 00 00 0 7f9948216000 7f994f81b000 rw p 00000000 00 00 0 7f994fc00000 7f994fe00000 rw 00000000 00 05 206094998 dev zero deleted 7f994fe00000 7f9950000000 p 00000000 00 00 0 7f995001c000 7f995601c000 rw p 00000000 00 00 0 7f995601c000 7f995601d000 p 00000000 00 00 0 7f995601d000 7f995881d000 rw p 00000000 00 00 0 7f995881d000 7f995881e000 p 00000000 00 00 0 7f995881e000 7f995f01e000 rw p 00000000 00 00 0 7f995f400000 7f995f600000 p 00000000 00 00 0 7f995f71f000 7f9962020000 rw p 00000000 00 00 0 7f9962400000 7f9962800000 p 00000000 00 00 0 7f9962821000 7f9964821000 rw p 00000000 00 00 0 7f9964c00000 7f9965000000 p 00000000 00 00 0 7f9965022000 7f9967022000 rw p 00000000 00 00 0 7f9967400000 7f9967800000 p 00000000 00 00 0 7f9967823000 7f9969823000 rw p 00000000 00 00 0 7f9969c00000 7f9969ed6000 rw 00000000 00 06 815 dev nvidiactl 7f9969ed6000 7f996a000000 p 00000000 00 00 0 7f996a024000 7f996c024000 rw p 00000000 00 00 0 7f996c400000 7f996c600000 rw 00000000 00 06 815 dev nvidiactl 7f996c600000 7f996c800000 rw 00000000 00 05 206139044 dev zero deleted 7f996c825000 7f996e825000 rw p 00000000 00 00 0 7f996ec00000 7f996ee00000 rw 00000000 00 06 815 dev nvidiactl 7f996ee00000 7f996f000000 rw 00000000 00 05 206139043 dev zero deleted 7f996f026000 7f9971026000 rw p 00000000 00 00 0 7f9971026000 7f9971027000 p 00000000 00 00 0 7f9971027000 7f9973827000 rw p 00000000 00 00 0 7f9973c00000 7f9973e00000 p 00000000 00 00 0 7f9973e29000 7f997902a000 rw p 00000000 00 00 0 7f997902d000 7f997d02e000 rw p 00000000 00 00 0 7f997d02e000 7f997d02f000 p 00000000 00 00 0 7f997d02f000 7f997f82f000 rw p 00000000 00 00 0 7f997f82f000 7f997f830000 p 00000000 00 00 0 7f997f830000 7f9984030000 rw p 00000000 00 00 0 7f9984400000 7f9984600000 p 00000000 00 00 0 7f9984731000 7f9987032000 rw p 00000000 00 00 0 7f9987171000 7f998c034000 rw p 00000000 00 00 0 7f998c0b2000 7f9996038000 rw p 00000000 00 00 0 7f9996038000 7f99978ee000 r xp 00000000 08 21 62790680 usr lib x86 64 linux gnu libicudata.so.55.1 7f99978ee000 7f9997aed000 p 018b6000 08 21 62790680 usr lib x86 64 linux gnu libicudata.so.55.1 7f9997aed000 7f9997aee000 r p 018b5000 08 21 62790680 usr lib x86 64 linux gnu libicudata.so.55.1 7f9997aee000 7f9997aef000 rw p 018b6000 08 21 62790680 usr lib x86 64 linux gnu libicudata.so.55.1 7f9997aef000 7f9997af2000 r xp 00000000 08 21 45488001 lib x86 64 linux gnu libkeyutils.so.1.5 7f9997af2000 7f9997cf1000 p 00003000 08 21 45488001 lib x86 64 linux gnu libkeyutils.so.1.5 7f9997cf1000 7f9997cf2000 r p 00002000 08 21 45488001 lib x86 64 linux gnu libkeyutils.so.1.5 7f9997cf2000 7f9997cf3000 rw p 00003000 08 21 45488001 lib x86 64 linux gnu libkeyutils.so.1.5 7f9997cf3000 7f9997e72000 r xp 00000000 08 21 62790694 usr lib x86 64 linux gnu libicuuc.so.55.1 7f9997e72000 7f9998072000 p 0017f000 08 21 62790694 usr lib x86 64 linux gnu libicuuc.so.55.1 7f9998072000 7f9998082000 r p 0017f000 08 21 62790694 usr lib x86 64 linux gnu libicuuc.so.55.1 7f9998082000 7f9998083000 rw p 0018f000 08 21 62790694 usr lib x86 64 linux gnu libicuuc.so.55.1 7f9998083000 7f9998087000 rw p 00000000 00 00 0 7f9998087000 7f9998091000 r xp 00000000 08 21 45746719 usr lib x86 64 linux gnu libkrb5support.so.0.1 7f9998091000 7f9998290000 p 0000a000 08 21 45746719 usr lib x86 64 linux gnu libkrb5support.so.0.1 7f9998290000 7f9998291000 r p 00009000 08 21 45746719 usr lib x86 64 linux gnu libkrb5support.so.0.1 7f9998291000 7f9998292000 rw p 0000a000 08 21 45746719 usr lib x86 64 linux gnu libkrb5support.so.0.1 7f9998292000 7f9998295000 r xp 00000000 08 21 38015890 lib x86 64 linux gnu libcom err.so.2.1 7f9998295000 7f9998494000 p 00003000 08 21 38015890 lib x86 64 linux gnu libcom err.so.2.1 7f9998494000 7f9998495000 r p 00002000 08 21 38015890 lib x86 64 linux gnu libcom err.so.2.1 7f9998495000 7f9998496000 rw p 00003000 08 21 38015890 lib x86 64 linux gnu libcom err.so.2.1 7f9998496000 7f99984c2000 r xp 00000000 08 21 45746713 usr lib x86 64 linux gnu libk5crypto.so.3.1 7f99984c2000 7f99986c1000 p 0002c000 08 21 45746713 usr lib x86 64 linux gnu libk5crypto.so.3.1 7f99986c1000 7f99986c3000 r p 0002b000 08 21 45746713 usr lib x86 64 linux gnu libk5crypto.so.3.1 7f99986c3000 7f99986c4000 rw p 0002d000 08 21 45746713 usr lib x86 64 linux gnu libk5crypto.so.3.1 7f99986c4000 7f99986c5000 rw p 00000000 00 00 0 7f99986c5000 7f9998788000 r xp 00000000 08 21 45746717 usr lib x86 64 linux gnu libkrb5.so.3.3 7f9998788000 7f9998988000 p 000c3000 08 21 45746717 usr lib x86 64 linux gnu libkrb5.so.3.3 7f9998988000 7f9998995000 r p 000c3000 08 21 45746717 usr lib x86 64 linux gnu libkrb5.so.3.3 7f9998995000 7f9998997000 rw p 000d0000 08 21 45746717 usr lib x86 64 linux gnu libkrb5.so.3.3 7f9998997000 7f99989a9000 r xp 00000000 08 21 38015908 lib x86 64 linux gnu libgpg error.so.0.17.0 7f99989a9000 7f9998ba9000 p 00012000 08 21 38015908 lib x86 64 linux gnu libgpg error.so.0.17.0 7f9998ba9000 7f9998baa000 r p 00012000 08 21 38015908 lib x86 64 linux gnu libgpg error.so.0.17.0 7f9998baa000 7f9998bab000 rw p 00013000 08 21 38015908 lib x86 64 linux gnu libgpg error.so.0.17.0 7f9998bab000 7f9998bcc000 r xp 00000000 08 21 53355465 usr lib x86 64 linux gnu libgomp.so.1.0.0 7f9998bcc000 7f9998dcb000 p 00021000 08 21 53355465 usr lib x86 64 linux gnu libgomp.so.1.0.0 7f9998dcb000 7f9998dcc000 r p 00020000 08 21 53355465 usr lib x86 64 linux gnu libgomp.so.1.0.0 7f9998dcc000 7f9998dcd000 rw p 00021000 08 21 53355465 usr lib x86 64 linux gnu libgomp.so.1.0.0 7f9998dcd000 7f9998dd3000 r xp 00000000 08 21 62790576 usr lib x86 64 linux gnu libdatrie.so.1.3.3 7f9998dd3000 7f9998fd3000 p 00006000 08 21 62790576 usr lib x86 64 linux gnu libdatrie.so.1.3.3 7f9998fd3000 7f9998fd4000 r p 00006000 08 21 62790576 usr lib x86 64 linux gnu libdatrie.so.1.3.3 7f9998fd4000 7f9998fd5000 rw p 00007000 08 21 62790576 usr lib x86 64 linux gnu libdatrie.so.1.3.3 7f9998fd5000 7f9998ff9000 r xp 00000000 08 21 62790651 usr lib x86 64 linux gnu libgraphite2.so.3.0.1 7f9998ff9000 7f99991f8000 p 00024000 08 21 62790651 usr lib x86 64 linux gnu libgraphite2.so.3.0.1 7f99991f8000 7f99991fa000 r p 00023000 08 21 62790651 usr lib x86 64 linux gnu libgraphite2.so.3.0.1 7f99991fa000 7f99991fb000 rw p 00025000 08 21 62790651 usr lib x86 64 linux gnu libgraphite2.so.3.0.1 7f99991fb000 7f999920c000 r xp 00000000 08 21 45746737 usr lib x86 64 linux gnu libtasn1.so.6.5.1 7f999920c000 7f999940c000 p 00011000 08 21 45746737 usr lib x86 64 linux gnu libtasn1.so.6.5.1 7f999940c000 7f999940d000 r p 00011000 08 21 45746737 usr lib x86 64 linux gnu libtasn1.so.6.5.1 7f999940d000 7f999940e000 rw p 00012000 08 21 45746737 usr lib x86 64 linux gnu libtasn1.so.6.5.1 7f999940e000 7f999943f000 r xp 00000000 08 21 45746711 usr lib x86 64 linux gnu libidn.so.11.6.15 7f999943f000 7f999963f000 p 00031000 08 21 45746711 usr lib x86 64 linux gnu libidn.so.11.6.15 7f999963f000 7f9999640000 r p 00031000 08 21 45746711 usr lib x86 64 linux gnu libidn.so.11.6.15 7f9999640000 7f9999641000 rw p 00032000 08 21 45746711 usr lib x86 64 linux gnu libidn.so.11.6.15 7f9999641000 7f999969a000 r xp 00000000 08 21 45746728 usr lib x86 64 linux gnu libp11 kit.so.0.1.0 7f999969a000 7f9999899000 p 00059000 08 21 45746728 usr lib x86 64 linux gnu libp11 kit.so.0.1.0 7f9999899000 7f99998a3000 r p 00058000 08 21 45746728 usr lib x86 64 linux gnu libp11 kit.so.0.1.0 7f99998a3000 7f99998a5000 rw p 00062000 08 21 45746728 usr lib x86 64 linux gnu libp11 kit.so.0.1.0 7f99998a5000 7f9999a56000 r xp 00000000 08 21 62790962 usr lib x86 64 linux gnu libxml2.so.2.9.3 7f9999a56000 7f9999c55000 p 001b1000 08 21 62790962 usr lib x86 64 linux gnu libxml2.so.2.9.3 7f9999c55000 7f9999c5d000 r p 001b0000 08 21 62790962 usr lib x86 64 linux gnu libxml2.so.2.9.3 7f9999c5d000 7f9999c5f000 rw p 001b8000 08 21 62790962 usr lib x86 64 linux gnu libxml2.so.2.9.3 7f9999c5f000 7f9999c60000 rw p 00000000 00 00 0 7f9999c60000 7f9999cdf000 r xp 00000000 08 21 45746693 usr lib x86 64 linux gnu libgmp.so.10.3.0 7f9999cdf000 7f9999ede000 p 0007f000 08 21 45746693 usr lib x86 64 linux gnu libgmp.so.10.3.0 7f9999ede000 7f9999edf000 r p 0007e000 08 21 45746693 usr lib x86 64 linux gnu libgmp.so.10.3.0 7f9999edf000 7f9999ee0000 rw p 0007f000 08 21 45746693 usr lib x86 64 linux gnu libgmp.so.10.3.0 7f9999ee0000 7f9999f14000 r xp 00000000 08 21 45746726 usr lib x86 64 linux gnu libnettle.so.6.2 7f9999f14000 7f999a113000 p 00034000 08 21 45746726 usr lib x86 64 linux gnu libnettle.so.6.2 7f999a113000 7f999a115000 r p 00033000 08 21 45746726 usr lib x86 64 linux gnu libnettle.so.6.2 7f999a115000 7f999a116000 rw p 00035000 08 21 45746726 usr lib x86 64 linux gnu libnettle.so.6.2 7f999a116000 7f999a148000 r xp 00000000 08 21 45746707 usr lib x86 64 linux gnu libhogweed.so.4.2 7f999a148000 7f999a347000 p 00032000 08 21 45746707 usr lib x86 64 linux gnu libhogweed.so.4.2 7f999a347000 7f999a348000 r p 00031000 08 21 45746707 usr lib x86 64 linux gnu libhogweed.so.4.2 7f999a348000 7f999a349000 rw p 00032000 08 21 45746707 usr lib x86 64 linux gnu libhogweed.so.4.2 7f999a349000 7f999a390000 r xp 00000000 08 21 45746699 usr lib x86 64 linux gnu libgssapi krb5.so.2.2 7f999a390000 7f999a58f000 p 00047000 08 21 45746699 usr lib x86 64 linux gnu libgssapi krb5.so.2.2 7f999a58f000 7f999a591000 r p 00046000 08 21 45746699 usr lib x86 64 linux gnu libgssapi krb5.so.2.2 7f999a591000 7f999a593000 rw p 00048000 08 21 45746699 usr lib x86 64 linux gnu libgssapi krb5.so.2.2 7f999a593000 7f999a66a000 r xp 00000000 08 21 38015906 lib x86 64 linux gnu libgcrypt.so.20.0.5 7f999a66a000 7f999a86a000 p 000d7000 08 21 38015906 lib x86 64 linux gnu libgcrypt.so.20.0.5 7f999a86a000 7f999a86b000 r p 000d7000 08 21 38015906 lib x86 64 linux gnu libgcrypt.so.20.0.5 7f999a86b000 7f999a873000 rw p 000d8000 08 21 38015906 lib x86 64 linux gnu libgcrypt.so.20.0.5 7f999a873000 7f999a874000 rw p 00000000 00 00 0 7f999a874000 7f999a8ef000 r xp 00000000 08 21 62790799 usr lib x86 64 linux gnu liborc 0.4.so.0.25.0 7f999a8ef000 7f999aaee000 p 0007b000 08 21 62790799 usr lib x86 64 linux gnu liborc 0.4.so.0.25.0 7f999aaee000 7f999aaf0000 r p 0007a000 08 21 62790799 usr lib x86 64 linux gnu liborc 0.4.so.0.25.0 7f999aaf0000 7f999aaf4000 rw p 0007c000 08 21 62790799 usr lib x86 64 linux gnu liborc 0.4.so.0.25.0 7f999aaf4000 7f999aafb000 r xp 00000000 08 21 62790719 usr lib x86 64 linux gnu libogg.so.0.8.2 7f999aafb000 7f999acfb000 p 00007000 08 21 62790719 usr lib x86 64 linux gnu libogg.so.0.8.2 7f999acfb000 7f999acfc000 r p 00007000 08 21 62790719 usr lib x86 64 linux gnu libogg.so.0.8.2 7f999acfc000 7f999acfd000 rw p 00008000 08 21 62790719 usr lib x86 64 linux gnu libogg.so.0.8.2 7f999acfd000 7f999ad07000 r xp 00000000 08 21 62790717 usr lib x86 64 linux gnu libnuma.so.1.0.0 7f999ad07000 7f999af06000 p 0000a000 08 21 62790717 usr lib x86 64 linux gnu libnuma.so.1.0.0 7f999af06000 7f999af07000 r p 00009000 08 21 62790717 usr lib x86 64 linux gnu libnuma.so.1.0.0 7f999af07000 7f999af08000 rw p 0000a000 08 21 62790717 usr lib x86 64 linux gnu libnuma.so.1.0.0 7f999af08000 7f999af36000 r xp 00000000 08 21 62790869 usr lib x86 64 linux gnu libsoxr.so.0.1.1 7f999af36000 7f999b135000 p 0002e000 08 21 62790869 usr lib x86 64 linux gnu libsoxr.so.0.1.1 7f999b135000 7f999b137000 r p 0002d000 08 21 62790869 usr lib x86 64 linux gnu libsoxr.so.0.1.1 7f999b137000 7f999b138000 rw p 0002f000 08 21 62790869 usr lib x86 64 linux gnu libsoxr.so.0.1.1 7f999b138000 7f999b16d000 rw p 00000000 00 00 0 7f999b16d000 7f999b192000 r xp 00000000 08 21 62790920 usr lib x86 64 linux gnu libv4lconvert.so.0.0.0 7f999b192000 7f999b392000 p 00025000 08 21 62790920 usr lib x86 64 linux gnu libv4lconvert.so.0.0.0 7f999b392000 7f999b394000 r p 00025000 08 21 62790920 usr lib x86 64 linux gnu libv4lconvert.so.0.0.0 7f999b394000 7f999b395000 rw p 00027000 08 21 62790920 usr lib x86 64 linux gnu libv4lconvert.so.0.0.0 7f999b395000 7f999b3e7000 rw p 00000000 00 00 0 7f999b3e7000 7f999b449000 r xp 00000000 08 21 62790530 usr lib x86 64 linux gnu libxt.so.6.0.0 7f999b449000 7f999b649000 p 00062000 08 21 62790530 usr lib x86 64 linux gnu libxt.so.6.0.0 7f999b649000 7f999b64a000 r p 00062000 08 21 62790530 usr lib x86 64 linux gnu libxt.so.6.0.0 7f999b64a000 7f999b64f000 rw p 00063000 08 21 62790530 usr lib x86 64 linux gnu libxt.so.6.0.0 7f999b64f000 7f999b650000 rw p 00000000 00 00 0 7f999b650000 7f999b658000 r xp 00000000 08 21 62790892 usr lib x86 64 linux gnu libthai.so.0.2.4 7f999b658000 7f999b857000 p 00008000 08 21 62790892 usr lib x86 64 linux gnu libthai.so.0.2.4 7f999b857000 7f999b858000 r p 00007000 08 21 62790892 usr lib x86 64 linux gnu libthai.so.0.2.4 7f999b858000 7f999b859000 rw p 00008000 08 21 62790892 usr lib x86 64 linux gnu libthai.so.0.2.4 7f999b859000 7f999b8b5000 r xp 00000000 08 21 62790678 usr lib x86 64 linux gnu libharfbuzz.so.0.10000.1 7f999b8b5000 7f999bab5000 p 0005c000 08 21 62790678 usr lib x86 64 linux gnu libharfbuzz.so.0.10000.1 7f999bab5000 7f999bab6000 r p 0005c000 08 21 62790678 usr lib x86 64 linux gnu libharfbuzz.so.0.10000.1 7f999bab6000 7f999bab7000 rw p 0005d000 08 21 62790678 usr lib x86 64 linux gnu libharfbuzz.so.0.10000.1 7f999bab7000 7f999bace000 r xp 00000000 08 21 38015955 lib x86 64 linux gnu libresolv 2.23.so 7f999bace000 7f999bcce000 p 00017000 08 21 38015955 lib x86 64 linux gnu libresolv 2.23.so 7f999bcce000 7f999bccf000 r p 00017000 08 21 38015955 lib x86 64 linux gnu libresolv 2.23.so 7f999bccf000 7f999bcd0000 rw p 00018000 08 21 38015955 lib x86 64 linux gnu libresolv 2.23.so 7f999bcd0000 7f999bcd2000 rw p 00000000 00 00 0 7f999bcd2000 7f999bcf1000 r xp 00000000 08 21 38015961 lib x86 64 linux gnu libselinux.so.1 7f999bcf1000 7f999bef0000 p 0001f000 08 21 38015961 lib x86 64 linux gnu libselinux.so.1 7f999bef0000 7f999bef1000 r p 0001e000 08 21 38015961 lib x86 64 linux gnu libselinux.so.1 7f999bef1000 7f999bef2000 rw p 0001f000 08 21 38015961 lib x86 64 linux gnu libselinux.so.1 7f999bef2000 7f999bef4000 rw p 00000000 00 00 0 7f999bef4000 7f999befc000 r xp 00000000 08 21 62790950 usr lib x86 64 linux gnu libxcb render.so.0.0.0 7f999befc000 7f999c0fc000 p 00008000 08 21 62790950 usr lib x86 64 linux gnu libxcb render.so.0.0.0 7f999c0fc000 7f999c0fd000 r p 00008000 08 21 62790950 usr lib x86 64 linux gnu libxcb render.so.0.0.0 7f999c0fd000 7f999c0fe000 rw p 00009000 08 21 62790950 usr lib x86 64 linux gnu libxcb render.so.0.0.0 7f999c0fe000 7f99a3f2c000 r xp 00000000 08 21 50074698 usr local cuda 9.0 targets x86 64 linux lib libcufft.so.9.0.176 7f99a3f2c000 7f99a412c000 p 07e2e000 08 21 50074698 usr local cuda 9.0 targets x86 64 linux lib libcufft.so.9.0.176 7f99a412c000 7f99a413b000 rw p 07e2e000 08 21 50074698 usr local cuda 9.0 targets x86 64 linux lib libcufft.so.9.0.176 7f99a413b000 7f99a419f000 rw p 00000000 00 00 0 7f99a419f000 7f99b77d1000 r xp 00000000 08 21 54406239 usr lib x86 64 linux gnu libcudnn.so.7.6.0 7f99b77d1000 7f99b79d1000 p 13632000 08 21 54406239 usr lib x86 64 linux gnu libcudnn.so.7.6.0 7f99b79d1000 7f99b7a4c000 rw p 13632000 08 21 54406239 usr lib x86 64 linux gnu libcudnn.so.7.6.0 7f99b7a4c000 7f99b7ade000 rw p 00000000 00 00 0 7f99b7ade000 7f99bc48d000 r xp 00000000 08 21 50074704 usr local cuda 9.0 targets x86 64 linux lib libcusolver.so.9.0.176 7f99bc48d000 7f99bc68d000 p 049af000 08 21 50074704 usr local cuda 9.0 targets x86 64 linux lib libcusolver.so.9.0.176 7f99bc68d000 7f99bc6c7000 rw p 049af000 08 21 50074704 usr local cuda 9.0 targets x86 64 linux lib libcusolver.so.9.0.176 7f99bc6c7000 7f99bc6d9000 rw p 00000000 00 00 0 7f99bc6d9000 7f99beb62000 r xp 00000000 08 21 50074702 usr local cuda 9.0 targets x86 64 linux lib libcurand.so.9.0.176 7f99beb62000 7f99bed61000 p 02489000 08 21 50074702 usr local cuda 9.0 targets x86 64 linux lib libcurand.so.9.0.176 7f99bed61000 7f99c0133000 rw p 02488000 08 21 50074702 usr local cuda 9.0 targets x86 64 linux lib libcurand.so.9.0.176 7f99c0133000 7f99c063d000 rw p 00000000 00 00 0 7f99c063d000 7f99d02b3000 r xp 00000000 08 21 63967275 usr local lib python2.7 dist packages mxnet 1.3.0 py2.7.egg mxnet libmxnet.so 7f99d02b3000 7f99d04b3000 p 0fc76000 08 21 63967275 usr local lib python2.7 dist packages mxnet 1.3.0 py2.7.egg mxnet libmxnet.so 7f99d04b3000 7f99d04ed000 r p 0fc76000 08 21 63967275 usr local lib python2.7 dist packages mxnet 1.3.0 py2.7.egg mxnet libmxnet.so 7f99d04ed000 7f99d0511000 rw p 0fcb0000 08 21 63967275 usr local lib python2.7 dist packages mxnet 1.3.0 py2.7.egg mxnet libmxnet.so 7f99d0511000 7f99d1029000 rw p 00000000 00 00 0 7f99d1029000 7f99d102e000 r xp 00000000 08 21 62790496 usr lib x86 64 linux gnu libxdmcp.so.6.0.0 7f99d102e000 7f99d122d000 p 00005000 08 21 62790496 usr lib x86 64 linux gnu libxdmcp.so.6.0.0 7f99d122d000 7f99d122e000 r p 00004000 08 21 62790496 usr lib x86 64 linux gnu libxdmcp.so.6.0.0 7f99d122e000 7f99d122f000 rw p 00005000 08 21 62790496 usr lib x86 64 linux gnu libxdmcp.so.6.0.0 7f99d122f000 7f99d1231000 r xp 00000000 08 21 62790477 usr lib x86 64 linux gnu libxau.so.6.0.0 7f99d1231000 7f99d1431000 p 00002000 08 21 62790477 usr lib x86 64 linux gnu libxau.so.6.0.0 7f99d1431000 7f99d1432000 r p 00002000 08 21 62790477 usr lib x86 64 linux gnu libxau.so.6.0.0 7f99d1432000 7f99d1433000 rw p 00003000 08 21 62790477 usr lib x86 64 linux gnu libxau.so.6.0.0 7f99d1433000 7f99d1454000 r xp 00000000 08 21 62790960 usr lib x86 64 linux gnu libxcb.so.1.1.0 7f99d1454000 7f99d1653000 p 00021000 08 21 62790960 usr lib x86 64 linux gnu libxcb.so.1.1.0 7f99d1653000 7f99d1654000 r p 00020000 08 21 62790960 usr lib x86 64 linux gnu libxcb.so.1.1.0 7f99d1654000 7f99d1655000 rw p 00021000 08 21 62790960 usr lib x86 64 linux gnu libxcb.so.1.1.0 7f99d1655000 7f99d1659000 r xp 00000000 08 21 38015980 lib x86 64 linux gnu libuuid.so.1.3.0 7f99d1659000 7f99d1858000 p 00004000 08 21 38015980 lib x86 64 linux gnu libuuid.so.1.3.0 7f99d1858000 7f99d1859000 r p 00003000 08 21 38015980 lib x86 64 linux gnu libuuid.so.1.3.0 7f99d1859000 7f99d185a000 rw p 00004000 08 21 38015980 lib x86 64 linux gnu libuuid.so.1.3.0 7f99d185a000 7f99d18c8000 r xp 00000000 08 21 38015948 lib x86 64 linux gnu libpcre.so.3.13.2 7f99d18c8000 7f99d1ac8000 p 0006e000 08 21 38015948 lib x86 64 linux gnu libpcre.so.3.13.2 7f99d1ac8000 7f99d1ac9000 r p 0006e000 08 21 38015948 lib x86 64 linux gnu libpcre.so.3.13.2 7f99d1ac9000 7f99d1aca000 rw p 0006f000 08 21 38015948 lib x86 64 linux gnu libpcre.so.3.13.2 7f99d1aca000 7f99d1bff000 r xp 00000000 08 21 62790473 usr lib x86 64 linux gnu libx11.so.6.3.0 7f99d1bff000 7f99d1dff000 p 00135000 08 21 62790473 usr lib x86 64 linux gnu libx11.so.6.3.0 7f99d1dff000 7f99d1e00000 r p 00135000 08 21 62790473 usr lib x86 64 linux gnu libx11.so.6.3.0 7f99d1e00000 7f99d1e04000 rw p 00136000 08 21 62790473 usr lib x86 64 linux gnu libx11.so.6.3.0 7f99d1e04000 7f99d1e15000 r xp 00000000 08 21 62790500 usr lib x86 64 linux gnu libxext.so.6.4.0 7f99d1e15000 7f99d2014000 p 00011000 08 21 62790500 usr lib x86 64 linux gnu libxext.so.6.4.0 7f99d2014000 7f99d2015000 r p 00010000 08 21 62790500 usr lib x86 64 linux gnu libxext.so.6.4.0 7f99d2015000 7f99d2016000 rw p 00011000 08 21 62790500 usr lib x86 64 linux gnu libxext.so.6.4.0 7f99d2016000 7f99d201f000 r xp 00000000 08 21 62790528 usr lib x86 64 linux gnu libxrender.so.1.3.0 7f99d201f000 7f99d221e000 p 00009000 08 21 62790528 usr lib x86 64 linux gnu libxrender.so.1.3.0 7f99d221e000 7f99d221f000 r p 00008000 08 21 62790528 usr lib x86 64 linux gnu libxrender.so.1.3.0 7f99d221f000 7f99d2220000 rw p 00009000 08 21 62790528 usr lib x86 64 linux gnu libxrender.so.1.3.0 7f99d2220000 7f99d2236000 r xp 00000000 08 21 62790441 usr lib x86 64 linux gnu libice.so.6.3.0 7f99d2236000 7f99d2435000 p 00016000 08 21 62790441 usr lib x86 64 linux gnu libice.so.6.3.0 7f99d2435000 7f99d2436000 r p 00015000 08 21 62790441 usr lib x86 64 linux gnu libice.so.6.3.0 7f99d2436000 7f99d2437000 rw p 00016000 08 21 62790441 usr lib x86 64 linux gnu libice.so.6.3.0 7f99d2437000 7f99d243a000 rw p 00000000 00 00 0 7f99d243a000 7f99d2441000 r xp 00000000 08 21 62790467 usr lib x86 64 linux gnu libsm.so.6.0.1 7f99d2441000 7f99d2640000 p 00007000 08 21 62790467 usr lib x86 64 linux gnu libsm.so.6.0.1 7f99d2640000 7f99d2641000 r p 00006000 08 21 62790467 usr lib x86 64 linux gnu libsm.so.6.0.1 7f99d2641000 7f99d2642000 rw p 00007000 08 21 62790467 usr lib x86 64 linux gnu libsm.so.6.0.1 7f99d2642000 7f99d2751000 r xp 00000000 08 21 62529459 lib x86 64 linux gnu libglib 2.0.so.0.4800.2 7f99d2751000 7f99d2950000 p 0010f000 08 21 62529459 lib x86 64 linux gnu libglib 2.0.so.0.4800.2 7f99d2950000 7f99d2951000 r p 0010e000 08 21 62529459 lib x86 64 linux gnu libglib 2.0.so.0.4800.2 7f99d2951000 7f99d2952000 rw p 0010f000 08 21 62529459 lib x86 64 linux gnu libglib 2.0.so.0.4800.2 7f99d2952000 7f99d2953000 rw p 00000000 00 00 0 7f99d2953000 7f99d2954000 r xp 00000000 08 21 62790657 usr lib x86 64 linux gnu libgthread 2.0.so.0.4800.2 7f99d2954000 7f99d2b53000 p 00001000 08 21 62790657 usr lib x86 64 linux gnu libgthread 2.0.so.0.4800.2 7f99d2b53000 7f99d2b54000 r p 00000000 08 21 62790657 usr lib x86 64 linux gnu libgthread 2.0.so.0.4800.2 7f99d2b54000 7f99d2b55000 rw p 00001000 08 21 62790657 usr lib x86 64 linux gnu libgthread 2.0.so.0.4800.2 7f99d2b55000 7f99d2e4e000 r xp 00000000 08 21 1447102 usr local lib python2.7 dist packages cv2 .libs libvpx 8459aeef.so.6.0.0 7f99d2e4e000 7f99d304e000 p 002f9000 08 21 1447102 usr local lib python2.7 dist packages cv2 .libs libvpx 8459aeef.so.6.0.0 7f99d304e000 7f99d3051000 rw p 002f9000 08 21 1447102 usr local lib python2.7 dist packages cv2 .libs libvpx 8459aeef.so.6.0.0 7f99d3051000 7f99d3058000 rw p 00000000 00 00 0 7f99d3058000 7f99d3059000 rw p 0032c000 08 21 1447102 usr local lib python2.7 dist packages cv2 .libs libvpx 8459aeef.so.6.0.0 7f99d3059000 7f99d3073000 r xp 00000000 08 21 1447098 usr local lib python2.7 dist packages cv2 .libs libswresample a49c020a.so.3.4.100 7f99d3073000 7f99d3273000 p 0001a000 08 21 1447098 usr local lib python2.7 dist packages cv2 .libs libswresample a49c020a.so.3.4.100 7f99d3273000 7f99d3275000 rw p 0001a000 08 21 1447098 usr local lib python2.7 dist packages cv2 .libs libswresample a49c020a.so.3.4.100 7f99d3275000 7f99d3277000 rw p 0001d000 08 21 1447098 usr local lib python2.7 dist packages cv2 .libs libswresample a49c020a.so.3.4.100 7f99d3277000 7f99d328d000 r xp 00000000 08 21 38015904 lib x86 64 linux gnu libgcc s.so.1 7f99d328d000 7f99d348c000 p 00016000 08 21 38015904 lib x86 64 linux gnu libgcc s.so.1 7f99d348c000 7f99d348d000 rw p 00015000 08 21 38015904 lib x86 64 linux gnu libgcc s.so.1 7f99d348d000 7f99d35ff000 r xp 00000000 08 21 38274693 usr lib x86 64 linux gnu libstdc .so.6.0.21 7f99d35ff000 7f99d37ff000 p 00172000 08 21 38274693 usr lib x86 64 linux gnu libstdc .so.6.0.21 7f99d37ff000 7f99d3809000 r p 00172000 08 21 38274693 usr lib x86 64 linux gnu libstdc .so.6.0.21 7f99d3809000 7f99d380b000 rw p 0017c000 08 21 38274693 usr lib x86 64 linux gnu libstdc .so.6.0.21 7f99d380b000 7f99d380f000 rw p 00000000 00 00 0 7f99d380f000 7f99d3b0b000 r xp 00000000 08 21 1447094 usr local lib python2.7 dist packages cv2 .libs libqtcore 3dbacd8a.so.4.8.7 7f99d3b0b000 7f99d3b1b000 p 002fc000 08 21 1447094 usr local lib python2.7 dist packages cv2 .libs libqtcore 3dbacd8a.so.4.8.7 7f99d3b1b000 7f99d3b47000 rw p 0030c000 08 21 1447094 usr local lib python2.7 dist packages cv2 .libs libqtcore 3dbacd8a.so.4.8.7 7f99d3b47000 7f99d3d0a000 p 00338000 08 21 1447094 usr local lib python2.7 dist packages cv2 .libs libqtcore 3dbacd8a.so.4.8.7 7f99d3d0a000 7f99d3d1a000 rw p 002fb000 08 21 1447094 usr local lib python2.7 dist packages cv2 .libs libqtcore 3dbacd8a.so.4.8.7 7f99d3d1a000 7f99d3d1b000 rw p 00000000 00 00 0 7f99d3d1b000 7f99d4823000 r xp 00000000 08 21 1447095 usr local lib python2.7 dist packages cv2 .libs libqtgui 6d0f14dd.so.4.8.7 7f99d4823000 7f99d4a23000 p 00b08000 08 21 1447095 usr local lib python2.7 dist packages cv2 .libs libqtgui 6d0f14dd.so.4.8.7 7f99d4a23000 7f99d4a7c000 rw p 00b08000 08 21 1447095 usr local lib python2.7 dist packages cv2 .libs libqtgui 6d0f14dd.so.4.8.7 7f99d4a7c000 7f99d4a7e000 rw p 00000000 00 00 0 7f99d4a7e000 7f99d4bac000 rw p 00b61000 08 21 1447095 usr local lib python2.7 dist packages cv2 .libs libqtgui 6d0f14dd.so.4.8.7 7f99d4bac000 7f99d57cb000 r xp 00000000 08 21 1447096 usr local lib python2.7 dist packages cv2 .libs libavcodec 874f3d51.so.58.47.106 7f99d57cb000 7f99d59cb000 p 00c1f000 08 21 1447096 usr local lib python2.7 dist packages cv2 .libs libavcodec 874f3d51.so.58.47.106 7f99d59cb000 7f99d5a2a000 rw p 00c1f000 08 21 1447096 usr local lib python2.7 dist packages cv2 .libs libavcodec 874f3d51.so.58.47.106 7f99d5a2a000 7f99d6208000 rw p 00000000 00 00 0 7f99d6208000 7f99d6218000 rw p 00c7f000 08 21 1447096 usr local lib python2.7 dist packages cv2 .libs libavcodec 874f3d51.so.58.47.106 7f99d6218000 7f99d7c28000 r xp 00000000 08 21 1447070 usr local lib python2.7 dist packages cv2 cv2.so 7f99d7c28000 7f99d7e28000 p 01a10000 08 21 1447070 usr local lib python2.7 dist packages cv2 cv2.so 7f99d7e28000 7f99d7ec1000 rw p 01a10000 08 21 1447070 usr local lib python2.7 dist packages cv2 cv2.so 7f99d7ec1000 7f99d7f7f000 rw p 00000000 00 00 0 7f99d7f7f000 7f99d8000000 rw p 01aaa000 08 21 1447070 usr local lib python2.7 dist packages cv2 cv2.so 7f99d8000000 7f99d8021000 rw p 00000000 00 00 0 7f99d8021000 7f99dc000000 p 00000000 00 00 0 7f99dc000000 7f99dc021000 rw p 00000000 00 00 0 7f99dc021000 7f99e0000000 p 00000000 00 00 0 7f99e0000000 7f99e0021000 rw p 00000000 00 00 0 7f99e0021000 7f99e4000000 p 00000000 00 00 0 7f99e414c000 7f99e4153000 r xp 00000000 08 21 38015957 lib x86 64 linux gnu librt 2.23.so 7f99e4153000 7f99e4352000 p 00007000 08 21 38015957 lib x86 64 linux gnu librt 2.23.so 7f99e4352000 7f99e4353000 r p 00006000 08 21 38015957 lib x86 64 linux gnu librt 2.23.so 7f99e4353000 7f99e4354000 rw p 00007000 08 21 38015957 lib x86 64 linux gnu librt 2.23.so 7f99e4354000 7f99e4378000 r xp 00000000 08 21 1447097 usr local lib python2.7 dist packages cv2 .libs libqttest 1183da5d.so.4.8.7 7f99e4378000 7f99e4578000 p 00024000 08 21 1447097 usr local lib python2.7 dist packages cv2 .libs libqttest 1183da5d.so.4.8.7 7f99e4578000 7f99e4581000 rw p 00024000 08 21 1447097 usr local lib python2.7 dist packages cv2 .libs libqttest 1183da5d.so.4.8.7 7f99e4581000 7f99e4601000 r xp 00000000 08 21 1447093 usr local lib python2.7 dist packages cv2 .libs libswscale 4e6f4703.so.5.4.100 7f99e4601000 7f99e4800000 p 00080000 08 21 1447093 usr local lib python2.7 dist packages cv2 .libs libswscale 4e6f4703.so.5.4.100 7f99e4800000 7f99e4802000 rw p 0007f000 08 21 1447093 usr local lib python2.7 dist packages cv2 .libs libswscale 4e6f4703.so.5.4.100 7f99e4802000 7f99e480a000 rw p 00000000 00 00 0 7f99e480a000 7f99e480c000 rw p 00082000 08 21 1447093 usr local lib python2.7 dist packages cv2 .libs libswscale 4e6f4703.so.5.4.100 7f99e480c000 7f99e4869000 r xp 00000000 08 21 1447100 usr local lib python2.7 dist packages cv2 .libs libavutil 473e9eb1.so.56.26.100 7f99e4869000 7f99e4a69000 p 0005d000 08 21 1447100 usr local lib python2.7 dist packages cv2 .libs libavutil 473e9eb1.so.56.26.100 7f99e4a69000 7f99e4a74000 rw p 0005d000 08 21 1447100 usr local lib python2.7 dist packages cv2 .libs libavutil 473e9eb1.so.56.26.100 7f99e4a74000 7f99e4a83000 rw p 00000000 00 00 0 7f99e4a83000 7f99e4a87000 rw p 00069000 08 21 1447100 usr local lib python2.7 dist packages cv2 .libs libavutil 473e9eb1.so.56.26.100 7f99e4a87000 7f99e4ca4000 r xp 00000000 08 21 1447101 usr local lib python2.7 dist packages cv2 .libs libavformat fb41c63f.so.58.26.101 7f99e4ca4000 7f99e4ea4000 p 0021d000 08 21 1447101 usr local lib python2.7 dist packages cv2 .libs libavformat fb41c63f.so.58.26.101 7f99e4ea4000 7f99e4ee4000 rw p 0021d000 08 21 1447101 usr local lib python2.7 dist packages cv2 .libs libavformat fb41c63f.so.58.26.101 7f99e4ee4000 7f99e4ef8000 r xp 00000000 08 21 1447099 usr local lib python2.7 dist packages cv2 .libs libz a147dcb0.so.1.2.3 7f99e4ef8000 7f99e50f7000 p 00014000 08 21 1447099 usr local lib python2.7 dist packages cv2 .libs libz a147dcb0.so.1.2.3 7f99e50f7000 7f99e50f8000 rw p 00013000 08 21 1447099 usr local lib python2.7 dist packages cv2 .libs libz a147dcb0.so.1.2.3 7f99e50f8000 7f99e50f9000 rw p 00015000 08 21 1447099 usr local lib python2.7 dist packages cv2 .libs libz a147dcb0.so.1.2.3 7f99e50f9000 7f99e50fa000 p 00000000 00 00 0 7f99e50fa000 7f99e58fa000 rw p 00000000 00 00 0 7f99e58fa000 7f99e58fb000 p 00000000 00 00 0 7f99e58fb000 7f99e60fb000 rw p 00000000 00 00 0 7f99e60fb000 7f99e60fc000 p 00000000 00 00 0 7f99e60fc000 7f99e68fc000 rw p 00000000 00 00 0 7f99e68fc000 7f99e6900000 r xp 00000000 08 21 62787987 usr lib python2.7 lib dynload termios.x86 64 linux gnu.so 7f99e6900000 7f99e6aff000 p 00004000 08 21 62787987 usr lib python2.7 lib dynload termios.x86 64 linux gnu.so 7f99e6aff000 7f99e6b00000 r p 00003000 08 21 62787987 usr lib python2.7 lib dynload termios.x86 64 linux gnu.so 7f99e6b00000 7f99e6b02000 rw p 00004000 08 21 62787987 usr lib python2.7 lib dynload termios.x86 64 linux gnu.so 7f99e6b02000 7f99e6b82000 rw p 00000000 00 00 0 7f99e6c02000 7f99e6dc2000 rw p 00000000 00 00 0 7f99e6dc2000 7f99e6de8000 r xp 00000000 08 21 62529457 lib x86 64 linux gnu libexpat.so.1.6.0 7f99e6de8000 7f99e6fe8000 p 00026000 08 21 62529457 lib x86 64 linux gnu libexpat.so.1.6.0 7f99e6fe8000 7f99e6fea000 r p 00026000 08 21 62529457 lib x86 64 linux gnu libexpat.so.1.6.0 7f99e6fea000 7f99e6feb000 rw p 00028000 08 21 62529457 lib x86 64 linux gnu libexpat.so.1.6.0 7f99e6feb000 7f99e6ffa000 r xp 00000000 08 21 62787984 usr lib python2.7 lib dynload pyexpat.x86 64 linux gnu.so 7f99e6ffa000 7f99e71f9000 p 0000f000 08 21 62787984 usr lib python2.7 lib dynload pyexpat.x86 64 linux gnu.so 7f99e71f9000 7f99e71fa000 r p 0000e000 08 21 62787984 usr lib python2.7 lib dynload pyexpat.x86 64 linux gnu.so 7f99e71fa000 7f99e71fc000 rw p 0000f000 08 21 62787984 usr lib python2.7 lib dynload pyexpat.x86 64 linux gnu.so 7f99e71fc000 7f99e727c000 rw p 00000000 00 00 0 7f99e727c000 7f99e73fc000 rw p 00000000 00 00 0 7f99e73fc000 7f99e7408000 r xp 00000000 08 21 62787966 usr lib python2.7 lib dynload json.x86 64 linux gnu.so 7f99e7408000 7f99e7607000 p 0000c000 08 21 62787966 usr lib python2.7 lib dynload json.x86 64 linux gnu.so 7f99e7607000 7f99e7608000 r p 0000b000 08 21 62787966 usr lib python2.7 lib dynload json.x86 64 linux gnu.so 7f99e7608000 7f99e7609000 rw p 0000c000 08 21 62787966 usr lib python2.7 lib dynload json.x86 64 linux gnu.so 7f99e7609000 7f99e7689000 rw p 00000000 00 00 0 7f99e7689000 7f99e76e7000 r xp 00000000 08 21 45488002 lib x86 64 linux gnu libssl.so.1.0.0 7f99e76e7000 7f99e78e7000 p 0005e000 08 21 45488002 lib x86 64 linux gnu libssl.so.1.0.0 7f99e78e7000 7f99e78eb000 r p 0005e000 08 21 45488002 lib x86 64 linux gnu libssl.so.1.0.0 7f99e78eb000 7f99e78f2000 rw p 00062000 08 21 45488002 lib x86 64 linux gnu libssl.so.1.0.0 7f99e78f2000 7f99e7907000 r xp 00000000 08 21 62787971 usr lib python2.7 lib dynload ssl.x86 64 linux gnu.so 7f99e7907000 7f99e7b06000 p 00015000 08 21 62787971 usr lib python2.7 lib dynload ssl.x86 64 linux gnu.so 7f99e7b06000 7f99e7b07000 r p 00014000 08 21 62787971 usr lib python2.7 lib dynload ssl.x86 64 linux gnu.so 7f99e7b07000 7f99e7b0b000 rw p 00015000 08 21 62787971 usr lib python2.7 lib dynload ssl.x86 64 linux gnu.so 7f99e7b0b000 7f99e7ccb000 rw p 00000000 00 00 0 7f99e7ccc000 7f99e7d0c000 rw p 00000000 00 00 0 7f99e7d0c000 7f99e7d10000 r xp 00000000 08 21 1442706 usr local lib python2.7 dist packages posixsubprocess32.so 7f99e7d10000 7f99e7f0f000 p 00004000 08 21 1442706 usr local lib python2.7 dist packages posixsubprocess32.so 7f99e7f0f000 7f99e7f10000 r p 00003000 08 21 1442706 usr local lib python2.7 dist packages posixsubprocess32.so 7f99e7f10000 7f99e7f11000 rw p 00004000 08 21 1442706 usr local lib python2.7 dist packages posixsubprocess32.so 7f99e7f11000 7f99e8051000 rw p 00000000 00 00 0 7f99e8051000 7f99e8102000 r xp 00000000 08 21 3018979 usr local lib python2.7 dist packages numpy random mtrand.so 7f99e8102000 7f99e8301000 p 000b1000 08 21 3018979 usr local lib python2.7 dist packages numpy random mtrand.so 7f99e8301000 7f99e8326000 rw p 000b0000 08 21 3018979 usr local lib python2.7 dist packages numpy random mtrand.so 7f99e8326000 7f99e8368000 rw p 00000000 00 00 0 7f99e8368000 7f99e8371000 r xp 00000000 08 21 3019001 usr local lib python2.7 dist packages numpy fft fftpack lite.so 7f99e8371000 7f99e8571000 p 00009000 08 21 3019001 usr local lib python2.7 dist packages numpy fft fftpack lite.so 7f99e8571000 7f99e8572000 rw p 00009000 08 21 3019001 usr local lib python2.7 dist packages numpy fft fftpack lite.so 7f99e8572000 7f99e85f2000 rw p 00000000 00 00 0 7f99e85f2000 7f99e85f3000 r xp 00000000 08 21 62787978 usr lib python2.7 lib dynload future builtins.x86 64 linux gnu.so 7f99e85f3000 7f99e87f2000 p 00001000 08 21 62787978 usr lib python2.7 lib dynload future builtins.x86 64 linux gnu.so 7f99e87f2000 7f99e87f3000 r p 00000000 08 21 62787978 usr lib python2.7 lib dynload future builtins.x86 64 linux gnu.so 7f99e87f3000 7f99e87f4000 rw p 00001000 08 21 62787978 usr lib python2.7 lib dynload future builtins.x86 64 linux gnu.so 7f99e87f4000 7f99e8874000 rw p 00000000 00 00 0 7f99e8874000 7f99e889f000 r xp 00000000 08 21 3018415 usr local lib python2.7 dist packages numpy linalg umath linalg.so 7f99e889f000 7f99e8a9e000 p 0002b000 08 21 3018415 usr local lib python2.7 dist packages numpy linalg umath linalg.so 7f99e8a9e000 7f99e8aa0000 rw p 0002a000 08 21 3018415 usr local lib python2.7 dist packages numpy linalg umath linalg.so 7f99e8aa0000 7f99e8aa3000 rw p 000d2000 08 21 3018415 usr local lib python2.7 dist packages numpy linalg umath linalg.so 7f99e8aa3000 7f99e8aa7000 r xp 00000000 08 21 3018416 usr local lib python2.7 dist packages numpy linalg lapack lite.so 7f99e8aa7000 7f99e8ca7000 p 00004000 08 21 3018416 usr local lib python2.7 dist packages numpy linalg lapack lite.so 7f99e8ca7000 7f99e8ca8000 rw p 00004000 08 21 3018416 usr local lib python2.7 dist packages numpy linalg lapack lite.so 7f99e8ca8000 7f99e8caa000 rw p 00019000 08 21 3018416 usr local lib python2.7 dist packages numpy linalg lapack lite.so 7f99e8caa000 7f99e8cea000 rw p 00000000 00 00 0 7f99e8cea000 7f99e8d09000 r xp 00000000 08 21 3018453 usr local lib python2.7 dist packages numpy core multiarray tests.so 7f99e8d09000 7f99e8f08000 p 0001f000 08 21 3018453 usr local lib python2.7 dist packages numpy core multiarray tests.so 7f99e8f08000 7f99e8f0a000 rw p 0001e000 08 21 3018453 usr local lib python2.7 dist packages numpy core multiarray tests.so 7f99e8f0a000 7f9a0cfca000 rw p 00000000 00 00 0 7f9a0cfca000 7f9a0cfd1000 r xp 00000000 08 21 45746691 usr lib x86 64 linux gnu libffi.so.6.0.4 7f9a0cfd1000 7f9a0d1d0000 p 00007000 08 21 45746691 usr lib x86 64 linux gnu libffi.so.6.0.4 7f9a0d1d0000 7f9a0d1d1000 r p 00006000 08 21 45746691 usr lib x86 64 linux gnu libffi.so.6.0.4 7f9a0d1d1000 7f9a0d1d2000 rw p 00007000 08 21 45746691 usr lib x86 64 linux gnu libffi.so.6.0.4 7f9a0d1d2000 7f9a0d1f0000 r xp 00000000 08 21 62787959 usr lib python2.7 lib dynload ctypes.x86 64 linux gnu.so 7f9a0d1f0000 7f9a0d3ef000 p 0001e000 08 21 62787959 usr lib python2.7 lib dynload ctypes.x86 64 linux gnu.so 7f9a0d3ef000 7f9a0d3f0000 r p 0001d000 08 21 62787959 usr lib python2.7 lib dynload ctypes.x86 64 linux gnu.so 7f9a0d3f0000 7f9a0d3f4000 rw p 0001e000 08 21 62787959 usr lib python2.7 lib dynload ctypes.x86 64 linux gnu.so 7f9a0d3f4000 7f9a0d434000 rw p 00000000 00 00 0 7f9a0d434000 7f9a0d435000 p 00000000 00 00 0 7f9a0d435000 7f9a0dc35000 rw p 00000000 00 00 0 7f9a0de25000 7f9a0e4a8000 rw p 00000000 00 00 0 7f9a0e4a8000 7f9a0e4ac000 r xp 00000000 08 21 1447721 usr local lib python2.7 dist packages scandir.so 7f9a0e4ac000 7f9a0e6ab000 p 00004000 08 21 1447721 usr local lib python2.7 dist packages scandir.so 7f9a0e6ab000 7f9a0e6ac000 r p 00003000 08 21 1447721 usr local lib python2.7 dist packages scandir.so 7f9a0e6ac000 7f9a0e6ad000 rw p 00004000 08 21 1447721 usr local lib python2.7 dist packages scandir.so 7f9a0e6ad000 7f9a0e6df000 r xp 00000000 08 21 2232259 usr local lib python2.7 dist packages pil .libs liblzma 6cd627ed.so.5.2.4 7f9a0e6df000 7f9a0e8df000 p 00032000 08 21 2232259 usr local lib python2.7 dist packages pil .libs liblzma 6cd627ed.so.5.2.4 7f9a0e8df000 7f9a0e8e0000 rw p 00032000 08 21 2232259 usr local lib python2.7 dist packages pil .libs liblzma 6cd627ed.so.5.2.4 7f9a0e8e0000 7f9a0e8e1000 rw p 00034000 08 21 2232259 usr local lib python2.7 dist packages pil .libs liblzma 6cd627ed.so.5.2.4 7f9a0e8e1000 7f9a0e970000 r xp 00000000 08 21 2232258 usr local lib python2.7 dist packages pil .libs libtiff 8267adfe.so.5.4.0 7f9a0e970000 7f9a0eb70000 p 0008f000 08 21 2232258 usr local lib python2.7 dist packages pil .libs libtiff 8267adfe.so.5.4.0 7f9a0eb70000 7f9a0eb74000 rw p 0008f000 08 21 2232258 usr local lib python2.7 dist packages pil .libs libtiff 8267adfe.so.5.4.0 7f9a0eb74000 7f9a0eb7c000 rw p 00094000 08 21 2232258 usr local lib python2.7 dist packages pil .libs libtiff 8267adfe.so.5.4.0 7f9a0eb7c000 7f9a0ebc3000 r xp 00000000 08 21 2232261 usr local lib python2.7 dist packages pil .libs libopenjp2 e366d6b0.so.2.1.0 7f9a0ebc3000 7f9a0edc3000 p 00047000 08 21 2232261 usr local lib python2.7 dist packages pil .libs libopenjp2 e366d6b0.so.2.1.0 7f9a0edc3000 7f9a0edc5000 rw p 00047000 08 21 2232261 usr local lib python2.7 dist packages pil .libs libopenjp2 e366d6b0.so.2.1.0 7f9a0edc5000 7f9a0edc7000 rw p 0004a000 08 21 2232261 usr local lib python2.7 dist packages pil .libs libopenjp2 e366d6b0.so.2.1.0 7f9a0edc7000 7f9a0ee18000 r xp 00000000 08 21 2232256 usr local lib python2.7 dist packages pil .libs libjpeg 3fe7dfc0.so.9.3.0 7f9a0ee18000 7f9a0f018000 p 00051000 08 21 2232256 usr local lib python2.7 dist packages pil .libs libjpeg 3fe7dfc0.so.9.3.0 7f9a0f018000 7f9a0f019000 rw p 00051000 08 21 2232256 usr local lib python2.7 dist packages pil .libs libjpeg 3fe7dfc0.so.9.3.0 7f9a0f019000 7f9a0f01b000 rw p 00053000 08 21 2232256 usr local lib python2.7 dist packages pil .libs libjpeg 3fe7dfc0.so.9.3.0 7f9a0f01b000 7f9a0f099000 r xp 00000000 08 21 2232104 usr local lib python2.7 dist packages pil imaging.so 7f9a0f099000 7f9a0f299000 p 0007e000 08 21 2232104 usr local lib python2.7 dist packages pil imaging.so 7f9a0f299000 7f9a0f2af000 rw p 0007e000 08 21 2232104 usr local lib python2.7 dist packages pil imaging.so 7f9a0f2af000 7f9a0f2b5000 r xp 00000000 08 21 62787969 usr lib python2.7 lib dynload multiprocessing.x86 64 linux gnu.so 7f9a0f2b5000 7f9a0f4b4000 p 00006000 08 21 62787969 usr lib python2.7 lib dynload multiprocessing.x86 64 linux gnu.so 7f9a0f4b4000 7f9a0f4b5000 r p 00005000 08 21 62787969 usr lib python2.7 lib dynload multiprocessing.x86 64 linux gnu.so 7f9a0f4b5000 7f9a0f4b6000 rw p 00006000 08 21 62787969 usr lib python2.7 lib dynload multiprocessing.x86 64 linux gnu.so 7f9a0f4b6000 7f9a0f4b8000 r xp 00000000 08 21 62790954 usr lib x86 64 linux gnu libxcb shm.so.0.0.0 7f9a0f4b8000 7f9a0f6b8000 p 00002000 08 21 62790954 usr lib x86 64 linux gnu libxcb shm.so.0.0.0 7f9a0f6b8000 7f9a0f6b9000 r p 00002000 08 21 62790954 usr lib x86 64 linux gnu libxcb shm.so.0.0.0 7f9a0f6b9000 7f9a0f6ba000 rw p 00003000 08 21 62790954 usr lib x86 64 linux gnu libxcb shm.so.0.0.0 7f9a0f6ba000 7f9a0f759000 r xp 00000000 08 21 62790841 usr lib x86 64 linux gnu libpixman 1.so.0.33.6 7f9a0f759000 7f9a0f959000 p 0009f000 08 21 62790841 usr lib x86 64 linux gnu libpixman 1.so.0.33.6 7f9a0f959000 7f9a0f961000 r p 0009f000 08 21 62790841 usr lib x86 64 linux gnu libpixman 1.so.0.33.6 7f9a0f961000 7f9a0f962000 rw p 000a7000 08 21 62790841 usr lib x86 64 linux gnu libpixman 1.so.0.33.6 7f9a0f962000 7f9a0fa06000 r xp 00000000 08 21 62790611 usr lib x86 64 linux gnu libfreetype.so.6.12.1 7f9a0fa06000 7f9a0fc05000 p 000a4000 08 21 62790611 usr lib x86 64 linux gnu libfreetype.so.6.12.1 7f9a0fc05000 7f9a0fc0b000 r p 000a3000 08 21 62790611 usr lib x86 64 linux gnu libfreetype.so.6.12.1 7f9a0fc0b000 7f9a0fc0c000 rw p 000a9000 08 21 62790611 usr lib x86 64 linux gnu libfreetype.so.6.12.1 7f9a0fc0c000 7f9a0fc1b000 r xp 00000000 08 21 38015882 lib x86 64 linux gnu libbz2.so.1.0.4 7f9a0fc1b000 7f9a0fe1a000 p 0000f000 08 21 38015882 lib x86 64 linux gnu libbz2.so.1.0.4 7f9a0fe1a000 7f9a0fe1b000 r p 0000e000 08 21 38015882 lib x86 64 linux gnu libbz2.so.1.0.4 7f9a0fe1b000 7f9a0fe1c000 rw p 0000f000 08 21 38015882 lib x86 64 linux gnu libbz2.so.1.0.4 7f9a0fe1c000 7f9a0ff3f000 r xp 00000000 08 21 45746695 usr lib x86 64 linux gnu libgnutls.so.30.6.2 7f9a0ff3f000 7f9a1013e000 p 00123000 08 21 45746695 usr lib x86 64 linux gnu libgnutls.so.30.6.2 7f9a1013e000 7f9a10149000 r p 00122000 08 21 45746695 usr lib x86 64 linux gnu libgnutls.so.30.6.2 7f9a10149000 7f9a1014b000 rw p 0012d000 08 21 45746695 usr lib x86 64 linux gnu libgnutls.so.30.6.2 7f9a1014b000 7f9a1014c000 rw p 00000000 00 00 0 7f9a1014c000 7f9a10193000 r xp 00000000 08 21 62790559 usr lib x86 64 linux gnu libbluray.so.1.9.2 7f9a10193000 7f9a10392000 p 00047000 08 21 62790559 usr lib x86 64 linux gnu libbluray.so.1.9.2 7f9a10392000 7f9a10394000 r p 00046000 08 21 62790559 usr lib x86 64 linux gnu libbluray.so.1.9.2 7f9a10394000 7f9a10395000 rw p 00048000 08 21 62790559 usr lib x86 64 linux gnu libbluray.so.1.9.2 7f9a10395000 7f9a103df000 r xp 00000000 08 21 62790639 usr lib x86 64 linux gnu libgme.so.0.6.0 7f9a103df000 7f9a105df000 p 0004a000 08 21 62790639 usr lib x86 64 linux gnu libgme.so.0.6.0 7f9a105df000 7f9a105e2000 r p 0004a000 08 21 62790639 usr lib x86 64 linux gnu libgme.so.0.6.0 7f9a105e2000 7f9a105e3000 rw p 0004d000 08 21 62790639 usr lib x86 64 linux gnu libgme.so.0.6.0 7f9a105e3000 7f9a1062d000 r xp 00000000 08 21 62790711 usr lib x86 64 linux gnu libmodplug.so.1.0.0 7f9a1062d000 7f9a1082d000 p 0004a000 08 21 62790711 usr lib x86 64 linux gnu libmodplug.so.1.0.0 7f9a1082d000 7f9a1082e000 r p 0004a000 08 21 62790711 usr lib x86 64 linux gnu libmodplug.so.1.0.0 7f9a1082e000 7f9a1082f000 rw p 0004b000 08 21 62790711 usr lib x86 64 linux gnu libmodplug.so.1.0.0 7f9a1082f000 7f9a1096e000 rw p 00000000 00 00 0 7f9a1096e000 7f9a10989000 r xp 00000000 08 21 45746731 usr lib x86 64 linux gnu librtmp.so.1 7f9a10989000 7f9a10b88000 p 0001b000 08 21 45746731 usr lib x86 64 linux gnu librtmp.so.1 7f9a10b88000 7f9a10b89000 r p 0001a000 08 21 45746731 usr lib x86 64 linux gnu librtmp.so.1 7f9a10b89000 7f9a10b8a000 rw p 0001b000 08 21 45746731 usr lib x86 64 linux gnu librtmp.so.1 7f9a10b8a000 7f9a10bd2000 r xp 00000000 08 21 62790873 usr lib x86 64 linux gnu libssh gcrypt.so.4.4.1 7f9a10bd2000 7f9a10dd1000 p 00048000 08 21 62790873 usr lib x86 64 linux gnu libssh gcrypt.so.4.4.1 7f9a10dd1000 7f9a10dd2000 r p 00047000 08 21 62790873 usr lib x86 64 linux gnu libssh gcrypt.so.4.4.1 7f9a10dd2000 7f9a10dd3000 rw p 00048000 08 21 62790873 usr lib x86 64 linux gnu libssh gcrypt.so.4.4.1 7f9a10dd3000 7f9a10ded000 r xp 00000000 08 21 62790573 usr lib x86 64 linux gnu libcrystalhd.so.3.6 7f9a10ded000 7f9a10fec000 p 0001a000 08 21 62790573 usr lib x86 64 linux gnu libcrystalhd.so.3.6 7f9a10fec000 7f9a10fed000 r p 00019000 08 21 62790573 usr lib x86 64 linux gnu libcrystalhd.so.3.6 7f9a10fed000 7f9a10fee000 rw p 0001a000 08 21 62790573 usr lib x86 64 linux gnu libcrystalhd.so.3.6 7f9a10fee000 7f9a10ffb000 r xp 00000000 08 21 62790653 usr lib x86 64 linux gnu libgsm.so.1.0.12 7f9a10ffb000 7f9a111fa000 p 0000d000 08 21 62790653 usr lib x86 64 linux gnu libgsm.so.1.0.12 7f9a111fa000 7f9a111fb000 r p 0000c000 08 21 62790653 usr lib x86 64 linux gnu libgsm.so.1.0.12 7f9a111fb000 7f9a111fc000 rw p 0000d000 08 21 62790653 usr lib x86 64 linux gnu libgsm.so.1.0.12 7f9a111fc000 7f9a11241000 r xp 00000000 08 21 62790713 usr lib x86 64 linux gnu libmp3lame.so.0.0.0 7f9a11241000 7f9a11441000 p 00045000 08 21 62790713 usr lib x86 64 linux gnu libmp3lame.so.0.0.0 7f9a11441000 7f9a11442000 r p 00045000 08 21 62790713 usr lib x86 64 linux gnu libmp3lame.so.0.0.0 7f9a11442000 7f9a11443000 rw p 00046000 08 21 62790713 usr lib x86 64 linux gnu libmp3lame.so.0.0.0 7f9a11443000 7f9a11471000 rw p 00000000 00 00 0 7f9a11471000 7f9a11493000 r xp 00000000 08 21 62790792 usr lib x86 64 linux gnu libopenjpeg.so.1.5.2 7f9a11493000 7f9a11692000 p 00022000 08 21 62790792 usr lib x86 64 linux gnu libopenjpeg.so.1.5.2 7f9a11692000 7f9a11693000 r p 00021000 08 21 62790792 usr lib x86 64 linux gnu libopenjpeg.so.1.5.2 7f9a11693000 7f9a11694000 rw p 00022000 08 21 62790792 usr lib x86 64 linux gnu libopenjpeg.so.1.5.2 7f9a11694000 7f9a116dd000 r xp 00000000 08 21 62790797 usr lib x86 64 linux gnu libopus.so.0.5.2 7f9a116dd000 7f9a118dc000 p 00049000 08 21 62790797 usr lib x86 64 linux gnu libopus.so.0.5.2 7f9a118dc000 7f9a118dd000 r p 00048000 08 21 62790797 usr lib x86 64 linux gnu libopus.so.0.5.2 7f9a118dd000 7f9a118de000 rw p 00049000 08 21 62790797 usr lib x86 64 linux gnu libopus.so.0.5.2 7f9a118de000 7f9a119af000 r xp 00000000 08 21 62790861 usr lib x86 64 linux gnu libschroedinger 1.0.so.0.11.0 7f9a119af000 7f9a11baf000 p 000d1000 08 21 62790861 usr lib x86 64 linux gnu libschroedinger 1.0.so.0.11.0 7f9a11baf000 7f9a11bb1000 r p 000d1000 08 21 62790861 usr lib x86 64 linux gnu libschroedinger 1.0.so.0.11.0 7f9a11bb1000 7f9a11bb2000 rw p 000d3000 08 21 62790861 usr lib x86 64 linux gnu libschroedinger 1.0.so.0.11.0 7f9a11bb2000 7f9a11bb3000 rw p 00000000 00 00 0 7f9a11bb3000 7f9a11bbf000 r xp 00000000 08 21 62790865 usr lib x86 64 linux gnu libshine.so.3.0.1 7f9a11bbf000 7f9a11dbe000 p 0000c000 08 21 62790865 usr lib x86 64 linux gnu libshine.so.3.0.1 7f9a11dbe000 7f9a11dbf000 r p 0000b000 08 21 62790865 usr lib x86 64 linux gnu libshine.so.3.0.1 7f9a11dbf000 7f9a11dc0000 rw p 0000c000 08 21 62790865 usr lib x86 64 linux gnu libshine.so.3.0.1 7f9a11dc0000 7f9a11dc7000 r xp 00000000 08 21 62790867 usr lib x86 64 linux gnu libsnappy.so.1.3.0 7f9a11dc7000 7f9a11fc6000 p 00007000 08 21 62790867 usr lib x86 64 linux gnu libsnappy.so.1.3.0 7f9a11fc6000 7f9a11fc7000 r p 00006000 08 21 62790867 usr lib x86 64 linux gnu libsnappy.so.1.3.0 7f9a11fc7000 7f9a11fc8000 rw p 00007000 08 21 62790867 usr lib x86 64 linux gnu libsnappy.so.1.3.0 7f9a11fc8000 7f9a159f9000 r xp 00000000 08 21 50074696 usr local cuda 9.0 targets x86 64 linux lib libcublas.so.9.0.480 7f9a159f9000 7f9a15bf9000 p 03a31000 08 21 50074696 usr local cuda 9.0 targets x86 64 linux lib libcublas.so.9.0.480 7f9a15bf9000 7f9a15c35000 rw p 03a31000 08 21 50074696 usr local cuda 9.0 targets x86 64 linux lib libcublas.so.9.0.480 7f9a15c35000 7f9a17c45000 rw p 00000000 00 00 0 7f9a17dd4000 7f9a17deb000 r xp 00000000 08 21 62790871 usr lib x86 64 linux gnu libspeex.so.1.5.0 7f9a17deb000 7f9a17feb000 p 00017000 08 21 62790871 usr lib x86 64 linux gnu libspeex.so.1.5.0 7f9a17feb000 7f9a17fec000 r p 00017000 08 21 62790871 usr lib x86 64 linux gnu libspeex.so.1.5.0 7f9a17fec000 7f9a17fed000 rw p 00018000 08 21 62790871 usr lib x86 64 linux gnu libspeex.so.1.5.0 7f9a17fed000 7f9a18006000 r xp 00000000 08 21 62790896 usr lib x86 64 linux gnu libtheoradec.so.1.1.4 7f9a18006000 7f9a18205000 p 00019000 08 21 62790896 usr lib x86 64 linux gnu libtheoradec.so.1.1.4 7f9a18205000 7f9a18206000 r p 00018000 08 21 62790896 usr lib x86 64 linux gnu libtheoradec.so.1.1.4 7f9a18206000 7f9a18207000 rw p 00019000 08 21 62790896 usr lib x86 64 linux gnu libtheoradec.so.1.1.4 7f9a18207000 7f9a18245000 r xp 00000000 08 21 62790898 usr lib x86 64 linux gnu libtheoraenc.so.1.1.2 7f9a18245000 7f9a18444000 p 0003e000 08 21 62790898 usr lib x86 64 linux gnu libtheoraenc.so.1.1.2 7f9a18444000 7f9a18445000 r p 0003d000 08 21 62790898 usr lib x86 64 linux gnu libtheoraenc.so.1.1.2 7f9a18445000 7f9a18446000 rw p 0003e000 08 21 62790898 usr lib x86 64 linux gnu libtheoraenc.so.1.1.2 7f9a18446000 7f9a3a446000 rw p 00000000 00 00 0 7f9a3a550000 7f9a3a56e000 r xp 00000000 08 21 62790909 usr lib x86 64 linux gnu libtwolame.so.0.0.0 7f9a3a56e000 7f9a3a76d000 p 0001e000 08 21 62790909 usr lib x86 64 linux gnu libtwolame.so.0.0.0 7f9a3a76d000 7f9a3a76e000 r p 0001d000 08 21 62790909 usr lib x86 64 linux gnu libtwolame.so.0.0.0 7f9a3a76e000 7f9a3a76f000 rw p 0001e000 08 21 62790909 usr lib x86 64 linux gnu libtwolame.so.0.0.0 7f9a3a76f000 7f9a3a773000 rw p 00000000 00 00 0 7f9a3a773000 7f9a3a79d000 r xp 00000000 08 21 62790927 usr lib x86 64 linux gnu libvorbis.so.0.4.8 7f9a3a79d000 7f9a3a99c000 p 0002a000 08 21 62790927 usr lib x86 64 linux gnu libvorbis.so.0.4.8 7f9a3a99c000 7f9a3a99d000 r p 00029000 08 21 62790927 usr lib x86 64 linux gnu libvorbis.so.0.4.8 7f9a3a99d000 7f9a3a99e000 rw p 0002a000 08 21 62790927 usr lib x86 64 linux gnu libvorbis.so.0.4.8 7f9a3a99e000 7f9a3aa2b000 r xp 00000000 08 21 62790929 usr lib x86 64 linux gnu libvorbisenc.so.2.0.11 7f9a3aa2b000 7f9a3ac2a000 p 0008d000 08 21 62790929 usr lib x86 64 linux gnu libvorbisenc.so.2.0.11 7f9a3ac2a000 7f9a3ac46000 r p 0008c000 08 21 62790929 usr lib x86 64 linux gnu libvorbisenc.so.2.0.11 7f9a3ac46000 7f9a3ac47000 rw p 000a8000 08 21 62790929 usr lib x86 64 linux gnu libvorbisenc.so.2.0.11 7f9a3ac47000 7f9a3ec47000 rw p 00000000 00 00 0 7f9a3edfb000 7f9a3f01a000 r xp 00000000 08 21 62790932 usr lib x86 64 linux gnu libvpx.so.3.0.0 7f9a3f01a000 7f9a3f219000 p 0021f000 08 21 62790932 usr lib x86 64 linux gnu libvpx.so.3.0.0 7f9a3f219000 7f9a3f21b000 r p 0021e000 08 21 62790932 usr lib x86 64 linux gnu libvpx.so.3.0.0 7f9a3f21b000 7f9a3f21c000 rw p 00220000 08 21 62790932 usr lib x86 64 linux gnu libvpx.so.3.0.0 7f9a3f21c000 7f9a3f21f000 rw p 00000000 00 00 0 7f9a3f21f000 7f9a3f247000 r xp 00000000 08 21 62790934 usr lib x86 64 linux gnu libwavpack.so.1.1.7 7f9a3f247000 7f9a3f446000 p 00028000 08 21 62790934 usr lib x86 64 linux gnu libwavpack.so.1.1.7 7f9a3f446000 7f9a3f447000 r p 00027000 08 21 62790934 usr lib x86 64 linux gnu libwavpack.so.1.1.7 7f9a3f447000 7f9a3f448000 rw p 00028000 08 21 62790934 usr lib x86 64 linux gnu libwavpack.so.1.1.7 7f9a3f448000 7f9a41448000 rw p 00000000 00 00 0 7f9a4148d000 7f9a414e6000 r xp 00000000 08 21 62790936 usr lib x86 64 linux gnu libwebp.so.5.0.4 7f9a414e6000 7f9a416e6000 p 00059000 08 21 62790936 usr lib x86 64 linux gnu libwebp.so.5.0.4 7f9a416e6000 7f9a416e7000 r p 00059000 08 21 62790936 usr lib x86 64 linux gnu libwebp.so.5.0.4 7f9a416e7000 7f9a416e9000 rw p 0005a000 08 21 62790936 usr lib x86 64 linux gnu libwebp.so.5.0.4 7f9a416e9000 7f9a41811000 r xp 00000000 08 21 62790937 usr lib x86 64 linux gnu libx264.so.148 7f9a41811000 7f9a41a10000 p 00128000 08 21 62790937 usr lib x86 64 linux gnu libx264.so.148 7f9a41a10000 7f9a41a11000 r p 00127000 08 21 62790937 usr lib x86 64 linux gnu libx264.so.148 7f9a41a11000 7f9a41a12000 rw p 00128000 08 21 62790937 usr lib x86 64 linux gnu libx264.so.148 7f9a41a12000 7f9a41a8d000 rw p 00000000 00 00 0 7f9a41a8d000 7f9a4249a000 r xp 00000000 08 21 62790938 usr lib x86 64 linux gnu libx265.so.79 7f9a4249a000 7f9a42699000 p 00a0d000 08 21 62790938 usr lib x86 64 linux gnu libx265.so.79 7f9a42699000 7f9a4269c000 r p 00a0c000 08 21 62790938 usr lib x86 64 linux gnu libx265.so.79 7f9a4269c000 7f9a4269f000 rw p 00a0f000 08 21 62790938 usr lib x86 64 linux gnu libx265.so.79 7f9a4269f000 7f9a426ac000 rw p 00000000 00 00 0 7f9a426ac000 7f9a4274d000 r xp 00000000 08 21 62790966 usr lib x86 64 linux gnu libxvidcore.so.4.3 7f9a4274d000 7f9a4294c000 p 000a1000 08 21 62790966 usr lib x86 64 linux gnu libxvidcore.so.4.3 7f9a4294c000 7f9a4294d000 r p 000a0000 08 21 62790966 usr lib x86 64 linux gnu libxvidcore.so.4.3 7f9a4294d000 7f9a42957000 rw p 000a1000 08 21 62790966 usr lib x86 64 linux gnu libxvidcore.so.4.3 7f9a42957000 7f9a429c0000 rw p 00000000 00 00 0 7f9a429c0000 7f9a42a37000 r xp 00000000 08 21 62790972 usr lib x86 64 linux gnu libzvbi.so.0.13.2 7f9a42a37000 7f9a42c36000 p 00077000 08 21 62790972 usr lib x86 64 linux gnu libzvbi.so.0.13.2 7f9a42c36000 7f9a42c3f000 r p 00076000 08 21 62790972 usr lib x86 64 linux gnu libzvbi.so.0.13.2 7f9a42c3f000 7f9a42c4b000 rw p 0007f000 08 21 62790972 usr lib x86 64 linux gnu libzvbi.so.0.13.2 7f9a42c4b000 7f9a44c4b000 rw p 00000000 00 00 0 7f9a44d20000 7f9a44d3b000 r xp 00000000 08 21 62790925 usr lib x86 64 linux gnu libva.so.1.3900.0 7f9a44d3b000 7f9a44f3a000 p 0001b000 08 21 62790925 usr lib x86 64 linux gnu libva.so.1.3900.0 7f9a44f3a000 7f9a44f3b000 r p 0001a000 08 21 62790925 usr lib x86 64 linux gnu libva.so.1.3900.0 7f9a44f3b000 7f9a44f3c000 rw p 0001b000 08 21 62790925 usr lib x86 64 linux gnu libva.so.1.3900.0 7f9a44f3c000 7f9a44f56000 r xp 00000000 08 21 62790879 usr lib x86 64 linux gnu libswresample ffmpeg.so.1.2.101 7f9a44f56000 7f9a45156000 p 0001a000 08 21 62790879 usr lib x86 64 linux gnu libswresample ffmpeg.so.1.2.101 7f9a45156000 7f9a45158000 r p 0001a000 08 21 62790879 usr lib x86 64 linux gnu libswresample ffmpeg.so.1.2.101 7f9a45158000 7f9a45159000 rw p 0001c000 08 21 62790879 usr lib x86 64 linux gnu libswresample ffmpeg.so.1.2.101 7f9a45159000 7f9a45162000 r xp 00000000 08 21 62790918 usr lib x86 64 linux gnu libv4l2.so.0.0.0 7f9a45162000 7f9a45361000 p 00009000 08 21 62790918 usr lib x86 64 linux gnu libv4l2.so.0.0.0 7f9a45361000 7f9a45362000 r p 00008000 08 21 62790918 usr lib x86 64 linux gnu libv4l2.so.0.0.0 7f9a45362000 7f9a45367000 rw p 00009000 08 21 62790918 usr lib x86 64 linux gnu libv4l2.so.0.0.0 7f9a45367000 7f9a4537e000 r xp 00000000 08 21 62529465 lib x86 64 linux gnu libusb 1.0.so.0.1.0 7f9a4537e000 7f9a4557d000 p 00017000 08 21 62529465 lib x86 64 linux gnu libusb 1.0.so.0.1.0 7f9a4557d000 7f9a4557e000 r p 00016000 08 21 62529465 lib x86 64 linux gnu libusb 1.0.so.0.1.0 7f9a4557e000 7f9a4557f000 rw p 00017000 08 21 62529465 lib x86 64 linux gnu libusb 1.0.so.0.1.0 7f9a4557f000 7f9a4558c000 r xp 00000000 08 21 62790859 usr lib x86 64 linux gnu libraw1394.so.11.1.0 7f9a4558c000 7f9a4578c000 p 0000d000 08 21 62790859 usr lib x86 64 linux gnu libraw1394.so.11.1.0 7f9a4578c000 7f9a4578d000 r p 0000d000 08 21 62790859 usr lib x86 64 linux gnu libraw1394.so.11.1.0 7f9a4578d000 7f9a4578e000 rw p 0000e000 08 21 62790859 usr lib x86 64 linux gnu libraw1394.so.11.1.0 7f9a4578e000 7f9a457ac000 r xp 00000000 08 21 62790815 usr lib x86 64 linux gnu libpangox 1.0.so.0.0.0 7f9a457ac000 7f9a459ac000 p 0001e000 08 21 62790815 usr lib x86 64 linux gnu libpangox 1.0.so.0.0.0 7f9a459ac000 7f9a459ad000 r p 0001e000 08 21 62790815 usr lib x86 64 linux gnu libpangox 1.0.so.0.0.0 7f9a459ad000 7f9a459ae000 rw p 0001f000 08 21 62790815 usr lib x86 64 linux gnu libpangox 1.0.so.0.0.0 7f9a459ae000 7f9a459c6000 r xp 00000000 08 21 62790518 usr lib x86 64 linux gnu libxmu.so.6.2.0 7f9a459c6000 7f9a45bc5000 p 00018000 08 21 62790518 usr lib x86 64 linux gnu libxmu.so.6.2.0 7f9a45bc5000 7f9a45bc6000 r p 00017000 08 21 62790518 usr lib x86 64 linux gnu libxmu.so.6.2.0 7f9a45bc6000 7f9a45bc7000 rw p 00018000 08 21 62790518 usr lib x86 64 linux gnu libxmu.so.6.2.0 7f9a45bc7000 7f9a45c34000 r xp 00000000 08 21 62790434 usr lib x86 64 linux gnu libglu.so.1.3.1 7f9a45c34000 7f9a45e34000 p 0006d000 08 21 62790434 usr lib x86 64 linux gnu libglu.so.1.3.1 7f9a45e34000 7f9a45e35000 r p 0006d000 08 21 62790434 usr lib x86 64 linux gnu libglu.so.1.3.1 7f9a45e35000 7f9a45e36000 rw p 0006e000 08 21 62790434 usr lib x86 64 linux gnu libglu.so.1.3.1 7f9a45e36000 7f9a45e38000 r xp 00000000 08 21 62790484 usr lib x86 64 linux gnu libxcomposite.so.1.0.0 7f9a45e38000 7f9a46037000 p 00002000 08 21 62790484 usr lib x86 64 linux gnu libxcomposite.so.1.0.0 7f9a46037000 7f9a46038000 r p 00001000 08 21 62790484 usr lib x86 64 linux gnu libxcomposite.so.1.0.0 7f9a46038000 7f9a46039000 rw p 00002000 08 21 62790484 usr lib x86 64 linux gnu libxcomposite.so.1.0.0 7f9a46039000 7f9a46042000 r xp 00000000 08 21 62790488 usr lib x86 64 linux gnu libxcursor.so.1.0.2 7f9a46042000 7f9a46241000 p 00009000 08 21 62790488 usr lib x86 64 linux gnu libxcursor.so.1.0.2 7f9a46241000 7f9a46242000 r p 00008000 08 21 62790488 usr lib x86 64 linux gnu libxcursor.so.1.0.2 7f9a46242000 7f9a46243000 rw p 00009000 08 21 62790488 usr lib x86 64 linux gnu libxcursor.so.1.0.2 7f9a46243000 7f9a4624d000 r xp 00000000 08 21 62790524 usr lib x86 64 linux gnu libxrandr.so.2.2.0 7f9a4624d000 7f9a4644c000 p 0000a000 08 21 62790524 usr lib x86 64 linux gnu libxrandr.so.2.2.0 7f9a4644c000 7f9a4644d000 r p 00009000 08 21 62790524 usr lib x86 64 linux gnu libxrandr.so.2.2.0 7f9a4644d000 7f9a4644e000 rw p 0000a000 08 21 62790524 usr lib x86 64 linux gnu libxrandr.so.2.2.0 7f9a4644e000 7f9a4844e000 rw p 00000000 00 00 0 7f9a48485000 7f9a48626000 rw p 00000000 00 00 0 7f9a48626000 7f9a48635000 r xp 00000000 08 21 62790512 usr lib x86 64 linux gnu libxi.so.6.1.0 7f9a48635000 7f9a48834000 p 0000f000 08 21 62790512 usr lib x86 64 linux gnu libxi.so.6.1.0 7f9a48834000 7f9a48835000 r p 0000e000 08 21 62790512 usr lib x86 64 linux gnu libxi.so.6.1.0 7f9a48835000 7f9a48836000 rw p 0000f000 08 21 62790512 usr lib x86 64 linux gnu libxi.so.6.1.0 7f9a48836000 7f9a48838000 r xp 00000000 08 21 62790516 usr lib x86 64 linux gnu libxinerama.so.1.0.0 7f9a48838000 7f9a48a37000 p 00002000 08 21 62790516 usr lib x86 64 linux gnu libxinerama.so.1.0.0 7f9a48a37000 7f9a48a38000 r p 00001000 08 21 62790516 usr lib x86 64 linux gnu libxinerama.so.1.0.0 7f9a48a38000 7f9a48a39000 rw p 00002000 08 21 62790516 usr lib x86 64 linux gnu libxinerama.so.1.0.0 7f9a48a39000 7f9a48a76000 r xp 00000000 08 21 62790606 usr lib x86 64 linux gnu libfontconfig.so.1.9.0 7f9a48a76000 7f9a48c75000 p 0003d000 08 21 62790606 usr lib x86 64 linux gnu libfontconfig.so.1.9.0 7f9a48c75000 7f9a48c77000 r p 0003c000 08 21 62790606 usr lib x86 64 linux gnu libfontconfig.so.1.9.0 7f9a48c77000 7f9a48c7c000 rw p 0003e000 08 21 62790606 usr lib x86 64 linux gnu libfontconfig.so.1.9.0 7f9a48c7c000 7f9a48cc5000 r xp 00000000 08 21 62790805 usr lib x86 64 linux gnu libpango 1.0.so.0.3800.1 7f9a48cc5000 7f9a48ec5000 p 00049000 08 21 62790805 usr lib x86 64 linux gnu libpango 1.0.so.0.3800.1 7f9a48ec5000 7f9a48ec7000 r p 00049000 08 21 62790805 usr lib x86 64 linux gnu libpango 1.0.so.0.3800.1 7f9a48ec7000 7f9a48ec8000 rw p 0004b000 08 21 62790805 usr lib x86 64 linux gnu libpango 1.0.so.0.3800.1 7f9a48ec8000 7f9a48edc000 r xp 00000000 08 21 62790813 usr lib x86 64 linux gnu libpangoft2 1.0.so.0.3800.1 7f9a48edc000 7f9a490dc000 p 00014000 08 21 62790813 usr lib x86 64 linux gnu libpangoft2 1.0.so.0.3800.1 7f9a490dc000 7f9a490dd000 r p 00014000 08 21 62790813 usr lib x86 64 linux gnu libpangoft2 1.0.so.0.3800.1 7f9a490dd000 7f9a490de000 rw p 00015000 08 21 62790813 usr lib x86 64 linux gnu libpangoft2 1.0.so.0.3800.1 7f9a490de000 7f9a4925e000 r xp 00000000 08 21 62790631 usr lib x86 64 linux gnu libgio 2.0.so.0.4800.2 7f9a4925e000 7f9a4945e000 p 00180000 08 21 62790631 usr lib x86 64 linux gnu libgio 2.0.so.0.4800.2 7f9a4945e000 7f9a49462000 r p 00180000 08 21 62790631 usr lib x86 64 linux gnu libgio 2.0.so.0.4800.2 7f9a49462000 7f9a49464000 rw p 00184000 08 21 62790631 usr lib x86 64 linux gnu libgio 2.0.so.0.4800.2 7f9a49464000 7f9a49466000 rw p 00000000 00 00 0 7f9a49466000 7f9a49487000 r xp 00000000 08 21 62790620 usr lib x86 64 linux gnu libgdk pixbuf 2.0.so.0.3200.2 7f9a49487000 7f9a49686000 p 00021000 08 21 62790620 usr lib x86 64 linux gnu libgdk pixbuf 2.0.so.0.3200.2 7f9a49686000 7f9a49687000 r p 00020000 08 21 62790620 usr lib x86 64 linux gnu libgdk pixbuf 2.0.so.0.3200.2 7f9a49687000 7f9a49688000 rw p 00021000 08 21 62790620 usr lib x86 64 linux gnu libgdk pixbuf 2.0.so.0.3200.2 7f9a49688000 7f9a49796000 r xp 00000000 08 21 62790571 usr lib x86 64 linux gnu libcairo.so.2.11400.6 7f9a49796000 7f9a49996000 p 0010e000 08 21 62790571 usr lib x86 64 linux gnu libcairo.so.2.11400.6 7f9a49996000 7f9a49999000 r p 0010e000 08 21 62790571 usr lib x86 64 linux gnu libcairo.so.2.11400.6 7f9a49999000 7f9a4999a000 rw p 00111000 08 21 62790571 usr lib x86 64 linux gnu libcairo.so.2.11400.6 7f9a4999a000 7f9a4999c000 rw p 00000000 00 00 0 7f9a4999c000 7f9a499be000 r xp 00000000 08 21 62790535 usr lib x86 64 linux gnu libatk 1.0.so.0.21809.1 7f9a499be000 7f9a49bbd000 p 00022000 08 21 62790535 usr lib x86 64 linux gnu libatk 1.0.so.0.21809.1 7f9a49bbd000 7f9a49bc0000 r p 00021000 08 21 62790535 usr lib x86 64 linux gnu libatk 1.0.so.0.21809.1 7f9a49bc0000 7f9a49bc1000 rw p 00024000 08 21 62790535 usr lib x86 64 linux gnu libatk 1.0.so.0.21809.1 7f9a49bc1000 7f9a4ba25000 r xp 00000000 08 21 62786977 usr lib libopenblasp r0.2.18.so 7f9a4ba25000 7f9a4bc24000 p 01e64000 08 21 62786977 usr lib libopenblasp r0.2.18.so 7f9a4bc24000 7f9a4bc2a000 r p 01e63000 08 21 62786977 usr lib libopenblasp r0.2.18.so 7f9a4bc2a000 7f9a4bc3c000 rw p 01e69000 08 21 62786977 usr lib libopenblasp r0.2.18.so 7f9a4bc3c000 7f9a4dc55000 rw p 00000000 00 00 0 7f9a4dc7a000 7f9a4ddba000 rw p 00000000 00 00 0 7f9a4ddba000 7f9a4ddc6000 r xp 00000000 08 21 62790809 usr lib x86 64 linux gnu libpangocairo 1.0.so.0.3800.1 7f9a4ddc6000 7f9a4dfc5000 p 0000c000 08 21 62790809 usr lib x86 64 linux gnu libpangocairo 1.0.so.0.3800.1 7f9a4dfc5000 7f9a4dfc6000 r p 0000b000 08 21 62790809 usr lib x86 64 linux gnu libpangocairo 1.0.so.0.3800.1 7f9a4dfc6000 7f9a4dfc7000 rw p 0000c000 08 21 62790809 usr lib x86 64 linux gnu libpangocairo 1.0.so.0.3800.1 7f9a4dfc7000 7f9a4dfca000 r xp 00000000 08 21 62790643 usr lib x86 64 linux gnu libgmodule 2.0.so.0.4800.2 7f9a4dfca000 7f9a4e1c9000 p 00003000 08 21 62790643 usr lib x86 64 linux gnu libgmodule 2.0.so.0.4800.2 7f9a4e1c9000 7f9a4e1ca000 r p 00002000 08 21 62790643 usr lib x86 64 linux gnu libgmodule 2.0.so.0.4800.2 7f9a4e1ca000 7f9a4e1cb000 rw p 00003000 08 21 62790643 usr lib x86 64 linux gnu libgmodule 2.0.so.0.4800.2 7f9a4e1cb000 7f9a4e1d0000 r xp 00000000 08 21 62790457 usr lib x86 64 linux gnu libilmthread 2 2.so.12.0.0 7f9a4e1d0000 7f9a4e3d0000 p 00005000 08 21 62790457 usr lib x86 64 linux gnu libilmthread 2 2.so.12.0.0 7f9a4e3d0000 7f9a4e3d1000 r p 00005000 08 21 62790457 usr lib x86 64 linux gnu libilmthread 2 2.so.12.0.0 7f9a4e3d1000 7f9a4e3d2000 rw p 00006000 08 21 62790457 usr lib x86 64 linux gnu libilmthread 2 2.so.12.0.0 7f9a4e3d2000 7f9a4e3ed000 r xp 00000000 08 21 62790443 usr lib x86 64 linux gnu libiex 2 2.so.12.0.0 7f9a4e3ed000 7f9a4e5ec000 p 0001b000 08 21 62790443 usr lib x86 64 linux gnu libiex 2 2.so.12.0.0 7f9a4e5ec000 7f9a4e5ef000 r p 0001a000 08 21 62790443 usr lib x86 64 linux gnu libiex 2 2.so.12.0.0 7f9a4e5ef000 7f9a4e5f0000 rw p 0001d000 08 21 62790443 usr lib x86 64 linux gnu libiex 2 2.so.12.0.0 7f9a4e5f0000 7f9a4e5fb000 r xp 00000000 08 21 62790701 usr lib x86 64 linux gnu libjbig.so.0 7f9a4e5fb000 7f9a4e7fa000 p 0000b000 08 21 62790701 usr lib x86 64 linux gnu libjbig.so.0 7f9a4e7fa000 7f9a4e7fb000 r p 0000a000 08 21 62790701 usr lib x86 64 linux gnu libjbig.so.0 7f9a4e7fb000 7f9a4e7fe000 rw p 0000b000 08 21 62790701 usr lib x86 64 linux gnu libjbig.so.0 7f9a4e7fe000 7f9a4e81f000 r xp 00000000 08 21 38015914 lib x86 64 linux gnu liblzma.so.5.0.0 7f9a4e81f000 7f9a4ea1e000 p 00021000 08 21 38015914 lib x86 64 linux gnu liblzma.so.5.0.0 7f9a4ea1e000 7f9a4ea1f000 r p 00020000 08 21 38015914 lib x86 64 linux gnu liblzma.so.5.0.0 7f9a4ea1f000 7f9a4ea20000 rw p 00021000 08 21 38015914 lib x86 64 linux gnu liblzma.so.5.0.0 7f9a4ea20000 7f9a4ea30000 r xp 00000000 08 21 62790582 usr lib x86 64 linux gnu libdrm.so.2.4.0 7f9a4ea30000 7f9a4ec30000 p 00010000 08 21 62790582 usr lib x86 64 linux gnu libdrm.so.2.4.0 7f9a4ec30000 7f9a4ec31000 r p 00010000 08 21 62790582 usr lib x86 64 linux gnu libdrm.so.2.4.0 7f9a4ec31000 7f9a4ec32000 rw p 00011000 08 21 62790582 usr lib x86 64 linux gnu libdrm.so.2.4.0 7f9a4ec32000 7f9a4ec37000 r xp 00000000 08 21 62790532 usr lib x86 64 linux gnu libxxf86vm.so.1.0.0 7f9a4ec37000 7f9a4ee36000 p 00005000 08 21 62790532 usr lib x86 64 linux gnu libxxf86vm.so.1.0.0 7f9a4ee36000 7f9a4ee37000 r p 00004000 08 21 62790532 usr lib x86 64 linux gnu libxxf86vm.so.1.0.0 7f9a4ee37000 7f9a4ee38000 rw p 00005000 08 21 62790532 usr lib x86 64 linux gnu libxxf86vm.so.1.0.0 7f9a4ee38000 7f9a4ee3c000 r xp 00000000 08 21 62790940 usr lib x86 64 linux gnu libxcb dri2.so.0.0.0 7f9a4ee3c000 7f9a4f03b000 p 00004000 08 21 62790940 usr lib x86 64 linux gnu libxcb dri2.so.0.0.0 7f9a4f03b000 7f9a4f03c000 r p 00003000 08 21 62790940 usr lib x86 64 linux gnu libxcb dri2.so.0.0.0 7f9a4f03c000 7f9a4f03d000 rw p 00004000 08 21 62790940 usr lib x86 64 linux gnu libxcb dri2.so.0.0.0 7f9a4f03d000 7f9a4f054000 r xp 00000000 08 21 62790944 usr lib x86 64 linux gnu libxcb glx.so.0.0.0 7f9a4f054000 7f9a4f253000 p 00017000 08 21 62790944 usr lib x86 64 linux gnu libxcb glx.so.0.0.0 7f9a4f253000 7f9a4f255000 r p 00016000 08 21 62790944 usr lib x86 64 linux gnu libxcb glx.so.0.0.0 7f9a4f255000 7f9a4f256000 rw p 00018000 08 21 62790944 usr lib x86 64 linux gnu libxcb glx.so.0.0.0 7f9a4f256000 7f9a4f257000 r xp 00000000 08 21 62790469 usr lib x86 64 linux gnu libx11 xcb.so.1.0.0 7f9a4f257000 7f9a4f456000 p 00001000 08 21 62790469 usr lib x86 64 linux gnu libx11 xcb.so.1.0.0 7f9a4f456000 7f9a4f457000 r p 00000000 08 21 62790469 usr lib x86 64 linux gnu libx11 xcb.so.1.0.0 7f9a4f457000 7f9a4f458000 rw p 00001000 08 21 62790469 usr lib x86 64 linux gnu libx11 xcb.so.1.0.0 7f9a4f458000 7f9a51458000 rw p 00000000 00 00 0 7f9a5145f000 7f9a5161f000 rw p 00000000 00 00 0 7f9a5161f000 7f9a51624000 r xp 00000000 08 21 62790504 usr lib x86 64 linux gnu libxfixes.so.3.1.0 7f9a51624000 7f9a51823000 p 00005000 08 21 62790504 usr lib x86 64 linux gnu libxfixes.so.3.1.0 7f9a51823000 7f9a51824000 r p 00004000 08 21 62790504 usr lib x86 64 linux gnu libxfixes.so.3.1.0 7f9a51824000 7f9a51825000 rw p 00005000 08 21 62790504 usr lib x86 64 linux gnu libxfixes.so.3.1.0 7f9a51825000 7f9a51827000 r xp 00000000 08 21 62790492 usr lib x86 64 linux gnu libxdamage.so.1.1.0 7f9a51827000 7f9a51a26000 p 00002000 08 21 62790492 usr lib x86 64 linux gnu libxdamage.so.1.1.0 7f9a51a26000 7f9a51a27000 r p 00001000 08 21 62790492 usr lib x86 64 linux gnu libxdamage.so.1.1.0 7f9a51a27000 7f9a51a28000 rw p 00002000 08 21 62790492 usr lib x86 64 linux gnu libxdamage.so.1.1.0 7f9a51a28000 7f9a51a54000 r xp 00000000 08 21 62790635 usr lib x86 64 linux gnu libglapi.so.0.0.0 7f9a51a54000 7f9a51c53000 p 0002c000 08 21 62790635 usr lib x86 64 linux gnu libglapi.so.0.0.0 7f9a51c53000 7f9a51c57000 r p 0002b000 08 21 62790635 usr lib x86 64 linux gnu libglapi.so.0.0.0 7f9a51c57000 7f9a51c58000 rw p 0002f000 08 21 62790635 usr lib x86 64 linux gnu libglapi.so.0.0.0 7f9a51c58000 7f9a57c59000 rw p 00000000 00 00 0 7f9a57c8d000 7f9a57e4d000 rw p 00000000 00 00 0 7f9a57e4d000 7f9a57e4e000 r xp 00000000 08 21 62790964 usr lib x86 64 linux gnu libxshmfence.so.1.0.0 7f9a57e4e000 7f9a5804e000 p 00001000 08 21 62790964 usr lib x86 64 linux gnu libxshmfence.so.1.0.0 7f9a5804e000 7f9a5804f000 r p 00001000 08 21 62790964 usr lib x86 64 linux gnu libxshmfence.so.1.0.0 7f9a5804f000 7f9a58050000 rw p 00002000 08 21 62790964 usr lib x86 64 linux gnu libxshmfence.so.1.0.0 7f9a58050000 7f9a58055000 r xp 00000000 08 21 62790956 usr lib x86 64 linux gnu libxcb sync.so.1.0.0 7f9a58055000 7f9a58255000 p 00005000 08 21 62790956 usr lib x86 64 linux gnu libxcb sync.so.1.0.0 7f9a58255000 7f9a58256000 r p 00005000 08 21 62790956 usr lib x86 64 linux gnu libxcb sync.so.1.0.0 7f9a58256000 7f9a58257000 rw p 00006000 08 21 62790956 usr lib x86 64 linux gnu libxcb sync.so.1.0.0 7f9a58257000 7f9a58259000 r xp 00000000 08 21 62790946 usr lib x86 64 linux gnu libxcb present.so.0.0.0 7f9a58259000 7f9a58458000 p 00002000 08 21 62790946 usr lib x86 64 linux gnu libxcb present.so.0.0.0 7f9a58458000 7f9a58459000 r p 00001000 08 21 62790946 usr lib x86 64 linux gnu libxcb present.so.0.0.0 7f9a58459000 7f9a5845a000 rw p 00002000 08 21 62790946 usr lib x86 64 linux gnu libxcb present.so.0.0.0 7f9a5845a000 7f9a5a45a000 rw p 00000000 00 00 0 7f9a5a48d000 7f9a5a5cd000 rw p 00000000 00 00 0 7f9a5a5cd000 7f9a5a5cf000 r xp 00000000 08 21 62790942 usr lib x86 64 linux gnu libxcb dri3.so.0.0.0 7f9a5a5cf000 7f9a5a7ce000 p 00002000 08 21 62790942 usr lib x86 64 linux gnu libxcb dri3.so.0.0.0 7f9a5a7ce000 7f9a5a7cf000 r p 00001000 08 21 62790942 usr lib x86 64 linux gnu libxcb dri3.so.0.0.0 7f9a5a7cf000 7f9a5a7d0000 rw p 00002000 08 21 62790942 usr lib x86 64 linux gnu libxcb dri3.so.0.0.0 7f9a5a7d0000 7f9a5a80e000 r xp 00000000 08 21 53355505 usr lib x86 64 linux gnu libquadmath.so.0.0.0 7f9a5a80e000 7f9a5aa0d000 p 0003e000 08 21 53355505 usr lib x86 64 linux gnu libquadmath.so.0.0.0 7f9a5aa0d000 7f9a5aa0e000 r p 0003d000 08 21 53355505 usr lib x86 64 linux gnu libquadmath.so.0.0.0 7f9a5aa0e000 7f9a5aa0f000 rw p 0003e000 08 21 53355505 usr lib x86 64 linux gnu libquadmath.so.0.0.0 7f9a5aa0f000 7f9a5aa4c000 r xp 00000000 08 33 1712536 usr lib x86 64 linux gnu libnvidia fatbinaryloader.so.390.48 7f9a5aa4c000 7f9a5ac4b000 p 0003d000 08 33 1712536 usr lib x86 64 linux gnu libnvidia fatbinaryloader.so.390.48 7f9a5ac4b000 7f9a5ac56000 rw p 0003c000 08 33 1712536 usr lib x86 64 linux gnu libnvidia fatbinaryloader.so.390.48 7f9a5ac56000 7f9a5cc5b000 rw p 00000000 00 00 0 7f9a5cc5e000 7f9a5cd9e000 rw p 00000000 00 00 0 7f9a5cd9e000 7f9a5ce23000 r xp 00000000 08 21 62790885 usr lib x86 64 linux gnu libswscale ffmpeg.so.3.1.101 7f9a5ce23000 7f9a5d022000 p 00085000 08 21 62790885 usr lib x86 64 linux gnu libswscale ffmpeg.so.3.1.101 7f9a5d022000 7f9a5d024000 r p 00084000 08 21 62790885 usr lib x86 64 linux gnu libswscale ffmpeg.so.3.1.101 7f9a5d024000 7f9a5d025000 rw p 00086000 08 21 62790885 usr lib x86 64 linux gnu libswscale ffmpeg.so.3.1.101 7f9a5d025000 7f9a5d02d000 rw p 00000000 00 00 0 7f9a5d02d000 7f9a5db5e000 r xp 00000000 08 21 62790543 usr lib x86 64 linux gnu libavcodec ffmpeg.so.56.60.100 7f9a5db5e000 7f9a5dd5d000 p 00b31000 08 21 62790543 usr lib x86 64 linux gnu libavcodec ffmpeg.so.56.60.100 7f9a5dd5d000 7f9a5dd88000 r p 00b30000 08 21 62790543 usr lib x86 64 linux gnu libavcodec ffmpeg.so.56.60.100 7f9a5dd88000 7f9a5ddab000 rw p 00b5b000 08 21 62790543 usr lib x86 64 linux gnu libavcodec ffmpeg.so.56.60.100 7f9a5ddab000 7f9a6045e000 rw p 00000000 00 00 0 7f9a60471000 7f9a604c7000 r xp 00000000 08 21 62790555 usr lib x86 64 linux gnu libavutil ffmpeg.so.54.31.100 7f9a604c7000 7f9a606c6000 p 00056000 08 21 62790555 usr lib x86 64 linux gnu libavutil ffmpeg.so.54.31.100 7f9a606c6000 7f9a606cc000 r p 00055000 08 21 62790555 usr lib x86 64 linux gnu libavutil ffmpeg.so.54.31.100 7f9a606cc000 7f9a606cd000 rw p 0005b000 08 21 62790555 usr lib x86 64 linux gnu libavutil ffmpeg.so.54.31.100 7f9a606cd000 7f9a606e0000 rw p 00000000 00 00 0 7f9a606e0000 7f9a608b6000 r xp 00000000 08 21 62790549 usr lib x86 64 linux gnu libavformat ffmpeg.so.56.40.101 7f9a608b6000 7f9a60ab6000 p 001d6000 08 21 62790549 usr lib x86 64 linux gnu libavformat ffmpeg.so.56.40.101 7f9a60ab6000 7f9a60aca000 r p 001d6000 08 21 62790549 usr lib x86 64 linux gnu libavformat ffmpeg.so.56.40.101 7f9a60aca000 7f9a60adf000 rw p 001ea000 08 21 62790549 usr lib x86 64 linux gnu libavformat ffmpeg.so.56.40.101 7f9a60adf000 7f9a60ae4000 r xp 00000000 08 21 62790916 usr lib x86 64 linux gnu libv4l1.so.0.0.0 7f9a60ae4000 7f9a60ce3000 p 00005000 08 21 62790916 usr lib x86 64 linux gnu libv4l1.so.0.0.0 7f9a60ce3000 7f9a60ce4000 r p 00004000 08 21 62790916 usr lib x86 64 linux gnu libv4l1.so.0.0.0 7f9a60ce4000 7f9a60ce5000 rw p 00005000 08 21 62790916 usr lib x86 64 linux gnu libv4l1.so.0.0.0 7f9a60ce5000 7f9a60d1a000 r xp 00000000 08 21 62790580 usr lib x86 64 linux gnu libdc1394.so.22.1.11 7f9a60d1a000 7f9a60f19000 p 00035000 08 21 62790580 usr lib x86 64 linux gnu libdc1394.so.22.1.11 7f9a60f19000 7f9a60f1a000 r p 00034000 08 21 62790580 usr lib x86 64 linux gnu libdc1394.so.22.1.11 7f9a60f1a000 7f9a60f1b000 rw p 00035000 08 21 62790580 usr lib x86 64 linux gnu libdc1394.so.22.1.11 7f9a60f1b000 7f9a60f5b000 rw p 00000000 00 00 0 7f9a60f5b000 7f9a60fb8000 r xp 00000000 08 21 62790625 usr lib x86 64 linux gnu libgdkglext x11 1.0.so.0.0.0 7f9a60fb8000 7f9a611b7000 p 0005d000 08 21 62790625 usr lib x86 64 linux gnu libgdkglext x11 1.0.so.0.0.0 7f9a611b7000 7f9a611b9000 r p 0005c000 08 21 62790625 usr lib x86 64 linux gnu libgdkglext x11 1.0.so.0.0.0 7f9a611b9000 7f9a611bf000 rw p 0005e000 08 21 62790625 usr lib x86 64 linux gnu libgdkglext x11 1.0.so.0.0.0 7f9a611bf000 7f9a611c2000 r xp 00000000 08 21 62790666 usr lib x86 64 linux gnu libgtkglext x11 1.0.so.0.0.0 7f9a611c2000 7f9a613c1000 p 00003000 08 21 62790666 usr lib x86 64 linux gnu libgtkglext x11 1.0.so.0.0.0 7f9a613c1000 7f9a613c2000 r p 00002000 08 21 62790666 usr lib x86 64 linux gnu libgtkglext x11 1.0.so.0.0.0 7f9a613c2000 7f9a613c3000 rw p 00003000 08 21 62790666 usr lib x86 64 linux gnu libgtkglext x11 1.0.so.0.0.0 7f9a613c3000 7f9a61415000 r xp 00000000 08 21 62790647 usr lib x86 64 linux gnu libgobject 2.0.so.0.4800.2 7f9a61415000 7f9a61614000 p 00052000 08 21 62790647 usr lib x86 64 linux gnu libgobject 2.0.so.0.4800.2 7f9a61614000 7f9a61615000 r p 00051000 08 21 62790647 usr lib x86 64 linux gnu libgobject 2.0.so.0.4800.2 7f9a61615000 7f9a61616000 rw p 00052000 08 21 62790647 usr lib x86 64 linux gnu libgobject 2.0.so.0.4800.2 7f9a61616000 7f9a61a54000 r xp 00000000 08 21 62790661 usr lib x86 64 linux gnu libgtk x11 2.0.so.0.2400.30 7f9a61a54000 7f9a61c53000 p 0043e000 08 21 62790661 usr lib x86 64 linux gnu libgtk x11 2.0.so.0.2400.30 7f9a61c53000 7f9a61c5a000 r p 0043d000 08 21 62790661 usr lib x86 64 linux gnu libgtk x11 2.0.so.0.2400.30 7f9a61c5a000 7f9a61c5e000 rw p 00444000 08 21 62790661 usr lib x86 64 linux gnu libgtk x11 2.0.so.0.2400.30 7f9a61c5e000 7f9a63c61000 rw p 00000000 00 00 0 7f9a63c9f000 7f9a63cdf000 rw p 00000000 00 00 0 7f9a63cdf000 7f9a63d8f000 r xp 00000000 08 21 62790617 usr lib x86 64 linux gnu libgdk x11 2.0.so.0.2400.30 7f9a63d8f000 7f9a63f8e000 p 000b0000 08 21 62790617 usr lib x86 64 linux gnu libgdk x11 2.0.so.0.2400.30 7f9a63f8e000 7f9a63f92000 r p 000af000 08 21 62790617 usr lib x86 64 linux gnu libgdk x11 2.0.so.0.2400.30 7f9a63f92000 7f9a63f94000 rw p 000b3000 08 21 62790617 usr lib x86 64 linux gnu libgdk x11 2.0.so.0.2400.30 7f9a63f94000 7f9a6415e000 r xp 00000000 08 21 62790449 usr lib x86 64 linux gnu libilmimf 2 2.so.22.0.0 7f9a6415e000 7f9a6435d000 p 001ca000 08 21 62790449 usr lib x86 64 linux gnu libilmimf 2 2.so.22.0.0 7f9a6435d000 7f9a64360000 r p 001c9000 08 21 62790449 usr lib x86 64 linux gnu libilmimf 2 2.so.22.0.0 7f9a64360000 7f9a64461000 rw p 001cc000 08 21 62790449 usr lib x86 64 linux gnu libilmimf 2 2.so.22.0.0 7f9a64461000 7f9a66462000 rw p 00000000 00 00 0 7f9a6648e000 7f9a6650e000 rw p 00000000 00 00 0 7f9a6650e000 7f9a66550000 r xp 00000000 08 21 62790437 usr lib x86 64 linux gnu libhalf.so.12.0.0 7f9a66550000 7f9a6674f000 p 00042000 08 21 62790437 usr lib x86 64 linux gnu libhalf.so.12.0.0 7f9a6674f000 7f9a66750000 r p 00041000 08 21 62790437 usr lib x86 64 linux gnu libhalf.so.12.0.0 7f9a66750000 7f9a66751000 rw p 00042000 08 21 62790437 usr lib x86 64 linux gnu libhalf.so.12.0.0 7f9a66751000 7f9a6679b000 r xp 00000000 08 21 62790698 usr lib x86 64 linux gnu libjasper.so.1.0.0 7f9a6679b000 7f9a6699a000 p 0004a000 08 21 62790698 usr lib x86 64 linux gnu libjasper.so.1.0.0 7f9a6699a000 7f9a6699b000 r p 00049000 08 21 62790698 usr lib x86 64 linux gnu libjasper.so.1.0.0 7f9a6699b000 7f9a6699f000 rw p 0004a000 08 21 62790698 usr lib x86 64 linux gnu libjasper.so.1.0.0 7f9a6699f000 7f9a669a6000 rw p 00000000 00 00 0 7f9a669a6000 7f9a66a17000 r xp 00000000 08 21 62790902 usr lib x86 64 linux gnu libtiff.so.5.2.4 7f9a66a17000 7f9a66c17000 p 00071000 08 21 62790902 usr lib x86 64 linux gnu libtiff.so.5.2.4 7f9a66c17000 7f9a66c18000 r p 00071000 08 21 62790902 usr lib x86 64 linux gnu libtiff.so.5.2.4 7f9a66c18000 7f9a66c1b000 rw p 00072000 08 21 62790902 usr lib x86 64 linux gnu libtiff.so.5.2.4 7f9a66c1b000 7f9a68045000 r xp 00000000 08 21 50074739 usr local cuda 9.0 targets x86 64 linux lib libnvrtc.so.9.0.176 7f9a68045000 7f9a68244000 p 0142a000 08 21 50074739 usr local cuda 9.0 targets x86 64 linux lib libnvrtc.so.9.0.176 7f9a68244000 7f9a683ee000 rw p 01429000 08 21 50074739 usr local cuda 9.0 targets x86 64 linux lib libnvrtc.so.9.0.176 7f9a683ee000 7f9a6a466000 rw p 00000000 00 00 0 7f9a6a46c000 7f9a6a5ac000 rw p 00000000 00 00 0 7f9a6a5ac000 7f9a6a5d0000 r xp 00000000 08 21 62529463 lib x86 64 linux gnu libpng12.so.0.54.0 7f9a6a5d0000 7f9a6a7cf000 p 00024000 08 21 62529463 lib x86 64 linux gnu libpng12.so.0.54.0 7f9a6a7cf000 7f9a6a7d0000 r p 00023000 08 21 62529463 lib x86 64 linux gnu libpng12.so.0.54.0 7f9a6a7d0000 7f9a6a7d1000 rw p 00024000 08 21 62529463 lib x86 64 linux gnu libpng12.so.0.54.0 7f9a6a7d1000 7f9a6a828000 r xp 00000000 08 21 62790705 usr lib x86 64 linux gnu libjpeg.so.8.0.2 7f9a6a828000 7f9a6aa28000 p 00057000 08 21 62790705 usr lib x86 64 linux gnu libjpeg.so.8.0.2 7f9a6aa28000 7f9a6aa29000 r p 00057000 08 21 62790705 usr lib x86 64 linux gnu libjpeg.so.8.0.2 7f9a6aa29000 7f9a6aa2a000 rw p 00058000 08 21 62790705 usr lib x86 64 linux gnu libjpeg.so.8.0.2 7f9a6aa2a000 7f9a6aa61000 r xp 00000000 08 21 62790888 usr lib x86 64 linux gnu libtbb.so.2 7f9a6aa61000 7f9a6ac61000 p 00037000 08 21 62790888 usr lib x86 64 linux gnu libtbb.so.2 7f9a6ac61000 7f9a6ac62000 r p 00037000 08 21 62790888 usr lib x86 64 linux gnu libtbb.so.2 7f9a6ac62000 7f9a6ac64000 rw p 00038000 08 21 62790888 usr lib x86 64 linux gnu libtbb.so.2 7f9a6ac64000 7f9a70c67000 rw p 00000000 00 00 0 7f9a70c79000 7f9a70db9000 rw p 00000000 00 00 0 7f9a70db9000 7f9a70e29000 r xp 00000000 08 21 62790976 usr lib x86 64 linux gnu mesa libgl.so.1.2.0 7f9a70e29000 7f9a71028000 p 00070000 08 21 62790976 usr lib x86 64 linux gnu mesa libgl.so.1.2.0 7f9a71028000 7f9a7102b000 r p 0006f000 08 21 62790976 usr lib x86 64 linux gnu mesa libgl.so.1.2.0 7f9a7102b000 7f9a7102c000 rw p 00072000 08 21 62790976 usr lib x86 64 linux gnu mesa libgl.so.1.2.0 7f9a7102c000 7f9a7102d000 rw p 00000000 00 00 0 7f9a7102d000 7f9a71156000 r xp 00000000 08 21 62790627 usr lib x86 64 linux gnu libgfortran.so.3.0.0 7f9a71156000 7f9a71355000 p 00129000 08 21 62790627 usr lib x86 64 linux gnu libgfortran.so.3.0.0 7f9a71355000 7f9a71356000 r p 00128000 08 21 62790627 usr lib x86 64 linux gnu libgfortran.so.3.0.0 7f9a71356000 7f9a71358000 rw p 00129000 08 21 62790627 usr lib x86 64 linux gnu libgfortran.so.3.0.0 7f9a71358000 7f9a71b9a000 r xp 00000000 08 33 1712531 usr lib x86 64 linux gnu libcuda.so.390.48 7f9a71b9a000 7f9a71d99000 p 00842000 08 33 1712531 usr lib x86 64 linux gnu libcuda.so.390.48 7f9a71d99000 7f9a71eea000 rw p 00841000 08 33 1712531 usr lib x86 64 linux gnu libcuda.so.390.48 7f9a71eea000 7f9a71ef8000 rw p 00000000 00 00 0 7f9a71ef8000 7f9a720e8000 r xp 00000000 08 21 62790751 usr lib x86 64 linux gnu libopencv imgproc.so.2.4.9 7f9a720e8000 7f9a722e7000 p 001f0000 08 21 62790751 usr lib x86 64 linux gnu libopencv imgproc.so.2.4.9 7f9a722e7000 7f9a722ed000 r p 001ef000 08 21 62790751 usr lib x86 64 linux gnu libopencv imgproc.so.2.4.9 7f9a722ed000 7f9a722ee000 rw p 001f5000 08 21 62790751 usr lib x86 64 linux gnu libopencv imgproc.so.2.4.9 7f9a722ee000 7f9a72383000 rw p 00000000 00 00 0 7f9a72383000 7f9a723d1000 r xp 00000000 08 21 62790747 usr lib x86 64 linux gnu libopencv highgui.so.2.4.9 7f9a723d1000 7f9a725d0000 p 0004e000 08 21 62790747 usr lib x86 64 linux gnu libopencv highgui.so.2.4.9 7f9a725d0000 7f9a725d3000 r p 0004d000 08 21 62790747 usr lib x86 64 linux gnu libopencv highgui.so.2.4.9 7f9a725d3000 7f9a725d4000 rw p 00050000 08 21 62790747 usr lib x86 64 linux gnu libopencv highgui.so.2.4.9 7f9a725d4000 7f9a727f1000 r xp 00000000 08 21 62790731 usr lib x86 64 linux gnu libopencv core.so.2.4.9 7f9a727f1000 7f9a729f1000 p 0021d000 08 21 62790731 usr lib x86 64 linux gnu libopencv core.so.2.4.9 7f9a729f1000 7f9a729f5000 r p 0021d000 08 21 62790731 usr lib x86 64 linux gnu libopencv core.so.2.4.9 7f9a729f5000 7f9a729fc000 rw p 00221000 08 21 62790731 usr lib x86 64 linux gnu libopencv core.so.2.4.9 7f9a729fc000 7f9a729fe000 rw p 00000000 00 00 0 7f9a729fe000 7f9a72a67000 r xp 00000000 08 21 48113201 usr local cuda 9.0 targets x86 64 linux lib libcudart.so.9.0.176 7f9a72a67000 7f9a72c66000 p 00069000 08 21 48113201 usr local cuda 9.0 targets x86 64 linux lib libcudart.so.9.0.176 7f9a72c66000 7f9a72c6a000 rw p 00068000 08 21 48113201 usr local cuda 9.0 targets x86 64 linux lib libcudart.so.9.0.176 7f9a72c6a000 7f9a72c6b000 rw p 00000000 00 00 0 7f9a72c6b000 7f9a72d5b000 r xp 00000000 08 21 3018825 usr local lib python2.7 dist packages numpy .libs libgfortran ed201abd.so.3.0.0 7f9a72d5b000 7f9a72f5a000 p 000f0000 08 21 3018825 usr local lib python2.7 dist packages numpy .libs libgfortran ed201abd.so.3.0.0 7f9a72f5a000 7f9a72f5c000 rw p 000ef000 08 21 3018825 usr local lib python2.7 dist packages numpy .libs libgfortran ed201abd.so.3.0.0 7f9a72f5c000 7f9a72f5d000 rw p 00000000 00 00 0 7f9a72f5d000 7f9a72f65000 rw p 000f2000 08 21 3018825 usr local lib python2.7 dist packages numpy .libs libgfortran ed201abd.so.3.0.0 7f9a72f65000 7f9a74a65000 r xp 00000000 08 21 3018824 usr local lib python2.7 dist packages numpy .libs libopenblasp r0 382c8f3a.3.5.dev.so 7f9a74a65000 7f9a74c65000 p 01b00000 08 21 3018824 usr local lib python2.7 dist packages numpy .libs libopenblasp r0 382c8f3a.3.5.dev.so 7f9a74c65000 7f9a74c7e000 rw p 01b00000 08 21 3018824 usr local lib python2.7 dist packages numpy .libs libopenblasp r0 382c8f3a.3.5.dev.so 7f9a74c7e000 7f9a74c89000 rw p 00000000 00 00 0 7f9a74c89000 7f9a74d01000 rw p 01beb000 08 21 3018824 usr local lib python2.7 dist packages numpy .libs libopenblasp r0 382c8f3a.3.5.dev.so 7f9a74d01000 7f9a75088000 r xp 00000000 08 21 3018433 usr local lib python2.7 dist packages numpy core multiarray umath.so 7f9a75088000 7f9a75287000 p 00387000 08 21 3018433 usr local lib python2.7 dist packages numpy core multiarray umath.so 7f9a75287000 7f9a752a6000 rw p 00386000 08 21 3018433 usr local lib python2.7 dist packages numpy core multiarray umath.so 7f9a752a6000 7f9a752c7000 rw p 00000000 00 00 0 7f9a752c7000 7f9a752ce000 rw p 012b0000 08 21 3018433 usr local lib python2.7 dist packages numpy core multiarray umath.so 7f9a752ce000 7f9a7534e000 rw p 00000000 00 00 0 7f9a7534e000 7f9a75569000 r xp 00000000 08 21 45487999 lib x86 64 linux gnu libcrypto.so.1.0.0 7f9a75569000 7f9a75768000 p 0021b000 08 21 45487999 lib x86 64 linux gnu libcrypto.so.1.0.0 7f9a75768000 7f9a75784000 r p 0021a000 08 21 45487999 lib x86 64 linux gnu libcrypto.so.1.0.0 7f9a75784000 7f9a75790000 rw p 00236000 08 21 45487999 lib x86 64 linux gnu libcrypto.so.1.0.0 7f9a75790000 7f9a75793000 rw p 00000000 00 00 0 7f9a75793000 7f9a75799000 r xp 00000000 08 21 62787964 usr lib python2.7 lib dynload hashlib.x86 64 linux gnu.so 7f9a75799000 7f9a75998000 p 00006000 08 21 62787964 usr lib python2.7 lib dynload hashlib.x86 64 linux gnu.so 7f9a75998000 7f9a75999000 r p 00005000 08 21 62787964 usr lib python2.7 lib dynload hashlib.x86 64 linux gnu.so 7f9a75999000 7f9a7599a000 rw p 00006000 08 21 62787964 usr lib python2.7 lib dynload hashlib.x86 64 linux gnu.so 7f9a7599b000 7f9a75b1b000 rw p 00000000 00 00 0 7f9a75b1b000 7f9a75c23000 r xp 00000000 08 21 38015915 lib x86 64 linux gnu libm 2.23.so 7f9a75c23000 7f9a75e22000 p 00108000 08 21 38015915 lib x86 64 linux gnu libm 2.23.so 7f9a75e22000 7f9a75e23000 r p 00107000 08 21 38015915 lib x86 64 linux gnu libm 2.23.so 7f9a75e23000 7f9a75e24000 rw p 00108000 08 21 38015915 lib x86 64 linux gnu libm 2.23.so 7f9a75e24000 7f9a75e3d000 r xp 00000000 08 21 38015982 lib x86 64 linux gnu libz.so.1.2.8 7f9a75e3d000 7f9a7603c000 p 00019000 08 21 38015982 lib x86 64 linux gnu libz.so.1.2.8 7f9a7603c000 7f9a7603d000 r p 00018000 08 21 38015982 lib x86 64 linux gnu libz.so.1.2.8 7f9a7603d000 7f9a7603e000 rw p 00019000 08 21 38015982 lib x86 64 linux gnu libz.so.1.2.8 7f9a7603e000 7f9a76040000 r xp 00000000 08 21 38015977 lib x86 64 linux gnu libutil 2.23.so 7f9a76040000 7f9a7623f000 p 00002000 08 21 38015977 lib x86 64 linux gnu libutil 2.23.so 7f9a7623f000 7f9a76240000 r p 00001000 08 21 38015977 lib x86 64 linux gnu libutil 2.23.so 7f9a76240000 7f9a76241000 rw p 00002000 08 21 38015977 lib x86 64 linux gnu libutil 2.23.so 7f9a76241000 7f9a76244000 r xp 00000000 08 21 38015896 lib x86 64 linux gnu libdl 2.23.so 7f9a76244000 7f9a76443000 p 00003000 08 21 38015896 lib x86 64 linux gnu libdl 2.23.so 7f9a76443000 7f9a76444000 r p 00002000 08 21 38015896 lib x86 64 linux gnu libdl 2.23.so 7f9a76444000 7f9a76445000 rw p 00003000 08 21 38015896 lib x86 64 linux gnu libdl 2.23.so 7f9a76445000 7f9a76605000 r xp 00000000 08 21 38015883 lib x86 64 linux gnu libc 2.23.so 7f9a76605000 7f9a76805000 p 001c0000 08 21 38015883 lib x86 64 linux gnu libc 2.23.so 7f9a76805000 7f9a76809000 r p 001c0000 08 21 38015883 lib x86 64 linux gnu libc 2.23.so 7f9a76809000 7f9a7680b000 rw p 001c4000 08 21 38015883 lib x86 64 linux gnu libc 2.23.so 7f9a7680b000 7f9a7680f000 rw p 00000000 00 00 0 7f9a7680f000 7f9a76827000 r xp 00000000 08 21 38015951 lib x86 64 linux gnu libpthread 2.23.so 7f9a76827000 7f9a76a26000 p 00018000 08 21 38015951 lib x86 64 linux gnu libpthread 2.23.so 7f9a76a26000 7f9a76a27000 r p 00017000 08 21 38015951 lib x86 64 linux gnu libpthread 2.23.so 7f9a76a27000 7f9a76a28000 rw p 00018000 08 21 38015951 lib x86 64 linux gnu libpthread 2.23.so 7f9a76a28000 7f9a76a2c000 rw p 00000000 00 00 0 7f9a76a2c000 7f9a76a52000 r xp 00000000 08 21 38015863 lib x86 64 linux gnu ld 2.23.so 7f9a76a8a000 7f9a76a8b000 rw p 00000000 00 00 0 7f9a76a8b000 7f9a76a8c000 rw 00000000 00 06 816 dev nvidia0 7f9a76a8c000 7f9a76a8d000 rw 00000000 00 06 815 dev nvidiactl 7f9a76a8d000 7f9a76a8e000 rw 00000000 00 06 816 dev nvidia0 7f9a76a8e000 7f9a76b4e000 rw p 00000000 00 00 0 7f9a76b4e000 7f9a76b4f000 rw 00000000 00 06 815 dev nvidiactl 7f9a76b4f000 7f9a76b50000 rw 00000000 00 06 816 dev nvidia0 7f9a76b50000 7f9a76b51000 rw 00000000 00 06 815 dev nvidiactl 7f9a76b51000 7f9a76b52000 rw 00000000 00 06 816 dev nvidia0 7f9a76b52000 7f9a76b70000 r xp 00000000 08 21 38015974 lib x86 64 linux gnu libudev.so.1.6.4 7f9a76b70000 7f9a76b71000 r p 0001d000 08 21 38015974 lib x86 64 linux gnu libudev.so.1.6.4 7f9a76b71000 7f9a76b72000 rw p 0001e000 08 21 38015974 lib x86 64 linux gnu libudev.so.1.6.4 7f9a76b72000 7f9a76b73000 rw 00000000 00 06 815 dev nvidiactl 7f9a76b73000 7f9a76b74000 rw 00000000 00 06 816 dev nvidia0 7f9a76b74000 7f9a76b75000 rw 00000000 00 06 815 dev nvidiactl 7f9a76b75000 7f9a76b76000 rw 00000000 00 06 816 dev nvidia0 7f9a76b76000 7f9a76b77000 rw 00000000 00 06 815 dev nvidiactl 7f9a76b77000 7f9a76b78000 rw 00000000 00 06 816 dev nvidia0 7f9a76b78000 7f9a76b79000 rw 00000000 00 06 815 dev nvidiactl 7f9a76b79000 7f9a76b7a000 rw 00000000 00 06 816 dev nvidia0 7f9a76b7a000 7f9a76b7b000 rw 00000000 00 06 815 dev nvidiactl 7f9a76b7b000 7f9a76b7c000 rw 00000000 00 06 816 dev nvidia0 7f9a76b7c000 7f9a76b7d000 rw 00000000 00 06 815 dev nvidiactl 7f9a76b7d000 7f9a76b7e000 rw 00000000 00 06 816 dev nvidia0 7f9a76b7e000 7f9a76b7f000 rw 00000000 00 06 815 dev nvidiactl 7f9a76b7f000 7f9a76c44000 rw p 00000000 00 00 0 7f9a76c44000 7f9a76c45000 rw 00000000 00 06 816 dev nvidia0 7f9a76c45000 7f9a76c46000 rw 00000000 00 06 815 dev nvidiactl 7f9a76c46000 7f9a76c47000 rw 00000000 00 06 816 dev nvidia0 7f9a76c47000 7f9a76c48000 rw 00000000 00 06 815 dev nvidiactl 7f9a76c48000 7f9a76c49000 rw 00000000 00 06 816 dev nvidia0 7f9a76c49000 7f9a76c4a000 rw 00000000 00 06 815 dev nvidiactl 7f9a76c4a000 7f9a76c4b000 rw 00000000 00 06 816 dev nvidia0 7f9a76c4b000 7f9a76c4c000 rw 00000000 00 06 815 dev nvidiactl 7f9a76c4c000 7f9a76c4d000 rw 00000000 00 06 816 dev nvidia0 7f9a76c4d000 7f9a76c4e000 rw 00000000 00 06 815 dev nvidiactl 7f9a76c4e000 7f9a76c4f000 rw 00000000 00 06 816 dev nvidia0 7f9a76c4f000 7f9a76c50000 rw 00000000 00 06 815 dev nvidiactl 7f9a76c50000 7f9a76c51000 rwxp 00000000 00 00 0 7f9a76c51000 7f9a76c52000 r p 00025000 08 21 38015863 lib x86 64 linux gnu ld 2.23.so 7f9a76c52000 7f9a76c53000 rw p 00026000 08 21 38015863 lib x86 64 linux gnu ld 2.23.so 7f9a76c53000 7f9a76c54000 rw p 00000000 00 00 0 7fff2cb1b000 7fff2cb3c000 rw p 00000000 00 00 0 stack 7fff2cb80000 7fff2cb83000 r p 00000000 00 00 0 vvar 7fff2cb83000 7fff2cb85000 r xp 00000000 00 00 0 vdso ffffffffff600000 ffffffffff601000 r xp 00000000 00 00 0 vsyscall bash line 1 56358 aborted core dumped env jetbrains remote run 1 library roots c users chenhuizhen .pycharm2018.3 system remote sources 823358608 1227070294 c users chenhuizhen .pycharm2018.3 system remote sources 823358608 graphviz 0.8.4 py2.7.egg! c users chenhuizhen .pycharm2018.3 system remote sources 823358608 554785109 c users chenhuizhen .pycharm2018.3 system remote sources 823358608 370154233 c users chenhuizhen .pycharm2018.3 system remote sources 823358608 518999124 c users chenhuizhen .pycharm2018.3 system remote sources 823358608 idna 2.6 py2.7.egg! c users chenhuizhen .pycharm2018.3 system remote sources 823358608 chardet 3.0.4 py2.7.egg! c users chenhuizhen .pycharm2018.3 system remote sources 823358608 201544331 c users chenhuizhen .pycharm2018.3 system remote sources 823358608 724150857 c users chenhuizhen .pycharm2018.3 system remote sources 823358608 154863933 c users chenhuizhen .pycharm2018.3 system remote sources 823358608 1227933812 c users chenhuizhen .pycharm2018.3 system remote sources 823358608 125940560 c users chenhuizhen .pycharm2018.3 system remote sources 823358608 2085782652 c users chenhuizhen .pycharm2018.3 system remote sources 823358608 201545290 c users chenhuizhen .pycharm2018.3 system remote sources 823358608 452379848 c users chenhuizhen .pycharm2018.3 system remote sources 823358608 1405239563 c users chenhuizhen .pycharm2018.3 system remote sources 823358608 1171821208 c users chenhuizhen .pycharm2018.3 system remote sources 823358608 530511828 c users chenhuizhen .pycharm2018.3 system remote sources 823358608 427714987 c users chenhuizhen .pycharm2018.3 system remote sources 823358608 541471831 c users chenhuizhen .pycharm2018.3 system remote sources 823358608 1437370484 c users chenhuizhen .pycharm2018.3 system remote sources 823358608 17574554 c users chenhuizhen .pycharm2018.3 system remote sources 823358608 669732926 c users chenhuizhen .pycharm2018.3 system remote sources 823358608 452463671 c users chenhuizhen .pycharm2018.3 system remote sources 823358608 662605150 c users chenhuizhen .pycharm2018.3 system python stubs 823358608 c program files jetbrains pycharm 2018.3.1 helpers python skeletons c program files jetbrains pycharm 2018.3.1 helpers typeshed stdlib 2 c program files jetbrains pycharm 2018.3.1 helpers typeshed stdlib 2and3 c program files jetbrains pycharm 2018.3.1 helpers typeshed third party 2 c program files jetbrains pycharm 2018.3.1 helpers typeshed third party 2and3 pydevd load values async true pythonpath opt kd pavement crackseg root .pycharm helpers pycharm matplotlib backend root .pycharm helpers third party thriftpy root .pycharm helpers pydev c users chenhuizhen .pycharm2018.3 system cythonextensions opt kd pavement crackseg pythonioencoding utf 8 pythondontwritebytecode 1 ipythonenable true pycharm matplotlib port 64619 pycharm hosted 1 pythonunbuffered 1 ide project roots opt kd pavement crackseg ' usr bin python' ' u' ' root .pycharm helpers pydev pydevd.py' ' multiproc' ' qt support auto' ' client' '0.0.0.0' ' port' '44975' ' file' ' opt kd pavement crackseg main crack segmentation.py' process finished exit code 134,0,"use different size inference, self.reshape error usr bin python' free invalid pointer 0x00007f9a484890a0","use different size inference, self.reshape error usr bin python' free invalid pointer 0x00007f9a484890a0 nfo 56358 2019 10 28 08 26 33 main crack segmentation.py 382 segmentation crack... usr local lib python2.7 dist packages mxnet 1.3.0 py2.7.egg mxnet module base module.py 66 userwarning data provided label shapes match names specified label names vs. 'softmax label' warnings.warn msg error usr bin python' free invalid pointer 0x00007f9a484890a0 backtrace lib x86 64 linux gnu libc.so.6 0x777e5 0x7f9a764bc7e5 lib x86 64 linux gnu libc.so.6 0x8037a 0x7f9a764c537a lib x86 64 linux gnu libc.so.6 cfree 0x4c 0x7f9a764c953c usr local lib python2.7 dist packages mxnet 1.3.0 py2.7.egg mxnet libmxnet.so mxexecutorreshape 0x1de7 0x7f99c3a2d5e7 usr lib x86 64 linux gnu libffi.so.6 ffi call unix64 0x4c 0x7f9a0cfcfe40 usr lib x86 64 linux gnu libffi.so.6 ffi call 0x2eb 0x7f9a0cfcf8ab usr lib python2.7 lib dynload ctypes.x86 64 linux gnu.so ctypes callproc 0x48f 0x7f9a0d1df3df usr lib python2.7 lib dynload ctypes.x86 64 linux gnu.so 0x11d82 0x7f9a0d1e3d82 usr bin python pyeval evalframeex 0x578d 0x4c166d usr bin python pyeval evalcodeex 0x306 0x4b9b66 usr bin python 0x4d57a3 usr bin python pyobject call 0x3e 0x4a587e usr bin python pyeval evalframeex 0x263e 0x4be51e usr bin python pyeval evalcodeex 0x306 0x4b9b66 usr bin python pyeval evalframeex 0x58e6 0x4c17c6 usr bin python pyeval evalframeex 0x553f 0x4c141f usr bin python pyeval evalcodeex 0x306 0x4b9b66 usr bin python pyeval evalframeex 0x58e6 0x4c17c6 usr bin python pyeval evalcodeex 0x306 0x4b9b66 usr bin python pyeval evalframeex 0x58e6 0x4c17c6 usr bin python pyeval evalframeex 0x553f 0x4c141f usr bin python pyeval evalframeex 0x553f 0x4c141f usr bin python pyeval evalcodeex 0x306 0x4b9b66 usr bin python 0x4eb69f usr bin python pyrun fileexflags 0x82 0x4e58f2 usr bin python 0x54aae7 usr bin python pyeval evalframeex 0x5f3e 0x4c1e1e usr bin python pyeval evalcodeex 0x306 0x4b9b66 usr bin python pyeval evalframeex 0x58e6 0x4c17c6 usr bin python pyeval evalframeex 0x553f 0x4c141f usr bin python pyeval evalcodeex 0x306 0x4b9b66 usr bin python 0x4eb69f usr bin python pyrun fileexflags 0x82 0x4e58f2 usr bin python pyrun simplefileexflags 0x186 0x4e41a6 usr bin python py main 0x54e 0x4938ce lib x86 64 linux gnu libc.so.6 libc start main 0xf0 0x7f9a76465830 usr bin python start 0x29 0x493299 memory map 00400000 006de000 r xp 00000000 08 21 62529595 usr bin python2.7 008dd000 008de000 r p 002dd000 08 21 62529595 usr bin python2.7 008de000 00955000 rw p 002de000 08 21 62529595 usr bin python2.7 00955000 00978000 rw p 00000000 00 00 0 00fae000 2d86a000 rw p 00000000 00 00 0 heap 200000000 200200000 rw 00000000 00 06 815 dev nvidiactl 200200000 200400000 p 00000000 00 00 0 200400000 200404000 rw 00000000 00 06 815 dev nvidiactl 200404000 200600000 p 00000000 00 00 0 200600000 200a00000 rw 00000000 00 06 815 dev nvidiactl 200a00000 201800000 p 00000000 00 00 0 201800000 201804000 rw 00000000 00 06 815 dev nvidiactl 201804000 201a00000 p 00000000 00 00 0 201a00000 201e00000 rw 00000000 00 06 815 dev nvidiactl 201e00000 202c00000 p 00000000 00 00 0 202c00000 202c04000 rw 00000000 00 06 815 dev nvidiactl 202c04000 202e00000 p 00000000 00 00 0 202e00000 203200000 rw 00000000 00 06 815 dev nvidiactl 203200000 204000000 p 00000000 00 00 0 204000000 204004000 rw 00000000 00 06 815 dev nvidiactl 204004000 204200000 p 00000000 00 00 0 204200000 204600000 rw 00000000 00 06 815 dev nvidiactl 204600000 205400000 p 00000000 00 00 0 205400000 205404000 rw 00000000 00 06 815 dev nvidiactl 205404000 205600000 p 00000000 00 00 0 205600000 205a00000 rw 00000000 00 06 815 dev nvidiactl 205a00000 206800000 p 00000000 00 00 0 206800000 206804000 rw 00000000 00 06 815 dev nvidiactl 206804000 206a00000 p 00000000 00 00 0 206a00000 206e00000 rw 00000000 00 06 815 dev nvidiactl 206e00000 207c00000 p 00000000 00 00 0 207c00000 207c04000 rw 00000000 00 06 815 dev nvidiactl 207c04000 207e00000 p 00000000 00 00 0 207e00000 208200000 rw 00000000 00 06 815 dev nvidiactl 208200000 209000000 p 00000000 00 00 0 209000000 209004000 rw 00000000 00 06 815 dev nvidiactl 209004000 209200000 p 00000000 00 00 0 209200000 209600000 rw 00000000 00 06 815 dev nvidiactl 209600000 20a400000 p 00000000 00 00 0 20a400000 20a404000 rw 00000000 00 06 815 dev nvidiactl 20a404000 20a600000 p 00000000 00 00 0 20a600000 20aa00000 rw 00000000 00 06 815 dev nvidiactl 20aa00000 20aa04000 rw 00000000 00 06 815 dev nvidiactl 20aa04000 20ac00000 p 00000000 00 00 0 20ac00000 20b000000 rw 00000000 00 06 815 dev nvidiactl 20b000000 20b004000 rw 00000000 00 06 815 dev nvidiactl 20b004000 20b200000 p 00000000 00 00 0 20b200000 20b600000 rw 00000000 00 06 815 dev nvidiactl 20b600000 20b604000 rw 00000000 00 06 815 dev nvidiactl 20b604000 20b800000 p 00000000 00 00 0 20b800000 20bc00000 rw 00000000 00 06 815 dev nvidiactl 20bc00000 20bc04000 rw 00000000 00 06 815 dev nvidiactl 20bc04000 20be00000 p 00000000 00 00 0 20be00000 20c200000 rw 00000000 00 06 815 dev nvidiactl 20c200000 20c204000 rw 00000000 00 06 815 dev nvidiactl 20c204000 20c400000 p 00000000 00 00 0 20c400000 20c800000 rw 00000000 00 06 815 dev nvidiactl 20c800000 20c804000 rw 00000000 00 06 815 dev nvidiactl 20c804000 20ca00000 p 00000000 00 00 0 20ca00000 20ce00000 rw 00000000 00 06 815 dev nvidiactl 20ce00000 20ce04000 rw 00000000 00 06 815 dev nvidiactl 20ce04000 20d000000 p 00000000 00 00 0 20d000000 20d400000 rw 00000000 00 06 815 dev nvidiactl 20d400000 20d600000 p 00000000 00 00 0 20d600000 20d800000 rw 00000000 00 06 815 dev nvidiactl 20d800000 c00200000 p 00000000 00 00 0 10000000000 10b04000000 p 00000000 00 00 0 7f9823829000 7f9834000000 rw p 00000000 00 00 0 7f9834000000 7f9834022000 rw p 00000000 00 00 0 7f9834022000 7f9838000000 p 00000000 00 00 0 7f9838000000 7f983a08a000 rw p 00000000 00 00 0 7f983a08a000 7f983c000000 p 00000000 00 00 0 7f983c000000 7f983c022000 rw p 00000000 00 00 0 7f983c022000 7f9840000000 p 00000000 00 00 0 7f9842807000 7f9849009000 rw p 00000000 00 00 0 7f984d7fe000 7f9854000000 rw p 00000000 00 00 0 7f9854000000 7f98569b9000 rw p 00000000 00 00 0 7f98569b9000 7f9858000000 p 00000000 00 00 0 7f985c000000 7f985db22000 rw p 00000000 00 00 0 7f985db22000 7f9860000000 p 00000000 00 00 0 7f9864000000 7f9866f05000 rw p 00000000 00 00 0 7f9866f05000 7f9868000000 p 00000000 00 00 0 7f9868000000 7f986be05000 rw p 00000000 00 00 0 7f986be05000 7f986c000000 p 00000000 00 00 0 7f986c000000 7f986ff25000 rw p 00000000 00 00 0 7f986ff25000 7f9870000000 p 00000000 00 00 0 7f9870200000 7f9870800000 p 00000000 00 00 0 7f98709fd000 7f98709fe000 p 00000000 00 00 0 7f98709fe000 7f98711fe000 rw p 00000000 00 00 0 7f98711fe000 7f98711ff000 p 00000000 00 00 0 7f98711ff000 7f98719ff000 rw p 00000000 00 00 0 7f98721fd000 7f98721fe000 p 00000000 00 00 0 7f98721fe000 7f98729fe000 rw p 00000000 00 00 0 7f98729fe000 7f98729ff000 p 00000000 00 00 0 7f98729ff000 7f98731ff000 rw p 00000000 00 00 0 7f98731ff000 7f9873200000 p 00000000 00 00 0 7f9873200000 7f9873a00000 rw p 00000000 00 00 0 7f9873a00000 7f9874000000 p 00000000 00 00 0 7f9874000000 7f9877da7000 rw p 00000000 00 00 0 7f9877da7000 7f9878000000 p 00000000 00 00 0 7f9878200000 7f9878400000 p 00000000 00 00 0 7f98785ff000 7f9878600000 p 00000000 00 00 0 7f9878600000 7f9878e00000 rw p 00000000 00 00 0 7f9878e00000 7f9894000000 p 00000000 00 00 0 7f9894000000 7f9898091000 rw p 00000000 00 00 0 7f9898091000 7f989c000000 p 00000000 00 00 0 7f989c000000 7f989ffff000 rw p 00000000 00 00 0 7f989ffff000 7f98a0000000 p 00000000 00 00 0 7f98a0000000 7f98a3f49000 rw p 00000000 00 00 0 7f98a3f49000 7f98a4000000 p 00000000 00 00 0 7f98a4000000 7f98a7fce000 rw p 00000000 00 00 0 7f98a7fce000 7f98a8000000 p 00000000 00 00 0 7f98a8000000 7f98abfee000 rw p 00000000 00 00 0 7f98abfee000 7f98ac000000 p 00000000 00 00 0 7f98ac000000 7f98aff87000 rw p 00000000 00 00 0 7f98aff87000 7f98b0000000 p 00000000 00 00 0 7f98b0000000 7f98b4000000 rw p 00000000 00 00 0 7f98b4000000 7f98b7efc000 rw p 00000000 00 00 0 7f98b7efc000 7f98b8000000 p 00000000 00 00 0 7f98b8000000 7f98bc000000 rw p 00000000 00 00 0 7f98bc000000 7f98bd0a3000 rw p 00000000 00 00 0 7f98bd0a3000 7f98c0000000 p 00000000 00 00 0 7f98c0000000 7f98c4000000 rw p 00000000 00 00 0 7f98c4000000 7f98c7ffe000 rw p 00000000 00 00 0 7f98c7ffe000 7f98c8000000 p 00000000 00 00 0 7f98c8000000 7f98cbffd000 rw p 00000000 00 00 0 7f98cbffd000 7f98cc000000 p 00000000 00 00 0 7f98cc000000 7f98d0000000 rw p 00000000 00 00 0 7f98d0000000 7f98d8000000 p 00000000 00 00 0 7f98d8000000 7f98d8021000 rw p 00000000 00 00 0 7f98d8021000 7f98dc000000 p 00000000 00 00 0 7f98dc200000 7f98e0000000 p 00000000 00 00 0 7f98e0000000 7f98e0021000 rw p 00000000 00 00 0 7f98e0021000 7f98e4000000 p 00000000 00 00 0 7f98e4200000 7f98e4e00000 p 00000000 00 00 0 7f98e4ffe000 7f98eaffe000 p 00000000 00 00 0 7f98eaffe000 7f98f4000000 rw p 00000000 00 00 0 7f98f4000000 7f98f4022000 rw p 00000000 00 00 0 7f98f4022000 7f98f8000000 p 00000000 00 00 0 7f98f8000000 7f98f8022000 rw p 00000000 00 00 0 7f98f8022000 7f98fc000000 p 00000000 00 00 0 7f98fc000000 7f98fc022000 rw p 00000000 00 00 0 7f98fc022000 7f9900000000 p 00000000 00 00 0 7f9900200000 7f9900800000 p 00000000 00 00 0 7f99009fc000 7f9904000000 rw p 00000000 00 00 0 7f9904000000 7f9904022000 rw p 00000000 00 00 0 7f9904022000 7f9908000000 p 00000000 00 00 0 7f9908248000 7f9909d4b000 rw p 00000000 00 00 0 7f9909d4b000 7f9909d54000 r xp 00000000 08 21 62787974 usr lib python2.7 lib dynload bz2.x86 64 linux gnu.so 7f9909d54000 7f9909f53000 p 00009000 08 21 62787974 usr lib python2.7 lib dynload bz2.x86 64 linux gnu.so 7f9909f53000 7f9909f54000 r p 00008000 08 21 62787974 usr lib python2.7 lib dynload bz2.x86 64 linux gnu.so 7f9909f54000 7f9909f56000 rw p 00009000 08 21 62787974 usr lib python2.7 lib dynload bz2.x86 64 linux gnu.so 7f9909f56000 7f9909f57000 p 00000000 00 00 0 7f9909f57000 7f990a757000 rw p 00000000 00 00 0 7f990a757000 7f990a767000 r xp 00000000 08 21 2233737 usr lib python2.7 dist packages h5py h5l.x86 64 linux gnu.so 7f990a767000 7f990a966000 p 00010000 08 21 2233737 usr lib python2.7 dist packages h5py h5l.x86 64 linux gnu.so 7f990a966000 7f990a967000 r p 0000f000 08 21 2233737 usr lib python2.7 dist packages h5py h5l.x86 64 linux gnu.so 7f990a967000 7f990a96a000 rw p 00010000 08 21 2233737 usr lib python2.7 dist packages h5py h5l.x86 64 linux gnu.so 7f990a96a000 7f990a97b000 r xp 00000000 08 21 2233672 usr lib python2.7 dist packages h5py h5o.x86 64 linux gnu.so 7f990a97b000 7f990ab7a000 p 00011000 08 21 2233672 usr lib python2.7 dist packages h5py h5o.x86 64 linux gnu.so 7f990ab7a000 7f990ab7b000 r p 00010000 08 21 2233672 usr lib python2.7 dist packages h5py h5o.x86 64 linux gnu.so 7f990ab7b000 7f990ab7e000 rw p 00011000 08 21 2233672 usr lib python2.7 dist packages h5py h5o.x86 64 linux gnu.so 7f990ab7e000 7f990ab7f000 rw p 00000000 00 00 0 7f990ab7f000 7f990ab83000 r xp 00000000 08 21 2233671 usr lib python2.7 dist packages h5py h5fd.x86 64 linux gnu.so 7f990ab83000 7f990ad82000 p 00004000 08 21 2233671 usr lib python2.7 dist packages h5py h5fd.x86 64 linux gnu.so 7f990ad82000 7f990ad83000 r p 00003000 08 21 2233671 usr lib python2.7 dist packages h5py h5fd.x86 64 linux gnu.so 7f990ad83000 7f990ad84000 rw p 00004000 08 21 2233671 usr lib python2.7 dist packages h5py h5fd.x86 64 linux gnu.so 7f990ad84000 7f990ad8f000 r xp 00000000 08 21 2233717 usr lib python2.7 dist packages h5py h5i.x86 64 linux gnu.so 7f990ad8f000 7f990af8e000 p 0000b000 08 21 2233717 usr lib python2.7 dist packages h5py h5i.x86 64 linux gnu.so 7f990af8e000 7f990af8f000 r p 0000a000 08 21 2233717 usr lib python2.7 dist packages h5py h5i.x86 64 linux gnu.so 7f990af8f000 7f990af91000 rw p 0000b000 08 21 2233717 usr lib python2.7 dist packages h5py h5i.x86 64 linux gnu.so 7f990af91000 7f990afa7000 r xp 00000000 08 21 2233684 usr lib python2.7 dist packages h5py h5g.x86 64 linux gnu.so 7f990afa7000 7f990b1a6000 p 00016000 08 21 2233684 usr lib python2.7 dist packages h5py h5g.x86 64 linux gnu.so 7f990b1a6000 7f990b1a7000 r p 00015000 08 21 2233684 usr lib python2.7 dist packages h5py h5g.x86 64 linux gnu.so 7f990b1a7000 7f990b1aa000 rw p 00016000 08 21 2233684 usr lib python2.7 dist packages h5py h5g.x86 64 linux gnu.so 7f990b1aa000 7f990b1bf000 r xp 00000000 08 21 2233735 usr lib python2.7 dist packages h5py h5f.x86 64 linux gnu.so 7f990b1bf000 7f990b3be000 p 00015000 08 21 2233735 usr lib python2.7 dist packages h5py h5f.x86 64 linux gnu.so 7f990b3be000 7f990b3bf000 r p 00014000 08 21 2233735 usr lib python2.7 dist packages h5py h5f.x86 64 linux gnu.so 7f990b3bf000 7f990b3c2000 rw p 00015000 08 21 2233735 usr lib python2.7 dist packages h5py h5f.x86 64 linux gnu.so 7f990b3c2000 7f990b3c3000 rw p 00000000 00 00 0 7f990b3c3000 7f990b3d0000 r xp 00000000 08 21 2233673 usr lib python2.7 dist packages h5py h5ds.x86 64 linux gnu.so 7f990b3d0000 7f990b5cf000 p 0000d000 08 21 2233673 usr lib python2.7 dist packages h5py h5ds.x86 64 linux gnu.so 7f990b5cf000 7f990b5d0000 r p 0000c000 08 21 2233673 usr lib python2.7 dist packages h5py h5ds.x86 64 linux gnu.so 7f990b5d0000 7f990b5d2000 rw p 0000d000 08 21 2233673 usr lib python2.7 dist packages h5py h5ds.x86 64 linux gnu.so 7f990b5d2000 7f990b5e6000 r xp 00000000 08 21 2233716 usr lib python2.7 dist packages h5py h5d.x86 64 linux gnu.so 7f990b5e6000 7f990b7e5000 p 00014000 08 21 2233716 usr lib python2.7 dist packages h5py h5d.x86 64 linux gnu.so 7f990b7e5000 7f990b7e6000 r p 00013000 08 21 2233716 usr lib python2.7 dist packages h5py h5d.x86 64 linux gnu.so 7f990b7e6000 7f990b7e9000 rw p 00014000 08 21 2233716 usr lib python2.7 dist packages h5py h5d.x86 64 linux gnu.so 7f990b7e9000 7f990b7f0000 r xp 00000000 08 21 2233682 usr lib python2.7 dist packages h5py proxy.x86 64 linux gnu.so 7f990b7f0000 7f990b9ef000 p 00007000 08 21 2233682 usr lib python2.7 dist packages h5py proxy.x86 64 linux gnu.so 7f990b9ef000 7f990b9f0000 r p 00006000 08 21 2233682 usr lib python2.7 dist packages h5py proxy.x86 64 linux gnu.so 7f990b9f0000 7f990b9f1000 rw p 00007000 08 21 2233682 usr lib python2.7 dist packages h5py proxy.x86 64 linux gnu.so 7f990b9f1000 7f990b9fc000 r xp 00000000 08 21 2233676 usr lib python2.7 dist packages h5py h5ac.x86 64 linux gnu.so 7f990b9fc000 7f990bbfb000 p 0000b000 08 21 2233676 usr lib python2.7 dist packages h5py h5ac.x86 64 linux gnu.so 7f990bbfb000 7f990bbfc000 r p 0000a000 08 21 2233676 usr lib python2.7 dist packages h5py h5ac.x86 64 linux gnu.so 7f990bbfc000 7f990bbfd000 rw p 0000b000 08 21 2233676 usr lib python2.7 dist packages h5py h5ac.x86 64 linux gnu.so 7f990bbfd000 7f990bc25000 r xp 00000000 08 21 2233678 usr lib python2.7 dist packages h5py h5p.x86 64 linux gnu.so 7f990bc25000 7f990be25000 p 00028000 08 21 2233678 usr lib python2.7 dist packages h5py h5p.x86 64 linux gnu.so 7f990be25000 7f990be26000 r p 00028000 08 21 2233678 usr lib python2.7 dist packages h5py h5p.x86 64 linux gnu.so 7f990be26000 7f990be2e000 rw p 00029000 08 21 2233678 usr lib python2.7 dist packages h5py h5p.x86 64 linux gnu.so 7f990be2e000 7f990be2f000 rw p 00000000 00 00 0 7f990be2f000 7f990be43000 r xp 00000000 08 21 2233718 usr lib python2.7 dist packages h5py h5s.x86 64 linux gnu.so 7f990be43000 7f990c042000 p 00014000 08 21 2233718 usr lib python2.7 dist packages h5py h5s.x86 64 linux gnu.so 7f990c042000 7f990c043000 r p 00013000 08 21 2233718 usr lib python2.7 dist packages h5py h5s.x86 64 linux gnu.so 7f990c043000 7f990c046000 rw p 00014000 08 21 2233718 usr lib python2.7 dist packages h5py h5s.x86 64 linux gnu.so 7f990c046000 7f990c047000 rw p 00000000 00 00 0 7f990c047000 7f990c05c000 r xp 00000000 08 21 2233681 usr lib python2.7 dist packages h5py h5a.x86 64 linux gnu.so 7f990c05c000 7f990c25c000 p 00015000 08 21 2233681 usr lib python2.7 dist packages h5py h5a.x86 64 linux gnu.so 7f990c25c000 7f990c25d000 r p 00015000 08 21 2233681 usr lib python2.7 dist packages h5py h5a.x86 64 linux gnu.so 7f990c25d000 7f990c260000 rw p 00016000 08 21 2233681 usr lib python2.7 dist packages h5py h5a.x86 64 linux gnu.so 7f990c260000 7f990c267000 r xp 00000000 08 21 2233721 usr lib python2.7 dist packages h5py h5z.x86 64 linux gnu.so 7f990c267000 7f990c466000 p 00007000 08 21 2233721 usr lib python2.7 dist packages h5py h5z.x86 64 linux gnu.so 7f990c466000 7f990c467000 r p 00006000 08 21 2233721 usr lib python2.7 dist packages h5py h5z.x86 64 linux gnu.so 7f990c467000 7f990c468000 rw p 00007000 08 21 2233721 usr lib python2.7 dist packages h5py h5z.x86 64 linux gnu.so 7f990c468000 7f990c469000 rw p 00000000 00 00 0 7f990c469000 7f990c47a000 r xp 00000000 08 21 2233675 usr lib python2.7 dist packages h5py h5.x86 64 linux gnu.so 7f990c47a000 7f990c679000 p 00011000 08 21 2233675 usr lib python2.7 dist packages h5py h5.x86 64 linux gnu.so 7f990c679000 7f990c67a000 r p 00010000 08 21 2233675 usr lib python2.7 dist packages h5py h5.x86 64 linux gnu.so 7f990c67a000 7f990c67c000 rw p 00011000 08 21 2233675 usr lib python2.7 dist packages h5py h5.x86 64 linux gnu.so 7f990c67c000 7f990c67d000 rw p 00000000 00 00 0 7f990c67d000 7f990c686000 r xp 00000000 08 21 2233720 usr lib python2.7 dist packages h5py utils.x86 64 linux gnu.so 7f990c686000 7f990c885000 p 00009000 08 21 2233720 usr lib python2.7 dist packages h5py utils.x86 64 linux gnu.so 7f990c885000 7f990c886000 r p 00008000 08 21 2233720 usr lib python2.7 dist packages h5py utils.x86 64 linux gnu.so 7f990c886000 7f990c887000 rw p 00009000 08 21 2233720 usr lib python2.7 dist packages h5py utils.x86 64 linux gnu.so 7f990c887000 7f990c8cb000 r xp 00000000 08 21 2233734 usr lib python2.7 dist packages h5py h5t.x86 64 linux gnu.so 7f990c8cb000 7f990caca000 p 00044000 08 21 2233734 usr lib python2.7 dist packages h5py h5t.x86 64 linux gnu.so 7f990caca000 7f990cacb000 r p 00043000 08 21 2233734 usr lib python2.7 dist packages h5py h5t.x86 64 linux gnu.so 7f990cacb000 7f990cad5000 rw p 00044000 08 21 2233734 usr lib python2.7 dist packages h5py h5t.x86 64 linux gnu.so 7f990cad5000 7f990caf3000 r xp 00000000 08 21 1053210 usr lib x86 64 linux gnu libhdf5 serial hl.so.10.0.2 7f990caf3000 7f990ccf2000 p 0001e000 08 21 1053210 usr lib x86 64 linux gnu libhdf5 serial hl.so.10.0.2 7f990ccf2000 7f990ccf3000 r p 0001d000 08 21 1053210 usr lib x86 64 linux gnu libhdf5 serial hl.so.10.0.2 7f990ccf3000 7f990ccf4000 rw p 0001e000 08 21 1053210 usr lib x86 64 linux gnu libhdf5 serial hl.so.10.0.2 7f990ccf4000 7f990ccf5000 rw p 00000000 00 00 0 7f990ccf5000 7f990cd1c000 r xp 00000000 08 21 2233674 usr lib python2.7 dist packages h5py defs.x86 64 linux gnu.so 7f990cd1c000 7f990cf1c000 p 00027000 08 21 2233674 usr lib python2.7 dist packages h5py defs.x86 64 linux gnu.so 7f990cf1c000 7f990cf1d000 r p 00027000 08 21 2233674 usr lib python2.7 dist packages h5py defs.x86 64 linux gnu.so 7f990cf1d000 7f990cf1f000 rw p 00028000 08 21 2233674 usr lib python2.7 dist packages h5py defs.x86 64 linux gnu.so 7f990cf1f000 7f990cf35000 r xp 00000000 08 21 2233736 usr lib python2.7 dist packages h5py objects.x86 64 linux gnu.so 7f990cf35000 7f990d134000 p 00016000 08 21 2233736 usr lib python2.7 dist packages h5py objects.x86 64 linux gnu.so 7f990d134000 7f990d135000 r p 00015000 08 21 2233736 usr lib python2.7 dist packages h5py objects.x86 64 linux gnu.so 7f990d135000 7f990d138000 rw p 00016000 08 21 2233736 usr lib python2.7 dist packages h5py objects.x86 64 linux gnu.so 7f990d138000 7f990d142000 r xp 00000000 08 21 2233715 usr lib python2.7 dist packages h5py h5r.x86 64 linux gnu.so 7f990d142000 7f990d341000 p 0000a000 08 21 2233715 usr lib python2.7 dist packages h5py h5r.x86 64 linux gnu.so 7f990d341000 7f990d342000 r p 00009000 08 21 2233715 usr lib python2.7 dist packages h5py h5r.x86 64 linux gnu.so 7f990d342000 7f990d344000 rw p 0000a000 08 21 2233715 usr lib python2.7 dist packages h5py h5r.x86 64 linux gnu.so 7f990d344000 7f990d351000 r xp 00000000 08 21 2233683 usr lib python2.7 dist packages h5py conv.x86 64 linux gnu.so 7f990d351000 7f990d550000 p 0000d000 08 21 2233683 usr lib python2.7 dist packages h5py conv.x86 64 linux gnu.so 7f990d550000 7f990d551000 r p 0000c000 08 21 2233683 usr lib python2.7 dist packages h5py conv.x86 64 linux gnu.so 7f990d551000 7f990d552000 rw p 0000d000 08 21 2233683 usr lib python2.7 dist packages h5py conv.x86 64 linux gnu.so 7f990d552000 7f990d559000 r xp 00000000 08 21 1053203 usr lib x86 64 linux gnu libaec.so.0.0.3 7f990d559000 7f990d758000 p 00007000 08 21 1053203 usr lib x86 64 linux gnu libaec.so.0.0.3 7f990d758000 7f990d759000 r p 00006000 08 21 1053203 usr lib x86 64 linux gnu libaec.so.0.0.3 7f990d759000 7f990d75a000 rw p 00007000 08 21 1053203 usr lib x86 64 linux gnu libaec.so.0.0.3 7f990d75a000 7f990d75c000 r xp 00000000 08 21 1053205 usr lib x86 64 linux gnu libsz.so.2.0.1 7f990d75c000 7f990d95b000 p 00002000 08 21 1053205 usr lib x86 64 linux gnu libsz.so.2.0.1 7f990d95b000 7f990d95c000 r p 00001000 08 21 1053205 usr lib x86 64 linux gnu libsz.so.2.0.1 7f990d95c000 7f990d95d000 rw p 00002000 08 21 1053205 usr lib x86 64 linux gnu libsz.so.2.0.1 7f990d95d000 7f990dbf1000 r xp 00000000 08 21 1053208 usr lib x86 64 linux gnu libhdf5 serial.so.10.1.0 7f990dbf1000 7f990ddf0000 p 00294000 08 21 1053208 usr lib x86 64 linux gnu libhdf5 serial.so.10.1.0 7f990ddf0000 7f990ddf5000 r p 00293000 08 21 1053208 usr lib x86 64 linux gnu libhdf5 serial.so.10.1.0 7f990ddf5000 7f990ddfa000 rw p 00298000 08 21 1053208 usr lib x86 64 linux gnu libhdf5 serial.so.10.1.0 7f990ddfa000 7f990ddfb000 rw p 00000000 00 00 0 7f990ddfb000 7f990de03000 r xp 00000000 08 21 2233738 usr lib python2.7 dist packages h5py errors.x86 64 linux gnu.so 7f990de03000 7f990e002000 p 00008000 08 21 2233738 usr lib python2.7 dist packages h5py errors.x86 64 linux gnu.so 7f990e002000 7f990e003000 r p 00007000 08 21 2233738 usr lib python2.7 dist packages h5py errors.x86 64 linux gnu.so 7f990e003000 7f990e004000 rw p 00008000 08 21 2233738 usr lib python2.7 dist packages h5py errors.x86 64 linux gnu.so 7f990e004000 7f993a004000 rw p 00000000 00 00 0 7f993a004000 7f993a005000 p 00000000 00 00 0 7f993a005000 7f993a805000 rw p 00000000 00 00 0 7f993a805000 7f993a806000 p 00000000 00 00 0 7f993a806000 7f993b006000 rw p 00000000 00 00 0 7f993b006000 7f993b007000 p 00000000 00 00 0 7f993b007000 7f993b807000 rw p 00000000 00 00 0 7f993b807000 7f993b808000 p 00000000 00 00 0 7f993b808000 7f993c008000 rw p 00000000 00 00 0 7f993c400000 7f993c600000 p 00000000 00 00 0 7f993c60a000 7f993f80b000 rw p 00000000 00 00 0 7f993fc00000 7f993fe00000 p 00000000 00 00 0 7f993fe00000 7f9940000000 rw 00000000 00 05 206094999 dev zero deleted 7f994000c000 7f994200c000 rw p 00000000 00 00 0 7f9942400000 7f9942600000 p 00000000 00 00 0 7f994260d000 7f994580f000 rw p 00000000 00 00 0 7f9945c00000 7f9946000000 p 00000000 00 00 0 7f9946010000 7f9948010000 rw p 00000000 00 00 0 7f9948216000 7f994f81b000 rw p 00000000 00 00 0 7f994fc00000 7f994fe00000 rw 00000000 00 05 206094998 dev zero deleted 7f994fe00000 7f9950000000 p 00000000 00 00 0 7f995001c000 7f995601c000 rw p 00000000 00 00 0 7f995601c000 7f995601d000 p 00000000 00 00 0 7f995601d000 7f995881d000 rw p 00000000 00 00 0 7f995881d000 7f995881e000 p 00000000 00 00 0 7f995881e000 7f995f01e000 rw p 00000000 00 00 0 7f995f400000 7f995f600000 p 00000000 00 00 0 7f995f71f000 7f9962020000 rw p 00000000 00 00 0 7f9962400000 7f9962800000 p 00000000 00 00 0 7f9962821000 7f9964821000 rw p 00000000 00 00 0 7f9964c00000 7f9965000000 p 00000000 00 00 0 7f9965022000 7f9967022000 rw p 00000000 00 00 0 7f9967400000 7f9967800000 p 00000000 00 00 0 7f9967823000 7f9969823000 rw p 00000000 00 00 0 7f9969c00000 7f9969ed6000 rw 00000000 00 06 815 dev nvidiactl 7f9969ed6000 7f996a000000 p 00000000 00 00 0 7f996a024000 7f996c024000 rw p 00000000 00 00 0 7f996c400000 7f996c600000 rw 00000000 00 06 815 dev nvidiactl 7f996c600000 7f996c800000 rw 00000000 00 05 206139044 dev zero deleted 7f996c825000 7f996e825000 rw p 00000000 00 00 0 7f996ec00000 7f996ee00000 rw 00000000 00 06 815 dev nvidiactl 7f996ee00000 7f996f000000 rw 00000000 00 05 206139043 dev zero deleted 7f996f026000 7f9971026000 rw p 00000000 00 00 0 7f9971026000 7f9971027000 p 00000000 00 00 0 7f9971027000 7f9973827000 rw p 00000000 00 00 0 7f9973c00000 7f9973e00000 p 00000000 00 00 0 7f9973e29000 7f997902a000 rw p 00000000 00 00 0 7f997902d000 7f997d02e000 rw p 00000000 00 00 0 7f997d02e000 7f997d02f000 p 00000000 00 00 0 7f997d02f000 7f997f82f000 rw p 00000000 00 00 0 7f997f82f000 7f997f830000 p 00000000 00 00 0 7f997f830000 7f9984030000 rw p 00000000 00 00 0 7f9984400000 7f9984600000 p 00000000 00 00 0 7f9984731000 7f9987032000 rw p 00000000 00 00 0 7f9987171000 7f998c034000 rw p 00000000 00 00 0 7f998c0b2000 7f9996038000 rw p 00000000 00 00 0 7f9996038000 7f99978ee000 r xp 00000000 08 21 62790680 usr lib x86 64 linux gnu libicudata.so.55.1 7f99978ee000 7f9997aed000 p 018b6000 08 21 62790680 usr lib x86 64 linux gnu libicudata.so.55.1 7f9997aed000 7f9997aee000 r p 018b5000 08 21 62790680 usr lib x86 64 linux gnu libicudata.so.55.1 7f9997aee000 7f9997aef000 rw p 018b6000 08 21 62790680 usr lib x86 64 linux gnu libicudata.so.55.1 7f9997aef000 7f9997af2000 r xp 00000000 08 21 45488001 lib x86 64 linux gnu libkeyutils.so.1.5 7f9997af2000 7f9997cf1000 p 00003000 08 21 45488001 lib x86 64 linux gnu libkeyutils.so.1.5 7f9997cf1000 7f9997cf2000 r p 00002000 08 21 45488001 lib x86 64 linux gnu libkeyutils.so.1.5 7f9997cf2000 7f9997cf3000 rw p 00003000 08 21 45488001 lib x86 64 linux gnu libkeyutils.so.1.5 7f9997cf3000 7f9997e72000 r xp 00000000 08 21 62790694 usr lib x86 64 linux gnu libicuuc.so.55.1 7f9997e72000 7f9998072000 p 0017f000 08 21 62790694 usr lib x86 64 linux gnu libicuuc.so.55.1 7f9998072000 7f9998082000 r p 0017f000 08 21 62790694 usr lib x86 64 linux gnu libicuuc.so.55.1 7f9998082000 7f9998083000 rw p 0018f000 08 21 62790694 usr lib x86 64 linux gnu libicuuc.so.55.1 7f9998083000 7f9998087000 rw p 00000000 00 00 0 7f9998087000 7f9998091000 r xp 00000000 08 21 45746719 usr lib x86 64 linux gnu libkrb5support.so.0.1 7f9998091000 7f9998290000 p 0000a000 08 21 45746719 usr lib x86 64 linux gnu libkrb5support.so.0.1 7f9998290000 7f9998291000 r p 00009000 08 21 45746719 usr lib x86 64 linux gnu libkrb5support.so.0.1 7f9998291000 7f9998292000 rw p 0000a000 08 21 45746719 usr lib x86 64 linux gnu libkrb5support.so.0.1 7f9998292000 7f9998295000 r xp 00000000 08 21 38015890 lib x86 64 linux gnu libcom err.so.2.1 7f9998295000 7f9998494000 p 00003000 08 21 38015890 lib x86 64 linux gnu libcom err.so.2.1 7f9998494000 7f9998495000 r p 00002000 08 21 38015890 lib x86 64 linux gnu libcom err.so.2.1 7f9998495000 7f9998496000 rw p 00003000 08 21 38015890 lib x86 64 linux gnu libcom err.so.2.1 7f9998496000 7f99984c2000 r xp 00000000 08 21 45746713 usr lib x86 64 linux gnu libk5crypto.so.3.1 7f99984c2000 7f99986c1000 p 0002c000 08 21 45746713 usr lib x86 64 linux gnu libk5crypto.so.3.1 7f99986c1000 7f99986c3000 r p 0002b000 08 21 45746713 usr lib x86 64 linux gnu libk5crypto.so.3.1 7f99986c3000 7f99986c4000 rw p 0002d000 08 21 45746713 usr lib x86 64 linux gnu libk5crypto.so.3.1 7f99986c4000 7f99986c5000 rw p 00000000 00 00 0 7f99986c5000 7f9998788000 r xp 00000000 08 21 45746717 usr lib x86 64 linux gnu libkrb5.so.3.3 7f9998788000 7f9998988000 p 000c3000 08 21 45746717 usr lib x86 64 linux gnu libkrb5.so.3.3 7f9998988000 7f9998995000 r p 000c3000 08 21 45746717 usr lib x86 64 linux gnu libkrb5.so.3.3 7f9998995000 7f9998997000 rw p 000d0000 08 21 45746717 usr lib x86 64 linux gnu libkrb5.so.3.3 7f9998997000 7f99989a9000 r xp 00000000 08 21 38015908 lib x86 64 linux gnu libgpg error.so.0.17.0 7f99989a9000 7f9998ba9000 p 00012000 08 21 38015908 lib x86 64 linux gnu libgpg error.so.0.17.0 7f9998ba9000 7f9998baa000 r p 00012000 08 21 38015908 lib x86 64 linux gnu libgpg error.so.0.17.0 7f9998baa000 7f9998bab000 rw p 00013000 08 21 38015908 lib x86 64 linux gnu libgpg error.so.0.17.0 7f9998bab000 7f9998bcc000 r xp 00000000 08 21 53355465 usr lib x86 64 linux gnu libgomp.so.1.0.0 7f9998bcc000 7f9998dcb000 p 00021000 08 21 53355465 usr lib x86 64 linux gnu libgomp.so.1.0.0 7f9998dcb000 7f9998dcc000 r p 00020000 08 21 53355465 usr lib x86 64 linux gnu libgomp.so.1.0.0 7f9998dcc000 7f9998dcd000 rw p 00021000 08 21 53355465 usr lib x86 64 linux gnu libgomp.so.1.0.0 7f9998dcd000 7f9998dd3000 r xp 00000000 08 21 62790576 usr lib x86 64 linux gnu libdatrie.so.1.3.3 7f9998dd3000 7f9998fd3000 p 00006000 08 21 62790576 usr lib x86 64 linux gnu libdatrie.so.1.3.3 7f9998fd3000 7f9998fd4000 r p 00006000 08 21 62790576 usr lib x86 64 linux gnu libdatrie.so.1.3.3 7f9998fd4000 7f9998fd5000 rw p 00007000 08 21 62790576 usr lib x86 64 linux gnu libdatrie.so.1.3.3 7f9998fd5000 7f9998ff9000 r xp 00000000 08 21 62790651 usr lib x86 64 linux gnu libgraphite2.so.3.0.1 7f9998ff9000 7f99991f8000 p 00024000 08 21 62790651 usr lib x86 64 linux gnu libgraphite2.so.3.0.1 7f99991f8000 7f99991fa000 r p 00023000 08 21 62790651 usr lib x86 64 linux gnu libgraphite2.so.3.0.1 7f99991fa000 7f99991fb000 rw p 00025000 08 21 62790651 usr lib x86 64 linux gnu libgraphite2.so.3.0.1 7f99991fb000 7f999920c000 r xp 00000000 08 21 45746737 usr lib x86 64 linux gnu libtasn1.so.6.5.1 7f999920c000 7f999940c000 p 00011000 08 21 45746737 usr lib x86 64 linux gnu libtasn1.so.6.5.1 7f999940c000 7f999940d000 r p 00011000 08 21 45746737 usr lib x86 64 linux gnu libtasn1.so.6.5.1 7f999940d000 7f999940e000 rw p 00012000 08 21 45746737 usr lib x86 64 linux gnu libtasn1.so.6.5.1 7f999940e000 7f999943f000 r xp 00000000 08 21 45746711 usr lib x86 64 linux gnu libidn.so.11.6.15 7f999943f000 7f999963f000 p 00031000 08 21 45746711 usr lib x86 64 linux gnu libidn.so.11.6.15 7f999963f000 7f9999640000 r p 00031000 08 21 45746711 usr lib x86 64 linux gnu libidn.so.11.6.15 7f9999640000 7f9999641000 rw p 00032000 08 21 45746711 usr lib x86 64 linux gnu libidn.so.11.6.15 7f9999641000 7f999969a000 r xp 00000000 08 21 45746728 usr lib x86 64 linux gnu libp11 kit.so.0.1.0 7f999969a000 7f9999899000 p 00059000 08 21 45746728 usr lib x86 64 linux gnu libp11 kit.so.0.1.0 7f9999899000 7f99998a3000 r p 00058000 08 21 45746728 usr lib x86 64 linux gnu libp11 kit.so.0.1.0 7f99998a3000 7f99998a5000 rw p 00062000 08 21 45746728 usr lib x86 64 linux gnu libp11 kit.so.0.1.0 7f99998a5000 7f9999a56000 r xp 00000000 08 21 62790962 usr lib x86 64 linux gnu libxml2.so.2.9.3 7f9999a56000 7f9999c55000 p 001b1000 08 21 62790962 usr lib x86 64 linux gnu libxml2.so.2.9.3 7f9999c55000 7f9999c5d000 r p 001b0000 08 21 62790962 usr lib x86 64 linux gnu libxml2.so.2.9.3 7f9999c5d000 7f9999c5f000 rw p 001b8000 08 21 62790962 usr lib x86 64 linux gnu libxml2.so.2.9.3 7f9999c5f000 7f9999c60000 rw p 00000000 00 00 0 7f9999c60000 7f9999cdf000 r xp 00000000 08 21 45746693 usr lib x86 64 linux gnu libgmp.so.10.3.0 7f9999cdf000 7f9999ede000 p 0007f000 08 21 45746693 usr lib x86 64 linux gnu libgmp.so.10.3.0 7f9999ede000 7f9999edf000 r p 0007e000 08 21 45746693 usr lib x86 64 linux gnu libgmp.so.10.3.0 7f9999edf000 7f9999ee0000 rw p 0007f000 08 21 45746693 usr lib x86 64 linux gnu libgmp.so.10.3.0 7f9999ee0000 7f9999f14000 r xp 00000000 08 21 45746726 usr lib x86 64 linux gnu libnettle.so.6.2 7f9999f14000 7f999a113000 p 00034000 08 21 45746726 usr lib x86 64 linux gnu libnettle.so.6.2 7f999a113000 7f999a115000 r p 00033000 08 21 45746726 usr lib x86 64 linux gnu libnettle.so.6.2 7f999a115000 7f999a116000 rw p 00035000 08 21 45746726 usr lib x86 64 linux gnu libnettle.so.6.2 7f999a116000 7f999a148000 r xp 00000000 08 21 45746707 usr lib x86 64 linux gnu libhogweed.so.4.2 7f999a148000 7f999a347000 p 00032000 08 21 45746707 usr lib x86 64 linux gnu libhogweed.so.4.2 7f999a347000 7f999a348000 r p 00031000 08 21 45746707 usr lib x86 64 linux gnu libhogweed.so.4.2 7f999a348000 7f999a349000 rw p 00032000 08 21 45746707 usr lib x86 64 linux gnu libhogweed.so.4.2 7f999a349000 7f999a390000 r xp 00000000 08 21 45746699 usr lib x86 64 linux gnu libgssapi krb5.so.2.2 7f999a390000 7f999a58f000 p 00047000 08 21 45746699 usr lib x86 64 linux gnu libgssapi krb5.so.2.2 7f999a58f000 7f999a591000 r p 00046000 08 21 45746699 usr lib x86 64 linux gnu libgssapi krb5.so.2.2 7f999a591000 7f999a593000 rw p 00048000 08 21 45746699 usr lib x86 64 linux gnu libgssapi krb5.so.2.2 7f999a593000 7f999a66a000 r xp 00000000 08 21 38015906 lib x86 64 linux gnu libgcrypt.so.20.0.5 7f999a66a000 7f999a86a000 p 000d7000 08 21 38015906 lib x86 64 linux gnu libgcrypt.so.20.0.5 7f999a86a000 7f999a86b000 r p 000d7000 08 21 38015906 lib x86 64 linux gnu libgcrypt.so.20.0.5 7f999a86b000 7f999a873000 rw p 000d8000 08 21 38015906 lib x86 64 linux gnu libgcrypt.so.20.0.5 7f999a873000 7f999a874000 rw p 00000000 00 00 0 7f999a874000 7f999a8ef000 r xp 00000000 08 21 62790799 usr lib x86 64 linux gnu liborc 0.4.so.0.25.0 7f999a8ef000 7f999aaee000 p 0007b000 08 21 62790799 usr lib x86 64 linux gnu liborc 0.4.so.0.25.0 7f999aaee000 7f999aaf0000 r p 0007a000 08 21 62790799 usr lib x86 64 linux gnu liborc 0.4.so.0.25.0 7f999aaf0000 7f999aaf4000 rw p 0007c000 08 21 62790799 usr lib x86 64 linux gnu liborc 0.4.so.0.25.0 7f999aaf4000 7f999aafb000 r xp 00000000 08 21 62790719 usr lib x86 64 linux gnu libogg.so.0.8.2 7f999aafb000 7f999acfb000 p 00007000 08 21 62790719 usr lib x86 64 linux gnu libogg.so.0.8.2 7f999acfb000 7f999acfc000 r p 00007000 08 21 62790719 usr lib x86 64 linux gnu libogg.so.0.8.2 7f999acfc000 7f999acfd000 rw p 00008000 08 21 62790719 usr lib x86 64 linux gnu libogg.so.0.8.2 7f999acfd000 7f999ad07000 r xp 00000000 08 21 62790717 usr lib x86 64 linux gnu libnuma.so.1.0.0 7f999ad07000 7f999af06000 p 0000a000 08 21 62790717 usr lib x86 64 linux gnu libnuma.so.1.0.0 7f999af06000 7f999af07000 r p 00009000 08 21 62790717 usr lib x86 64 linux gnu libnuma.so.1.0.0 7f999af07000 7f999af08000 rw p 0000a000 08 21 62790717 usr lib x86 64 linux gnu libnuma.so.1.0.0 7f999af08000 7f999af36000 r xp 00000000 08 21 62790869 usr lib x86 64 linux gnu libsoxr.so.0.1.1 7f999af36000 7f999b135000 p 0002e000 08 21 62790869 usr lib x86 64 linux gnu libsoxr.so.0.1.1 7f999b135000 7f999b137000 r p 0002d000 08 21 62790869 usr lib x86 64 linux gnu libsoxr.so.0.1.1 7f999b137000 7f999b138000 rw p 0002f000 08 21 62790869 usr lib x86 64 linux gnu libsoxr.so.0.1.1 7f999b138000 7f999b16d000 rw p 00000000 00 00 0 7f999b16d000 7f999b192000 r xp 00000000 08 21 62790920 usr lib x86 64 linux gnu libv4lconvert.so.0.0.0 7f999b192000 7f999b392000 p 00025000 08 21 62790920 usr lib x86 64 linux gnu libv4lconvert.so.0.0.0 7f999b392000 7f999b394000 r p 00025000 08 21 62790920 usr lib x86 64 linux gnu libv4lconvert.so.0.0.0 7f999b394000 7f999b395000 rw p 00027000 08 21 62790920 usr lib x86 64 linux gnu libv4lconvert.so.0.0.0 7f999b395000 7f999b3e7000 rw p 00000000 00 00 0 7f999b3e7000 7f999b449000 r xp 00000000 08 21 62790530 usr lib x86 64 linux gnu libxt.so.6.0.0 7f999b449000 7f999b649000 p 00062000 08 21 62790530 usr lib x86 64 linux gnu libxt.so.6.0.0 7f999b649000 7f999b64a000 r p 00062000 08 21 62790530 usr lib x86 64 linux gnu libxt.so.6.0.0 7f999b64a000 7f999b64f000 rw p 00063000 08 21 62790530 usr lib x86 64 linux gnu libxt.so.6.0.0 7f999b64f000 7f999b650000 rw p 00000000 00 00 0 7f999b650000 7f999b658000 r xp 00000000 08 21 62790892 usr lib x86 64 linux gnu libthai.so.0.2.4 7f999b658000 7f999b857000 p 00008000 08 21 62790892 usr lib x86 64 linux gnu libthai.so.0.2.4 7f999b857000 7f999b858000 r p 00007000 08 21 62790892 usr lib x86 64 linux gnu libthai.so.0.2.4 7f999b858000 7f999b859000 rw p 00008000 08 21 62790892 usr lib x86 64 linux gnu libthai.so.0.2.4 7f999b859000 7f999b8b5000 r xp 00000000 08 21 62790678 usr lib x86 64 linux gnu libharfbuzz.so.0.10000.1 7f999b8b5000 7f999bab5000 p 0005c000 08 21 62790678 usr lib x86 64 linux gnu libharfbuzz.so.0.10000.1 7f999bab5000 7f999bab6000 r p 0005c000 08 21 62790678 usr lib x86 64 linux gnu libharfbuzz.so.0.10000.1 7f999bab6000 7f999bab7000 rw p 0005d000 08 21 62790678 usr lib x86 64 linux gnu libharfbuzz.so.0.10000.1 7f999bab7000 7f999bace000 r xp 00000000 08 21 38015955 lib x86 64 linux gnu libresolv 2.23.so 7f999bace000 7f999bcce000 p 00017000 08 21 38015955 lib x86 64 linux gnu libresolv 2.23.so 7f999bcce000 7f999bccf000 r p 00017000 08 21 38015955 lib x86 64 linux gnu libresolv 2.23.so 7f999bccf000 7f999bcd0000 rw p 00018000 08 21 38015955 lib x86 64 linux gnu libresolv 2.23.so 7f999bcd0000 7f999bcd2000 rw p 00000000 00 00 0 7f999bcd2000 7f999bcf1000 r xp 00000000 08 21 38015961 lib x86 64 linux gnu libselinux.so.1 7f999bcf1000 7f999bef0000 p 0001f000 08 21 38015961 lib x86 64 linux gnu libselinux.so.1 7f999bef0000 7f999bef1000 r p 0001e000 08 21 38015961 lib x86 64 linux gnu libselinux.so.1 7f999bef1000 7f999bef2000 rw p 0001f000 08 21 38015961 lib x86 64 linux gnu libselinux.so.1 7f999bef2000 7f999bef4000 rw p 00000000 00 00 0 7f999bef4000 7f999befc000 r xp 00000000 08 21 62790950 usr lib x86 64 linux gnu libxcb render.so.0.0.0 7f999befc000 7f999c0fc000 p 00008000 08 21 62790950 usr lib x86 64 linux gnu libxcb render.so.0.0.0 7f999c0fc000 7f999c0fd000 r p 00008000 08 21 62790950 usr lib x86 64 linux gnu libxcb render.so.0.0.0 7f999c0fd000 7f999c0fe000 rw p 00009000 08 21 62790950 usr lib x86 64 linux gnu libxcb render.so.0.0.0 7f999c0fe000 7f99a3f2c000 r xp 00000000 08 21 50074698 usr local cuda 9.0 targets x86 64 linux lib libcufft.so.9.0.176 7f99a3f2c000 7f99a412c000 p 07e2e000 08 21 50074698 usr local cuda 9.0 targets x86 64 linux lib libcufft.so.9.0.176 7f99a412c000 7f99a413b000 rw p 07e2e000 08 21 50074698 usr local cuda 9.0 targets x86 64 linux lib libcufft.so.9.0.176 7f99a413b000 7f99a419f000 rw p 00000000 00 00 0 7f99a419f000 7f99b77d1000 r xp 00000000 08 21 54406239 usr lib x86 64 linux gnu libcudnn.so.7.6.0 7f99b77d1000 7f99b79d1000 p 13632000 08 21 54406239 usr lib x86 64 linux gnu libcudnn.so.7.6.0 7f99b79d1000 7f99b7a4c000 rw p 13632000 08 21 54406239 usr lib x86 64 linux gnu libcudnn.so.7.6.0 7f99b7a4c000 7f99b7ade000 rw p 00000000 00 00 0 7f99b7ade000 7f99bc48d000 r xp 00000000 08 21 50074704 usr local cuda 9.0 targets x86 64 linux lib libcusolver.so.9.0.176 7f99bc48d000 7f99bc68d000 p 049af000 08 21 50074704 usr local cuda 9.0 targets x86 64 linux lib libcusolver.so.9.0.176 7f99bc68d000 7f99bc6c7000 rw p 049af000 08 21 50074704 usr local cuda 9.0 targets x86 64 linux lib libcusolver.so.9.0.176 7f99bc6c7000 7f99bc6d9000 rw p 00000000 00 00 0 7f99bc6d9000 7f99beb62000 r xp 00000000 08 21 50074702 usr local cuda 9.0 targets x86 64 linux lib libcurand.so.9.0.176 7f99beb62000 7f99bed61000 p 02489000 08 21 50074702 usr local cuda 9.0 targets x86 64 linux lib libcurand.so.9.0.176 7f99bed61000 7f99c0133000 rw p 02488000 08 21 50074702 usr local cuda 9.0 targets x86 64 linux lib libcurand.so.9.0.176 7f99c0133000 7f99c063d000 rw p 00000000 00 00 0 7f99c063d000 7f99d02b3000 r xp 00000000 08 21 63967275 usr local lib python2.7 dist packages mxnet 1.3.0 py2.7.egg mxnet libmxnet.so 7f99d02b3000 7f99d04b3000 p 0fc76000 08 21 63967275 usr local lib python2.7 dist packages mxnet 1.3.0 py2.7.egg mxnet libmxnet.so 7f99d04b3000 7f99d04ed000 r p 0fc76000 08 21 63967275 usr local lib python2.7 dist packages mxnet 1.3.0 py2.7.egg mxnet libmxnet.so 7f99d04ed000 7f99d0511000 rw p 0fcb0000 08 21 63967275 usr local lib python2.7 dist packages mxnet 1.3.0 py2.7.egg mxnet libmxnet.so 7f99d0511000 7f99d1029000 rw p 00000000 00 00 0 7f99d1029000 7f99d102e000 r xp 00000000 08 21 62790496 usr lib x86 64 linux gnu libxdmcp.so.6.0.0 7f99d102e000 7f99d122d000 p 00005000 08 21 62790496 usr lib x86 64 linux gnu libxdmcp.so.6.0.0 7f99d122d000 7f99d122e000 r p 00004000 08 21 62790496 usr lib x86 64 linux gnu libxdmcp.so.6.0.0 7f99d122e000 7f99d122f000 rw p 00005000 08 21 62790496 usr lib x86 64 linux gnu libxdmcp.so.6.0.0 7f99d122f000 7f99d1231000 r xp 00000000 08 21 62790477 usr lib x86 64 linux gnu libxau.so.6.0.0 7f99d1231000 7f99d1431000 p 00002000 08 21 62790477 usr lib x86 64 linux gnu libxau.so.6.0.0 7f99d1431000 7f99d1432000 r p 00002000 08 21 62790477 usr lib x86 64 linux gnu libxau.so.6.0.0 7f99d1432000 7f99d1433000 rw p 00003000 08 21 62790477 usr lib x86 64 linux gnu libxau.so.6.0.0 7f99d1433000 7f99d1454000 r xp 00000000 08 21 62790960 usr lib x86 64 linux gnu libxcb.so.1.1.0 7f99d1454000 7f99d1653000 p 00021000 08 21 62790960 usr lib x86 64 linux gnu libxcb.so.1.1.0 7f99d1653000 7f99d1654000 r p 00020000 08 21 62790960 usr lib x86 64 linux gnu libxcb.so.1.1.0 7f99d1654000 7f99d1655000 rw p 00021000 08 21 62790960 usr lib x86 64 linux gnu libxcb.so.1.1.0 7f99d1655000 7f99d1659000 r xp 00000000 08 21 38015980 lib x86 64 linux gnu libuuid.so.1.3.0 7f99d1659000 7f99d1858000 p 00004000 08 21 38015980 lib x86 64 linux gnu libuuid.so.1.3.0 7f99d1858000 7f99d1859000 r p 00003000 08 21 38015980 lib x86 64 linux gnu libuuid.so.1.3.0 7f99d1859000 7f99d185a000 rw p 00004000 08 21 38015980 lib x86 64 linux gnu libuuid.so.1.3.0 7f99d185a000 7f99d18c8000 r xp 00000000 08 21 38015948 lib x86 64 linux gnu libpcre.so.3.13.2 7f99d18c8000 7f99d1ac8000 p 0006e000 08 21 38015948 lib x86 64 linux gnu libpcre.so.3.13.2 7f99d1ac8000 7f99d1ac9000 r p 0006e000 08 21 38015948 lib x86 64 linux gnu libpcre.so.3.13.2 7f99d1ac9000 7f99d1aca000 rw p 0006f000 08 21 38015948 lib x86 64 linux gnu libpcre.so.3.13.2 7f99d1aca000 7f99d1bff000 r xp 00000000 08 21 62790473 usr lib x86 64 linux gnu libx11.so.6.3.0 7f99d1bff000 7f99d1dff000 p 00135000 08 21 62790473 usr lib x86 64 linux gnu libx11.so.6.3.0 7f99d1dff000 7f99d1e00000 r p 00135000 08 21 62790473 usr lib x86 64 linux gnu libx11.so.6.3.0 7f99d1e00000 7f99d1e04000 rw p 00136000 08 21 62790473 usr lib x86 64 linux gnu libx11.so.6.3.0 7f99d1e04000 7f99d1e15000 r xp 00000000 08 21 62790500 usr lib x86 64 linux gnu libxext.so.6.4.0 7f99d1e15000 7f99d2014000 p 00011000 08 21 62790500 usr lib x86 64 linux gnu libxext.so.6.4.0 7f99d2014000 7f99d2015000 r p 00010000 08 21 62790500 usr lib x86 64 linux gnu libxext.so.6.4.0 7f99d2015000 7f99d2016000 rw p 00011000 08 21 62790500 usr lib x86 64 linux gnu libxext.so.6.4.0 7f99d2016000 7f99d201f000 r xp 00000000 08 21 62790528 usr lib x86 64 linux gnu libxrender.so.1.3.0 7f99d201f000 7f99d221e000 p 00009000 08 21 62790528 usr lib x86 64 linux gnu libxrender.so.1.3.0 7f99d221e000 7f99d221f000 r p 00008000 08 21 62790528 usr lib x86 64 linux gnu libxrender.so.1.3.0 7f99d221f000 7f99d2220000 rw p 00009000 08 21 62790528 usr lib x86 64 linux gnu libxrender.so.1.3.0 7f99d2220000 7f99d2236000 r xp 00000000 08 21 62790441 usr lib x86 64 linux gnu libice.so.6.3.0 7f99d2236000 7f99d2435000 p 00016000 08 21 62790441 usr lib x86 64 linux gnu libice.so.6.3.0 7f99d2435000 7f99d2436000 r p 00015000 08 21 62790441 usr lib x86 64 linux gnu libice.so.6.3.0 7f99d2436000 7f99d2437000 rw p 00016000 08 21 62790441 usr lib x86 64 linux gnu libice.so.6.3.0 7f99d2437000 7f99d243a000 rw p 00000000 00 00 0 7f99d243a000 7f99d2441000 r xp 00000000 08 21 62790467 usr lib x86 64 linux gnu libsm.so.6.0.1 7f99d2441000 7f99d2640000 p 00007000 08 21 62790467 usr lib x86 64 linux gnu libsm.so.6.0.1 7f99d2640000 7f99d2641000 r p 00006000 08 21 62790467 usr lib x86 64 linux gnu libsm.so.6.0.1 7f99d2641000 7f99d2642000 rw p 00007000 08 21 62790467 usr lib x86 64 linux gnu libsm.so.6.0.1 7f99d2642000 7f99d2751000 r xp 00000000 08 21 62529459 lib x86 64 linux gnu libglib 2.0.so.0.4800.2 7f99d2751000 7f99d2950000 p 0010f000 08 21 62529459 lib x86 64 linux gnu libglib 2.0.so.0.4800.2 7f99d2950000 7f99d2951000 r p 0010e000 08 21 62529459 lib x86 64 linux gnu libglib 2.0.so.0.4800.2 7f99d2951000 7f99d2952000 rw p 0010f000 08 21 62529459 lib x86 64 linux gnu libglib 2.0.so.0.4800.2 7f99d2952000 7f99d2953000 rw p 00000000 00 00 0 7f99d2953000 7f99d2954000 r xp 00000000 08 21 62790657 usr lib x86 64 linux gnu libgthread 2.0.so.0.4800.2 7f99d2954000 7f99d2b53000 p 00001000 08 21 62790657 usr lib x86 64 linux gnu libgthread 2.0.so.0.4800.2 7f99d2b53000 7f99d2b54000 r p 00000000 08 21 62790657 usr lib x86 64 linux gnu libgthread 2.0.so.0.4800.2 7f99d2b54000 7f99d2b55000 rw p 00001000 08 21 62790657 usr lib x86 64 linux gnu libgthread 2.0.so.0.4800.2 7f99d2b55000 7f99d2e4e000 r xp 00000000 08 21 1447102 usr local lib python2.7 dist packages cv2 .libs libvpx 8459aeef.so.6.0.0 7f99d2e4e000 7f99d304e000 p 002f9000 08 21 1447102 usr local lib python2.7 dist packages cv2 .libs libvpx 8459aeef.so.6.0.0 7f99d304e000 7f99d3051000 rw p 002f9000 08 21 1447102 usr local lib python2.7 dist packages cv2 .libs libvpx 8459aeef.so.6.0.0 7f99d3051000 7f99d3058000 rw p 00000000 00 00 0 7f99d3058000 7f99d3059000 rw p 0032c000 08 21 1447102 usr local lib python2.7 dist packages cv2 .libs libvpx 8459aeef.so.6.0.0 7f99d3059000 7f99d3073000 r xp 00000000 08 21 1447098 usr local lib python2.7 dist packages cv2 .libs libswresample a49c020a.so.3.4.100 7f99d3073000 7f99d3273000 p 0001a000 08 21 1447098 usr local lib python2.7 dist packages cv2 .libs libswresample a49c020a.so.3.4.100 7f99d3273000 7f99d3275000 rw p 0001a000 08 21 1447098 usr local lib python2.7 dist packages cv2 .libs libswresample a49c020a.so.3.4.100 7f99d3275000 7f99d3277000 rw p 0001d000 08 21 1447098 usr local lib python2.7 dist packages cv2 .libs libswresample a49c020a.so.3.4.100 7f99d3277000 7f99d328d000 r xp 00000000 08 21 38015904 lib x86 64 linux gnu libgcc s.so.1 7f99d328d000 7f99d348c000 p 00016000 08 21 38015904 lib x86 64 linux gnu libgcc s.so.1 7f99d348c000 7f99d348d000 rw p 00015000 08 21 38015904 lib x86 64 linux gnu libgcc s.so.1 7f99d348d000 7f99d35ff000 r xp 00000000 08 21 38274693 usr lib x86 64 linux gnu libstdc .so.6.0.21 7f99d35ff000 7f99d37ff000 p 00172000 08 21 38274693 usr lib x86 64 linux gnu libstdc .so.6.0.21 7f99d37ff000 7f99d3809000 r p 00172000 08 21 38274693 usr lib x86 64 linux gnu libstdc .so.6.0.21 7f99d3809000 7f99d380b000 rw p 0017c000 08 21 38274693 usr lib x86 64 linux gnu libstdc .so.6.0.21 7f99d380b000 7f99d380f000 rw p 00000000 00 00 0 7f99d380f000 7f99d3b0b000 r xp 00000000 08 21 1447094 usr local lib python2.7 dist packages cv2 .libs libqtcore 3dbacd8a.so.4.8.7 7f99d3b0b000 7f99d3b1b000 p 002fc000 08 21 1447094 usr local lib python2.7 dist packages cv2 .libs libqtcore 3dbacd8a.so.4.8.7 7f99d3b1b000 7f99d3b47000 rw p 0030c000 08 21 1447094 usr local lib python2.7 dist packages cv2 .libs libqtcore 3dbacd8a.so.4.8.7 7f99d3b47000 7f99d3d0a000 p 00338000 08 21 1447094 usr local lib python2.7 dist packages cv2 .libs libqtcore 3dbacd8a.so.4.8.7 7f99d3d0a000 7f99d3d1a000 rw p 002fb000 08 21 1447094 usr local lib python2.7 dist packages cv2 .libs libqtcore 3dbacd8a.so.4.8.7 7f99d3d1a000 7f99d3d1b000 rw p 00000000 00 00 0 7f99d3d1b000 7f99d4823000 r xp 00000000 08 21 1447095 usr local lib python2.7 dist packages cv2 .libs libqtgui 6d0f14dd.so.4.8.7 7f99d4823000 7f99d4a23000 p 00b08000 08 21 1447095 usr local lib python2.7 dist packages cv2 .libs libqtgui 6d0f14dd.so.4.8.7 7f99d4a23000 7f99d4a7c000 rw p 00b08000 08 21 1447095 usr local lib python2.7 dist packages cv2 .libs libqtgui 6d0f14dd.so.4.8.7 7f99d4a7c000 7f99d4a7e000 rw p 00000000 00 00 0 7f99d4a7e000 7f99d4bac000 rw p 00b61000 08 21 1447095 usr local lib python2.7 dist packages cv2 .libs libqtgui 6d0f14dd.so.4.8.7 7f99d4bac000 7f99d57cb000 r xp 00000000 08 21 1447096 usr local lib python2.7 dist packages cv2 .libs libavcodec 874f3d51.so.58.47.106 7f99d57cb000 7f99d59cb000 p 00c1f000 08 21 1447096 usr local lib python2.7 dist packages cv2 .libs libavcodec 874f3d51.so.58.47.106 7f99d59cb000 7f99d5a2a000 rw p 00c1f000 08 21 1447096 usr local lib python2.7 dist packages cv2 .libs libavcodec 874f3d51.so.58.47.106 7f99d5a2a000 7f99d6208000 rw p 00000000 00 00 0 7f99d6208000 7f99d6218000 rw p 00c7f000 08 21 1447096 usr local lib python2.7 dist packages cv2 .libs libavcodec 874f3d51.so.58.47.106 7f99d6218000 7f99d7c28000 r xp 00000000 08 21 1447070 usr local lib python2.7 dist packages cv2 cv2.so 7f99d7c28000 7f99d7e28000 p 01a10000 08 21 1447070 usr local lib python2.7 dist packages cv2 cv2.so 7f99d7e28000 7f99d7ec1000 rw p 01a10000 08 21 1447070 usr local lib python2.7 dist packages cv2 cv2.so 7f99d7ec1000 7f99d7f7f000 rw p 00000000 00 00 0 7f99d7f7f000 7f99d8000000 rw p 01aaa000 08 21 1447070 usr local lib python2.7 dist packages cv2 cv2.so 7f99d8000000 7f99d8021000 rw p 00000000 00 00 0 7f99d8021000 7f99dc000000 p 00000000 00 00 0 7f99dc000000 7f99dc021000 rw p 00000000 00 00 0 7f99dc021000 7f99e0000000 p 00000000 00 00 0 7f99e0000000 7f99e0021000 rw p 00000000 00 00 0 7f99e0021000 7f99e4000000 p 00000000 00 00 0 7f99e414c000 7f99e4153000 r xp 00000000 08 21 38015957 lib x86 64 linux gnu librt 2.23.so 7f99e4153000 7f99e4352000 p 00007000 08 21 38015957 lib x86 64 linux gnu librt 2.23.so 7f99e4352000 7f99e4353000 r p 00006000 08 21 38015957 lib x86 64 linux gnu librt 2.23.so 7f99e4353000 7f99e4354000 rw p 00007000 08 21 38015957 lib x86 64 linux gnu librt 2.23.so 7f99e4354000 7f99e4378000 r xp 00000000 08 21 1447097 usr local lib python2.7 dist packages cv2 .libs libqttest 1183da5d.so.4.8.7 7f99e4378000 7f99e4578000 p 00024000 08 21 1447097 usr local lib python2.7 dist packages cv2 .libs libqttest 1183da5d.so.4.8.7 7f99e4578000 7f99e4581000 rw p 00024000 08 21 1447097 usr local lib python2.7 dist packages cv2 .libs libqttest 1183da5d.so.4.8.7 7f99e4581000 7f99e4601000 r xp 00000000 08 21 1447093 usr local lib python2.7 dist packages cv2 .libs libswscale 4e6f4703.so.5.4.100 7f99e4601000 7f99e4800000 p 00080000 08 21 1447093 usr local lib python2.7 dist packages cv2 .libs libswscale 4e6f4703.so.5.4.100 7f99e4800000 7f99e4802000 rw p 0007f000 08 21 1447093 usr local lib python2.7 dist packages cv2 .libs libswscale 4e6f4703.so.5.4.100 7f99e4802000 7f99e480a000 rw p 00000000 00 00 0 7f99e480a000 7f99e480c000 rw p 00082000 08 21 1447093 usr local lib python2.7 dist packages cv2 .libs libswscale 4e6f4703.so.5.4.100 7f99e480c000 7f99e4869000 r xp 00000000 08 21 1447100 usr local lib python2.7 dist packages cv2 .libs libavutil 473e9eb1.so.56.26.100 7f99e4869000 7f99e4a69000 p 0005d000 08 21 1447100 usr local lib python2.7 dist packages cv2 .libs libavutil 473e9eb1.so.56.26.100 7f99e4a69000 7f99e4a74000 rw p 0005d000 08 21 1447100 usr local lib python2.7 dist packages cv2 .libs libavutil 473e9eb1.so.56.26.100 7f99e4a74000 7f99e4a83000 rw p 00000000 00 00 0 7f99e4a83000 7f99e4a87000 rw p 00069000 08 21 1447100 usr local lib python2.7 dist packages cv2 .libs libavutil 473e9eb1.so.56.26.100 7f99e4a87000 7f99e4ca4000 r xp 00000000 08 21 1447101 usr local lib python2.7 dist packages cv2 .libs libavformat fb41c63f.so.58.26.101 7f99e4ca4000 7f99e4ea4000 p 0021d000 08 21 1447101 usr local lib python2.7 dist packages cv2 .libs libavformat fb41c63f.so.58.26.101 7f99e4ea4000 7f99e4ee4000 rw p 0021d000 08 21 1447101 usr local lib python2.7 dist packages cv2 .libs libavformat fb41c63f.so.58.26.101 7f99e4ee4000 7f99e4ef8000 r xp 00000000 08 21 1447099 usr local lib python2.7 dist packages cv2 .libs libz a147dcb0.so.1.2.3 7f99e4ef8000 7f99e50f7000 p 00014000 08 21 1447099 usr local lib python2.7 dist packages cv2 .libs libz a147dcb0.so.1.2.3 7f99e50f7000 7f99e50f8000 rw p 00013000 08 21 1447099 usr local lib python2.7 dist packages cv2 .libs libz a147dcb0.so.1.2.3 7f99e50f8000 7f99e50f9000 rw p 00015000 08 21 1447099 usr local lib python2.7 dist packages cv2 .libs libz a147dcb0.so.1.2.3 7f99e50f9000 7f99e50fa000 p 00000000 00 00 0 7f99e50fa000 7f99e58fa000 rw p 00000000 00 00 0 7f99e58fa000 7f99e58fb000 p 00000000 00 00 0 7f99e58fb000 7f99e60fb000 rw p 00000000 00 00 0 7f99e60fb000 7f99e60fc000 p 00000000 00 00 0 7f99e60fc000 7f99e68fc000 rw p 00000000 00 00 0 7f99e68fc000 7f99e6900000 r xp 00000000 08 21 62787987 usr lib python2.7 lib dynload termios.x86 64 linux gnu.so 7f99e6900000 7f99e6aff000 p 00004000 08 21 62787987 usr lib python2.7 lib dynload termios.x86 64 linux gnu.so 7f99e6aff000 7f99e6b00000 r p 00003000 08 21 62787987 usr lib python2.7 lib dynload termios.x86 64 linux gnu.so 7f99e6b00000 7f99e6b02000 rw p 00004000 08 21 62787987 usr lib python2.7 lib dynload termios.x86 64 linux gnu.so 7f99e6b02000 7f99e6b82000 rw p 00000000 00 00 0 7f99e6c02000 7f99e6dc2000 rw p 00000000 00 00 0 7f99e6dc2000 7f99e6de8000 r xp 00000000 08 21 62529457 lib x86 64 linux gnu libexpat.so.1.6.0 7f99e6de8000 7f99e6fe8000 p 00026000 08 21 62529457 lib x86 64 linux gnu libexpat.so.1.6.0 7f99e6fe8000 7f99e6fea000 r p 00026000 08 21 62529457 lib x86 64 linux gnu libexpat.so.1.6.0 7f99e6fea000 7f99e6feb000 rw p 00028000 08 21 62529457 lib x86 64 linux gnu libexpat.so.1.6.0 7f99e6feb000 7f99e6ffa000 r xp 00000000 08 21 62787984 usr lib python2.7 lib dynload pyexpat.x86 64 linux gnu.so 7f99e6ffa000 7f99e71f9000 p 0000f000 08 21 62787984 usr lib python2.7 lib dynload pyexpat.x86 64 linux gnu.so 7f99e71f9000 7f99e71fa000 r p 0000e000 08 21 62787984 usr lib python2.7 lib dynload pyexpat.x86 64 linux gnu.so 7f99e71fa000 7f99e71fc000 rw p 0000f000 08 21 62787984 usr lib python2.7 lib dynload pyexpat.x86 64 linux gnu.so 7f99e71fc000 7f99e727c000 rw p 00000000 00 00 0 7f99e727c000 7f99e73fc000 rw p 00000000 00 00 0 7f99e73fc000 7f99e7408000 r xp 00000000 08 21 62787966 usr lib python2.7 lib dynload json.x86 64 linux gnu.so 7f99e7408000 7f99e7607000 p 0000c000 08 21 62787966 usr lib python2.7 lib dynload json.x86 64 linux gnu.so 7f99e7607000 7f99e7608000 r p 0000b000 08 21 62787966 usr lib python2.7 lib dynload json.x86 64 linux gnu.so 7f99e7608000 7f99e7609000 rw p 0000c000 08 21 62787966 usr lib python2.7 lib dynload json.x86 64 linux gnu.so 7f99e7609000 7f99e7689000 rw p 00000000 00 00 0 7f99e7689000 7f99e76e7000 r xp 00000000 08 21 45488002 lib x86 64 linux gnu libssl.so.1.0.0 7f99e76e7000 7f99e78e7000 p 0005e000 08 21 45488002 lib x86 64 linux gnu libssl.so.1.0.0 7f99e78e7000 7f99e78eb000 r p 0005e000 08 21 45488002 lib x86 64 linux gnu libssl.so.1.0.0 7f99e78eb000 7f99e78f2000 rw p 00062000 08 21 45488002 lib x86 64 linux gnu libssl.so.1.0.0 7f99e78f2000 7f99e7907000 r xp 00000000 08 21 62787971 usr lib python2.7 lib dynload ssl.x86 64 linux gnu.so 7f99e7907000 7f99e7b06000 p 00015000 08 21 62787971 usr lib python2.7 lib dynload ssl.x86 64 linux gnu.so 7f99e7b06000 7f99e7b07000 r p 00014000 08 21 62787971 usr lib python2.7 lib dynload ssl.x86 64 linux gnu.so 7f99e7b07000 7f99e7b0b000 rw p 00015000 08 21 62787971 usr lib python2.7 lib dynload ssl.x86 64 linux gnu.so 7f99e7b0b000 7f99e7ccb000 rw p 00000000 00 00 0 7f99e7ccc000 7f99e7d0c000 rw p 00000000 00 00 0 7f99e7d0c000 7f99e7d10000 r xp 00000000 08 21 1442706 usr local lib python2.7 dist packages posixsubprocess32.so 7f99e7d10000 7f99e7f0f000 p 00004000 08 21 1442706 usr local lib python2.7 dist packages posixsubprocess32.so 7f99e7f0f000 7f99e7f10000 r p 00003000 08 21 1442706 usr local lib python2.7 dist packages posixsubprocess32.so 7f99e7f10000 7f99e7f11000 rw p 00004000 08 21 1442706 usr local lib python2.7 dist packages posixsubprocess32.so 7f99e7f11000 7f99e8051000 rw p 00000000 00 00 0 7f99e8051000 7f99e8102000 r xp 00000000 08 21 3018979 usr local lib python2.7 dist packages numpy random mtrand.so 7f99e8102000 7f99e8301000 p 000b1000 08 21 3018979 usr local lib python2.7 dist packages numpy random mtrand.so 7f99e8301000 7f99e8326000 rw p 000b0000 08 21 3018979 usr local lib python2.7 dist packages numpy random mtrand.so 7f99e8326000 7f99e8368000 rw p 00000000 00 00 0 7f99e8368000 7f99e8371000 r xp 00000000 08 21 3019001 usr local lib python2.7 dist packages numpy fft fftpack lite.so 7f99e8371000 7f99e8571000 p 00009000 08 21 3019001 usr local lib python2.7 dist packages numpy fft fftpack lite.so 7f99e8571000 7f99e8572000 rw p 00009000 08 21 3019001 usr local lib python2.7 dist packages numpy fft fftpack lite.so 7f99e8572000 7f99e85f2000 rw p 00000000 00 00 0 7f99e85f2000 7f99e85f3000 r xp 00000000 08 21 62787978 usr lib python2.7 lib dynload future builtins.x86 64 linux gnu.so 7f99e85f3000 7f99e87f2000 p 00001000 08 21 62787978 usr lib python2.7 lib dynload future builtins.x86 64 linux gnu.so 7f99e87f2000 7f99e87f3000 r p 00000000 08 21 62787978 usr lib python2.7 lib dynload future builtins.x86 64 linux gnu.so 7f99e87f3000 7f99e87f4000 rw p 00001000 08 21 62787978 usr lib python2.7 lib dynload future builtins.x86 64 linux gnu.so 7f99e87f4000 7f99e8874000 rw p 00000000 00 00 0 7f99e8874000 7f99e889f000 r xp 00000000 08 21 3018415 usr local lib python2.7 dist packages numpy linalg umath linalg.so 7f99e889f000 7f99e8a9e000 p 0002b000 08 21 3018415 usr local lib python2.7 dist packages numpy linalg umath linalg.so 7f99e8a9e000 7f99e8aa0000 rw p 0002a000 08 21 3018415 usr local lib python2.7 dist packages numpy linalg umath linalg.so 7f99e8aa0000 7f99e8aa3000 rw p 000d2000 08 21 3018415 usr local lib python2.7 dist packages numpy linalg umath linalg.so 7f99e8aa3000 7f99e8aa7000 r xp 00000000 08 21 3018416 usr local lib python2.7 dist packages numpy linalg lapack lite.so 7f99e8aa7000 7f99e8ca7000 p 00004000 08 21 3018416 usr local lib python2.7 dist packages numpy linalg lapack lite.so 7f99e8ca7000 7f99e8ca8000 rw p 00004000 08 21 3018416 usr local lib python2.7 dist packages numpy linalg lapack lite.so 7f99e8ca8000 7f99e8caa000 rw p 00019000 08 21 3018416 usr local lib python2.7 dist packages numpy linalg lapack lite.so 7f99e8caa000 7f99e8cea000 rw p 00000000 00 00 0 7f99e8cea000 7f99e8d09000 r xp 00000000 08 21 3018453 usr local lib python2.7 dist packages numpy core multiarray tests.so 7f99e8d09000 7f99e8f08000 p 0001f000 08 21 3018453 usr local lib python2.7 dist packages numpy core multiarray tests.so 7f99e8f08000 7f99e8f0a000 rw p 0001e000 08 21 3018453 usr local lib python2.7 dist packages numpy core multiarray tests.so 7f99e8f0a000 7f9a0cfca000 rw p 00000000 00 00 0 7f9a0cfca000 7f9a0cfd1000 r xp 00000000 08 21 45746691 usr lib x86 64 linux gnu libffi.so.6.0.4 7f9a0cfd1000 7f9a0d1d0000 p 00007000 08 21 45746691 usr lib x86 64 linux gnu libffi.so.6.0.4 7f9a0d1d0000 7f9a0d1d1000 r p 00006000 08 21 45746691 usr lib x86 64 linux gnu libffi.so.6.0.4 7f9a0d1d1000 7f9a0d1d2000 rw p 00007000 08 21 45746691 usr lib x86 64 linux gnu libffi.so.6.0.4 7f9a0d1d2000 7f9a0d1f0000 r xp 00000000 08 21 62787959 usr lib python2.7 lib dynload ctypes.x86 64 linux gnu.so 7f9a0d1f0000 7f9a0d3ef000 p 0001e000 08 21 62787959 usr lib python2.7 lib dynload ctypes.x86 64 linux gnu.so 7f9a0d3ef000 7f9a0d3f0000 r p 0001d000 08 21 62787959 usr lib python2.7 lib dynload ctypes.x86 64 linux gnu.so 7f9a0d3f0000 7f9a0d3f4000 rw p 0001e000 08 21 62787959 usr lib python2.7 lib dynload ctypes.x86 64 linux gnu.so 7f9a0d3f4000 7f9a0d434000 rw p 00000000 00 00 0 7f9a0d434000 7f9a0d435000 p 00000000 00 00 0 7f9a0d435000 7f9a0dc35000 rw p 00000000 00 00 0 7f9a0de25000 7f9a0e4a8000 rw p 00000000 00 00 0 7f9a0e4a8000 7f9a0e4ac000 r xp 00000000 08 21 1447721 usr local lib python2.7 dist packages scandir.so 7f9a0e4ac000 7f9a0e6ab000 p 00004000 08 21 1447721 usr local lib python2.7 dist packages scandir.so 7f9a0e6ab000 7f9a0e6ac000 r p 00003000 08 21 1447721 usr local lib python2.7 dist packages scandir.so 7f9a0e6ac000 7f9a0e6ad000 rw p 00004000 08 21 1447721 usr local lib python2.7 dist packages scandir.so 7f9a0e6ad000 7f9a0e6df000 r xp 00000000 08 21 2232259 usr local lib python2.7 dist packages pil .libs liblzma 6cd627ed.so.5.2.4 7f9a0e6df000 7f9a0e8df000 p 00032000 08 21 2232259 usr local lib python2.7 dist packages pil .libs liblzma 6cd627ed.so.5.2.4 7f9a0e8df000 7f9a0e8e0000 rw p 00032000 08 21 2232259 usr local lib python2.7 dist packages pil .libs liblzma 6cd627ed.so.5.2.4 7f9a0e8e0000 7f9a0e8e1000 rw p 00034000 08 21 2232259 usr local lib python2.7 dist packages pil .libs liblzma 6cd627ed.so.5.2.4 7f9a0e8e1000 7f9a0e970000 r xp 00000000 08 21 2232258 usr local lib python2.7 dist packages pil .libs libtiff 8267adfe.so.5.4.0 7f9a0e970000 7f9a0eb70000 p 0008f000 08 21 2232258 usr local lib python2.7 dist packages pil .libs libtiff 8267adfe.so.5.4.0 7f9a0eb70000 7f9a0eb74000 rw p 0008f000 08 21 2232258 usr local lib python2.7 dist packages pil .libs libtiff 8267adfe.so.5.4.0 7f9a0eb74000 7f9a0eb7c000 rw p 00094000 08 21 2232258 usr local lib python2.7 dist packages pil .libs libtiff 8267adfe.so.5.4.0 7f9a0eb7c000 7f9a0ebc3000 r xp 00000000 08 21 2232261 usr local lib python2.7 dist packages pil .libs libopenjp2 e366d6b0.so.2.1.0 7f9a0ebc3000 7f9a0edc3000 p 00047000 08 21 2232261 usr local lib python2.7 dist packages pil .libs libopenjp2 e366d6b0.so.2.1.0 7f9a0edc3000 7f9a0edc5000 rw p 00047000 08 21 2232261 usr local lib python2.7 dist packages pil .libs libopenjp2 e366d6b0.so.2.1.0 7f9a0edc5000 7f9a0edc7000 rw p 0004a000 08 21 2232261 usr local lib python2.7 dist packages pil .libs libopenjp2 e366d6b0.so.2.1.0 7f9a0edc7000 7f9a0ee18000 r xp 00000000 08 21 2232256 usr local lib python2.7 dist packages pil .libs libjpeg 3fe7dfc0.so.9.3.0 7f9a0ee18000 7f9a0f018000 p 00051000 08 21 2232256 usr local lib python2.7 dist packages pil .libs libjpeg 3fe7dfc0.so.9.3.0 7f9a0f018000 7f9a0f019000 rw p 00051000 08 21 2232256 usr local lib python2.7 dist packages pil .libs libjpeg 3fe7dfc0.so.9.3.0 7f9a0f019000 7f9a0f01b000 rw p 00053000 08 21 2232256 usr local lib python2.7 dist packages pil .libs libjpeg 3fe7dfc0.so.9.3.0 7f9a0f01b000 7f9a0f099000 r xp 00000000 08 21 2232104 usr local lib python2.7 dist packages pil imaging.so 7f9a0f099000 7f9a0f299000 p 0007e000 08 21 2232104 usr local lib python2.7 dist packages pil imaging.so 7f9a0f299000 7f9a0f2af000 rw p 0007e000 08 21 2232104 usr local lib python2.7 dist packages pil imaging.so 7f9a0f2af000 7f9a0f2b5000 r xp 00000000 08 21 62787969 usr lib python2.7 lib dynload multiprocessing.x86 64 linux gnu.so 7f9a0f2b5000 7f9a0f4b4000 p 00006000 08 21 62787969 usr lib python2.7 lib dynload multiprocessing.x86 64 linux gnu.so 7f9a0f4b4000 7f9a0f4b5000 r p 00005000 08 21 62787969 usr lib python2.7 lib dynload multiprocessing.x86 64 linux gnu.so 7f9a0f4b5000 7f9a0f4b6000 rw p 00006000 08 21 62787969 usr lib python2.7 lib dynload multiprocessing.x86 64 linux gnu.so 7f9a0f4b6000 7f9a0f4b8000 r xp 00000000 08 21 62790954 usr lib x86 64 linux gnu libxcb shm.so.0.0.0 7f9a0f4b8000 7f9a0f6b8000 p 00002000 08 21 62790954 usr lib x86 64 linux gnu libxcb shm.so.0.0.0 7f9a0f6b8000 7f9a0f6b9000 r p 00002000 08 21 62790954 usr lib x86 64 linux gnu libxcb shm.so.0.0.0 7f9a0f6b9000 7f9a0f6ba000 rw p 00003000 08 21 62790954 usr lib x86 64 linux gnu libxcb shm.so.0.0.0 7f9a0f6ba000 7f9a0f759000 r xp 00000000 08 21 62790841 usr lib x86 64 linux gnu libpixman 1.so.0.33.6 7f9a0f759000 7f9a0f959000 p 0009f000 08 21 62790841 usr lib x86 64 linux gnu libpixman 1.so.0.33.6 7f9a0f959000 7f9a0f961000 r p 0009f000 08 21 62790841 usr lib x86 64 linux gnu libpixman 1.so.0.33.6 7f9a0f961000 7f9a0f962000 rw p 000a7000 08 21 62790841 usr lib x86 64 linux gnu libpixman 1.so.0.33.6 7f9a0f962000 7f9a0fa06000 r xp 00000000 08 21 62790611 usr lib x86 64 linux gnu libfreetype.so.6.12.1 7f9a0fa06000 7f9a0fc05000 p 000a4000 08 21 62790611 usr lib x86 64 linux gnu libfreetype.so.6.12.1 7f9a0fc05000 7f9a0fc0b000 r p 000a3000 08 21 62790611 usr lib x86 64 linux gnu libfreetype.so.6.12.1 7f9a0fc0b000 7f9a0fc0c000 rw p 000a9000 08 21 62790611 usr lib x86 64 linux gnu libfreetype.so.6.12.1 7f9a0fc0c000 7f9a0fc1b000 r xp 00000000 08 21 38015882 lib x86 64 linux gnu libbz2.so.1.0.4 7f9a0fc1b000 7f9a0fe1a000 p 0000f000 08 21 38015882 lib x86 64 linux gnu libbz2.so.1.0.4 7f9a0fe1a000 7f9a0fe1b000 r p 0000e000 08 21 38015882 lib x86 64 linux gnu libbz2.so.1.0.4 7f9a0fe1b000 7f9a0fe1c000 rw p 0000f000 08 21 38015882 lib x86 64 linux gnu libbz2.so.1.0.4 7f9a0fe1c000 7f9a0ff3f000 r xp 00000000 08 21 45746695 usr lib x86 64 linux gnu libgnutls.so.30.6.2 7f9a0ff3f000 7f9a1013e000 p 00123000 08 21 45746695 usr lib x86 64 linux gnu libgnutls.so.30.6.2 7f9a1013e000 7f9a10149000 r p 00122000 08 21 45746695 usr lib x86 64 linux gnu libgnutls.so.30.6.2 7f9a10149000 7f9a1014b000 rw p 0012d000 08 21 45746695 usr lib x86 64 linux gnu libgnutls.so.30.6.2 7f9a1014b000 7f9a1014c000 rw p 00000000 00 00 0 7f9a1014c000 7f9a10193000 r xp 00000000 08 21 62790559 usr lib x86 64 linux gnu libbluray.so.1.9.2 7f9a10193000 7f9a10392000 p 00047000 08 21 62790559 usr lib x86 64 linux gnu libbluray.so.1.9.2 7f9a10392000 7f9a10394000 r p 00046000 08 21 62790559 usr lib x86 64 linux gnu libbluray.so.1.9.2 7f9a10394000 7f9a10395000 rw p 00048000 08 21 62790559 usr lib x86 64 linux gnu libbluray.so.1.9.2 7f9a10395000 7f9a103df000 r xp 00000000 08 21 62790639 usr lib x86 64 linux gnu libgme.so.0.6.0 7f9a103df000 7f9a105df000 p 0004a000 08 21 62790639 usr lib x86 64 linux gnu libgme.so.0.6.0 7f9a105df000 7f9a105e2000 r p 0004a000 08 21 62790639 usr lib x86 64 linux gnu libgme.so.0.6.0 7f9a105e2000 7f9a105e3000 rw p 0004d000 08 21 62790639 usr lib x86 64 linux gnu libgme.so.0.6.0 7f9a105e3000 7f9a1062d000 r xp 00000000 08 21 62790711 usr lib x86 64 linux gnu libmodplug.so.1.0.0 7f9a1062d000 7f9a1082d000 p 0004a000 08 21 62790711 usr lib x86 64 linux gnu libmodplug.so.1.0.0 7f9a1082d000 7f9a1082e000 r p 0004a000 08 21 62790711 usr lib x86 64 linux gnu libmodplug.so.1.0.0 7f9a1082e000 7f9a1082f000 rw p 0004b000 08 21 62790711 usr lib x86 64 linux gnu libmodplug.so.1.0.0 7f9a1082f000 7f9a1096e000 rw p 00000000 00 00 0 7f9a1096e000 7f9a10989000 r xp 00000000 08 21 45746731 usr lib x86 64 linux gnu librtmp.so.1 7f9a10989000 7f9a10b88000 p 0001b000 08 21 45746731 usr lib x86 64 linux gnu librtmp.so.1 7f9a10b88000 7f9a10b89000 r p 0001a000 08 21 45746731 usr lib x86 64 linux gnu librtmp.so.1 7f9a10b89000 7f9a10b8a000 rw p 0001b000 08 21 45746731 usr lib x86 64 linux gnu librtmp.so.1 7f9a10b8a000 7f9a10bd2000 r xp 00000000 08 21 62790873 usr lib x86 64 linux gnu libssh gcrypt.so.4.4.1 7f9a10bd2000 7f9a10dd1000 p 00048000 08 21 62790873 usr lib x86 64 linux gnu libssh gcrypt.so.4.4.1 7f9a10dd1000 7f9a10dd2000 r p 00047000 08 21 62790873 usr lib x86 64 linux gnu libssh gcrypt.so.4.4.1 7f9a10dd2000 7f9a10dd3000 rw p 00048000 08 21 62790873 usr lib x86 64 linux gnu libssh gcrypt.so.4.4.1 7f9a10dd3000 7f9a10ded000 r xp 00000000 08 21 62790573 usr lib x86 64 linux gnu libcrystalhd.so.3.6 7f9a10ded000 7f9a10fec000 p 0001a000 08 21 62790573 usr lib x86 64 linux gnu libcrystalhd.so.3.6 7f9a10fec000 7f9a10fed000 r p 00019000 08 21 62790573 usr lib x86 64 linux gnu libcrystalhd.so.3.6 7f9a10fed000 7f9a10fee000 rw p 0001a000 08 21 62790573 usr lib x86 64 linux gnu libcrystalhd.so.3.6 7f9a10fee000 7f9a10ffb000 r xp 00000000 08 21 62790653 usr lib x86 64 linux gnu libgsm.so.1.0.12 7f9a10ffb000 7f9a111fa000 p 0000d000 08 21 62790653 usr lib x86 64 linux gnu libgsm.so.1.0.12 7f9a111fa000 7f9a111fb000 r p 0000c000 08 21 62790653 usr lib x86 64 linux gnu libgsm.so.1.0.12 7f9a111fb000 7f9a111fc000 rw p 0000d000 08 21 62790653 usr lib x86 64 linux gnu libgsm.so.1.0.12 7f9a111fc000 7f9a11241000 r xp 00000000 08 21 62790713 usr lib x86 64 linux gnu libmp3lame.so.0.0.0 7f9a11241000 7f9a11441000 p 00045000 08 21 62790713 usr lib x86 64 linux gnu libmp3lame.so.0.0.0 7f9a11441000 7f9a11442000 r p 00045000 08 21 62790713 usr lib x86 64 linux gnu libmp3lame.so.0.0.0 7f9a11442000 7f9a11443000 rw p 00046000 08 21 62790713 usr lib x86 64 linux gnu libmp3lame.so.0.0.0 7f9a11443000 7f9a11471000 rw p 00000000 00 00 0 7f9a11471000 7f9a11493000 r xp 00000000 08 21 62790792 usr lib x86 64 linux gnu libopenjpeg.so.1.5.2 7f9a11493000 7f9a11692000 p 00022000 08 21 62790792 usr lib x86 64 linux gnu libopenjpeg.so.1.5.2 7f9a11692000 7f9a11693000 r p 00021000 08 21 62790792 usr lib x86 64 linux gnu libopenjpeg.so.1.5.2 7f9a11693000 7f9a11694000 rw p 00022000 08 21 62790792 usr lib x86 64 linux gnu libopenjpeg.so.1.5.2 7f9a11694000 7f9a116dd000 r xp 00000000 08 21 62790797 usr lib x86 64 linux gnu libopus.so.0.5.2 7f9a116dd000 7f9a118dc000 p 00049000 08 21 62790797 usr lib x86 64 linux gnu libopus.so.0.5.2 7f9a118dc000 7f9a118dd000 r p 00048000 08 21 62790797 usr lib x86 64 linux gnu libopus.so.0.5.2 7f9a118dd000 7f9a118de000 rw p 00049000 08 21 62790797 usr lib x86 64 linux gnu libopus.so.0.5.2 7f9a118de000 7f9a119af000 r xp 00000000 08 21 62790861 usr lib x86 64 linux gnu libschroedinger 1.0.so.0.11.0 7f9a119af000 7f9a11baf000 p 000d1000 08 21 62790861 usr lib x86 64 linux gnu libschroedinger 1.0.so.0.11.0 7f9a11baf000 7f9a11bb1000 r p 000d1000 08 21 62790861 usr lib x86 64 linux gnu libschroedinger 1.0.so.0.11.0 7f9a11bb1000 7f9a11bb2000 rw p 000d3000 08 21 62790861 usr lib x86 64 linux gnu libschroedinger 1.0.so.0.11.0 7f9a11bb2000 7f9a11bb3000 rw p 00000000 00 00 0 7f9a11bb3000 7f9a11bbf000 r xp 00000000 08 21 62790865 usr lib x86 64 linux gnu libshine.so.3.0.1 7f9a11bbf000 7f9a11dbe000 p 0000c000 08 21 62790865 usr lib x86 64 linux gnu libshine.so.3.0.1 7f9a11dbe000 7f9a11dbf000 r p 0000b000 08 21 62790865 usr lib x86 64 linux gnu libshine.so.3.0.1 7f9a11dbf000 7f9a11dc0000 rw p 0000c000 08 21 62790865 usr lib x86 64 linux gnu libshine.so.3.0.1 7f9a11dc0000 7f9a11dc7000 r xp 00000000 08 21 62790867 usr lib x86 64 linux gnu libsnappy.so.1.3.0 7f9a11dc7000 7f9a11fc6000 p 00007000 08 21 62790867 usr lib x86 64 linux gnu libsnappy.so.1.3.0 7f9a11fc6000 7f9a11fc7000 r p 00006000 08 21 62790867 usr lib x86 64 linux gnu libsnappy.so.1.3.0 7f9a11fc7000 7f9a11fc8000 rw p 00007000 08 21 62790867 usr lib x86 64 linux gnu libsnappy.so.1.3.0 7f9a11fc8000 7f9a159f9000 r xp 00000000 08 21 50074696 usr local cuda 9.0 targets x86 64 linux lib libcublas.so.9.0.480 7f9a159f9000 7f9a15bf9000 p 03a31000 08 21 50074696 usr local cuda 9.0 targets x86 64 linux lib libcublas.so.9.0.480 7f9a15bf9000 7f9a15c35000 rw p 03a31000 08 21 50074696 usr local cuda 9.0 targets x86 64 linux lib libcublas.so.9.0.480 7f9a15c35000 7f9a17c45000 rw p 00000000 00 00 0 7f9a17dd4000 7f9a17deb000 r xp 00000000 08 21 62790871 usr lib x86 64 linux gnu libspeex.so.1.5.0 7f9a17deb000 7f9a17feb000 p 00017000 08 21 62790871 usr lib x86 64 linux gnu libspeex.so.1.5.0 7f9a17feb000 7f9a17fec000 r p 00017000 08 21 62790871 usr lib x86 64 linux gnu libspeex.so.1.5.0 7f9a17fec000 7f9a17fed000 rw p 00018000 08 21 62790871 usr lib x86 64 linux gnu libspeex.so.1.5.0 7f9a17fed000 7f9a18006000 r xp 00000000 08 21 62790896 usr lib x86 64 linux gnu libtheoradec.so.1.1.4 7f9a18006000 7f9a18205000 p 00019000 08 21 62790896 usr lib x86 64 linux gnu libtheoradec.so.1.1.4 7f9a18205000 7f9a18206000 r p 00018000 08 21 62790896 usr lib x86 64 linux gnu libtheoradec.so.1.1.4 7f9a18206000 7f9a18207000 rw p 00019000 08 21 62790896 usr lib x86 64 linux gnu libtheoradec.so.1.1.4 7f9a18207000 7f9a18245000 r xp 00000000 08 21 62790898 usr lib x86 64 linux gnu libtheoraenc.so.1.1.2 7f9a18245000 7f9a18444000 p 0003e000 08 21 62790898 usr lib x86 64 linux gnu libtheoraenc.so.1.1.2 7f9a18444000 7f9a18445000 r p 0003d000 08 21 62790898 usr lib x86 64 linux gnu libtheoraenc.so.1.1.2 7f9a18445000 7f9a18446000 rw p 0003e000 08 21 62790898 usr lib x86 64 linux gnu libtheoraenc.so.1.1.2 7f9a18446000 7f9a3a446000 rw p 00000000 00 00 0 7f9a3a550000 7f9a3a56e000 r xp 00000000 08 21 62790909 usr lib x86 64 linux gnu libtwolame.so.0.0.0 7f9a3a56e000 7f9a3a76d000 p 0001e000 08 21 62790909 usr lib x86 64 linux gnu libtwolame.so.0.0.0 7f9a3a76d000 7f9a3a76e000 r p 0001d000 08 21 62790909 usr lib x86 64 linux gnu libtwolame.so.0.0.0 7f9a3a76e000 7f9a3a76f000 rw p 0001e000 08 21 62790909 usr lib x86 64 linux gnu libtwolame.so.0.0.0 7f9a3a76f000 7f9a3a773000 rw p 00000000 00 00 0 7f9a3a773000 7f9a3a79d000 r xp 00000000 08 21 62790927 usr lib x86 64 linux gnu libvorbis.so.0.4.8 7f9a3a79d000 7f9a3a99c000 p 0002a000 08 21 62790927 usr lib x86 64 linux gnu libvorbis.so.0.4.8 7f9a3a99c000 7f9a3a99d000 r p 00029000 08 21 62790927 usr lib x86 64 linux gnu libvorbis.so.0.4.8 7f9a3a99d000 7f9a3a99e000 rw p 0002a000 08 21 62790927 usr lib x86 64 linux gnu libvorbis.so.0.4.8 7f9a3a99e000 7f9a3aa2b000 r xp 00000000 08 21 62790929 usr lib x86 64 linux gnu libvorbisenc.so.2.0.11 7f9a3aa2b000 7f9a3ac2a000 p 0008d000 08 21 62790929 usr lib x86 64 linux gnu libvorbisenc.so.2.0.11 7f9a3ac2a000 7f9a3ac46000 r p 0008c000 08 21 62790929 usr lib x86 64 linux gnu libvorbisenc.so.2.0.11 7f9a3ac46000 7f9a3ac47000 rw p 000a8000 08 21 62790929 usr lib x86 64 linux gnu libvorbisenc.so.2.0.11 7f9a3ac47000 7f9a3ec47000 rw p 00000000 00 00 0 7f9a3edfb000 7f9a3f01a000 r xp 00000000 08 21 62790932 usr lib x86 64 linux gnu libvpx.so.3.0.0 7f9a3f01a000 7f9a3f219000 p 0021f000 08 21 62790932 usr lib x86 64 linux gnu libvpx.so.3.0.0 7f9a3f219000 7f9a3f21b000 r p 0021e000 08 21 62790932 usr lib x86 64 linux gnu libvpx.so.3.0.0 7f9a3f21b000 7f9a3f21c000 rw p 00220000 08 21 62790932 usr lib x86 64 linux gnu libvpx.so.3.0.0 7f9a3f21c000 7f9a3f21f000 rw p 00000000 00 00 0 7f9a3f21f000 7f9a3f247000 r xp 00000000 08 21 62790934 usr lib x86 64 linux gnu libwavpack.so.1.1.7 7f9a3f247000 7f9a3f446000 p 00028000 08 21 62790934 usr lib x86 64 linux gnu libwavpack.so.1.1.7 7f9a3f446000 7f9a3f447000 r p 00027000 08 21 62790934 usr lib x86 64 linux gnu libwavpack.so.1.1.7 7f9a3f447000 7f9a3f448000 rw p 00028000 08 21 62790934 usr lib x86 64 linux gnu libwavpack.so.1.1.7 7f9a3f448000 7f9a41448000 rw p 00000000 00 00 0 7f9a4148d000 7f9a414e6000 r xp 00000000 08 21 62790936 usr lib x86 64 linux gnu libwebp.so.5.0.4 7f9a414e6000 7f9a416e6000 p 00059000 08 21 62790936 usr lib x86 64 linux gnu libwebp.so.5.0.4 7f9a416e6000 7f9a416e7000 r p 00059000 08 21 62790936 usr lib x86 64 linux gnu libwebp.so.5.0.4 7f9a416e7000 7f9a416e9000 rw p 0005a000 08 21 62790936 usr lib x86 64 linux gnu libwebp.so.5.0.4 7f9a416e9000 7f9a41811000 r xp 00000000 08 21 62790937 usr lib x86 64 linux gnu libx264.so.148 7f9a41811000 7f9a41a10000 p 00128000 08 21 62790937 usr lib x86 64 linux gnu libx264.so.148 7f9a41a10000 7f9a41a11000 r p 00127000 08 21 62790937 usr lib x86 64 linux gnu libx264.so.148 7f9a41a11000 7f9a41a12000 rw p 00128000 08 21 62790937 usr lib x86 64 linux gnu libx264.so.148 7f9a41a12000 7f9a41a8d000 rw p 00000000 00 00 0 7f9a41a8d000 7f9a4249a000 r xp 00000000 08 21 62790938 usr lib x86 64 linux gnu libx265.so.79 7f9a4249a000 7f9a42699000 p 00a0d000 08 21 62790938 usr lib x86 64 linux gnu libx265.so.79 7f9a42699000 7f9a4269c000 r p 00a0c000 08 21 62790938 usr lib x86 64 linux gnu libx265.so.79 7f9a4269c000 7f9a4269f000 rw p 00a0f000 08 21 62790938 usr lib x86 64 linux gnu libx265.so.79 7f9a4269f000 7f9a426ac000 rw p 00000000 00 00 0 7f9a426ac000 7f9a4274d000 r xp 00000000 08 21 62790966 usr lib x86 64 linux gnu libxvidcore.so.4.3 7f9a4274d000 7f9a4294c000 p 000a1000 08 21 62790966 usr lib x86 64 linux gnu libxvidcore.so.4.3 7f9a4294c000 7f9a4294d000 r p 000a0000 08 21 62790966 usr lib x86 64 linux gnu libxvidcore.so.4.3 7f9a4294d000 7f9a42957000 rw p 000a1000 08 21 62790966 usr lib x86 64 linux gnu libxvidcore.so.4.3 7f9a42957000 7f9a429c0000 rw p 00000000 00 00 0 7f9a429c0000 7f9a42a37000 r xp 00000000 08 21 62790972 usr lib x86 64 linux gnu libzvbi.so.0.13.2 7f9a42a37000 7f9a42c36000 p 00077000 08 21 62790972 usr lib x86 64 linux gnu libzvbi.so.0.13.2 7f9a42c36000 7f9a42c3f000 r p 00076000 08 21 62790972 usr lib x86 64 linux gnu libzvbi.so.0.13.2 7f9a42c3f000 7f9a42c4b000 rw p 0007f000 08 21 62790972 usr lib x86 64 linux gnu libzvbi.so.0.13.2 7f9a42c4b000 7f9a44c4b000 rw p 00000000 00 00 0 7f9a44d20000 7f9a44d3b000 r xp 00000000 08 21 62790925 usr lib x86 64 linux gnu libva.so.1.3900.0 7f9a44d3b000 7f9a44f3a000 p 0001b000 08 21 62790925 usr lib x86 64 linux gnu libva.so.1.3900.0 7f9a44f3a000 7f9a44f3b000 r p 0001a000 08 21 62790925 usr lib x86 64 linux gnu libva.so.1.3900.0 7f9a44f3b000 7f9a44f3c000 rw p 0001b000 08 21 62790925 usr lib x86 64 linux gnu libva.so.1.3900.0 7f9a44f3c000 7f9a44f56000 r xp 00000000 08 21 62790879 usr lib x86 64 linux gnu libswresample ffmpeg.so.1.2.101 7f9a44f56000 7f9a45156000 p 0001a000 08 21 62790879 usr lib x86 64 linux gnu libswresample ffmpeg.so.1.2.101 7f9a45156000 7f9a45158000 r p 0001a000 08 21 62790879 usr lib x86 64 linux gnu libswresample ffmpeg.so.1.2.101 7f9a45158000 7f9a45159000 rw p 0001c000 08 21 62790879 usr lib x86 64 linux gnu libswresample ffmpeg.so.1.2.101 7f9a45159000 7f9a45162000 r xp 00000000 08 21 62790918 usr lib x86 64 linux gnu libv4l2.so.0.0.0 7f9a45162000 7f9a45361000 p 00009000 08 21 62790918 usr lib x86 64 linux gnu libv4l2.so.0.0.0 7f9a45361000 7f9a45362000 r p 00008000 08 21 62790918 usr lib x86 64 linux gnu libv4l2.so.0.0.0 7f9a45362000 7f9a45367000 rw p 00009000 08 21 62790918 usr lib x86 64 linux gnu libv4l2.so.0.0.0 7f9a45367000 7f9a4537e000 r xp 00000000 08 21 62529465 lib x86 64 linux gnu libusb 1.0.so.0.1.0 7f9a4537e000 7f9a4557d000 p 00017000 08 21 62529465 lib x86 64 linux gnu libusb 1.0.so.0.1.0 7f9a4557d000 7f9a4557e000 r p 00016000 08 21 62529465 lib x86 64 linux gnu libusb 1.0.so.0.1.0 7f9a4557e000 7f9a4557f000 rw p 00017000 08 21 62529465 lib x86 64 linux gnu libusb 1.0.so.0.1.0 7f9a4557f000 7f9a4558c000 r xp 00000000 08 21 62790859 usr lib x86 64 linux gnu libraw1394.so.11.1.0 7f9a4558c000 7f9a4578c000 p 0000d000 08 21 62790859 usr lib x86 64 linux gnu libraw1394.so.11.1.0 7f9a4578c000 7f9a4578d000 r p 0000d000 08 21 62790859 usr lib x86 64 linux gnu libraw1394.so.11.1.0 7f9a4578d000 7f9a4578e000 rw p 0000e000 08 21 62790859 usr lib x86 64 linux gnu libraw1394.so.11.1.0 7f9a4578e000 7f9a457ac000 r xp 00000000 08 21 62790815 usr lib x86 64 linux gnu libpangox 1.0.so.0.0.0 7f9a457ac000 7f9a459ac000 p 0001e000 08 21 62790815 usr lib x86 64 linux gnu libpangox 1.0.so.0.0.0 7f9a459ac000 7f9a459ad000 r p 0001e000 08 21 62790815 usr lib x86 64 linux gnu libpangox 1.0.so.0.0.0 7f9a459ad000 7f9a459ae000 rw p 0001f000 08 21 62790815 usr lib x86 64 linux gnu libpangox 1.0.so.0.0.0 7f9a459ae000 7f9a459c6000 r xp 00000000 08 21 62790518 usr lib x86 64 linux gnu libxmu.so.6.2.0 7f9a459c6000 7f9a45bc5000 p 00018000 08 21 62790518 usr lib x86 64 linux gnu libxmu.so.6.2.0 7f9a45bc5000 7f9a45bc6000 r p 00017000 08 21 62790518 usr lib x86 64 linux gnu libxmu.so.6.2.0 7f9a45bc6000 7f9a45bc7000 rw p 00018000 08 21 62790518 usr lib x86 64 linux gnu libxmu.so.6.2.0 7f9a45bc7000 7f9a45c34000 r xp 00000000 08 21 62790434 usr lib x86 64 linux gnu libglu.so.1.3.1 7f9a45c34000 7f9a45e34000 p 0006d000 08 21 62790434 usr lib x86 64 linux gnu libglu.so.1.3.1 7f9a45e34000 7f9a45e35000 r p 0006d000 08 21 62790434 usr lib x86 64 linux gnu libglu.so.1.3.1 7f9a45e35000 7f9a45e36000 rw p 0006e000 08 21 62790434 usr lib x86 64 linux gnu libglu.so.1.3.1 7f9a45e36000 7f9a45e38000 r xp 00000000 08 21 62790484 usr lib x86 64 linux gnu libxcomposite.so.1.0.0 7f9a45e38000 7f9a46037000 p 00002000 08 21 62790484 usr lib x86 64 linux gnu libxcomposite.so.1.0.0 7f9a46037000 7f9a46038000 r p 00001000 08 21 62790484 usr lib x86 64 linux gnu libxcomposite.so.1.0.0 7f9a46038000 7f9a46039000 rw p 00002000 08 21 62790484 usr lib x86 64 linux gnu libxcomposite.so.1.0.0 7f9a46039000 7f9a46042000 r xp 00000000 08 21 62790488 usr lib x86 64 linux gnu libxcursor.so.1.0.2 7f9a46042000 7f9a46241000 p 00009000 08 21 62790488 usr lib x86 64 linux gnu libxcursor.so.1.0.2 7f9a46241000 7f9a46242000 r p 00008000 08 21 62790488 usr lib x86 64 linux gnu libxcursor.so.1.0.2 7f9a46242000 7f9a46243000 rw p 00009000 08 21 62790488 usr lib x86 64 linux gnu libxcursor.so.1.0.2 7f9a46243000 7f9a4624d000 r xp 00000000 08 21 62790524 usr lib x86 64 linux gnu libxrandr.so.2.2.0 7f9a4624d000 7f9a4644c000 p 0000a000 08 21 62790524 usr lib x86 64 linux gnu libxrandr.so.2.2.0 7f9a4644c000 7f9a4644d000 r p 00009000 08 21 62790524 usr lib x86 64 linux gnu libxrandr.so.2.2.0 7f9a4644d000 7f9a4644e000 rw p 0000a000 08 21 62790524 usr lib x86 64 linux gnu libxrandr.so.2.2.0 7f9a4644e000 7f9a4844e000 rw p 00000000 00 00 0 7f9a48485000 7f9a48626000 rw p 00000000 00 00 0 7f9a48626000 7f9a48635000 r xp 00000000 08 21 62790512 usr lib x86 64 linux gnu libxi.so.6.1.0 7f9a48635000 7f9a48834000 p 0000f000 08 21 62790512 usr lib x86 64 linux gnu libxi.so.6.1.0 7f9a48834000 7f9a48835000 r p 0000e000 08 21 62790512 usr lib x86 64 linux gnu libxi.so.6.1.0 7f9a48835000 7f9a48836000 rw p 0000f000 08 21 62790512 usr lib x86 64 linux gnu libxi.so.6.1.0 7f9a48836000 7f9a48838000 r xp 00000000 08 21 62790516 usr lib x86 64 linux gnu libxinerama.so.1.0.0 7f9a48838000 7f9a48a37000 p 00002000 08 21 62790516 usr lib x86 64 linux gnu libxinerama.so.1.0.0 7f9a48a37000 7f9a48a38000 r p 00001000 08 21 62790516 usr lib x86 64 linux gnu libxinerama.so.1.0.0 7f9a48a38000 7f9a48a39000 rw p 00002000 08 21 62790516 usr lib x86 64 linux gnu libxinerama.so.1.0.0 7f9a48a39000 7f9a48a76000 r xp 00000000 08 21 62790606 usr lib x86 64 linux gnu libfontconfig.so.1.9.0 7f9a48a76000 7f9a48c75000 p 0003d000 08 21 62790606 usr lib x86 64 linux gnu libfontconfig.so.1.9.0 7f9a48c75000 7f9a48c77000 r p 0003c000 08 21 62790606 usr lib x86 64 linux gnu libfontconfig.so.1.9.0 7f9a48c77000 7f9a48c7c000 rw p 0003e000 08 21 62790606 usr lib x86 64 linux gnu libfontconfig.so.1.9.0 7f9a48c7c000 7f9a48cc5000 r xp 00000000 08 21 62790805 usr lib x86 64 linux gnu libpango 1.0.so.0.3800.1 7f9a48cc5000 7f9a48ec5000 p 00049000 08 21 62790805 usr lib x86 64 linux gnu libpango 1.0.so.0.3800.1 7f9a48ec5000 7f9a48ec7000 r p 00049000 08 21 62790805 usr lib x86 64 linux gnu libpango 1.0.so.0.3800.1 7f9a48ec7000 7f9a48ec8000 rw p 0004b000 08 21 62790805 usr lib x86 64 linux gnu libpango 1.0.so.0.3800.1 7f9a48ec8000 7f9a48edc000 r xp 00000000 08 21 62790813 usr lib x86 64 linux gnu libpangoft2 1.0.so.0.3800.1 7f9a48edc000 7f9a490dc000 p 00014000 08 21 62790813 usr lib x86 64 linux gnu libpangoft2 1.0.so.0.3800.1 7f9a490dc000 7f9a490dd000 r p 00014000 08 21 62790813 usr lib x86 64 linux gnu libpangoft2 1.0.so.0.3800.1 7f9a490dd000 7f9a490de000 rw p 00015000 08 21 62790813 usr lib x86 64 linux gnu libpangoft2 1.0.so.0.3800.1 7f9a490de000 7f9a4925e000 r xp 00000000 08 21 62790631 usr lib x86 64 linux gnu libgio 2.0.so.0.4800.2 7f9a4925e000 7f9a4945e000 p 00180000 08 21 62790631 usr lib x86 64 linux gnu libgio 2.0.so.0.4800.2 7f9a4945e000 7f9a49462000 r p 00180000 08 21 62790631 usr lib x86 64 linux gnu libgio 2.0.so.0.4800.2 7f9a49462000 7f9a49464000 rw p 00184000 08 21 62790631 usr lib x86 64 linux gnu libgio 2.0.so.0.4800.2 7f9a49464000 7f9a49466000 rw p 00000000 00 00 0 7f9a49466000 7f9a49487000 r xp 00000000 08 21 62790620 usr lib x86 64 linux gnu libgdk pixbuf 2.0.so.0.3200.2 7f9a49487000 7f9a49686000 p 00021000 08 21 62790620 usr lib x86 64 linux gnu libgdk pixbuf 2.0.so.0.3200.2 7f9a49686000 7f9a49687000 r p 00020000 08 21 62790620 usr lib x86 64 linux gnu libgdk pixbuf 2.0.so.0.3200.2 7f9a49687000 7f9a49688000 rw p 00021000 08 21 62790620 usr lib x86 64 linux gnu libgdk pixbuf 2.0.so.0.3200.2 7f9a49688000 7f9a49796000 r xp 00000000 08 21 62790571 usr lib x86 64 linux gnu libcairo.so.2.11400.6 7f9a49796000 7f9a49996000 p 0010e000 08 21 62790571 usr lib x86 64 linux gnu libcairo.so.2.11400.6 7f9a49996000 7f9a49999000 r p 0010e000 08 21 62790571 usr lib x86 64 linux gnu libcairo.so.2.11400.6 7f9a49999000 7f9a4999a000 rw p 00111000 08 21 62790571 usr lib x86 64 linux gnu libcairo.so.2.11400.6 7f9a4999a000 7f9a4999c000 rw p 00000000 00 00 0 7f9a4999c000 7f9a499be000 r xp 00000000 08 21 62790535 usr lib x86 64 linux gnu libatk 1.0.so.0.21809.1 7f9a499be000 7f9a49bbd000 p 00022000 08 21 62790535 usr lib x86 64 linux gnu libatk 1.0.so.0.21809.1 7f9a49bbd000 7f9a49bc0000 r p 00021000 08 21 62790535 usr lib x86 64 linux gnu libatk 1.0.so.0.21809.1 7f9a49bc0000 7f9a49bc1000 rw p 00024000 08 21 62790535 usr lib x86 64 linux gnu libatk 1.0.so.0.21809.1 7f9a49bc1000 7f9a4ba25000 r xp 00000000 08 21 62786977 usr lib libopenblasp r0.2.18.so 7f9a4ba25000 7f9a4bc24000 p 01e64000 08 21 62786977 usr lib libopenblasp r0.2.18.so 7f9a4bc24000 7f9a4bc2a000 r p 01e63000 08 21 62786977 usr lib libopenblasp r0.2.18.so 7f9a4bc2a000 7f9a4bc3c000 rw p 01e69000 08 21 62786977 usr lib libopenblasp r0.2.18.so 7f9a4bc3c000 7f9a4dc55000 rw p 00000000 00 00 0 7f9a4dc7a000 7f9a4ddba000 rw p 00000000 00 00 0 7f9a4ddba000 7f9a4ddc6000 r xp 00000000 08 21 62790809 usr lib x86 64 linux gnu libpangocairo 1.0.so.0.3800.1 7f9a4ddc6000 7f9a4dfc5000 p 0000c000 08 21 62790809 usr lib x86 64 linux gnu libpangocairo 1.0.so.0.3800.1 7f9a4dfc5000 7f9a4dfc6000 r p 0000b000 08 21 62790809 usr lib x86 64 linux gnu libpangocairo 1.0.so.0.3800.1 7f9a4dfc6000 7f9a4dfc7000 rw p 0000c000 08 21 62790809 usr lib x86 64 linux gnu libpangocairo 1.0.so.0.3800.1 7f9a4dfc7000 7f9a4dfca000 r xp 00000000 08 21 62790643 usr lib x86 64 linux gnu libgmodule 2.0.so.0.4800.2 7f9a4dfca000 7f9a4e1c9000 p 00003000 08 21 62790643 usr lib x86 64 linux gnu libgmodule 2.0.so.0.4800.2 7f9a4e1c9000 7f9a4e1ca000 r p 00002000 08 21 62790643 usr lib x86 64 linux gnu libgmodule 2.0.so.0.4800.2 7f9a4e1ca000 7f9a4e1cb000 rw p 00003000 08 21 62790643 usr lib x86 64 linux gnu libgmodule 2.0.so.0.4800.2 7f9a4e1cb000 7f9a4e1d0000 r xp 00000000 08 21 62790457 usr lib x86 64 linux gnu libilmthread 2 2.so.12.0.0 7f9a4e1d0000 7f9a4e3d0000 p 00005000 08 21 62790457 usr lib x86 64 linux gnu libilmthread 2 2.so.12.0.0 7f9a4e3d0000 7f9a4e3d1000 r p 00005000 08 21 62790457 usr lib x86 64 linux gnu libilmthread 2 2.so.12.0.0 7f9a4e3d1000 7f9a4e3d2000 rw p 00006000 08 21 62790457 usr lib x86 64 linux gnu libilmthread 2 2.so.12.0.0 7f9a4e3d2000 7f9a4e3ed000 r xp 00000000 08 21 62790443 usr lib x86 64 linux gnu libiex 2 2.so.12.0.0 7f9a4e3ed000 7f9a4e5ec000 p 0001b000 08 21 62790443 usr lib x86 64 linux gnu libiex 2 2.so.12.0.0 7f9a4e5ec000 7f9a4e5ef000 r p 0001a000 08 21 62790443 usr lib x86 64 linux gnu libiex 2 2.so.12.0.0 7f9a4e5ef000 7f9a4e5f0000 rw p 0001d000 08 21 62790443 usr lib x86 64 linux gnu libiex 2 2.so.12.0.0 7f9a4e5f0000 7f9a4e5fb000 r xp 00000000 08 21 62790701 usr lib x86 64 linux gnu libjbig.so.0 7f9a4e5fb000 7f9a4e7fa000 p 0000b000 08 21 62790701 usr lib x86 64 linux gnu libjbig.so.0 7f9a4e7fa000 7f9a4e7fb000 r p 0000a000 08 21 62790701 usr lib x86 64 linux gnu libjbig.so.0 7f9a4e7fb000 7f9a4e7fe000 rw p 0000b000 08 21 62790701 usr lib x86 64 linux gnu libjbig.so.0 7f9a4e7fe000 7f9a4e81f000 r xp 00000000 08 21 38015914 lib x86 64 linux gnu liblzma.so.5.0.0 7f9a4e81f000 7f9a4ea1e000 p 00021000 08 21 38015914 lib x86 64 linux gnu liblzma.so.5.0.0 7f9a4ea1e000 7f9a4ea1f000 r p 00020000 08 21 38015914 lib x86 64 linux gnu liblzma.so.5.0.0 7f9a4ea1f000 7f9a4ea20000 rw p 00021000 08 21 38015914 lib x86 64 linux gnu liblzma.so.5.0.0 7f9a4ea20000 7f9a4ea30000 r xp 00000000 08 21 62790582 usr lib x86 64 linux gnu libdrm.so.2.4.0 7f9a4ea30000 7f9a4ec30000 p 00010000 08 21 62790582 usr lib x86 64 linux gnu libdrm.so.2.4.0 7f9a4ec30000 7f9a4ec31000 r p 00010000 08 21 62790582 usr lib x86 64 linux gnu libdrm.so.2.4.0 7f9a4ec31000 7f9a4ec32000 rw p 00011000 08 21 62790582 usr lib x86 64 linux gnu libdrm.so.2.4.0 7f9a4ec32000 7f9a4ec37000 r xp 00000000 08 21 62790532 usr lib x86 64 linux gnu libxxf86vm.so.1.0.0 7f9a4ec37000 7f9a4ee36000 p 00005000 08 21 62790532 usr lib x86 64 linux gnu libxxf86vm.so.1.0.0 7f9a4ee36000 7f9a4ee37000 r p 00004000 08 21 62790532 usr lib x86 64 linux gnu libxxf86vm.so.1.0.0 7f9a4ee37000 7f9a4ee38000 rw p 00005000 08 21 62790532 usr lib x86 64 linux gnu libxxf86vm.so.1.0.0 7f9a4ee38000 7f9a4ee3c000 r xp 00000000 08 21 62790940 usr lib x86 64 linux gnu libxcb dri2.so.0.0.0 7f9a4ee3c000 7f9a4f03b000 p 00004000 08 21 62790940 usr lib x86 64 linux gnu libxcb dri2.so.0.0.0 7f9a4f03b000 7f9a4f03c000 r p 00003000 08 21 62790940 usr lib x86 64 linux gnu libxcb dri2.so.0.0.0 7f9a4f03c000 7f9a4f03d000 rw p 00004000 08 21 62790940 usr lib x86 64 linux gnu libxcb dri2.so.0.0.0 7f9a4f03d000 7f9a4f054000 r xp 00000000 08 21 62790944 usr lib x86 64 linux gnu libxcb glx.so.0.0.0 7f9a4f054000 7f9a4f253000 p 00017000 08 21 62790944 usr lib x86 64 linux gnu libxcb glx.so.0.0.0 7f9a4f253000 7f9a4f255000 r p 00016000 08 21 62790944 usr lib x86 64 linux gnu libxcb glx.so.0.0.0 7f9a4f255000 7f9a4f256000 rw p 00018000 08 21 62790944 usr lib x86 64 linux gnu libxcb glx.so.0.0.0 7f9a4f256000 7f9a4f257000 r xp 00000000 08 21 62790469 usr lib x86 64 linux gnu libx11 xcb.so.1.0.0 7f9a4f257000 7f9a4f456000 p 00001000 08 21 62790469 usr lib x86 64 linux gnu libx11 xcb.so.1.0.0 7f9a4f456000 7f9a4f457000 r p 00000000 08 21 62790469 usr lib x86 64 linux gnu libx11 xcb.so.1.0.0 7f9a4f457000 7f9a4f458000 rw p 00001000 08 21 62790469 usr lib x86 64 linux gnu libx11 xcb.so.1.0.0 7f9a4f458000 7f9a51458000 rw p 00000000 00 00 0 7f9a5145f000 7f9a5161f000 rw p 00000000 00 00 0 7f9a5161f000 7f9a51624000 r xp 00000000 08 21 62790504 usr lib x86 64 linux gnu libxfixes.so.3.1.0 7f9a51624000 7f9a51823000 p 00005000 08 21 62790504 usr lib x86 64 linux gnu libxfixes.so.3.1.0 7f9a51823000 7f9a51824000 r p 00004000 08 21 62790504 usr lib x86 64 linux gnu libxfixes.so.3.1.0 7f9a51824000 7f9a51825000 rw p 00005000 08 21 62790504 usr lib x86 64 linux gnu libxfixes.so.3.1.0 7f9a51825000 7f9a51827000 r xp 00000000 08 21 62790492 usr lib x86 64 linux gnu libxdamage.so.1.1.0 7f9a51827000 7f9a51a26000 p 00002000 08 21 62790492 usr lib x86 64 linux gnu libxdamage.so.1.1.0 7f9a51a26000 7f9a51a27000 r p 00001000 08 21 62790492 usr lib x86 64 linux gnu libxdamage.so.1.1.0 7f9a51a27000 7f9a51a28000 rw p 00002000 08 21 62790492 usr lib x86 64 linux gnu libxdamage.so.1.1.0 7f9a51a28000 7f9a51a54000 r xp 00000000 08 21 62790635 usr lib x86 64 linux gnu libglapi.so.0.0.0 7f9a51a54000 7f9a51c53000 p 0002c000 08 21 62790635 usr lib x86 64 linux gnu libglapi.so.0.0.0 7f9a51c53000 7f9a51c57000 r p 0002b000 08 21 62790635 usr lib x86 64 linux gnu libglapi.so.0.0.0 7f9a51c57000 7f9a51c58000 rw p 0002f000 08 21 62790635 usr lib x86 64 linux gnu libglapi.so.0.0.0 7f9a51c58000 7f9a57c59000 rw p 00000000 00 00 0 7f9a57c8d000 7f9a57e4d000 rw p 00000000 00 00 0 7f9a57e4d000 7f9a57e4e000 r xp 00000000 08 21 62790964 usr lib x86 64 linux gnu libxshmfence.so.1.0.0 7f9a57e4e000 7f9a5804e000 p 00001000 08 21 62790964 usr lib x86 64 linux gnu libxshmfence.so.1.0.0 7f9a5804e000 7f9a5804f000 r p 00001000 08 21 62790964 usr lib x86 64 linux gnu libxshmfence.so.1.0.0 7f9a5804f000 7f9a58050000 rw p 00002000 08 21 62790964 usr lib x86 64 linux gnu libxshmfence.so.1.0.0 7f9a58050000 7f9a58055000 r xp 00000000 08 21 62790956 usr lib x86 64 linux gnu libxcb sync.so.1.0.0 7f9a58055000 7f9a58255000 p 00005000 08 21 62790956 usr lib x86 64 linux gnu libxcb sync.so.1.0.0 7f9a58255000 7f9a58256000 r p 00005000 08 21 62790956 usr lib x86 64 linux gnu libxcb sync.so.1.0.0 7f9a58256000 7f9a58257000 rw p 00006000 08 21 62790956 usr lib x86 64 linux gnu libxcb sync.so.1.0.0 7f9a58257000 7f9a58259000 r xp 00000000 08 21 62790946 usr lib x86 64 linux gnu libxcb present.so.0.0.0 7f9a58259000 7f9a58458000 p 00002000 08 21 62790946 usr lib x86 64 linux gnu libxcb present.so.0.0.0 7f9a58458000 7f9a58459000 r p 00001000 08 21 62790946 usr lib x86 64 linux gnu libxcb present.so.0.0.0 7f9a58459000 7f9a5845a000 rw p 00002000 08 21 62790946 usr lib x86 64 linux gnu libxcb present.so.0.0.0 7f9a5845a000 7f9a5a45a000 rw p 00000000 00 00 0 7f9a5a48d000 7f9a5a5cd000 rw p 00000000 00 00 0 7f9a5a5cd000 7f9a5a5cf000 r xp 00000000 08 21 62790942 usr lib x86 64 linux gnu libxcb dri3.so.0.0.0 7f9a5a5cf000 7f9a5a7ce000 p 00002000 08 21 62790942 usr lib x86 64 linux gnu libxcb dri3.so.0.0.0 7f9a5a7ce000 7f9a5a7cf000 r p 00001000 08 21 62790942 usr lib x86 64 linux gnu libxcb dri3.so.0.0.0 7f9a5a7cf000 7f9a5a7d0000 rw p 00002000 08 21 62790942 usr lib x86 64 linux gnu libxcb dri3.so.0.0.0 7f9a5a7d0000 7f9a5a80e000 r xp 00000000 08 21 53355505 usr lib x86 64 linux gnu libquadmath.so.0.0.0 7f9a5a80e000 7f9a5aa0d000 p 0003e000 08 21 53355505 usr lib x86 64 linux gnu libquadmath.so.0.0.0 7f9a5aa0d000 7f9a5aa0e000 r p 0003d000 08 21 53355505 usr lib x86 64 linux gnu libquadmath.so.0.0.0 7f9a5aa0e000 7f9a5aa0f000 rw p 0003e000 08 21 53355505 usr lib x86 64 linux gnu libquadmath.so.0.0.0 7f9a5aa0f000 7f9a5aa4c000 r xp 00000000 08 33 1712536 usr lib x86 64 linux gnu libnvidia fatbinaryloader.so.390.48 7f9a5aa4c000 7f9a5ac4b000 p 0003d000 08 33 1712536 usr lib x86 64 linux gnu libnvidia fatbinaryloader.so.390.48 7f9a5ac4b000 7f9a5ac56000 rw p 0003c000 08 33 1712536 usr lib x86 64 linux gnu libnvidia fatbinaryloader.so.390.48 7f9a5ac56000 7f9a5cc5b000 rw p 00000000 00 00 0 7f9a5cc5e000 7f9a5cd9e000 rw p 00000000 00 00 0 7f9a5cd9e000 7f9a5ce23000 r xp 00000000 08 21 62790885 usr lib x86 64 linux gnu libswscale ffmpeg.so.3.1.101 7f9a5ce23000 7f9a5d022000 p 00085000 08 21 62790885 usr lib x86 64 linux gnu libswscale ffmpeg.so.3.1.101 7f9a5d022000 7f9a5d024000 r p 00084000 08 21 62790885 usr lib x86 64 linux gnu libswscale ffmpeg.so.3.1.101 7f9a5d024000 7f9a5d025000 rw p 00086000 08 21 62790885 usr lib x86 64 linux gnu libswscale ffmpeg.so.3.1.101 7f9a5d025000 7f9a5d02d000 rw p 00000000 00 00 0 7f9a5d02d000 7f9a5db5e000 r xp 00000000 08 21 62790543 usr lib x86 64 linux gnu libavcodec ffmpeg.so.56.60.100 7f9a5db5e000 7f9a5dd5d000 p 00b31000 08 21 62790543 usr lib x86 64 linux gnu libavcodec ffmpeg.so.56.60.100 7f9a5dd5d000 7f9a5dd88000 r p 00b30000 08 21 62790543 usr lib x86 64 linux gnu libavcodec ffmpeg.so.56.60.100 7f9a5dd88000 7f9a5ddab000 rw p 00b5b000 08 21 62790543 usr lib x86 64 linux gnu libavcodec ffmpeg.so.56.60.100 7f9a5ddab000 7f9a6045e000 rw p 00000000 00 00 0 7f9a60471000 7f9a604c7000 r xp 00000000 08 21 62790555 usr lib x86 64 linux gnu libavutil ffmpeg.so.54.31.100 7f9a604c7000 7f9a606c6000 p 00056000 08 21 62790555 usr lib x86 64 linux gnu libavutil ffmpeg.so.54.31.100 7f9a606c6000 7f9a606cc000 r p 00055000 08 21 62790555 usr lib x86 64 linux gnu libavutil ffmpeg.so.54.31.100 7f9a606cc000 7f9a606cd000 rw p 0005b000 08 21 62790555 usr lib x86 64 linux gnu libavutil ffmpeg.so.54.31.100 7f9a606cd000 7f9a606e0000 rw p 00000000 00 00 0 7f9a606e0000 7f9a608b6000 r xp 00000000 08 21 62790549 usr lib x86 64 linux gnu libavformat ffmpeg.so.56.40.101 7f9a608b6000 7f9a60ab6000 p 001d6000 08 21 62790549 usr lib x86 64 linux gnu libavformat ffmpeg.so.56.40.101 7f9a60ab6000 7f9a60aca000 r p 001d6000 08 21 62790549 usr lib x86 64 linux gnu libavformat ffmpeg.so.56.40.101 7f9a60aca000 7f9a60adf000 rw p 001ea000 08 21 62790549 usr lib x86 64 linux gnu libavformat ffmpeg.so.56.40.101 7f9a60adf000 7f9a60ae4000 r xp 00000000 08 21 62790916 usr lib x86 64 linux gnu libv4l1.so.0.0.0 7f9a60ae4000 7f9a60ce3000 p 00005000 08 21 62790916 usr lib x86 64 linux gnu libv4l1.so.0.0.0 7f9a60ce3000 7f9a60ce4000 r p 00004000 08 21 62790916 usr lib x86 64 linux gnu libv4l1.so.0.0.0 7f9a60ce4000 7f9a60ce5000 rw p 00005000 08 21 62790916 usr lib x86 64 linux gnu libv4l1.so.0.0.0 7f9a60ce5000 7f9a60d1a000 r xp 00000000 08 21 62790580 usr lib x86 64 linux gnu libdc1394.so.22.1.11 7f9a60d1a000 7f9a60f19000 p 00035000 08 21 62790580 usr lib x86 64 linux gnu libdc1394.so.22.1.11 7f9a60f19000 7f9a60f1a000 r p 00034000 08 21 62790580 usr lib x86 64 linux gnu libdc1394.so.22.1.11 7f9a60f1a000 7f9a60f1b000 rw p 00035000 08 21 62790580 usr lib x86 64 linux gnu libdc1394.so.22.1.11 7f9a60f1b000 7f9a60f5b000 rw p 00000000 00 00 0 7f9a60f5b000 7f9a60fb8000 r xp 00000000 08 21 62790625 usr lib x86 64 linux gnu libgdkglext x11 1.0.so.0.0.0 7f9a60fb8000 7f9a611b7000 p 0005d000 08 21 62790625 usr lib x86 64 linux gnu libgdkglext x11 1.0.so.0.0.0 7f9a611b7000 7f9a611b9000 r p 0005c000 08 21 62790625 usr lib x86 64 linux gnu libgdkglext x11 1.0.so.0.0.0 7f9a611b9000 7f9a611bf000 rw p 0005e000 08 21 62790625 usr lib x86 64 linux gnu libgdkglext x11 1.0.so.0.0.0 7f9a611bf000 7f9a611c2000 r xp 00000000 08 21 62790666 usr lib x86 64 linux gnu libgtkglext x11 1.0.so.0.0.0 7f9a611c2000 7f9a613c1000 p 00003000 08 21 62790666 usr lib x86 64 linux gnu libgtkglext x11 1.0.so.0.0.0 7f9a613c1000 7f9a613c2000 r p 00002000 08 21 62790666 usr lib x86 64 linux gnu libgtkglext x11 1.0.so.0.0.0 7f9a613c2000 7f9a613c3000 rw p 00003000 08 21 62790666 usr lib x86 64 linux gnu libgtkglext x11 1.0.so.0.0.0 7f9a613c3000 7f9a61415000 r xp 00000000 08 21 62790647 usr lib x86 64 linux gnu libgobject 2.0.so.0.4800.2 7f9a61415000 7f9a61614000 p 00052000 08 21 62790647 usr lib x86 64 linux gnu libgobject 2.0.so.0.4800.2 7f9a61614000 7f9a61615000 r p 00051000 08 21 62790647 usr lib x86 64 linux gnu libgobject 2.0.so.0.4800.2 7f9a61615000 7f9a61616000 rw p 00052000 08 21 62790647 usr lib x86 64 linux gnu libgobject 2.0.so.0.4800.2 7f9a61616000 7f9a61a54000 r xp 00000000 08 21 62790661 usr lib x86 64 linux gnu libgtk x11 2.0.so.0.2400.30 7f9a61a54000 7f9a61c53000 p 0043e000 08 21 62790661 usr lib x86 64 linux gnu libgtk x11 2.0.so.0.2400.30 7f9a61c53000 7f9a61c5a000 r p 0043d000 08 21 62790661 usr lib x86 64 linux gnu libgtk x11 2.0.so.0.2400.30 7f9a61c5a000 7f9a61c5e000 rw p 00444000 08 21 62790661 usr lib x86 64 linux gnu libgtk x11 2.0.so.0.2400.30 7f9a61c5e000 7f9a63c61000 rw p 00000000 00 00 0 7f9a63c9f000 7f9a63cdf000 rw p 00000000 00 00 0 7f9a63cdf000 7f9a63d8f000 r xp 00000000 08 21 62790617 usr lib x86 64 linux gnu libgdk x11 2.0.so.0.2400.30 7f9a63d8f000 7f9a63f8e000 p 000b0000 08 21 62790617 usr lib x86 64 linux gnu libgdk x11 2.0.so.0.2400.30 7f9a63f8e000 7f9a63f92000 r p 000af000 08 21 62790617 usr lib x86 64 linux gnu libgdk x11 2.0.so.0.2400.30 7f9a63f92000 7f9a63f94000 rw p 000b3000 08 21 62790617 usr lib x86 64 linux gnu libgdk x11 2.0.so.0.2400.30 7f9a63f94000 7f9a6415e000 r xp 00000000 08 21 62790449 usr lib x86 64 linux gnu libilmimf 2 2.so.22.0.0 7f9a6415e000 7f9a6435d000 p 001ca000 08 21 62790449 usr lib x86 64 linux gnu libilmimf 2 2.so.22.0.0 7f9a6435d000 7f9a64360000 r p 001c9000 08 21 62790449 usr lib x86 64 linux gnu libilmimf 2 2.so.22.0.0 7f9a64360000 7f9a64461000 rw p 001cc000 08 21 62790449 usr lib x86 64 linux gnu libilmimf 2 2.so.22.0.0 7f9a64461000 7f9a66462000 rw p 00000000 00 00 0 7f9a6648e000 7f9a6650e000 rw p 00000000 00 00 0 7f9a6650e000 7f9a66550000 r xp 00000000 08 21 62790437 usr lib x86 64 linux gnu libhalf.so.12.0.0 7f9a66550000 7f9a6674f000 p 00042000 08 21 62790437 usr lib x86 64 linux gnu libhalf.so.12.0.0 7f9a6674f000 7f9a66750000 r p 00041000 08 21 62790437 usr lib x86 64 linux gnu libhalf.so.12.0.0 7f9a66750000 7f9a66751000 rw p 00042000 08 21 62790437 usr lib x86 64 linux gnu libhalf.so.12.0.0 7f9a66751000 7f9a6679b000 r xp 00000000 08 21 62790698 usr lib x86 64 linux gnu libjasper.so.1.0.0 7f9a6679b000 7f9a6699a000 p 0004a000 08 21 62790698 usr lib x86 64 linux gnu libjasper.so.1.0.0 7f9a6699a000 7f9a6699b000 r p 00049000 08 21 62790698 usr lib x86 64 linux gnu libjasper.so.1.0.0 7f9a6699b000 7f9a6699f000 rw p 0004a000 08 21 62790698 usr lib x86 64 linux gnu libjasper.so.1.0.0 7f9a6699f000 7f9a669a6000 rw p 00000000 00 00 0 7f9a669a6000 7f9a66a17000 r xp 00000000 08 21 62790902 usr lib x86 64 linux gnu libtiff.so.5.2.4 7f9a66a17000 7f9a66c17000 p 00071000 08 21 62790902 usr lib x86 64 linux gnu libtiff.so.5.2.4 7f9a66c17000 7f9a66c18000 r p 00071000 08 21 62790902 usr lib x86 64 linux gnu libtiff.so.5.2.4 7f9a66c18000 7f9a66c1b000 rw p 00072000 08 21 62790902 usr lib x86 64 linux gnu libtiff.so.5.2.4 7f9a66c1b000 7f9a68045000 r xp 00000000 08 21 50074739 usr local cuda 9.0 targets x86 64 linux lib libnvrtc.so.9.0.176 7f9a68045000 7f9a68244000 p 0142a000 08 21 50074739 usr local cuda 9.0 targets x86 64 linux lib libnvrtc.so.9.0.176 7f9a68244000 7f9a683ee000 rw p 01429000 08 21 50074739 usr local cuda 9.0 targets x86 64 linux lib libnvrtc.so.9.0.176 7f9a683ee000 7f9a6a466000 rw p 00000000 00 00 0 7f9a6a46c000 7f9a6a5ac000 rw p 00000000 00 00 0 7f9a6a5ac000 7f9a6a5d0000 r xp 00000000 08 21 62529463 lib x86 64 linux gnu libpng12.so.0.54.0 7f9a6a5d0000 7f9a6a7cf000 p 00024000 08 21 62529463 lib x86 64 linux gnu libpng12.so.0.54.0 7f9a6a7cf000 7f9a6a7d0000 r p 00023000 08 21 62529463 lib x86 64 linux gnu libpng12.so.0.54.0 7f9a6a7d0000 7f9a6a7d1000 rw p 00024000 08 21 62529463 lib x86 64 linux gnu libpng12.so.0.54.0 7f9a6a7d1000 7f9a6a828000 r xp 00000000 08 21 62790705 usr lib x86 64 linux gnu libjpeg.so.8.0.2 7f9a6a828000 7f9a6aa28000 p 00057000 08 21 62790705 usr lib x86 64 linux gnu libjpeg.so.8.0.2 7f9a6aa28000 7f9a6aa29000 r p 00057000 08 21 62790705 usr lib x86 64 linux gnu libjpeg.so.8.0.2 7f9a6aa29000 7f9a6aa2a000 rw p 00058000 08 21 62790705 usr lib x86 64 linux gnu libjpeg.so.8.0.2 7f9a6aa2a000 7f9a6aa61000 r xp 00000000 08 21 62790888 usr lib x86 64 linux gnu libtbb.so.2 7f9a6aa61000 7f9a6ac61000 p 00037000 08 21 62790888 usr lib x86 64 linux gnu libtbb.so.2 7f9a6ac61000 7f9a6ac62000 r p 00037000 08 21 62790888 usr lib x86 64 linux gnu libtbb.so.2 7f9a6ac62000 7f9a6ac64000 rw p 00038000 08 21 62790888 usr lib x86 64 linux gnu libtbb.so.2 7f9a6ac64000 7f9a70c67000 rw p 00000000 00 00 0 7f9a70c79000 7f9a70db9000 rw p 00000000 00 00 0 7f9a70db9000 7f9a70e29000 r xp 00000000 08 21 62790976 usr lib x86 64 linux gnu mesa libgl.so.1.2.0 7f9a70e29000 7f9a71028000 p 00070000 08 21 62790976 usr lib x86 64 linux gnu mesa libgl.so.1.2.0 7f9a71028000 7f9a7102b000 r p 0006f000 08 21 62790976 usr lib x86 64 linux gnu mesa libgl.so.1.2.0 7f9a7102b000 7f9a7102c000 rw p 00072000 08 21 62790976 usr lib x86 64 linux gnu mesa libgl.so.1.2.0 7f9a7102c000 7f9a7102d000 rw p 00000000 00 00 0 7f9a7102d000 7f9a71156000 r xp 00000000 08 21 62790627 usr lib x86 64 linux gnu libgfortran.so.3.0.0 7f9a71156000 7f9a71355000 p 00129000 08 21 62790627 usr lib x86 64 linux gnu libgfortran.so.3.0.0 7f9a71355000 7f9a71356000 r p 00128000 08 21 62790627 usr lib x86 64 linux gnu libgfortran.so.3.0.0 7f9a71356000 7f9a71358000 rw p 00129000 08 21 62790627 usr lib x86 64 linux gnu libgfortran.so.3.0.0 7f9a71358000 7f9a71b9a000 r xp 00000000 08 33 1712531 usr lib x86 64 linux gnu libcuda.so.390.48 7f9a71b9a000 7f9a71d99000 p 00842000 08 33 1712531 usr lib x86 64 linux gnu libcuda.so.390.48 7f9a71d99000 7f9a71eea000 rw p 00841000 08 33 1712531 usr lib x86 64 linux gnu libcuda.so.390.48 7f9a71eea000 7f9a71ef8000 rw p 00000000 00 00 0 7f9a71ef8000 7f9a720e8000 r xp 00000000 08 21 62790751 usr lib x86 64 linux gnu libopencv imgproc.so.2.4.9 7f9a720e8000 7f9a722e7000 p 001f0000 08 21 62790751 usr lib x86 64 linux gnu libopencv imgproc.so.2.4.9 7f9a722e7000 7f9a722ed000 r p 001ef000 08 21 62790751 usr lib x86 64 linux gnu libopencv imgproc.so.2.4.9 7f9a722ed000 7f9a722ee000 rw p 001f5000 08 21 62790751 usr lib x86 64 linux gnu libopencv imgproc.so.2.4.9 7f9a722ee000 7f9a72383000 rw p 00000000 00 00 0 7f9a72383000 7f9a723d1000 r xp 00000000 08 21 62790747 usr lib x86 64 linux gnu libopencv highgui.so.2.4.9 7f9a723d1000 7f9a725d0000 p 0004e000 08 21 62790747 usr lib x86 64 linux gnu libopencv highgui.so.2.4.9 7f9a725d0000 7f9a725d3000 r p 0004d000 08 21 62790747 usr lib x86 64 linux gnu libopencv highgui.so.2.4.9 7f9a725d3000 7f9a725d4000 rw p 00050000 08 21 62790747 usr lib x86 64 linux gnu libopencv highgui.so.2.4.9 7f9a725d4000 7f9a727f1000 r xp 00000000 08 21 62790731 usr lib x86 64 linux gnu libopencv core.so.2.4.9 7f9a727f1000 7f9a729f1000 p 0021d000 08 21 62790731 usr lib x86 64 linux gnu libopencv core.so.2.4.9 7f9a729f1000 7f9a729f5000 r p 0021d000 08 21 62790731 usr lib x86 64 linux gnu libopencv core.so.2.4.9 7f9a729f5000 7f9a729fc000 rw p 00221000 08 21 62790731 usr lib x86 64 linux gnu libopencv core.so.2.4.9 7f9a729fc000 7f9a729fe000 rw p 00000000 00 00 0 7f9a729fe000 7f9a72a67000 r xp 00000000 08 21 48113201 usr local cuda 9.0 targets x86 64 linux lib libcudart.so.9.0.176 7f9a72a67000 7f9a72c66000 p 00069000 08 21 48113201 usr local cuda 9.0 targets x86 64 linux lib libcudart.so.9.0.176 7f9a72c66000 7f9a72c6a000 rw p 00068000 08 21 48113201 usr local cuda 9.0 targets x86 64 linux lib libcudart.so.9.0.176 7f9a72c6a000 7f9a72c6b000 rw p 00000000 00 00 0 7f9a72c6b000 7f9a72d5b000 r xp 00000000 08 21 3018825 usr local lib python2.7 dist packages numpy .libs libgfortran ed201abd.so.3.0.0 7f9a72d5b000 7f9a72f5a000 p 000f0000 08 21 3018825 usr local lib python2.7 dist packages numpy .libs libgfortran ed201abd.so.3.0.0 7f9a72f5a000 7f9a72f5c000 rw p 000ef000 08 21 3018825 usr local lib python2.7 dist packages numpy .libs libgfortran ed201abd.so.3.0.0 7f9a72f5c000 7f9a72f5d000 rw p 00000000 00 00 0 7f9a72f5d000 7f9a72f65000 rw p 000f2000 08 21 3018825 usr local lib python2.7 dist packages numpy .libs libgfortran ed201abd.so.3.0.0 7f9a72f65000 7f9a74a65000 r xp 00000000 08 21 3018824 usr local lib python2.7 dist packages numpy .libs libopenblasp r0 382c8f3a.3.5.dev.so 7f9a74a65000 7f9a74c65000 p 01b00000 08 21 3018824 usr local lib python2.7 dist packages numpy .libs libopenblasp r0 382c8f3a.3.5.dev.so 7f9a74c65000 7f9a74c7e000 rw p 01b00000 08 21 3018824 usr local lib python2.7 dist packages numpy .libs libopenblasp r0 382c8f3a.3.5.dev.so 7f9a74c7e000 7f9a74c89000 rw p 00000000 00 00 0 7f9a74c89000 7f9a74d01000 rw p 01beb000 08 21 3018824 usr local lib python2.7 dist packages numpy .libs libopenblasp r0 382c8f3a.3.5.dev.so 7f9a74d01000 7f9a75088000 r xp 00000000 08 21 3018433 usr local lib python2.7 dist packages numpy core multiarray umath.so 7f9a75088000 7f9a75287000 p 00387000 08 21 3018433 usr local lib python2.7 dist packages numpy core multiarray umath.so 7f9a75287000 7f9a752a6000 rw p 00386000 08 21 3018433 usr local lib python2.7 dist packages numpy core multiarray umath.so 7f9a752a6000 7f9a752c7000 rw p 00000000 00 00 0 7f9a752c7000 7f9a752ce000 rw p 012b0000 08 21 3018433 usr local lib python2.7 dist packages numpy core multiarray umath.so 7f9a752ce000 7f9a7534e000 rw p 00000000 00 00 0 7f9a7534e000 7f9a75569000 r xp 00000000 08 21 45487999 lib x86 64 linux gnu libcrypto.so.1.0.0 7f9a75569000 7f9a75768000 p 0021b000 08 21 45487999 lib x86 64 linux gnu libcrypto.so.1.0.0 7f9a75768000 7f9a75784000 r p 0021a000 08 21 45487999 lib x86 64 linux gnu libcrypto.so.1.0.0 7f9a75784000 7f9a75790000 rw p 00236000 08 21 45487999 lib x86 64 linux gnu libcrypto.so.1.0.0 7f9a75790000 7f9a75793000 rw p 00000000 00 00 0 7f9a75793000 7f9a75799000 r xp 00000000 08 21 62787964 usr lib python2.7 lib dynload hashlib.x86 64 linux gnu.so 7f9a75799000 7f9a75998000 p 00006000 08 21 62787964 usr lib python2.7 lib dynload hashlib.x86 64 linux gnu.so 7f9a75998000 7f9a75999000 r p 00005000 08 21 62787964 usr lib python2.7 lib dynload hashlib.x86 64 linux gnu.so 7f9a75999000 7f9a7599a000 rw p 00006000 08 21 62787964 usr lib python2.7 lib dynload hashlib.x86 64 linux gnu.so 7f9a7599b000 7f9a75b1b000 rw p 00000000 00 00 0 7f9a75b1b000 7f9a75c23000 r xp 00000000 08 21 38015915 lib x86 64 linux gnu libm 2.23.so 7f9a75c23000 7f9a75e22000 p 00108000 08 21 38015915 lib x86 64 linux gnu libm 2.23.so 7f9a75e22000 7f9a75e23000 r p 00107000 08 21 38015915 lib x86 64 linux gnu libm 2.23.so 7f9a75e23000 7f9a75e24000 rw p 00108000 08 21 38015915 lib x86 64 linux gnu libm 2.23.so 7f9a75e24000 7f9a75e3d000 r xp 00000000 08 21 38015982 lib x86 64 linux gnu libz.so.1.2.8 7f9a75e3d000 7f9a7603c000 p 00019000 08 21 38015982 lib x86 64 linux gnu libz.so.1.2.8 7f9a7603c000 7f9a7603d000 r p 00018000 08 21 38015982 lib x86 64 linux gnu libz.so.1.2.8 7f9a7603d000 7f9a7603e000 rw p 00019000 08 21 38015982 lib x86 64 linux gnu libz.so.1.2.8 7f9a7603e000 7f9a76040000 r xp 00000000 08 21 38015977 lib x86 64 linux gnu libutil 2.23.so 7f9a76040000 7f9a7623f000 p 00002000 08 21 38015977 lib x86 64 linux gnu libutil 2.23.so 7f9a7623f000 7f9a76240000 r p 00001000 08 21 38015977 lib x86 64 linux gnu libutil 2.23.so 7f9a76240000 7f9a76241000 rw p 00002000 08 21 38015977 lib x86 64 linux gnu libutil 2.23.so 7f9a76241000 7f9a76244000 r xp 00000000 08 21 38015896 lib x86 64 linux gnu libdl 2.23.so 7f9a76244000 7f9a76443000 p 00003000 08 21 38015896 lib x86 64 linux gnu libdl 2.23.so 7f9a76443000 7f9a76444000 r p 00002000 08 21 38015896 lib x86 64 linux gnu libdl 2.23.so 7f9a76444000 7f9a76445000 rw p 00003000 08 21 38015896 lib x86 64 linux gnu libdl 2.23.so 7f9a76445000 7f9a76605000 r xp 00000000 08 21 38015883 lib x86 64 linux gnu libc 2.23.so 7f9a76605000 7f9a76805000 p 001c0000 08 21 38015883 lib x86 64 linux gnu libc 2.23.so 7f9a76805000 7f9a76809000 r p 001c0000 08 21 38015883 lib x86 64 linux gnu libc 2.23.so 7f9a76809000 7f9a7680b000 rw p 001c4000 08 21 38015883 lib x86 64 linux gnu libc 2.23.so 7f9a7680b000 7f9a7680f000 rw p 00000000 00 00 0 7f9a7680f000 7f9a76827000 r xp 00000000 08 21 38015951 lib x86 64 linux gnu libpthread 2.23.so 7f9a76827000 7f9a76a26000 p 00018000 08 21 38015951 lib x86 64 linux gnu libpthread 2.23.so 7f9a76a26000 7f9a76a27000 r p 00017000 08 21 38015951 lib x86 64 linux gnu libpthread 2.23.so 7f9a76a27000 7f9a76a28000 rw p 00018000 08 21 38015951 lib x86 64 linux gnu libpthread 2.23.so 7f9a76a28000 7f9a76a2c000 rw p 00000000 00 00 0 7f9a76a2c000 7f9a76a52000 r xp 00000000 08 21 38015863 lib x86 64 linux gnu ld 2.23.so 7f9a76a8a000 7f9a76a8b000 rw p 00000000 00 00 0 7f9a76a8b000 7f9a76a8c000 rw 00000000 00 06 816 dev nvidia0 7f9a76a8c000 7f9a76a8d000 rw 00000000 00 06 815 dev nvidiactl 7f9a76a8d000 7f9a76a8e000 rw 00000000 00 06 816 dev nvidia0 7f9a76a8e000 7f9a76b4e000 rw p 00000000 00 00 0 7f9a76b4e000 7f9a76b4f000 rw 00000000 00 06 815 dev nvidiactl 7f9a76b4f000 7f9a76b50000 rw 00000000 00 06 816 dev nvidia0 7f9a76b50000 7f9a76b51000 rw 00000000 00 06 815 dev nvidiactl 7f9a76b51000 7f9a76b52000 rw 00000000 00 06 816 dev nvidia0 7f9a76b52000 7f9a76b70000 r xp 00000000 08 21 38015974 lib x86 64 linux gnu libudev.so.1.6.4 7f9a76b70000 7f9a76b71000 r p 0001d000 08 21 38015974 lib x86 64 linux gnu libudev.so.1.6.4 7f9a76b71000 7f9a76b72000 rw p 0001e000 08 21 38015974 lib x86 64 linux gnu libudev.so.1.6.4 7f9a76b72000 7f9a76b73000 rw 00000000 00 06 815 dev nvidiactl 7f9a76b73000 7f9a76b74000 rw 00000000 00 06 816 dev nvidia0 7f9a76b74000 7f9a76b75000 rw 00000000 00 06 815 dev nvidiactl 7f9a76b75000 7f9a76b76000 rw 00000000 00 06 816 dev nvidia0 7f9a76b76000 7f9a76b77000 rw 00000000 00 06 815 dev nvidiactl 7f9a76b77000 7f9a76b78000 rw 00000000 00 06 816 dev nvidia0 7f9a76b78000 7f9a76b79000 rw 00000000 00 06 815 dev nvidiactl 7f9a76b79000 7f9a76b7a000 rw 00000000 00 06 816 dev nvidia0 7f9a76b7a000 7f9a76b7b000 rw 00000000 00 06 815 dev nvidiactl 7f9a76b7b000 7f9a76b7c000 rw 00000000 00 06 816 dev nvidia0 7f9a76b7c000 7f9a76b7d000 rw 00000000 00 06 815 dev nvidiactl 7f9a76b7d000 7f9a76b7e000 rw 00000000 00 06 816 dev nvidia0 7f9a76b7e000 7f9a76b7f000 rw 00000000 00 06 815 dev nvidiactl 7f9a76b7f000 7f9a76c44000 rw p 00000000 00 00 0 7f9a76c44000 7f9a76c45000 rw 00000000 00 06 816 dev nvidia0 7f9a76c45000 7f9a76c46000 rw 00000000 00 06 815 dev nvidiactl 7f9a76c46000 7f9a76c47000 rw 00000000 00 06 816 dev nvidia0 7f9a76c47000 7f9a76c48000 rw 00000000 00 06 815 dev nvidiactl 7f9a76c48000 7f9a76c49000 rw 00000000 00 06 816 dev nvidia0 7f9a76c49000 7f9a76c4a000 rw 00000000 00 06 815 dev nvidiactl 7f9a76c4a000 7f9a76c4b000 rw 00000000 00 06 816 dev nvidia0 7f9a76c4b000 7f9a76c4c000 rw 00000000 00 06 815 dev nvidiactl 7f9a76c4c000 7f9a76c4d000 rw 00000000 00 06 816 dev nvidia0 7f9a76c4d000 7f9a76c4e000 rw 00000000 00 06 815 dev nvidiactl 7f9a76c4e000 7f9a76c4f000 rw 00000000 00 06 816 dev nvidia0 7f9a76c4f000 7f9a76c50000 rw 00000000 00 06 815 dev nvidiactl 7f9a76c50000 7f9a76c51000 rwxp 00000000 00 00 0 7f9a76c51000 7f9a76c52000 r p 00025000 08 21 38015863 lib x86 64 linux gnu ld 2.23.so 7f9a76c52000 7f9a76c53000 rw p 00026000 08 21 38015863 lib x86 64 linux gnu ld 2.23.so 7f9a76c53000 7f9a76c54000 rw p 00000000 00 00 0 7fff2cb1b000 7fff2cb3c000 rw p 00000000 00 00 0 stack 7fff2cb80000 7fff2cb83000 r p 00000000 00 00 0 vvar 7fff2cb83000 7fff2cb85000 r xp 00000000 00 00 0 vdso ffffffffff600000 ffffffffff601000 r xp 00000000 00 00 0 vsyscall bash line 1 56358 aborted core dumped env jetbrains remote run 1 library roots c users chenhuizhen .pycharm2018.3 system remote sources 823358608 1227070294 c users chenhuizhen .pycharm2018.3 system remote sources 823358608 graphviz 0.8.4 py2.7.egg! c users chenhuizhen .pycharm2018.3 system remote sources 823358608 554785109 c users chenhuizhen .pycharm2018.3 system remote sources 823358608 370154233 c users chenhuizhen .pycharm2018.3 system remote sources 823358608 518999124 c users chenhuizhen .pycharm2018.3 system remote sources 823358608 idna 2.6 py2.7.egg! c users chenhuizhen .pycharm2018.3 system remote sources 823358608 chardet 3.0.4 py2.7.egg! c users chenhuizhen .pycharm2018.3 system remote sources 823358608 201544331 c users chenhuizhen .pycharm2018.3 system remote sources 823358608 724150857 c users chenhuizhen .pycharm2018.3 system remote sources 823358608 154863933 c users chenhuizhen .pycharm2018.3 system remote sources 823358608 1227933812 c users chenhuizhen .pycharm2018.3 system remote sources 823358608 125940560 c users chenhuizhen .pycharm2018.3 system remote sources 823358608 2085782652 c users chenhuizhen .pycharm2018.3 system remote sources 823358608 201545290 c users chenhuizhen .pycharm2018.3 system remote sources 823358608 452379848 c users chenhuizhen .pycharm2018.3 system remote sources 823358608 1405239563 c users chenhuizhen .pycharm2018.3 system remote sources 823358608 1171821208 c users chenhuizhen .pycharm2018.3 system remote sources 823358608 530511828 c users chenhuizhen .pycharm2018.3 system remote sources 823358608 427714987 c users chenhuizhen .pycharm2018.3 system remote sources 823358608 541471831 c users chenhuizhen .pycharm2018.3 system remote sources 823358608 1437370484 c users chenhuizhen .pycharm2018.3 system remote sources 823358608 17574554 c users chenhuizhen .pycharm2018.3 system remote sources 823358608 669732926 c users chenhuizhen .pycharm2018.3 system remote sources 823358608 452463671 c users chenhuizhen .pycharm2018.3 system remote sources 823358608 662605150 c users chenhuizhen .pycharm2018.3 system python stubs 823358608 c program files jetbrains pycharm 2018.3.1 helpers python skeletons c program files jetbrains pycharm 2018.3.1 helpers typeshed stdlib 2 c program files jetbrains pycharm 2018.3.1 helpers typeshed stdlib 2and3 c program files jetbrains pycharm 2018.3.1 helpers typeshed third party 2 c program files jetbrains pycharm 2018.3.1 helpers typeshed third party 2and3 pydevd load values async true pythonpath opt kd pavement crackseg root .pycharm helpers pycharm matplotlib backend root .pycharm helpers third party thriftpy root .pycharm helpers pydev c users chenhuizhen .pycharm2018.3 system cythonextensions opt kd pavement crackseg pythonioencoding utf 8 pythondontwritebytecode 1 ipythonenable true pycharm matplotlib port 64619 pycharm hosted 1 pythonunbuffered 1 ide project roots opt kd pavement crackseg ' usr bin python' ' u' ' root .pycharm helpers pydev pydevd.py' ' multiproc' ' qt support auto' ' client' '0.0.0.0' ' port' '44975' ' file' ' opt kd pavement crackseg main crack segmentation.py' process finished exit code 134"
incubator-mxnet,11879,scala many warnings due different dependencies requiring different versions scala below. one option consider using scala 2.12 needs analysis breaks compatibility.,0,mxnet scala scala build produces warnings.,mxnet scala scala build produces warnings. scala many warnings due different dependencies requiring different versions scala below. one option consider using scala 2.12 needs analysis breaks compatibility.
incubator-mxnet,8444,"built mxnet cpp, copied libmxnet.so libmxnet static.a project well headers, build error says exactly happened?",0,include mxnet cpp operator.hpp 157 31 error expected ' ' end declaration std vector outputs output,"include mxnet cpp operator.hpp 157 31 error expected ' ' end declaration std vector outputs output built mxnet cpp, copied libmxnet.so libmxnet static.a project well headers, build error says exactly happened?"
incubator-mxnet,2710,"hi all, support elastic sgd ?",0,elastic sgd,"elastic sgd hi all, support elastic sgd ?"
incubator-mxnet,16515,"run mxnet c interface example image classification predict.cc https github.com apache incubator mxnet blob master example image classification predict cpp image classification predict.cc windows7 visual studio 14 2015, cannot exits normally. comment mxnet predictor free api , exit successfully.",0,"mxnet c interface, run image classification predict.cc example cannot exits normally","mxnet c interface, run image classification predict.cc example cannot exits normally run mxnet c interface example image classification predict.cc https github.com apache incubator mxnet blob master example image classification predict cpp image classification predict.cc windows7 visual studio 14 2015, cannot exits normally. comment mxnet predictor free api , exit successfully."
incubator-mxnet,11654,"11631, build passing http jenkins.mxnet ci.amazon ml.com blue organizations jenkins incubator mxnet detail pr 11631 8 pipeline, status pr updated. ! screen shot 2018 07 11 2 16 43 pm https user images.githubusercontent.com 11465736 42599755 20d30ab4 8515 11e8 903c d92e93eab429.png",0,ci problem build status reflected pr,"ci problem build status reflected pr 11631, build passing http jenkins.mxnet ci.amazon ml.com blue organizations jenkins incubator mxnet detail pr 11631 8 pipeline, status pr updated. ! screen shot 2018 07 11 2 16 43 pm https user images.githubusercontent.com 11465736 42599755 20d30ab4 8515 11e8 903c d92e93eab429.png"
incubator-mxnet,11462,"description small batch, sparse linear classification uses cpu, throughput small. environment info required machine used aws ami, c5.9xlarge, steps repro 1. pip2 install mxnet mkl 2. git clone mxnet 3. directory , run see throughput around 600 samples sec. tried set things like . seems setting this, cpu usage reduces half cores used , throughput reduced. even case setting . question set things increase throughput linear classfication training single machine multiple cores? mxnet currently optimize direction i.e. using things like hogwild! . thanks advance. lcytzk",0,throughput sparse linear classification small small batch size,"throughput sparse linear classification small small batch size description small batch, sparse linear classification uses cpu, throughput small. environment info required machine used aws ami, c5.9xlarge, steps repro 1. pip2 install mxnet mkl 2. git clone mxnet 3. directory , run see throughput around 600 samples sec. tried set things like . seems setting this, cpu usage reduces half cores used , throughput reduced. even case setting . question set things increase throughput linear classfication training single machine multiple cores? mxnet currently optimize direction i.e. using things like hogwild! . thanks advance. lcytzk"
incubator-mxnet,4449,"environment info operating system win7 64 compiler vs 2013 12.0.3 update 4 3rd library cudnn 4, openblas, opencv2, cuda 7.5 mxnet version yajiedesign gpu release 2016 12 29,2016 11 25,2016 09 09 python version distribution anaconda python 2.7.11 error info mxnet.base.mxneterror 15 45 06 program files x86 jenkins workspace mxnet mxnet src operator rnn.cu 24 rnn available cudnn moment. description 1. cnn module run quickly produce good results gpu. cudnn put c program files nvidia gpu computing toolkit cuda v7.5. think cudnn ok 2. run rnn cell demo.py, error shows mxnet.base.mxneterror 15 45 06 program files x86 jenkins workspace mxnet mxnet src operator rnn.cu 24 rnn available cudnn moment. 3. change many many prebuilt yajiedesign version, error occurs still. addition, would appreciate rnn corresponding models symbols, like codes bucket io.py, programmed standard mxnet library easily use. means, compose several existed symbol, wrap input samples, labels get complete char rnn model speech rnn model. ideal assumption data mx.sym.sequencevariable name 'data',bucket 5,10,15 rnn mx.sym.rnn name 'rnn',data ...,num filter",0,error occured runing rnn cell demo.py yajiedesign mxnet release,"error occured runing rnn cell demo.py yajiedesign mxnet release environment info operating system win7 64 compiler vs 2013 12.0.3 update 4 3rd library cudnn 4, openblas, opencv2, cuda 7.5 mxnet version yajiedesign gpu release 2016 12 29,2016 11 25,2016 09 09 python version distribution anaconda python 2.7.11 error info mxnet.base.mxneterror 15 45 06 program files x86 jenkins workspace mxnet mxnet src operator rnn.cu 24 rnn available cudnn moment. description 1. cnn module run quickly produce good results gpu. cudnn put c program files nvidia gpu computing toolkit cuda v7.5. think cudnn ok 2. run rnn cell demo.py, error shows mxnet.base.mxneterror 15 45 06 program files x86 jenkins workspace mxnet mxnet src operator rnn.cu 24 rnn available cudnn moment. 3. change many many prebuilt yajiedesign version, error occurs still. addition, would appreciate rnn corresponding models symbols, like codes bucket io.py, programmed standard mxnet library easily use. means, compose several existed symbol, wrap input samples, labels get complete char rnn model speech rnn model. ideal assumption data mx.sym.sequencevariable name 'data',bucket 5,10,15 rnn mx.sym.rnn name 'rnn',data ...,num filter"
incubator-mxnet,3685,"followed tutorial http mxnet.io multi devices.html distributed training multiple machines try train example image classification train mnist.py. failed launcher mpi ssh. successfully run script never trains report error. stuck kill manually. set environment variables nodes export dmlc interface em3 export ps verbose 2 hosts 158.1xx.x.80 158.1xx.x.81 ssh script running, training processes distributed remover servers... nothing happens afterwards. .. .. tools launch.py launcher ssh n 2 1 h hosts python train mnist.py network lenet gpus 0,1,2,3 kv store dist sync 17 16 55 src van.cc 69 bind role scheduler, id 1, ip 158.1xx.x.80, port 9091, recovery 0 17 17 00 src van.cc 155 ? 1. meta request 0, timestamp 0, control cmd add node, node role server, ip 158.1xx.x.80, port 50368, recovery 0 17 17 00 src van.cc 155 ? 1. meta request 0, timestamp 0, control cmd add node, node role worker, ip 158.1xx.x.80, port 51877, recovery 0 killed signal 2. killed signal 2. killed signal 2. 2016 11 01 17 22 15,238 info stop luancher try increase number worker .. .. tools launch.py launcher ssh n 4 1 h hosts python train mnist.py network lenet gpus 0,1,2,3 kv store dist sync 17 25 19 src van.cc 69 bind role scheduler, id 1, ip 158.1xx.x.80, port 9092, recovery 0 17 25 24 src van.cc 155 ? 1. meta request 0, timestamp 0, control cmd add node, node role worker, ip 158.1xx.x.80, port 36202, recovery 0 17 25 29 src van.cc 155 ? 1. meta request 0, timestamp 0, control cmd add node, node role server, ip 158.1xx.x.80, port 52936, recovery 0 17 25 29 src van.cc 155 ? 1. meta request 0, timestamp 0, control cmd add node, node role worker, ip 158.1xx.x.80, port 45130, recovery 0 killed signal 2. killed signal 2. killed signal 2. killed signal 2. killed signal 2. 2016 11 01 17 28 41,265 info stop luancher find roles assigned 158.1xx.x.80 only, normal? mpi open mpi 1.10.1 training processes can't even launch remote server... .. .. tools launch.py launcher mpi n 4 1 h hosts python train mnist.py network lenet gpus 0,1,2,3 kv store dist sync 2016 11 01 17 35 05,797 info start 4 workers mpirun 17 35 06 src van.cc 69 bind role scheduler, id 1, ip 158.182.9.80, port 9091, recovery 0 2016 11 01 17 49 08,476 info stop luancher tested mpi applications, work fine. mpi bandwidth test reach 105.93mb sec. missing anything makes training multiple nodes fail? please help. document gives little info situation. thanks advance!",0,"cannot train multiple nodes, port blocked firewall","cannot train multiple nodes, port blocked firewall followed tutorial http mxnet.io multi devices.html distributed training multiple machines try train example image classification train mnist.py. failed launcher mpi ssh. successfully run script never trains report error. stuck kill manually. set environment variables nodes export dmlc interface em3 export ps verbose 2 hosts 158.1xx.x.80 158.1xx.x.81 ssh script running, training processes distributed remover servers... nothing happens afterwards. .. .. tools launch.py launcher ssh n 2 1 h hosts python train mnist.py network lenet gpus 0,1,2,3 kv store dist sync 17 16 55 src van.cc 69 bind role scheduler, id 1, ip 158.1xx.x.80, port 9091, recovery 0 17 17 00 src van.cc 155 ? 1. meta request 0, timestamp 0, control cmd add node, node role server, ip 158.1xx.x.80, port 50368, recovery 0 17 17 00 src van.cc 155 ? 1. meta request 0, timestamp 0, control cmd add node, node role worker, ip 158.1xx.x.80, port 51877, recovery 0 killed signal 2. killed signal 2. killed signal 2. 2016 11 01 17 22 15,238 info stop luancher try increase number worker .. .. tools launch.py launcher ssh n 4 1 h hosts python train mnist.py network lenet gpus 0,1,2,3 kv store dist sync 17 25 19 src van.cc 69 bind role scheduler, id 1, ip 158.1xx.x.80, port 9092, recovery 0 17 25 24 src van.cc 155 ? 1. meta request 0, timestamp 0, control cmd add node, node role worker, ip 158.1xx.x.80, port 36202, recovery 0 17 25 29 src van.cc 155 ? 1. meta request 0, timestamp 0, control cmd add node, node role server, ip 158.1xx.x.80, port 52936, recovery 0 17 25 29 src van.cc 155 ? 1. meta request 0, timestamp 0, control cmd add node, node role worker, ip 158.1xx.x.80, port 45130, recovery 0 killed signal 2. killed signal 2. killed signal 2. killed signal 2. killed signal 2. 2016 11 01 17 28 41,265 info stop luancher find roles assigned 158.1xx.x.80 only, normal? mpi open mpi 1.10.1 training processes can't even launch remote server... .. .. tools launch.py launcher mpi n 4 1 h hosts python train mnist.py network lenet gpus 0,1,2,3 kv store dist sync 2016 11 01 17 35 05,797 info start 4 workers mpirun 17 35 06 src van.cc 69 bind role scheduler, id 1, ip 158.182.9.80, port 9091, recovery 0 2016 11 01 17 49 08,476 info stop luancher tested mpi applications, work fine. mpi bandwidth test reach 105.93mb sec. missing anything makes training multiple nodes fail? please help. document gives little info situation. thanks advance!"
incubator-mxnet,8741,"description default install mxnet currently hangs running types inference particular setup c5 instances. setup openblas library is, example, installed ubuntu 14.04 issue. issue may effecting hardware skylake architecture, deterministically failing c5s. environment info required ubuntu 14.04 steps reproduce launch c5 instance type docker support. build following dockerfile package used python r scala julia perl test. also verified fails stack python. mxnet commit hash fd45517614842bfa1d32d1ba54a200eb4a0dd377 error message one frame symbols stack tried solve it? 1. new builds openblas fix issue. 2. copying openblas build ships ubuntu 16.04 fixes issue. todos test see effects skylake new cpus. contact openblas attempt get fix. create minimum reproducible example calls openblas directly bypass mxnet code pass mrt openblas. modify internal docker images use ubuntu 16.04 default base.",0,inference openblas ubuntu 14.04 hangs c5 instances.,"inference openblas ubuntu 14.04 hangs c5 instances. description default install mxnet currently hangs running types inference particular setup c5 instances. setup openblas library is, example, installed ubuntu 14.04 issue. issue may effecting hardware skylake architecture, deterministically failing c5s. environment info required ubuntu 14.04 steps reproduce launch c5 instance type docker support. build following dockerfile package used python r scala julia perl test. also verified fails stack python. mxnet commit hash fd45517614842bfa1d32d1ba54a200eb4a0dd377 error message one frame symbols stack tried solve it? 1. new builds openblas fix issue. 2. copying openblas build ships ubuntu 16.04 fixes issue. todos test see effects skylake new cpus. contact openblas attempt get fix. create minimum reproducible example calls openblas directly bypass mxnet code pass mrt openblas. modify internal docker images use ubuntu 16.04 default base."
incubator-mxnet,9193,"build mxnet use nccl 1 use nccl path usr local lib version mnxet 1.0 version nccl 1.3.4 file usr local lib run test nccl.py, seems stop multi gpus tests. set nccl debug info, output below. program seems stop going on. usage gpu like below.",0,use nccl kvstore,"use nccl kvstore build mxnet use nccl 1 use nccl path usr local lib version mnxet 1.0 version nccl 1.3.4 file usr local lib run test nccl.py, seems stop multi gpus tests. set nccl debug info, output below. program seems stop going on. usage gpu like below."
incubator-mxnet,2192,hello wanted something mx.sym.sum. 2 outputs want diff sum. leg1 leg2 outputs fullyconnected minibatch size 128 tell sum symbol take account sum per row instead summing whole matrix ?,0,mx.sym.sum take batch size account ?,mx.sym.sum take batch size account ? hello wanted something mx.sym.sum. 2 outputs want diff sum. leg1 leg2 outputs fullyconnected minibatch size 128 tell sum symbol take account sum per row instead summing whole matrix ?
incubator-mxnet,13664,"note providing complete information concise form best way get help. issue template serves checklist essential information technical issues bug reports. non technical issues feature requests, feel free present information believe best form. q discussion, please start discussion thread https discuss.mxnet.io description brief description problem 2 sentences. environment info required python diagnose.py package used python r scala julia using scala user, please provide 1. java version 2. maven version 3. scala runtime applicable r user, please provide r build info required built source compiler gcc clang mingw visual studio mxnet commit hash paste output here. build config paste content config.mk, build command. error message paste complete error message, including stack trace. minimum reproducible example using code, please provide short script reproduces error. otherwise, please provide link existing example. steps reproduce paste commands ran produced error. 1. 2. tried solve it? 1. 2.",0,dj,"dj note providing complete information concise form best way get help. issue template serves checklist essential information technical issues bug reports. non technical issues feature requests, feel free present information believe best form. q discussion, please start discussion thread https discuss.mxnet.io description brief description problem 2 sentences. environment info required python diagnose.py package used python r scala julia using scala user, please provide 1. java version 2. maven version 3. scala runtime applicable r user, please provide r build info required built source compiler gcc clang mingw visual studio mxnet commit hash paste output here. build config paste content config.mk, build command. error message paste complete error message, including stack trace. minimum reproducible example using code, please provide short script reproduces error. otherwise, please provide link existing example. steps reproduce paste commands ran produced error. 1. 2. tried solve it? 1. 2."
incubator-mxnet,4224,"environment info operating system ubuntu14.04 compiler gcc4.8.4 package used python r scala julia python mxnet version 0.7.0 installed source yes mxnet commit hash using python package, please provide python version distribution 2.7.6 using r package, please provide r error message faster rcnn train error python train end2end.py downsize voc2007 train sets to1096 images,my 980 4g memory, still report memory, want know solve problem.thank you! yx yx x8dtl mxnet example mx rcnn master python train end2end.pycalled argument namespace begin epoch 0, dataset 'pascalvoc', dataset path 'data vocdevkit', end epoch 10, epoch 1, flip true, frequent 20, gpus '0', image set '2007 trainval', kvstore 'device', lr 0.001, lr step 50000, network 'vgg', prefix 'model e2e', pretrained 'model vgg16', resume false, root path 'data', work load list none 'eps' 1e 14, 'image stride' 0, 'pixel means' array 123.68 , 116.779, 103.939 , 'scales' 600, 1000 , 'test' 'batch images' 1, 'has rpn' false, 'nms' 0.3, 'rpn min size' 16, 'rpn nms thresh' 0.7, 'rpn post nms top n' 300, 'rpn pre nms top n' 6000 , 'train' 'batch images' 1, 'batch rois' 128, 'bbox means' 0.0, 0.0, 0.0, 0.0 , 'bbox normalization precomputed' true, 'bbox regression thresh' 0.5, 'bbox stds' 0.1, 0.1, 0.2, 0.2 , 'bbox weights' array 1., 1., 1., 1. , 'bg thresh hi' 0.5, 'bg thresh lo' 0.0, 'end2end' true, 'fg fraction' 0.25, 'fg thresh' 0.5, 'rpn batch size' 256, 'rpn bbox weights' 1.0, 1.0, 1.0, 1.0 , 'rpn clobber positives' false, 'rpn fg fraction' 0.5, 'rpn min size' 16, 'rpn negative overlap' 0.3, 'rpn nms thresh' 0.7, 'rpn positive overlap' 0.7, 'rpn positive weight' 1.0, 'rpn post nms top n' 6000, 'rpn pre nms top n' 12000 num images 1096 voc 2007 trainval gt roidb loaded data cache voc 2007 trainval gt roidb.pkl append flipped images roidb 20 41 11 src engine engine.cc 36 mxnet start using engine naiveengine providing maximum shape 'data', 1, 3, 1000, 1000 , 'gt boxes', 1, 100, 5 'label', 1, 34596 , 'bbox target', 1, 36, 62, 62 , 'bbox weight', 1, 36, 62, 62 output shape 'bbox loss reshape output' 1l, 128l, 84l , 'blockgrad0 output' 1l, 128l , 'cls prob reshape output' 1l, 128l, 21l , 'rpn bbox loss output' 1l, 36l, 37l, 50l , 'rpn cls prob output' 1l, 2l, 333l, 50l 20 41 15 home yx mxnet dmlc core include dmlc . logging.h 235 20 41 15 src storage . pooled storage manager.h 79 cudamalloc failed memory traceback recent call last file train end2end.py , line 185, main file train end2end.py , line 182, main lr args.lr, lr step args.lr step file train end2end.py , line 133, train net arg params arg params, aux params aux params, begin epoch begin epoch, num epoch end epoch file usr local lib python2.7 dist packages mxnet 0.7.0 py2.7.egg mxnet module base module.py , line 379, fit self.update file home yx mxnet example mx rcnn master rcnn core module.py , line 183, update self. curr module.update file usr local lib python2.7 dist packages mxnet 0.7.0 py2.7.egg mxnet module module.py , line 419, update kvstore self. kvstore file usr local lib python2.7 dist packages mxnet 0.7.0 py2.7.egg mxnet model.py , line 115, update params updater index num device k, g, w file usr local lib python2.7 dist packages mxnet 0.7.0 py2.7.egg mxnet optimizer.py , line 822, updater optimizer.update index, weight, grad, states index file usr local lib python2.7 dist packages mxnet 0.7.0 py2.7.egg mxnet optimizer.py , line 298, update grad grad self.rescale grad file usr local lib python2.7 dist packages mxnet 0.7.0 py2.7.egg mxnet ndarray.py , line 138, mul return multiply self, file usr local lib python2.7 dist packages mxnet 0.7.0 py2.7.egg mxnet ndarray.py , line 744, multiply none file usr local lib python2.7 dist packages mxnet 0.7.0 py2.7.egg mxnet ndarray.py , line 655, ufunc helper return lfn scalar lhs, float rhs file usr local lib python2.7 dist packages mxnet 0.7.0 py2.7.egg mxnet ndarray.py , line 1263, generic ndarray function c array ctypes.c char p, c str str kwargs.values file usr local lib python2.7 dist packages mxnet 0.7.0 py2.7.egg mxnet base.py , line 77, check call raise mxneterror py str lib.mxgetlasterror mxnet.base.mxneterror 20 41 15 src storage . pooled storage manager.h 79 cudamalloc failed memory terminate called without active exception minimum reproducible example using code, please provide short script reproduces error. steps reproduce running standard examples, please provide commands run lead error. 1.python train end2end.py 2. 3. tried solve it? 1.downsize train sets, error still occur. 2. 3.",0,cudamalloc failed memory,"cudamalloc failed memory environment info operating system ubuntu14.04 compiler gcc4.8.4 package used python r scala julia python mxnet version 0.7.0 installed source yes mxnet commit hash using python package, please provide python version distribution 2.7.6 using r package, please provide r error message faster rcnn train error python train end2end.py downsize voc2007 train sets to1096 images,my 980 4g memory, still report memory, want know solve problem.thank you! yx yx x8dtl mxnet example mx rcnn master python train end2end.pycalled argument namespace begin epoch 0, dataset 'pascalvoc', dataset path 'data vocdevkit', end epoch 10, epoch 1, flip true, frequent 20, gpus '0', image set '2007 trainval', kvstore 'device', lr 0.001, lr step 50000, network 'vgg', prefix 'model e2e', pretrained 'model vgg16', resume false, root path 'data', work load list none 'eps' 1e 14, 'image stride' 0, 'pixel means' array 123.68 , 116.779, 103.939 , 'scales' 600, 1000 , 'test' 'batch images' 1, 'has rpn' false, 'nms' 0.3, 'rpn min size' 16, 'rpn nms thresh' 0.7, 'rpn post nms top n' 300, 'rpn pre nms top n' 6000 , 'train' 'batch images' 1, 'batch rois' 128, 'bbox means' 0.0, 0.0, 0.0, 0.0 , 'bbox normalization precomputed' true, 'bbox regression thresh' 0.5, 'bbox stds' 0.1, 0.1, 0.2, 0.2 , 'bbox weights' array 1., 1., 1., 1. , 'bg thresh hi' 0.5, 'bg thresh lo' 0.0, 'end2end' true, 'fg fraction' 0.25, 'fg thresh' 0.5, 'rpn batch size' 256, 'rpn bbox weights' 1.0, 1.0, 1.0, 1.0 , 'rpn clobber positives' false, 'rpn fg fraction' 0.5, 'rpn min size' 16, 'rpn negative overlap' 0.3, 'rpn nms thresh' 0.7, 'rpn positive overlap' 0.7, 'rpn positive weight' 1.0, 'rpn post nms top n' 6000, 'rpn pre nms top n' 12000 num images 1096 voc 2007 trainval gt roidb loaded data cache voc 2007 trainval gt roidb.pkl append flipped images roidb 20 41 11 src engine engine.cc 36 mxnet start using engine naiveengine providing maximum shape 'data', 1, 3, 1000, 1000 , 'gt boxes', 1, 100, 5 'label', 1, 34596 , 'bbox target', 1, 36, 62, 62 , 'bbox weight', 1, 36, 62, 62 output shape 'bbox loss reshape output' 1l, 128l, 84l , 'blockgrad0 output' 1l, 128l , 'cls prob reshape output' 1l, 128l, 21l , 'rpn bbox loss output' 1l, 36l, 37l, 50l , 'rpn cls prob output' 1l, 2l, 333l, 50l 20 41 15 home yx mxnet dmlc core include dmlc . logging.h 235 20 41 15 src storage . pooled storage manager.h 79 cudamalloc failed memory traceback recent call last file train end2end.py , line 185, main file train end2end.py , line 182, main lr args.lr, lr step args.lr step file train end2end.py , line 133, train net arg params arg params, aux params aux params, begin epoch begin epoch, num epoch end epoch file usr local lib python2.7 dist packages mxnet 0.7.0 py2.7.egg mxnet module base module.py , line 379, fit self.update file home yx mxnet example mx rcnn master rcnn core module.py , line 183, update self. curr module.update file usr local lib python2.7 dist packages mxnet 0.7.0 py2.7.egg mxnet module module.py , line 419, update kvstore self. kvstore file usr local lib python2.7 dist packages mxnet 0.7.0 py2.7.egg mxnet model.py , line 115, update params updater index num device k, g, w file usr local lib python2.7 dist packages mxnet 0.7.0 py2.7.egg mxnet optimizer.py , line 822, updater optimizer.update index, weight, grad, states index file usr local lib python2.7 dist packages mxnet 0.7.0 py2.7.egg mxnet optimizer.py , line 298, update grad grad self.rescale grad file usr local lib python2.7 dist packages mxnet 0.7.0 py2.7.egg mxnet ndarray.py , line 138, mul return multiply self, file usr local lib python2.7 dist packages mxnet 0.7.0 py2.7.egg mxnet ndarray.py , line 744, multiply none file usr local lib python2.7 dist packages mxnet 0.7.0 py2.7.egg mxnet ndarray.py , line 655, ufunc helper return lfn scalar lhs, float rhs file usr local lib python2.7 dist packages mxnet 0.7.0 py2.7.egg mxnet ndarray.py , line 1263, generic ndarray function c array ctypes.c char p, c str str kwargs.values file usr local lib python2.7 dist packages mxnet 0.7.0 py2.7.egg mxnet base.py , line 77, check call raise mxneterror py str lib.mxgetlasterror mxnet.base.mxneterror 20 41 15 src storage . pooled storage manager.h 79 cudamalloc failed memory terminate called without active exception minimum reproducible example using code, please provide short script reproduces error. steps reproduce running standard examples, please provide commands run lead error. 1.python train end2end.py 2. 3. tried solve it? 1.downsize train sets, error still occur. 2. 3."
incubator-mxnet,15254,mxnet mxnet full 2.11 linux x86 64 gpu 1.5.0 snapshot cannot support cuda10.1? used maven packages check env java version maven version cuda version nvidia version os check test errors ? compute,0,mxnet mxnet full 2.11 linux x86 64 gpu 1.5.0 snapshot cannot support cuda10.1?,mxnet mxnet full 2.11 linux x86 64 gpu 1.5.0 snapshot cannot support cuda10.1? mxnet mxnet full 2.11 linux x86 64 gpu 1.5.0 snapshot cannot support cuda10.1? used maven packages check env java version maven version cuda version nvidia version os check test errors ? compute
incubator-mxnet,9962,"example, want group 2 outputs, split loss, could it? return list work, encounter error loss, since forward need symbol ndarray piiswrong",0,group 2 outputs gluon like sym.group,"group 2 outputs gluon like sym.group example, want group 2 outputs, split loss, could it? return list work, encounter error loss, since forward need symbol ndarray piiswrong"
incubator-mxnet,15514,"trying build latest head, dmlc.dll generated, dmlc.lib using ms visual studio 2015 . therefore, building mxnet dll fails 1 build started project mxnet, configuration release x64 1 link fatal error lnk1181 cannot open input file '3rdparty dmlc core release dmlc.lib'",0,dmlc.lib generated,"dmlc.lib generated trying build latest head, dmlc.dll generated, dmlc.lib using ms visual studio 2015 . therefore, building mxnet dll fails 1 build started project mxnet, configuration release x64 1 link fatal error lnk1181 cannot open input file '3rdparty dmlc core release dmlc.lib'"
incubator-mxnet,15201,referrencing tensorrt quantization tutorials blog posts etc though appear contrib section docs. would great add them. aaronmarkham,0,add tensorrt quantization docs website,add tensorrt quantization docs website referrencing tensorrt quantization tutorials blog posts etc though appear contrib section docs. would great add them. aaronmarkham
incubator-mxnet,14828,"hi, please help use im2rec.py python downloads im2rec.py downloads test . center crop resize 224 pack label get error opencv opencv virtual machine python downloads im2rec.py downloads test . center crop resize 224 pack label creating .rec file home opencv downloads test dog.lst home opencv downloads test multiprocessing available, fall back single threaded encoding imread read blank none image file home opencv . data img h 87.jpg traceback recent call last file downloads im2rec.py , line 386, record.write idx item 0 , file home opencv .local lib python2.7 site packages mxnet recordio.py , line 285, write idx self.write buf file home opencv .local lib python2.7 site packages mxnet recordio.py , line 135, write ctypes.c size len buf typeerror object type nonetype len mean 87.jpg https images.guru ipyr",0,typeerror object type nonetype len,"typeerror object type nonetype len hi, please help use im2rec.py python downloads im2rec.py downloads test . center crop resize 224 pack label get error opencv opencv virtual machine python downloads im2rec.py downloads test . center crop resize 224 pack label creating .rec file home opencv downloads test dog.lst home opencv downloads test multiprocessing available, fall back single threaded encoding imread read blank none image file home opencv . data img h 87.jpg traceback recent call last file downloads im2rec.py , line 386, record.write idx item 0 , file home opencv .local lib python2.7 site packages mxnet recordio.py , line 285, write idx self.write buf file home opencv .local lib python2.7 site packages mxnet recordio.py , line 135, write ctypes.c size len buf typeerror object type nonetype len mean 87.jpg https images.guru ipyr"
incubator-mxnet,12480,"description installing mxnet python3 virtualenv, get following error environment info required package used python r scala julia using python error message steps reproduce 1.",0,cannot install mxnet could find version satisfies requirement mxnet,"cannot install mxnet could find version satisfies requirement mxnet description installing mxnet python3 virtualenv, get following error environment info required package used python r scala julia using python error message steps reproduce 1."
incubator-mxnet,9538,initialize key kvstore. want init key 3 another shape program. reinitialize key? method remove key kvstore?,0,reinitialize key kvstore?,reinitialize key kvstore? initialize key kvstore. want init key 3 another shape program. reinitialize key? method remove key kvstore?
incubator-mxnet,207,"hi all, tried install mac 10.10 first run demo small datasets. deleted fopenmp suggested xgboost. got libmxnet.so libmxnet.a lib folder. still failed run example mnist.py suggested tutorial. reported dynamic module define init function . next solve problem? guys tutorial installing mac? thanks lot",0,"building fail mac 10.10, dynamic module define init function","building fail mac 10.10, dynamic module define init function hi all, tried install mac 10.10 first run demo small datasets. deleted fopenmp suggested xgboost. got libmxnet.so libmxnet.a lib folder. still failed run example mnist.py suggested tutorial. reported dynamic module define init function . next solve problem? guys tutorial installing mac? thanks lot"
incubator-mxnet,11289,test currently disabled since test requires instance 4 gpus available mxnet ci moment. adding new instance type roadmap.,0,nightly tests enable kvstore single node test,nightly tests enable kvstore single node test test currently disabled since test requires instance 4 gpus available mxnet ci moment. adding new instance type roadmap.
incubator-mxnet,2125,using today's github latest commit b5a369384e566490211cfb458bc9936d6c2a5678 following error. seems like function damaged? gives,0,typeerror create got unexpected keyword argument 'top k' ?,typeerror create got unexpected keyword argument 'top k' ? using today's github latest commit b5a369384e566490211cfb458bc9936d6c2a5678 following error. seems like function damaged? gives
incubator-mxnet,1127,"hi, specific 2 gpus devices command likes this, . file train mnist.py example provided source code. check gpus status via command nvidia smi, show likes, ! gpu https cloud.githubusercontent.com assets 4373620 12064726 811420c6 b005 11e5 96fe 6bdaa550d571.png process name 'python' issued command above. someone please help look this.",0,gpu cores actually use match set command,"gpu cores actually use match set command hi, specific 2 gpus devices command likes this, . file train mnist.py example provided source code. check gpus status via command nvidia smi, show likes, ! gpu https cloud.githubusercontent.com assets 4373620 12064726 811420c6 b005 11e5 96fe 6bdaa550d571.png process name 'python' issued command above. someone please help look this."
incubator-mxnet,7970,"mnist data loader fails unless resources package manually installed. resources package dependency installed along mxnet? environment info operating system mac os 10.12.6 compiler package used python r scala julia python 3.6.1 gcc 4.2.1 compatible apple llvm 8.1.0 clang 802.0.42 mxnet version 0.11.0 error message running code straight dope book, mx.gluon.data.dataloader fails download mnist data set unless python package resources manually installed. see https github.com zackchase mxnet straight dope issues 229",0,mnist data loader requires python resources package installed,"mnist data loader requires python resources package installed mnist data loader fails unless resources package manually installed. resources package dependency installed along mxnet? environment info operating system mac os 10.12.6 compiler package used python r scala julia python 3.6.1 gcc 4.2.1 compatible apple llvm 8.1.0 clang 802.0.42 mxnet version 0.11.0 error message running code straight dope book, mx.gluon.data.dataloader fails download mnist data set unless python package resources manually installed. see https github.com zackchase mxnet straight dope issues 229"
incubator-mxnet,6186,"bugs installation issues, please provide following information. information provide, likely people able help you. environment info operating system ubuntu 12.04 compiler g 4.8 nvcc package used python r scala julia mxnet version installed source source mxnet commit hash using python package, please provide python version distribution using r package, please provide r src operator . cudnn deconvolution inl.h 124 error identifier undefined detected instantiation mxnet op cudnndeconvolutionop cudnndeconvolutionop mxnet op deconvolutionparam, int, int, const std vector , const std vector , const mxnet context dtype float src operator deconvolution.cu 47 error message please paste full error message, including stack trace. minimum reproducible example using code, please provide short script reproduces error. steps reproduce running standard examples, please provide commands run lead error. 1. make tried solve it? 1. line 136 out.dptr change ptr, line 258 gwmat.dptr also compile, know whether correct.",0,cudnn deconvolution inl.h,"cudnn deconvolution inl.h bugs installation issues, please provide following information. information provide, likely people able help you. environment info operating system ubuntu 12.04 compiler g 4.8 nvcc package used python r scala julia mxnet version installed source source mxnet commit hash using python package, please provide python version distribution using r package, please provide r src operator . cudnn deconvolution inl.h 124 error identifier undefined detected instantiation mxnet op cudnndeconvolutionop cudnndeconvolutionop mxnet op deconvolutionparam, int, int, const std vector , const std vector , const mxnet context dtype float src operator deconvolution.cu 47 error message please paste full error message, including stack trace. minimum reproducible example using code, please provide short script reproduces error. steps reproduce running standard examples, please provide commands run lead error. 1. make tried solve it? 1. line 136 out.dptr change ptr, line 258 gwmat.dptr also compile, know whether correct."
incubator-mxnet,5059,"hi, find problems updating param num update optimizer.py. example, first iter, first grad gets lr num update 0 https github.com dmlc mxnet blob master python mxnet optimizer.py l264 , updates num update 1, next grad others get lr num update 1,. second iter, num update first grad 1, others 2. ........ may lead different grads get different lr. matter use multifactorscheduler get lr https github.com dmlc mxnet blob master python mxnet lr scheduler.py l85 . however, matter use schedulers poly scheduler https github.com bvlc caffe blob master src caffe solvers sgd solver.cpp l20 . think function update count changed https github.com dmlc mxnet blob master python mxnet optimizer.py l136 index self. index update count self. index update count index self.begin num update self.num update self. index update count index self. index update count index 1 changed https github.com dmlc mxnet blob master python mxnet optimizer.py l264 self. update count index lr self. get lr index wd self. get wd index",0,questions param num update optimizer.py,"questions param num update optimizer.py hi, find problems updating param num update optimizer.py. example, first iter, first grad gets lr num update 0 https github.com dmlc mxnet blob master python mxnet optimizer.py l264 , updates num update 1, next grad others get lr num update 1,. second iter, num update first grad 1, others 2. ........ may lead different grads get different lr. matter use multifactorscheduler get lr https github.com dmlc mxnet blob master python mxnet lr scheduler.py l85 . however, matter use schedulers poly scheduler https github.com bvlc caffe blob master src caffe solvers sgd solver.cpp l20 . think function update count changed https github.com dmlc mxnet blob master python mxnet optimizer.py l136 index self. index update count self. index update count index self.begin num update self.num update self. index update count index self. index update count index 1 changed https github.com dmlc mxnet blob master python mxnet optimizer.py l264 self. update count index lr self. get lr index wd self. get wd index"
incubator-mxnet,817,"hi, got 'segmentation fault' try use pre trained model operation. model converted vgg16 http www.robots.ox.ac.uk vgg software deep caffe vgg ilsvrc 16 layers.caffemodel caffe model tool mxnet. code works comment last line. code got gdb ! gdb http r.loli.io nbnf3i.png",0,segmentation fault using 'simple bind',"segmentation fault using 'simple bind' hi, got 'segmentation fault' try use pre trained model operation. model converted vgg16 http www.robots.ox.ac.uk vgg software deep caffe vgg ilsvrc 16 layers.caffemodel caffe model tool mxnet. code works comment last line. code got gdb ! gdb http r.loli.io nbnf3i.png"
incubator-mxnet,5062,implement gradient normalization clip followed file optimizer.py right?,0,question implementing gradient normalization clip,question implementing gradient normalization clip implement gradient normalization clip followed file optimizer.py right?
incubator-mxnet,5580,say layer need input parameters e.g. size 10 . set custom operator? see examples http mxnet.io new op.html,0,create custom operator extra parameters python?,create custom operator extra parameters python? say layer need input parameters e.g. size 10 . set custom operator? see examples http mxnet.io new op.html
incubator-mxnet,16249,"note providing complete information concise form best way get help. issue template serves checklist essential information technical issues bug reports. non technical issues feature requests, feel free present information believe best form. q discussion, please start discussion thread https discuss.mxnet.io description link obtained searching google duckduckgo mxnet gluon https mxnet.incubator.apache.org versions master gluon link however appears lead found error. steps reproduce paste https mxnet.incubator.apache.org versions master gluon web browser",0,links broken https mxnet.incubator.apache.org versions master gluon,"links broken https mxnet.incubator.apache.org versions master gluon note providing complete information concise form best way get help. issue template serves checklist essential information technical issues bug reports. non technical issues feature requests, feel free present information believe best form. q discussion, please start discussion thread https discuss.mxnet.io description link obtained searching google duckduckgo mxnet gluon https mxnet.incubator.apache.org versions master gluon link however appears lead found error. steps reproduce paste https mxnet.incubator.apache.org versions master gluon web browser"
incubator-mxnet,10476,try use mxnet test seed 1117986172,0,flaky test test operator.test reduce,flaky test test operator.test reduce try use mxnet test seed 1117986172
incubator-mxnet,10919,"gluon.block method accept strings, also pathlib.path types new python 3.",0,gluon.block load params method accept python3 pathlib.path,"gluon.block load params method accept python3 pathlib.path gluon.block method accept strings, also pathlib.path types new python 3."
incubator-mxnet,11165,"use imageiter read jpg files,. batch size set 512. mxnet cpu worker nthreads set 48 use 0.35s per batch. use 0.0618s per batch use imagerecorditer read rec file. imageiter slow? many augmenters called python affect speed? mxnet interface task include read,decode,resize,crop,tanspose,rgb mean c ?",0,imageiter much slower imagerecorditer,"imageiter much slower imagerecorditer use imageiter read jpg files,. batch size set 512. mxnet cpu worker nthreads set 48 use 0.35s per batch. use 0.0618s per batch use imagerecorditer read rec file. imageiter slow? many augmenters called python affect speed? mxnet interface task include read,decode,resize,crop,tanspose,rgb mean c ?"
incubator-mxnet,2968,"trained resnet 50 imanget, top1 error 25.41 top 5 error 7.93 , good, anyone reproduced now? hope somebody release model study task code https github.com tornadomeet resnet, support imagnet cifar training different depth.",0,training resnet imagenet,"training resnet imagenet trained resnet 50 imanget, top1 error 25.41 top 5 error 7.93 , good, anyone reproduced now? hope somebody release model study task code https github.com tornadomeet resnet, support imagnet cifar training different depth."
incubator-mxnet,17109,"scope 1. symbolblock equivalent c c , unify executor implementation symbol module one gluon blocks 2. migrate versions inference api",0,mxnet 2.0 item 3.2 unify executor,"mxnet 2.0 item 3.2 unify executor scope 1. symbolblock equivalent c c , unify executor implementation symbol module one gluon blocks 2. migrate versions inference api"
incubator-mxnet,7684,"try follow custom iterator section's code tutorial page https mxnet.incubator.apache.org tutorials basic data.html. found two line never executed! data mx.nd.array g 1 d,g zip self. provide data, self.data gen label mx.nd.array g 1 d,g zip self. provide label, self.label gen python's version python3.5 mxnet 0.95. could help fix problem?",0,python3.5 mxnet execute loop,"python3.5 mxnet execute loop try follow custom iterator section's code tutorial page https mxnet.incubator.apache.org tutorials basic data.html. found two line never executed! data mx.nd.array g 1 d,g zip self. provide data, self.data gen label mx.nd.array g 1 d,g zip self. provide label, self.label gen python's version python3.5 mxnet 0.95. could help fix problem?"
incubator-mxnet,8542,"note providing complete information concise form best way get help. issue template serves checklist essential information technical issues. issue non technical, feel free present information believe best form. description dependency www.csie.ntu.edu.tw changed hosted s3 bucket. see https github.com apache incubator mxnet blob master tests python unittest test io.py l218",0,remove dependency http www.csie.ntu.edu.tw libsvmiter test,"remove dependency http www.csie.ntu.edu.tw libsvmiter test note providing complete information concise form best way get help. issue template serves checklist essential information technical issues. issue non technical, feel free present information believe best form. description dependency www.csie.ntu.edu.tw changed hosted s3 bucket. see https github.com apache incubator mxnet blob master tests python unittest test io.py l218"
incubator-mxnet,5153,"run end2end captcha recognition ocr example, get segmentation fault. original blog url http blog.xlvector.net 2016 05 mxnet ocr cnn changed devs, gpu cpu, lack hardware gpu main code environment info operating system virtual machine linux mint 18 base ubuntu 16.04 , 64bit compiler gcc 5.4.0 package used python r scala julia python 2.7.12 mxnet version get git master 2017 2 23 using python package, please provide opencv 3.2 error message run pycharm process finished exit code 139 interrupted signal 11 sigsegv gdb py bt traceback recent call first file test g.py , line 56, iter img cv2.imdecode img, cv2.imread color file home mao mxnet python mxnet model.py , line 236, train multi device data batch train data file home mao mxnet python mxnet model.py , line 816, fit sym gen self.sym gen file test g.py , line 135, model.fit x data train, eval data data test, eval metric accuracy, batch end callback mx.callback.speedometer 32, 50 , gdb bt 0 0x0000000000000000 ?? 1 0x00007ffff18149ee cv imdecode cv inputarray const , int usr lib x86 64 linux gnu libopencv highgui.so.2.4 2 0x00007fffd9728f92 pyopencv cv imdecode object , object , object usr lib python2.7 dist packages cv2.so 3 0x00000000004c468a call function oparg , pp stack 0x7fffffffd440 .. python ceval.c 4350 4 pyeval evalframeex .. python ceval.c 2987 5 0x00000000004dddca gen send ex.isra.0.lto priv .. objects genobject.c 85 6 0x00000000004c4c6f pyeval evalframeex .. python ceval.c 2806 7 0x00000000004c2765 pyeval evalcodeex .. python ceval.c 3582 8 0x00000000004ca099 fast function nk 17, na , n , pp stack 0x7fffffffd7e0, func .. python ceval.c 4445 9 call function oparg , pp stack 0x7fffffffd7e0 .. python ceval.c 4370 10 pyeval evalframeex .. python ceval.c 2987 11 0x00000000004c2765 pyeval evalcodeex .. python ceval.c 3582 12 0x00000000004ca099 fast function nk 4, na , n , pp stack 0x7fffffffd9f0, func .. python ceval.c 4445 13 call function oparg , pp stack 0x7fffffffd9f0 type continue, q quit .. python ceval.c 4370 14 pyeval evalframeex .. python ceval.c 2987 15 0x00000000004c2765 pyeval evalcodeex .. python ceval.c 3582 16 0x00000000004c2509 pyeval evalcode co , globals , locals .. python ceval.c 669 17 0x00000000004f1def run mod.lto priv .. python pythonrun.c 1376 18 0x00000000004ec652 pyrun fileexflags .. python pythonrun.c 1362 19 0x00000000004eae31 pyrun simplefileexflags .. python pythonrun.c 948 20 0x000000000049e14a py main .. modules main.c 640 21 0x00007ffff7811830 libc start main main 0x49dab0 , argc 2, argv 0x7fffffffde38, init , fini , rtld fini , stack end 0x7fffffffde28 .. csu libc start.c 291 22 0x000000000049d9d9 start tried solve it? find point interrupte . split part another file, generate img captcha module, decode opencv. run well. run example, mxnet, decode opencv segmentation fault. what's wrong?",0,end2end captcha recognition ocr segmentation fault,"end2end captcha recognition ocr segmentation fault run end2end captcha recognition ocr example, get segmentation fault. original blog url http blog.xlvector.net 2016 05 mxnet ocr cnn changed devs, gpu cpu, lack hardware gpu main code environment info operating system virtual machine linux mint 18 base ubuntu 16.04 , 64bit compiler gcc 5.4.0 package used python r scala julia python 2.7.12 mxnet version get git master 2017 2 23 using python package, please provide opencv 3.2 error message run pycharm process finished exit code 139 interrupted signal 11 sigsegv gdb py bt traceback recent call first file test g.py , line 56, iter img cv2.imdecode img, cv2.imread color file home mao mxnet python mxnet model.py , line 236, train multi device data batch train data file home mao mxnet python mxnet model.py , line 816, fit sym gen self.sym gen file test g.py , line 135, model.fit x data train, eval data data test, eval metric accuracy, batch end callback mx.callback.speedometer 32, 50 , gdb bt 0 0x0000000000000000 ?? 1 0x00007ffff18149ee cv imdecode cv inputarray const , int usr lib x86 64 linux gnu libopencv highgui.so.2.4 2 0x00007fffd9728f92 pyopencv cv imdecode object , object , object usr lib python2.7 dist packages cv2.so 3 0x00000000004c468a call function oparg , pp stack 0x7fffffffd440 .. python ceval.c 4350 4 pyeval evalframeex .. python ceval.c 2987 5 0x00000000004dddca gen send ex.isra.0.lto priv .. objects genobject.c 85 6 0x00000000004c4c6f pyeval evalframeex .. python ceval.c 2806 7 0x00000000004c2765 pyeval evalcodeex .. python ceval.c 3582 8 0x00000000004ca099 fast function nk 17, na , n , pp stack 0x7fffffffd7e0, func .. python ceval.c 4445 9 call function oparg , pp stack 0x7fffffffd7e0 .. python ceval.c 4370 10 pyeval evalframeex .. python ceval.c 2987 11 0x00000000004c2765 pyeval evalcodeex .. python ceval.c 3582 12 0x00000000004ca099 fast function nk 4, na , n , pp stack 0x7fffffffd9f0, func .. python ceval.c 4445 13 call function oparg , pp stack 0x7fffffffd9f0 type continue, q quit .. python ceval.c 4370 14 pyeval evalframeex .. python ceval.c 2987 15 0x00000000004c2765 pyeval evalcodeex .. python ceval.c 3582 16 0x00000000004c2509 pyeval evalcode co , globals , locals .. python ceval.c 669 17 0x00000000004f1def run mod.lto priv .. python pythonrun.c 1376 18 0x00000000004ec652 pyrun fileexflags .. python pythonrun.c 1362 19 0x00000000004eae31 pyrun simplefileexflags .. python pythonrun.c 948 20 0x000000000049e14a py main .. modules main.c 640 21 0x00007ffff7811830 libc start main main 0x49dab0 , argc 2, argv 0x7fffffffde38, init , fini , rtld fini , stack end 0x7fffffffde28 .. csu libc start.c 291 22 0x000000000049d9d9 start tried solve it? find point interrupte . split part another file, generate img captcha module, decode opencv. run well. run example, mxnet, decode opencv segmentation fault. what's wrong?"
incubator-mxnet,8969,"couple issues incubator mxnet example image classification benchmark score.py. prevent script running. issue 1 def get symbol network, batch size image shape 3,299,299 network 'inception v3' else 3,224,224 num layers 0 get symbol vgg16 accept zero value number layers. quick fix insert num layers 0 network 'vgg' num layers 16 use solution resnet split extract number layers issue 2 devs mx.gpu 0 len get gpus 0 else get gpus relies nvidia smi installed machines gpus say, c4 instances aws .",0,couple issues benchmark score.py,"couple issues benchmark score.py couple issues incubator mxnet example image classification benchmark score.py. prevent script running. issue 1 def get symbol network, batch size image shape 3,299,299 network 'inception v3' else 3,224,224 num layers 0 get symbol vgg16 accept zero value number layers. quick fix insert num layers 0 network 'vgg' num layers 16 use solution resnet split extract number layers issue 2 devs mx.gpu 0 len get gpus 0 else get gpus relies nvidia smi installed machines gpus say, c4 instances aws ."
incubator-mxnet,2555,recent problem... perhaps due recent changes code.,0,oserror exception stack overflow,oserror exception stack overflow recent problem... perhaps due recent changes code.
incubator-mxnet,10012,top level readme.md two links logos point builds.apache.org deprecated. must point new ci fixed similar pr https github.com apache incubator mxnet pull 9908,0,bug two broken links top level readme.md file point old ci,bug two broken links top level readme.md file point old ci top level readme.md two links logos point builds.apache.org deprecated. must point new ci fixed similar pr https github.com apache incubator mxnet pull 9908
incubator-mxnet,11708,"unit test title using fixed seed mask flakiness. suggested action 1. evaluate whether test flaky without fixed seed. not, remove seed. else move 2 2. test flaky, determine whether actual uncaught edge case. so, fix operator. else move 3 3. numerical instability inevitable, adjust tolerance level appropriately.",0,test ndarray.test cached fixed seed mask flakiness,"test ndarray.test cached fixed seed mask flakiness unit test title using fixed seed mask flakiness. suggested action 1. evaluate whether test flaky without fixed seed. not, remove seed. else move 2 2. test flaky, determine whether actual uncaught edge case. so, fix operator. else move 3 3. numerical instability inevitable, adjust tolerance level appropriately."
incubator-mxnet,12377,http jenkins.mxnet ci.amazon ml.com blue organizations jenkins incubator mxnet detail master 1529 pipeline,0,flaky test test mkldnn.test activation,flaky test test mkldnn.test activation http jenkins.mxnet ci.amazon ml.com blue organizations jenkins incubator mxnet detail master 1529 pipeline
incubator-mxnet,9758,"want write custom operation python, output shape unknown running 'forward '. write 'infer shape ' function? seems must infer output shape input shape, without input data.",0,unknown output shape inference,"unknown output shape inference want write custom operation python, output shape unknown running 'forward '. write 'infer shape ' function? seems must infer output shape input shape, without input data."
incubator-mxnet,1306,"hello everyone, datasets mnist, used cnn algorithm practice ml. found reference tutorial, page 6 7 http web.pdx.edu jduh courses archive geog481w07 students ludwig imageconvolution.pdf . guess default kernel '1' instances matrix. make smoothly kernel like slide. mxnet code r. best, salmon.",0,make smooth kernel convolution neural networks?,"make smooth kernel convolution neural networks? hello everyone, datasets mnist, used cnn algorithm practice ml. found reference tutorial, page 6 7 http web.pdx.edu jduh courses archive geog481w07 students ludwig imageconvolution.pdf . guess default kernel '1' instances matrix. make smoothly kernel like slide. mxnet code r. best, salmon."
incubator-mxnet,5590,appears calling one callback seem destroy value batchendparam. using callbacks like results following log info root epoch 0 batch 10 speed 60.70 samples sec train accuracy 0.122869 info root epoch 0 batch 20 speed 60.85 samples sec train accuracy 0.157031 info root epoch 0 batch 20 speed 61.00 samples sec train accuracy nan info root epoch 0 batch 30 speed 61.01 samples sec train accuracy 0.164844 info root epoch 0 batch 40 speed 61.20 samples sec train accuracy 0.191406 info root epoch 0 batch 40 speed 61.39 samples sec train accuracy nan info root epoch 0 batch 50 speed 60.78 samples sec train accuracy 0.194531 info root epoch 0 batch 60 speed 60.84 samples sec train accuracy 0.221875 info root epoch 0 batch 60 speed 60.90 samples sec train accuracy nan,0,callbacks destroy 'value' batchendparam subsequent callbacks type,callbacks destroy 'value' batchendparam subsequent callbacks type appears calling one callback seem destroy value batchendparam. using callbacks like results following log info root epoch 0 batch 10 speed 60.70 samples sec train accuracy 0.122869 info root epoch 0 batch 20 speed 60.85 samples sec train accuracy 0.157031 info root epoch 0 batch 20 speed 61.00 samples sec train accuracy nan info root epoch 0 batch 30 speed 61.01 samples sec train accuracy 0.164844 info root epoch 0 batch 40 speed 61.20 samples sec train accuracy 0.191406 info root epoch 0 batch 40 speed 61.39 samples sec train accuracy nan info root epoch 0 batch 50 speed 60.78 samples sec train accuracy 0.194531 info root epoch 0 batch 60 speed 60.84 samples sec train accuracy 0.221875 info root epoch 0 batch 60 speed 60.90 samples sec train accuracy nan
incubator-mxnet,11897,"want fixed layer's parameter befor 'fc7'layer,and change code like run using pretrained model ,and save checkpoint one epoch,,,i compare model pretrained model,,,i find layers' parameters 'fc7'layer also changed,,,i know why,,,can anyone please help me.",0,fixed param names module use,"fixed param names module use want fixed layer's parameter befor 'fc7'layer,and change code like run using pretrained model ,and save checkpoint one epoch,,,i compare model pretrained model,,,i find layers' parameters 'fc7'layer also changed,,,i know why,,,can anyone please help me."
incubator-mxnet,13274,"description r test failing intermitently environment info required test failing ci could flaky r user, please provide r http jenkins.mxnet ci.amazon ml.com blue organizations jenkins incubator mxnet detail pr 13266 1 pipeline observing behavior multiple prs",0,r test failing intermitently,"r test failing intermitently description r test failing intermitently environment info required test failing ci could flaky r user, please provide r http jenkins.mxnet ci.amazon ml.com blue organizations jenkins incubator mxnet detail pr 13266 1 pipeline observing behavior multiple prs"
incubator-mxnet,3758,someone successfully installed gpu version mxnet gpu version r could please post instructions....the instructions install work...,0,windows 10 pro mxnet gpu r environment,windows 10 pro mxnet gpu r environment someone successfully installed gpu version mxnet gpu version r could please post instructions....the instructions install work...
incubator-mxnet,8042,"hai, utility converts network written mxnet .json format mxnet symbols level code? options available kindly let us know.",0,convert network written json format mxnet symbol code?,"convert network written json format mxnet symbol code? hai, utility converts network written mxnet .json format mxnet symbols level code? options available kindly let us know."
incubator-mxnet,2732,limit using use one cpu core multicore machine r?,0,r limit mxnet one cpu core?,r limit mxnet one cpu core? limit using use one cpu core multicore machine r?
incubator-mxnet,4029,"hi, using cnn r perform image classification k different classes. however, want try another approach one vs classification label, one label would 1 rest would 0. question is, output layer use? use mx.symbol.softmaxoutput fullyconnected layer one neuron useless always predict 1 even nan every example. linearregressionoutput gives similar result. basic idea need output layer performs 0.5 threshold single output neuron last fullyconnected layer. sorry dumb question, still new field. thanks advanced.",0,one vs classification,"one vs classification hi, using cnn r perform image classification k different classes. however, want try another approach one vs classification label, one label would 1 rest would 0. question is, output layer use? use mx.symbol.softmaxoutput fullyconnected layer one neuron useless always predict 1 even nan every example. linearregressionoutput gives similar result. basic idea need output layer performs 0.5 threshold single output neuron last fullyconnected layer. sorry dumb question, still new field. thanks advanced."
incubator-mxnet,2506,exception unknown layer batchnorm!,0,caffe converter resnet error batchnorm,caffe converter resnet error batchnorm exception unknown layer batchnorm!
incubator-mxnet,16937,description weird problem expected output deepnumpy outputs,0,numpy zero size tensor add zero size sum raises exception,numpy zero size tensor add zero size sum raises exception description weird problem expected output deepnumpy outputs
incubator-mxnet,715,"say like binary classification example , instance sparse .",0,could give one text classification example ?,"could give one text classification example ? say like binary classification example , instance sparse ."
incubator-mxnet,919,"convolution, sometimes need image small sparse places. sparse operation avoid unnecessary calculations. improve performance arm cpu drastically.",0,sparse operators formal schedule,"sparse operators formal schedule convolution, sometimes need image small sparse places. sparse operation avoid unnecessary calculations. improve performance arm cpu drastically."
incubator-mxnet,2763,"hi all, want get shape certain symbol, take look following code get shape ? know symbol, one use get shape symbols. situation, return list shapes, respect symbol node compute symbol graph, say. want get shape , order uncertain, one get expected symbol shape wants. solutions problem?",0,get shape certain symbol,"get shape certain symbol hi all, want get shape certain symbol, take look following code get shape ? know symbol, one use get shape symbols. situation, return list shapes, respect symbol node compute symbol graph, say. want get shape , order uncertain, one get expected symbol shape wants. solutions problem?"
incubator-mxnet,5258,"bugs installation issues, please provide following information. information provide, likely people able help you. environment info operating system ubuntu15.10 compiler gcc 4.9x package used python r scala julia python mxnet version 0.9.4 installed source yes mxnet commit hash be38c5b84030a63d0ab51f19737f99a75a7feb23 using python package, please provide python version distribution python2.7 using r package, please provide r error message please paste full error message, including stack trace. minimum reproducible example using code, please provide short script reproduces error. following code caption module use pretrain vgg extract fc layer ouputs, input caption module name , create custom dataiter, process vgg copy . however, raises error time. steps reproduce running standard examples, please provide commands run lead error. 1. 2. 3. tried solve it? 1.try pop item 2.use module api, bind symbol. 3.",0,embedding layer support calculate data gradient,"embedding layer support calculate data gradient bugs installation issues, please provide following information. information provide, likely people able help you. environment info operating system ubuntu15.10 compiler gcc 4.9x package used python r scala julia python mxnet version 0.9.4 installed source yes mxnet commit hash be38c5b84030a63d0ab51f19737f99a75a7feb23 using python package, please provide python version distribution python2.7 using r package, please provide r error message please paste full error message, including stack trace. minimum reproducible example using code, please provide short script reproduces error. following code caption module use pretrain vgg extract fc layer ouputs, input caption module name , create custom dataiter, process vgg copy . however, raises error time. steps reproduce running standard examples, please provide commands run lead error. 1. 2. 3. tried solve it? 1.try pop item 2.use module api, bind symbol. 3."
incubator-mxnet,12523,"description minor problems scala intellij tutorial https mxnet.incubator.apache.org tutorials scala mxnet scala intellij.html code https github.com apache incubator mxnet blob master docs tutorials scala mxnet scala intellij.md could fixed improve experience new users following tutorial. issues added mxnet package dependency pom.xml copying xml tutorial, package version specified 1.2.0 could found intellij notification error . updating version 1.2.1 fixed issue. initial code, sample specification file specs.scala missing dependency. causes error trying run app step 6 running, following warning shows able resolve adding org.slf4j api log4j12 dependencies tutorial additionally include file target classes log4j.properties might worth including note users building mxnet scala source use scala mxnet built opposed version maven changing pom.xml. systempath including user , looks like environment info required floatnp.floatingnp.float64 np.dtype float .type package used python r scala julia scala scala user, please provide 1. java version 2. maven version 3. scala runtime applicable specified pom.xml 2.11.8 scala.compat.version 2.11 mxnet commit hash ac4ef212f6269469f3f3827da49e43fb42f1398f mxnet label bot doc, scala",0,updates tutorial run mxnet scala examples using intellij ide macos,"updates tutorial run mxnet scala examples using intellij ide macos description minor problems scala intellij tutorial https mxnet.incubator.apache.org tutorials scala mxnet scala intellij.html code https github.com apache incubator mxnet blob master docs tutorials scala mxnet scala intellij.md could fixed improve experience new users following tutorial. issues added mxnet package dependency pom.xml copying xml tutorial, package version specified 1.2.0 could found intellij notification error . updating version 1.2.1 fixed issue. initial code, sample specification file specs.scala missing dependency. causes error trying run app step 6 running, following warning shows able resolve adding org.slf4j api log4j12 dependencies tutorial additionally include file target classes log4j.properties might worth including note users building mxnet scala source use scala mxnet built opposed version maven changing pom.xml. systempath including user , looks like environment info required floatnp.floatingnp.float64 np.dtype float .type package used python r scala julia scala scala user, please provide 1. java version 2. maven version 3. scala runtime applicable specified pom.xml 2.11.8 scala.compat.version 2.11 mxnet commit hash ac4ef212f6269469f3f3827da49e43fb42f1398f mxnet label bot doc, scala"
incubator-mxnet,7522,"hi dear mxnet users developers understand example use intermediate layer output prediction. however, training validation, possible use intermediate layer output calculate evaluation metric? thanks lot",0,calculate evaluation metric using intermediate layer output?,"calculate evaluation metric using intermediate layer output? hi dear mxnet users developers understand example use intermediate layer output prediction. however, training validation, possible use intermediate layer output calculate evaluation metric? thanks lot"
incubator-mxnet,11735,"unit test title using fixed seed mask flakiness. suggested action 1. evaluate whether test flaky without fixed seed. not, remove seed. else move 2 2. test flaky, determine whether actual uncaught edge case. so, fix operator. else move 3 3. numerical instability inevitable, adjust tolerance level appropriately.",0,test optimizer.test ftml fixed seed mask flakiness,"test optimizer.test ftml fixed seed mask flakiness unit test title using fixed seed mask flakiness. suggested action 1. evaluate whether test flaky without fixed seed. not, remove seed. else move 2 2. test flaky, determine whether actual uncaught edge case. so, fix operator. else move 3 3. numerical instability inevitable, adjust tolerance level appropriately."
incubator-mxnet,3096,"tried using part ami data, using default.cfg transform file like got error changed transform file following error reported modifed default.cfg another error reported please show set ydim, thanks.",0,set parameter speech demo,"set parameter speech demo tried using part ami data, using default.cfg transform file like got error changed transform file following error reported modifed default.cfg another error reported please show set ydim, thanks."
incubator-mxnet,4471,"going much inconveniences tensorflow theano, trying mxnet. read mxnet api, control flow operation, i.e., functionally equivalent tf.select tf.cond theano.switch theano.ifelse . missing functionality limit toolkit's capability handling variable length sequences. take note without control flow operators, still perform lstm based variable sequence length prediction using masking. however, still something cannot do. example, training rnn language model, would like reset state vector specific trainable parameter whenever encounter sentence begin, i.e., . hereby hope developer implement important function. thanks!",0,lack control flow operators functionally inferior tensorflow theano,"lack control flow operators functionally inferior tensorflow theano going much inconveniences tensorflow theano, trying mxnet. read mxnet api, control flow operation, i.e., functionally equivalent tf.select tf.cond theano.switch theano.ifelse . missing functionality limit toolkit's capability handling variable length sequences. take note without control flow operators, still perform lstm based variable sequence length prediction using masking. however, still something cannot do. example, training rnn language model, would like reset state vector specific trainable parameter whenever encounter sentence begin, i.e., . hereby hope developer implement important function. thanks!"
incubator-mxnet,5370,"hello, using mxnet r package walking following tutorial http mxnet.io tutorials r charrnnmodel.html see lstm trained used generate text. however, way use lstm model evaluate test text? using language models, use perplexity evaluate test data. lower perplexity, closer language model test data. higher perplexity, farther away language model test data. equivalent score generated using lstm? thanks, aarthi environment info operating system macos compiler package used python r scala julia r r r version 3.3.2 2016 10 31 platform x86 64 apple darwin13.4.0 64 bit running macos sierra 10.12.3 locale 1 en us.utf 8 en us.utf 8 en us.utf 8 c en us.utf 8 en us.utf 8 attached base packages 1 stats graphics grdevices utils datasets methods base attached packages 1 mxnet 0.9.4 h2o 3.10.2.2 ggplot2 2.2.1 plyr 1.8.4 loaded via namespace attached 1 rcpp 0.12.7 knitr 1.15.1 magrittr 1.5 devtools 1.12.0 5 munsell 0.4.3 colorspace 1.2 7 r6 2.2.0 stringr 1.1.0 9 httr 1.2.1 visnetwork 1.0.3 tools 3.3.2 drat 0.1.2 13 grid 3.3.2 gtable 0.2.0 git2r 0.18.0 withr 1.0.2 17 htmltools 0.3.5 lazyeval 0.2.0 assertthat 0.1 digest 0.6.10 21 tibble 1.2 codetools 0.2 15 htmlwidgets 0.7 bitops 1.0 6 25 rcurl 1.95 4.8 curl 2.2 memoise 1.0.0 labeling 0.3 29 stringi 1.1.2 scales 0.4.1 jsonlite 1.1 error message error message",0,lstm mxnet perplexity score,"lstm mxnet perplexity score hello, using mxnet r package walking following tutorial http mxnet.io tutorials r charrnnmodel.html see lstm trained used generate text. however, way use lstm model evaluate test text? using language models, use perplexity evaluate test data. lower perplexity, closer language model test data. higher perplexity, farther away language model test data. equivalent score generated using lstm? thanks, aarthi environment info operating system macos compiler package used python r scala julia r r r version 3.3.2 2016 10 31 platform x86 64 apple darwin13.4.0 64 bit running macos sierra 10.12.3 locale 1 en us.utf 8 en us.utf 8 en us.utf 8 c en us.utf 8 en us.utf 8 attached base packages 1 stats graphics grdevices utils datasets methods base attached packages 1 mxnet 0.9.4 h2o 3.10.2.2 ggplot2 2.2.1 plyr 1.8.4 loaded via namespace attached 1 rcpp 0.12.7 knitr 1.15.1 magrittr 1.5 devtools 1.12.0 5 munsell 0.4.3 colorspace 1.2 7 r6 2.2.0 stringr 1.1.0 9 httr 1.2.1 visnetwork 1.0.3 tools 3.3.2 drat 0.1.2 13 grid 3.3.2 gtable 0.2.0 git2r 0.18.0 withr 1.0.2 17 htmltools 0.3.5 lazyeval 0.2.0 assertthat 0.1 digest 0.6.10 21 tibble 1.2 codetools 0.2 15 htmlwidgets 0.7 bitops 1.0 6 25 rcurl 1.95 4.8 curl 2.2 memoise 1.0.0 labeling 0.3 29 stringi 1.1.2 scales 0.4.1 jsonlite 1.1 error message error message"
incubator-mxnet,13080,failure occurring jenkins scala gpu task. failure occurring running make scalapkg build core module. one sample error ending identified number jenkins runs produced http jenkins.mxnet ci.amazon ml.com blue organizations jenkins incubator mxnet detail pr 13077 1 pipeline http jenkins.mxnet ci.amazon ml.com blue organizations jenkins incubator mxnet detail pr 13071 1 pipeline http jenkins.mxnet ci.amazon ml.com blue organizations jenkins incubator mxnet detail pr 13052 2 pipeline problem also observed dev jenkins http jenkins.mxnet ci dev.amazon ml.com blue organizations jenkins restricted publish artifacts detail automate maven 61 pipeline,0,jenkins failing scala gpu,jenkins failing scala gpu failure occurring jenkins scala gpu task. failure occurring running make scalapkg build core module. one sample error ending identified number jenkins runs produced http jenkins.mxnet ci.amazon ml.com blue organizations jenkins incubator mxnet detail pr 13077 1 pipeline http jenkins.mxnet ci.amazon ml.com blue organizations jenkins incubator mxnet detail pr 13071 1 pipeline http jenkins.mxnet ci.amazon ml.com blue organizations jenkins incubator mxnet detail pr 13052 2 pipeline problem also observed dev jenkins http jenkins.mxnet ci dev.amazon ml.com blue organizations jenkins restricted publish artifacts detail automate maven 61 pipeline
incubator-mxnet,6600,"trying run converted caffe model, resnet 50 model trained set images. converted model using caffe converter recent master branch mxnet. however, trying run scala bindings, following problem. seems found scala library see . note could run vgg 16 model, edits. environment info operating system mac osx el capitan 10.11.5 compiler sbt package used python r scala julia scala mxnet version using maven respositories https mvnrepository.com error message please paste full error message, including stack trace. minimum reproducible example using code, please provide short script reproduces error. basically, layer model definition json produces problem full model definition, resnet 50 layer model, tried solve it? 1. tried run published jar, problem persists. 2. checked scala library, seem .",0,missing broadcast add layer mxnet scala binding,"missing broadcast add layer mxnet scala binding trying run converted caffe model, resnet 50 model trained set images. converted model using caffe converter recent master branch mxnet. however, trying run scala bindings, following problem. seems found scala library see . note could run vgg 16 model, edits. environment info operating system mac osx el capitan 10.11.5 compiler sbt package used python r scala julia scala mxnet version using maven respositories https mvnrepository.com error message please paste full error message, including stack trace. minimum reproducible example using code, please provide short script reproduces error. basically, layer model definition json produces problem full model definition, resnet 50 layer model, tried solve it? 1. tried run published jar, problem persists. 2. checked scala library, seem ."
incubator-mxnet,12843,following code work. generates error,0,mxnet work scipy int64,mxnet work scipy int64 following code work. generates error
incubator-mxnet,5028,"implemented model python sucessfully error arises c param symbol.json 278523 bytes param 0020.params 38367630 bytes 15 32 30 e opensource mxnet0.9.3 mxnet src nnvm legacy json util.cc 153 load ing symbol saved previous version v0.9.1. attempting upgrade... 15 32 39 e opensource mxnet0.9.3 mxnet nnvm include dmlc logging.h 300 15 3 2 39 e opensource mxnet0.9.3 mxnet src operator . softmax output inl.h 307 ch eck failed type dtype 4 vs. 0 layer requires uniform type. e xpected 0 v.s. given 4 label assertion failed pred hnd, file e opensource mxnet0.9.3 mxnet example image cl assification predict cpp image classification predict.cc, line 227 code fix it?",0,"want implement cnn lstm ctc c instead python, error arises","want implement cnn lstm ctc c instead python, error arises implemented model python sucessfully error arises c param symbol.json 278523 bytes param 0020.params 38367630 bytes 15 32 30 e opensource mxnet0.9.3 mxnet src nnvm legacy json util.cc 153 load ing symbol saved previous version v0.9.1. attempting upgrade... 15 32 39 e opensource mxnet0.9.3 mxnet nnvm include dmlc logging.h 300 15 3 2 39 e opensource mxnet0.9.3 mxnet src operator . softmax output inl.h 307 ch eck failed type dtype 4 vs. 0 layer requires uniform type. e xpected 0 v.s. given 4 label assertion failed pred hnd, file e opensource mxnet0.9.3 mxnet example image cl assification predict cpp image classification predict.cc, line 227 code fix it?"
incubator-mxnet,4539,"know best place communicate this, tried copy pasting scala example http mxnet.io tutorials scala mnist.html website compile. got compile what's described below. sure whether right fix later get core dumped post separate issue . heads guys aware problem website. environment info operating system ubuntu 16.10 amd64 compiler sbt package used python r scala julia scala package scalaversion 2.11.8 mxnet version ml.dmlc.mxnet mxnet full 2.10 linux x86 64 gpu 0.1.1 error message minimum reproducible example steps reproduce running standard examples, please provide commands run lead error. 1. clone https github.com danimateos mxnet playground tree broken 2. tried solve it? removed got compile.",0,scala example website compile,"scala example website compile know best place communicate this, tried copy pasting scala example http mxnet.io tutorials scala mnist.html website compile. got compile what's described below. sure whether right fix later get core dumped post separate issue . heads guys aware problem website. environment info operating system ubuntu 16.10 amd64 compiler sbt package used python r scala julia scala package scalaversion 2.11.8 mxnet version ml.dmlc.mxnet mxnet full 2.10 linux x86 64 gpu 0.1.1 error message minimum reproducible example steps reproduce running standard examples, please provide commands run lead error. 1. clone https github.com danimateos mxnet playground tree broken 2. tried solve it? removed got compile."
incubator-mxnet,4638,"use sgd momentum, train 10 epoches first save parameters, state created mx.optimizer.updater gradient updates momentum saved, want train model 10th epoch, state initialized zeros, want, want know whether solution problem? thanks",0,issue using sgd momentum,"issue using sgd momentum use sgd momentum, train 10 epoches first save parameters, state created mx.optimizer.updater gradient updates momentum saved, want train model 10th epoch, state initialized zeros, want, want know whether solution problem? thanks"
incubator-mxnet,4303,test pad scala code iosuite.scala ndarrayiter hasnext false size 32,0,ndarrayiter hasnext,ndarrayiter hasnext test pad scala code iosuite.scala ndarrayiter hasnext false size 32
incubator-mxnet,3086,"r, wasted hours figuring labels zero indexed, r natural assume everything 1 indexed. attached file, labels 0 indexed, predictions appear inaccurate. suggest documentation 'mx.mlp' 'mx.model.feedforward.create' others use softmax included something saying 'y' 'label' zero indexed case softmax r.txt https github.com dmlc mxnet files 429063 r.txt",0,"r documentation, remind user softmax labels 0 indexed","r documentation, remind user softmax labels 0 indexed r, wasted hours figuring labels zero indexed, r natural assume everything 1 indexed. attached file, labels 0 indexed, predictions appear inaccurate. suggest documentation 'mx.mlp' 'mx.model.feedforward.create' others use softmax included something saying 'y' 'label' zero indexed case softmax r.txt https github.com dmlc mxnet files 429063 r.txt"
incubator-mxnet,11430,"description problem happened self created dataset lstm bucketing example cpu dataset rnn time major works good environment info required floatnp.floatingnp.float64 np.dtype float .type package used python r scala julia python mxnet commit hash 7c1acb489a2d7546ffac55f5c2764f1e92e77306 build config paste content config.mk, build command. error message anaconda3 lib python3.6 site packages h5py init .py 36 futurewarning conversion second argument issubdtype deprecated. future, treated . . conv import register converters register converters warning discarded 0 sentences longer largest bucket. warning discarded 0 sentences longer largest bucket. traceback recent call last file anaconda3 lib python3.6 site packages mxnet symbol symbol.py , line 1521, simple bind ctypes.byref exe handle file anaconda3 lib python3.6 site packages mxnet base.py , line 210, check call raise mxneterror py str lib.mxgetlasterror mxnet.base.mxneterror error operator split0 13 51 38 src operator . slice channel inl.h 208 check failed dshape real axis param .num outputs 0u 10 vs. 0 trying split 1 th axis input tensor shape 32,30,200 num outputs 20 evenly sized chunks, possible 20 evenly divide 30 stack trace returned 10 entries bt 0 0 libmxnet.so 0x000000011c0dcab4 libmxnet.so 19124 bt 1 1 libmxnet.so 0x000000011c0dc86f libmxnet.so 18543 bt 2 2 libmxnet.so 0x000000011d6873f1 mxtvmbridge 3552369 bt 3 3 libmxnet.so 0x000000011d3170ce mxndlistfree 1644494 bt 4 4 libmxnet.so 0x000000011d1d477a mxndlistfree 323194 bt 5 5 libmxnet.so 0x000000011d1cce8c mxndlistfree 292236 bt 6 6 libmxnet.so 0x000000011d1bf8d6 mxndlistfree 237526 bt 7 7 libmxnet.so 0x000000011d1c544a mxndlistfree 260938 bt 8 8 libmxnet.so 0x000000011d151e40 mxexecutorsimplebind 8656 bt 9 9 libffi.6.dylib 0x000000010dea8884 ffi call unix64 76 handling exception, another exception occurred traceback recent call last file lstm bucketing.py , line 126, batch end callback mx.callback.speedometer args.batch size, args.disp batches, auto reset false file anaconda3 lib python3.6 site packages mxnet module base module.py , line 515, fit self.forward backward data batch file anaconda3 lib python3.6 site packages mxnet module base module.py , line 194, forward backward self.forward data batch, train true file anaconda3 lib python3.6 site packages mxnet module bucketing module.py , line 455, forward data batch.provide label file anaconda3 lib python3.6 site packages mxnet module bucketing module.py , line 376, switch bucket force rebind false, shared module self. buckets self. default bucket key file anaconda3 lib python3.6 site packages mxnet module module.py , line 430, bind state names self. state names file anaconda3 lib python3.6 site packages mxnet module executor group.py , line 279, init self.bind exec data shapes, label shapes, shared group file anaconda3 lib python3.6 site packages mxnet module executor group.py , line 375, bind exec shared group file anaconda3 lib python3.6 site packages mxnet module executor group.py , line 662, bind ith exec shared buffer shared data arrays, input shapes file anaconda3 lib python3.6 site packages mxnet symbol symbol.py , line 1527, simple bind raise runtimeerror error msg runtimeerror simple bind error. arguments data 32, 30 softmax label 32, 30 error operator split0 13 51 38 src operator . slice channel inl.h 208 check failed dshape real axis param .num outputs 0u 10 vs. 0 trying split 1 th axis input tensor shape 32,30,200 num outputs 20 evenly sized chunks, possible 20 evenly divide 30 stack trace returned 10 entries bt 0 0 libmxnet.so 0x000000011c0dcab4 libmxnet.so 19124 bt 1 1 libmxnet.so 0x000000011c0dc86f libmxnet.so 18543 bt 2 2 libmxnet.so 0x000000011d6873f1 mxtvmbridge 3552369 bt 3 3 libmxnet.so 0x000000011d3170ce mxndlistfree 1644494 bt 4 4 libmxnet.so 0x000000011d1d477a mxndlistfree 323194 bt 5 5 libmxnet.so 0x000000011d1cce8c mxndlistfree 292236 bt 6 6 libmxnet.so 0x000000011d1bf8d6 mxndlistfree 237526 bt 7 7 libmxnet.so 0x000000011d1c544a mxndlistfree 260938 bt 8 8 libmxnet.so 0x000000011d151e40 mxexecutorsimplebind 8656 bt 9 9 libffi.6.dylib 0x000000010dea8884 ffi call unix64 76 steps reproduce paste commands ran produced error. 1. python lstm bucketing.py",0,lstm bucketing example evenly divide problem self create dataset,"lstm bucketing example evenly divide problem self create dataset description problem happened self created dataset lstm bucketing example cpu dataset rnn time major works good environment info required floatnp.floatingnp.float64 np.dtype float .type package used python r scala julia python mxnet commit hash 7c1acb489a2d7546ffac55f5c2764f1e92e77306 build config paste content config.mk, build command. error message anaconda3 lib python3.6 site packages h5py init .py 36 futurewarning conversion second argument issubdtype deprecated. future, treated . . conv import register converters register converters warning discarded 0 sentences longer largest bucket. warning discarded 0 sentences longer largest bucket. traceback recent call last file anaconda3 lib python3.6 site packages mxnet symbol symbol.py , line 1521, simple bind ctypes.byref exe handle file anaconda3 lib python3.6 site packages mxnet base.py , line 210, check call raise mxneterror py str lib.mxgetlasterror mxnet.base.mxneterror error operator split0 13 51 38 src operator . slice channel inl.h 208 check failed dshape real axis param .num outputs 0u 10 vs. 0 trying split 1 th axis input tensor shape 32,30,200 num outputs 20 evenly sized chunks, possible 20 evenly divide 30 stack trace returned 10 entries bt 0 0 libmxnet.so 0x000000011c0dcab4 libmxnet.so 19124 bt 1 1 libmxnet.so 0x000000011c0dc86f libmxnet.so 18543 bt 2 2 libmxnet.so 0x000000011d6873f1 mxtvmbridge 3552369 bt 3 3 libmxnet.so 0x000000011d3170ce mxndlistfree 1644494 bt 4 4 libmxnet.so 0x000000011d1d477a mxndlistfree 323194 bt 5 5 libmxnet.so 0x000000011d1cce8c mxndlistfree 292236 bt 6 6 libmxnet.so 0x000000011d1bf8d6 mxndlistfree 237526 bt 7 7 libmxnet.so 0x000000011d1c544a mxndlistfree 260938 bt 8 8 libmxnet.so 0x000000011d151e40 mxexecutorsimplebind 8656 bt 9 9 libffi.6.dylib 0x000000010dea8884 ffi call unix64 76 handling exception, another exception occurred traceback recent call last file lstm bucketing.py , line 126, batch end callback mx.callback.speedometer args.batch size, args.disp batches, auto reset false file anaconda3 lib python3.6 site packages mxnet module base module.py , line 515, fit self.forward backward data batch file anaconda3 lib python3.6 site packages mxnet module base module.py , line 194, forward backward self.forward data batch, train true file anaconda3 lib python3.6 site packages mxnet module bucketing module.py , line 455, forward data batch.provide label file anaconda3 lib python3.6 site packages mxnet module bucketing module.py , line 376, switch bucket force rebind false, shared module self. buckets self. default bucket key file anaconda3 lib python3.6 site packages mxnet module module.py , line 430, bind state names self. state names file anaconda3 lib python3.6 site packages mxnet module executor group.py , line 279, init self.bind exec data shapes, label shapes, shared group file anaconda3 lib python3.6 site packages mxnet module executor group.py , line 375, bind exec shared group file anaconda3 lib python3.6 site packages mxnet module executor group.py , line 662, bind ith exec shared buffer shared data arrays, input shapes file anaconda3 lib python3.6 site packages mxnet symbol symbol.py , line 1527, simple bind raise runtimeerror error msg runtimeerror simple bind error. arguments data 32, 30 softmax label 32, 30 error operator split0 13 51 38 src operator . slice channel inl.h 208 check failed dshape real axis param .num outputs 0u 10 vs. 0 trying split 1 th axis input tensor shape 32,30,200 num outputs 20 evenly sized chunks, possible 20 evenly divide 30 stack trace returned 10 entries bt 0 0 libmxnet.so 0x000000011c0dcab4 libmxnet.so 19124 bt 1 1 libmxnet.so 0x000000011c0dc86f libmxnet.so 18543 bt 2 2 libmxnet.so 0x000000011d6873f1 mxtvmbridge 3552369 bt 3 3 libmxnet.so 0x000000011d3170ce mxndlistfree 1644494 bt 4 4 libmxnet.so 0x000000011d1d477a mxndlistfree 323194 bt 5 5 libmxnet.so 0x000000011d1cce8c mxndlistfree 292236 bt 6 6 libmxnet.so 0x000000011d1bf8d6 mxndlistfree 237526 bt 7 7 libmxnet.so 0x000000011d1c544a mxndlistfree 260938 bt 8 8 libmxnet.so 0x000000011d151e40 mxexecutorsimplebind 8656 bt 9 9 libffi.6.dylib 0x000000010dea8884 ffi call unix64 76 steps reproduce paste commands ran produced error. 1. python lstm bucketing.py"
incubator-mxnet,14112,line 33 msvc ? mkldnnvc,0,cmakelists.txt,cmakelists.txt line 33 msvc ? mkldnnvc
incubator-mxnet,5900,"trying use crossentropyloss layer output, get error saying needs 2 inputs. data label . instead use softmaxoutput, need provide data, label provided via training iterator. way use cross entropy loss providing data? minimum reproducible example depth 3 data mx.sym.variable 'data' layer 1 defined net mx.sym.fullyconnected data data,name 'fc1',num hidden 64 net mx.sym.activation data net,name 'ac1',act type relu net mx.sym.dropout data net,name 'dl1',p 0.1 remaining hidden layers layer xrange 2,depth 1 net mx.sym.fullyconnected data net,name 'fc' str layer ,num hidden 64 net mx.sym.activation data net,name 'ac' str layer ,act type relu net mx.sym.dropout data net,name 'dl' str layer ,p 0.1 output layer net mx.sym.fullyconnected data net,name 'fcout',num hidden 10 net mx.sym.softmaxoutput data net,name 'softmax' net rt.crossentropyloss data net return net uncommenting second last line commenting line gives error",0,using crossentropyloss output layer,"using crossentropyloss output layer trying use crossentropyloss layer output, get error saying needs 2 inputs. data label . instead use softmaxoutput, need provide data, label provided via training iterator. way use cross entropy loss providing data? minimum reproducible example depth 3 data mx.sym.variable 'data' layer 1 defined net mx.sym.fullyconnected data data,name 'fc1',num hidden 64 net mx.sym.activation data net,name 'ac1',act type relu net mx.sym.dropout data net,name 'dl1',p 0.1 remaining hidden layers layer xrange 2,depth 1 net mx.sym.fullyconnected data net,name 'fc' str layer ,num hidden 64 net mx.sym.activation data net,name 'ac' str layer ,act type relu net mx.sym.dropout data net,name 'dl' str layer ,p 0.1 output layer net mx.sym.fullyconnected data net,name 'fcout',num hidden 10 net mx.sym.softmaxoutput data net,name 'softmax' net rt.crossentropyloss data net return net uncommenting second last line commenting line gives error"
incubator-mxnet,1814,"dear all, want initialise parameters batchnorm hand. initialise yet encounter following error checking type . notice aux, wondering proper way initialising parameters batchnorm? thanks advance.",0,initialise batchnorm hand?,"initialise batchnorm hand? dear all, want initialise parameters batchnorm hand. initialise yet encounter following error checking type . notice aux, wondering proper way initialising parameters batchnorm? thanks advance."
incubator-mxnet,6115,"hi, want add batchnorm layer i2h gru. code way, gamma beta bn shared time step. aux states can't shared like get error seems aux params unrolled max input length, duplicate names allowed. also try following code speech recognition example, seems cannot solve unfixed number params due variable length input case. returns aux params index range error. find several issues related, seems soloved, like 3076 2663. thanks time effort.",0,share auxiliary states batchnorm rnn variable length input,"share auxiliary states batchnorm rnn variable length input hi, want add batchnorm layer i2h gru. code way, gamma beta bn shared time step. aux states can't shared like get error seems aux params unrolled max input length, duplicate names allowed. also try following code speech recognition example, seems cannot solve unfixed number params due variable length input case. returns aux params index range error. find several issues related, seems soloved, like 3076 2663. thanks time effort."
incubator-mxnet,2514,error like pkg config cflags opencvpkg config cflags opencv,0,warp ctc plugin compilation error,warp ctc plugin compilation error error like pkg config cflags opencvpkg config cflags opencv
incubator-mxnet,6530,nvvm's nnsymbollistinputnames implement new method symbol list input names piiswrong,0,nvvm's nnsymbollistinputnames,nvvm's nnsymbollistinputnames nvvm's nnsymbollistinputnames implement new method symbol list input names piiswrong
incubator-mxnet,4800,"propose following improvements mxnet scala package, mostly making idiomatic user friendly scala users could form higher level api retaining current api lower level api current api full , idiomatic scala. manually rewrite normal scala functions default parameters. without reference mxnet python api really lost use layers functions. rnns utilize scala etc operations automatically unroll rnn computation graph. e.g. make lstm unit function type , say unroll lstm network. revamping bucketio databatch etc. stuff scala idiomatic. support gpu mac os x attempted failed better documentations better api executors gradients maps etc. tentative typesafe s. encode rank order tensor type system scala good at! , ndarray like means rank 3 cell contains done typeclass phantom type pattern scala shapeless https github.com milessabin shapeless . concatenation slicing etc. encode ranks resulting ndarray symbol. enhances compile time type safety, e.g. layer function whose type . cannot pass symbol embedding layer. tentative migrate build system . first come prototype also use nlp research discuss. javelinjs ldpe2g piiswrong jermainewang",0,proposed improvements mxnet scala package,"proposed improvements mxnet scala package propose following improvements mxnet scala package, mostly making idiomatic user friendly scala users could form higher level api retaining current api lower level api current api full , idiomatic scala. manually rewrite normal scala functions default parameters. without reference mxnet python api really lost use layers functions. rnns utilize scala etc operations automatically unroll rnn computation graph. e.g. make lstm unit function type , say unroll lstm network. revamping bucketio databatch etc. stuff scala idiomatic. support gpu mac os x attempted failed better documentations better api executors gradients maps etc. tentative typesafe s. encode rank order tensor type system scala good at! , ndarray like means rank 3 cell contains done typeclass phantom type pattern scala shapeless https github.com milessabin shapeless . concatenation slicing etc. encode ranks resulting ndarray symbol. enhances compile time type safety, e.g. layer function whose type . cannot pass symbol embedding layer. tentative migrate build system . first come prototype also use nlp research discuss. javelinjs ldpe2g piiswrong jermainewang"
incubator-mxnet,8564,"description test test operator gpu.test residual fused causing cuda 'too many resources' error tests latest build mxnet. environment info required p2 instance dlami cuda 9, latest mxnet built cmake. package used python r scala julia python build info required built source compiler gcc. mxnet commit hash 990bab865f5c8da168a2e68c4c6cc04fa3b117d7 build config cmake gninja .. error message minimum reproducible example using code, please provide short script reproduces error. otherwise, please provide link existing example. steps reproduce paste commands ran produced error. 1. build using cmake 2. install 3. run gpu tests python3 nose verbose incubator mxnet tests python gpu. note running test individually works. tried solve it? 1. mark test crashing.",0,test crash test operator gpu.test residual fused,"test crash test operator gpu.test residual fused description test test operator gpu.test residual fused causing cuda 'too many resources' error tests latest build mxnet. environment info required p2 instance dlami cuda 9, latest mxnet built cmake. package used python r scala julia python build info required built source compiler gcc. mxnet commit hash 990bab865f5c8da168a2e68c4c6cc04fa3b117d7 build config cmake gninja .. error message minimum reproducible example using code, please provide short script reproduces error. otherwise, please provide link existing example. steps reproduce paste commands ran produced error. 1. build using cmake 2. install 3. run gpu tests python3 nose verbose incubator mxnet tests python gpu. note running test individually works. tried solve it? 1. mark test crashing."
incubator-mxnet,7279,"want send pictures rcnn network prediction batch. current rcnn function like detect predictor support this. support function, change achieve goal?",0,batch processing rcnn prediction,"batch processing rcnn prediction want send pictures rcnn network prediction batch. current rcnn function like detect predictor support this. support function, change achieve goal?"
incubator-mxnet,16930,"description current estimator implementation, fit batch class method estimator class. common workflow fit batch model forwards training batch generate outputs compute loss functions. problem design flexible enough different model forward interfaces task. example, fit batch base estimator trains current batch label prediction task example, model forward interface return value predict labels. estimator compatible model using forward interface. however, another model label prediction task different forward interface , base estimator compatible model even though models share loss functions, training evaluation metrics. real world example found lm models https github.com dmlc gluon nlp blob c03665bafb1e0fe0fa5c2a59bbb4845393fbf9ba src gluonnlp model train language model.py . shares forward interface, whereas different one. forward interface forward interface straightforward workaround create new customized estimator model interface. bring issue need create standalone estimator time see new model interface even task. machine learning community, common see different model forward logic task. approach leads prohibitively many estimators simple task. lm example, need create even training logic two estimators same. prevent estimator explosion issue, suggest adding support plug play customized similar estimator class. given existing estimator , modify method take extra argument . call use models different interface. provided, use default method.",0,plug play fit batch estimator class,"plug play fit batch estimator class description current estimator implementation, fit batch class method estimator class. common workflow fit batch model forwards training batch generate outputs compute loss functions. problem design flexible enough different model forward interfaces task. example, fit batch base estimator trains current batch label prediction task example, model forward interface return value predict labels. estimator compatible model using forward interface. however, another model label prediction task different forward interface , base estimator compatible model even though models share loss functions, training evaluation metrics. real world example found lm models https github.com dmlc gluon nlp blob c03665bafb1e0fe0fa5c2a59bbb4845393fbf9ba src gluonnlp model train language model.py . shares forward interface, whereas different one. forward interface forward interface straightforward workaround create new customized estimator model interface. bring issue need create standalone estimator time see new model interface even task. machine learning community, common see different model forward logic task. approach leads prohibitively many estimators simple task. lm example, need create even training logic two estimators same. prevent estimator explosion issue, suggest adding support plug play customized similar estimator class. given existing estimator , modify method take extra argument . call use models different interface. provided, use default method."
incubator-mxnet,3435,want use output intermediate layer model. get ?,0,get output intermediate layer?,get output intermediate layer? want use output intermediate layer model. get ?
incubator-mxnet,8299,description test cifar10 fails ci master build. failed build https builds.apache.org blue organizations jenkins incubator mxnet detail master 532 pipeline environment info required ci build python 2 cpu mxnet commit hash 1c1c788916d672ee3cafdc4c91d7002a94a59d13 error message steps reproduce build run unit test,0,test cifar10 fails ci master build,test cifar10 fails ci master build description test cifar10 fails ci master build. failed build https builds.apache.org blue organizations jenkins incubator mxnet detail master 532 pipeline environment info required ci build python 2 cpu mxnet commit hash 1c1c788916d672ee3cafdc4c91d7002a94a59d13 error message steps reproduce build run unit test
incubator-mxnet,1099,nan,0,forward,forward nan
incubator-mxnet,16723,"ptrendx find fusedop support boolean type. following script trigger error. stack trace also manually disable fuse op, generate correct answer.",0,bug fused op support boolean type,"bug fused op support boolean type ptrendx find fusedop support boolean type. following script trigger error. stack trace also manually disable fuse op, generate correct answer."
incubator-mxnet,5591,"environment info operating system ubuntu 16.04 compiler package used python r scala julia mxnet version python version distribution , installed virtualenv pypi error message minimum reproducible example using code, please provide short script reproduces error. 1. trying run script https www.kaggle.com drn01z3 data science bowl 2017 mxnet xgboost baseline lb 0 57 instead incorrectly telling there's cuda capable device, exist tried solve it? 1. searched bit find solution. ubuntu installation script pollutes global environment want it. normally run keras tensorflow theano stuff think there's issue system.",0,check failed e cudasuccess cuda cuda capable device detected,"check failed e cudasuccess cuda cuda capable device detected environment info operating system ubuntu 16.04 compiler package used python r scala julia mxnet version python version distribution , installed virtualenv pypi error message minimum reproducible example using code, please provide short script reproduces error. 1. trying run script https www.kaggle.com drn01z3 data science bowl 2017 mxnet xgboost baseline lb 0 57 instead incorrectly telling there's cuda capable device, exist tried solve it? 1. searched bit find solution. ubuntu installation script pollutes global environment want it. normally run keras tensorflow theano stuff think there's issue system."
incubator-mxnet,8827,"size data split axis 1, return incorrect result. example, 1 import mxnet mx 2 data mx.nd.ones 2,3,4 3 data mx.nd.split data, axis 1, num outputs data.shape 1 , squeeze axis false 4 len data 4 3 5 data 0 .shape 5 2l, 1l, 4l but, splitting x along axis 1 below, return 2 ndarray. 6 x data 0 shape 2l, 1l, 4l 8 x mx.nd.split x, axis 1, num outputs x.shape 1 , squeeze axis false 9 len x 9 2 10 x 0 .shape 10 1l, 4l bug ?",0,question mx.nd.split ?,"question mx.nd.split ? size data split axis 1, return incorrect result. example, 1 import mxnet mx 2 data mx.nd.ones 2,3,4 3 data mx.nd.split data, axis 1, num outputs data.shape 1 , squeeze axis false 4 len data 4 3 5 data 0 .shape 5 2l, 1l, 4l but, splitting x along axis 1 below, return 2 ndarray. 6 x data 0 shape 2l, 1l, 4l 8 x mx.nd.split x, axis 1, num outputs x.shape 1 , squeeze axis false 9 len x 9 2 10 x 0 .shape 10 1l, 4l bug ?"
incubator-mxnet,297,"hi, want develop tools manage dataset split,merge,append,global shuffle something , found interface init iter init vector args , construct params, param complex like tshape btw, api imagerrecorditer python prefectcher cpp , imagerecoredioiter cpp another class...it easy mix",0,construct params imagerecordioiter cpp?,"construct params imagerecordioiter cpp? hi, want develop tools manage dataset split,merge,append,global shuffle something , found interface init iter init vector args , construct params, param complex like tshape btw, api imagerrecorditer python prefectcher cpp , imagerecoredioiter cpp another class...it easy mix"
incubator-mxnet,3924,"mli piiswrong , unit test operator rnn far?",0,unit test rnn?,"unit test rnn? mli piiswrong , unit test operator rnn far?"
incubator-mxnet,7912,"hi, building conv layer sharing weights another one, using 557. since default grad req 'write' need update weights using gradient two conv layer, need change 'add'? affect layers say limit 'add' req shared weights? thanks.",0,need change grad req sharing weights?,"need change grad req sharing weights? hi, building conv layer sharing weights another one, using 557. since default grad req 'write' need update weights using gradient two conv layer, need change 'add'? affect layers say limit 'add' req shared weights? thanks."
incubator-mxnet,6611,"sometimes need share weights network. its' advised mxnet reuse variable applyinng one weight referencing multiple times. that's ok variable attributes like lr mult wd mult fixed. want change learning rate different network structure still share weight. solution manually call symbol.set attr different learning rate different module initialization, ok? simpler way. ok define multiple variables name case? thanks.",0,reuse variable different attributes,"reuse variable different attributes sometimes need share weights network. its' advised mxnet reuse variable applyinng one weight referencing multiple times. that's ok variable attributes like lr mult wd mult fixed. want change learning rate different network structure still share weight. solution manually call symbol.set attr different learning rate different module initialization, ok? simpler way. ok define multiple variables name case? thanks."
incubator-mxnet,9588,"related 9587. current logic supports what's called macro f1 logic take mean f1 scores mini batches. micro f1 also supported keeping tp, fp, fn counts across batches. note metrics share counter logics mindful oo design. https github.com apache incubator mxnet blob master python mxnet metric.py",0,metric tp fp tn fn precision recall f1 macro micro versions,"metric tp fp tn fn precision recall f1 macro micro versions related 9587. current logic supports what's called macro f1 logic take mean f1 scores mini batches. micro f1 also supported keeping tp, fp, fn counts across batches. note metrics share counter logics mindful oo design. https github.com apache incubator mxnet blob master python mxnet metric.py"
incubator-mxnet,5775,"trying follow standard installation guide http mxnet.io get started amazonlinux setup.html, get following error. ec2 user ip xx x xx xxx build sudo make prefix usr local install .... 89 built target opencv cudaoptflow pch dephelp 89 generating precomp.hpp.gch opencv cudaoptflow release.gch 89 built target pch generate opencv cudaoptflow 89 building nvcc device object modules cudaoptflow cmakefiles cuda compile.dir src cuda . cuda compile generated pyrlk.cu.o nvcc error 'cicc' died due signal 11 invalid memory reference cmake error cuda compile generated pyrlk.cu.o.cmake 266 message error generating file home ec2 user opencv build modules cudaoptflow cmakefiles cuda compile.dir src cuda . cuda compile generated pyrlk.cu.o make 2 modules cudaoptflow cmakefiles cuda compile.dir src cuda . cuda compile generated pyrlk.cu.o error 1 make 1 modules cudaoptflow cmakefiles opencv cudaoptflow.dir error 2 make error 2",0,install mxnet amazon linux,"install mxnet amazon linux trying follow standard installation guide http mxnet.io get started amazonlinux setup.html, get following error. ec2 user ip xx x xx xxx build sudo make prefix usr local install .... 89 built target opencv cudaoptflow pch dephelp 89 generating precomp.hpp.gch opencv cudaoptflow release.gch 89 built target pch generate opencv cudaoptflow 89 building nvcc device object modules cudaoptflow cmakefiles cuda compile.dir src cuda . cuda compile generated pyrlk.cu.o nvcc error 'cicc' died due signal 11 invalid memory reference cmake error cuda compile generated pyrlk.cu.o.cmake 266 message error generating file home ec2 user opencv build modules cudaoptflow cmakefiles cuda compile.dir src cuda . cuda compile generated pyrlk.cu.o make 2 modules cudaoptflow cmakefiles cuda compile.dir src cuda . cuda compile generated pyrlk.cu.o error 1 make 1 modules cudaoptflow cmakefiles opencv cudaoptflow.dir error 2 make error 2"
incubator-mxnet,17167,"import mxnet illegal instruction core dumped processor inter xeon r cpu x5570 2.93 ghz graphics geforce gtx 960 pcle sse2 cuda 10.0, cudnn 7.4.2, opencv 4.1.2. help ?",0,can't install mxnet cu100,"can't install mxnet cu100 import mxnet illegal instruction core dumped processor inter xeon r cpu x5570 2.93 ghz graphics geforce gtx 960 pcle sse2 cuda 10.0, cudnn 7.4.2, opencv 4.1.2. help ?"
incubator-mxnet,9610,"hi, following tutorial http mxnet.incubator.apache.org tutorials python predict image.html wondering would use extracted features create bounding box image. also assertion code fails run code different model using resnet 18 1, 512 traceback recent call last file predict.py , line 79, assert features.shape 1, 2048 assertionerror features shape 1,512 . would affect feature map? thanks.",0,feature extraction object detection,"feature extraction object detection hi, following tutorial http mxnet.incubator.apache.org tutorials python predict image.html wondering would use extracted features create bounding box image. also assertion code fails run code different model using resnet 18 1, 512 traceback recent call last file predict.py , line 79, assert features.shape 1, 2048 assertionerror features shape 1,512 . would affect feature map? thanks."
incubator-mxnet,1976,"files use method, available python 3. instead, python 3 , also available python 2 apparently python 2's version slower. non issue, since mxnet's codebase, appears invoked initializing modules layers dictionary, cost marginal items dict initialization happens start program . far aware, behavior same, less drop replacement. alternatives suggested http python future.org compatible idioms.html iterating dict keys values items attach pull request shortly, done full test sweep gpus busy.",0,iteritems breaks python 3 compatibility,"iteritems breaks python 3 compatibility files use method, available python 3. instead, python 3 , also available python 2 apparently python 2's version slower. non issue, since mxnet's codebase, appears invoked initializing modules layers dictionary, cost marginal items dict initialization happens start program . far aware, behavior same, less drop replacement. alternatives suggested http python future.org compatible idioms.html iterating dict keys values items attach pull request shortly, done full test sweep gpus busy."
incubator-mxnet,6930,"fail install use guide https gist.github.com thirdwing 89aa9bfc588ade138496e6932072152c fail command c gpu mxnet r cmd install multiarch r package installing library 'c users statislove documents r win library 3.4' installing source package 'mxnet' libs make nothing done all'. installing c users statislove documents r win library 3.4 mxnet libs x64 r demo inst preparing package lazy loading help man pages found package 'mxnet' installing help indices building package indices installing vignettes testing installed package loaded error package namespace load failed 'mxnet' .onload failed loadnamespace 'mxnet', details call indl x, as.logical local , as.logical , error 'c users statislove documents r win library 3.4 mxnet libs x64 libmxnet.dll' loadlibrary failure . error loading failed removing 'c users statislove documents r win library 3.4 mxnet' mistake?",0,fail install mxnet windows 10,"fail install mxnet windows 10 fail install use guide https gist.github.com thirdwing 89aa9bfc588ade138496e6932072152c fail command c gpu mxnet r cmd install multiarch r package installing library 'c users statislove documents r win library 3.4' installing source package 'mxnet' libs make nothing done all'. installing c users statislove documents r win library 3.4 mxnet libs x64 r demo inst preparing package lazy loading help man pages found package 'mxnet' installing help indices building package indices installing vignettes testing installed package loaded error package namespace load failed 'mxnet' .onload failed loadnamespace 'mxnet', details call indl x, as.logical local , as.logical , error 'c users statislove documents r win library 3.4 mxnet libs x64 libmxnet.dll' loadlibrary failure . error loading failed removing 'c users statislove documents r win library 3.4 mxnet' mistake?"
incubator-mxnet,636,run char lstm notebook https github.com dmlc mxnet blob master example rnn char lstm.ipynb without modifing anything compared training validation original notebook github results got called sgd . looks like something changed model overfitting little bit validation little bit better. noticed running python 2.7.10 stored version github using python 3.4.2 ! image https cloud.githubusercontent.com assets 608789 11270580 2aafa0f4 8e8e 11e5 93ec 749177256b1a.png,0,char lstm notebook gives different overfeat results saved notebook github py2 3?,char lstm notebook gives different overfeat results saved notebook github py2 3? run char lstm notebook https github.com dmlc mxnet blob master example rnn char lstm.ipynb without modifing anything compared training validation original notebook github results got called sgd . looks like something changed model overfitting little bit validation little bit better. noticed running python 2.7.10 stored version github using python 3.4.2 ! image https cloud.githubusercontent.com assets 608789 11270580 2aafa0f4 8e8e 11e5 93ec 749177256b1a.png
incubator-mxnet,3604,"hi, getting error cnn text classification example. believe error return statement defined able run setup cnn model function step step. new python able catch issue. error nameerror traceback recent call last 1 cnn model setup cnn model mx.cpu , batch size, sentence size, num embed, vocab size, dropout 0.5, embedding false setup cnn model ctx, batch size, sentence size, num embed, vocab size, dropout, initializer, embedding 36 label cnn exec.arg dict 'softmax label' 37 38 return cnnmodel cnn exec cnn exec, symbol cnn, data data, label label, param blocks param blocks nameerror name 'cnnmodel' defined code def setup cnn model ctx, batch size, sentence size, num embed, vocab size, dropout 0.5, initializer mx.initializer.uniform 0.1 , embedding true cnn model setup cnn model mx.cpu , batch size, sentence size, num embed, vocab size, dropout 0.5, embedding false",0,nameerror name 'cnnmodel' defined,"nameerror name 'cnnmodel' defined hi, getting error cnn text classification example. believe error return statement defined able run setup cnn model function step step. new python able catch issue. error nameerror traceback recent call last 1 cnn model setup cnn model mx.cpu , batch size, sentence size, num embed, vocab size, dropout 0.5, embedding false setup cnn model ctx, batch size, sentence size, num embed, vocab size, dropout, initializer, embedding 36 label cnn exec.arg dict 'softmax label' 37 38 return cnnmodel cnn exec cnn exec, symbol cnn, data data, label label, param blocks param blocks nameerror name 'cnnmodel' defined code def setup cnn model ctx, batch size, sentence size, num embed, vocab size, dropout 0.5, initializer mx.initializer.uniform 0.1 , embedding true cnn model setup cnn model mx.cpu , batch size, sentence size, num embed, vocab size, dropout 0.5, embedding false"
incubator-mxnet,13718,"note providing complete information concise form best way get help. issue template serves checklist essential information technical issues bug reports. non technical issues feature requests, feel free present information believe best form. q discussion, please start discussion thread https discuss.mxnet.io description brief description problem 2 sentences. environment info required python diagnose.py package used python r scala julia using scala user, please provide 1. java version 2. maven version 3. scala runtime applicable r user, please provide r build info required built source compiler gcc clang mingw visual studio mxnet commit hash paste output here. build config paste content config.mk, build command. error message paste complete error message, including stack trace. minimum reproducible example using code, please provide short script reproduces error. otherwise, please provide link existing example. steps reproduce paste commands ran produced error. 1. 2. tried solve it? 1. 2.",0,help,"help note providing complete information concise form best way get help. issue template serves checklist essential information technical issues bug reports. non technical issues feature requests, feel free present information believe best form. q discussion, please start discussion thread https discuss.mxnet.io description brief description problem 2 sentences. environment info required python diagnose.py package used python r scala julia using scala user, please provide 1. java version 2. maven version 3. scala runtime applicable r user, please provide r build info required built source compiler gcc clang mingw visual studio mxnet commit hash paste output here. build config paste content config.mk, build command. error message paste complete error message, including stack trace. minimum reproducible example using code, please provide short script reproduces error. otherwise, please provide link existing example. steps reproduce paste commands ran produced error. 1. 2. tried solve it? 1. 2."
incubator-mxnet,3665,"thanks providing matlab binding mxnet. matlab binding works well use predict multi class classification, gives wrong result try binary classification. predicted probability always like 0.9998 0.0002 , index 0 near 1 index 1 near zero. someone help solve problem?",0,matlab c api gives wrong predict result binary classification,"matlab c api gives wrong predict result binary classification thanks providing matlab binding mxnet. matlab binding works well use predict multi class classification, gives wrong result try binary classification. predicted probability always like 0.9998 0.0002 , index 0 near 1 index 1 near zero. someone help solve problem?"
incubator-mxnet,4485,shall write custom eval metric?,0,print loss lr current batch,print loss lr current batch shall write custom eval metric?
incubator-mxnet,1841,"setup.py, there's packages 'mxnet' , installation, mxnet.module copied installation folder, import globally raise import error. add packages 'mxnet', 'mxnet.module' solve problem.",0,python submodule 'module' installed 'python setup.py install',"python submodule 'module' installed 'python setup.py install' setup.py, there's packages 'mxnet' , installation, mxnet.module copied installation folder, import globally raise import error. add packages 'mxnet', 'mxnet.module' solve problem."
incubator-mxnet,1932,"delete msse3 msshadow cflags mshadow.mk, occur fatal error emmintrin.h file directory . ps. cpu architecture arm.",0,compile mxnet jetson tk1when complie mxnet jetson tki meet g error unrecognitzed command line option ' msse3'.,"compile mxnet jetson tk1when complie mxnet jetson tki meet g error unrecognitzed command line option ' msse3'. delete msse3 msshadow cflags mshadow.mk, occur fatal error emmintrin.h file directory . ps. cpu architecture arm."
incubator-mxnet,15367,seen pr https github.com apache incubator mxnet pull 15364 proposed add get registered operators get operator arguments python bindings. feedback welcome.,0,rfc add public functions get list registered operators arguments,rfc add public functions get list registered operators arguments seen pr https github.com apache incubator mxnet pull 15364 proposed add get registered operators get operator arguments python bindings. feedback welcome.
incubator-mxnet,8660,"note providing complete information concise form best way get help. issue template serves checklist essential information technical issues bug reports. non technical issues feature requests, feel free present information believe best form. q discussion, please start discussion thread https discuss.mxnet.io description gradients seem incorrect used autograd. see script bottom details. ptrendx insight this? environment info required python diagnose.py package used python r scala julia using scala user, please provide 1. java version 2. maven version 3. scala runtime applicable r user, please provide r build info required built source compiler gcc clang mingw visual studio mxnet commit hash v0.12 paste output here. build config paste content config.mk, build command. error message paste complete error message, including stack trace. minimum reproducible example using code, please provide short script reproduces error. otherwise, please provide link existing example. steps reproduce paste commands ran produced error. 1. 2. tried solve it? 1. 2.",0,incorrect autograd results elemwise add,"incorrect autograd results elemwise add note providing complete information concise form best way get help. issue template serves checklist essential information technical issues bug reports. non technical issues feature requests, feel free present information believe best form. q discussion, please start discussion thread https discuss.mxnet.io description gradients seem incorrect used autograd. see script bottom details. ptrendx insight this? environment info required python diagnose.py package used python r scala julia using scala user, please provide 1. java version 2. maven version 3. scala runtime applicable r user, please provide r build info required built source compiler gcc clang mingw visual studio mxnet commit hash v0.12 paste output here. build config paste content config.mk, build command. error message paste complete error message, including stack trace. minimum reproducible example using code, please provide short script reproduces error. otherwise, please provide link existing example. steps reproduce paste commands ran produced error. 1. 2. tried solve it? 1. 2."
incubator-mxnet,6605,"trying mxnet code api, result time finish cpu workload 1.327401 sec time finish cpu gpu workloads 117.675283 sec error compilation, use configurations caffe compiled machine. gpu 980m . caffe pc conduct gpu computation once, yet mxnet pc wait 100 seconds least. what's wrong configuration?",0,large time cost gpu computation,"large time cost gpu computation trying mxnet code api, result time finish cpu workload 1.327401 sec time finish cpu gpu workloads 117.675283 sec error compilation, use configurations caffe compiled machine. gpu 980m . caffe pc conduct gpu computation once, yet mxnet pc wait 100 seconds least. what's wrong configuration?"
incubator-mxnet,12567,"description latest mxnet pip prebuilt packages, using gluon data loader num worker certain number, may raise error related issue https github.com xianyi openblas issues 1735 environment info required ubuntu 16",0,1.3.0 release pre built package contains buggy openblas version. causing gluon data loader large num workers crash,"1.3.0 release pre built package contains buggy openblas version. causing gluon data loader large num workers crash description latest mxnet pip prebuilt packages, using gluon data loader num worker certain number, may raise error related issue https github.com xianyi openblas issues 1735 environment info required ubuntu 16"
incubator-mxnet,978,"several interesting addons, requires additional dependency, optionally installed, thread used vote name addon folder, well suggestions dynamically load addons. example ongoing addons include torch operator caffe adapter. please vote name like 1. 2. 3.",0,vote folder name optional addon,"vote folder name optional addon several interesting addons, requires additional dependency, optionally installed, thread used vote name addon folder, well suggestions dynamically load addons. example ongoing addons include torch operator caffe adapter. please vote name like 1. 2. 3."
incubator-mxnet,1411,training 11 gb image record file always get killed memory. wondering mxnet load entire image memory? so. thanks!,0,deal huge image record file,deal huge image record file training 11 gb image record file always get killed memory. wondering mxnet load entire image memory? so. thanks!
incubator-mxnet,7163,"two questions distributed training mxnet. first, reading code train mnist.py, understanding, changes need make 1 pass dist sync dist async kvstore optimizer module.fit method 2 make hosts file make sure ips list ssh able mpirun able 3 call launch.py tools understanding correct missing something? second, want verify sync frequency distributed setting. weights synced every time call module.update ? want sync machine every n batches, way something like module.local update every batch module.global update every n batch? thanks advance!",0,sync frequency distributed training,"sync frequency distributed training two questions distributed training mxnet. first, reading code train mnist.py, understanding, changes need make 1 pass dist sync dist async kvstore optimizer module.fit method 2 make hosts file make sure ips list ssh able mpirun able 3 call launch.py tools understanding correct missing something? second, want verify sync frequency distributed setting. weights synced every time call module.update ? want sync machine every n batches, way something like module.local update every batch module.global update every n batch? thanks advance!"
incubator-mxnet,4971,"dear trying implement structure follows image1 res net except last layer diff predict result image2 res net except last layer image combined slice channel image1 res net except last layer diff predict result image2 res net except last layer way easier. trying second way. problem use previous resnet symbols this, original resnet symbol get variable named 'data' input, make output slice channel transmit data variable 'data' initialize network pretrained parameters? examples smarter ways this.",0,mxnet symbol implementation details?,"mxnet symbol implementation details? dear trying implement structure follows image1 res net except last layer diff predict result image2 res net except last layer image combined slice channel image1 res net except last layer diff predict result image2 res net except last layer way easier. trying second way. problem use previous resnet symbols this, original resnet symbol get variable named 'data' input, make output slice channel transmit data variable 'data' initialize network pretrained parameters? examples smarter ways this."
incubator-mxnet,12437,"may well right place this, see webpage specific repo mxnet https mxnet.apache.org install index.html?platform linux language python processor gpu says get cuda 9.0 first, step 2 says install mxnet using cuda 9.2. also says sure one is, far sure possible, gather latest libraries imperative use older 9.0 library. expect installation questions answered, pointing possible error webpage. note providing complete information concise form best way get help. issue template serves checklist essential information technical issues bug reports. non technical issues feature requests, feel free present information believe best form. q discussion, please start discussion thread https discuss.mxnet.io description mxnet.apache.org website possible error, points multiple versions cuda neccesity installation. environment info required ubuntu 18.04, chromium, x86 64, nvidia gt 10 series steps reproduce paste commands ran produced error. 1. go https mxnet.apache.org install index.html?platform linux language python processor gpu 2. search cuda, find 1, 2, 5 ten results tried solve it? 1. let lot know 2. attempting install cudnn 9.0 looks like latest cuda library 9.2?",0,mxnet install page linux mentions two different versions cuda,"mxnet install page linux mentions two different versions cuda may well right place this, see webpage specific repo mxnet https mxnet.apache.org install index.html?platform linux language python processor gpu says get cuda 9.0 first, step 2 says install mxnet using cuda 9.2. also says sure one is, far sure possible, gather latest libraries imperative use older 9.0 library. expect installation questions answered, pointing possible error webpage. note providing complete information concise form best way get help. issue template serves checklist essential information technical issues bug reports. non technical issues feature requests, feel free present information believe best form. q discussion, please start discussion thread https discuss.mxnet.io description mxnet.apache.org website possible error, points multiple versions cuda neccesity installation. environment info required ubuntu 18.04, chromium, x86 64, nvidia gt 10 series steps reproduce paste commands ran produced error. 1. go https mxnet.apache.org install index.html?platform linux language python processor gpu 2. search cuda, find 1, 2, 5 ten results tried solve it? 1. let lot know 2. attempting install cudnn 9.0 looks like latest cuda library 9.2?"
incubator-mxnet,7219,"hi, trying r use constant minimum instance custom mae loss function cap 1. that? tried lro2 mx.symbol.makeloss mx.symbol.min mx.symbol.abs mx.symbol.reshape fc3, shape 0 label ,1.0 ,name lro2 returns error mx.varg.symbol.min list symbol.cc 302 rcheck failed keys .length ! 0 non symbol parameters accepted via key value style. thanks!",0,r include minimum function makeloss,"r include minimum function makeloss hi, trying r use constant minimum instance custom mae loss function cap 1. that? tried lro2 mx.symbol.makeloss mx.symbol.min mx.symbol.abs mx.symbol.reshape fc3, shape 0 label ,1.0 ,name lro2 returns error mx.varg.symbol.min list symbol.cc 302 rcheck failed keys .length ! 0 non symbol parameters accepted via key value style. thanks!"
incubator-mxnet,464,"functions example code provided documentation. purpose, r package may include raw data process. 3 possibilities add custom data directly package like xgboost r package add dependency package contains data. right now, mlbench used rmarkdown documents. also possible include example without data mark example code executable package compilation example tested . tqchen hetong007 thirdwing think do?",0,r add example data package able example code function documentation,"r add example data package able example code function documentation functions example code provided documentation. purpose, r package may include raw data process. 3 possibilities add custom data directly package like xgboost r package add dependency package contains data. right now, mlbench used rmarkdown documents. also possible include example without data mark example code executable package compilation example tested . tqchen hetong007 thirdwing think do?"
incubator-mxnet,2055,"reading code example image classification train mnist.py. argument input shape. refer source code src io iter mnist.cc, find parameter input shape. doc list argument too. delete two lines, everything looks ok.",0,argument input shape required mnistiterator?,"argument input shape required mnistiterator? reading code example image classification train mnist.py. argument input shape. refer source code src io iter mnist.cc, find parameter input shape. doc list argument too. delete two lines, everything looks ok."
incubator-mxnet,2862,"want create new operator implement hierarchical softmax writing c code. moment, operator works fine cpu mode, crash gpu mode. one cause issue need access entry tensor, i.e., .... .... correct way access one entry gpu mode? thanks.",0,access entry tensor mshadow using gpu mode?,"access entry tensor mshadow using gpu mode? want create new operator implement hierarchical softmax writing c code. moment, operator works fine cpu mode, crash gpu mode. one cause issue need access entry tensor, i.e., .... .... correct way access one entry gpu mode? thanks."
incubator-mxnet,15166,"description asnumpy fails float16 gradient cpu gpu contexts. interestingly, printing variable data16 mre , works cpu, every time gpu. environment info required package used python r scala julia python error message minimum reproducible example using code, please provide short script reproduces error. otherwise, please provide link existing example. steps reproduce paste commands ran produced error. run tried solve it? n",0,asnumpy fails float16 gradient,"asnumpy fails float16 gradient description asnumpy fails float16 gradient cpu gpu contexts. interestingly, printing variable data16 mre , works cpu, every time gpu. environment info required package used python r scala julia python error message minimum reproducible example using code, please provide short script reproduces error. otherwise, please provide link existing example. steps reproduce paste commands ran produced error. run tried solve it? n"
incubator-mxnet,1811,trying get predictions images using imagenet 21k inception model cpp classification code prediction cpu slow. prediction takes 1.06 seconds single image tried give images batch still speed gain. thought giving images batch speed things up. something wrong? way make predictions fast cpu?,0,mxnet prediction cpu slow,mxnet prediction cpu slow trying get predictions images using imagenet 21k inception model cpp classification code prediction cpu slow. prediction takes 1.06 seconds single image tried give images batch still speed gain. thought giving images batch speed things up. something wrong? way make predictions fast cpu?
incubator-mxnet,8358,"description 'module' object attribute 'nd' run command , short mxnet python program validating mxnet installation runs successfully terminal. environment info required system ubuntu 14.04 mxnet version 0.11.0 package used python r scala julia using python. build info required built source compiler gcc clang mingw visual studio gcc build config make j nproc use opencv 1 use blas openblas use cuda 1 use cuda path usr local cuda use cudnn 1 error message traceback recent call last file mxnet mxnet.py , line 1, import mxnet mx file home qy documents pythonprj mxnet mxnet.py , line 3, mx.nd.array 1, 2, 3 attributeerror 'module' object attribute 'nd' details installing mxnet, run official test sample terminal everything okay. however, write code python file , run ., returned mentioned error. seems find mxnet library.",0,'module' object attribute 'nd',"'module' object attribute 'nd' description 'module' object attribute 'nd' run command , short mxnet python program validating mxnet installation runs successfully terminal. environment info required system ubuntu 14.04 mxnet version 0.11.0 package used python r scala julia using python. build info required built source compiler gcc clang mingw visual studio gcc build config make j nproc use opencv 1 use blas openblas use cuda 1 use cuda path usr local cuda use cudnn 1 error message traceback recent call last file mxnet mxnet.py , line 1, import mxnet mx file home qy documents pythonprj mxnet mxnet.py , line 3, mx.nd.array 1, 2, 3 attributeerror 'module' object attribute 'nd' details installing mxnet, run official test sample terminal everything okay. however, write code python file , run ., returned mentioned error. seems find mxnet library."
incubator-mxnet,424,"like dimshuffle function theano, swapaxes numpy, symbol?",0,add swapaxes symbol,"add swapaxes symbol like dimshuffle function theano, swapaxes numpy, symbol?"
incubator-mxnet,13386,"currently master branch run clojure package, see warnings reason occurring new operator added brought dynamically backend scala package. removed adding https github.com apache incubator mxnet blob master contrib clojure package src org apache clojure mxnet ndarray.clj l19 https github.com apache incubator mxnet blob master contrib clojure package src org apache clojure mxnet symbol.clj. also need added generated code well https github.com apache incubator mxnet blob master contrib clojure package src dev generator.clj l215 https github.com apache incubator mxnet blob master contrib clojure package src dev generator.clj l308",0,clojure remove refer warning,"clojure remove refer warning currently master branch run clojure package, see warnings reason occurring new operator added brought dynamically backend scala package. removed adding https github.com apache incubator mxnet blob master contrib clojure package src org apache clojure mxnet ndarray.clj l19 https github.com apache incubator mxnet blob master contrib clojure package src org apache clojure mxnet symbol.clj. also need added generated code well https github.com apache incubator mxnet blob master contrib clojure package src dev generator.clj l215 https github.com apache incubator mxnet blob master contrib clojure package src dev generator.clj l308"
incubator-mxnet,11612,find government auctions valueable resources offer online.,0,find government repositories,find government repositories find government auctions valueable resources offer online.
incubator-mxnet,14030,"description hi, there. would like use accelerate project, flaky problem. example works mxnet built source myself, work mxnet installed pip. environment info required package used python r scala julia python build info required built source compiler gcc clang mingw visual studio gcc mxnet commit hash e37ff53f0a3a0cc9276a7a1d4aff0deab91c40df build config minimum reproducible example https github.com wkcn test tvm bridge example, tvm packed func.h simplyfied tvm. steps reproduce 1. clone example 2. change path libmxnet.so line 55 code. 3. make 4. . test tried solve it? function https github.com wkcn test tvm bridge blob master test.cpp l28 reproducible example, wrong sometimes wrong mxnet installed pip, namely . however, result correct mxnet built myself, latest mxnet source. pr 9880, merrymercy met similar problem. https github.com apache incubator mxnet pull 9880 issuecomment 421779923 seems temporary variable released calling function https github.com apache incubator mxnet blob master src nnvm tvm bridge.cc l178 optimization compiler. thanks!",0,problem abi compatibility mxtvmbridge,"problem abi compatibility mxtvmbridge description hi, there. would like use accelerate project, flaky problem. example works mxnet built source myself, work mxnet installed pip. environment info required package used python r scala julia python build info required built source compiler gcc clang mingw visual studio gcc mxnet commit hash e37ff53f0a3a0cc9276a7a1d4aff0deab91c40df build config minimum reproducible example https github.com wkcn test tvm bridge example, tvm packed func.h simplyfied tvm. steps reproduce 1. clone example 2. change path libmxnet.so line 55 code. 3. make 4. . test tried solve it? function https github.com wkcn test tvm bridge blob master test.cpp l28 reproducible example, wrong sometimes wrong mxnet installed pip, namely . however, result correct mxnet built myself, latest mxnet source. pr 9880, merrymercy met similar problem. https github.com apache incubator mxnet pull 9880 issuecomment 421779923 seems temporary variable released calling function https github.com apache incubator mxnet blob master src nnvm tvm bridge.cc l178 optimization compiler. thanks!"
incubator-mxnet,12318,causes sphinx fail processing api docs shorthand references used. used workaround 12317 reference functions long way. means reference could ! 2018 08 23 15 08 55 https user images.githubusercontent.com 5974205 44555096 657b4100 a6e8 11e8 8ea5 49cf3db31063.png shorthand route work. example roshrini,0,sphinx unable access mxnet onnx module functions,sphinx unable access mxnet onnx module functions causes sphinx fail processing api docs shorthand references used. used workaround 12317 reference functions long way. means reference could ! 2018 08 23 15 08 55 https user images.githubusercontent.com 5974205 44555096 657b4100 a6e8 11e8 8ea5 49cf3db31063.png shorthand route work. example roshrini
incubator-mxnet,4469,"bugs installation issues, please provide following information. information provide, likely people able help you. environment info operating system mac os x 10.12.2 compiler package used python r scala julia scala mxnet version 0.9.1 installed source mxnet commit hash d0728ca552d919fd40616149016ba4dd664c4e20 using python package, please provide python version distribution using r package, please provide r error message please paste full error message, including stack trace. make scalapkg cd myhome tools mxnet scala package mvn clean package posx x86 64 gpu dcxx g dcflags dmshadow force stream wall o3 myhome tools mxnet mshadow myhome tools mxnet dmlc core include fpic myhome tools mxnet nnvm include iinclude funroll loops wno unused parameter wno unknown pragmas wno unused local typedefs msse3 usr local cuda include dmshadow use cblas 1 dmshadow use mkl 0 dmshadow rabit ps 0 dmshadow dist ps 0 dmsdhadow use pascal 0 dmxnet use opencv 1 usr local cellar opencv 2.4.13.2 include opencv usr local cellar opencv 2.4.13.2 include dmshadow use cudnn 1 usr local opt openblas include dmxnet use dist kvstore myhome tools mxnet ps lite include myhome tools mxnet deps include dmxnet use nvrtc 0 dldflags pthread lm lcudart lcublas lcurand l usr local cuda lib64 l usr local cuda lib lopenblas l usr local cellar opencv 2.4.13.2 lib lopencv calib3d lopencv contrib lopencv core lopencv features2d lopencv flann lopencv gpu lopencv highgui lopencv imgproc lopencv legacy lopencv ml lopencv nonfree lopencv objdetect lopencv ocl lopencv photo lopencv stitching lopencv superres lopencv ts lopencv video lopencv videostab lcudnn l usr local opt openblas lib l usr local lib graphviz myhome tools mxnet deps lib libprotobuf lite.a myhome tools mxnet deps lib libzmq.a lcuda dlddeps myhome tools mxnet ps lite build libps.a myhome tools mxnet dmlc core libdmlc.a myhome tools mxnet nnvm lib libnnvm.a info scanning projects... error error problems encountered processing poms error 'dependencies.dependency.artifactid' ml.dmlc.mxnet libmxnet init scala platform libtype value 'libmxnet init scala platform ' match valid id pattern. line 77, column 19 error build could read 1 project help 1 error error project ml.dmlc.mxnet mxnet macros 2.11 0.1.2 snapshot myhome tools mxnet scala package macros pom.xml 1 error error 'dependencies.dependency.artifactid' ml.dmlc.mxnet libmxnet init scala platform libtype value 'libmxnet init scala platform ' match valid id pattern. line 77, column 19 error error see full stack trace errors, run maven e switch. error run maven using x switch enable full debug logging. error error information errors possible solutions, please read following articles error help 1 http cwiki.apache.org confluence display maven projectbuildingexception make scalapkg error 1 minimum reproducible example using code, please provide short script reproduces error. make scalapkg steps reproduce running standard examples, please provide commands run lead error. building mxnet 1. make scalapkg 2. 3. tried solve it? 1. modify pom.xml 2. 3.",0,error make scalapkg,"error make scalapkg bugs installation issues, please provide following information. information provide, likely people able help you. environment info operating system mac os x 10.12.2 compiler package used python r scala julia scala mxnet version 0.9.1 installed source mxnet commit hash d0728ca552d919fd40616149016ba4dd664c4e20 using python package, please provide python version distribution using r package, please provide r error message please paste full error message, including stack trace. make scalapkg cd myhome tools mxnet scala package mvn clean package posx x86 64 gpu dcxx g dcflags dmshadow force stream wall o3 myhome tools mxnet mshadow myhome tools mxnet dmlc core include fpic myhome tools mxnet nnvm include iinclude funroll loops wno unused parameter wno unknown pragmas wno unused local typedefs msse3 usr local cuda include dmshadow use cblas 1 dmshadow use mkl 0 dmshadow rabit ps 0 dmshadow dist ps 0 dmsdhadow use pascal 0 dmxnet use opencv 1 usr local cellar opencv 2.4.13.2 include opencv usr local cellar opencv 2.4.13.2 include dmshadow use cudnn 1 usr local opt openblas include dmxnet use dist kvstore myhome tools mxnet ps lite include myhome tools mxnet deps include dmxnet use nvrtc 0 dldflags pthread lm lcudart lcublas lcurand l usr local cuda lib64 l usr local cuda lib lopenblas l usr local cellar opencv 2.4.13.2 lib lopencv calib3d lopencv contrib lopencv core lopencv features2d lopencv flann lopencv gpu lopencv highgui lopencv imgproc lopencv legacy lopencv ml lopencv nonfree lopencv objdetect lopencv ocl lopencv photo lopencv stitching lopencv superres lopencv ts lopencv video lopencv videostab lcudnn l usr local opt openblas lib l usr local lib graphviz myhome tools mxnet deps lib libprotobuf lite.a myhome tools mxnet deps lib libzmq.a lcuda dlddeps myhome tools mxnet ps lite build libps.a myhome tools mxnet dmlc core libdmlc.a myhome tools mxnet nnvm lib libnnvm.a info scanning projects... error error problems encountered processing poms error 'dependencies.dependency.artifactid' ml.dmlc.mxnet libmxnet init scala platform libtype value 'libmxnet init scala platform ' match valid id pattern. line 77, column 19 error build could read 1 project help 1 error error project ml.dmlc.mxnet mxnet macros 2.11 0.1.2 snapshot myhome tools mxnet scala package macros pom.xml 1 error error 'dependencies.dependency.artifactid' ml.dmlc.mxnet libmxnet init scala platform libtype value 'libmxnet init scala platform ' match valid id pattern. line 77, column 19 error error see full stack trace errors, run maven e switch. error run maven using x switch enable full debug logging. error error information errors possible solutions, please read following articles error help 1 http cwiki.apache.org confluence display maven projectbuildingexception make scalapkg error 1 minimum reproducible example using code, please provide short script reproduces error. make scalapkg steps reproduce running standard examples, please provide commands run lead error. building mxnet 1. make scalapkg 2. 3. tried solve it? 1. modify pom.xml 2. 3."
incubator-mxnet,2705,"hi, would like know ways passing array values layer wise , using feedforward class create model amalgamation, like using prediction method feedforward thanks",0,creating model along predicting amalgamation,"creating model along predicting amalgamation hi, would like know ways passing array values layer wise , using feedforward class create model amalgamation, like using prediction method feedforward thanks"
incubator-mxnet,2924,"windows 7 vs 2013 cuda 7.5 cmake 3.3.0 build cpu, succeed, build cuda, failed output log like build mxnet cuda windows 2015, ok, familiar cmake, ideas this? thanks. linux gcc ok here.",0,cmake build failed compile ndarray function.cu windows,"cmake build failed compile ndarray function.cu windows windows 7 vs 2013 cuda 7.5 cmake 3.3.0 build cpu, succeed, build cuda, failed output log like build mxnet cuda windows 2015, ok, familiar cmake, ideas this? thanks. linux gcc ok here."
incubator-mxnet,7813,"training alexnet cnn imagenet data, see performance improvement fact see slight performance degradation increasing number gpus python train imagenet.py data train local imagenet mxnet data mxnet data.rec data val local imagenet mxnet data mxnet data test.rec gpus 0,1,2,3 network alexnet batch size 256 num epochs 1 kv store device per epoch batch size gpu 64 , 1 gpu, time cost 910 sec 2 gpu, time cost 924 sec 4 gpu, time cost 964 sec 4 titan xps however, synthetic data shown demo https github.com apache incubator mxnet blob master example image classification readme.md see good scalability.",0,performance improve scalability issue gpus running train imagenet.py,"performance improve scalability issue gpus running train imagenet.py training alexnet cnn imagenet data, see performance improvement fact see slight performance degradation increasing number gpus python train imagenet.py data train local imagenet mxnet data mxnet data.rec data val local imagenet mxnet data mxnet data test.rec gpus 0,1,2,3 network alexnet batch size 256 num epochs 1 kv store device per epoch batch size gpu 64 , 1 gpu, time cost 910 sec 2 gpu, time cost 924 sec 4 gpu, time cost 964 sec 4 titan xps however, synthetic data shown demo https github.com apache incubator mxnet blob master example image classification readme.md see good scalability."
incubator-mxnet,2160,"recently, reproducing result fcn xs mxnet example, exactly follow instruction readme.md file, fcn 8s output like below, train accuracy around 0.6 0.8, never get 0.9. output log may like training fcn 8s info root start training gpu 3 info root epoch 0 batch 50 speed 1.16 samples sec train accuracy 0.894318 info root epoch 0 batch 100 speed 1.11 samples sec train accuracy 0.904681 info root epoch 0 batch 150 speed 1.13 samples sec train accuracy 0.908053 info root epoch 0 batch 200 speed 1.12 samples sec train accuracy 0.912219 info root epoch 0 batch 250 speed 1.13 samples sec train accuracy 0.914238 info root epoch 0 batch 300 speed 1.13 samples sec train accuracy 0.912170 info root epoch 0 batch 350 speed 1.12 samples sec train accuracy 0.912080 do. first train fcn 32s, change epoch 31 train fcn 16s, change learning rate 10e 12 epoch 27 finally train fcn 8s, change learning rate 10e 14 epoch 19 anyone tell output log fcn 32s fcn 16s, end beginning log ok, figure training stage wrong",0,ouput log training fcn 32s fcn 16s,"ouput log training fcn 32s fcn 16s recently, reproducing result fcn xs mxnet example, exactly follow instruction readme.md file, fcn 8s output like below, train accuracy around 0.6 0.8, never get 0.9. output log may like training fcn 8s info root start training gpu 3 info root epoch 0 batch 50 speed 1.16 samples sec train accuracy 0.894318 info root epoch 0 batch 100 speed 1.11 samples sec train accuracy 0.904681 info root epoch 0 batch 150 speed 1.13 samples sec train accuracy 0.908053 info root epoch 0 batch 200 speed 1.12 samples sec train accuracy 0.912219 info root epoch 0 batch 250 speed 1.13 samples sec train accuracy 0.914238 info root epoch 0 batch 300 speed 1.13 samples sec train accuracy 0.912170 info root epoch 0 batch 350 speed 1.12 samples sec train accuracy 0.912080 do. first train fcn 32s, change epoch 31 train fcn 16s, change learning rate 10e 12 epoch 27 finally train fcn 8s, change learning rate 10e 14 epoch 19 anyone tell output log fcn 32s fcn 16s, end beginning log ok, figure training stage wrong"
incubator-mxnet,9544,"hi , quick question. prepared data using im2rec tool. training sample label vector length l. however, first 3 elements flags used identify samples backward gradient computed otherwise send 0. custom loss layer multi task learning, 3 flags . therefore, output predicted length l 3. custom loss layer takes account. wanted know mxnet allows kind unusual labelling. using mxnet 0.12.1, cudnn v5, cuda8, ubuntu 14, python 2.7, gcc 4.9",0,label length different predicted output length,"label length different predicted output length hi , quick question. prepared data using im2rec tool. training sample label vector length l. however, first 3 elements flags used identify samples backward gradient computed otherwise send 0. custom loss layer multi task learning, 3 flags . therefore, output predicted length l 3. custom loss layer takes account. wanted know mxnet allows kind unusual labelling. using mxnet 0.12.1, cudnn v5, cuda8, ubuntu 14, python 2.7, gcc 4.9"
incubator-mxnet,3212,"motherboard intel b85 support sli technology two pcie interfaces, two gtx1080 gpu cards construct mxnet system. found utilizations two gtx1080 slow, speed two gtx1080 slower single gtx1080. although increased batch sizes, speed two gpus still lower one gpu. solve problem?",0,motherboard need sli technology support multiple gpus mxnet?,"motherboard need sli technology support multiple gpus mxnet? motherboard intel b85 support sli technology two pcie interfaces, two gtx1080 gpu cards construct mxnet system. found utilizations two gtx1080 slow, speed two gtx1080 slower single gtx1080. although increased batch sizes, speed two gpus still lower one gpu. solve problem?"
incubator-mxnet,15647,"retinaface root docker20 data2 yanmengkai face retinaface pip install mxnet looking indexes https pypi.tuna.tsinghua.edu.cn simple requirement already satisfied mxnet root anaconda3 lib python3.7 site packages 1.5.0 requirement already satisfied numpy1.16.0 root anaconda3 lib python3.7 site packages mxnet 1.16.4 requirement already satisfied graphviz 0.8.1 root anaconda3 lib python3.7 site packages mxnet 0.8.4 requirement already satisfied requests 2.20.0 root anaconda3 lib python3.7 site packages mxnet 2.22.0 requirement already satisfied chardet 3.0.2 root anaconda3 lib python3.7 site packages requests 2.20.0 mxnet 3.0.4 requirement already satisfied urllib3! 1.25.0,! 1.25.1, 1.21.1 root anaconda3 lib python3.7 site packages requests 2.20.0 mxnet 1.23 requirement already satisfied idna 2.5 root anaconda3 lib python3.7 site packages requests 2.20.0 mxnet 2.7 requirement already satisfied certifi 2017.4.17 root anaconda3 lib python3.7 site packages requests 2.20.0 mxnet 2018.8.24 retinaface root docker20 data2 yanmengkai face retinaface python python 3.7.0 default, jun 28 2018, 13 15 42 gcc 7.2.0 anaconda, inc. linux type help , copyright , credits license information. import mxnet segmentation fault 11 stack trace retinaface root docker20 data2 yanmengkai face retinaface",0,segmentation fault 11,"segmentation fault 11 retinaface root docker20 data2 yanmengkai face retinaface pip install mxnet looking indexes https pypi.tuna.tsinghua.edu.cn simple requirement already satisfied mxnet root anaconda3 lib python3.7 site packages 1.5.0 requirement already satisfied numpy1.16.0 root anaconda3 lib python3.7 site packages mxnet 1.16.4 requirement already satisfied graphviz 0.8.1 root anaconda3 lib python3.7 site packages mxnet 0.8.4 requirement already satisfied requests 2.20.0 root anaconda3 lib python3.7 site packages mxnet 2.22.0 requirement already satisfied chardet 3.0.2 root anaconda3 lib python3.7 site packages requests 2.20.0 mxnet 3.0.4 requirement already satisfied urllib3! 1.25.0,! 1.25.1, 1.21.1 root anaconda3 lib python3.7 site packages requests 2.20.0 mxnet 1.23 requirement already satisfied idna 2.5 root anaconda3 lib python3.7 site packages requests 2.20.0 mxnet 2.7 requirement already satisfied certifi 2017.4.17 root anaconda3 lib python3.7 site packages requests 2.20.0 mxnet 2018.8.24 retinaface root docker20 data2 yanmengkai face retinaface python python 3.7.0 default, jun 28 2018, 13 15 42 gcc 7.2.0 anaconda, inc. linux type help , copyright , credits license information. import mxnet segmentation fault 11 stack trace retinaface root docker20 data2 yanmengkai face retinaface"
incubator-mxnet,2724,"script 'mxnet example image classification train imagenet.py' , data def get iterator args, kv data shape 3, args.data shape, args.data shape train mx.io.imagerecorditer path imgrec os.path.join args.data dir, args.train dataset , mean r 123.68, mean g 116.779, mean b 103.939, data shape data shape, batch size args.batch size, rand crop true, rand mirror true, num parts kv.num workers, part index kv.rank get value ' mean r, mean g, mean b' ? another question, ' data shape' size picture ? instance, picture size 1920 1080, ' data shape' set 3 1920 1080? crop picture size 1080 1080, ' data shape' set 3 1080 1080? mli thank help.",0,get value 'mean r',"get value 'mean r' script 'mxnet example image classification train imagenet.py' , data def get iterator args, kv data shape 3, args.data shape, args.data shape train mx.io.imagerecorditer path imgrec os.path.join args.data dir, args.train dataset , mean r 123.68, mean g 116.779, mean b 103.939, data shape data shape, batch size args.batch size, rand crop true, rand mirror true, num parts kv.num workers, part index kv.rank get value ' mean r, mean g, mean b' ? another question, ' data shape' size picture ? instance, picture size 1920 1080, ' data shape' set 3 1920 1080? crop picture size 1080 1080, ' data shape' set 3 1080 1080? mli thank help."
incubator-mxnet,4447,"following figure, compare commonly used data augmentation techniques resnet 50 imagenet. random crop, random flip, scale augmentations shown effective , provide baseline scale aug . jpeg compression quality 90 aspect ratio augmentation , curves indicates almost effects . color augmentation , shown accuracy actually decreased . ! notes aug https cloud.githubusercontent.com assets 3815006 21560667 6f909480 ce9d 11e6 9340 9d8af5f41837.png compare state art implementation https github.com tornadomeet resnet resnet 50 imagenet, top 1 accuracy 74.55 achieved. suggested https github.com tornadomeet resnet, canceling scale color aspect augmentation improve results. fair comparison, https github.com tornadomeet resnet actually achieved 74.20 accuracy 95 th epoch right cancellation scale color aspect augmentations . hence, conclude augmentations beyond random crop, random flip, scale actually hurt performance 74.55 74.20 .",0,note data augmenation,"note data augmenation following figure, compare commonly used data augmentation techniques resnet 50 imagenet. random crop, random flip, scale augmentations shown effective , provide baseline scale aug . jpeg compression quality 90 aspect ratio augmentation , curves indicates almost effects . color augmentation , shown accuracy actually decreased . ! notes aug https cloud.githubusercontent.com assets 3815006 21560667 6f909480 ce9d 11e6 9340 9d8af5f41837.png compare state art implementation https github.com tornadomeet resnet resnet 50 imagenet, top 1 accuracy 74.55 achieved. suggested https github.com tornadomeet resnet, canceling scale color aspect augmentation improve results. fair comparison, https github.com tornadomeet resnet actually achieved 74.20 accuracy 95 th epoch right cancellation scale color aspect augmentations . hence, conclude augmentations beyond random crop, random flip, scale actually hurt performance 74.55 74.20 ."
incubator-mxnet,1504,could give example code?,0,build siamese network contrastive loss?,build siamese network contrastive loss? could give example code?
incubator-mxnet,1067,"linux blas put libblas.a libcblas.a usr local lib, make, cmd complains outside built gcc4.9.0 bin g dmshadow force stream wall o3 i. mshadow i. dmlc core include fpic iinclude msse3 funroll loops wno unused parameter wno unknown pragmas dmshadow use cuda 0 dmshadow use cblas 1 dmshadow use mkl 0 dmshadow rabit ps 0 dmshadow dist ps 0 dmxnet use opencv 1 fopenmp bin im2rec tools im2rec.cc build resource.o build c api c api.o build c api c api error.o build c api c predict api.o build common mxrtc.o build common tblob op registry.o build engine engine.o build engine naive engine.o build engine threaded engine.o build engine threaded engine perdevice.o build engine threaded engine pooled.o build io io.o build io iter csv.o build io iter image recordio.o build io iter mnist.o build kvstore kvstore.o build ndarray ndarray.o build ndarray ndarray function.o build ndarray unary function.o build operator activation.o build operator batch norm.o build operator block grad.o build operator concat.o build operator convolution.o build operator cross device copy.o build operator cudnn batch norm.o build operator deconvolution.o build operator dropout.o build operator elementwise binary op.o build operator elementwise binary scalar op.o build operator elementwise sum.o build operator embedding.o build operator fully connected.o build operator identity attach kl sparse reg.o build operator leaky relu.o build operator lrn.o build operator native op.o build operator ndarray op.o build operator operator.o build operator pooling.o build operator regression output.o build operator reshape.o build operator slice channel.o build operator softmax activation.o build operator softmax output.o build operator swapaxis.o build operator upsampling.o build optimizer optimizer.o build optimizer sgd.o build storage storage.o build symbol graph executor.o build symbol static graph.o build symbol symbol.o dmlc core libdmlc.a pthread lm lcblas lrt lcblas usr local lib libcblas.a sdotsub.o function sdot ' usr local lib libcblas.a cblas sgemm.o function sgemm ' cblas sgemm.c .text 0x1e6 undefined reference sgemm ' collect2 error ld returned 1 exit status make bin im2rec error 1",0,install error sdotsub.f .text 0x7 undefined reference sdot ',"install error sdotsub.f .text 0x7 undefined reference sdot ' linux blas put libblas.a libcblas.a usr local lib, make, cmd complains outside built gcc4.9.0 bin g dmshadow force stream wall o3 i. mshadow i. dmlc core include fpic iinclude msse3 funroll loops wno unused parameter wno unknown pragmas dmshadow use cuda 0 dmshadow use cblas 1 dmshadow use mkl 0 dmshadow rabit ps 0 dmshadow dist ps 0 dmxnet use opencv 1 fopenmp bin im2rec tools im2rec.cc build resource.o build c api c api.o build c api c api error.o build c api c predict api.o build common mxrtc.o build common tblob op registry.o build engine engine.o build engine naive engine.o build engine threaded engine.o build engine threaded engine perdevice.o build engine threaded engine pooled.o build io io.o build io iter csv.o build io iter image recordio.o build io iter mnist.o build kvstore kvstore.o build ndarray ndarray.o build ndarray ndarray function.o build ndarray unary function.o build operator activation.o build operator batch norm.o build operator block grad.o build operator concat.o build operator convolution.o build operator cross device copy.o build operator cudnn batch norm.o build operator deconvolution.o build operator dropout.o build operator elementwise binary op.o build operator elementwise binary scalar op.o build operator elementwise sum.o build operator embedding.o build operator fully connected.o build operator identity attach kl sparse reg.o build operator leaky relu.o build operator lrn.o build operator native op.o build operator ndarray op.o build operator operator.o build operator pooling.o build operator regression output.o build operator reshape.o build operator slice channel.o build operator softmax activation.o build operator softmax output.o build operator swapaxis.o build operator upsampling.o build optimizer optimizer.o build optimizer sgd.o build storage storage.o build symbol graph executor.o build symbol static graph.o build symbol symbol.o dmlc core libdmlc.a pthread lm lcblas lrt lcblas usr local lib libcblas.a sdotsub.o function sdot ' usr local lib libcblas.a cblas sgemm.o function sgemm ' cblas sgemm.c .text 0x1e6 undefined reference sgemm ' collect2 error ld returned 1 exit status make bin im2rec error 1"
incubator-mxnet,15069,"profiler rfc introducing new apis introducing new apis motivation mxnet comes profiler allows users monitor performance models two metrics time memory consumption. internally, operator calls, c api calls, memory allocation deallocation represented events. functions calls, know start finish time events therefore duration. memory operations, know time allocation deallocation size memory chunk. ! screen shot 2019 05 24 4 16 39 pm https user images.githubusercontent.com 10722037 58362190 49f4c080 7e49 11e9 92a3 23664384544b.png currently, profiler function called return aggregate statistics, include min, max, average entries device memory, operator, c api. current return value string data presented table fashion refer screenshot . however, table nicely formatted, meant read humans easily parse able otherwise program. so, need api returns aggregate stats json string. specification new api, , introduced. two parameters 1. sort specifies statistic sort entries. defaults avg valid options min , max , avg . 2. ascending specifies entries sorted. defaults false valid options true, false . expected use cases include 1. customers interested events stats others, customize data presentation efficiently monitor models. 2. customers easily pass stats automated performance tests monitoring tools. need parse table like string returned . 3. new api immediately useful new operator level benchmark tool sandeep krishnamurthy work on. cwiki https cwiki.apache.org confluence display mxnet mxnet operator benchmarks. structure json return value shown below. four layer dictionary structure. 1st layer time , memory , unit . 2nd layer category operators apis fall into. 3rd layer operators apis. finally, 4th layer stats. notice time unit ms memory unit byte. asides also another new api, , clear aggregate statistics now. typical use case like complex case, suppose want use profiler benchmark various sections model, call end section supposedly end loop neatly like fixing output dumps ! screen shot 2019 05 23 5 23 56 pm https user images.githubusercontent.com 10722037 58362201 7c062280 7e49 11e9 88ec 3ab102c95795.png currently labeling table slightly off. memory related entries labels usage rather time . time ms column also make sense memory entries, removed memory entries. new table labeling look like f q 1. can't use current dumps api? use current dumps api basically get save information, need manually parse table good user experience. 1. add new profiler api back end rather python parser utility returns json? use new api different languages make sure return consistent.",0,profiler rfc introducing new apis,"profiler rfc introducing new apis profiler rfc introducing new apis introducing new apis motivation mxnet comes profiler allows users monitor performance models two metrics time memory consumption. internally, operator calls, c api calls, memory allocation deallocation represented events. functions calls, know start finish time events therefore duration. memory operations, know time allocation deallocation size memory chunk. ! screen shot 2019 05 24 4 16 39 pm https user images.githubusercontent.com 10722037 58362190 49f4c080 7e49 11e9 92a3 23664384544b.png currently, profiler function called return aggregate statistics, include min, max, average entries device memory, operator, c api. current return value string data presented table fashion refer screenshot . however, table nicely formatted, meant read humans easily parse able otherwise program. so, need api returns aggregate stats json string. specification new api, , introduced. two parameters 1. sort specifies statistic sort entries. defaults avg valid options min , max , avg . 2. ascending specifies entries sorted. defaults false valid options true, false . expected use cases include 1. customers interested events stats others, customize data presentation efficiently monitor models. 2. customers easily pass stats automated performance tests monitoring tools. need parse table like string returned . 3. new api immediately useful new operator level benchmark tool sandeep krishnamurthy work on. cwiki https cwiki.apache.org confluence display mxnet mxnet operator benchmarks. structure json return value shown below. four layer dictionary structure. 1st layer time , memory , unit . 2nd layer category operators apis fall into. 3rd layer operators apis. finally, 4th layer stats. notice time unit ms memory unit byte. asides also another new api, , clear aggregate statistics now. typical use case like complex case, suppose want use profiler benchmark various sections model, call end section supposedly end loop neatly like fixing output dumps ! screen shot 2019 05 23 5 23 56 pm https user images.githubusercontent.com 10722037 58362201 7c062280 7e49 11e9 88ec 3ab102c95795.png currently labeling table slightly off. memory related entries labels usage rather time . time ms column also make sense memory entries, removed memory entries. new table labeling look like f q 1. can't use current dumps api? use current dumps api basically get save information, need manually parse table good user experience. 1. add new profiler api back end rather python parser utility returns json? use new api different languages make sure return consistent."
incubator-mxnet,10520,"looks operator counter aggregated output 2x real value. used gluon model mainly copied https mxnet.incubator.apache.org tutorials gluon gluon.html, contains 2 convolutions forward pass profiling, aggregated convolution counter 4 instead 2, counter doubled ops well. mx.gpu 0 mx.gpu 0 , mx.gpu 1",0,mxnet operator profile aggregate counter issue,"mxnet operator profile aggregate counter issue looks operator counter aggregated output 2x real value. used gluon model mainly copied https mxnet.incubator.apache.org tutorials gluon gluon.html, contains 2 convolutions forward pass profiling, aggregated convolution counter 4 instead 2, counter doubled ops well. mx.gpu 0 mx.gpu 0 , mx.gpu 1"
incubator-mxnet,5416,"running resnet 50 model http data.mxnet.io models imagenet test caffe produces error importing batchnorm layers. caffe installed, using pure protobuf version importer. nirbenz, mli environment info osx mxnet version 0.9.4 dea87660c9d3b55ccd28a812116c93ca9e5032c2 protobuf version python version 3.5 error message converting layer conv1, wmat shape 64, 3, 7, 7 , bias shape 64, traceback recent call last file convert model.py , line 162, main file convert model.py , line 158, main convert model args.prototxt, args.caffemodel, args.save model name file convert model.py , line 127, convert model mean mean.reshape aux shape dic mean name attributeerror 'repeatedscalarfieldcontainer' object attribute 'reshape'",0,caffe importer fails resnet 50,"caffe importer fails resnet 50 running resnet 50 model http data.mxnet.io models imagenet test caffe produces error importing batchnorm layers. caffe installed, using pure protobuf version importer. nirbenz, mli environment info osx mxnet version 0.9.4 dea87660c9d3b55ccd28a812116c93ca9e5032c2 protobuf version python version 3.5 error message converting layer conv1, wmat shape 64, 3, 7, 7 , bias shape 64, traceback recent call last file convert model.py , line 162, main file convert model.py , line 158, main convert model args.prototxt, args.caffemodel, args.save model name file convert model.py , line 127, convert model mean mean.reshape aux shape dic mean name attributeerror 'repeatedscalarfieldcontainer' object attribute 'reshape'"
incubator-mxnet,5098,"recently run tutorial handwritten digit recognition http mxnet.io tutorials python mnist.html , change optimizer option optimizer object following got poor result. find optimizer class set mini batch optimization parameter rescale grad 1.0 batch size . correct code intended design?",0,optimizer object may properly initialized,"optimizer object may properly initialized recently run tutorial handwritten digit recognition http mxnet.io tutorials python mnist.html , change optimizer option optimizer object following got poor result. find optimizer class set mini batch optimization parameter rescale grad 1.0 batch size . correct code intended design?"
incubator-mxnet,793,"yesterday read paper, find weight decay 0.05, correct? mli",0,mxnet paper googlenet weight decay parameter,"mxnet paper googlenet weight decay parameter yesterday read paper, find weight decay 0.05, correct? mli"
incubator-mxnet,9623,"trying get familiar mxnet framework browsing tutorial section https mxnet.apache.org tutorials index.html. come across illustrations generating vector representations words, like word2vec models https en.wikipedia.org wiki word2vec . would make sense create pr one tutorial. something make sense code repo.",0,tutorial word embeddings using mxnet,"tutorial word embeddings using mxnet trying get familiar mxnet framework browsing tutorial section https mxnet.apache.org tutorials index.html. come across illustrations generating vector representations words, like word2vec models https en.wikipedia.org wiki word2vec . would make sense create pr one tutorial. something make sense code repo."
incubator-mxnet,449,"mxnet looks promising would like give try. want train triplet network like google facenet. one key issue get succefull training selection good triplet data fed network. since generating possible triplets tractable, selecting training good compromise. currently possible add additional steps data iterator sgd step one statistics current batch using current model order select good triplets ? idea transform source data set labeled image triplet training. currently, use fuel blocks frameworks chain different transformer achieve this. may mxnet already kind data pipeline transformation, plug fuel mxnet ?",0,real time data augmentation online batch generation.,"real time data augmentation online batch generation. mxnet looks promising would like give try. want train triplet network like google facenet. one key issue get succefull training selection good triplet data fed network. since generating possible triplets tractable, selecting training good compromise. currently possible add additional steps data iterator sgd step one statistics current batch using current model order select good triplets ? idea transform source data set labeled image triplet training. currently, use fuel blocks frameworks chain different transformer achieve this. may mxnet already kind data pipeline transformation, plug fuel mxnet ?"
incubator-mxnet,4436,"tested amalgamation v0.9.1pre get work. environment amalgamates perfectly v0.8.0. initial error get commenting out, tried manually change paths comment headers located source. one issue files linked incorrectly, e.g. several elementwise operators missing eventually amalgamation simply fails without explanation something significant change requirements amalgamation? environment info operating system android amalgamation mxnet commit hash 84e5155fc26563ef8bebc08f201e8a88d4c3845e",0,amalgamation android broken v0.9.1pre,"amalgamation android broken v0.9.1pre tested amalgamation v0.9.1pre get work. environment amalgamates perfectly v0.8.0. initial error get commenting out, tried manually change paths comment headers located source. one issue files linked incorrectly, e.g. several elementwise operators missing eventually amalgamation simply fails without explanation something significant change requirements amalgamation? environment info operating system android amalgamation mxnet commit hash 84e5155fc26563ef8bebc08f201e8a88d4c3845e"
incubator-mxnet,1525,problem . check data iterator,0,check failed ! idx2label .end fail find imagelabel id 470889,check failed ! idx2label .end fail find imagelabel id 470889 problem . check data iterator
incubator-mxnet,2398,sometimes raise error,0,neural art save image error,neural art save image error sometimes raise error
incubator-mxnet,3137,"sample size batch size ! 0, last batch generated next still initialized ndarray.empty using original batch size set pad batch size real size . however, mxnet training process seems ignore pad field databatch. whole batch taken calculation, may produce nan result linear regression model. find uninitialized part last batch contains randomly huge numbers like xxxxxe10. using ndarray.zeros initialize batch discarding last batch fix case. possible use different batch size different batch dataiter object?",0,batch padding bug ml.dmlc.mxnet.spark.io.labeledpointiter,"batch padding bug ml.dmlc.mxnet.spark.io.labeledpointiter sample size batch size ! 0, last batch generated next still initialized ndarray.empty using original batch size set pad batch size real size . however, mxnet training process seems ignore pad field databatch. whole batch taken calculation, may produce nan result linear regression model. find uninitialized part last batch contains randomly huge numbers like xxxxxe10. using ndarray.zeros initialize batch discarding last batch fix case. possible use different batch size different batch dataiter object?"
incubator-mxnet,2857,"training code following, like code example.",0,"several iteration, model.exec.output 0 .asnumpy get slower slower !!","several iteration, model.exec.output 0 .asnumpy get slower slower !! training code following, like code example."
incubator-mxnet,11263,java' double free corruption fasttop 0x00007f91e0db8270 backtrace lib x86 64 linux gnu libc.so.6 0x777e5 0x7f9527a337e5 lib x86 64 linux gnu libc.so.6 0x8037a 0x7f9527a3c37a lib x86 64 linux gnu libc.so.6 cfree 0x4c 0x7f9527a4053c work mxnet scala package native linux x86 64 gpu target libmxnet scala linux x86 64 gpu.so znst10 hashtableinst7 cxx1112basic stringicst11char traitsicesaiceeest4pairiks5 s5 esais8 enst8 detail10 select1stest8equal tois5 est4hashis5 ensa 18 mod range hashingensa 20 default ranged hashensa 20 prime rehash policyensa 17 hashtable traitsilb1elb0elb1eeee9 assigniznsl aserksl eulpknsa 10 hash nodeis8 lb1eeee0 eevso rkt 0x81 0x7f92a8a7ef11 work mxnet scala package native linux x86 64 gpu target libmxnet scala linux x86 64 gpu.so znst10 hashtableinst7 cxx1112basic stringicst11char traitsicesaiceeest4pairiks5 s5 esais8 enst8 detail10 select1stest8equal tois5 est4hashis5 ensa 18 mod range hashingensa 20 default ranged hashensa 20 prime rehash policyensa 17 hashtable traitsilb1elb0elb1eeeeaserksl 0xa2 0x7f92a8a7f212 work mxnet scala package native linux x86 64 gpu target libmxnet scala linux x86 64 gpu.so mxinitpsenv 0x346 0x7f92ab65d686 work mxnet scala package native linux x86 64 gpu target libmxnet scala linux x86 64 gpu.so java org apache mxnet libinfo mxinitpsenv 0xf5 0x7f92a89c8895 0x7f95110184e7 please see http jenkins.mxnet ci.amazon ml.com blue organizations jenkins incubator mxnet detail pr 11210 5 pipeline,0,memory corruption issue scala test,memory corruption issue scala test java' double free corruption fasttop 0x00007f91e0db8270 backtrace lib x86 64 linux gnu libc.so.6 0x777e5 0x7f9527a337e5 lib x86 64 linux gnu libc.so.6 0x8037a 0x7f9527a3c37a lib x86 64 linux gnu libc.so.6 cfree 0x4c 0x7f9527a4053c work mxnet scala package native linux x86 64 gpu target libmxnet scala linux x86 64 gpu.so znst10 hashtableinst7 cxx1112basic stringicst11char traitsicesaiceeest4pairiks5 s5 esais8 enst8 detail10 select1stest8equal tois5 est4hashis5 ensa 18 mod range hashingensa 20 default ranged hashensa 20 prime rehash policyensa 17 hashtable traitsilb1elb0elb1eeee9 assigniznsl aserksl eulpknsa 10 hash nodeis8 lb1eeee0 eevso rkt 0x81 0x7f92a8a7ef11 work mxnet scala package native linux x86 64 gpu target libmxnet scala linux x86 64 gpu.so znst10 hashtableinst7 cxx1112basic stringicst11char traitsicesaiceeest4pairiks5 s5 esais8 enst8 detail10 select1stest8equal tois5 est4hashis5 ensa 18 mod range hashingensa 20 default ranged hashensa 20 prime rehash policyensa 17 hashtable traitsilb1elb0elb1eeeeaserksl 0xa2 0x7f92a8a7f212 work mxnet scala package native linux x86 64 gpu target libmxnet scala linux x86 64 gpu.so mxinitpsenv 0x346 0x7f92ab65d686 work mxnet scala package native linux x86 64 gpu target libmxnet scala linux x86 64 gpu.so java org apache mxnet libinfo mxinitpsenv 0xf5 0x7f92a89c8895 0x7f95110184e7 please see http jenkins.mxnet ci.amazon ml.com blue organizations jenkins incubator mxnet detail pr 11210 5 pipeline
incubator-mxnet,13024,description sphinx throws error generating docs function. error,0,mxnet.random.seed docs error,mxnet.random.seed docs error description sphinx throws error generating docs function. error
incubator-mxnet,4465,"environment info operating system compiler package used python r scala julia mxnet version python version distribution error message root mxnet dmlc core' g c o3 wall wno unknown pragmas iinclude std c 0x fopenmp fpic ddmlc use hdfs 0 ddmlc use s3 0 ddmlc use azure 0 msse2 line split.o src io line split.cc usr local cuda bin nvcc std c 11 xcompiler force inlines g o3 ccbin g gencode arch compute 30,code sm 30 gencode arch compute 35,code sm 35 gencode arch compute 50,code sm 50 gencode arch compute 50,code compute 50 xcompiler dmshadow force stream wall o3 root mxnet mshadow root mxnet dmlc core include fpic iinclude funroll loops wno unused parameter wno unknown pragmas wno unused local typedefs msse3 usr local cuda include dmshadow use cblas 1 dmshadow use mkl 0 dmshadow rabit ps 0 dmshadow dist ps 0 dmsdhadow use pascal 0 dmxnet use opencv 1 usr include opencv fopenmp dmshadow use cudnn 1 dmxnet use nvrtc 0 mt build src ndarray ndarray function gpu.o src ndarray ndarray function.cu build src ndarray ndarray function gpu.d usr local cuda bin nvcc std c 11 xcompiler force inlines g o3 ccbin g gencode arch compute 30,code sm 30 gencode arch compute 35,code sm 35 gencode arch compute 50,code sm 50 gencode arch compute 50,code compute 50 xcompiler dmshadow force stream wall o3 root mxnet mshadow root mxnet dmlc core include fpic iinclude funroll loops wno unused parameter wno unknown pragmas wno unused local typedefs msse3 usr local cuda include dmshadow use cblas 1 dmshadow use mkl 0 dmshadow rabit ps 0 dmshadow dist ps 0 dmsdhadow use pascal 0 dmxnet use opencv 1 usr include opencv fopenmp dmshadow use cudnn 1 dmxnet use nvrtc 0 mt build src operator activation gpu.o src operator activation.cu build src operator activation gpu.d g c o3 wall wno unknown pragmas iinclude std c 0x fopenmp fpic ddmlc use hdfs 0 ddmlc use s3 0 ddmlc use azure 0 msse2 recordio split.o src io recordio split.cc usr local cuda bin nvcc c build src ndarray ndarray function gpu.o std c 11 xcompiler force inlines g o3 ccbin g gencode arch compute 30,code sm 30 gencode arch compute 35,code sm 35 gencode arch compute 50,code sm 50 gencode arch compute 50,code compute 50 xcompiler dmshadow force stream wall o3 root mxnet mshadow root mxnet dmlc core include fpic iinclude funroll loops wno unused parameter wno unknown pragmas wno unused local typedefs msse3 usr local cuda include dmshadow use cblas 1 dmshadow use mkl 0 dmshadow rabit ps 0 dmshadow dist ps 0 dmsdhadow use pascal 0 dmxnet use opencv 1 usr include opencv fopenmp dmshadow use cudnn 1 dmxnet use nvrtc 0 src ndarray ndarray function.cu g c o3 wall wno unknown pragmas iinclude std c 0x fopenmp fpic ddmlc use hdfs 0 ddmlc use s3 0 ddmlc use azure 0 msse2 input split base.o src io input split base.cc usr local cuda bin nvcc c build src operator activation gpu.o std c 11 xcompiler force inlines g o3 ccbin g gencode arch compute 30,code sm 30 gencode arch compute 35,code sm 35 gencode arch compute 50,code sm 50 gencode arch compute 50,code compute 50 xcompiler dmshadow force stream wall o3 root mxnet mshadow root mxnet dmlc core include fpic iinclude funroll loops wno unused parameter wno unknown pragmas wno unused local typedefs msse3 usr local cuda include dmshadow use cblas 1 dmshadow use mkl 0 dmshadow rabit ps 0 dmshadow dist ps 0 dmsdhadow use pascal 0 dmxnet use opencv 1 usr include opencv fopenmp dmshadow use cudnn 1 dmxnet use nvrtc 0 src operator activation.cu usr local cuda bin nvcc std c 11 xcompiler force inlines g o3 ccbin g gencode arch compute 30,code sm 30 gencode arch compute 35,code sm 35 gencode arch compute 50,code sm 50 gencode arch compute 50,code compute 50 xcompiler dmshadow force stream wall o3 root mxnet mshadow root mxnet dmlc core include fpic iinclude funroll loops wno unused parameter wno unknown pragmas wno unused local typedefs msse3 usr local cuda include dmshadow use cblas 1 dmshadow use mkl 0 dmshadow rabit ps 0 dmshadow dist ps 0 dmsdhadow use pascal 0 dmxnet use opencv 1 usr include opencv fopenmp dmshadow use cudnn 1 dmxnet use nvrtc 0 mt build src operator batch norm gpu.o src operator batch norm.cu build src operator batch norm gpu.d g c o3 wall wno unknown pragmas iinclude std c 0x fopenmp fpic ddmlc use hdfs 0 ddmlc use s3 0 ddmlc use azure 0 msse2 io.o src io.cc usr local cuda bin nvcc c build src operator batch norm gpu.o std c 11 xcompiler force inlines g o3 ccbin g gencode arch compute 30,code sm 30 gencode arch compute 35,code sm 35 gencode arch compute 50,code sm 50 gencode arch compute 50,code compute 50 xcompiler dmshadow force stream wall o3 root mxnet mshadow root mxnet dmlc core include fpic iinclude funroll loops wno unused parameter wno unknown pragmas wno unused local typedefs msse3 usr local cuda include dmshadow use cblas 1 dmshadow use mkl 0 dmshadow rabit ps 0 dmshadow dist ps 0 dmsdhadow use pascal 0 dmxnet use opencv 1 usr include opencv fopenmp dmshadow use cudnn 1 dmxnet use nvrtc 0 src operator batch norm.cu usr local cuda bin nvcc std c 11 xcompiler force inlines g o3 ccbin g gencode arch compute 30,code sm 30 gencode arch compute 35,code sm 35 gencode arch compute 50,code sm 50 gencode arch compute 50,code compute 50 xcompiler dmshadow force stream wall o3 root mxnet mshadow root mxnet dmlc core include fpic iinclude funroll loops wno unused parameter wno unknown pragmas wno unused local typedefs msse3 usr local cuda include dmshadow use cblas 1 dmshadow use mkl 0 dmshadow rabit ps 0 dmshadow dist ps 0 dmsdhadow use pascal 0 dmxnet use opencv 1 usr include opencv fopenmp dmshadow use cudnn 1 dmxnet use nvrtc 0 mt build src operator block grad gpu.o src operator block grad.cu build src operator block grad gpu.d g c o3 wall wno unknown pragmas iinclude std c 0x fopenmp fpic ddmlc use hdfs 0 ddmlc use s3 0 ddmlc use azure 0 msse2 local filesys.o src io local filesys.cc usr local cuda bin nvcc std c 11 xcompiler force inlines g o3 ccbin g gencode arch compute 30,code sm 30 gencode arch compute 35,code sm 35 gencode arch compute 50,code sm 50 gencode arch compute 50,code compute 50 xcompiler dmshadow force stream wall o3 root mxnet mshadow root mxnet dmlc core include fpic iinclude funroll loops wno unused parameter wno unknown pragmas wno unused local typedefs msse3 usr local cuda include dmshadow use cblas 1 dmshadow use mkl 0 dmshadow rabit ps 0 dmshadow dist ps 0 dmsdhadow use pascal 0 dmxnet use opencv 1 usr include opencv fopenmp dmshadow use cudnn 1 dmxnet use nvrtc 0 mt build src operator broadcast mask op gpu.o src operator broadcast mask op.cu build src operator broadcast mask op gpu.d g c o3 wall wno unknown pragmas iinclude std c 0x fopenmp fpic ddmlc use hdfs 0 ddmlc use s3 0 ddmlc use azure 0 msse2 data.o src data.cc usr local cuda bin nvcc c build src operator block grad gpu.o std c 11 xcompiler force inlines g o3 ccbin g gencode arch compute 30,code sm 30 gencode arch compute 35,code sm 35 gencode arch compute 50,code sm 50 gencode arch compute 50,code compute 50 xcompiler dmshadow force stream wall o3 root mxnet mshadow root mxnet dmlc core include fpic iinclude funroll loops wno unused parameter wno unknown pragmas wno unused local typedefs msse3 usr local cuda include dmshadow use cblas 1 dmshadow use mkl 0 dmshadow rabit ps 0 dmshadow dist ps 0 dmsdhadow use pascal 0 dmxnet use opencv 1 usr include opencv fopenmp dmshadow use cudnn 1 dmxnet use nvrtc 0 src operator block grad.cu usr local cuda bin nvcc c build src operator broadcast mask op gpu.o std c 11 xcompiler force inlines g o3 ccbin g gencode arch compute 30,code sm 30 gencode arch compute 35,code sm 35 gencode arch compute 50,code sm 50 gencode arch compute 50,code compute 50 xcompiler dmshadow force stream wall o3 root mxnet mshadow root mxnet dmlc core include fpic iinclude funroll loops wno unused parameter wno unknown pragmas wno unused local typedefs msse3 usr local cuda include dmshadow use cblas 1 dmshadow use mkl 0 dmshadow rabit ps 0 dmshadow dist ps 0 dmsdhadow use pascal 0 dmxnet use opencv 1 usr include opencv fopenmp dmshadow use cudnn 1 dmxnet use nvrtc 0 src operator broadcast mask op.cu g c o3 wall wno unknown pragmas iinclude std c 0x fopenmp fpic ddmlc use hdfs 0 ddmlc use s3 0 ddmlc use azure 0 msse2 recordio.o src recordio.cc g c o3 wall wno unknown pragmas iinclude std c 0x fopenmp fpic ddmlc use hdfs 0 ddmlc use s3 0 ddmlc use azure 0 msse2 config.o src config.cc usr local cuda bin nvcc std c 11 xcompiler force inlines g o3 ccbin g gencode arch compute 30,code sm 30 gencode arch compute 35,code sm 35 gencode arch compute 50,code sm 50 gencode arch compute 50,code compute 50 xcompiler dmshadow force stream wall o3 root mxnet mshadow root mxnet dmlc core include fpic iinclude funroll loops wno unused parameter wno unknown pragmas wno unused local typedefs msse3 usr local cuda include dmshadow use cblas 1 dmshadow use mkl 0 dmshadow rabit ps 0 dmshadow dist ps 0 dmsdhadow use pascal 0 dmxnet use opencv 1 usr include opencv fopenmp dmshadow use cudnn 1 dmxnet use nvrtc 0 mt build src operator broadcast reduce op gpu.o src operator broadcast reduce op.cu build src operator broadcast reduce op gpu.d ar cr libdmlc.a line split.o recordio split.o input split base.o io.o local filesys.o data.o recordio.o config.o make 1 leaving directory mxnet graphstorageallocator get long, mxnet tshape ' graph memory allocator.cc .text 0x6da undefined reference could help understand error, would really appreciate it. thanks! besir",0,mxnet installation fails ubuntu 14.04.4 lts,"mxnet installation fails ubuntu 14.04.4 lts environment info operating system compiler package used python r scala julia mxnet version python version distribution error message root mxnet dmlc core' g c o3 wall wno unknown pragmas iinclude std c 0x fopenmp fpic ddmlc use hdfs 0 ddmlc use s3 0 ddmlc use azure 0 msse2 line split.o src io line split.cc usr local cuda bin nvcc std c 11 xcompiler force inlines g o3 ccbin g gencode arch compute 30,code sm 30 gencode arch compute 35,code sm 35 gencode arch compute 50,code sm 50 gencode arch compute 50,code compute 50 xcompiler dmshadow force stream wall o3 root mxnet mshadow root mxnet dmlc core include fpic iinclude funroll loops wno unused parameter wno unknown pragmas wno unused local typedefs msse3 usr local cuda include dmshadow use cblas 1 dmshadow use mkl 0 dmshadow rabit ps 0 dmshadow dist ps 0 dmsdhadow use pascal 0 dmxnet use opencv 1 usr include opencv fopenmp dmshadow use cudnn 1 dmxnet use nvrtc 0 mt build src ndarray ndarray function gpu.o src ndarray ndarray function.cu build src ndarray ndarray function gpu.d usr local cuda bin nvcc std c 11 xcompiler force inlines g o3 ccbin g gencode arch compute 30,code sm 30 gencode arch compute 35,code sm 35 gencode arch compute 50,code sm 50 gencode arch compute 50,code compute 50 xcompiler dmshadow force stream wall o3 root mxnet mshadow root mxnet dmlc core include fpic iinclude funroll loops wno unused parameter wno unknown pragmas wno unused local typedefs msse3 usr local cuda include dmshadow use cblas 1 dmshadow use mkl 0 dmshadow rabit ps 0 dmshadow dist ps 0 dmsdhadow use pascal 0 dmxnet use opencv 1 usr include opencv fopenmp dmshadow use cudnn 1 dmxnet use nvrtc 0 mt build src operator activation gpu.o src operator activation.cu build src operator activation gpu.d g c o3 wall wno unknown pragmas iinclude std c 0x fopenmp fpic ddmlc use hdfs 0 ddmlc use s3 0 ddmlc use azure 0 msse2 recordio split.o src io recordio split.cc usr local cuda bin nvcc c build src ndarray ndarray function gpu.o std c 11 xcompiler force inlines g o3 ccbin g gencode arch compute 30,code sm 30 gencode arch compute 35,code sm 35 gencode arch compute 50,code sm 50 gencode arch compute 50,code compute 50 xcompiler dmshadow force stream wall o3 root mxnet mshadow root mxnet dmlc core include fpic iinclude funroll loops wno unused parameter wno unknown pragmas wno unused local typedefs msse3 usr local cuda include dmshadow use cblas 1 dmshadow use mkl 0 dmshadow rabit ps 0 dmshadow dist ps 0 dmsdhadow use pascal 0 dmxnet use opencv 1 usr include opencv fopenmp dmshadow use cudnn 1 dmxnet use nvrtc 0 src ndarray ndarray function.cu g c o3 wall wno unknown pragmas iinclude std c 0x fopenmp fpic ddmlc use hdfs 0 ddmlc use s3 0 ddmlc use azure 0 msse2 input split base.o src io input split base.cc usr local cuda bin nvcc c build src operator activation gpu.o std c 11 xcompiler force inlines g o3 ccbin g gencode arch compute 30,code sm 30 gencode arch compute 35,code sm 35 gencode arch compute 50,code sm 50 gencode arch compute 50,code compute 50 xcompiler dmshadow force stream wall o3 root mxnet mshadow root mxnet dmlc core include fpic iinclude funroll loops wno unused parameter wno unknown pragmas wno unused local typedefs msse3 usr local cuda include dmshadow use cblas 1 dmshadow use mkl 0 dmshadow rabit ps 0 dmshadow dist ps 0 dmsdhadow use pascal 0 dmxnet use opencv 1 usr include opencv fopenmp dmshadow use cudnn 1 dmxnet use nvrtc 0 src operator activation.cu usr local cuda bin nvcc std c 11 xcompiler force inlines g o3 ccbin g gencode arch compute 30,code sm 30 gencode arch compute 35,code sm 35 gencode arch compute 50,code sm 50 gencode arch compute 50,code compute 50 xcompiler dmshadow force stream wall o3 root mxnet mshadow root mxnet dmlc core include fpic iinclude funroll loops wno unused parameter wno unknown pragmas wno unused local typedefs msse3 usr local cuda include dmshadow use cblas 1 dmshadow use mkl 0 dmshadow rabit ps 0 dmshadow dist ps 0 dmsdhadow use pascal 0 dmxnet use opencv 1 usr include opencv fopenmp dmshadow use cudnn 1 dmxnet use nvrtc 0 mt build src operator batch norm gpu.o src operator batch norm.cu build src operator batch norm gpu.d g c o3 wall wno unknown pragmas iinclude std c 0x fopenmp fpic ddmlc use hdfs 0 ddmlc use s3 0 ddmlc use azure 0 msse2 io.o src io.cc usr local cuda bin nvcc c build src operator batch norm gpu.o std c 11 xcompiler force inlines g o3 ccbin g gencode arch compute 30,code sm 30 gencode arch compute 35,code sm 35 gencode arch compute 50,code sm 50 gencode arch compute 50,code compute 50 xcompiler dmshadow force stream wall o3 root mxnet mshadow root mxnet dmlc core include fpic iinclude funroll loops wno unused parameter wno unknown pragmas wno unused local typedefs msse3 usr local cuda include dmshadow use cblas 1 dmshadow use mkl 0 dmshadow rabit ps 0 dmshadow dist ps 0 dmsdhadow use pascal 0 dmxnet use opencv 1 usr include opencv fopenmp dmshadow use cudnn 1 dmxnet use nvrtc 0 src operator batch norm.cu usr local cuda bin nvcc std c 11 xcompiler force inlines g o3 ccbin g gencode arch compute 30,code sm 30 gencode arch compute 35,code sm 35 gencode arch compute 50,code sm 50 gencode arch compute 50,code compute 50 xcompiler dmshadow force stream wall o3 root mxnet mshadow root mxnet dmlc core include fpic iinclude funroll loops wno unused parameter wno unknown pragmas wno unused local typedefs msse3 usr local cuda include dmshadow use cblas 1 dmshadow use mkl 0 dmshadow rabit ps 0 dmshadow dist ps 0 dmsdhadow use pascal 0 dmxnet use opencv 1 usr include opencv fopenmp dmshadow use cudnn 1 dmxnet use nvrtc 0 mt build src operator block grad gpu.o src operator block grad.cu build src operator block grad gpu.d g c o3 wall wno unknown pragmas iinclude std c 0x fopenmp fpic ddmlc use hdfs 0 ddmlc use s3 0 ddmlc use azure 0 msse2 local filesys.o src io local filesys.cc usr local cuda bin nvcc std c 11 xcompiler force inlines g o3 ccbin g gencode arch compute 30,code sm 30 gencode arch compute 35,code sm 35 gencode arch compute 50,code sm 50 gencode arch compute 50,code compute 50 xcompiler dmshadow force stream wall o3 root mxnet mshadow root mxnet dmlc core include fpic iinclude funroll loops wno unused parameter wno unknown pragmas wno unused local typedefs msse3 usr local cuda include dmshadow use cblas 1 dmshadow use mkl 0 dmshadow rabit ps 0 dmshadow dist ps 0 dmsdhadow use pascal 0 dmxnet use opencv 1 usr include opencv fopenmp dmshadow use cudnn 1 dmxnet use nvrtc 0 mt build src operator broadcast mask op gpu.o src operator broadcast mask op.cu build src operator broadcast mask op gpu.d g c o3 wall wno unknown pragmas iinclude std c 0x fopenmp fpic ddmlc use hdfs 0 ddmlc use s3 0 ddmlc use azure 0 msse2 data.o src data.cc usr local cuda bin nvcc c build src operator block grad gpu.o std c 11 xcompiler force inlines g o3 ccbin g gencode arch compute 30,code sm 30 gencode arch compute 35,code sm 35 gencode arch compute 50,code sm 50 gencode arch compute 50,code compute 50 xcompiler dmshadow force stream wall o3 root mxnet mshadow root mxnet dmlc core include fpic iinclude funroll loops wno unused parameter wno unknown pragmas wno unused local typedefs msse3 usr local cuda include dmshadow use cblas 1 dmshadow use mkl 0 dmshadow rabit ps 0 dmshadow dist ps 0 dmsdhadow use pascal 0 dmxnet use opencv 1 usr include opencv fopenmp dmshadow use cudnn 1 dmxnet use nvrtc 0 src operator block grad.cu usr local cuda bin nvcc c build src operator broadcast mask op gpu.o std c 11 xcompiler force inlines g o3 ccbin g gencode arch compute 30,code sm 30 gencode arch compute 35,code sm 35 gencode arch compute 50,code sm 50 gencode arch compute 50,code compute 50 xcompiler dmshadow force stream wall o3 root mxnet mshadow root mxnet dmlc core include fpic iinclude funroll loops wno unused parameter wno unknown pragmas wno unused local typedefs msse3 usr local cuda include dmshadow use cblas 1 dmshadow use mkl 0 dmshadow rabit ps 0 dmshadow dist ps 0 dmsdhadow use pascal 0 dmxnet use opencv 1 usr include opencv fopenmp dmshadow use cudnn 1 dmxnet use nvrtc 0 src operator broadcast mask op.cu g c o3 wall wno unknown pragmas iinclude std c 0x fopenmp fpic ddmlc use hdfs 0 ddmlc use s3 0 ddmlc use azure 0 msse2 recordio.o src recordio.cc g c o3 wall wno unknown pragmas iinclude std c 0x fopenmp fpic ddmlc use hdfs 0 ddmlc use s3 0 ddmlc use azure 0 msse2 config.o src config.cc usr local cuda bin nvcc std c 11 xcompiler force inlines g o3 ccbin g gencode arch compute 30,code sm 30 gencode arch compute 35,code sm 35 gencode arch compute 50,code sm 50 gencode arch compute 50,code compute 50 xcompiler dmshadow force stream wall o3 root mxnet mshadow root mxnet dmlc core include fpic iinclude funroll loops wno unused parameter wno unknown pragmas wno unused local typedefs msse3 usr local cuda include dmshadow use cblas 1 dmshadow use mkl 0 dmshadow rabit ps 0 dmshadow dist ps 0 dmsdhadow use pascal 0 dmxnet use opencv 1 usr include opencv fopenmp dmshadow use cudnn 1 dmxnet use nvrtc 0 mt build src operator broadcast reduce op gpu.o src operator broadcast reduce op.cu build src operator broadcast reduce op gpu.d ar cr libdmlc.a line split.o recordio split.o input split base.o io.o local filesys.o data.o recordio.o config.o make 1 leaving directory mxnet graphstorageallocator get long, mxnet tshape ' graph memory allocator.cc .text 0x6da undefined reference could help understand error, would really appreciate it. thanks! besir"
incubator-mxnet,13923,"hi, recently tried installing mxnet rstudio windows 7. command got error got finally, found somewhere seaching fix installed trying load library know windows 7 fully supported hope case. file server missing something? would grateful someone could help thanks session info r version 3.5.1 2018 07 02 platform i386 w64 mingw32 i386 32 bit running windows 7 x64 build 7601 service pack 1 matrix products default locale 1 lc collate polish poland.1250 lc ctype polish poland.1250 lc monetary polish poland.1250 4 lc numeric c lc time polish poland.1250 attached base packages 1 stats graphics grdevices utils datasets methods base attached packages 1 arules 1.6 2 matrix 1.2 14 loaded via namespace attached 1 coin 1.2 2 lattice 0.20 38 codetools 0.2 16 mvtnorm 1.0 8 zoo 1.8 4 6 mass 7.3 51.1 grid 3.5.1 stats4 3.5.1 multcomp 1.4 8 strucchange 1.5 1 11 party 1.3 1 sandwich 2.5 0 splines 3.5.1 th.data 1.0 9 tools 3.5.1 16 survival 2.43 3 compiler 3.5.1 modeltools 0.2 22",0,cannot install mxnet r cannot open url,"cannot install mxnet r cannot open url hi, recently tried installing mxnet rstudio windows 7. command got error got finally, found somewhere seaching fix installed trying load library know windows 7 fully supported hope case. file server missing something? would grateful someone could help thanks session info r version 3.5.1 2018 07 02 platform i386 w64 mingw32 i386 32 bit running windows 7 x64 build 7601 service pack 1 matrix products default locale 1 lc collate polish poland.1250 lc ctype polish poland.1250 lc monetary polish poland.1250 4 lc numeric c lc time polish poland.1250 attached base packages 1 stats graphics grdevices utils datasets methods base attached packages 1 arules 1.6 2 matrix 1.2 14 loaded via namespace attached 1 coin 1.2 2 lattice 0.20 38 codetools 0.2 16 mvtnorm 1.0 8 zoo 1.8 4 6 mass 7.3 51.1 grid 3.5.1 stats4 3.5.1 multcomp 1.4 8 strucchange 1.5 1 11 party 1.3 1 sandwich 2.5 0 splines 3.5.1 th.data 1.0 9 tools 3.5.1 16 survival 2.43 3 compiler 3.5.1 modeltools 0.2 22"
incubator-mxnet,2783,example bidirectional lstm https github.com dmlc mxnet tree master example bi lstm sort misses gen data.py script referenced readme maybe get committed due gitignore rules? xlvector still script lying around?,0,missing script example bi lstm sort,missing script example bi lstm sort example bidirectional lstm https github.com dmlc mxnet tree master example bi lstm sort misses gen data.py script referenced readme maybe get committed due gitignore rules? xlvector still script lying around?
incubator-mxnet,10387,please see http jenkins.mxnet ci.amazon ml.com blue organizations jenkins incubator mxnet detail pr 10374 1 pipeline,0,flaky test scala test arange,flaky test scala test arange please see http jenkins.mxnet ci.amazon ml.com blue organizations jenkins incubator mxnet detail pr 10374 1 pipeline
incubator-mxnet,384,"using mx.model.load checkpoint save checkpoint store restore model. seems like gpu cpu information also stored .params file. try load probably machine gpu , basically create mx.ndarray use copyto pass value loaded. 1. load assigning device number? cannot load trained gpu 3 try load machine one gpu gpu 0 . 2. delete old value? touch gpu planning use. 3. sometime reload model try use .asnumpy print results, cause segmentation fault servers problem . error message program received signal sigsegv, segmentation fault. 0x00000036ee8897cb memcpy lib64 libc.so.6 still print .context .shape, asnumpy work. thanks!!",0,question load checkpoint save checkpoint,"question load checkpoint save checkpoint using mx.model.load checkpoint save checkpoint store restore model. seems like gpu cpu information also stored .params file. try load probably machine gpu , basically create mx.ndarray use copyto pass value loaded. 1. load assigning device number? cannot load trained gpu 3 try load machine one gpu gpu 0 . 2. delete old value? touch gpu planning use. 3. sometime reload model try use .asnumpy print results, cause segmentation fault servers problem . error message program received signal sigsegv, segmentation fault. 0x00000036ee8897cb memcpy lib64 libc.so.6 still print .context .shape, asnumpy work. thanks!!"
incubator-mxnet,4661,"bugs installation issues, please provide following information. information provide, likely people able help you. environment info operating system os x 10.11.6 compiler default compile turtoris package used python r scala julia python mxnet version installed source 0.9.1 mxnet commit hash 5656c8601265a437c1cb7ea18a6f1661f346c8b5 python version distribution 2.7.11 error message please paste full error message, including stack trace. minimum reproducible example run example source code steps reproduce 1. install mxnet setup official document http mxnet.io get started osx setup.html install mxnet python 2. download image segementation pre trained model baidu.yun fcn8s vgg16 0019 3. run example image segmentation.py segementation fault 11",0,segmentation fault 11 python 2.7.11 osx,"segmentation fault 11 python 2.7.11 osx bugs installation issues, please provide following information. information provide, likely people able help you. environment info operating system os x 10.11.6 compiler default compile turtoris package used python r scala julia python mxnet version installed source 0.9.1 mxnet commit hash 5656c8601265a437c1cb7ea18a6f1661f346c8b5 python version distribution 2.7.11 error message please paste full error message, including stack trace. minimum reproducible example run example source code steps reproduce 1. install mxnet setup official document http mxnet.io get started osx setup.html install mxnet python 2. download image segementation pre trained model baidu.yun fcn8s vgg16 0019 3. run example image segmentation.py segementation fault 11"
incubator-mxnet,14389,nightly build package windows uploaded pypi build bot march 10 seems broken. file https files.pythonhosted.org packages 9a bc ee216a4dbe1433d881906513768cf61f9d52c1f8dee133f69f99e1ee9ec7 mxnet cu80 1.5.0b20190310 py2.py3 none win amd64.whl shows size 0 bytes download 259mb size. md5 sha 256 checksums also different. causing issues pypi mirror sync mxnet cu80 never complete sync due broken files. please fix possible.,0,broken pip package mxnet cu80 1.5.0b20190310,broken pip package mxnet cu80 1.5.0b20190310 nightly build package windows uploaded pypi build bot march 10 seems broken. file https files.pythonhosted.org packages 9a bc ee216a4dbe1433d881906513768cf61f9d52c1f8dee133f69f99e1ee9ec7 mxnet cu80 1.5.0b20190310 py2.py3 none win amd64.whl shows size 0 bytes download 259mb size. md5 sha 256 checksums also different. causing issues pypi mirror sync mxnet cu80 never complete sync due broken files. please fix possible.
incubator-mxnet,4945,"anyone usable android library mxnet vision v0.8.0? tried compile amalgamation many times, broken using android app.",0,upload usable android library libmxnet predict.so please!!!,"upload usable android library libmxnet predict.so please!!! anyone usable android library mxnet vision v0.8.0? tried compile amalgamation many times, broken using android app."
incubator-mxnet,467,"try compile lstm quite long time steps 200 , following error occurs, seems whatever reason, 100 hard limit number args compiled ? wonder whether change bound memory thanks !",0,raising num args bound ?,"raising num args bound ? try compile lstm quite long time steps 200 , following error occurs, seems whatever reason, 100 hard limit number args compiled ? wonder whether change bound memory thanks !"
incubator-mxnet,3967,"use symbol.simple bind train network batch batch, code snippet exe self.symbol.simple bind ctx self.ctx, input shapes batch train data exe.forward train true exe.backward print print exe.arg dict 'bn conv1 gamma' .asnumpy i, pair enumerate zip exe.arg arrays, exe.grad arrays weight, grad pair 3 print arr arrays print weight.asnumpy print arg dict print exe.arg dict 'bn conv1 gamma' .asnumpy self.updater i, grad, weight 'bn conv1 gamma' 3rd 0 self.symbol.list arguments , found value exe.arg dict 'bn conv1 gamma' .asnumpy different different position, really confused me.",0,value executor.arg dict bn conv1 gamma different different position one iteration,"value executor.arg dict bn conv1 gamma different different position one iteration use symbol.simple bind train network batch batch, code snippet exe self.symbol.simple bind ctx self.ctx, input shapes batch train data exe.forward train true exe.backward print print exe.arg dict 'bn conv1 gamma' .asnumpy i, pair enumerate zip exe.arg arrays, exe.grad arrays weight, grad pair 3 print arr arrays print weight.asnumpy print arg dict print exe.arg dict 'bn conv1 gamma' .asnumpy self.updater i, grad, weight 'bn conv1 gamma' 3rd 0 self.symbol.list arguments , found value exe.arg dict 'bn conv1 gamma' .asnumpy different different position, really confused me."
incubator-mxnet,8933,"600 classes image dataset, image belongs one classes. train dataset? correct choice present image label 600 1 binary vector, contains one 1 . bulid network 600 outputs, activate logisticregressionoutput btw, mxnet set loss function logistic loss automatically? obtain 0.49 map dataset, reported 36.1 paper. parameters set paper",0,train data multi class label,"train data multi class label 600 classes image dataset, image belongs one classes. train dataset? correct choice present image label 600 1 binary vector, contains one 1 . bulid network 600 outputs, activate logisticregressionoutput btw, mxnet set loss function logistic loss automatically? obtain 0.49 map dataset, reported 36.1 paper. parameters set paper"
incubator-mxnet,331,"attempting install cluster running red hat enterprise linux 6. loaded opencv 3.0.0, link .so files. however, still fail, saying need find . cannot find opencv installation, point right direction? see error message pkg config cflags opencvopencv.pc' pkg config path environment variable package 'opencv' found g std c 0x c dmshadow force stream wall o3 i. mshadow i. dmlc core include fpic iinclude msse3 funroll loops wno unused parameter wno unknown pragmas dmshadow use cuda 0 dmshadow use cblas 1 dmshadow use mkl 0 dmshadow rabit ps 0 dmshadow dist ps 0 dmxnet use opencv 1 fopenmp c src c api.cc build c api.o package opencv found pkg config search path. perhaps add directory containing pkg config cflags opencvopencv.pc' pkg config path environment variable package 'opencv' found g std c 0x c dmshadow force stream wall o3 i. mshadow i. dmlc core include fpic iinclude msse3 funroll loops wno unused parameter wno unknown pragmas dmshadow use cuda 0 dmshadow use cblas 1 dmshadow use mkl 0 dmshadow rabit ps 0 dmshadow dist ps 0 dmxnet use opencv 1 fopenmp c src resource.cc build resource.o package opencv found pkg config search path. perhaps add directory containing pkg config cflags opencvopencv.pc' pkg config path environment variable package 'opencv' found g std c 0x c dmshadow force stream wall o3 i. mshadow i. dmlc core include fpic iinclude msse3 funroll loops wno unused parameter wno unknown pragmas dmshadow use cuda 0 dmshadow use cblas 1 dmshadow use mkl 0 dmshadow rabit ps 0 dmshadow dist ps 0 dmxnet use opencv 1 fopenmp c src engine engine.cc build engine engine.o package opencv found pkg config search path. perhaps add directory containing pkg config cflags opencvopencv.pc' pkg config path environment variable package 'opencv' found g std c 0x c dmshadow force stream wall o3 i. mshadow i. dmlc core include fpic iinclude msse3 funroll loops wno unused parameter wno unknown pragmas dmshadow use cuda 0 dmshadow use cblas 1 dmshadow use mkl 0 dmshadow rabit ps 0 dmshadow dist ps 0 dmxnet use opencv 1 fopenmp c src engine naive engine.cc build engine naive engine.o package opencv found pkg config search path. perhaps add directory containing pkg config cflags opencvopencv.pc' pkg config path environment variable package 'opencv' found g std c 0x c dmshadow force stream wall o3 i. mshadow i. dmlc core include fpic iinclude msse3 funroll loops wno unused parameter wno unknown pragmas dmshadow use cuda 0 dmshadow use cblas 1 dmshadow use mkl 0 dmshadow rabit ps 0 dmshadow dist ps 0 dmxnet use opencv 1 fopenmp c src engine threaded engine.cc build engine threaded engine.o package opencv found pkg config search path. perhaps add directory containing pkg config cflags opencvopencv.pc' pkg config path environment variable package 'opencv' found g std c 0x c dmshadow force stream wall o3 i. mshadow i. dmlc core include fpic iinclude msse3 funroll loops wno unused parameter wno unknown pragmas dmshadow use cuda 0 dmshadow use cblas 1 dmshadow use mkl 0 dmshadow rabit ps 0 dmshadow dist ps 0 dmxnet use opencv 1 fopenmp c src engine threaded engine perdevice.cc build engine threaded engine perdevice.o package opencv found pkg config search path. perhaps add directory containing pkg config cflags opencvopencv.pc' pkg config path environment variable package 'opencv' found g std c 0x c dmshadow force stream wall o3 i. mshadow i. dmlc core include fpic iinclude msse3 funroll loops wno unused parameter wno unknown pragmas dmshadow use cuda 0 dmshadow use cblas 1 dmshadow use mkl 0 dmshadow rabit ps 0 dmshadow dist ps 0 dmxnet use opencv 1 fopenmp c src engine threaded engine pooled.cc build engine threaded engine pooled.o package opencv found pkg config search path. perhaps add directory containing pkg config cflags opencvopencv.pc' pkg config path environment variable package 'opencv' found g std c 0x c dmshadow force stream wall o3 i. mshadow i. dmlc core include fpic iinclude msse3 funroll loops wno unused parameter wno unknown pragmas dmshadow use cuda 0 dmshadow use cblas 1 dmshadow use mkl 0 dmshadow rabit ps 0 dmshadow dist ps 0 dmxnet use opencv 1 fopenmp c src io io.cc build io io.o package opencv found pkg config search path. perhaps add directory containing",0,opencv found,"opencv found attempting install cluster running red hat enterprise linux 6. loaded opencv 3.0.0, link .so files. however, still fail, saying need find . cannot find opencv installation, point right direction? see error message pkg config cflags opencvopencv.pc' pkg config path environment variable package 'opencv' found g std c 0x c dmshadow force stream wall o3 i. mshadow i. dmlc core include fpic iinclude msse3 funroll loops wno unused parameter wno unknown pragmas dmshadow use cuda 0 dmshadow use cblas 1 dmshadow use mkl 0 dmshadow rabit ps 0 dmshadow dist ps 0 dmxnet use opencv 1 fopenmp c src c api.cc build c api.o package opencv found pkg config search path. perhaps add directory containing pkg config cflags opencvopencv.pc' pkg config path environment variable package 'opencv' found g std c 0x c dmshadow force stream wall o3 i. mshadow i. dmlc core include fpic iinclude msse3 funroll loops wno unused parameter wno unknown pragmas dmshadow use cuda 0 dmshadow use cblas 1 dmshadow use mkl 0 dmshadow rabit ps 0 dmshadow dist ps 0 dmxnet use opencv 1 fopenmp c src resource.cc build resource.o package opencv found pkg config search path. perhaps add directory containing pkg config cflags opencvopencv.pc' pkg config path environment variable package 'opencv' found g std c 0x c dmshadow force stream wall o3 i. mshadow i. dmlc core include fpic iinclude msse3 funroll loops wno unused parameter wno unknown pragmas dmshadow use cuda 0 dmshadow use cblas 1 dmshadow use mkl 0 dmshadow rabit ps 0 dmshadow dist ps 0 dmxnet use opencv 1 fopenmp c src engine engine.cc build engine engine.o package opencv found pkg config search path. perhaps add directory containing pkg config cflags opencvopencv.pc' pkg config path environment variable package 'opencv' found g std c 0x c dmshadow force stream wall o3 i. mshadow i. dmlc core include fpic iinclude msse3 funroll loops wno unused parameter wno unknown pragmas dmshadow use cuda 0 dmshadow use cblas 1 dmshadow use mkl 0 dmshadow rabit ps 0 dmshadow dist ps 0 dmxnet use opencv 1 fopenmp c src engine naive engine.cc build engine naive engine.o package opencv found pkg config search path. perhaps add directory containing pkg config cflags opencvopencv.pc' pkg config path environment variable package 'opencv' found g std c 0x c dmshadow force stream wall o3 i. mshadow i. dmlc core include fpic iinclude msse3 funroll loops wno unused parameter wno unknown pragmas dmshadow use cuda 0 dmshadow use cblas 1 dmshadow use mkl 0 dmshadow rabit ps 0 dmshadow dist ps 0 dmxnet use opencv 1 fopenmp c src engine threaded engine.cc build engine threaded engine.o package opencv found pkg config search path. perhaps add directory containing pkg config cflags opencvopencv.pc' pkg config path environment variable package 'opencv' found g std c 0x c dmshadow force stream wall o3 i. mshadow i. dmlc core include fpic iinclude msse3 funroll loops wno unused parameter wno unknown pragmas dmshadow use cuda 0 dmshadow use cblas 1 dmshadow use mkl 0 dmshadow rabit ps 0 dmshadow dist ps 0 dmxnet use opencv 1 fopenmp c src engine threaded engine perdevice.cc build engine threaded engine perdevice.o package opencv found pkg config search path. perhaps add directory containing pkg config cflags opencvopencv.pc' pkg config path environment variable package 'opencv' found g std c 0x c dmshadow force stream wall o3 i. mshadow i. dmlc core include fpic iinclude msse3 funroll loops wno unused parameter wno unknown pragmas dmshadow use cuda 0 dmshadow use cblas 1 dmshadow use mkl 0 dmshadow rabit ps 0 dmshadow dist ps 0 dmxnet use opencv 1 fopenmp c src engine threaded engine pooled.cc build engine threaded engine pooled.o package opencv found pkg config search path. perhaps add directory containing pkg config cflags opencvopencv.pc' pkg config path environment variable package 'opencv' found g std c 0x c dmshadow force stream wall o3 i. mshadow i. dmlc core include fpic iinclude msse3 funroll loops wno unused parameter wno unknown pragmas dmshadow use cuda 0 dmshadow use cblas 1 dmshadow use mkl 0 dmshadow rabit ps 0 dmshadow dist ps 0 dmxnet use opencv 1 fopenmp c src io io.cc build io io.o package opencv found pkg config search path. perhaps add directory containing"
incubator-mxnet,12532,"hi, environment mxnet v1.0.0 want use caffe operator implement custom layers,such solution mxnet example caffe follow install command described https mxnet.incubator.apache.org faq caffe.html, solve problems? 1. git clone https github.com bvlc caffe 2. cd caffe wget https github.com bvlc caffe pull 4527.patch git apply 4527.patch thanks lot !! also cmake method configuration. caffe path home caffe mxnet plugins plugin caffe caffe.mk ! image https user images.githubusercontent.com 31586393 45429203 63981280 b6d5 11e8 9738 107f94cdc2ef.png",0,caffeop mxnet build failed,"caffeop mxnet build failed hi, environment mxnet v1.0.0 want use caffe operator implement custom layers,such solution mxnet example caffe follow install command described https mxnet.incubator.apache.org faq caffe.html, solve problems? 1. git clone https github.com bvlc caffe 2. cd caffe wget https github.com bvlc caffe pull 4527.patch git apply 4527.patch thanks lot !! also cmake method configuration. caffe path home caffe mxnet plugins plugin caffe caffe.mk ! image https user images.githubusercontent.com 31586393 45429203 63981280 b6d5 11e8 9738 107f94cdc2ef.png"
incubator-mxnet,13875,"may related 13831 description importing mxnet causes oserrors subprocess environment info required scientific linux 7.5 python 3.6.3 mxnet 1.5.0 packages tried multiple computers running different cuda builds error message paste complete error message, including stack trace. minimum reproducible example using following script using appropriate commands eventually give error message traceback recent call last file subcrash.py , line 13, ret subprocess.call 'ls', ' ' , stdout subprocess.pipe file opt rh rh python36 root usr lib64 python3.6 subprocess.py , line 267, call popen popenargs, kwargs p file opt rh rh python36 root usr lib64 python3.6 subprocess.py , line 709, init restore signals, start new session file opt rh rh python36 root usr lib64 python3.6 subprocess.py , line 1344, execute child raise child exception type errno num, err msg, err filename oserror errno 14 bad address 'ls' seem matter executable. tried solve it? even know start. try putting stack tract pdb break.",0,importing mxnet causing subprocess crash,"importing mxnet causing subprocess crash may related 13831 description importing mxnet causes oserrors subprocess environment info required scientific linux 7.5 python 3.6.3 mxnet 1.5.0 packages tried multiple computers running different cuda builds error message paste complete error message, including stack trace. minimum reproducible example using following script using appropriate commands eventually give error message traceback recent call last file subcrash.py , line 13, ret subprocess.call 'ls', ' ' , stdout subprocess.pipe file opt rh rh python36 root usr lib64 python3.6 subprocess.py , line 267, call popen popenargs, kwargs p file opt rh rh python36 root usr lib64 python3.6 subprocess.py , line 709, init restore signals, start new session file opt rh rh python36 root usr lib64 python3.6 subprocess.py , line 1344, execute child raise child exception type errno num, err msg, err filename oserror errno 14 bad address 'ls' seem matter executable. tried solve it? even know start. try putting stack tract pdb break."
incubator-mxnet,2975,"https github.com dmlc mxnet blob master example rcnn rcnn rpn generate.py l30 bn layers, erase moving mean moving var aux, lead wrong results know special reasons this? precedenceguo",0,"question rcnn exmaple, set aux zero testing?","question rcnn exmaple, set aux zero testing? https github.com dmlc mxnet blob master example rcnn rcnn rpn generate.py l30 bn layers, erase moving mean moving var aux, lead wrong results know special reasons this? precedenceguo"
incubator-mxnet,11334,"ref https discuss.mxnet.io initializing parameters symbolblock 1213 looks like symbolblock initializing parameters using initializer given block.initialize function, including bias, cannot initialized xavier. alought post https discuss.mxnet.io initializing parameters symbolblock 1213 suggests solution, addition, think try initialize parameters given symbol json file. e.g. better way please let know.",0,cannot initializing parameters symbolblock.,"cannot initializing parameters symbolblock. ref https discuss.mxnet.io initializing parameters symbolblock 1213 looks like symbolblock initializing parameters using initializer given block.initialize function, including bias, cannot initialized xavier. alought post https discuss.mxnet.io initializing parameters symbolblock 1213 suggests solution, addition, think try initialize parameters given symbol json file. e.g. better way please let know."
incubator-mxnet,5455,"softmax label really good default name, many times softmax. think 'label' would better default name. proposed refactor would change default values 'softmax label' 'label'.",0,refactor 'softmax label' 'label',"refactor 'softmax label' 'label' softmax label really good default name, many times softmax. think 'label' would better default name. proposed refactor would change default values 'softmax label' 'label'."
incubator-mxnet,11241,setup run following script gives following error note there's error changed 'write'. also reproduced build mxnet source commit 5b99b25e5f6ab3a20c7bcf4821a6af0a1a95f823 conv1d cudnn initially introduced.,0,conv1d throws cudnn status execution failed,conv1d throws cudnn status execution failed setup run following script gives following error note there's error changed 'write'. also reproduced build mxnet source commit 5b99b25e5f6ab3a20c7bcf4821a6af0a1a95f823 conv1d cudnn initially introduced.
incubator-mxnet,16101,"description bug mx.nd.where https beta.mxnet.io api ndarray autogen mxnet.ndarray.where.html?highlight mxnet.ndarray.where shows incorrect behavior one inputs ndarray zero size. reproducible example output weird seems ndarray zero size checked. expect would raise error showing shape x must same, according docs mx.nd.where https beta.mxnet.io api ndarray autogen mxnet.ndarray.where.html?highlight mxnet.ndarray.where . broadcast supported latest version still output. also little dangerous outputs incorrect answers rather error messages, users forget type mx.nd.array 4 . feature request 1. broadcast currently, two limitations mx.nd.where x must shape condition shape x, must 1d array whose size x first dimension size similar np.where https docs.scipy.org doc numpy 1.13.0 reference generated numpy.where.html , would great mx.nd.where supports broadcast make sure cond, x, shape, even different shapes input. 2. scalar inputs cond, x situations, want give constant value true false. would user friendly programmers need type instead environment info required build info required built source compiler gcc clang mingw visual studio gcc mxnet commit hash 03f12f0fe706d35c93a2cf721b6101bcbffeb07d build config plain cmakelist.txt use nccl 1",0,feature request np.where compatible operator,"feature request np.where compatible operator description bug mx.nd.where https beta.mxnet.io api ndarray autogen mxnet.ndarray.where.html?highlight mxnet.ndarray.where shows incorrect behavior one inputs ndarray zero size. reproducible example output weird seems ndarray zero size checked. expect would raise error showing shape x must same, according docs mx.nd.where https beta.mxnet.io api ndarray autogen mxnet.ndarray.where.html?highlight mxnet.ndarray.where . broadcast supported latest version still output. also little dangerous outputs incorrect answers rather error messages, users forget type mx.nd.array 4 . feature request 1. broadcast currently, two limitations mx.nd.where x must shape condition shape x, must 1d array whose size x first dimension size similar np.where https docs.scipy.org doc numpy 1.13.0 reference generated numpy.where.html , would great mx.nd.where supports broadcast make sure cond, x, shape, even different shapes input. 2. scalar inputs cond, x situations, want give constant value true false. would user friendly programmers need type instead environment info required build info required built source compiler gcc clang mingw visual studio gcc mxnet commit hash 03f12f0fe706d35c93a2cf721b6101bcbffeb07d build config plain cmakelist.txt use nccl 1"
incubator-mxnet,9408,description wip initial error http jenkins.mxnet ci.amazon ml.com blue organizations jenkins incubator mxnet detail pr 9406 merge 1 pipeline future errors http jenkins.mxnet ci.amazon ml.com blue organizations jenkins incubator mxnet detail pr 9406 merge 9 pipeline git status jenkins master,0,ci merging possible unmerged files.,ci merging possible unmerged files. description wip initial error http jenkins.mxnet ci.amazon ml.com blue organizations jenkins incubator mxnet detail pr 9406 merge 1 pipeline future errors http jenkins.mxnet ci.amazon ml.com blue organizations jenkins incubator mxnet detail pr 9406 merge 9 pipeline git status jenkins master
incubator-mxnet,1198,"sometimes travis breaked cpp test job pr log time correct, due changed code something else? met problem times.",0,travis sometimes suspend threaded engine test.cc,"travis sometimes suspend threaded engine test.cc sometimes travis breaked cpp test job pr log time correct, due changed code something else? met problem times."
incubator-mxnet,12294,"trying install mxnet r use library. step1 install package install.packages drat , repos https cran.rstudio.com drat addrepo dmlc install.packages mxnet comment console trying url 'https cran.rstudio.com bin macosx el capitan contrib 3.5 drat 0.1.4.tgz' content type 'application x gzip' length 64654 bytes 63 kb downloaded 63 kb downloaded binary packages var folders rq vv5bw20n3nv91pccc5krh55h0000gn rtmpwtd63g downloaded packages step2 install library library mxnet error error package namespace load failed mxnet .onload failed loadnamespace 'mxnet', details call dyn.load file, dllpath dllpath, error unable load shared object ' library frameworks r.framework versions 3.5 resources library mxnet libs libmxnet.so' dlopen library frameworks r.framework versions 3.5 resources library mxnet libs libmxnet.so, 10 library loaded usr local opt openblas lib libopenblasp r0.3.1.dylib referenced library frameworks r.framework versions 3.5 resources library mxnet libs libmxnet.so reason image found addition warning message replacing previous import scales viridis pal viridis viridis pal loading diagrammer sessioninfo r version 3.5.1 2018 07 02 platform x86 64 apple darwin15.6.0 64 bit running macos sierra 10.12.6 matrix products default blas system library frameworks accelerate.framework versions frameworks veclib.framework versions libblas.dylib lapack library frameworks r.framework versions 3.5 resources lib librlapack.dylib locale 1 en us.utf 8 en us.utf 8 en us.utf 8 c en us.utf 8 en us.utf 8 attached base packages 1 stats graphics grdevices utils datasets methods base loaded via namespace attached 1 rcpp 0.12.18 pillar 1.3.0 compiler 3.5.1 rcolorbrewer 1.1 2 influencer 0.1.0 plyr 1.8.4 bindr 0.1.1 viridis 0.5.1 tools 3.5.1 10 digest 0.6.15 jsonlite 1.5 tibble 1.4.2 gtable 0.2.0 viridislite 0.3.0 rgexf 0.15.3 pkgconfig 2.0.2 rlang 0.2.2 igraph 1.2.2 19 rstudioapi 0.7 yaml 2.2.0 bindrcpp 0.2.2 gridextra 2.3 stringr 1.3.1 diagrammer 0.9.0 dplyr 0.7.6 htmlwidgets 1.2 grid 3.5.1 28 tidyselect 0.2.4 glue 1.3.0 r6 2.2.2 rook 1.1 1 xml 3.98 1.16 ggplot2 3.0.0 purrr 0.2.5 magrittr 1.5 scales 1.0.0 37 htmltools 0.3.6 assertthat 0.2.0 colorspace 1.3 2 brew 1.0 6 stringi 1.2.4 visnetwork 2.0.4 lazyeval 0.2.1 munsell 0.5.0 crayon 1.3.4 please help?",0,mxnet installation library failing r,"mxnet installation library failing r trying install mxnet r use library. step1 install package install.packages drat , repos https cran.rstudio.com drat addrepo dmlc install.packages mxnet comment console trying url 'https cran.rstudio.com bin macosx el capitan contrib 3.5 drat 0.1.4.tgz' content type 'application x gzip' length 64654 bytes 63 kb downloaded 63 kb downloaded binary packages var folders rq vv5bw20n3nv91pccc5krh55h0000gn rtmpwtd63g downloaded packages step2 install library library mxnet error error package namespace load failed mxnet .onload failed loadnamespace 'mxnet', details call dyn.load file, dllpath dllpath, error unable load shared object ' library frameworks r.framework versions 3.5 resources library mxnet libs libmxnet.so' dlopen library frameworks r.framework versions 3.5 resources library mxnet libs libmxnet.so, 10 library loaded usr local opt openblas lib libopenblasp r0.3.1.dylib referenced library frameworks r.framework versions 3.5 resources library mxnet libs libmxnet.so reason image found addition warning message replacing previous import scales viridis pal viridis viridis pal loading diagrammer sessioninfo r version 3.5.1 2018 07 02 platform x86 64 apple darwin15.6.0 64 bit running macos sierra 10.12.6 matrix products default blas system library frameworks accelerate.framework versions frameworks veclib.framework versions libblas.dylib lapack library frameworks r.framework versions 3.5 resources lib librlapack.dylib locale 1 en us.utf 8 en us.utf 8 en us.utf 8 c en us.utf 8 en us.utf 8 attached base packages 1 stats graphics grdevices utils datasets methods base loaded via namespace attached 1 rcpp 0.12.18 pillar 1.3.0 compiler 3.5.1 rcolorbrewer 1.1 2 influencer 0.1.0 plyr 1.8.4 bindr 0.1.1 viridis 0.5.1 tools 3.5.1 10 digest 0.6.15 jsonlite 1.5 tibble 1.4.2 gtable 0.2.0 viridislite 0.3.0 rgexf 0.15.3 pkgconfig 2.0.2 rlang 0.2.2 igraph 1.2.2 19 rstudioapi 0.7 yaml 2.2.0 bindrcpp 0.2.2 gridextra 2.3 stringr 1.3.1 diagrammer 0.9.0 dplyr 0.7.6 htmlwidgets 1.2 grid 3.5.1 28 tidyselect 0.2.4 glue 1.3.0 r6 2.2.2 rook 1.1 1 xml 3.98 1.16 ggplot2 3.0.0 purrr 0.2.5 magrittr 1.5 scales 1.0.0 37 htmltools 0.3.6 assertthat 0.2.0 colorspace 1.3 2 brew 1.0 6 stringi 1.2.4 visnetwork 2.0.4 lazyeval 0.2.1 munsell 0.5.0 crayon 1.3.4 please help?"
incubator-mxnet,11508,"guess. right? trainer.allreduce grads autograd.record logits model input loss criterion logits, target loss.backward grads i.grad ctx model.params.values gluon.utils.clip global norm grads, args.grad clip trainer.update args.batch size",0,questionare examples gradients clipping gluon?,"questionare examples gradients clipping gluon? guess. right? trainer.allreduce grads autograd.record logits model input loss criterion logits, target loss.backward grads i.grad ctx model.params.values gluon.utils.clip global norm grads, args.grad clip trainer.update args.batch size"
incubator-mxnet,5218,import mxnet libdc1394 error failed initialize libdc1394 01 20 36 mnt mxnet docker mxnet93 dmlc core include dmlc logging.h 300 01 20 36 src operator nnpack nnpack util.h 25 nnp initialize failed status 51 stack trace returned 48 entries bt 0 usr local lib python2.7 dist packages mxnet 0.9.3 py2.7.egg mxnet libmxnet.so zn5mxnet2op16nnpackinitializec1ev 0x2fb 0x7fb4ce55a17b bt 1 usr local lib python2.7 dist packages mxnet 0.9.3 py2.7.egg mxnet libmxnet.so 0x2517d6 0x7fb4ce4ed7d6 bt 2 lib64 ld linux x86 64.so.2 0x1010a 0x7fb4d412c10a bt 3 lib64 ld linux x86 64.so.2 0x101f3 0x7fb4d412c1f3 bt 4 lib64 ld linux x86 64.so.2 0x14c30 0x7fb4d4130c30 bt 5 lib64 ld linux x86 64.so.2 0xffc4 0x7fb4d412bfc4 bt 6 lib64 ld linux x86 64.so.2 0x1437b 0x7fb4d413037b bt 7 lib x86 64 linux gnu libdl.so.2 0x102b 0x7fb4d393602b bt 8 lib64 ld linux x86 64.so.2 0xffc4 0x7fb4d412bfc4 bt 9 lib x86 64 linux gnu libdl.so.2 0x162d 0x7fb4d393662d bt 10 lib x86 64 linux gnu libdl.so.2 dlopen 0x31 0x7fb4d39360c1 bt 11 usr lib python2.7 lib dynload ctypes.x86 64 linux gnu.so 0x741b 0x7fb4d298341b bt 12 python pyeval evalframeex 0x41d 0x523f6d bt 13 python 0x568b3a bt 14 python 0x4c2604 bt 15 python 0x4d1c5c bt 16 python 0x55f6db bt 17 python pyeval evalframeex 0x98d 0x5244dd bt 18 python pyeval evalcodeex 0x2b1 0x555551 bt 19 python pyeval evalframeex 0x1a10 0x525560 bt 20 python pyeval evalcodeex 0x2b1 0x555551 bt 21 python pyeval evalcode 0x32 0x5b41e2 bt 22 python pyimport execcodemoduleex 0xaa 0x5b429a bt 23 python 0x5942af bt 24 python 0x55642f bt 25 python 0x556838 bt 26 python 0x556d9b bt 27 python 0x569cd8 bt 28 python pyeval callobjectwithkeywords 0x6b 0x4c8c8b bt 29 python pyeval evalframeex 0x2958 0x5264a8 bt 30 python pyeval evalcodeex 0x2b1 0x555551 bt 31 python pyeval evalcode 0x32 0x5b41e2 bt 32 python pyimport execcodemoduleex 0xaa 0x5b429a bt 33 python 0x5942af bt 34 python 0x465804 bt 35 python 0x55642f bt 36 python 0x556838 bt 37 python 0x556c4b bt 38 python 0x569c08 bt 39 python pyeval callobjectwithkeywords 0x6b 0x4c8c8b bt 40 python pyeval evalframeex 0x2958 0x5264a8 bt 41 python 0x567d14 bt 42 python pyrun interactiveoneflags 0x18c 0x465a2d bt 43 python pyrun interactiveloopflags 0xaa 0x465b49 bt 44 python pyrun anyfileexflags 0x37 0x4661fe bt 45 python py main 0xb5e 0x466d92 bt 46 lib x86 64 linux gnu libc.so.6 libc start main 0xf5 0x7fb4d3b5af45 bt 47 python 0x577c2e terminate called throwing instance 'dmlc error' 01 20 36 src operator nnpack nnpack util.h 25 nnp initialize failed status 51 stack trace returned 48 entries bt 0 usr local lib python2.7 dist packages mxnet 0.9.3 py2.7.egg mxnet libmxnet.so zn5mxnet2op16nnpackinitializec1ev 0x2fb 0x7fb4ce55a17b bt 1 usr local lib python2.7 dist packages mxnet 0.9.3 py2.7.egg mxnet libmxnet.so 0x2517d6 0x7fb4ce4ed7d6 bt 2 lib64 ld linux x86 64.so.2 0x1010a 0x7fb4d412c10a bt 3 lib64 ld linux x86 64.so.2 0x101f3 0x7fb4d412c1f3 bt 4 lib64 ld linux x86 64.so.2 0x14c30 0x7fb4d4130c30 bt 5 lib64 ld linux x86 64.so.2 0xffc4 0x7fb4d412bfc4 bt 6 lib64 ld linux x86 64.so.2 0x1437b 0x7fb4d413037b bt 7 lib x86 64 linux gnu libdl.so.2 0x102b 0x7fb4d393602b bt 8 lib64 ld linux x86 64.so.2 0xffc4 0x7fb4d412bfc4 bt 9 lib x86 64 linux gnu libdl.so.2 0x162d 0x7fb4d393662d bt 10 lib x86 64 linux gnu libdl.so.2 dlopen 0x31 0x7fb4d39360c1 bt 11 usr lib python2.7 lib dynload ctypes.x86 64 linux gnu.so 0x741b 0x7fb4d298341b bt 12 python pyeval evalframeex 0x41d 0x523f6d bt 13 python 0x568b3a bt 14 python 0x4c2604 bt 15 python 0x4d1c5c bt 16 python 0x55f6db bt 17 python pyeval evalframeex 0x98d 0x5244dd bt 18 python pyeval evalcodeex 0x2b1 0x555551 bt 19 python pyeval evalframeex 0x1a10 0x525560 bt 20 python pyeval evalcodeex 0x2b1 0x555551 bt 21 python pyeval evalcode 0x32 0x5b41e2 bt 22 python pyimport execcodemoduleex 0xaa 0x5b429a bt 23 python 0x5942af bt 24 python 0x55642f bt 25 python 0x556838 bt 26 python 0x556d9b bt 27 python 0x569cd8 bt 28 python pyeval callobjectwithkeywords 0x6b 0x4c8c8b bt 29 python pyeval evalframeex 0x2958 0x5264a8 bt 30 python pyeval evalcodeex 0x2b1 0x555551 bt 31 python pyeval evalcode 0x32 0x5b41e2 bt 32 python pyimport execcodemoduleex 0xaa 0x5b429a bt 33 python 0x5942af bt 34 python 0x465804 bt 35 python 0x55642f bt 36 python 0x556838 bt 37 python 0x556c4b bt 38 python 0x569c08 bt 39 python pyeval callobjectwithkeywords 0x6b 0x4c8c8b bt 40 python pyeval evalframeex 0x2958 0x5264a8 bt 41 python 0x567d14 bt 42 python pyrun interactiveoneflags 0x18c 0x465a2d bt 43 python pyrun interactiveloopflags 0xaa 0x465b49 bt 44 python pyrun anyfileexflags 0x37 0x4661fe bt 45 python py main 0xb5e 0x466d92 bt 46 lib x86 64 linux gnu libc.so.6 libc start main 0xf5 0x7fb4d3b5af45 bt 47 python 0x577c2e aborted core dumped,0,core dumped try compile mxnet0.9.3 nnpack support,core dumped try compile mxnet0.9.3 nnpack support import mxnet libdc1394 error failed initialize libdc1394 01 20 36 mnt mxnet docker mxnet93 dmlc core include dmlc logging.h 300 01 20 36 src operator nnpack nnpack util.h 25 nnp initialize failed status 51 stack trace returned 48 entries bt 0 usr local lib python2.7 dist packages mxnet 0.9.3 py2.7.egg mxnet libmxnet.so zn5mxnet2op16nnpackinitializec1ev 0x2fb 0x7fb4ce55a17b bt 1 usr local lib python2.7 dist packages mxnet 0.9.3 py2.7.egg mxnet libmxnet.so 0x2517d6 0x7fb4ce4ed7d6 bt 2 lib64 ld linux x86 64.so.2 0x1010a 0x7fb4d412c10a bt 3 lib64 ld linux x86 64.so.2 0x101f3 0x7fb4d412c1f3 bt 4 lib64 ld linux x86 64.so.2 0x14c30 0x7fb4d4130c30 bt 5 lib64 ld linux x86 64.so.2 0xffc4 0x7fb4d412bfc4 bt 6 lib64 ld linux x86 64.so.2 0x1437b 0x7fb4d413037b bt 7 lib x86 64 linux gnu libdl.so.2 0x102b 0x7fb4d393602b bt 8 lib64 ld linux x86 64.so.2 0xffc4 0x7fb4d412bfc4 bt 9 lib x86 64 linux gnu libdl.so.2 0x162d 0x7fb4d393662d bt 10 lib x86 64 linux gnu libdl.so.2 dlopen 0x31 0x7fb4d39360c1 bt 11 usr lib python2.7 lib dynload ctypes.x86 64 linux gnu.so 0x741b 0x7fb4d298341b bt 12 python pyeval evalframeex 0x41d 0x523f6d bt 13 python 0x568b3a bt 14 python 0x4c2604 bt 15 python 0x4d1c5c bt 16 python 0x55f6db bt 17 python pyeval evalframeex 0x98d 0x5244dd bt 18 python pyeval evalcodeex 0x2b1 0x555551 bt 19 python pyeval evalframeex 0x1a10 0x525560 bt 20 python pyeval evalcodeex 0x2b1 0x555551 bt 21 python pyeval evalcode 0x32 0x5b41e2 bt 22 python pyimport execcodemoduleex 0xaa 0x5b429a bt 23 python 0x5942af bt 24 python 0x55642f bt 25 python 0x556838 bt 26 python 0x556d9b bt 27 python 0x569cd8 bt 28 python pyeval callobjectwithkeywords 0x6b 0x4c8c8b bt 29 python pyeval evalframeex 0x2958 0x5264a8 bt 30 python pyeval evalcodeex 0x2b1 0x555551 bt 31 python pyeval evalcode 0x32 0x5b41e2 bt 32 python pyimport execcodemoduleex 0xaa 0x5b429a bt 33 python 0x5942af bt 34 python 0x465804 bt 35 python 0x55642f bt 36 python 0x556838 bt 37 python 0x556c4b bt 38 python 0x569c08 bt 39 python pyeval callobjectwithkeywords 0x6b 0x4c8c8b bt 40 python pyeval evalframeex 0x2958 0x5264a8 bt 41 python 0x567d14 bt 42 python pyrun interactiveoneflags 0x18c 0x465a2d bt 43 python pyrun interactiveloopflags 0xaa 0x465b49 bt 44 python pyrun anyfileexflags 0x37 0x4661fe bt 45 python py main 0xb5e 0x466d92 bt 46 lib x86 64 linux gnu libc.so.6 libc start main 0xf5 0x7fb4d3b5af45 bt 47 python 0x577c2e terminate called throwing instance 'dmlc error' 01 20 36 src operator nnpack nnpack util.h 25 nnp initialize failed status 51 stack trace returned 48 entries bt 0 usr local lib python2.7 dist packages mxnet 0.9.3 py2.7.egg mxnet libmxnet.so zn5mxnet2op16nnpackinitializec1ev 0x2fb 0x7fb4ce55a17b bt 1 usr local lib python2.7 dist packages mxnet 0.9.3 py2.7.egg mxnet libmxnet.so 0x2517d6 0x7fb4ce4ed7d6 bt 2 lib64 ld linux x86 64.so.2 0x1010a 0x7fb4d412c10a bt 3 lib64 ld linux x86 64.so.2 0x101f3 0x7fb4d412c1f3 bt 4 lib64 ld linux x86 64.so.2 0x14c30 0x7fb4d4130c30 bt 5 lib64 ld linux x86 64.so.2 0xffc4 0x7fb4d412bfc4 bt 6 lib64 ld linux x86 64.so.2 0x1437b 0x7fb4d413037b bt 7 lib x86 64 linux gnu libdl.so.2 0x102b 0x7fb4d393602b bt 8 lib64 ld linux x86 64.so.2 0xffc4 0x7fb4d412bfc4 bt 9 lib x86 64 linux gnu libdl.so.2 0x162d 0x7fb4d393662d bt 10 lib x86 64 linux gnu libdl.so.2 dlopen 0x31 0x7fb4d39360c1 bt 11 usr lib python2.7 lib dynload ctypes.x86 64 linux gnu.so 0x741b 0x7fb4d298341b bt 12 python pyeval evalframeex 0x41d 0x523f6d bt 13 python 0x568b3a bt 14 python 0x4c2604 bt 15 python 0x4d1c5c bt 16 python 0x55f6db bt 17 python pyeval evalframeex 0x98d 0x5244dd bt 18 python pyeval evalcodeex 0x2b1 0x555551 bt 19 python pyeval evalframeex 0x1a10 0x525560 bt 20 python pyeval evalcodeex 0x2b1 0x555551 bt 21 python pyeval evalcode 0x32 0x5b41e2 bt 22 python pyimport execcodemoduleex 0xaa 0x5b429a bt 23 python 0x5942af bt 24 python 0x55642f bt 25 python 0x556838 bt 26 python 0x556d9b bt 27 python 0x569cd8 bt 28 python pyeval callobjectwithkeywords 0x6b 0x4c8c8b bt 29 python pyeval evalframeex 0x2958 0x5264a8 bt 30 python pyeval evalcodeex 0x2b1 0x555551 bt 31 python pyeval evalcode 0x32 0x5b41e2 bt 32 python pyimport execcodemoduleex 0xaa 0x5b429a bt 33 python 0x5942af bt 34 python 0x465804 bt 35 python 0x55642f bt 36 python 0x556838 bt 37 python 0x556c4b bt 38 python 0x569c08 bt 39 python pyeval callobjectwithkeywords 0x6b 0x4c8c8b bt 40 python pyeval evalframeex 0x2958 0x5264a8 bt 41 python 0x567d14 bt 42 python pyrun interactiveoneflags 0x18c 0x465a2d bt 43 python pyrun interactiveloopflags 0xaa 0x465b49 bt 44 python pyrun anyfileexflags 0x37 0x4661fe bt 45 python py main 0xb5e 0x466d92 bt 46 lib x86 64 linux gnu libc.so.6 libc start main 0xf5 0x7fb4d3b5af45 bt 47 python 0x577c2e aborted core dumped
incubator-mxnet,329,"configure.win src search library, put things libmxnet.dll right position proper error message tell user install things exist. ideally compile x64 version. makefile option winrpack, pack binary distributable version package. ship libmxnet.dll, need assume thirdparty dlls path. also fine assume libmxnet.dll path, treat library maybe easier us go cran .",0,"r, windows process tracking","r, windows process tracking configure.win src search library, put things libmxnet.dll right position proper error message tell user install things exist. ideally compile x64 version. makefile option winrpack, pack binary distributable version package. ship libmxnet.dll, need assume thirdparty dlls path. also fine assume libmxnet.dll path, treat library maybe easier us go cran ."
incubator-mxnet,6796,"hi, new mxnet. interested implementing new c layer, wondering debug process. recompile whole framework test it? simpler way. seems like recompiling takes way much time. find useful info tutorial. hope get hints. thanks. mli piiswrong tqchen",0,debug c layer,"debug c layer hi, new mxnet. interested implementing new c layer, wondering debug process. recompile whole framework test it? simpler way. seems like recompiling takes way much time. find useful info tutorial. hope get hints. thanks. mli piiswrong tqchen"
incubator-mxnet,12497,"using mxnet 1.3.0 onnx 1.3, conversion function registered op type arange yet, arange function surported?",0,onnx arange op supported!,"onnx arange op supported! using mxnet 1.3.0 onnx 1.3, conversion function registered op type arange yet, arange function surported?"
incubator-mxnet,14245,using python. trying visualize global pooling kernel crashes. visualization code fails trying run layer works setting kernel labels bug,0,visualize global pooling kernel crashes.,visualize global pooling kernel crashes. using python. trying visualize global pooling kernel crashes. visualization code fails trying run layer works setting kernel labels bug
incubator-mxnet,2715,"try use mnistiter get bacth data it. read document corresponding page. suppose use dataiter's getdata method. throw error use way. could one explain happen please? error message traceback recent call last file testmnist.py , line 58, x test batch testiter.getdata file c python34 lib site packages mxnet 0.7.0 py3.4.egg mxnet io.py , line 504, getdata check call lib.mxdataitergetdata self.handle, ctypes.byref hdl oserror exception access violation reading 0x0000000000000000",0,mnistiter call getdata method throw error,"mnistiter call getdata method throw error try use mnistiter get bacth data it. read document corresponding page. suppose use dataiter's getdata method. throw error use way. could one explain happen please? error message traceback recent call last file testmnist.py , line 58, x test batch testiter.getdata file c python34 lib site packages mxnet 0.7.0 py3.4.egg mxnet io.py , line 504, getdata check call lib.mxdataitergetdata self.handle, ctypes.byref hdl oserror exception access violation reading 0x0000000000000000"
incubator-mxnet,4960,"hi, feature supporting varaible batch size? lstm validation, need fill buckets duplicated examples number example enough. alough bypass issue flating bucket data speical designed bucket key, elegant solution?",0,variable batch size,"variable batch size hi, feature supporting varaible batch size? lstm validation, need fill buckets duplicated examples number example enough. alough bypass issue flating bucket data speical designed bucket key, elegant solution?"
incubator-mxnet,2699,"output mean? softmax loss output first layers output? softmax loss output,how get it?",0,meaning ouput image segmentation.py,"meaning ouput image segmentation.py output mean? softmax loss output first layers output? softmax loss output,how get it?"
incubator-mxnet,8514,! label2 https user images.githubusercontent.com 13029886 32312590 9120270e bfd9 11e7 9fdb de29aece2422.png ! res2 https user images.githubusercontent.com 13029886 32312591 9159ea7a bfd9 11e7 8658 e3fa1ce90bf2.png,0,upload result segnet,upload result segnet ! label2 https user images.githubusercontent.com 13029886 32312590 9120270e bfd9 11e7 9fdb de29aece2422.png ! res2 https user images.githubusercontent.com 13029886 32312591 9159ea7a bfd9 11e7 8658 e3fa1ce90bf2.png
incubator-mxnet,14426,"minimum reproducible example. reproduce, require exception handling support waitall pr https github.com apache incubator mxnet pull 14397 . issue found ci failures running test random.py windows. hidden earlier waitall exception rethrow support. issue may around since pr added 10367 currently working fixing this.",0,mx.random.seed ctx failures gpu build run cpu context,"mx.random.seed ctx failures gpu build run cpu context minimum reproducible example. reproduce, require exception handling support waitall pr https github.com apache incubator mxnet pull 14397 . issue found ci failures running test random.py windows. hidden earlier waitall exception rethrow support. issue may around since pr added 10367 currently working fixing this."
incubator-mxnet,7094,function sample code save mxnet model keras model .h5 load keras model mxnet?,0,save mxnet model keras vice versa,save mxnet model keras vice versa function sample code save mxnet model keras model .h5 load keras model mxnet?
incubator-mxnet,7472,"question usually, neural network trained using training, validation test set. continuous series data, event new training data occurring every 1 5 seconds, possible continuously train update recurrent neural network using mxnet? need care reuse previous training data points want update weights slightly ! new event. behaviour game like system depending expressed intentional behaviour players features , output system estimated continuously adapted processing . system learn way, able cope with, certain extend, changing player behaviour needs remember certain patterns weeks possible, months, ago. probably mainly lstm. storing data retrain system data close impossible 1. estimate there's 10 100gb data per day varying 2. retraining every time, let's say, 10 seconds, existing data would take long. want system continuously trains real data, splitting training testing validation sets 1. training set real data, comparing actual state system prediction previously made 2. there's validation, besides fact system validates 3. testing done every new event. predictive power continuously determined. done mxnet, training data stream? dl4j, there's https deeplearning4j.org usingrnns test time predictions one step time , one update model fit, one step time data iterator runs fit every x seconds environment info really relevant, well, mind providing operating system compiler ? package used python r scala julia r mxnet version",0,continuously train rnn training data stream?,"continuously train rnn training data stream? question usually, neural network trained using training, validation test set. continuous series data, event new training data occurring every 1 5 seconds, possible continuously train update recurrent neural network using mxnet? need care reuse previous training data points want update weights slightly ! new event. behaviour game like system depending expressed intentional behaviour players features , output system estimated continuously adapted processing . system learn way, able cope with, certain extend, changing player behaviour needs remember certain patterns weeks possible, months, ago. probably mainly lstm. storing data retrain system data close impossible 1. estimate there's 10 100gb data per day varying 2. retraining every time, let's say, 10 seconds, existing data would take long. want system continuously trains real data, splitting training testing validation sets 1. training set real data, comparing actual state system prediction previously made 2. there's validation, besides fact system validates 3. testing done every new event. predictive power continuously determined. done mxnet, training data stream? dl4j, there's https deeplearning4j.org usingrnns test time predictions one step time , one update model fit, one step time data iterator runs fit every x seconds environment info really relevant, well, mind providing operating system compiler ? package used python r scala julia r mxnet version"
incubator-mxnet,9989,"note providing complete information concise form best way get help. issue template serves checklist essential information technical issues bug reports. non technical issues feature requests, feel free present information believe best form. q discussion, please start discussion thread https discuss.mxnet.io description cannot train gluon style transfer, needs outside autograd.record block need call backward. environment info required python info 'version ', '2.7.10' 'compiler ', 'gcc 4.1.2' 'build ', 'default', 'jun 29 2015 12 45 31' 'arch ', '64bit', 'elf' pip info corresponding pip install current python. mxnet info asset common software thirdparty mxnet 1.0.0 build1 python2.7 mxnet optimizer.py 136 userwarning warning new optimizer mxnet.optimizer.nag overriding existing optimizer mxnet.optimizer.nag optimizer.opt registry name . name 'version ', '1.1.0' 'directory ', ' asset common software thirdparty mxnet 1.0.0 build1 python2.7 mxnet' hashtag found. installed pre built package. system info 'platform ', 'linux 3.10.105 1.el6.elrepo.x86 64 x86 64 centos 6.2 final' 'system ', 'linux' 'node ', 'bladerunner' 'release ', '3.10.105 1.el6.elrepo.x86 64' 'version ', ' 1 smp fri feb 10 10 48 08 est 2017' hardware info 'machine ', 'x86 64' 'processor ', 'x86 64' architecture x86 64 cpu op mode 32 bit, 64 bit byte order little endian cpu 12 line cpu list 0 11 thread per core 1 core per socket 6 socket 2 numa node 2 vendor id genuineintel cpu family 6 model 63 model name intel r xeon r cpu e5 2609 v3 1.90ghz stepping 2 cpu mhz 1900.000 bogomips 3796.70 virtualization vt x l1d cache 32k l1i cache 32k l2 cache 256k l3 cache 15360k numa node0 cpu 0 5 numa node1 cpu 6 11 network test setting timeout 10 error open mxnet https github.com apache incubator mxnet, , dns finished 0.0260591506958 sec. error open pypi https pypi.python.org pypi pip, , dns finished 0.170429944992 sec. error open fashionmnist https apache mxnet.s3 accelerate.dualstack.amazonaws.com gluon dataset fashion mnist train labels idx1 ubyte.gz, , dns finished 0.204452037811 sec. error open conda https repo.continuum.io pkgs free , , dns finished 0.154680967331 sec. error open gluon tutorial en http gluon.mxnet.io, , dns finished 0.381160974503 sec. error open gluon tutorial cn https zh.gluon.ai, , dns finished 0.432467937469 sec. package used python r scala julia python build info required built source compiler gcc clang mingw visual studio gcc 4.8.5 centos 6.2 mxnet commit hash b73c57c526396d6485bdf65986e3819c54eb7bd9 build config error message minimum reproducible example run main.py https github.com apache incubator mxnet tree master example gluon style transfer follows main.py train dataset dev coco dataset style folder images styles save model dir models download coco dataset style images steps reproduce 1. install mxnet 2. get installed version environment 3. cd example gluon style transfer 4. python main.py train dataset dev coco dataset style folder images styles save model dir models tried solve it? 1. move https github.com apache incubator mxnet blob master example gluon style transfer main.py l82 2. l79 l80 3. model train produces bad result",0,cannot train example gluon style transfer,"cannot train example gluon style transfer note providing complete information concise form best way get help. issue template serves checklist essential information technical issues bug reports. non technical issues feature requests, feel free present information believe best form. q discussion, please start discussion thread https discuss.mxnet.io description cannot train gluon style transfer, needs outside autograd.record block need call backward. environment info required python info 'version ', '2.7.10' 'compiler ', 'gcc 4.1.2' 'build ', 'default', 'jun 29 2015 12 45 31' 'arch ', '64bit', 'elf' pip info corresponding pip install current python. mxnet info asset common software thirdparty mxnet 1.0.0 build1 python2.7 mxnet optimizer.py 136 userwarning warning new optimizer mxnet.optimizer.nag overriding existing optimizer mxnet.optimizer.nag optimizer.opt registry name . name 'version ', '1.1.0' 'directory ', ' asset common software thirdparty mxnet 1.0.0 build1 python2.7 mxnet' hashtag found. installed pre built package. system info 'platform ', 'linux 3.10.105 1.el6.elrepo.x86 64 x86 64 centos 6.2 final' 'system ', 'linux' 'node ', 'bladerunner' 'release ', '3.10.105 1.el6.elrepo.x86 64' 'version ', ' 1 smp fri feb 10 10 48 08 est 2017' hardware info 'machine ', 'x86 64' 'processor ', 'x86 64' architecture x86 64 cpu op mode 32 bit, 64 bit byte order little endian cpu 12 line cpu list 0 11 thread per core 1 core per socket 6 socket 2 numa node 2 vendor id genuineintel cpu family 6 model 63 model name intel r xeon r cpu e5 2609 v3 1.90ghz stepping 2 cpu mhz 1900.000 bogomips 3796.70 virtualization vt x l1d cache 32k l1i cache 32k l2 cache 256k l3 cache 15360k numa node0 cpu 0 5 numa node1 cpu 6 11 network test setting timeout 10 error open mxnet https github.com apache incubator mxnet, , dns finished 0.0260591506958 sec. error open pypi https pypi.python.org pypi pip, , dns finished 0.170429944992 sec. error open fashionmnist https apache mxnet.s3 accelerate.dualstack.amazonaws.com gluon dataset fashion mnist train labels idx1 ubyte.gz, , dns finished 0.204452037811 sec. error open conda https repo.continuum.io pkgs free , , dns finished 0.154680967331 sec. error open gluon tutorial en http gluon.mxnet.io, , dns finished 0.381160974503 sec. error open gluon tutorial cn https zh.gluon.ai, , dns finished 0.432467937469 sec. package used python r scala julia python build info required built source compiler gcc clang mingw visual studio gcc 4.8.5 centos 6.2 mxnet commit hash b73c57c526396d6485bdf65986e3819c54eb7bd9 build config error message minimum reproducible example run main.py https github.com apache incubator mxnet tree master example gluon style transfer follows main.py train dataset dev coco dataset style folder images styles save model dir models download coco dataset style images steps reproduce 1. install mxnet 2. get installed version environment 3. cd example gluon style transfer 4. python main.py train dataset dev coco dataset style folder images styles save model dir models tried solve it? 1. move https github.com apache incubator mxnet blob master example gluon style transfer main.py l82 2. l79 l80 3. model train produces bad result"
incubator-mxnet,3710,"hi, want load labeled unlabeled data mxnet. follow example https github.com dmlc mxnet blob master example fcn xs data.py l96 url create dataiter. implement method unlabel data? thanks attention.",0,deal unlabel data dataiter?,"deal unlabel data dataiter? hi, want load labeled unlabeled data mxnet. follow example https github.com dmlc mxnet blob master example fcn xs data.py l96 url create dataiter. implement method unlabel data? thanks attention."
incubator-mxnet,6073,"python .. .. tools launch.py n 2 launcher yarn python train mnist.py network lenet kv store dist sync 17 05 03 01 24 49 warn util.nativecodeloader unable load native hadoop library platform... using builtin java classes applicable 17 05 03 01 24 50 warn shortcircuit.domainsocketfactory short circuit local reads feature cannot used libhadoop cannot loaded. 17 05 03 01 24 50 info impl.timelineclientimpl timeline service address http x.x.x.x 8188 ws v1 timeline 17 05 03 01 24 50 info client.rmproxy connecting resourcemanager x.x.x.x 8050 17 05 03 01 24 50 info client.ahsproxy connecting application history server x.x.x.x 10200 17 05 03 01 24 51 info dmlc.client jobname dmlc nworker 2,nsever 2 python,username root 17 05 03 01 24 51 info dmlc.client submitting application application 1489701902537 0147 17 05 03 01 24 51 info impl.yarnclientimpl submitted application application 1489701902537 0147 application application 1489701902537 0147 finished state finished 1493774702568 diagnostics., num tasks4, finished 0, failed 4 dmlc task 0 failed 3times available queues default 17 05 03 01 25 02 info impl.yarnclientimpl killed application application 1489701902537 0147 config.mk mxnet template configuration compiling mxnet want change configuration, please use following steps. assume root directory mxnet. first copy file local changes ignored git cp make config.mk . next modify according entries, compile make build parallel 8 threads make j8 choice compiler export cc gcc export cxx g export nvcc nvcc whether compile options mxnet developer dev 0 whether compile debug debug 0 whether compiler profiler use profiler additional link flags want add add ldflags additional compile flags want add add cflags matrix computation libraries cpu gpu whether use cuda compile use cuda 0 add path cuda library link compile flag already add environment variable, leave none use cuda path usr local cuda use cuda path none whether use cudnn r3 library use cudnn 0 cuda architecture setting going them. cuda 6.0, comment 50 lines compatibility. cuda arch gencode arch compute 30,code sm 30 gencode arch compute 35,code sm 35 gencode arch compute 50,code sm 50 gencode arch compute 50,code compute 50 whether use cuda runtime compiling writing kernels native language i.e. python use nvrtc 0 whether use opencv compilation disable it, however, able use imbin iterator use opencv 0 use openmp parallelization use openmp 1 mkl ml library intel cpu xeon phi please refer mkl readme.md details mkl ml library folder, need root usr local change user home directory standard user use blas! mkl mklml root usr local whether use mkl2017 library use mkl2017 0 whether use mkl2017 experimental feature high performance prerequisite use mkl2017 1 use mkl2017 experimental 0 whether use nnpack library use nnpack 0 choose version blas want use mkl, blas, atlas, openblas default use atlas linux apple osx uname shell uname ifeq uname , darwin use blas apple else use blas atlas endif add path intel library, may need mkl, add path environment variable use intel path none use mkl blas, choose static link automatically allow python wrapper ifeq use mkl2017 , 0 ifeq use blas , mkl use static mkl 1 endif else use static mkl none endif settings power arm arch arch shell uname ifneq , filter arch , armv6l armv7l powerpc64le ppc64le aarch64 use sse 0 else use sse 1 endif distributed computing whether enable multi machine supporting use dist kvstore 1 whether allow read write hdfs directly. yes, hadoop required use hdfs 0 path libjvm.so. required use hdfs 1 libjvm java home jre lib amd64 server whether allow read write aws s3 directly. yes, libcurl4 openssl dev required, installed ubuntu sudo apt get install libcurl4 openssl dev use s3 0 additional operators path folders containing projects specific operators want put src operators extra operators features create c interface package use cpp package 0 plugins whether use caffe integration. requires installing caffe. also need add caffe path build lib ld library path caffe path home caffe mxnet plugins plugin caffe caffe.mk whether use torch integration. requires installing torch. also need add torch path install lib ld library path torch path home torch mxnet plugins plugin torch torch.mk warpctc path home warp ctc mxnet plugins plugin warpctc warpctc.mk whether use sframe integration. requires build sframe git github.com dato code sframe.git sframe path home sframe mxnet plugins plugin sframe plugin.mk use blas atlas add cflags usr include openblas add cflags usr include openblas l usr local lib add ldflags lopencv core lopencv imgproc lopencv imgcodecs someone please tell wrong set up?",0,unable run mxnet yarn launcher,"unable run mxnet yarn launcher python .. .. tools launch.py n 2 launcher yarn python train mnist.py network lenet kv store dist sync 17 05 03 01 24 49 warn util.nativecodeloader unable load native hadoop library platform... using builtin java classes applicable 17 05 03 01 24 50 warn shortcircuit.domainsocketfactory short circuit local reads feature cannot used libhadoop cannot loaded. 17 05 03 01 24 50 info impl.timelineclientimpl timeline service address http x.x.x.x 8188 ws v1 timeline 17 05 03 01 24 50 info client.rmproxy connecting resourcemanager x.x.x.x 8050 17 05 03 01 24 50 info client.ahsproxy connecting application history server x.x.x.x 10200 17 05 03 01 24 51 info dmlc.client jobname dmlc nworker 2,nsever 2 python,username root 17 05 03 01 24 51 info dmlc.client submitting application application 1489701902537 0147 17 05 03 01 24 51 info impl.yarnclientimpl submitted application application 1489701902537 0147 application application 1489701902537 0147 finished state finished 1493774702568 diagnostics., num tasks4, finished 0, failed 4 dmlc task 0 failed 3times available queues default 17 05 03 01 25 02 info impl.yarnclientimpl killed application application 1489701902537 0147 config.mk mxnet template configuration compiling mxnet want change configuration, please use following steps. assume root directory mxnet. first copy file local changes ignored git cp make config.mk . next modify according entries, compile make build parallel 8 threads make j8 choice compiler export cc gcc export cxx g export nvcc nvcc whether compile options mxnet developer dev 0 whether compile debug debug 0 whether compiler profiler use profiler additional link flags want add add ldflags additional compile flags want add add cflags matrix computation libraries cpu gpu whether use cuda compile use cuda 0 add path cuda library link compile flag already add environment variable, leave none use cuda path usr local cuda use cuda path none whether use cudnn r3 library use cudnn 0 cuda architecture setting going them. cuda 6.0, comment 50 lines compatibility. cuda arch gencode arch compute 30,code sm 30 gencode arch compute 35,code sm 35 gencode arch compute 50,code sm 50 gencode arch compute 50,code compute 50 whether use cuda runtime compiling writing kernels native language i.e. python use nvrtc 0 whether use opencv compilation disable it, however, able use imbin iterator use opencv 0 use openmp parallelization use openmp 1 mkl ml library intel cpu xeon phi please refer mkl readme.md details mkl ml library folder, need root usr local change user home directory standard user use blas! mkl mklml root usr local whether use mkl2017 library use mkl2017 0 whether use mkl2017 experimental feature high performance prerequisite use mkl2017 1 use mkl2017 experimental 0 whether use nnpack library use nnpack 0 choose version blas want use mkl, blas, atlas, openblas default use atlas linux apple osx uname shell uname ifeq uname , darwin use blas apple else use blas atlas endif add path intel library, may need mkl, add path environment variable use intel path none use mkl blas, choose static link automatically allow python wrapper ifeq use mkl2017 , 0 ifeq use blas , mkl use static mkl 1 endif else use static mkl none endif settings power arm arch arch shell uname ifneq , filter arch , armv6l armv7l powerpc64le ppc64le aarch64 use sse 0 else use sse 1 endif distributed computing whether enable multi machine supporting use dist kvstore 1 whether allow read write hdfs directly. yes, hadoop required use hdfs 0 path libjvm.so. required use hdfs 1 libjvm java home jre lib amd64 server whether allow read write aws s3 directly. yes, libcurl4 openssl dev required, installed ubuntu sudo apt get install libcurl4 openssl dev use s3 0 additional operators path folders containing projects specific operators want put src operators extra operators features create c interface package use cpp package 0 plugins whether use caffe integration. requires installing caffe. also need add caffe path build lib ld library path caffe path home caffe mxnet plugins plugin caffe caffe.mk whether use torch integration. requires installing torch. also need add torch path install lib ld library path torch path home torch mxnet plugins plugin torch torch.mk warpctc path home warp ctc mxnet plugins plugin warpctc warpctc.mk whether use sframe integration. requires build sframe git github.com dato code sframe.git sframe path home sframe mxnet plugins plugin sframe plugin.mk use blas atlas add cflags usr include openblas add cflags usr include openblas l usr local lib add ldflags lopencv core lopencv imgproc lopencv imgcodecs someone please tell wrong set up?"
incubator-mxnet,3158,"following code test model however, takes long test train even bit longer worried training mistake something way wrong.",0,test using mx.mod.module?,"test using mx.mod.module? following code test model however, takes long test train even bit longer worried training mistake something way wrong."
incubator-mxnet,2438,"model.py python package, update params executor manager.param arrays, executor manager.grad arrays, updater updater, num device len ctx , kvstore kvstore used updating parameters. however, seen code, parameters gamma beta batch normalization operator included executor manager.aux arrays, included executor manager.param arrays. parameters batch normalization updated iteration ?",0,update aux parameters?,"update aux parameters? model.py python package, update params executor manager.param arrays, executor manager.grad arrays, updater updater, num device len ctx , kvstore kvstore used updating parameters. however, seen code, parameters gamma beta batch normalization operator included executor manager.aux arrays, included executor manager.param arrays. parameters batch normalization updated iteration ?"
incubator-mxnet,4906,"hi, attempting load ark files mxnet, exactly sure go it. go speech demo example, there, custom io files loading sequences certain format lstms. trying simply load ark file acoustic model, wondering could point exactly files use speech demo work around go it. venkatesh yzhang87",0,load ark files mxnet,"load ark files mxnet hi, attempting load ark files mxnet, exactly sure go it. go speech demo example, there, custom io files loading sequences certain format lstms. trying simply load ark file acoustic model, wondering could point exactly files use speech demo work around go it. venkatesh yzhang87"
incubator-mxnet,13739,"description building mxnet openblas fails due missing file 'cblas.h'. anyone meet error? environment info package used python r scala julia using python. build info required built source compiler gcc clang mingw visual studio g mxnet commit hash paste output here. 3bfcd93dff33d27a6087e39dd9387718456b5f51 build config paste content config.mk, build command. export cpath usr include openblas make j40 use blas openblas use cuda 1 use cudnn 1 use cuda path usr local cuda 9.0 use opencv 1 error message paste complete error message, including stack trace. file included home mxnet workspace source incubator mxnet 3rdparty mshadow mshadow tensor.h 16 0, include mxnet . base.h 32, include mxnet operator util.h 43, src operator contrib nnz.cc 25 home mxnet workspace source incubator mxnet 3rdparty mshadow mshadow . base.h 162 23 fatal error cblas.h file directory include compilation terminated. steps reproduce export cpath usr include openblas make j40 use blas openblas use cuda 1 use cudnn 1 use cuda path usr local cuda 9.0 use opencv 1",0,building mxnet openblas fails,"building mxnet openblas fails description building mxnet openblas fails due missing file 'cblas.h'. anyone meet error? environment info package used python r scala julia using python. build info required built source compiler gcc clang mingw visual studio g mxnet commit hash paste output here. 3bfcd93dff33d27a6087e39dd9387718456b5f51 build config paste content config.mk, build command. export cpath usr include openblas make j40 use blas openblas use cuda 1 use cudnn 1 use cuda path usr local cuda 9.0 use opencv 1 error message paste complete error message, including stack trace. file included home mxnet workspace source incubator mxnet 3rdparty mshadow mshadow tensor.h 16 0, include mxnet . base.h 32, include mxnet operator util.h 43, src operator contrib nnz.cc 25 home mxnet workspace source incubator mxnet 3rdparty mshadow mshadow . base.h 162 23 fatal error cblas.h file directory include compilation terminated. steps reproduce export cpath usr include openblas make j40 use blas openblas use cuda 1 use cudnn 1 use cuda path usr local cuda 9.0 use opencv 1"
incubator-mxnet,5950,"writing op, want unit test, find test examples ops, want know easy way test it.",0,unit test ops ?,"unit test ops ? writing op, want unit test, find test examples ops, want know easy way test it."
incubator-mxnet,9696,build mxnet source follows step 7002 https github.com apache incubator mxnet issues 7002 .but found works well use python3 error occurred python2 attributeerror 'module' object attribute 'warpctc',0,warpctc module object tribute 'warpctc',warpctc module object tribute 'warpctc' build mxnet source follows step 7002 https github.com apache incubator mxnet issues 7002 .but found works well use python3 error occurred python2 attributeerror 'module' object attribute 'warpctc'
incubator-mxnet,4902,"operating system android compiler android ndkr13 using api 21 also 19 tried build mxnet android using 'make android 1' finally get 'libmxnet predict.so'. tried use android project, coulde run throw exception solve problem? make mistakes compiling? mxnet version tried newest vision 0.8.0 vision, got exception.",0,dlopen failed cannot locate symbol cblas sgemm referenced libmxnet predict.so,"dlopen failed cannot locate symbol cblas sgemm referenced libmxnet predict.so operating system android compiler android ndkr13 using api 21 also 19 tried build mxnet android using 'make android 1' finally get 'libmxnet predict.so'. tried use android project, coulde run throw exception solve problem? make mistakes compiling? mxnet version tried newest vision 0.8.0 vision, got exception."
incubator-mxnet,13710,env python gluon cv scripts detection ssd train ssd.py info root start training epoch 0 error python' malloc memory corruption 0x00007f540c1a9550 backtrace lib64 libc.so.6 0x82c86 0x7f54d42dec86 lib64 libc.so.6 libc malloc 0x4c 0x7f54d42e184c lib64 libstdc .so.6 znwm 0x1d 0x7f54b48c7ecd data workspace mxnet python mxnet .. .. lib libmxnet.so 0x3c042d9 0x7f54691982d9 data workspace mxnet python mxnet .. .. lib libmxnet.so zn5mxnet6engine14threadedengine15executeoprblockens 10runcontextepns0 8oprblocke 0x589 0x7f5469194249 error full trach message error.log https github.com dmlc gluon cv files 2699683 error.log,0,error python' malloc memory corruption 0x00007f540c0a6190,error python' malloc memory corruption 0x00007f540c0a6190 env python gluon cv scripts detection ssd train ssd.py info root start training epoch 0 error python' malloc memory corruption 0x00007f540c1a9550 backtrace lib64 libc.so.6 0x82c86 0x7f54d42dec86 lib64 libc.so.6 libc malloc 0x4c 0x7f54d42e184c lib64 libstdc .so.6 znwm 0x1d 0x7f54b48c7ecd data workspace mxnet python mxnet .. .. lib libmxnet.so 0x3c042d9 0x7f54691982d9 data workspace mxnet python mxnet .. .. lib libmxnet.so zn5mxnet6engine14threadedengine15executeoprblockens 10runcontextepns0 8oprblocke 0x589 0x7f5469194249 error full trach message error.log https github.com dmlc gluon cv files 2699683 error.log
incubator-mxnet,2765,"imagenetrecordio, , recordio, , ! https github.com dengdan blog posts blob master github issue original n01484850 95.jpeg?raw true ! https github.com dengdan blog posts blob master github issue showed 2.png?raw true ! https github.com dengdan blog posts blob master github issue original n01484850 17.jpeg?raw true ! https github.com dengdan blog posts blob master github issue showed 1.png?raw true ! https github.com dengdan blog posts blob master github issue original n01484850 114.jpeg?raw true ! https github.com dengdan blog posts blob master github issue showed 3.png?raw true recordio recordio, jupyter notebook center cropresize, ?",0,imagenetrecordio,"imagenetrecordio imagenetrecordio, , recordio, , ! https github.com dengdan blog posts blob master github issue original n01484850 95.jpeg?raw true ! https github.com dengdan blog posts blob master github issue showed 2.png?raw true ! https github.com dengdan blog posts blob master github issue original n01484850 17.jpeg?raw true ! https github.com dengdan blog posts blob master github issue showed 1.png?raw true ! https github.com dengdan blog posts blob master github issue original n01484850 114.jpeg?raw true ! https github.com dengdan blog posts blob master github issue showed 3.png?raw true recordio recordio, jupyter notebook center cropresize, ?"
incubator-mxnet,2499,distributed mxnet apache hadoop yarn cannot well trained data directly read hdfs. anybody offers example?,0,completed example distributed yarn,completed example distributed yarn distributed mxnet apache hadoop yarn cannot well trained data directly read hdfs. anybody offers example?
incubator-mxnet,4407,"description speaks 4 arguments. inside two. fact, function waits two arguments. parameters x0 function? use parameters?",0,use function mx.nd.slice.axis ?,"use function mx.nd.slice.axis ? description speaks 4 arguments. inside two. fact, function waits two arguments. parameters x0 function? use parameters?"
incubator-mxnet,10906,tested opencv python 3.4.0.12 pypi.,0,transforms.compose working expected.,transforms.compose working expected. tested opencv python 3.4.0.12 pypi.
incubator-mxnet,4255,"bugs installation issues, please provide following information. information provide, likely people able help you. environment info operating system ubuntu 16.04 compiler gcc package used python r scala julia python mxnet version 0.7 installed source yes mxnet commit hash 487c22a50541686cc3fd207ad4656dbd2f9fa969 using python package, please provide python version distribution 2.7 using r package, please provide r error message mxneterror traceback recent call last 1 mx.nd.save s3 imagetraining.xxxx.com testing , a,b home ubuntu .local lib python2.7 site packages mxnet 0.7.0 py2.7.egg mxnet ndarray.pyc save fname, data 1082 mx uint len handles , 1083 c array ndarrayhandle, handles , 1084 keys 1085 1086 def imdecode str img, clip rect 0, 0, 0, 0 , none, index 0, channels 3, mean none home ubuntu .local lib python2.7 site packages mxnet 0.7.0 py2.7.egg mxnet base.pyc check call ret 75 76 ret ! 0 77 raise mxneterror py str lib.mxgetlasterror 78 79 sys.version info 0 3 mxneterror 03 32 57 src io s3 filesys.cc 682 check failed num retry max error retry maximum retry time reached minimum reproducible example using code, please provide short script reproduces error. create new jyputer notebook, run following python , make sure bucket name s3 period mx.nd.zeros 100, 200 b 1 mx.nd.save s3 imagetraining.xxx.com testing , a,b steps reproduce running standard examples, please provide commands run lead error. 1. start jupyter. 2. create notebook sample code 3. run fail tried solve it? 1. tried another s3 bucket without period bucket name. worked. 2. document s3 http docs.aws.amazon.com amazons3 latest dev virtualhosting.html, found following description using virtual hosted style buckets ssl, ssl wild card certificate matches buckets contain periods. work around this, use http write certificate verification logic. 3. looked source code dmlc core src io s3 filesys.cc 682, found source code using virtual host style url access s3, following code shows surl https path .host .s3.amazonaws.com ' ' removebeginslash path .name args virtual host style method fail ssl issue target bucket period bucket name.",0,mx.nd.save function can't used save s3 bucket periods bucket name,"mx.nd.save function can't used save s3 bucket periods bucket name bugs installation issues, please provide following information. information provide, likely people able help you. environment info operating system ubuntu 16.04 compiler gcc package used python r scala julia python mxnet version 0.7 installed source yes mxnet commit hash 487c22a50541686cc3fd207ad4656dbd2f9fa969 using python package, please provide python version distribution 2.7 using r package, please provide r error message mxneterror traceback recent call last 1 mx.nd.save s3 imagetraining.xxxx.com testing , a,b home ubuntu .local lib python2.7 site packages mxnet 0.7.0 py2.7.egg mxnet ndarray.pyc save fname, data 1082 mx uint len handles , 1083 c array ndarrayhandle, handles , 1084 keys 1085 1086 def imdecode str img, clip rect 0, 0, 0, 0 , none, index 0, channels 3, mean none home ubuntu .local lib python2.7 site packages mxnet 0.7.0 py2.7.egg mxnet base.pyc check call ret 75 76 ret ! 0 77 raise mxneterror py str lib.mxgetlasterror 78 79 sys.version info 0 3 mxneterror 03 32 57 src io s3 filesys.cc 682 check failed num retry max error retry maximum retry time reached minimum reproducible example using code, please provide short script reproduces error. create new jyputer notebook, run following python , make sure bucket name s3 period mx.nd.zeros 100, 200 b 1 mx.nd.save s3 imagetraining.xxx.com testing , a,b steps reproduce running standard examples, please provide commands run lead error. 1. start jupyter. 2. create notebook sample code 3. run fail tried solve it? 1. tried another s3 bucket without period bucket name. worked. 2. document s3 http docs.aws.amazon.com amazons3 latest dev virtualhosting.html, found following description using virtual hosted style buckets ssl, ssl wild card certificate matches buckets contain periods. work around this, use http write certificate verification logic. 3. looked source code dmlc core src io s3 filesys.cc 682, found source code using virtual host style url access s3, following code shows surl https path .host .s3.amazonaws.com ' ' removebeginslash path .name args virtual host style method fail ssl issue target bucket period bucket name."
incubator-mxnet,9392,recently found mobilenet gluon version model.param. would like symbolic version mobilenet model.param also .json format file. anything avilable modelzoo json mobilenet.param file. kindly reply... szha updates this....,0,official version mobilenet pretrained model .json file,official version mobilenet pretrained model .json file recently found mobilenet gluon version model.param. would like symbolic version mobilenet model.param also .json format file. anything avilable modelzoo json mobilenet.param file. kindly reply... szha updates this....
incubator-mxnet,8914,"description group context used model parallelism neural network train. mx.sym.logisticregressionoutput works used loss calculation, customized operator. environment info required os ubuntu 14.04.5 lts python 2.7.13 mxnet 0.12.1, pulled master branch built source package used python r scala julia python mxnet, numpy, scipy build info required built source bulit command line following https mxnet.incubator.apache.org get started install.html mxnet commit hash 2f8c1e83f94e84a25a48d2cd43136030fb3f2d1e build config change enable profiler. error message traceback recent call last file ctypes callbacks.c , line 315, 'calling callback function' file git mxnet python mxnet operator.py , line 621, creator op prop prop cls kwargs typeerror init got unexpected keyword argument ' ctx group ' minimum reproducible example script generate network, works fc2 mx.symbol.fullyconnected data concat, name 'fcout ' str gpu , num hidden dim num gpus loss mx.sym.logisticregressionoutput data fc2, label labs script generate network used customized operator, work fc2 mx.symbol.fullyconnected data concat, name 'fcout ' str gpu , num hidden dim num gpus act2 mx.sym.activation data fc2, name 'acout ' str gpu , act type 'sigmoid' loss mx.sym.custom data act2, label label, name 'ce ' str gpu , op type 'crossentropyloss' custom op support yet group context?",0,custom operator supported group context?,"custom operator supported group context? description group context used model parallelism neural network train. mx.sym.logisticregressionoutput works used loss calculation, customized operator. environment info required os ubuntu 14.04.5 lts python 2.7.13 mxnet 0.12.1, pulled master branch built source package used python r scala julia python mxnet, numpy, scipy build info required built source bulit command line following https mxnet.incubator.apache.org get started install.html mxnet commit hash 2f8c1e83f94e84a25a48d2cd43136030fb3f2d1e build config change enable profiler. error message traceback recent call last file ctypes callbacks.c , line 315, 'calling callback function' file git mxnet python mxnet operator.py , line 621, creator op prop prop cls kwargs typeerror init got unexpected keyword argument ' ctx group ' minimum reproducible example script generate network, works fc2 mx.symbol.fullyconnected data concat, name 'fcout ' str gpu , num hidden dim num gpus loss mx.sym.logisticregressionoutput data fc2, label labs script generate network used customized operator, work fc2 mx.symbol.fullyconnected data concat, name 'fcout ' str gpu , num hidden dim num gpus act2 mx.sym.activation data fc2, name 'acout ' str gpu , act type 'sigmoid' loss mx.sym.custom data act2, label label, name 'ce ' str gpu , op type 'crossentropyloss' custom op support yet group context?"
incubator-mxnet,15337,"description current mxnet master dev branch, pypi version 1.5.0b20190623 breaks loading certain mxnet models mxnet mkl mxnet cu100 , previously loaded successfully mxnet 1.4.1. model uses grouped depthwise a.ka. depthwise seperable convolutions could cause issue models e.g. crazyarafish 0.5.0 risev1.zip still work correctly usual. environment info using python, problem also occurs building mxnet cpp package source. error message minimum reproducible example steps reproduce download release https github.com queensgambit crazyara releases install python chess https github.com niklasf python chess . extract run commandline. details install instructions found install guide https github.com queensgambit crazyara wiki installation guide alternatively, load mxnet model directory manually python. someones idea recent change causes this? include automated unit tests mxnet ensure loading different model types preserved version updates?",0,current mxnet dev master breaks loading certain models,"current mxnet dev master breaks loading certain models description current mxnet master dev branch, pypi version 1.5.0b20190623 breaks loading certain mxnet models mxnet mkl mxnet cu100 , previously loaded successfully mxnet 1.4.1. model uses grouped depthwise a.ka. depthwise seperable convolutions could cause issue models e.g. crazyarafish 0.5.0 risev1.zip still work correctly usual. environment info using python, problem also occurs building mxnet cpp package source. error message minimum reproducible example steps reproduce download release https github.com queensgambit crazyara releases install python chess https github.com niklasf python chess . extract run commandline. details install instructions found install guide https github.com queensgambit crazyara wiki installation guide alternatively, load mxnet model directory manually python. someones idea recent change causes this? include automated unit tests mxnet ensure loading different model types preserved version updates?"
incubator-mxnet,695,why? least doc updated,0,mnist example disappeared,mnist example disappeared why? least doc updated
incubator-mxnet,8685,"note providing complete information concise form best way get help. issue template serves checklist essential information technical issues bug reports. non technical issues feature requests, feel free present information believe best form. q discussion, please start discussion thread https discuss.mxnet.io description currently running error trying build single jar osx. get following error environment info required mac osx. maven. scala package used python r scala julia using scala user, please provide 1. java version 1.8.0 144 2. maven version 3.5.0 3. scala runtime applicable 2.11.8 error message minimum reproducible example maven dependencies, plugins, assembly.xml assembly.xml steps reproduce paste commands ran produced error. 1. use dependencies plug pom above. tried solve it? 1. added assembly.xml page, luck jni.",0,error adding file set 'ml.dmlc.mxnet libmxnet scala osx x86 64 cpu jnilib v0.11.1a',"error adding file set 'ml.dmlc.mxnet libmxnet scala osx x86 64 cpu jnilib v0.11.1a' note providing complete information concise form best way get help. issue template serves checklist essential information technical issues bug reports. non technical issues feature requests, feel free present information believe best form. q discussion, please start discussion thread https discuss.mxnet.io description currently running error trying build single jar osx. get following error environment info required mac osx. maven. scala package used python r scala julia using scala user, please provide 1. java version 1.8.0 144 2. maven version 3.5.0 3. scala runtime applicable 2.11.8 error message minimum reproducible example maven dependencies, plugins, assembly.xml assembly.xml steps reproduce paste commands ran produced error. 1. use dependencies plug pom above. tried solve it? 1. added assembly.xml page, luck jni."
incubator-mxnet,7224,"hello, like use lstm combination cnn feature extractor classify videos. using mxnet cu80 0.10.0.post2 python 2.7.9. pretrained resnet 50 loaded chop final classification layer lstm expects input ntc format, thus forward pass first puts entire sequence cnn reshapes extracted features match lstm. lstm bind looks like sequences length model use bucketing forward backward pass however works cnn backward pass get following error really understand output cnn exactly dimension gradient lstm reshaping tried follow dcgan example uses similar technique. mention 2048 feature dimension last resnet layer, sequence lenght fixed 10 batch size case number sequences per batch 2, explains 20 2048. lstm grad ndarray size 20x2048 cnn output. thank's help, max",0,passing gradient 2 networks,"passing gradient 2 networks hello, like use lstm combination cnn feature extractor classify videos. using mxnet cu80 0.10.0.post2 python 2.7.9. pretrained resnet 50 loaded chop final classification layer lstm expects input ntc format, thus forward pass first puts entire sequence cnn reshapes extracted features match lstm. lstm bind looks like sequences length model use bucketing forward backward pass however works cnn backward pass get following error really understand output cnn exactly dimension gradient lstm reshaping tried follow dcgan example uses similar technique. mention 2048 feature dimension last resnet layer, sequence lenght fixed 10 batch size case number sequences per batch 2, explains 20 2048. lstm grad ndarray size 20x2048 cnn output. thank's help, max"
incubator-mxnet,10901,"check latest master, build . run . happens configurations, matter whether cpu gpu etc.",0,broken test sparse operator.test sparse mathematical core scipy 1.1.0,"broken test sparse operator.test sparse mathematical core scipy 1.1.0 check latest master, build . run . happens configurations, matter whether cpu gpu etc."
incubator-mxnet,1336,"hi, problems trying install mxnet r using pre built windows binary hosted https github.com dmlc mxnet releases using instructions installation guide page. seems work ok, mxnet library r gets created. however, run handwritten digits tutorial using lenet, using gpu seem optimise correctly. downloaded windows release zip github today. training using cpu similar example start training 1 devices 1 train accuracy 0.55708830548926 training using gpu different example start training 1 devices 1 train accuracy 0.0983770883054893 2 train accuracy 0.0983809523809524 3 train accuracy 0.0983809523809524 4 train accuracy 0.0983809523809524 5 train accuracy 0.0983809523809524 realise information given going hard diagnose, probably made mistake installation. help would appreciated. regards, chris attached log run r cmd install mxnet r cmd install output.txt https github.com dmlc mxnet files 101155 mxnet.r.cmd.install.output.txt",0,issue installing mxnet r package using gpu windows,"issue installing mxnet r package using gpu windows hi, problems trying install mxnet r using pre built windows binary hosted https github.com dmlc mxnet releases using instructions installation guide page. seems work ok, mxnet library r gets created. however, run handwritten digits tutorial using lenet, using gpu seem optimise correctly. downloaded windows release zip github today. training using cpu similar example start training 1 devices 1 train accuracy 0.55708830548926 training using gpu different example start training 1 devices 1 train accuracy 0.0983770883054893 2 train accuracy 0.0983809523809524 3 train accuracy 0.0983809523809524 4 train accuracy 0.0983809523809524 5 train accuracy 0.0983809523809524 realise information given going hard diagnose, probably made mistake installation. help would appreciated. regards, chris attached log run r cmd install mxnet r cmd install output.txt https github.com dmlc mxnet files 101155 mxnet.r.cmd.install.output.txt"
incubator-mxnet,3837,"looking resnet config file ilsvrc12 dataset, find one cifar10 dataset https github.com dmlc mxnet blob master example image classification symbol resnet.py resnet config file ilsvrc12 dataset? thank much!",0,resnet ilsvrc12 dataset,"resnet ilsvrc12 dataset looking resnet config file ilsvrc12 dataset, find one cifar10 dataset https github.com dmlc mxnet blob master example image classification symbol resnet.py resnet config file ilsvrc12 dataset? thank much!"
incubator-mxnet,3607,"dear team, trying use cnn dataset 6x8 data. working r. dataset following header attaching full dataset.csv dataset.zip https github.com dmlc mxnet files 548954 dataset.zip code using following run r code get following understand reason. may ask see discrepancy code? thanks. cheers.",0,cnn 6x8 data r,"cnn 6x8 data r dear team, trying use cnn dataset 6x8 data. working r. dataset following header attaching full dataset.csv dataset.zip https github.com dmlc mxnet files 548954 dataset.zip code using following run r code get following understand reason. may ask see discrepancy code? thanks. cheers."
incubator-mxnet,11039,download r linux . download r linux? thank,0,linux r,linux r download r linux . download r linux? thank
incubator-mxnet,1794,built torch got errors follows use modified . continued make procedure got happy results. maybe small bug related torch installation. hope helpful potential users.,0,error build torch,error build torch built torch got errors follows use modified . continued make procedure got happy results. maybe small bug related torch installation. hope helpful potential users.
incubator-mxnet,11576,nan,0,attributeerror 'resnetv2' object attribute 'save parameters',attributeerror 'resnetv2' object attribute 'save parameters' nan
incubator-mxnet,2749,"xavier initializer mx.init.xavier factor type , magnitude 3 , msra initializer mx.init.xavier factor type , rnd type gaussian , magnitude 2 . default initializer adopts strange magnitude 2.34, special reason this?",0,"special reason default initializer mx.init.xavier factor type , magnitude 2.34 take magnitude 2.34?","special reason default initializer mx.init.xavier factor type , magnitude 2.34 take magnitude 2.34? xavier initializer mx.init.xavier factor type , magnitude 3 , msra initializer mx.init.xavier factor type , rnd type gaussian , magnitude 2 . default initializer adopts strange magnitude 2.34, special reason this?"
incubator-mxnet,894,"got following error package compilation ndarray.cc static member function static void mxnet r ndarray save const list , const string ndarray.cc static member function static void mxnet r ndarray save const list , const string ndarray.cc 220 15 error ambiguous overload operator operand types std vectorstd cxx11 basic string rcpp namesproxypolicyrcpp vector const namesproxy lst names data lst.names",0,r package compilation error,"r package compilation error got following error package compilation ndarray.cc static member function static void mxnet r ndarray save const list , const string ndarray.cc static member function static void mxnet r ndarray save const list , const string ndarray.cc 220 15 error ambiguous overload operator operand types std vectorstd cxx11 basic string rcpp namesproxypolicyrcpp vector const namesproxy lst names data lst.names"
incubator-mxnet,5016,"hi, new mxnet, walking kaggle project familiar python interface. something confused try call mx.mod.module.predict function complete final work. since headache called mx.mod.module.score function, predict function also work fine unless bug around. read python source code find bug. following info displayed run code notice index range problems, fault directed , check variable find equals 1. that's key problem. next find defined. since initialize variable following code check find assigned 455 line. change , successfully fixed bug. still sure whether right code second way also fix problem. idea little bug? certainly fix bug, could give chance help pull request merge little work mxnet? thank you,",0,little bug mxnet python code,"little bug mxnet python code hi, new mxnet, walking kaggle project familiar python interface. something confused try call mx.mod.module.predict function complete final work. since headache called mx.mod.module.score function, predict function also work fine unless bug around. read python source code find bug. following info displayed run code notice index range problems, fault directed , check variable find equals 1. that's key problem. next find defined. since initialize variable following code check find assigned 455 line. change , successfully fixed bug. still sure whether right code second way also fix problem. idea little bug? certainly fix bug, could give chance help pull request merge little work mxnet? thank you,"
incubator-mxnet,8062,"bugs installation issues, please provide following information. information provide, likely people able help you. environment info operating system deeplearninig ami compiler package used python r scala julia mxnet version installed source mxnet commit hash ae975e5f8a70f9e2c36f78278f2553cdd4d87e79 using python package, please provide python version distribution using r package, please provide r error message please paste full error message, including stack trace. minimum reproducible example using code, please provide short script reproduces error. steps reproduce running standard examples, please provide commands run lead error. 1. run code. kind error libsvm iterator. 2. 3.",0,csviter libsvmiter returning correct number batches per epoch,"csviter libsvmiter returning correct number batches per epoch bugs installation issues, please provide following information. information provide, likely people able help you. environment info operating system deeplearninig ami compiler package used python r scala julia mxnet version installed source mxnet commit hash ae975e5f8a70f9e2c36f78278f2553cdd4d87e79 using python package, please provide python version distribution using r package, please provide r error message please paste full error message, including stack trace. minimum reproducible example using code, please provide short script reproduces error. steps reproduce running standard examples, please provide commands run lead error. 1. run code. kind error libsvm iterator. 2. 3."
incubator-mxnet,1118,nan,0,r mx.symbol.bind missing one parameter,r mx.symbol.bind missing one parameter nan
incubator-mxnet,8037,"want maximize dice coefficient minimize smooth l1 loss time, tried multiple dice coefficient seems work.",0,maximize one loss function minimize another one?,"maximize one loss function minimize another one? want maximize dice coefficient minimize smooth l1 loss time, tried multiple dice coefficient seems work."
incubator-mxnet,1008,"help information mx.simple.bind simple. can't figure really use help message. also, can't find sample code used function. wonder help understand usage little better. many thanks help.",0,r use mx.simple.bind r package,"r use mx.simple.bind r package help information mx.simple.bind simple. can't figure really use help message. also, can't find sample code used function. wonder help understand usage little better. many thanks help."
incubator-mxnet,3914,im2rec.pyimage.lstinteger image index,0,im2rec,im2rec im2rec.pyimage.lstinteger image index
incubator-mxnet,2149,"trying run mxnet nvidia gpu clusters, sugon hpc. know subject jobs though . gpu programming quite new me. tutorial follow?",0,run mxnet gpu clusters?,"run mxnet gpu clusters? trying run mxnet nvidia gpu clusters, sugon hpc. know subject jobs though . gpu programming quite new me. tutorial follow?"
incubator-mxnet,12640,"description basemodule.fit function, forward backward needs 0.00055s, update needs 0.079953s, next data iter needs 0.000265s. however, run forward backward next data iter together spend 2.919867s. could please tell reason time bottleneck? environment info required platform macos language python 2.7 platform version mxnet 1.2.0 device 2.9 ghz intel core i5 test runs using single cpu memory 8 gb 1867 mhz ddr3 install tool pip steps reproduce basemodule.fit function run forward backward result calc time 0.000551 run next data iter result dataiter time 0.000265 run forward backward next data iter together result calc dataiter time 2.919867 tried solve it? 1. read source code forward backward imagerecorditer idea. 2. found little modify data batch variable would also cause time bottleneck loop forward backward. thanks lot kind !",0,time bottleneck forward backward next train iter,"time bottleneck forward backward next train iter description basemodule.fit function, forward backward needs 0.00055s, update needs 0.079953s, next data iter needs 0.000265s. however, run forward backward next data iter together spend 2.919867s. could please tell reason time bottleneck? environment info required platform macos language python 2.7 platform version mxnet 1.2.0 device 2.9 ghz intel core i5 test runs using single cpu memory 8 gb 1867 mhz ddr3 install tool pip steps reproduce basemodule.fit function run forward backward result calc time 0.000551 run next data iter result dataiter time 0.000265 run forward backward next data iter together result calc dataiter time 2.919867 tried solve it? 1. read source code forward backward imagerecorditer idea. 2. found little modify data batch variable would also cause time bottleneck loop forward backward. thanks lot kind !"
incubator-mxnet,7552,networks suitable cifar 10 besides inception resnet?,0,networks cifar 10.,networks cifar 10. networks suitable cifar 10 besides inception resnet?
incubator-mxnet,9359,data consists lines like following 1 4 1 6 1 15 1 21 1 35 1 40 1 57 1 63 1 67 1 73 1 74 1 77 1 80 1 83 1 n,0,gluon's dnn support data format libsvm mxnet's?,gluon's dnn support data format libsvm mxnet's? data consists lines like following 1 4 1 6 1 15 1 21 1 35 1 40 1 57 1 63 1 67 1 73 1 74 1 77 1 80 1 83 1 n
incubator-mxnet,3759,nan,0,get result intermediate network?,get result intermediate network? nan
incubator-mxnet,10890,"idea similar gan, instead two neural networks want use neural network generator prebuilt glm discriminator. possible using mxnet symbols? example pseudo code",0,r possible train mxnet symbols glm pretrained model loss?,"r possible train mxnet symbols glm pretrained model loss? idea similar gan, instead two neural networks want use neural network generator prebuilt glm discriminator. possible using mxnet symbols? example pseudo code"
incubator-mxnet,7127,environment info operating system windows package used r installed using binaries. parameter use regularization l2 feed forward neural network? implement this. could find references weight decay parameter wd . sure include it. example would much appreciated.,0,use regularization feed forward neural network r using mxnet?,use regularization feed forward neural network r using mxnet? environment info operating system windows package used r installed using binaries. parameter use regularization l2 feed forward neural network? implement this. could find references weight decay parameter wd . sure include it. example would much appreciated.
incubator-mxnet,1455,"train cifar train imagenet.py, rand crop set true default, however, crop ratio size manually set. actually happen behind setting?",0,rand crop example,"rand crop example train cifar train imagenet.py, rand crop set true default, however, crop ratio size manually set. actually happen behind setting?"
incubator-mxnet,3659,"questions embedding layer 1 function embedding layer ? keras ,it says turn positive integers indexes dense vectors fixed size. eg. 4 , 20 0.25, 0.1 , 0.6, 0.2 meaning ? used ? 2 try simple codes see embedding layer works use code xlvector blog http blog.xlvector.net 2016 05 mxnet regression classification concret continuous features 201 seriesdata 1 range 100 101 range 100 range 10000 k random.randint 0, 199 count 1000 seriesdata k j range count dis random.random 10 price seriesdata k math.sqrt 1.0 dis print str price ' t' str dis ' t' str k dis dis mx.symbol.variable 'dis' price price mx.symbol.variable 'price' price interval price interval mx.symbol.variable 'price interval' series series mx.symbol.variable 'series' series mx.symbol.embedding data series, input dim 200, output dim 100, name series embed ne mx.symbol.flatten series out, name series flatten arg , out,aux ne.infer shape series 6,1 print arg print ne.list arguments print execute ne.bind ctx mx.cpu ,args 'dis flatten' seriesdata execute.forward execute.outputs 0 print out.asnumpy console 6l, 1l , 200l, 100l 'series', 'series embed weight' 6l, 100l throw error raise valueerror 'must specify arguments s' arg key valueerror must specify arguments args know means give values series embed weight . example, values give it?",0,questions embedding layer,"questions embedding layer questions embedding layer 1 function embedding layer ? keras ,it says turn positive integers indexes dense vectors fixed size. eg. 4 , 20 0.25, 0.1 , 0.6, 0.2 meaning ? used ? 2 try simple codes see embedding layer works use code xlvector blog http blog.xlvector.net 2016 05 mxnet regression classification concret continuous features 201 seriesdata 1 range 100 101 range 100 range 10000 k random.randint 0, 199 count 1000 seriesdata k j range count dis random.random 10 price seriesdata k math.sqrt 1.0 dis print str price ' t' str dis ' t' str k dis dis mx.symbol.variable 'dis' price price mx.symbol.variable 'price' price interval price interval mx.symbol.variable 'price interval' series series mx.symbol.variable 'series' series mx.symbol.embedding data series, input dim 200, output dim 100, name series embed ne mx.symbol.flatten series out, name series flatten arg , out,aux ne.infer shape series 6,1 print arg print ne.list arguments print execute ne.bind ctx mx.cpu ,args 'dis flatten' seriesdata execute.forward execute.outputs 0 print out.asnumpy console 6l, 1l , 200l, 100l 'series', 'series embed weight' 6l, 100l throw error raise valueerror 'must specify arguments s' arg key valueerror must specify arguments args know means give values series embed weight . example, values give it?"
incubator-mxnet,9372,"description try compile mxnet source code get error usr bin ld cannot find lcuda even try reset path cuda use cuda path apps2 cuda 8.0.61 , however, compiler keep finding cuda system folder. environment info required red hat enterprise linux workstation release 6.7 santiago build info required built source compiler gcc clang mingw visual studio gcc 4.8.2 error message usr bin ld cannot find lcuda collect2 error ld returned 1 exit status make usr bin im2rec error 1 bin ld cannot find lcuda make waiting unfinished jobs.... collect2 error ld returned 1 exit status make lib libmxnet.so error 1 tried solve it? 1. add following two lines config.mk files add ldflags l apps2 cuda 8.0.61 lib64 add cflags apps2 cuda 8.0.61 include command line use compile make j 10 use opencv 1 use blas openblas use cuda 1 use cuda path apps2 cuda 8.0.61 one thing need mentioned cuda installed default system path. installed cuda customized directory. please help solve problem. thank much",0,compile error usr bin ld cannot find lcuda,"compile error usr bin ld cannot find lcuda description try compile mxnet source code get error usr bin ld cannot find lcuda even try reset path cuda use cuda path apps2 cuda 8.0.61 , however, compiler keep finding cuda system folder. environment info required red hat enterprise linux workstation release 6.7 santiago build info required built source compiler gcc clang mingw visual studio gcc 4.8.2 error message usr bin ld cannot find lcuda collect2 error ld returned 1 exit status make usr bin im2rec error 1 bin ld cannot find lcuda make waiting unfinished jobs.... collect2 error ld returned 1 exit status make lib libmxnet.so error 1 tried solve it? 1. add following two lines config.mk files add ldflags l apps2 cuda 8.0.61 lib64 add cflags apps2 cuda 8.0.61 include command line use compile make j 10 use opencv 1 use blas openblas use cuda 1 use cuda path apps2 cuda 8.0.61 one thing need mentioned cuda installed default system path. installed cuda customized directory. please help solve problem. thank much"
incubator-mxnet,9971,"using pip install mxnet cu90 onnx mxnet causes pip reinstall mxnet, removing gpu support. changing order packages installed address issue.",0,onnx mxnet pip package prevents gpu use,"onnx mxnet pip package prevents gpu use using pip install mxnet cu90 onnx mxnet causes pip reinstall mxnet, removing gpu support. changing order packages installed address issue."
incubator-mxnet,17113,"description see https github.com apache incubator mxnet pull 17098 please help review it. currently fail systems, openmp detects attempts build nvptx offloading target needed, whose build fails .",0,3rdparty openmp cmake build broken systems,"3rdparty openmp cmake build broken systems description see https github.com apache incubator mxnet pull 17098 please help review it. currently fail systems, openmp detects attempts build nvptx offloading target needed, whose build fails ."
incubator-mxnet,4249,add support std deviation operator. refer 4207 could done.,0,keras add support std deviation operator.,keras add support std deviation operator. add support std deviation operator. refer 4207 could done.
incubator-mxnet,7457,"minimal example used load symbol c api work. unless missing something completely like compiler flag, otherwise environment info operating system centos compiler gcc package used python r scala julia c mxnet version 0.9.3 error message please paste full error message, including stack trace. 16 17 17 share tools mxnet dmlc core include dmlc . logging.h 300 16 17 17 src core op.cc 55 check failed op ! nullptr operator fullyconnected registered stack trace returned 10 entries bt 0 share tools mxnet lib libmxnet.so zn4nnvm2op3geterkss 0x329 0x7fcb0323f179 bt 1 share tools mxnet lib libmxnet.so 0xef8268 0x7fcb03227268 bt 2 share tools mxnet lib libmxnet.so zn4dmlc20jsonobjectreadhelper13readallfieldsepns 10jsonreadere 0x100 0x7fcb0322d680 bt 3 share tools mxnet lib libmxnet.so 0xef70ef 0x7fcb032260ef bt 4 share tools mxnet lib libmxnet.so znst17 function handlerifn4nnvm5graphes1 eps2 e9 invokeerkst9 datas1 0x11f 0x7fcb02e8c3ef bt 5 share tools mxnet lib libmxnet.so zn4nnvm11applypassesens 5grapherkst6vectorisssaissee 0x501 0x7fcb03232b51 bt 6 share tools mxnet lib libmxnet.so zn5mxnet18loadlegacyjsonpassen4nnvm5graphe 0x180 0x7fcb02e851c0 bt 7 share tools mxnet lib libmxnet.so znst17 function handlerifn4nnvm5graphes1 eps2 e9 invokeerkst9 datas1 0x11f 0x7fcb02e8c3ef bt 8 share tools mxnet lib libmxnet.so zn4nnvm11applypassesens 5grapherkst6vectorisssaissee 0x501 0x7fcb03232b51 bt 9 share tools mxnet lib libmxnet.so zn4nnvm9applypassens 5grapherkss 0x8e 0x7fcb0318006e minimum reproducible example test.c include include mxnet c api.h int main void const char symfn net symbol.json symbolhandle sym mxsymbolcreatefromfile symfn, sym return 0 steps reproduce running standard examples, please provide commands run lead error. 1. compiled gcc i.. include l. wl, whole archive lmxnet wl, whole archive test.c testrun 2. run tried solve it? 1. including every header include mxnet directory 2. copying compiler flags make file",0,minimal c example fails register operators,"minimal c example fails register operators minimal example used load symbol c api work. unless missing something completely like compiler flag, otherwise environment info operating system centos compiler gcc package used python r scala julia c mxnet version 0.9.3 error message please paste full error message, including stack trace. 16 17 17 share tools mxnet dmlc core include dmlc . logging.h 300 16 17 17 src core op.cc 55 check failed op ! nullptr operator fullyconnected registered stack trace returned 10 entries bt 0 share tools mxnet lib libmxnet.so zn4nnvm2op3geterkss 0x329 0x7fcb0323f179 bt 1 share tools mxnet lib libmxnet.so 0xef8268 0x7fcb03227268 bt 2 share tools mxnet lib libmxnet.so zn4dmlc20jsonobjectreadhelper13readallfieldsepns 10jsonreadere 0x100 0x7fcb0322d680 bt 3 share tools mxnet lib libmxnet.so 0xef70ef 0x7fcb032260ef bt 4 share tools mxnet lib libmxnet.so znst17 function handlerifn4nnvm5graphes1 eps2 e9 invokeerkst9 datas1 0x11f 0x7fcb02e8c3ef bt 5 share tools mxnet lib libmxnet.so zn4nnvm11applypassesens 5grapherkst6vectorisssaissee 0x501 0x7fcb03232b51 bt 6 share tools mxnet lib libmxnet.so zn5mxnet18loadlegacyjsonpassen4nnvm5graphe 0x180 0x7fcb02e851c0 bt 7 share tools mxnet lib libmxnet.so znst17 function handlerifn4nnvm5graphes1 eps2 e9 invokeerkst9 datas1 0x11f 0x7fcb02e8c3ef bt 8 share tools mxnet lib libmxnet.so zn4nnvm11applypassesens 5grapherkst6vectorisssaissee 0x501 0x7fcb03232b51 bt 9 share tools mxnet lib libmxnet.so zn4nnvm9applypassens 5grapherkss 0x8e 0x7fcb0318006e minimum reproducible example test.c include include mxnet c api.h int main void const char symfn net symbol.json symbolhandle sym mxsymbolcreatefromfile symfn, sym return 0 steps reproduce running standard examples, please provide commands run lead error. 1. compiled gcc i.. include l. wl, whole archive lmxnet wl, whole archive test.c testrun 2. run tried solve it? 1. including every header include mxnet directory 2. copying compiler flags make file"
incubator-mxnet,3205,"compiled warp ctc mxnet successful. add warp ctc config.mkand compiled mxnet run lstm ocr.py, problem disappear captcha.image import imagecaptcha traceback recent call last file , line 1, importerror module named captcha.image idea this?",0,importerror module named captcha.image,"importerror module named captcha.image compiled warp ctc mxnet successful. add warp ctc config.mkand compiled mxnet run lstm ocr.py, problem disappear captcha.image import imagecaptcha traceback recent call last file , line 1, importerror module named captcha.image idea this?"
incubator-mxnet,7997,"hi, run train imagenet benchmark option, see epoch executes 50 batches irrespective batch size. way change number batches? thanks, saiful",0,question benchmarking imagenet,"question benchmarking imagenet hi, run train imagenet benchmark option, see epoch executes 50 batches irrespective batch size. way change number batches? thanks, saiful"
incubator-mxnet,13011,description sphinx throwing errors generating docs . error,0,mxnet.module.basemodule docs errors,mxnet.module.basemodule docs errors description sphinx throwing errors generating docs . error
incubator-mxnet,3192,whether get neural output finish training,0,get layer output,get layer output whether get neural output finish training
incubator-mxnet,4699,"environment info operating system ubuntu 14.04 compiler gcc 4.8 package used python r scala julia scala mxnet version v0.9 installed source mxnet commit hash 6d05979cce53041f356204b17db2effb09371328 error message steps reproduce 1. compile mxnet cuda nvrtc compile scalapkg 2. 3. tried solve it? locate commit cause problem, pr 4528 roll back commit 50a3a3184e3034a98b2d4ad82f186d035803ab9b 4528 problem solved. also check cuda doc, error code 201 means cuda error invalid context 201 frequently indicates context bound current thread. also returned context passed api call valid handle context cuctxdestroy invoked . also returned user mixes different api versions i.e. 3010 context 3020 api calls . see cuctxgetapiversion details. javelinjs piiswrong",0,scala fail run example examplecustomopwithrtc.scala,"scala fail run example examplecustomopwithrtc.scala environment info operating system ubuntu 14.04 compiler gcc 4.8 package used python r scala julia scala mxnet version v0.9 installed source mxnet commit hash 6d05979cce53041f356204b17db2effb09371328 error message steps reproduce 1. compile mxnet cuda nvrtc compile scalapkg 2. 3. tried solve it? locate commit cause problem, pr 4528 roll back commit 50a3a3184e3034a98b2d4ad82f186d035803ab9b 4528 problem solved. also check cuda doc, error code 201 means cuda error invalid context 201 frequently indicates context bound current thread. also returned context passed api call valid handle context cuctxdestroy invoked . also returned user mixes different api versions i.e. 3010 context 3020 api calls . see cuctxgetapiversion details. javelinjs piiswrong"
incubator-mxnet,5176,"hi! got following error try demo code, thought usage fine since problems numpy, typeerror occur normal? best regards, haria",0,source array must array like object,"source array must array like object hi! got following error try demo code, thought usage fine since problems numpy, typeerror occur normal? best regards, haria"
incubator-mxnet,4236,"want find max value tensor. think method follows tensor data data activation kdata .flatto2d shape maxshape shape 1 tensor max newtensor maxshape,dtype 0 ,false max pool pad data,0,0 ,shape,data.shape 0 ,data.shape 1 ,1,1 concise way? want use max value following calculation gpu ,i can't access using max 0 cpu kernal, solve it?",0,find max value tensor,"find max value tensor want find max value tensor. think method follows tensor data data activation kdata .flatto2d shape maxshape shape 1 tensor max newtensor maxshape,dtype 0 ,false max pool pad data,0,0 ,shape,data.shape 0 ,data.shape 1 ,1,1 concise way? want use max value following calculation gpu ,i can't access using max 0 cpu kernal, solve it?"
incubator-mxnet,7533,"call mx.nd.smooth l1 without scalar parameter cause mxnet crash steps reproduce running standard examples, please provide commands run lead error. tried solve it? specify scalar solve however, example, argument specified , confusing. http mxnet.io api python symbol.html mxnet.symbol.smooth l1 add default value scalar 1.0 , fix document, fix uncaught exception anyway",0,call symbol nd.smooth l1 without scalar causing uncaught exception,"call symbol nd.smooth l1 without scalar causing uncaught exception call mx.nd.smooth l1 without scalar parameter cause mxnet crash steps reproduce running standard examples, please provide commands run lead error. tried solve it? specify scalar solve however, example, argument specified , confusing. http mxnet.io api python symbol.html mxnet.symbol.smooth l1 add default value scalar 1.0 , fix document, fix uncaught exception anyway"
incubator-mxnet,7766,"fixed size pooling spp commonly used cnn, however found spp exist mxnet symbols.",0,implement fixed size pooling like spatial pyramid pooling?,"implement fixed size pooling like spatial pyramid pooling? fixed size pooling spp commonly used cnn, however found spp exist mxnet symbols."
incubator-mxnet,3347,https github.com dmlc mxnet blob master docs system note engine.md case study multi gpu neural net original sample code revised following? problem graph show ! https raw.githubusercontent.com dmlc web data master mxnet engine dep net.png,0,typo dependency engine deep learning ?,typo dependency engine deep learning ? https github.com dmlc mxnet blob master docs system note engine.md case study multi gpu neural net original sample code revised following? problem graph show ! https raw.githubusercontent.com dmlc web data master mxnet engine dep net.png
incubator-mxnet,11567,"description hi, following pr builds failing dockcross linux arm64. https github.com apache incubator mxnet pull 11478 https github.com apache incubator mxnet pull 11548 probably issue dockerfile https hub.docker.com r mxnetci dockcross linux arm64 tags marcoabreu please take look? anirudh2290 environment info required python diagnose.py package used python r scala julia using scala user, please provide 1. java version 2. maven version 3. scala runtime applicable r user, please provide r build info required built source compiler gcc clang mingw visual studio mxnet commit hash paste output here. build config paste content config.mk, build command. error message paste complete error message, including stack trace. minimum reproducible example using code, please provide short script reproduces error. otherwise, please provide link existing example. steps reproduce paste commands ran produced error. 1. 2. tried solve it? 1. 2.",0,mxnet v1.2.0 branch pr build failure,"mxnet v1.2.0 branch pr build failure description hi, following pr builds failing dockcross linux arm64. https github.com apache incubator mxnet pull 11478 https github.com apache incubator mxnet pull 11548 probably issue dockerfile https hub.docker.com r mxnetci dockcross linux arm64 tags marcoabreu please take look? anirudh2290 environment info required python diagnose.py package used python r scala julia using scala user, please provide 1. java version 2. maven version 3. scala runtime applicable r user, please provide r build info required built source compiler gcc clang mingw visual studio mxnet commit hash paste output here. build config paste content config.mk, build command. error message paste complete error message, including stack trace. minimum reproducible example using code, please provide short script reproduces error. otherwise, please provide link existing example. steps reproduce paste commands ran produced error. 1. 2. tried solve it? 1. 2."
incubator-mxnet,16037,"description 1 create rnn op bind 2 run forward pass 3 change ndarray holding rnn parameters 4 run forward pass output change, unless second forward pass performed training mode . setting fix issue, using build without mkl dnn does. severly impacts training validation set, evaluating performance validation set typically performed several updates weights. case, validation shows improvement output layer stuck first training iteration. environment info required package used python r scala julia python build info required built source compiler gcc clang mingw visual studio gcc mxnet commit hash 076b2f330c60f05cb939beea28dd04cd571a34c0 build config plain config.mk, except use opencv 0 minimum reproducible example using build mkl dnn, script print something like shows output change changing weights unless forward pass performed training mode. setting fix issue, using build without mkl dnn does.",0,lstm mkl dnn produces wrong output weights changed,"lstm mkl dnn produces wrong output weights changed description 1 create rnn op bind 2 run forward pass 3 change ndarray holding rnn parameters 4 run forward pass output change, unless second forward pass performed training mode . setting fix issue, using build without mkl dnn does. severly impacts training validation set, evaluating performance validation set typically performed several updates weights. case, validation shows improvement output layer stuck first training iteration. environment info required package used python r scala julia python build info required built source compiler gcc clang mingw visual studio gcc mxnet commit hash 076b2f330c60f05cb939beea28dd04cd571a34c0 build config plain config.mk, except use opencv 0 minimum reproducible example using build mkl dnn, script print something like shows output change changing weights unless forward pass performed training mode. setting fix issue, using build without mkl dnn does."
incubator-mxnet,5224,"hello, use mxnet face detection model 'mtcnn' inference image, shows error doubt whether caused mxnet not. give advices solve problem?",0,include dmlc logging.h 235 00 59 21 src io local filesys.cc 154 check failed allow null localfilesystem fail open model det1 symbol.json,"include dmlc logging.h 235 00 59 21 src io local filesys.cc 154 check failed allow null localfilesystem fail open model det1 symbol.json hello, use mxnet face detection model 'mtcnn' inference image, shows error doubt whether caused mxnet not. give advices solve problem?"
incubator-mxnet,6095,"calling train several times, ran gpu memory pooled storage manager.h 80 cudamalloc failed memory looks like gpu buffers never cleared memory runs out. needs solved able use mxnet larger application, multiple train attempts made sequence, without restarting app.",0,multiple calls train cause cudamalloc fail,"multiple calls train cause cudamalloc fail calling train several times, ran gpu memory pooled storage manager.h 80 cudamalloc failed memory looks like gpu buffers never cleared memory runs out. needs solved able use mxnet larger application, multiple train attempts made sequence, without restarting app."
incubator-mxnet,9711,"presently, use 4 dim data flow build models, batchsize x w x h x channels batchsize x channels x h x w . possible build 5 dim data flow? maybe batchsize x capsule x w x h x channels , useful ideas like capsnet reshape data. build high dimensions models complicated better structure data extraction performance. according mobilenetv2 , performance relu better high dimensions low dimensions. maybe even better build entire model high dimensions operations. more, 6 dim 7 dim operations, even x dim operations? kind operations possible? possible, hard implement them?",0,possible support 5 dim x dim data operation,"possible support 5 dim x dim data operation presently, use 4 dim data flow build models, batchsize x w x h x channels batchsize x channels x h x w . possible build 5 dim data flow? maybe batchsize x capsule x w x h x channels , useful ideas like capsnet reshape data. build high dimensions models complicated better structure data extraction performance. according mobilenetv2 , performance relu better high dimensions low dimensions. maybe even better build entire model high dimensions operations. more, 6 dim 7 dim operations, even x dim operations? kind operations possible? possible, hard implement them?"
incubator-mxnet,6005,"finally, found training loss decreased kvstore. kvstore get new weight value init. kvstore updates weight itself. set param affect weight kvstore actually! but, training, need exchange weight value periodically. so, suggest kvstore accept new weight value using set params training time. piiswrong tqchen",0,advice set params using kvstore,"advice set params using kvstore finally, found training loss decreased kvstore. kvstore get new weight value init. kvstore updates weight itself. set param affect weight kvstore actually! but, training, need exchange weight value periodically. so, suggest kvstore accept new weight value using set params training time. piiswrong tqchen"
incubator-mxnet,9847,tried implement simple linear regression ndarray imperative style. batchdatabatch think point error src imperative . imperative utils.h 123 check failed infertype.count attrs.op operator linearregressionoutput missing finfertype attribute . also got similar errors . get errors regression want. imperative style regression outputs may used frequently anyway could useful sometimes listed api document anyway. please fix them.,0,imperative regression output layers broken,imperative regression output layers broken tried implement simple linear regression ndarray imperative style. batchdatabatch think point error src imperative . imperative utils.h 123 check failed infertype.count attrs.op operator linearregressionoutput missing finfertype attribute . also got similar errors . get errors regression want. imperative style regression outputs may used frequently anyway could useful sometimes listed api document anyway. please fix them.
incubator-mxnet,9826,try test mtcnn face detection repository https github.com pangyupo mxnet mtcnn face detection iteration got severe crash mxnet error https gist.github.com edmbernard 91731e795decd7b7c5456cb0d7a1d303 able reproduce code. code python mxnet use c . someone idea come ? note maybe linked code use old api feedforward note gpu got error cpu got error,0,crash mxnet error python3' corrupted double linked list 0x00007f1c4b2e09d0,crash mxnet error python3' corrupted double linked list 0x00007f1c4b2e09d0 try test mtcnn face detection repository https github.com pangyupo mxnet mtcnn face detection iteration got severe crash mxnet error https gist.github.com edmbernard 91731e795decd7b7c5456cb0d7a1d303 able reproduce code. code python mxnet use c . someone idea come ? note maybe linked code use old api feedforward note gpu got error cpu got error
incubator-mxnet,5163,"use im2rec according lst file generate rec files. could generate synset.txt files? use ndarrayiter, find relation sequence prediction label file? many thanks.",0,generate synset.txt files?,"generate synset.txt files? use im2rec according lst file generate rec files. could generate synset.txt files? use ndarrayiter, find relation sequence prediction label file? many thanks."
incubator-mxnet,4660,"environment info operating system ubuntu 14.04 compiler gcc 4.8 package used python r scala julia python mxnet version 0.9.1 0.7.1 nightly 32cb6bc installed source mxnet commit hash 32cb6bc using python package, please provide python version distribution anaconda 2.7.11 error message used cnn lstm ctc ocr tasks, work well mxnet 0.7 version, use latest mxnet version 0.9.1 block forever minimum reproducible example steps reproduce 1. 2. 3. tried solve it?",0,cudnn convolution performance tests cnn lstm ctc,"cudnn convolution performance tests cnn lstm ctc environment info operating system ubuntu 14.04 compiler gcc 4.8 package used python r scala julia python mxnet version 0.9.1 0.7.1 nightly 32cb6bc installed source mxnet commit hash 32cb6bc using python package, please provide python version distribution anaconda 2.7.11 error message used cnn lstm ctc ocr tasks, work well mxnet 0.7 version, use latest mxnet version 0.9.1 block forever minimum reproducible example steps reproduce 1. 2. 3. tried solve it?"
incubator-mxnet,3684,"hi javelinjs use mxnet scala deploy model works great, engineering team stress tested service found service could stop response while. printed info here's code",0,would predict ndarray.toarray cause block ?,"would predict ndarray.toarray cause block ? hi javelinjs use mxnet scala deploy model works great, engineering team stress tested service found service could stop response while. printed info here's code"
incubator-mxnet,371,"recall, version 1 cxxnet, utility creating .bin image mean iterate dataset, compute mean image, save jpg, convert . bin im2rec, wondering utility missed documentation?",0,computing image mean,"computing image mean recall, version 1 cxxnet, utility creating .bin image mean iterate dataset, compute mean image, save jpg, convert . bin im2rec, wondering utility missed documentation?"
incubator-mxnet,8391,"trying use cifar dataset train mobilenet, since origin size 32x32 resize 224x224 size mobilenet input resize them, encounter problem mxnet source dmlc core include dmlc . logging.h 308 16 20 13 include mxnet . . tensor blob.h 275 check failed shape .size shape.size 4515840000 vs. 220872704 tblob.get shape new old shape match total elements resize image, ok, check meomery use, seems memory leak happened, computer 64gb memory, program use all. parts script one help?",0,use 224x224 size train mobilenet encounter memory problem,"use 224x224 size train mobilenet encounter memory problem trying use cifar dataset train mobilenet, since origin size 32x32 resize 224x224 size mobilenet input resize them, encounter problem mxnet source dmlc core include dmlc . logging.h 308 16 20 13 include mxnet . . tensor blob.h 275 check failed shape .size shape.size 4515840000 vs. 220872704 tblob.get shape new old shape match total elements resize image, ok, check meomery use, seems memory leak happened, computer 64gb memory, program use all. parts script one help?"
incubator-mxnet,14961,http jenkins.mxnet ci.amazon ml.com blue organizations jenkins mxnet validation 2funix gpu detail pr 14959 2 pipeline. tensorrt build fails,0,ci build failures unix gpu tensorrt,ci build failures unix gpu tensorrt http jenkins.mxnet ci.amazon ml.com blue organizations jenkins mxnet validation 2funix gpu detail pr 14959 2 pipeline. tensorrt build fails
incubator-mxnet,3244,"checked latest master julia 0.4.6 latest 0.6. pkg.test runs correctly 0.6, fails 0.4.6 usr bin julia check bounds yes code coverage none color yes home colin .julia v0.4 mxnet test runtests.jl",0,"pkg.test v 0.6 succeeds, fails 0.4.6","pkg.test v 0.6 succeeds, fails 0.4.6 checked latest master julia 0.4.6 latest 0.6. pkg.test runs correctly 0.6, fails 0.4.6 usr bin julia check bounds yes code coverage none color yes home colin .julia v0.4 mxnet test runtests.jl"
incubator-mxnet,9176,"currently return dense ndarray. instead return row sparse ndarray directly. causing extra conversions checkpointing model, calls https github.com apache incubator mxnet blob master python mxnet module executor group.py l414 l416 turn calls . support sparse ndarray output two operators need update infer storage logic https github.com apache incubator mxnet blob 05047ad8fee4e8ee63ae2b7f96e7e9c7684fa4a0 src operator tensor elemwise binary scalar op basic.cc l61 l66 also also return sparse result.",0,row sparse ndarray 0 return dense ndarray,"row sparse ndarray 0 return dense ndarray currently return dense ndarray. instead return row sparse ndarray directly. causing extra conversions checkpointing model, calls https github.com apache incubator mxnet blob master python mxnet module executor group.py l414 l416 turn calls . support sparse ndarray output two operators need update infer storage logic https github.com apache incubator mxnet blob 05047ad8fee4e8ee63ae2b7f96e7e9c7684fa4a0 src operator tensor elemwise binary scalar op basic.cc l61 l66 also also return sparse result."
incubator-mxnet,10648,"description code link https github.com apache incubator mxnet tree master example sparse wide deep examlpe wide deep runs faster win10 cpu linux gpu tesla m40 24gb environment info required win10 python info version 3.5.2 compiler msc v.1900 64 bit amd64 build 'default', 'jul 5 2016 11 41 13' arch '64bit', 'windowspe' pip info version 10.0.1 mxnet info version 1.1.0 hashtag found. installed pre built package. system info platform windows 10 10.0.15063 sp0 system windows node minhozhou pc0 release 10 version 10.0.15063 hardware info machine amd64 processor intel64 family 6 model 158 stepping 9, genuineintel name intel r core tm i5 7500 cpu 3.40ghz linux python info 'version ', '2.7.5' 'compiler ', 'gcc 4.8.5 20150623 red hat 4.8.5 4 ' 'build ', 'default', 'nov 20 2015 02 00 19' 'arch ', '64bit', 'elf' pip info 'version ', '9.0.1' system info 'platform ', 'linux 3.10.104 1 tlinux2 0041.tl2 x86 64 centos 7.2 final' 'system ', 'linux' 'release ', '3.10.104 1 tlinux2 0041.tl2' 'version ', ' 1 smp fri oct 28 20 58 27 cst 2016' hardware info 'machine ', 'x86 64' 'processor ', 'x86 64' architecture x86 64 cpu op mode 32 bit, 64 bit byte order little endian cpu 56 line cpu list 0 55 thread per core 2 core per socket 14 socket 2 numa node 2 vendor id genuineintel cpu family 6 model 79 model name intel r xeon r cpu e5 2680 v4 2.40ghz stepping 1 cpu mhz 2401.000 bogomips 4801.66 virtualization vt x l1d cache 32k l1i cache 32k l2 cache 256k l3 cache 35840k numa node0 cpu 0 13,28 41 numa node1 cpu 14 27,42 55 log win10 2018 04 23 11 33 37,170 epoch 0, accuracy 0.7957062007874016 2018 04 23 11 33 37,173 saved checkpoint checkpoint 0000.params 2018 04 23 11 33 37,175 saved optimizer state checkpoint 0000.states 2018 04 23 11 33 37,276 epoch 1 batch 100 speed 127659.84 samples sec accuracy 0.811797 2018 04 23 11 33 37,378 epoch 1 batch 200 speed 126396.03 samples sec accuracy 0.826719 2018 04 23 11 33 37,484 epoch 1, accuracy 0.8258489173228346 2018 04 23 11 33 37,488 saved checkpoint checkpoint 0001.params 2018 04 23 11 33 37,489 saved optimizer state checkpoint 0001.states 2018 04 23 11 33 37,593 epoch 2 batch 100 speed 125157.28 samples sec accuracy 0.826250 2018 04 23 11 33 37,696 epoch 2 batch 200 speed 123941.43 samples sec accuracy 0.829922 2018 04 23 11 33 37,806 epoch 2, accuracy 0.8415969488188977 linux 2018 04 23 11 43 50,156 epoch 1 batch 100 speed 24695.55 samples sec accuracy 0.782266 2018 04 23 11 43 50,755 epoch 1 batch 200 speed 21366.17 samples sec accuracy 0.783750 2018 04 23 11 43 51,558 epoch 1, accuracy 0.795583169291 2018 04 23 11 43 51,570 saved checkpoint checkpoint 0001.params 2018 04 23 11 43 51,574 saved optimizer state checkpoint 0001.states 2018 04 23 11 43 52,132 epoch 2 batch 100 speed 23574.39 samples sec accuracy 0.789219 2018 04 23 11 43 52,721 epoch 2 batch 200 speed 21724.27 samples sec accuracy 0.793438 2018 04 23 11 43 53,718 epoch 2, accuracy 0.799950787402 win10 127659.84 samples sec linux 23574.39 samples sec",0,examlpe wide deep runs faster win10 cpu linux gpu tesla m40 24gb,"examlpe wide deep runs faster win10 cpu linux gpu tesla m40 24gb description code link https github.com apache incubator mxnet tree master example sparse wide deep examlpe wide deep runs faster win10 cpu linux gpu tesla m40 24gb environment info required win10 python info version 3.5.2 compiler msc v.1900 64 bit amd64 build 'default', 'jul 5 2016 11 41 13' arch '64bit', 'windowspe' pip info version 10.0.1 mxnet info version 1.1.0 hashtag found. installed pre built package. system info platform windows 10 10.0.15063 sp0 system windows node minhozhou pc0 release 10 version 10.0.15063 hardware info machine amd64 processor intel64 family 6 model 158 stepping 9, genuineintel name intel r core tm i5 7500 cpu 3.40ghz linux python info 'version ', '2.7.5' 'compiler ', 'gcc 4.8.5 20150623 red hat 4.8.5 4 ' 'build ', 'default', 'nov 20 2015 02 00 19' 'arch ', '64bit', 'elf' pip info 'version ', '9.0.1' system info 'platform ', 'linux 3.10.104 1 tlinux2 0041.tl2 x86 64 centos 7.2 final' 'system ', 'linux' 'release ', '3.10.104 1 tlinux2 0041.tl2' 'version ', ' 1 smp fri oct 28 20 58 27 cst 2016' hardware info 'machine ', 'x86 64' 'processor ', 'x86 64' architecture x86 64 cpu op mode 32 bit, 64 bit byte order little endian cpu 56 line cpu list 0 55 thread per core 2 core per socket 14 socket 2 numa node 2 vendor id genuineintel cpu family 6 model 79 model name intel r xeon r cpu e5 2680 v4 2.40ghz stepping 1 cpu mhz 2401.000 bogomips 4801.66 virtualization vt x l1d cache 32k l1i cache 32k l2 cache 256k l3 cache 35840k numa node0 cpu 0 13,28 41 numa node1 cpu 14 27,42 55 log win10 2018 04 23 11 33 37,170 epoch 0, accuracy 0.7957062007874016 2018 04 23 11 33 37,173 saved checkpoint checkpoint 0000.params 2018 04 23 11 33 37,175 saved optimizer state checkpoint 0000.states 2018 04 23 11 33 37,276 epoch 1 batch 100 speed 127659.84 samples sec accuracy 0.811797 2018 04 23 11 33 37,378 epoch 1 batch 200 speed 126396.03 samples sec accuracy 0.826719 2018 04 23 11 33 37,484 epoch 1, accuracy 0.8258489173228346 2018 04 23 11 33 37,488 saved checkpoint checkpoint 0001.params 2018 04 23 11 33 37,489 saved optimizer state checkpoint 0001.states 2018 04 23 11 33 37,593 epoch 2 batch 100 speed 125157.28 samples sec accuracy 0.826250 2018 04 23 11 33 37,696 epoch 2 batch 200 speed 123941.43 samples sec accuracy 0.829922 2018 04 23 11 33 37,806 epoch 2, accuracy 0.8415969488188977 linux 2018 04 23 11 43 50,156 epoch 1 batch 100 speed 24695.55 samples sec accuracy 0.782266 2018 04 23 11 43 50,755 epoch 1 batch 200 speed 21366.17 samples sec accuracy 0.783750 2018 04 23 11 43 51,558 epoch 1, accuracy 0.795583169291 2018 04 23 11 43 51,570 saved checkpoint checkpoint 0001.params 2018 04 23 11 43 51,574 saved optimizer state checkpoint 0001.states 2018 04 23 11 43 52,132 epoch 2 batch 100 speed 23574.39 samples sec accuracy 0.789219 2018 04 23 11 43 52,721 epoch 2 batch 200 speed 21724.27 samples sec accuracy 0.793438 2018 04 23 11 43 53,718 epoch 2, accuracy 0.799950787402 win10 127659.84 samples sec linux 23574.39 samples sec"
incubator-mxnet,12869,"hi, model trained python want use c getting 512d float features. c gives different output try image classification predict.cpp want get data code 512d float vector python output ? model https pan.baidu.com 1mj6x7mk https www.dropbox.com ou8v3c307vyzawc model r50 arcface ms1m refine v1.zip?dl 0 isnt clear method c feature extraction ? best",0,python trained model gives different features set c,"python trained model gives different features set c hi, model trained python want use c getting 512d float features. c gives different output try image classification predict.cpp want get data code 512d float vector python output ? model https pan.baidu.com 1mj6x7mk https www.dropbox.com ou8v3c307vyzawc model r50 arcface ms1m refine v1.zip?dl 0 isnt clear method c feature extraction ? best"
incubator-mxnet,9703,"description brief description problem 2 sentences. can't run mx.nd.smooth l1 environment info required python info version 3.5.2 compiler msc v.1900 64 bit amd64 build 'v3.5.2 4def2a2901a5', 'jun 25 2016 22 18 55' arch '64bit', 'windowspe' pip info version 9.0.1 directory c users 67009 appdata local programs python python35 lib site packages pip mxnet info version 1.0.0 directory c users 67009 appdata local programs python python35 lib site packages mxnet hashtag found. installed pre built package. system info platform windows 10 10.0.16299 sp0 system windows node desktop nracbb8 release 10 version 10.0.16299 hardware info machine amd64 processor intel64 family 6 model 60 stepping 3, genuineintel name intel r core tm i7 4710hq cpu 2.50ghz network test setting timeout 10 timing pypi https pypi.python.org pypi pip, dns 0.0393 sec, load 1.9675 sec. timing conda https repo.continuum.io pkgs free , dns 0.0338 sec, load 1.6308 sec. timing gluon tutorial en http gluon.mxnet.io, dns 0.0321 sec, load 0.4048 sec. timing fashionmnist https apache mxnet.s3 accelerate.dualstack.amazonaws.com gluon dataset fashion mnist train labels idx1 ubyte.gz, dns 0.2478 sec, load 1.1301 sec. timing gluon tutorial cn https zh.gluon.ai, dns 0.0341 sec, load 1.5128 sec. timing mxnet https github.com apache incubator mxnet, dns 0.0072 sec, load 1.3811 sec. build info required built source n error message traceback recent call last file , line 1, b mx.nd.smooth l1 file , line 48, smooth l1 file c users 67009 appdata local programs python python35 lib site packages mxnet ctypes ndarray.py , line 92, imperative invoke ctypes.byref stypes oserror winerror 529697949 windows error 0xe06d7363 steps reproduce",0,can't run mx.nd.smooth l1,"can't run mx.nd.smooth l1 description brief description problem 2 sentences. can't run mx.nd.smooth l1 environment info required python info version 3.5.2 compiler msc v.1900 64 bit amd64 build 'v3.5.2 4def2a2901a5', 'jun 25 2016 22 18 55' arch '64bit', 'windowspe' pip info version 9.0.1 directory c users 67009 appdata local programs python python35 lib site packages pip mxnet info version 1.0.0 directory c users 67009 appdata local programs python python35 lib site packages mxnet hashtag found. installed pre built package. system info platform windows 10 10.0.16299 sp0 system windows node desktop nracbb8 release 10 version 10.0.16299 hardware info machine amd64 processor intel64 family 6 model 60 stepping 3, genuineintel name intel r core tm i7 4710hq cpu 2.50ghz network test setting timeout 10 timing pypi https pypi.python.org pypi pip, dns 0.0393 sec, load 1.9675 sec. timing conda https repo.continuum.io pkgs free , dns 0.0338 sec, load 1.6308 sec. timing gluon tutorial en http gluon.mxnet.io, dns 0.0321 sec, load 0.4048 sec. timing fashionmnist https apache mxnet.s3 accelerate.dualstack.amazonaws.com gluon dataset fashion mnist train labels idx1 ubyte.gz, dns 0.2478 sec, load 1.1301 sec. timing gluon tutorial cn https zh.gluon.ai, dns 0.0341 sec, load 1.5128 sec. timing mxnet https github.com apache incubator mxnet, dns 0.0072 sec, load 1.3811 sec. build info required built source n error message traceback recent call last file , line 1, b mx.nd.smooth l1 file , line 48, smooth l1 file c users 67009 appdata local programs python python35 lib site packages mxnet ctypes ndarray.py , line 92, imperative invoke ctypes.byref stypes oserror winerror 529697949 windows error 0xe06d7363 steps reproduce"
incubator-mxnet,861,"hi, wonder fix parameter layer training, like different learning rate different layer make zero. thank",0,fix layer training big network,"fix layer training big network hi, wonder fix parameter layer training, like different learning rate different layer make zero. thank"
incubator-mxnet,3062,"want implement cnn text failed get evaluation metric batch end callback like exactly top k, accuracy here's network use dataiter shape netowrk's makes confused evaluation metrics show up. suppose even though provide .",0,batch end callback show eval metric,"batch end callback show eval metric want implement cnn text failed get evaluation metric batch end callback like exactly top k, accuracy here's network use dataiter shape netowrk's makes confused evaluation metrics show up. suppose even though provide ."
incubator-mxnet,8531,"suppose two feature maps f1 f2 output network. want compute convolution f1 f2. assume f1 shape 1, c, 10, 10 f2 shape 1, c, 3, 3 wanted result shape 1, 1, 8, 8 pad 0, stride 1 dilate 1. implement using mxnet? come one possible way uses mx.sym.correlation, cannot get idea correlation operator computes reading doc. or, set weight mx.sym.convolution layer f2, data f1? would interfere propagation grads training?",0,get correlation result two feature maps?,"get correlation result two feature maps? suppose two feature maps f1 f2 output network. want compute convolution f1 f2. assume f1 shape 1, c, 10, 10 f2 shape 1, c, 3, 3 wanted result shape 1, 1, 8, 8 pad 0, stride 1 dilate 1. implement using mxnet? come one possible way uses mx.sym.correlation, cannot get idea correlation operator computes reading doc. or, set weight mx.sym.convolution layer f2, data f1? would interfere propagation grads training?"
incubator-mxnet,12539,http jenkins.mxnet ci.amazon ml.com blue organizations jenkins nightlytests onbinaries detail nightlytests onbinaries 148 pipeline,0,failing nighty test test pixel2pixel test notebooks single gpu.straightdopesinglegputests,failing nighty test test pixel2pixel test notebooks single gpu.straightdopesinglegputests http jenkins.mxnet ci.amazon ml.com blue organizations jenkins nightlytests onbinaries detail nightlytests onbinaries 148 pipeline
incubator-mxnet,8902,"throw exception storage fallback currently storage fallback happens, log message printed console, hard user figure fallback happens code even use naiveengine debugging. throw exception storage fallback happens help user debug it. https github.com apache incubator mxnet blob master src operator operator common.h l526 missing storage fallback message exception ndarray copy cast storage may happen source destination ndarray different storage types. currently converted silence user aware. sparse vector shape 10 https github.com apache incubator mxnet issues 8817 still discussion. either fix least let vm crash. distributed training tutorial sparse maybe kvstore device done. testing https github.com apache incubator mxnet issues 8709 https github.com apache incubator mxnet issues 8542 8980",0,todo list usability improvement sparse tensor,"todo list usability improvement sparse tensor throw exception storage fallback currently storage fallback happens, log message printed console, hard user figure fallback happens code even use naiveengine debugging. throw exception storage fallback happens help user debug it. https github.com apache incubator mxnet blob master src operator operator common.h l526 missing storage fallback message exception ndarray copy cast storage may happen source destination ndarray different storage types. currently converted silence user aware. sparse vector shape 10 https github.com apache incubator mxnet issues 8817 still discussion. either fix least let vm crash. distributed training tutorial sparse maybe kvstore device done. testing https github.com apache incubator mxnet issues 8709 https github.com apache incubator mxnet issues 8542 8980"
incubator-mxnet,6181,"implementing tree lstm's predicting negativity positivity sentences mxnet using bucketingmodule https github.com dmlc mxnet blob master docs bucketing.md this. tree lstm structured similar structure parse tree sentence. therefore, training sample need different structure. using one bucket per input sample. also, lstm blocks single structure shared weights. example, tree consisting n nodes, k nodes share weights n k share weights. partition limited two. fact, often case blocks share weights others. first problem training. train using 'sgd', 'momentum' 0.0, training works fine. however, change momentum try use optimizers 'adam', training returns inconsistency shape error due self.states dictionary updater class optimizer.py https github.com dmlc mxnet blob master python mxnet optimizer.py . initializes self.states dictionary first sentence. second sentence fed, index states necessarily first sentence. updater uses already created states first sentence fails look time second equation. second question mini batch training setting? code https github.com forougha neuraltrig available github. main file run neuraltrig.py. note order run code, 'allow extra params' flag needs sat true exec .copy params arg params, aux params, allow extra params true function set params self, arg params, aux params file executor group.py https github.com dmlc mxnet blob master python mxnet module executor group.py line 332. environment info operating system mac os x el capitan compiler clang 800.0.38 package used python r scala julia python mxnet version installed source mxnet commit hash 58e334639c4d5a875bb5b8b33036c3fab8ed7115 using python package, please provide python version distribution python 2.7.13 anaconda 4.3.1 x86 64 using r package, please provide r error message 08 11 32 users forough mxnet dmlc core include dmlc logging.h 300 08 11 32 src operator nn .. tensor .. elemwise op common.h 31 check failed assign dattr, vec incompatible attr node 2 th input expected 1,120 , got 120,240 stack trace returned 9 entries bt 0 0 libmxnet.so 0x00000001074d5f35 zn4dmlc15logmessagefatald2ev 37 bt 1 1 libmxnet.so 0x00000001074d3539 zn4dmlc15logmessagefatald1ev 9 bt 2 2 libmxnet.so 0x000000010754de72 zzn5mxnet2op12elemwiseattrin4nnvm6tshapeexadl zns0 13shape noneerks3 eexadl zns0 12shape assigneps3 s5 eelb1eeebrkns2 9nodeattrsepnst3 16vectorit nsa 9allocatorisc eeeesg rksc enkulpnsb is3 nsd is3 eeeepkce clesl sn 498 bt 3 3 libmxnet.so 0x000000010754db26 zn5mxnet2op12elemwiseattrin4nnvm6tshapeexadl zns0 13shape noneerks3 eexadl zns0 12shape assigneps3 s5 eelb1eeebrkns2 9nodeattrsepnst3 16vectorit nsa 9allocatorisc eeeesg rksc 198 bt 4 4 libmxnet.so 0x0000000107bf7e92 zn5mxnet2op13elemwiseshapeili3eli1eeebrkn4nnvm9nodeattrsepnst3 16vectorins2 6tshapeens6 9allocatoris8 eeeesc 226 bt 5 5 libmxnet.so 0x0000000107a11b7e z12setshapetypepkn4nnvm2operkns 9nodeattrserkn5mxnet7contexterknst3 16vectorins6 7ndarrayensa 9allocatorisc eeeerkipsf 1326 bt 6 6 libmxnet.so 0x0000000107a156b5 mximperativeinvoke 1093 bt 7 7 ctypes.so 0x0000000101e59f57 ffi call unix64 79 bt 8 8 ??? 0x00007fff5e40bc90 0x0 140734774688912 traceback recent call last file neuraltrig.py , line 214, main file neuraltrig.py , line 184, main epoch end callback mx.rnn.do rnn checkpoint cell, 'trainedmodel', 1 file users forough anaconda2 lib python2.7 site packages mxnet 0.9.5 py2.7.egg mxnet module base module.py , line 475, fit self.update file users forough anaconda2 lib python2.7 site packages mxnet 0.9.5 py2.7.egg mxnet module bucketing module.py , line 401, update self. curr module.update file users forough anaconda2 lib python2.7 site packages mxnet 0.9.5 py2.7.egg mxnet module module.py , line 570, update kvstore self. kvstore file users forough anaconda2 lib python2.7 site packages mxnet 0.9.5 py2.7.egg mxnet model.py , line 123, update params updater index num device k, g, w file users forough anaconda2 lib python2.7 site packages mxnet 0.9.5 py2.7.egg mxnet optimizer.py , line 690, call self.optimizer.update index, weight, grad, self.states index file users forough anaconda2 lib python2.7 site packages mxnet 0.9.5 py2.7.egg mxnet optimizer.py , line 324, update lr lr, wd wd, self.kwargs file users forough anaconda2 lib python2.7 site packages mxnet 0.9.5 py2.7.egg mxnet ctypes ndarray.py , line 133, generic ndarray function c array ctypes.c char p, c str str kwargs.values file users forough anaconda2 lib python2.7 site packages mxnet 0.9.5 py2.7.egg mxnet base.py , line 78, check call raise mxneterror py str lib.mxgetlasterror mxnet.base.mxneterror 08 11 32 src operator nn .. tensor .. elemwise op common.h 31 check failed assign dattr, vec incompatible attr node 2 th input expected 1,120 , got 120,240 stack trace returned 9 entries bt 0 0 libmxnet.so 0x00000001074d5f35 zn4dmlc15logmessagefatald2ev 37 bt 1 1 libmxnet.so 0x00000001074d3539 zn4dmlc15logmessagefatald1ev 9 bt 2 2 libmxnet.so 0x000000010754de72 zzn5mxnet2op12elemwiseattrin4nnvm6tshapeexadl zns0 13shape noneerks3 eexadl zns0 12shape assigneps3 s5 eelb1eeebrkns2 9nodeattrsepnst3 16vectorit nsa 9allocatorisc eeeesg rksc enkulpnsb is3 nsd is3 eeeepkce clesl sn 498 bt 3 3 libmxnet.so 0x000000010754db26 zn5mxnet2op12elemwiseattrin4nnvm6tshapeexadl zns0 13shape noneerks3 eexadl zns0 12shape assigneps3 s5 eelb1eeebrkns2 9nodeattrsepnst3 16vectorit nsa 9allocatorisc eeeesg rksc 198 bt 4 4 libmxnet.so 0x0000000107bf7e92 zn5mxnet2op13elemwiseshapeili3eli1eeebrkn4nnvm9nodeattrsepnst3 16vectorins2 6tshapeens6 9allocatoris8 eeeesc 226 bt 5 5 libmxnet.so 0x0000000107a11b7e z12setshapetypepkn4nnvm2operkns 9nodeattrserkn5mxnet7contexterknst3 16vectorins6 7ndarrayensa 9allocatorisc eeeerkipsf 1326 bt 6 6 libmxnet.so 0x0000000107a156b5 mximperativeinvoke 1093 bt 7 7 ctypes.so 0x0000000101e59f57 ffi call unix64 79 bt 8 8 ??? 0x00007fff5e40bc90 0x0 140734774688912 minimum reproducible example code https github.com forougha neuraltrig git. main file run neuraltrig.py. steps reproduce running standard examples, please provide commands run lead error. code https github.com forougha neuraltrig git. main file run neuraltrig.py. set momentum something zero fails. use another optimzier also fails. note order run code, 'allow extra params' flag needs sat true exec .copy params arg params, aux params, allow extra params true function set params self, arg params, aux params file executor group.py https github.com dmlc mxnet blob master python mxnet module executor group.py line 332.",0,problem implementing tree lstm's mxnet,"problem implementing tree lstm's mxnet implementing tree lstm's predicting negativity positivity sentences mxnet using bucketingmodule https github.com dmlc mxnet blob master docs bucketing.md this. tree lstm structured similar structure parse tree sentence. therefore, training sample need different structure. using one bucket per input sample. also, lstm blocks single structure shared weights. example, tree consisting n nodes, k nodes share weights n k share weights. partition limited two. fact, often case blocks share weights others. first problem training. train using 'sgd', 'momentum' 0.0, training works fine. however, change momentum try use optimizers 'adam', training returns inconsistency shape error due self.states dictionary updater class optimizer.py https github.com dmlc mxnet blob master python mxnet optimizer.py . initializes self.states dictionary first sentence. second sentence fed, index states necessarily first sentence. updater uses already created states first sentence fails look time second equation. second question mini batch training setting? code https github.com forougha neuraltrig available github. main file run neuraltrig.py. note order run code, 'allow extra params' flag needs sat true exec .copy params arg params, aux params, allow extra params true function set params self, arg params, aux params file executor group.py https github.com dmlc mxnet blob master python mxnet module executor group.py line 332. environment info operating system mac os x el capitan compiler clang 800.0.38 package used python r scala julia python mxnet version installed source mxnet commit hash 58e334639c4d5a875bb5b8b33036c3fab8ed7115 using python package, please provide python version distribution python 2.7.13 anaconda 4.3.1 x86 64 using r package, please provide r error message 08 11 32 users forough mxnet dmlc core include dmlc logging.h 300 08 11 32 src operator nn .. tensor .. elemwise op common.h 31 check failed assign dattr, vec incompatible attr node 2 th input expected 1,120 , got 120,240 stack trace returned 9 entries bt 0 0 libmxnet.so 0x00000001074d5f35 zn4dmlc15logmessagefatald2ev 37 bt 1 1 libmxnet.so 0x00000001074d3539 zn4dmlc15logmessagefatald1ev 9 bt 2 2 libmxnet.so 0x000000010754de72 zzn5mxnet2op12elemwiseattrin4nnvm6tshapeexadl zns0 13shape noneerks3 eexadl zns0 12shape assigneps3 s5 eelb1eeebrkns2 9nodeattrsepnst3 16vectorit nsa 9allocatorisc eeeesg rksc enkulpnsb is3 nsd is3 eeeepkce clesl sn 498 bt 3 3 libmxnet.so 0x000000010754db26 zn5mxnet2op12elemwiseattrin4nnvm6tshapeexadl zns0 13shape noneerks3 eexadl zns0 12shape assigneps3 s5 eelb1eeebrkns2 9nodeattrsepnst3 16vectorit nsa 9allocatorisc eeeesg rksc 198 bt 4 4 libmxnet.so 0x0000000107bf7e92 zn5mxnet2op13elemwiseshapeili3eli1eeebrkn4nnvm9nodeattrsepnst3 16vectorins2 6tshapeens6 9allocatoris8 eeeesc 226 bt 5 5 libmxnet.so 0x0000000107a11b7e z12setshapetypepkn4nnvm2operkns 9nodeattrserkn5mxnet7contexterknst3 16vectorins6 7ndarrayensa 9allocatorisc eeeerkipsf 1326 bt 6 6 libmxnet.so 0x0000000107a156b5 mximperativeinvoke 1093 bt 7 7 ctypes.so 0x0000000101e59f57 ffi call unix64 79 bt 8 8 ??? 0x00007fff5e40bc90 0x0 140734774688912 traceback recent call last file neuraltrig.py , line 214, main file neuraltrig.py , line 184, main epoch end callback mx.rnn.do rnn checkpoint cell, 'trainedmodel', 1 file users forough anaconda2 lib python2.7 site packages mxnet 0.9.5 py2.7.egg mxnet module base module.py , line 475, fit self.update file users forough anaconda2 lib python2.7 site packages mxnet 0.9.5 py2.7.egg mxnet module bucketing module.py , line 401, update self. curr module.update file users forough anaconda2 lib python2.7 site packages mxnet 0.9.5 py2.7.egg mxnet module module.py , line 570, update kvstore self. kvstore file users forough anaconda2 lib python2.7 site packages mxnet 0.9.5 py2.7.egg mxnet model.py , line 123, update params updater index num device k, g, w file users forough anaconda2 lib python2.7 site packages mxnet 0.9.5 py2.7.egg mxnet optimizer.py , line 690, call self.optimizer.update index, weight, grad, self.states index file users forough anaconda2 lib python2.7 site packages mxnet 0.9.5 py2.7.egg mxnet optimizer.py , line 324, update lr lr, wd wd, self.kwargs file users forough anaconda2 lib python2.7 site packages mxnet 0.9.5 py2.7.egg mxnet ctypes ndarray.py , line 133, generic ndarray function c array ctypes.c char p, c str str kwargs.values file users forough anaconda2 lib python2.7 site packages mxnet 0.9.5 py2.7.egg mxnet base.py , line 78, check call raise mxneterror py str lib.mxgetlasterror mxnet.base.mxneterror 08 11 32 src operator nn .. tensor .. elemwise op common.h 31 check failed assign dattr, vec incompatible attr node 2 th input expected 1,120 , got 120,240 stack trace returned 9 entries bt 0 0 libmxnet.so 0x00000001074d5f35 zn4dmlc15logmessagefatald2ev 37 bt 1 1 libmxnet.so 0x00000001074d3539 zn4dmlc15logmessagefatald1ev 9 bt 2 2 libmxnet.so 0x000000010754de72 zzn5mxnet2op12elemwiseattrin4nnvm6tshapeexadl zns0 13shape noneerks3 eexadl zns0 12shape assigneps3 s5 eelb1eeebrkns2 9nodeattrsepnst3 16vectorit nsa 9allocatorisc eeeesg rksc enkulpnsb is3 nsd is3 eeeepkce clesl sn 498 bt 3 3 libmxnet.so 0x000000010754db26 zn5mxnet2op12elemwiseattrin4nnvm6tshapeexadl zns0 13shape noneerks3 eexadl zns0 12shape assigneps3 s5 eelb1eeebrkns2 9nodeattrsepnst3 16vectorit nsa 9allocatorisc eeeesg rksc 198 bt 4 4 libmxnet.so 0x0000000107bf7e92 zn5mxnet2op13elemwiseshapeili3eli1eeebrkn4nnvm9nodeattrsepnst3 16vectorins2 6tshapeens6 9allocatoris8 eeeesc 226 bt 5 5 libmxnet.so 0x0000000107a11b7e z12setshapetypepkn4nnvm2operkns 9nodeattrserkn5mxnet7contexterknst3 16vectorins6 7ndarrayensa 9allocatorisc eeeerkipsf 1326 bt 6 6 libmxnet.so 0x0000000107a156b5 mximperativeinvoke 1093 bt 7 7 ctypes.so 0x0000000101e59f57 ffi call unix64 79 bt 8 8 ??? 0x00007fff5e40bc90 0x0 140734774688912 minimum reproducible example code https github.com forougha neuraltrig git. main file run neuraltrig.py. steps reproduce running standard examples, please provide commands run lead error. code https github.com forougha neuraltrig git. main file run neuraltrig.py. set momentum something zero fails. use another optimzier also fails. note order run code, 'allow extra params' flag needs sat true exec .copy params arg params, aux params, allow extra params true function set params self, arg params, aux params file executor group.py https github.com dmlc mxnet blob master python mxnet module executor group.py line 332."
incubator-mxnet,3353,"ran example image classification train mnist.py batch size 10000, would 6 batches forword training val set none forword function first fully connect layer called 6 times. added debug codes follows ! p hayw f 0 11 u3j https cloud.githubusercontent.com assets 21171292 18749082 de8e6ada 8107 11e6 9df1 3f52a0650dcc.png ! o466yu tq yrezt3oy https cloud.githubusercontent.com assets 21171292 18749092 e5510e18 8107 11e6 9b6c 40553ccf2522.png log printed supposed ! smuqtw sdk ou 6 ji https cloud.githubusercontent.com assets 21171292 18749230 6e527bfc 8108 11e6 9e58 a54226372fd3.png logger.info load123 python executed 6 times supposed, log info forward123 fully connected inl.h ? piiswrong",0,log info working supposed mxnet c code ?,"log info working supposed mxnet c code ? ran example image classification train mnist.py batch size 10000, would 6 batches forword training val set none forword function first fully connect layer called 6 times. added debug codes follows ! p hayw f 0 11 u3j https cloud.githubusercontent.com assets 21171292 18749082 de8e6ada 8107 11e6 9df1 3f52a0650dcc.png ! o466yu tq yrezt3oy https cloud.githubusercontent.com assets 21171292 18749092 e5510e18 8107 11e6 9b6c 40553ccf2522.png log printed supposed ! smuqtw sdk ou 6 ji https cloud.githubusercontent.com assets 21171292 18749230 6e527bfc 8108 11e6 9e58 a54226372fd3.png logger.info load123 python executed 6 times supposed, log info forward123 fully connected inl.h ? piiswrong"
incubator-mxnet,5943,"bugs installation issues, please provide following information. information provide, likely people able help you. environment info operating system ubuntu 16.0.4 compiler g package used python r scala julia python mxnet version 0.9.3 installed source mxnet commit hash 0.9.3 release using python package, please provide python 3.5 python version distribution python 3.5 using r package, please provide r error message please paste full error message, including stack trace. mx mxnet 0.9.3 example image classification python train mnist.py network mlp gpus 0,1 info root epoch 0 batch 100 speed 189.20 samples sec train accuracy 0.100402 mx mxnet 0.9.3 example image classification python train mnist.py network mlp gpus 0 info root epoch 0 batch 300 speed 81716.01 samples sec train accuracy 0.928594 minimum reproducible example using code, please provide short script reproduces error. steps reproduce running standard examples, please provide commands run lead error. 1. python train mnist.py network mlp 2. python train mnist.py network mlp gpus 0 3. python train mnist.py network mlp gpus 0,1 tried solve it? 1. 2. 3.",0,multi gpu training speed low,"multi gpu training speed low bugs installation issues, please provide following information. information provide, likely people able help you. environment info operating system ubuntu 16.0.4 compiler g package used python r scala julia python mxnet version 0.9.3 installed source mxnet commit hash 0.9.3 release using python package, please provide python 3.5 python version distribution python 3.5 using r package, please provide r error message please paste full error message, including stack trace. mx mxnet 0.9.3 example image classification python train mnist.py network mlp gpus 0,1 info root epoch 0 batch 100 speed 189.20 samples sec train accuracy 0.100402 mx mxnet 0.9.3 example image classification python train mnist.py network mlp gpus 0 info root epoch 0 batch 300 speed 81716.01 samples sec train accuracy 0.928594 minimum reproducible example using code, please provide short script reproduces error. steps reproduce running standard examples, please provide commands run lead error. 1. python train mnist.py network mlp 2. python train mnist.py network mlp gpus 0 3. python train mnist.py network mlp gpus 0,1 tried solve it? 1. 2. 3."
incubator-mxnet,2657,thanks.,0,edit write mxnet model json file converter code?,edit write mxnet model json file converter code? thanks.
incubator-mxnet,2539,"hello,everyone.i mxnet python beginer.so use mxnet ,i encounter errors confuse much. wanna use mxnet image's super resolution.so set dataiter object,i refer data.py fcn xs example. use gray images,so modify codes. set batch size 1 changed codes follows ''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''' def read img self, img name, label name img image.open os.path.join self.root dir, img name label image.open os.path.join self.root dir, label name img np.array img, dtype np.float32 h, w, c label np.array label h, w img img self.mean img np.expand dims img, axis 0 1, c, h, w img np.expand dims img, axis 0 label np.array label h, w label np.expand dims label, axis 0 1, h, w '''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''' ''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''' def next self return one dict contains data label self.iter next self.data, self.label self. read return self.data name self.data 0 1 , self.label name self.label 0 1 change numpy nd mydata mx.nd.array self.data 0 1 ,self.ctx mylabel mx.nd.array self.label 0 1 ,self.ctx return mx.io.databatch data mydata,label mylabel else raise stopiteration ''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''' create dataiter object ,it right.but execute following command ,i get errors ''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''' model mx.model.feedforward.create mynet,x train data,num epoch 10,learning rate 0.01 traceback recent call last file , line 1, file opt mxnet python mxnet model.py , line 901, create eval batch end callback eval batch end callback file opt mxnet python mxnet model.py , line 784, fit sym gen self.sym gen file opt mxnet python mxnet model.py , line 222, train multi device executor manager.load data batch data batch file opt mxnet python mxnet executor manager.py , line 394, load data batch self.curr execgrp.load data batch data batch file opt mxnet python mxnet executor manager.py , line 245, load data batch load data data batch, self.data arrays file opt mxnet python mxnet executor manager.py , line 87, load data load general batch.data, targets file opt mxnet python mxnet executor manager.py , line 78, load general src, targets zip data, targets file opt mxnet python mxnet ndarray.py , line 212, getitem raise valueerror 'ndarray support continuous slicing axis 0' valueerror ndarray support continuous slicing axis 0 '''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''' idea error.i hope expert give advice.thank much",0,valueerror ndarray support continous slicing axis 0 happened,"valueerror ndarray support continous slicing axis 0 happened hello,everyone.i mxnet python beginer.so use mxnet ,i encounter errors confuse much. wanna use mxnet image's super resolution.so set dataiter object,i refer data.py fcn xs example. use gray images,so modify codes. set batch size 1 changed codes follows ''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''' def read img self, img name, label name img image.open os.path.join self.root dir, img name label image.open os.path.join self.root dir, label name img np.array img, dtype np.float32 h, w, c label np.array label h, w img img self.mean img np.expand dims img, axis 0 1, c, h, w img np.expand dims img, axis 0 label np.array label h, w label np.expand dims label, axis 0 1, h, w '''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''' ''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''' def next self return one dict contains data label self.iter next self.data, self.label self. read return self.data name self.data 0 1 , self.label name self.label 0 1 change numpy nd mydata mx.nd.array self.data 0 1 ,self.ctx mylabel mx.nd.array self.label 0 1 ,self.ctx return mx.io.databatch data mydata,label mylabel else raise stopiteration ''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''' create dataiter object ,it right.but execute following command ,i get errors ''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''' model mx.model.feedforward.create mynet,x train data,num epoch 10,learning rate 0.01 traceback recent call last file , line 1, file opt mxnet python mxnet model.py , line 901, create eval batch end callback eval batch end callback file opt mxnet python mxnet model.py , line 784, fit sym gen self.sym gen file opt mxnet python mxnet model.py , line 222, train multi device executor manager.load data batch data batch file opt mxnet python mxnet executor manager.py , line 394, load data batch self.curr execgrp.load data batch data batch file opt mxnet python mxnet executor manager.py , line 245, load data batch load data data batch, self.data arrays file opt mxnet python mxnet executor manager.py , line 87, load data load general batch.data, targets file opt mxnet python mxnet executor manager.py , line 78, load general src, targets zip data, targets file opt mxnet python mxnet ndarray.py , line 212, getitem raise valueerror 'ndarray support continuous slicing axis 0' valueerror ndarray support continuous slicing axis 0 '''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''' idea error.i hope expert give advice.thank much"
incubator-mxnet,1563,"wonder added, joins list symbols symbol. feature important want build complicated end end symbol graph.",0,mx.symbol.joinlist needed?,"mx.symbol.joinlist needed? wonder added, joins list symbols symbol. feature important want build complicated end end symbol graph."
incubator-mxnet,11535,"note providing complete information concise form best way get help. issue template serves checklist essential information technical issues bug reports. non technical issues feature requests, feel free present information believe best form. q discussion, please start discussion thread https discuss.mxnet.io description ubuntu16.04 gpu pip, follows instruction official documentation web page https mxnet.incubator.apache.org install index.html?platform linux language python processor gpu . follow instructions, install mxnet cu92 cannot run example code correctly. environment info required package used python2.7.12 build info first download intalled cuda 9.2 cudnn7.1 nvidia website. error message see next section full info minimum reproducible example use official website's example code interpret envirionment. code output steps reproduce install ubuntu16.04 install cuda9.2 install cudnn7.1 use python 2.7.12 open terminal type tried solve it? note build official caffe installed cuda9.2 cudnn7.1 https github.com bvlc caffe . gives cudnn warnings complete compilation.",0,installed mxnet cu92 ubuntu can't run example code correctly,"installed mxnet cu92 ubuntu can't run example code correctly note providing complete information concise form best way get help. issue template serves checklist essential information technical issues bug reports. non technical issues feature requests, feel free present information believe best form. q discussion, please start discussion thread https discuss.mxnet.io description ubuntu16.04 gpu pip, follows instruction official documentation web page https mxnet.incubator.apache.org install index.html?platform linux language python processor gpu . follow instructions, install mxnet cu92 cannot run example code correctly. environment info required package used python2.7.12 build info first download intalled cuda 9.2 cudnn7.1 nvidia website. error message see next section full info minimum reproducible example use official website's example code interpret envirionment. code output steps reproduce install ubuntu16.04 install cuda9.2 install cudnn7.1 use python 2.7.12 open terminal type tried solve it? note build official caffe installed cuda9.2 cudnn7.1 https github.com bvlc caffe . gives cudnn warnings complete compilation."
incubator-mxnet,5250,"bugs installation issues, please provide following information. information provide, likely people able help you. environment info operating system ubuntu15.10 compiler gcc 9.4.x package used python r scala julia python mxnet version 0.9.4 installed source yeap mxnet commit hash be38c5b84030a63d0ab51f19737f99a75a7feb23 using python package, please provide anaconda4.2 python version distribution using r package, please provide r error message please paste full error message, including stack trace. minimum reproducible example using code, please provide short script reproduces error. also, custom dataprovider, , , , word padding 37, use bucketingmodule. steps reproduce code, run run line , raise segfault, comment works well. dataiter works well test times. think error must related update metric... however, example lstm bucketing.py runs well..i .",0,mx.metric.perplexity 1 update metric raise segement fault,"mx.metric.perplexity 1 update metric raise segement fault bugs installation issues, please provide following information. information provide, likely people able help you. environment info operating system ubuntu15.10 compiler gcc 9.4.x package used python r scala julia python mxnet version 0.9.4 installed source yeap mxnet commit hash be38c5b84030a63d0ab51f19737f99a75a7feb23 using python package, please provide anaconda4.2 python version distribution using r package, please provide r error message please paste full error message, including stack trace. minimum reproducible example using code, please provide short script reproduces error. also, custom dataprovider, , , , word padding 37, use bucketingmodule. steps reproduce code, run run line , raise segfault, comment works well. dataiter works well test times. think error must related update metric... however, example lstm bucketing.py runs well..i ."
incubator-mxnet,8510,"note providing complete information concise form best way get help. issue template serves checklist essential information technical issues. issue non technical, feel free present information believe best form. description mnist accuracy differ slightly training local gpu distributed training gpu accuracy training cpu. environment info required centos 7 mxnet latest master python diagnose.py package used python r scala julia using python! scala user, please provide 1. java version 2. maven version 3. scala runtime applicable r user, please provide r build info required built source train mnist.py data shuffle turned accuracy run same. compiler gcc clang mingw visual studio gcc 4.9 mxnet commit hash paste output here. 8592e1cd3b9f79cff740f29e599ba7788a454c54 build config paste content config.mk, build command. use distributed kvstore 1 use cudnn 1 use cuda usr local cuda error message 1 server 1 client training train mnist.py without gpu info root epoch 0 batch 100 speed 3606.78 samples sec accuracy 0.807859 info root epoch 0 batch 200 speed 3747.11 samples sec accuracy 0.895469 info root epoch 0 batch 300 speed 3982.58 samples sec accuracy 0.912656 info root epoch 0 batch 400 speed 3960.68 samples sec accuracy 0.928594 info root epoch 0 batch 500 speed 5376.55 samples sec accuracy 0.933906 info root epoch 0 batch 600 speed 4133.33 samples sec accuracy 0.943906 info root epoch 0 batch 700 speed 5935.95 samples sec accuracy 0.944688 info root epoch 0 batch 800 speed 3966.74 samples sec accuracy 0.940312 info root epoch 0 batch 900 speed 3803.05 samples sec accuracy 0.953906 info root epoch 0 train accuracy 0.966639 info root epoch 0 time cost 14.467 info root epoch 0 validation accuracy 0.943670 1 server 1 client training train mnist.py gpu info root epoch 0 batch 100 speed 13700.01 samples sec accuracy 0.807859 info root epoch 0 batch 200 speed 27909.23 samples sec accuracy 0.895469 info root epoch 0 batch 300 speed 23719.16 samples sec accuracy 0.910781 info root epoch 0 batch 400 speed 30796.60 samples sec accuracy 0.925312 info root epoch 0 batch 500 speed 26746.35 samples sec accuracy 0.933906 info root epoch 0 batch 600 speed 29120.16 samples sec accuracy 0.943906 info root epoch 0 batch 700 speed 30805.82 samples sec accuracy 0.944531 info root epoch 0 batch 800 speed 22852.49 samples sec accuracy 0.937813 info root epoch 0 batch 900 speed 28238.62 samples sec accuracy 0.952031 info root epoch 0 train accuracy 0.969172 info root epoch 0 time cost 2.452 info root epoch 0 validation accuracy 0.936505 minimum reproducible example using code, please provide short script reproduces error. otherwise, please provide link existing example. pgrep python xargs kill 9 rm train profile.json dev null pgrep memcheck amd64 xargs kill 9 dev null export dmlc ps root port 9091 export dmlc num worker 1 export dmlc num server 1 export dmlc ps root uri 127.0.0.1 export dmlc role scheduler python train mnist.py export dmlc role server export dmlc server id 0 python train mnist.py kv store dist sync gpus 0 export dmlc role worker export dmlc worker id 0 python train mnist.py kv store dist sync num epochs 1 gpus 0 steps reproduce paste commands ran produced error. 1. run example toggle gpus 0 2. observe accuracy differences. tried solve it? 1. problem seems initialization params gpu. 2. first divergence two training first gradient sent first batch.",0,minor differences distributed training gpu without gpu.,"minor differences distributed training gpu without gpu. note providing complete information concise form best way get help. issue template serves checklist essential information technical issues. issue non technical, feel free present information believe best form. description mnist accuracy differ slightly training local gpu distributed training gpu accuracy training cpu. environment info required centos 7 mxnet latest master python diagnose.py package used python r scala julia using python! scala user, please provide 1. java version 2. maven version 3. scala runtime applicable r user, please provide r build info required built source train mnist.py data shuffle turned accuracy run same. compiler gcc clang mingw visual studio gcc 4.9 mxnet commit hash paste output here. 8592e1cd3b9f79cff740f29e599ba7788a454c54 build config paste content config.mk, build command. use distributed kvstore 1 use cudnn 1 use cuda usr local cuda error message 1 server 1 client training train mnist.py without gpu info root epoch 0 batch 100 speed 3606.78 samples sec accuracy 0.807859 info root epoch 0 batch 200 speed 3747.11 samples sec accuracy 0.895469 info root epoch 0 batch 300 speed 3982.58 samples sec accuracy 0.912656 info root epoch 0 batch 400 speed 3960.68 samples sec accuracy 0.928594 info root epoch 0 batch 500 speed 5376.55 samples sec accuracy 0.933906 info root epoch 0 batch 600 speed 4133.33 samples sec accuracy 0.943906 info root epoch 0 batch 700 speed 5935.95 samples sec accuracy 0.944688 info root epoch 0 batch 800 speed 3966.74 samples sec accuracy 0.940312 info root epoch 0 batch 900 speed 3803.05 samples sec accuracy 0.953906 info root epoch 0 train accuracy 0.966639 info root epoch 0 time cost 14.467 info root epoch 0 validation accuracy 0.943670 1 server 1 client training train mnist.py gpu info root epoch 0 batch 100 speed 13700.01 samples sec accuracy 0.807859 info root epoch 0 batch 200 speed 27909.23 samples sec accuracy 0.895469 info root epoch 0 batch 300 speed 23719.16 samples sec accuracy 0.910781 info root epoch 0 batch 400 speed 30796.60 samples sec accuracy 0.925312 info root epoch 0 batch 500 speed 26746.35 samples sec accuracy 0.933906 info root epoch 0 batch 600 speed 29120.16 samples sec accuracy 0.943906 info root epoch 0 batch 700 speed 30805.82 samples sec accuracy 0.944531 info root epoch 0 batch 800 speed 22852.49 samples sec accuracy 0.937813 info root epoch 0 batch 900 speed 28238.62 samples sec accuracy 0.952031 info root epoch 0 train accuracy 0.969172 info root epoch 0 time cost 2.452 info root epoch 0 validation accuracy 0.936505 minimum reproducible example using code, please provide short script reproduces error. otherwise, please provide link existing example. pgrep python xargs kill 9 rm train profile.json dev null pgrep memcheck amd64 xargs kill 9 dev null export dmlc ps root port 9091 export dmlc num worker 1 export dmlc num server 1 export dmlc ps root uri 127.0.0.1 export dmlc role scheduler python train mnist.py export dmlc role server export dmlc server id 0 python train mnist.py kv store dist sync gpus 0 export dmlc role worker export dmlc worker id 0 python train mnist.py kv store dist sync num epochs 1 gpus 0 steps reproduce paste commands ran produced error. 1. run example toggle gpus 0 2. observe accuracy differences. tried solve it? 1. problem seems initialization params gpu. 2. first divergence two training first gradient sent first batch."
incubator-mxnet,9745,"error caused eval data.pad found https github.com apache incubator mxnet blob 8205e24d99b95cc2971006f3453a7e7addac7ffe python mxnet module base module.py l295 templates need change https mxnet.incubator.apache.org api python io.html develop new iterator quite sure example simple'', also needs include pad return https mxnet.incubator.apache.org tutorials basic data.html custom iterator thanks, yifei",0,multiiter example also return pad next self,"multiiter example also return pad next self error caused eval data.pad found https github.com apache incubator mxnet blob 8205e24d99b95cc2971006f3453a7e7addac7ffe python mxnet module base module.py l295 templates need change https mxnet.incubator.apache.org api python io.html develop new iterator quite sure example simple'', also needs include pad return https mxnet.incubator.apache.org tutorials basic data.html custom iterator thanks, yifei"
incubator-mxnet,66,naiveengine works well,0,threadengine buggy cudnn,threadengine buggy cudnn naiveengine works well
incubator-mxnet,11108,"https github.com apache incubator mxnet blob 2dbd143e4892bb9ad4aa1835c79f0046603e3531 makefile l221 refer mxnet makefile above, search paths gperftools devel jemalloc devel . gperftools devel jemalloc devel package installed via yum place dynamic library files tl,dr title said.",0,seems gperftools devel jemalloc devel detected correctly rhel redhat enterprise linux,"seems gperftools devel jemalloc devel detected correctly rhel redhat enterprise linux https github.com apache incubator mxnet blob 2dbd143e4892bb9ad4aa1835c79f0046603e3531 makefile l221 refer mxnet makefile above, search paths gperftools devel jemalloc devel . gperftools devel jemalloc devel package installed via yum place dynamic library files tl,dr title said."
incubator-mxnet,1253,"hi, change optimizer adam. example, get following error message correct way setting optimizer?",0,running adam optimizer error,"running adam optimizer error hi, change optimizer adam. example, get following error message correct way setting optimizer?"
incubator-mxnet,4301,mxnet still support 3d convolution,0,"use 'mx.symbol.convolution' 3d kernel, get problem volume convolution implmented mshadow .","use 'mx.symbol.convolution' 3d kernel, get problem volume convolution implmented mshadow . mxnet still support 3d convolution"
incubator-mxnet,16495,probably related git filters adding files. https stackoverflow.com questions 8006393 force add despite gitignore file,0,docs gluon.data. missing,docs gluon.data. missing probably related git filters adding files. https stackoverflow.com questions 8006393 force add despite gitignore file
incubator-mxnet,5233,"although mxnet documentation mention possible use c mshadow cuda create custom operations, real examples used custom cuda kernel taking gpu pointers part custom operation. anyone provide simple example flow?",0,create custom gpu operation custom cuda kernels computation,"create custom gpu operation custom cuda kernels computation although mxnet documentation mention possible use c mshadow cuda create custom operations, real examples used custom cuda kernel taking gpu pointers part custom operation. anyone provide simple example flow?"
incubator-mxnet,12526,"description mnist smokescreen tests breaking latest build mxnet 1.3.0b20180911 result pr 12285 index range error. environment info required breaks linux osx. build mxnet 1.3.0b20180909 fine, 1.3.0b20180911 faulty.",0,12285 breaks ndarrayiter 3d arrays,"12285 breaks ndarrayiter 3d arrays description mnist smokescreen tests breaking latest build mxnet 1.3.0b20180911 result pr 12285 index range error. environment info required breaks linux osx. build mxnet 1.3.0b20180909 fine, 1.3.0b20180911 faulty."
incubator-mxnet,5243,"hello, trying compile newest mxnet windows, using visual studio. constantly getting error can't fix it. cub directory included mxnet source cmake seems added additional include directories line project properties, c c . line creates problem sort op inl.cuh file ! copyright c 2017 contributors file sort op inl.cuh brief cuda implementations sort op.h ifndef mxnet operator tensor sort op inl cuh define mxnet operator tensor sort op inl cuh include include include cuda version 7000 include endif line generates error. double checked cub folder device radix sort.cuh file exist appropriate location pointed entry additional include directories. else may cause error? file included .cuh extension, belongs cuda kernel example, cause problem visual studio trying include? thanks advance",0,error c1083 cannot open include file 'cub device device radix sort.cuh' file directory,"error c1083 cannot open include file 'cub device device radix sort.cuh' file directory hello, trying compile newest mxnet windows, using visual studio. constantly getting error can't fix it. cub directory included mxnet source cmake seems added additional include directories line project properties, c c . line creates problem sort op inl.cuh file ! copyright c 2017 contributors file sort op inl.cuh brief cuda implementations sort op.h ifndef mxnet operator tensor sort op inl cuh define mxnet operator tensor sort op inl cuh include include include cuda version 7000 include endif line generates error. double checked cub folder device radix sort.cuh file exist appropriate location pointed entry additional include directories. else may cause error? file included .cuh extension, belongs cuda kernel example, cause problem visual studio trying include? thanks advance"
incubator-mxnet,9131,code crashes caused experienced mxnet 1.0.0 macos sierra python 3.6.3,0,random uniform causes vm crash,random uniform causes vm crash code crashes caused experienced mxnet 1.0.0 macos sierra python 3.6.3
incubator-mxnet,2883,notice experiments use mean img file set path mean img iterator.but can't find tool mxnet generate mean file. someone tell ?,0,generate mean file dataset used iterator.,generate mean file dataset used iterator. notice experiments use mean img file set path mean img iterator.but can't find tool mxnet generate mean file. someone tell ?
incubator-mxnet,14280,"description compiling source code occurred link libzmq error. environment info required package used python r scala julia using python. build info required built source compiler gcc clang mingw visual studio gcc mxnet commit hash 7c617ccc7a8655f3b93acdfac8aeee20eee2a778 build config cmakelists.txt, set build files error message std sp counted deleter, gnu cxx lock policy 2 dispose ' van.cc .text. znst19 sp counted deleteripczn2ps6zmqvan7recvmsgepns1 7messageeeuls0 e saiveln9 gnu cxx12 lock policye2ee10 disposeev znst19 sp counted deleteripczn2ps6zmqvan7recvmsgepns1 7messageeeuls0 e saiveln9 gnu cxx12 lock policye2ee10 disposeev 0x9 undefined reference ps zmqvan stop ' van.cc .text. zn2ps6zmqvan4stopev zn2ps6zmqvan4stopev 0xe5 undefined reference zmq close' van.cc .text. zn2ps6zmqvan4stopev zn2ps6zmqvan4stopev 0x2c2 undefined reference zmq close' van.cc .text. zn2ps6zmqvan4stopev zn2ps6zmqvan4stopev 0x55d undefined reference ps zmqvan bind ps node const , int ' van.cc .text. zn2ps6zmqvan4binderkns 4nodeei zn2ps6zmqvan4binderkns 4nodeei 0x3a undefined reference zmq bind' van.cc .text. zn2ps6zmqvan4binderkns 4nodeei zn2ps6zmqvan4binderkns 4nodeei 0x3e7 undefined reference ps zmqvan connect ps node const ' van.cc .text. zn2ps6zmqvan7connecterkns 4nodee zn2ps6zmqvan7connecterkns 4nodee 0x20d undefined reference zmq setsockopt' van.cc .text. zn2ps6zmqvan7connecterkns 4nodee zn2ps6zmqvan7connecterkns 4nodee 0x423 undefined reference zmq strerror' van.cc .text. zn2ps6zmqvan7connecterkns 4nodee zn2ps6zmqvan7connecterkns 4nodee 0x5c5 undefined reference zmq strerror' 3rdparty ps lite libpslite.a van.cc.o function zmq ctx new' van.cc .text. zn2ps6zmqvan5startei zn2ps6zmqvan5startei 0x11d undefined reference zmq ctx new' van.cc .text. zn2ps6zmqvan5startei zn2ps6zmqvan5startei 0x15e undefined reference zmq ctx set' 3rdparty ps lite libpslite.a van.cc.o function zmq msg init data' van.cc .text. zn2ps6zmqvan7sendmsgerkns 7messagee zn2ps6zmqvan7sendmsgerkns 7messagee 0x267 undefined reference zmq msg init data' van.cc .text. zn2ps6zmqvan7sendmsgerkns 7messagee zn2ps6zmqvan7sendmsgerkns 7messagee 0x348 undefined reference zmq strerror' 3rdparty ps lite libpslite.a van.cc.o function zmq msg init' van.cc .text. zn2ps6zmqvan7recvmsgepns 7messagee zn2ps6zmqvan7recvmsgepns 7messagee 0xf6 undefined reference zmq msg data' van.cc .text. zn2ps6zmqvan7recvmsgepns 7messagee zn2ps6zmqvan7recvmsgepns 7messagee 0x116 undefined reference zmq msg size' van.cc .text. zn2ps6zmqvan7recvmsgepns 7messagee zn2ps6zmqvan7recvmsgepns 7messagee 0x217 undefined reference zmq strerror' van.cc .text. zn2ps6zmqvan7recvmsgepns 7messagee zn2ps6zmqvan7recvmsgepns 7messagee 0x300 undefined reference zmq msg more' van.cc .text. zn2ps6zmqvan7recvmsgepns 7messagee zn2ps6zmqvan7recvmsgepns 7messagee 0x3d0 undefined reference zmq msg close' van.cc .text. zn2ps6zmqvan7recvmsgepns 7messagee zn2ps6zmqvan7recvmsgepns 7messagee 0x3ff undefined reference zmq msg data' van.cc .text. zn2ps6zmqvan7recvmsgepns 7messagee zn2ps6zmqvan7recvmsgepns 7messagee 0x56a undefined reference steps reproduce 1. mkdir cmake build cd cmake build 2. cmake dblas open duse opencv 1 gninja .. 3. ninja v tried solve it? 1. use makefile install mxnet, failed error, either. alternative information 1. opencv version 4.0.1",0,compiling source code error,"compiling source code error description compiling source code occurred link libzmq error. environment info required package used python r scala julia using python. build info required built source compiler gcc clang mingw visual studio gcc mxnet commit hash 7c617ccc7a8655f3b93acdfac8aeee20eee2a778 build config cmakelists.txt, set build files error message std sp counted deleter, gnu cxx lock policy 2 dispose ' van.cc .text. znst19 sp counted deleteripczn2ps6zmqvan7recvmsgepns1 7messageeeuls0 e saiveln9 gnu cxx12 lock policye2ee10 disposeev znst19 sp counted deleteripczn2ps6zmqvan7recvmsgepns1 7messageeeuls0 e saiveln9 gnu cxx12 lock policye2ee10 disposeev 0x9 undefined reference ps zmqvan stop ' van.cc .text. zn2ps6zmqvan4stopev zn2ps6zmqvan4stopev 0xe5 undefined reference zmq close' van.cc .text. zn2ps6zmqvan4stopev zn2ps6zmqvan4stopev 0x2c2 undefined reference zmq close' van.cc .text. zn2ps6zmqvan4stopev zn2ps6zmqvan4stopev 0x55d undefined reference ps zmqvan bind ps node const , int ' van.cc .text. zn2ps6zmqvan4binderkns 4nodeei zn2ps6zmqvan4binderkns 4nodeei 0x3a undefined reference zmq bind' van.cc .text. zn2ps6zmqvan4binderkns 4nodeei zn2ps6zmqvan4binderkns 4nodeei 0x3e7 undefined reference ps zmqvan connect ps node const ' van.cc .text. zn2ps6zmqvan7connecterkns 4nodee zn2ps6zmqvan7connecterkns 4nodee 0x20d undefined reference zmq setsockopt' van.cc .text. zn2ps6zmqvan7connecterkns 4nodee zn2ps6zmqvan7connecterkns 4nodee 0x423 undefined reference zmq strerror' van.cc .text. zn2ps6zmqvan7connecterkns 4nodee zn2ps6zmqvan7connecterkns 4nodee 0x5c5 undefined reference zmq strerror' 3rdparty ps lite libpslite.a van.cc.o function zmq ctx new' van.cc .text. zn2ps6zmqvan5startei zn2ps6zmqvan5startei 0x11d undefined reference zmq ctx new' van.cc .text. zn2ps6zmqvan5startei zn2ps6zmqvan5startei 0x15e undefined reference zmq ctx set' 3rdparty ps lite libpslite.a van.cc.o function zmq msg init data' van.cc .text. zn2ps6zmqvan7sendmsgerkns 7messagee zn2ps6zmqvan7sendmsgerkns 7messagee 0x267 undefined reference zmq msg init data' van.cc .text. zn2ps6zmqvan7sendmsgerkns 7messagee zn2ps6zmqvan7sendmsgerkns 7messagee 0x348 undefined reference zmq strerror' 3rdparty ps lite libpslite.a van.cc.o function zmq msg init' van.cc .text. zn2ps6zmqvan7recvmsgepns 7messagee zn2ps6zmqvan7recvmsgepns 7messagee 0xf6 undefined reference zmq msg data' van.cc .text. zn2ps6zmqvan7recvmsgepns 7messagee zn2ps6zmqvan7recvmsgepns 7messagee 0x116 undefined reference zmq msg size' van.cc .text. zn2ps6zmqvan7recvmsgepns 7messagee zn2ps6zmqvan7recvmsgepns 7messagee 0x217 undefined reference zmq strerror' van.cc .text. zn2ps6zmqvan7recvmsgepns 7messagee zn2ps6zmqvan7recvmsgepns 7messagee 0x300 undefined reference zmq msg more' van.cc .text. zn2ps6zmqvan7recvmsgepns 7messagee zn2ps6zmqvan7recvmsgepns 7messagee 0x3d0 undefined reference zmq msg close' van.cc .text. zn2ps6zmqvan7recvmsgepns 7messagee zn2ps6zmqvan7recvmsgepns 7messagee 0x3ff undefined reference zmq msg data' van.cc .text. zn2ps6zmqvan7recvmsgepns 7messagee zn2ps6zmqvan7recvmsgepns 7messagee 0x56a undefined reference steps reproduce 1. mkdir cmake build cd cmake build 2. cmake dblas open duse opencv 1 gninja .. 3. ninja v tried solve it? 1. use makefile install mxnet, failed error, either. alternative information 1. opencv version 4.0.1"
incubator-mxnet,5275,"noticed mxnet.symbol defined mxnet.symbol.tanh mnet.symbol.leakyrelu however, mxnet.symbol.relu mxnet.symbol.sigmoid defined. functions defined within think bad couple reasons appears disorganized. like functions grouped like folders. makes library overly married neural network notation. could imagine times someone might want access sigmoid hinge functions think activation functions. think relu better spell relu sigmoid available mxnet.symbol bugs installation issues, please provide following information. information provide, likely people able help you. environment info operating system compiler package used python r scala julia mxnet version installed source mxnet commit hash using python package, please provide python version distribution using r package, please provide r error message please paste full error message, including stack trace. minimum reproducible example using code, please provide short script reproduces error. steps reproduce running standard examples, please provide commands run lead error. 1. 2. 3. tried solve it? 1. 2. 3.",0,mxnet.symbol.relu sigmoid ?,"mxnet.symbol.relu sigmoid ? noticed mxnet.symbol defined mxnet.symbol.tanh mnet.symbol.leakyrelu however, mxnet.symbol.relu mxnet.symbol.sigmoid defined. functions defined within think bad couple reasons appears disorganized. like functions grouped like folders. makes library overly married neural network notation. could imagine times someone might want access sigmoid hinge functions think activation functions. think relu better spell relu sigmoid available mxnet.symbol bugs installation issues, please provide following information. information provide, likely people able help you. environment info operating system compiler package used python r scala julia mxnet version installed source mxnet commit hash using python package, please provide python version distribution using r package, please provide r error message please paste full error message, including stack trace. minimum reproducible example using code, please provide short script reproduces error. steps reproduce running standard examples, please provide commands run lead error. 1. 2. 3. tried solve it? 1. 2. 3."
incubator-mxnet,12923,"troubled problem days, need everyone's help, thank you! environment gpu tesla p4 cpu intel r xeon r gold 6133 cpu 2.50ghz. appearance program receives image data server. period time, program starts appear similar deadlock may caused requests, cannot accurately reproduced tested mxnet versions 1.0, 1.2, 1.3, program showed appearance. program running process called python engine c multithreaded program uses mxnet python api. seen stack information, mxndarraysynccopytocpu waits condition variable execution, program always stuck place. stack information thread 85 thread 0x7f3cba52f700 lwp 41394 0 0x00007f3d582fd6d5 pthread cond wait glibc 2.3.2 lib64 libpthread.so.0 1 0x00007f3d580979bc gthread cond wait mutex , cond data home xxx gcc build gcc 4.9.4 build x86 64 redhat linux libstdc v3 include x86 64 redhat linux bits gthr default.h 864 2 std condition variable wait , lock .. .. .. .. .. libstdc v3 src c 11 condition variable.cc 52 3 0x00007f3c7bcb86d5 ?? app anaconda2 lib python2.7 site packages mxnet libmxnet.so 4 0x00007f3c7bd94b4d ?? app anaconda2 lib python2.7 site packages mxnet libmxnet.so 5 0x00007f3c7be7e9c3 ?? app anaconda2 lib python2.7 site packages mxnet libmxnet.so 6 0x00007f3c7bc516db mxndarraysynccopytocpu app anaconda2 lib python2.7 site packages mxnet libmxnet.so 7 0x00007f3d53e15adc ffi call unix64 app libs . libffi.so.6 8 0x00007f3d53e15282 ffi call app libs . libffi.so.6 9 0x00007f3bfdd09376 call function pointer argcount 3, resmem 0x7f3b3c1c4040, restype , atypes , avalues 0x7f3b3c1c4010, pproc 0x7f3c7bc516b0 , flags 4353 home xxx minonda conda bld python 2.7 1482296880985 work python 2.7.13 modules ctypes callproc.c 841 10 ctypes callproc pproc 0x7f3c7bc516b0 , argtuple 0x7f3b3c1c4130, flags 4353, argtypes , restype 0x1616b80, checker 0x0 home xxx minonda conda bld python 2.7 1482296880985 work python 2.7.13 modules ctypes callproc.c 1184 11 0x00007f3bfdd00db3 pycfuncptr call self , inargs , kwds 0x0 home xxx minonda conda bld python 2.7 1482296880985 work python 2.7.13 modules ctypes ctypes.c 3979 12 0x00007f3d52c42e93 pyobject call func 0x7f3d2a11a050, arg , kw objects abstract.c 2547 13 0x00007f3d52cf580d call nk , na , pp stack 0x7f3b3c1c43b8, func 0x7f3d2a11a050 python ceval.c 4569 14 call function oparg , pp stack 0x7f3b3c1c43b8 python ceval.c 4374 15 pyeval evalframeex f , throwflag python ceval.c 2989 16 0x00007f3d52cf7c3e pyeval evalcodeex co 0x7f3d3f730030, globals , locals , args , argcount 1, kws 0x7f3d2a186fd0, kwcount 0, defs 0x0, defcount 0, closure 0x0 python ceval.c 3584 17 0x00007f3d52cf71f7 fast function nk , na 1, n , pp stack 0x7f3b3c1c45d8, func 0x7f3d3f6ee5f0 python ceval.c 4447 18 call function oparg , pp stack 0x7f3b3c1c45d8 python ceval.c 4372 19 pyeval evalframeex f , throwflag python ceval.c 2989 20 0x00007f3d52cf7345 fast function nk , na , n , pp stack 0x7f3b3c1c4748, func 0x7f3d2aea9c80 python ceval.c 4437 21 call function oparg , pp stack 0x7f3b3c1c4748 python ceval.c 4372 22 pyeval evalframeex f , throwflag python ceval.c 2989 23 0x00007f3d52cf7c3e pyeval evalcodeex co 0x7f3d528fcc30, globals , locals , args , argcount 2, kws 0x7f3d2a18dc68, kwcount 0, defs 0x0, defcount 0, closure 0x0 python ceval.c 3584 24 0x00007f3d52cf71f7 fast function nk , na 2, n , pp stack 0x7f3b3c1c4968, func 0x7f3d2a33f0c8 python ceval.c 4447 25 call function oparg , pp stack 0x7f3b3c1c4968 python ceval.c 4372 26 pyeval evalframeex f , throwflag python ceval.c 2989 27 0x00007f3d52cf7345 fast function nk , na , n , pp stack 0x7f3b3c1c4ad8, func 0x7f3d2a33f410 python ceval.c 4437 28 call function oparg , pp stack 0x7f3b3c1c4ad8 python ceval.c 4372 29 pyeval evalframeex f , throwflag python ceval.c 2989 30 0x00007f3d52cf7c3e pyeval evalcodeex co 0x7f3d52963db0, globals , locals , args , argcount 1, kws 0x0, kwcount 0, defs 0x0, defcount 0, closure 0x0 python ceval.c 3584 31 0x00007f3d52c72a61 function call func 0x7f3d2a33f8c0, arg 0x7f3d529377d0, kw 0x0 objects funcobject.c 523 32 0x00007f3d52c42e93 pyobject call func 0x7f3d2a33f8c0, arg , kw objects abstract.c 2547 33 0x00007f3d52ced7b3 pyeval callobjectwithkeywords func 0x7f3d2a33f8c0, arg 0x7f3d529377d0, kw python ceval.c 4221 34 0x00007f3d52d13468 pyeval callmethod obj , methodname , format python modsupport.c 612 35 0x00007f3d5303141f ?? 36 0x0000000000000000 ?? addition occasions threads blocked time, stack information below, stack information unrelated cpu thread. strange thing actually libmxnet.so thread 70 thread 0x7f3b0bff6700 lwp 41409 0 0x00007f3d582fd6d5 pthread cond wait glibc 2.3.2 lib64 libpthread.so.0 1 0x00007f3d580979bc gthread cond wait mutex , cond data home xxx gcc build gcc 4.9.4 build x86 64 redhat linux libstdc v3 include x86 64 redhat linux bits gthr default.h 864 2 std condition variable wait , lock .. .. .. .. .. libstdc v3 src c 11 condition variable.cc 52 3 0x00007f3c7bcb88a3 ?? app anaconda2 lib python2.7 site packages mxnet libmxnet.so 4 0x00007f3c7bcc0339 ?? app anaconda2 lib python2.7 site packages mxnet libmxnet.so 5 0x00007f3d577c4702 fork lib64 libc.so.6 ......",0,deadlock happend calling mxndarraysynccopytocpu ?,"deadlock happend calling mxndarraysynccopytocpu ? troubled problem days, need everyone's help, thank you! environment gpu tesla p4 cpu intel r xeon r gold 6133 cpu 2.50ghz. appearance program receives image data server. period time, program starts appear similar deadlock may caused requests, cannot accurately reproduced tested mxnet versions 1.0, 1.2, 1.3, program showed appearance. program running process called python engine c multithreaded program uses mxnet python api. seen stack information, mxndarraysynccopytocpu waits condition variable execution, program always stuck place. stack information thread 85 thread 0x7f3cba52f700 lwp 41394 0 0x00007f3d582fd6d5 pthread cond wait glibc 2.3.2 lib64 libpthread.so.0 1 0x00007f3d580979bc gthread cond wait mutex , cond data home xxx gcc build gcc 4.9.4 build x86 64 redhat linux libstdc v3 include x86 64 redhat linux bits gthr default.h 864 2 std condition variable wait , lock .. .. .. .. .. libstdc v3 src c 11 condition variable.cc 52 3 0x00007f3c7bcb86d5 ?? app anaconda2 lib python2.7 site packages mxnet libmxnet.so 4 0x00007f3c7bd94b4d ?? app anaconda2 lib python2.7 site packages mxnet libmxnet.so 5 0x00007f3c7be7e9c3 ?? app anaconda2 lib python2.7 site packages mxnet libmxnet.so 6 0x00007f3c7bc516db mxndarraysynccopytocpu app anaconda2 lib python2.7 site packages mxnet libmxnet.so 7 0x00007f3d53e15adc ffi call unix64 app libs . libffi.so.6 8 0x00007f3d53e15282 ffi call app libs . libffi.so.6 9 0x00007f3bfdd09376 call function pointer argcount 3, resmem 0x7f3b3c1c4040, restype , atypes , avalues 0x7f3b3c1c4010, pproc 0x7f3c7bc516b0 , flags 4353 home xxx minonda conda bld python 2.7 1482296880985 work python 2.7.13 modules ctypes callproc.c 841 10 ctypes callproc pproc 0x7f3c7bc516b0 , argtuple 0x7f3b3c1c4130, flags 4353, argtypes , restype 0x1616b80, checker 0x0 home xxx minonda conda bld python 2.7 1482296880985 work python 2.7.13 modules ctypes callproc.c 1184 11 0x00007f3bfdd00db3 pycfuncptr call self , inargs , kwds 0x0 home xxx minonda conda bld python 2.7 1482296880985 work python 2.7.13 modules ctypes ctypes.c 3979 12 0x00007f3d52c42e93 pyobject call func 0x7f3d2a11a050, arg , kw objects abstract.c 2547 13 0x00007f3d52cf580d call nk , na , pp stack 0x7f3b3c1c43b8, func 0x7f3d2a11a050 python ceval.c 4569 14 call function oparg , pp stack 0x7f3b3c1c43b8 python ceval.c 4374 15 pyeval evalframeex f , throwflag python ceval.c 2989 16 0x00007f3d52cf7c3e pyeval evalcodeex co 0x7f3d3f730030, globals , locals , args , argcount 1, kws 0x7f3d2a186fd0, kwcount 0, defs 0x0, defcount 0, closure 0x0 python ceval.c 3584 17 0x00007f3d52cf71f7 fast function nk , na 1, n , pp stack 0x7f3b3c1c45d8, func 0x7f3d3f6ee5f0 python ceval.c 4447 18 call function oparg , pp stack 0x7f3b3c1c45d8 python ceval.c 4372 19 pyeval evalframeex f , throwflag python ceval.c 2989 20 0x00007f3d52cf7345 fast function nk , na , n , pp stack 0x7f3b3c1c4748, func 0x7f3d2aea9c80 python ceval.c 4437 21 call function oparg , pp stack 0x7f3b3c1c4748 python ceval.c 4372 22 pyeval evalframeex f , throwflag python ceval.c 2989 23 0x00007f3d52cf7c3e pyeval evalcodeex co 0x7f3d528fcc30, globals , locals , args , argcount 2, kws 0x7f3d2a18dc68, kwcount 0, defs 0x0, defcount 0, closure 0x0 python ceval.c 3584 24 0x00007f3d52cf71f7 fast function nk , na 2, n , pp stack 0x7f3b3c1c4968, func 0x7f3d2a33f0c8 python ceval.c 4447 25 call function oparg , pp stack 0x7f3b3c1c4968 python ceval.c 4372 26 pyeval evalframeex f , throwflag python ceval.c 2989 27 0x00007f3d52cf7345 fast function nk , na , n , pp stack 0x7f3b3c1c4ad8, func 0x7f3d2a33f410 python ceval.c 4437 28 call function oparg , pp stack 0x7f3b3c1c4ad8 python ceval.c 4372 29 pyeval evalframeex f , throwflag python ceval.c 2989 30 0x00007f3d52cf7c3e pyeval evalcodeex co 0x7f3d52963db0, globals , locals , args , argcount 1, kws 0x0, kwcount 0, defs 0x0, defcount 0, closure 0x0 python ceval.c 3584 31 0x00007f3d52c72a61 function call func 0x7f3d2a33f8c0, arg 0x7f3d529377d0, kw 0x0 objects funcobject.c 523 32 0x00007f3d52c42e93 pyobject call func 0x7f3d2a33f8c0, arg , kw objects abstract.c 2547 33 0x00007f3d52ced7b3 pyeval callobjectwithkeywords func 0x7f3d2a33f8c0, arg 0x7f3d529377d0, kw python ceval.c 4221 34 0x00007f3d52d13468 pyeval callmethod obj , methodname , format python modsupport.c 612 35 0x00007f3d5303141f ?? 36 0x0000000000000000 ?? addition occasions threads blocked time, stack information below, stack information unrelated cpu thread. strange thing actually libmxnet.so thread 70 thread 0x7f3b0bff6700 lwp 41409 0 0x00007f3d582fd6d5 pthread cond wait glibc 2.3.2 lib64 libpthread.so.0 1 0x00007f3d580979bc gthread cond wait mutex , cond data home xxx gcc build gcc 4.9.4 build x86 64 redhat linux libstdc v3 include x86 64 redhat linux bits gthr default.h 864 2 std condition variable wait , lock .. .. .. .. .. libstdc v3 src c 11 condition variable.cc 52 3 0x00007f3c7bcb88a3 ?? app anaconda2 lib python2.7 site packages mxnet libmxnet.so 4 0x00007f3c7bcc0339 ?? app anaconda2 lib python2.7 site packages mxnet libmxnet.so 5 0x00007f3d577c4702 fork lib64 libc.so.6 ......"
incubator-mxnet,6814,"read page http mxnet tqchen.readthedocs.io en latest system multi node.html find alexnet imagenet example. anyone give help here? local allreduce cpu similar local update cpu except averaged gradients copied back devices, weights updated devices. faster 1 weight size large use device accelerate computation increase workload k times . examples alexnet imagenet.",0,find link 'local allreduce cpu' example according mxnet doc,"find link 'local allreduce cpu' example according mxnet doc read page http mxnet tqchen.readthedocs.io en latest system multi node.html find alexnet imagenet example. anyone give help here? local allreduce cpu similar local update cpu except averaged gradients copied back devices, weights updated devices. faster 1 weight size large use device accelerate computation increase workload k times . examples alexnet imagenet."
incubator-mxnet,13374,outputs diagnose.py floatnp.floatingnp.float64 np.dtype float .typefloatnp.floatingnp.float64 np.dtype float .type,0,bug valueerror multiple outputs name resnetv1b0 layers1 relu0 fwd output .,bug valueerror multiple outputs name resnetv1b0 layers1 relu0 fwd output . outputs diagnose.py floatnp.floatingnp.float64 np.dtype float .typefloatnp.floatingnp.float64 np.dtype float .type
incubator-mxnet,9198,description https mxnet.incubator.apache.org architecture release note 0 9.html?highlight cython image io mx.image provides set fast image processing api leverage mxnet engine automatically parallelize processing. write decoding automatically run parallel. tried parallel decoding using mxnet.image.imdecode function. working. build info required built source compiler gcc clang mingw visual studio mxnet commit hash paste output here. build config steps reproduce tried solve it? 1. build latest sources,0,parallel decoding mxnet.image.imdecode working,parallel decoding mxnet.image.imdecode working description https mxnet.incubator.apache.org architecture release note 0 9.html?highlight cython image io mx.image provides set fast image processing api leverage mxnet engine automatically parallelize processing. write decoding automatically run parallel. tried parallel decoding using mxnet.image.imdecode function. working. build info required built source compiler gcc clang mingw visual studio mxnet commit hash paste output here. build config steps reproduce tried solve it? 1. build latest sources
incubator-mxnet,1875,"new, got error 07 33 45 info start training gpu 0 19 36 00 src symbol static graph.cc 383 memory efficient gradient aggregation on... disable, set mxnet exec inplace grad sum cap big number 19 36 00 src symbol static graph.cc 383 memory efficient gradient aggregation on... disable, set mxnet exec inplace grad sum cap big number 19 36 00 src symbol static graph.cc 383 memory efficient gradient aggregation on... disable, set mxnet exec inplace grad sum cap big number 19 36 00 src symbol static graph.cc 383 memory efficient gradient aggregation on... disable, set mxnet exec inplace grad sum cap big number 19 36 00 src symbol static graph.cc 383 memory efficient gradient aggregation on... disable, set mxnet exec inplace grad sum cap big number 19 36 00 src symbol static graph.cc 383 memory efficient gradient aggregation on... disable, set mxnet exec inplace grad sum cap big number 19 36 00 src symbol static graph.cc 383 memory efficient gradient aggregation on... disable, set mxnet exec inplace grad sum cap big number 19 36 00 src symbol static graph.cc 383 memory efficient gradient aggregation on... disable, set mxnet exec inplace grad sum cap big number 19 36 00 src symbol static graph.cc 383 memory efficient gradient aggregation on... disable, set mxnet exec inplace grad sum cap big number 19 36 00 src symbol static graph.cc 383 memory efficient gradient aggregation on... disable, set mxnet exec inplace grad sum cap big number 19 36 00 src symbol static graph.cc 383 memory efficient gradient aggregation on... disable, set mxnet exec inplace grad sum cap big number 19 36 00 src symbol static graph.cc 383 memory efficient gradient aggregation on... disable, set mxnet exec inplace grad sum cap big number traceback recent call last file char rnn.py , line 120, epoch end callback mx.callback.do checkpoint obama file usr local lib python2.7 dist packages mxnet 0.5.0 py2.7.egg mxnet model.py , line 774, fit sym gen self.sym gen file usr local lib python2.7 dist packages mxnet 0.5.0 py2.7.egg mxnet model.py , line 244, train multi device executor manager.update metric eval metric, data batch.label file usr local lib python2.7 dist packages mxnet 0.5.0 py2.7.egg mxnet executor manager.py , line 406, update metric self.curr execgrp.update metric metric, labels file usr local lib python2.7 dist packages mxnet 0.5.0 py2.7.egg mxnet executor manager.py , line 262, update metric metric.update labels slice, texec.outputs file usr local lib python2.7 dist packages mxnet 0.5.0 py2.7.egg mxnet metric.py , line 338, update self.sum metric self. feval label, pred file usr local lib python2.7 dist packages mxnet 0.5.0 py2.7.egg mxnet metric.py , line 355, feval return numpy feval label, pred file char rnn.py , line 50, perplexity loss np.log max 1e 10, pred int label typeerror length 1 arrays converted python scalars",0,run example rnn char rnn.ipynb failed,"run example rnn char rnn.ipynb failed new, got error 07 33 45 info start training gpu 0 19 36 00 src symbol static graph.cc 383 memory efficient gradient aggregation on... disable, set mxnet exec inplace grad sum cap big number 19 36 00 src symbol static graph.cc 383 memory efficient gradient aggregation on... disable, set mxnet exec inplace grad sum cap big number 19 36 00 src symbol static graph.cc 383 memory efficient gradient aggregation on... disable, set mxnet exec inplace grad sum cap big number 19 36 00 src symbol static graph.cc 383 memory efficient gradient aggregation on... disable, set mxnet exec inplace grad sum cap big number 19 36 00 src symbol static graph.cc 383 memory efficient gradient aggregation on... disable, set mxnet exec inplace grad sum cap big number 19 36 00 src symbol static graph.cc 383 memory efficient gradient aggregation on... disable, set mxnet exec inplace grad sum cap big number 19 36 00 src symbol static graph.cc 383 memory efficient gradient aggregation on... disable, set mxnet exec inplace grad sum cap big number 19 36 00 src symbol static graph.cc 383 memory efficient gradient aggregation on... disable, set mxnet exec inplace grad sum cap big number 19 36 00 src symbol static graph.cc 383 memory efficient gradient aggregation on... disable, set mxnet exec inplace grad sum cap big number 19 36 00 src symbol static graph.cc 383 memory efficient gradient aggregation on... disable, set mxnet exec inplace grad sum cap big number 19 36 00 src symbol static graph.cc 383 memory efficient gradient aggregation on... disable, set mxnet exec inplace grad sum cap big number 19 36 00 src symbol static graph.cc 383 memory efficient gradient aggregation on... disable, set mxnet exec inplace grad sum cap big number traceback recent call last file char rnn.py , line 120, epoch end callback mx.callback.do checkpoint obama file usr local lib python2.7 dist packages mxnet 0.5.0 py2.7.egg mxnet model.py , line 774, fit sym gen self.sym gen file usr local lib python2.7 dist packages mxnet 0.5.0 py2.7.egg mxnet model.py , line 244, train multi device executor manager.update metric eval metric, data batch.label file usr local lib python2.7 dist packages mxnet 0.5.0 py2.7.egg mxnet executor manager.py , line 406, update metric self.curr execgrp.update metric metric, labels file usr local lib python2.7 dist packages mxnet 0.5.0 py2.7.egg mxnet executor manager.py , line 262, update metric metric.update labels slice, texec.outputs file usr local lib python2.7 dist packages mxnet 0.5.0 py2.7.egg mxnet metric.py , line 338, update self.sum metric self. feval label, pred file usr local lib python2.7 dist packages mxnet 0.5.0 py2.7.egg mxnet metric.py , line 355, feval return numpy feval label, pred file char rnn.py , line 50, perplexity loss np.log max 1e 10, pred int label typeerror length 1 arrays converted python scalars"
incubator-mxnet,11407,http jenkins.mxnet ci.amazon ml.com blue organizations jenkins incubator mxnet detail pr 11358 9 pipeline,0,corrupt image fails caffe converter test,corrupt image fails caffe converter test http jenkins.mxnet ci.amazon ml.com blue organizations jenkins incubator mxnet detail pr 11358 9 pipeline
incubator-mxnet,6628,"hi folks. now, trying run image classification predict.cc https github.com dmlc mxnet blob master example image classification predict cpp image classification predict.cc sample code image classification using pre trained model. facing error point creating predictor like, error got like, know solve run code, please help out. thanks advance! environment info operating system os x 10.12.4 compiler appleclang package used python r scala julia c mxnet commit hash 918d48526481cf424197079ae47841e1b8afe399 error message whole error message below. minimum reproducible example modified name model files paths. use models downloaded https pan.baidu.com 1sjxkrqx . image used size 240 x 240. ! apple https user images.githubusercontent.com 19764434 26961671 85bf35de 4d1b 11e7 9c48 f25434ff8638.jpg since running local project, wrote like this. steps reproduce tried run program command received error.",0,c tutorial error model file image classification predict.cc seems like broken,"c tutorial error model file image classification predict.cc seems like broken hi folks. now, trying run image classification predict.cc https github.com dmlc mxnet blob master example image classification predict cpp image classification predict.cc sample code image classification using pre trained model. facing error point creating predictor like, error got like, know solve run code, please help out. thanks advance! environment info operating system os x 10.12.4 compiler appleclang package used python r scala julia c mxnet commit hash 918d48526481cf424197079ae47841e1b8afe399 error message whole error message below. minimum reproducible example modified name model files paths. use models downloaded https pan.baidu.com 1sjxkrqx . image used size 240 x 240. ! apple https user images.githubusercontent.com 19764434 26961671 85bf35de 4d1b 11e7 9c48 f25434ff8638.jpg since running local project, wrote like this. steps reproduce tried run program command received error."
incubator-mxnet,3668,"using mxnet get used interface mxnet.model.feedforward, method save checkpoint . method returns json file symbol definition .params saving args aux params. new api mxnet.module can't figure get save symbol easily module, makes complex use finetune models based interface, since loading model requires json file symbol definition. there's property allows us get symbol module. inherit classes like , comprise stack modules build complex networks. makes harder access whole symbol modules, even though get symbols submodules recursive way concat them, still straightforward me.",0,difficult get save symbols new api mxnet.module,"difficult get save symbols new api mxnet.module using mxnet get used interface mxnet.model.feedforward, method save checkpoint . method returns json file symbol definition .params saving args aux params. new api mxnet.module can't figure get save symbol easily module, makes complex use finetune models based interface, since loading model requires json file symbol definition. there's property allows us get symbol module. inherit classes like , comprise stack modules build complex networks. makes harder access whole symbol modules, even though get symbols submodules recursive way concat them, still straightforward me."
