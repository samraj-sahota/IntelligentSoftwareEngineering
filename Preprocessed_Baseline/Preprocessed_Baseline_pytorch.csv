Repository,Number,Body,class,Title,Combined_Text
pytorch,15872,"bug beginning training, gpu memory usage high, i.e. 9gb. iterations, consumption stable 1.4 gb. can't set larger batch size. reproduce change crop size https github.com pytorch examples blob 0.4 imagenet main.py l122 reproduce problem extent, i.e. 7.5gb beginning 4.5gb stable. training scripts complicated dataset process. environment",1,gpu memory usage high beginning training.,"gpu memory usage high beginning training. bug beginning training, gpu memory usage high, i.e. 9gb. iterations, consumption stable 1.4 gb. can't set larger batch size. reproduce change crop size https github.com pytorch examples blob 0.4 imagenet main.py l122 reproduce problem extent, i.e. 7.5gb beginning 4.5gb stable. training scripts complicated dataset process. environment"
pytorch,8710,"issue description torch.nn.mseloss seems get inaccurate calculation size average true reduce true code example tensors 0.1's 0's, desired output 0.01. 0.1 0 2 0.01. get correct result code however, something like below, get inaccurate results turned size average reduce calculate mean manually, get desired output kind know cannot get exactly number using float datatype, 0.01 0.00983 seems me. quite sure intended like using correctly. system info pytorch version 0.4.0 debug build cuda used build pytorch 9.0.176 os ubuntu 16.04.3 lts gcc version ubuntu 5.4.0 6ubuntu1 16.04.9 5.4.0 20160609 cmake version version 3.5.1 python version 2.7 cuda available yes cuda runtime version could collect gpu models configuration gpu 0 geforce gtx 1080 ti gpu 1 geforce gtx 1080 ti gpu 2 geforce gtx 1080 ti nvidia driver version 387.34 cudnn version probably one following usr lib x86 64 linux gnu libcudnn.so.7.1.2 usr lib x86 64 linux gnu libcudnn static v7.a usr local cuda 9.0 targets x86 64 linux lib libcudnn.so usr local cuda 9.0 targets x86 64 linux lib libcudnn.so.7 usr local cuda 9.0 targets x86 64 linux lib libcudnn.so.7.1.4 usr local cuda 9.0 targets x86 64 linux lib libcudnn static.a versions relevant libraries pip numpy 1.14.5 pip torch 0.4.0 pip torchvision 0.2.1 conda could collect",1,incorrect calculation torch.nn.mseloss,"incorrect calculation torch.nn.mseloss issue description torch.nn.mseloss seems get inaccurate calculation size average true reduce true code example tensors 0.1's 0's, desired output 0.01. 0.1 0 2 0.01. get correct result code however, something like below, get inaccurate results turned size average reduce calculate mean manually, get desired output kind know cannot get exactly number using float datatype, 0.01 0.00983 seems me. quite sure intended like using correctly. system info pytorch version 0.4.0 debug build cuda used build pytorch 9.0.176 os ubuntu 16.04.3 lts gcc version ubuntu 5.4.0 6ubuntu1 16.04.9 5.4.0 20160609 cmake version version 3.5.1 python version 2.7 cuda available yes cuda runtime version could collect gpu models configuration gpu 0 geforce gtx 1080 ti gpu 1 geforce gtx 1080 ti gpu 2 geforce gtx 1080 ti nvidia driver version 387.34 cudnn version probably one following usr lib x86 64 linux gnu libcudnn.so.7.1.2 usr lib x86 64 linux gnu libcudnn static v7.a usr local cuda 9.0 targets x86 64 linux lib libcudnn.so usr local cuda 9.0 targets x86 64 linux lib libcudnn.so.7 usr local cuda 9.0 targets x86 64 linux lib libcudnn.so.7.1.4 usr local cuda 9.0 targets x86 64 linux lib libcudnn static.a versions relevant libraries pip numpy 1.14.5 pip torch 0.4.0 pip torchvision 0.2.1 conda could collect"
pytorch,13843,"bug possible cpu side memory leak even fitting gpu using pytorch 0.4.1. reproduce quite new pytorch, used tf keras extensively past, trying use pytorch replacement. decided start small seq2seq skip thought model, cobbled together using pytorch nlp tutorials. everything seems work fine run small scale tests, however, use code run large scale fit 3000 separate paragraphs paragraph variable number sentences notice system ram usage slowly goes script runs, eventually hits 100 box becomes unresponsive force rebooted. linux box 64 gb ram, script starts, usage 4.7 climbs steadily time 100 , point box becomes unresponsive. since new pytorch, sure perhaps something blatantly wrong code would account behaviour? core fitting logic, runs script linux helper functions create tensors passed model converting text words indices predefined gensim dictionary and, finally, skipthought model itself, helper classes apologies wall code expected behavior memory usage linux linearly increase fitting script runs, especially point box dies. environment pytorch version e.g., 1.0 0.4.1 os e.g., linux linux ubuntu 16.04 installed pytorch , , source pip install torch build command used compiling source n python version 3.6.6 cuda cudnn version cuda 9.0.176 cudnn 7.4.1.5 gpu models configuration tesla v100 relevant information",1,possible cpu side memory leak even fitting gpu,"possible cpu side memory leak even fitting gpu bug possible cpu side memory leak even fitting gpu using pytorch 0.4.1. reproduce quite new pytorch, used tf keras extensively past, trying use pytorch replacement. decided start small seq2seq skip thought model, cobbled together using pytorch nlp tutorials. everything seems work fine run small scale tests, however, use code run large scale fit 3000 separate paragraphs paragraph variable number sentences notice system ram usage slowly goes script runs, eventually hits 100 box becomes unresponsive force rebooted. linux box 64 gb ram, script starts, usage 4.7 climbs steadily time 100 , point box becomes unresponsive. since new pytorch, sure perhaps something blatantly wrong code would account behaviour? core fitting logic, runs script linux helper functions create tensors passed model converting text words indices predefined gensim dictionary and, finally, skipthought model itself, helper classes apologies wall code expected behavior memory usage linux linearly increase fitting script runs, especially point box dies. environment pytorch version e.g., 1.0 0.4.1 os e.g., linux linux ubuntu 16.04 installed pytorch , , source pip install torch build command used compiling source n python version 3.6.6 cuda cudnn version cuda 9.0.176 cudnn 7.4.1.5 gpu models configuration tesla v100 relevant information"
pytorch,537,"first call .cuda takes one minute run system titan x swapping gtx 980ti exact system results normal timings although slight delay first call . laptop gtx 960m , almost instant.",1,initial call .cuda slow titan x,"initial call .cuda slow titan x first call .cuda takes one minute run system titan x swapping gtx 980ti exact system results normal timings although slight delay first call . laptop gtx 960m , almost instant."
pytorch,7714,"gradient penalty gp means minimize l2 norm gradient w.r.t input images. work ubuntu14.04, python2.7, cuda8.0 cudnn. version pytorch 0.4.0. find slow apply gradient penalty gp training cifar10 resnet18. test average running time step without gp 4 times slower compared standard training! code better implementation? slow is?",1,slow gradient penalty!,"slow gradient penalty! gradient penalty gp means minimize l2 norm gradient w.r.t input images. work ubuntu14.04, python2.7, cuda8.0 cudnn. version pytorch 0.4.0. find slow apply gradient penalty gp training cifar10 resnet18. test average running time step without gp 4 times slower compared standard training! code better implementation? slow is?"
pytorch,28198,"dlrm performance regression check cause little regression dlrm https github.com facebookresearch dlrm benchmark. without check dlrm benchmark result like check , result like profiling data show time increased , operations 1559.31ms 1503.48ms 27.14ms 14.58ms 22.38ms 9.79966ms reproduce steps reproduce behavior 1. download dlrm https github.com facebookresearch dlrm 1. modify bench dlrm benchmark.sh run pytorch cpu version, build 0 cpu 1 gpu 0 pt 1 c2 0 export two kmp variables export kmp blocktime 1 export kmp affinity granularity fine,compact,1,0 1. run bench dlrm benchmark.sh skx8180 machine. performance profiling data stored file model1 cpu pt 28.prof 'this' got commit id d0a4b2f586e0901c3c65f1f0e0bae15364e28821 'before' got commit id 42e7eb0426190e07339f03d4e6afb61b7ff5ae9c expected behavior dlrm performance impacted, thanks environment please copy paste output environment collection script https raw.githubusercontent.com pytorch pytorch master torch utils collect env.py fill checklist manually . get script run pytorch version e.g., 1.0 commit id d0a4b2f586e0901c3c65f1f0e0bae15364e28821 os e.g., linux ubuntu 16.04.5 lts installed pytorch , , source build command used compiling source python setup.py install python version 3.7 cuda cudnn version n gpu models configuration n relevant information gcc version ubuntu 8.3.0 16ubuntu3 16.04 8.3.0 cmake version version 3.14.4 pip3 numpy 1.16.2 pip3 numpydoc 0.8.0 conda blas 1.0 mkl conda mkl 2019.0 pypi 0 pypi conda mkl devel 2019.3 200 conda mkl include 2019.0 pypi 0 pypi conda mkl service 1.1.2 py37he904b0f 5 conda mkl fft 1.0.10 py37ha843d7b 0 conda mkl random 1.0.2 py37hd81dba3 0 additional context cc ezyang gchanan zou3519 jerryzh168",1,dlrm performance regression 26963,"dlrm performance regression 26963 dlrm performance regression check cause little regression dlrm https github.com facebookresearch dlrm benchmark. without check dlrm benchmark result like check , result like profiling data show time increased , operations 1559.31ms 1503.48ms 27.14ms 14.58ms 22.38ms 9.79966ms reproduce steps reproduce behavior 1. download dlrm https github.com facebookresearch dlrm 1. modify bench dlrm benchmark.sh run pytorch cpu version, build 0 cpu 1 gpu 0 pt 1 c2 0 export two kmp variables export kmp blocktime 1 export kmp affinity granularity fine,compact,1,0 1. run bench dlrm benchmark.sh skx8180 machine. performance profiling data stored file model1 cpu pt 28.prof 'this' got commit id d0a4b2f586e0901c3c65f1f0e0bae15364e28821 'before' got commit id 42e7eb0426190e07339f03d4e6afb61b7ff5ae9c expected behavior dlrm performance impacted, thanks environment please copy paste output environment collection script https raw.githubusercontent.com pytorch pytorch master torch utils collect env.py fill checklist manually . get script run pytorch version e.g., 1.0 commit id d0a4b2f586e0901c3c65f1f0e0bae15364e28821 os e.g., linux ubuntu 16.04.5 lts installed pytorch , , source build command used compiling source python setup.py install python version 3.7 cuda cudnn version n gpu models configuration n relevant information gcc version ubuntu 8.3.0 16ubuntu3 16.04 8.3.0 cmake version version 3.14.4 pip3 numpy 1.16.2 pip3 numpydoc 0.8.0 conda blas 1.0 mkl conda mkl 2019.0 pypi 0 pypi conda mkl devel 2019.3 200 conda mkl include 2019.0 pypi 0 pypi conda mkl service 1.1.2 py37he904b0f 5 conda mkl fft 1.0.10 py37ha843d7b 0 conda mkl random 1.0.2 py37hd81dba3 0 additional context cc ezyang gchanan zou3519 jerryzh168"
pytorch,26165,bug problem loading saved checkpoint pytorch model seperate thread. cpu memory keeps increasing never released. multi threading version script increases ram usage iteration ends 978 resv memory htop output . single threading version holds 374 resv memory htop output . randomly initialize model without state dict loading versions use smaller amount memory. reproduce expected behavior expect ram cleared reference held created models. additional info traced issue https github.com pytorch pytorch blob 33221b19acc3dcacb11c38fdbff65d9a6ce90866 torch nn modules module.py l775 commenting line makes multi threading version behave single threading one. environment pytorch version 1.2.0 os ubuntu 18.04 installed pytorch pip python version 3.5.7 cc ezyang gchanan zou3519 jerryzh168,1,memory leak multithreading environment loading checkpoint,memory leak multithreading environment loading checkpoint bug problem loading saved checkpoint pytorch model seperate thread. cpu memory keeps increasing never released. multi threading version script increases ram usage iteration ends 978 resv memory htop output . single threading version holds 374 resv memory htop output . randomly initialize model without state dict loading versions use smaller amount memory. reproduce expected behavior expect ram cleared reference held created models. additional info traced issue https github.com pytorch pytorch blob 33221b19acc3dcacb11c38fdbff65d9a6ce90866 torch nn modules module.py l775 commenting line makes multi threading version behave single threading one. environment pytorch version 1.2.0 os ubuntu 18.04 installed pytorch pip python version 3.5.7 cc ezyang gchanan zou3519 jerryzh168
pytorch,13886,"trying compute gradient 1 x without using pytorch's autograd. use formula grad 1 x, x 1 x 2 compare result gradient given pytorch's autograd, different. code output anyone explain ?",1,get wrong pytorch derivative division,"get wrong pytorch derivative division trying compute gradient 1 x without using pytorch's autograd. use formula grad 1 x, x 1 x 2 compare result gradient given pytorch's autograd, different. code output anyone explain ?"
pytorch,31295,"bug reproduce steps reproduce behavior 1. cpu 2. gpu tensor 1, 0, 4 , device 'cuda 0' reduction 'none', 'sum', 'mean' r reduction f.log softmax i, dim 1 loss f.nll loss m, target, w, reduction r none tensor 0.0455, 0.1291, 0.8693 , device 'cuda 0', grad fn sum tensor 0.9530, device 'cuda 0', grad fn mean tensor 2.3681, device 'cuda 0', grad fn loss f.nll loss m, target, w, reduction 'none' loss.mean mean tensor 0.8193, grad fn environment pytorch version 1.3.1 debug build cuda used build pytorch 10.1.243 os ubuntu 18.04.3 lts gcc version ubuntu 7.4.0 1ubuntu1 18.04.1 7.4.0 cmake version version 3.0.2 python version 3.6 cuda available yes cuda runtime version 10.1.243 gpu models configuration gpu 0 titan x pascal gpu 1 titan x pascal nvidia driver version 430.50 cudnn version usr lib x86 64 linux gnu libcudnn.so.7.6.5 versions relevant libraries pip3 numpy 1.13.3 conda blas 1.0 mkl conda mkl 2019.4 243 conda mkl service 2.3.0 py36he904b0f 0 conda mkl fft 1.0.14 py36ha843d7b 0 conda mkl random 1.1.0 py36hd6b4f25 0 conda pytorch 1.3.1 py3.6 cuda10.1.243 cudnn7.6.3 0 pytorch conda torchvision 0.4.2 py36 cu101 pytorch cc ezyang gchanan zou3519",1,nll loss weights reduction 'mean' gives wrong result,"nll loss weights reduction 'mean' gives wrong result bug reproduce steps reproduce behavior 1. cpu 2. gpu tensor 1, 0, 4 , device 'cuda 0' reduction 'none', 'sum', 'mean' r reduction f.log softmax i, dim 1 loss f.nll loss m, target, w, reduction r none tensor 0.0455, 0.1291, 0.8693 , device 'cuda 0', grad fn sum tensor 0.9530, device 'cuda 0', grad fn mean tensor 2.3681, device 'cuda 0', grad fn loss f.nll loss m, target, w, reduction 'none' loss.mean mean tensor 0.8193, grad fn environment pytorch version 1.3.1 debug build cuda used build pytorch 10.1.243 os ubuntu 18.04.3 lts gcc version ubuntu 7.4.0 1ubuntu1 18.04.1 7.4.0 cmake version version 3.0.2 python version 3.6 cuda available yes cuda runtime version 10.1.243 gpu models configuration gpu 0 titan x pascal gpu 1 titan x pascal nvidia driver version 430.50 cudnn version usr lib x86 64 linux gnu libcudnn.so.7.6.5 versions relevant libraries pip3 numpy 1.13.3 conda blas 1.0 mkl conda mkl 2019.4 243 conda mkl service 2.3.0 py36he904b0f 0 conda mkl fft 1.0.14 py36ha843d7b 0 conda mkl random 1.1.0 py36hd6b4f25 0 conda pytorch 1.3.1 py3.6 cuda10.1.243 cudnn7.6.3 0 pytorch conda torchvision 0.4.2 py36 cu101 pytorch cc ezyang gchanan zou3519"
pytorch,1088,"hi, wanted write rnn scratch using pytorch cuda capabilities ran preliminary tests compare speed cpu vs gpu. task simple consists loop mimicking update internal state x rnn recurrent weight matrix j. using quadro k620 cuda 8.0. size x n 1000 seems trade off, gpu implementation consistently getting slower number iterations increases ran tests different sizes j matrix behaviour seems pretty systematic . example running times get running enclosed script number iterations 100, 1000, 10000, 100000 cpu 0.010117292404174805, 0.058980703353881836, 0.45785975456237793, 4.512230634689331 gpu 0.0019445419311523438, 0.05474495887756348, 0.7503962516784668, 7.011191129684448 really appreciate help this. thanks advance. test script following",1,gpu slower cpu simple rnn test code,"gpu slower cpu simple rnn test code hi, wanted write rnn scratch using pytorch cuda capabilities ran preliminary tests compare speed cpu vs gpu. task simple consists loop mimicking update internal state x rnn recurrent weight matrix j. using quadro k620 cuda 8.0. size x n 1000 seems trade off, gpu implementation consistently getting slower number iterations increases ran tests different sizes j matrix behaviour seems pretty systematic . example running times get running enclosed script number iterations 100, 1000, 10000, 100000 cpu 0.010117292404174805, 0.058980703353881836, 0.45785975456237793, 4.512230634689331 gpu 0.0019445419311523438, 0.05474495887756348, 0.7503962516784668, 7.011191129684448 really appreciate help this. thanks advance. test script following"
pytorch,11333,"issue description initialize multivariatenormal https github.com pytorch pytorch blob bb7d1837bc164b06e1d0826a20a8f7e8338a44e3 torch distributions multivariate normal.py l76 object covariance matrix provided batch dimension too, slow gpu, always slower gpu cpu. even seems leak memory without batch dimension cpu gpu, different intensity check n parameter code example . one strange thing uses cpu ram running gpu. digging bit deeper, experienced tensor's potrf function, probably root cause. code example outputs testing multivariatenormal testing potrf system info collecting environment information... pytorch version 0.5.0a0 e9ad743 debug build cuda used build pytorch 9.2.88 os ubuntu 18.04.1 lts gcc version ubuntu 7.3.0 16ubuntu3 7.3.0 cmake version version 3.12.0 python version 3.6 cuda available yes cuda runtime version 9.2.148 gpu models configuration gpu 0 geforce gtx 1080 ti gpu 1 geforce gtx 1080 ti gpu 2 geforce gtx 1080 ti nvidia driver version 396.54 cudnn version probably one following usr lib x86 64 linux gnu libcudnn.so.7.2.1 usr lib x86 64 linux gnu libcudnn static v7.a versions relevant libraries pip could collect conda magma cuda91 2.3.0 1 pytorch conda torch 0.5.0a0 e9ad743 conda torchfile 0.1.0 conda torchnet 0.0.4 conda torchvision 0.2.1 pytorch used docker container pip version pip 18.0 opt conda lib python3.6 site packages pip python 3.6",1,multivariatenormal potrf slow gpu seems memory leak,"multivariatenormal potrf slow gpu seems memory leak issue description initialize multivariatenormal https github.com pytorch pytorch blob bb7d1837bc164b06e1d0826a20a8f7e8338a44e3 torch distributions multivariate normal.py l76 object covariance matrix provided batch dimension too, slow gpu, always slower gpu cpu. even seems leak memory without batch dimension cpu gpu, different intensity check n parameter code example . one strange thing uses cpu ram running gpu. digging bit deeper, experienced tensor's potrf function, probably root cause. code example outputs testing multivariatenormal testing potrf system info collecting environment information... pytorch version 0.5.0a0 e9ad743 debug build cuda used build pytorch 9.2.88 os ubuntu 18.04.1 lts gcc version ubuntu 7.3.0 16ubuntu3 7.3.0 cmake version version 3.12.0 python version 3.6 cuda available yes cuda runtime version 9.2.148 gpu models configuration gpu 0 geforce gtx 1080 ti gpu 1 geforce gtx 1080 ti gpu 2 geforce gtx 1080 ti nvidia driver version 396.54 cudnn version probably one following usr lib x86 64 linux gnu libcudnn.so.7.2.1 usr lib x86 64 linux gnu libcudnn static v7.a versions relevant libraries pip could collect conda magma cuda91 2.3.0 1 pytorch conda torch 0.5.0a0 e9ad743 conda torchfile 0.1.0 conda torchnet 0.0.4 conda torchvision 0.2.1 pytorch used docker container pip version pip 18.0 opt conda lib python3.6 site packages pip python 3.6"
pytorch,3665,"hi everyone, find slight memory leak training lstm networks. running environment torch 0.2.0.3 cuda 7.5 os ubuntu 16.04lts issue similar discuss https discuss.pytorch.org tracking suspected memory leak 1130 9 . following discussion, found disable cudnn adding solve problem speed affected either. quite confused memory leak using cudnn. results cudnn enabled, memory leak occurs every epoch. screenshot ! cudnn enable https user images.githubusercontent.com 9111828 32723137 558d6d6a c8a7 11e7 8934 644aeb1aa762.png results cudnn disabled, memory leak almost eleminated although happens first several epochs . screenshot ! cudnn disable https user images.githubusercontent.com 9111828 32723151 638edee4 c8a7 11e7 825b f6f604b7aaf2.png",1,slight memory leak lstm,"slight memory leak lstm hi everyone, find slight memory leak training lstm networks. running environment torch 0.2.0.3 cuda 7.5 os ubuntu 16.04lts issue similar discuss https discuss.pytorch.org tracking suspected memory leak 1130 9 . following discussion, found disable cudnn adding solve problem speed affected either. quite confused memory leak using cudnn. results cudnn enabled, memory leak occurs every epoch. screenshot ! cudnn enable https user images.githubusercontent.com 9111828 32723137 558d6d6a c8a7 11e7 8934 644aeb1aa762.png results cudnn disabled, memory leak almost eleminated although happens first several epochs . screenshot ! cudnn disable https user images.githubusercontent.com 9111828 32723151 638edee4 c8a7 11e7 825b f6f604b7aaf2.png"
pytorch,20053,"issue currently cpu https github.com pytorch pytorch blob master aten src aten accumulatetype.h l34 suggest small discussion whether better switch float. motivation prompted wanchaol asking https gist.github.com wanchaol aceb0a1e8d3a93c8853689730b0f709f error, think three possible reasons consider changing consistency gpu, support platforms float faster e.g. arm32 get rid ub. seem recall apaszke preferring current behaviour year ago back context 6855, always dispatched double auxilliary function cpu . pitch switch generally. alternatives switch specific platforms e.g. arm32 , somehow get rid ub warning. additional context e.g. ljk53 might interested changing android specifically generally.",1,rfc accscalar float cpu,"rfc accscalar float cpu issue currently cpu https github.com pytorch pytorch blob master aten src aten accumulatetype.h l34 suggest small discussion whether better switch float. motivation prompted wanchaol asking https gist.github.com wanchaol aceb0a1e8d3a93c8853689730b0f709f error, think three possible reasons consider changing consistency gpu, support platforms float faster e.g. arm32 get rid ub. seem recall apaszke preferring current behaviour year ago back context 6855, always dispatched double auxilliary function cpu . pitch switch generally. alternatives switch specific platforms e.g. arm32 , somehow get rid ub warning. additional context e.g. ljk53 might interested changing android specifically generally."
pytorch,28761,"bug slow passed h5py dataset. reproduce create new hdf5 file 1000x1000 float32 dataset load back tensor slow. however, dataset converted numpy array first, performs much faster resulting tensors equal. perhaps manual numpy array conversion avoids expensive conversion python nested list floats. cc vitalyfedyunin ngimel",1,torch.tensor slow passed h5py dataset.,"torch.tensor slow passed h5py dataset. bug slow passed h5py dataset. reproduce create new hdf5 file 1000x1000 float32 dataset load back tensor slow. however, dataset converted numpy array first, performs much faster resulting tensors equal. perhaps manual numpy array conversion avoids expensive conversion python nested list floats. cc vitalyfedyunin ngimel"
pytorch,23642,"bug latest change dataloader 19228 leads severe performance regression large scale training 30 . finally root cause theses change https github.com pytorch pytorch blob master torch utils data dataloader.py l889 l891. causes exit epoch additional 5 seconds. reproduce steps reproduce behavior expected behavior exit basically free pytorch 1.1, takes 5s pytorch 1.2. environment additional context suggest fix recover previous lines around https github.com pytorch pytorch blob master torch utils data dataloader.py l889 l891. example, following code fix problem",1,performance regression dataloader,"performance regression dataloader bug latest change dataloader 19228 leads severe performance regression large scale training 30 . finally root cause theses change https github.com pytorch pytorch blob master torch utils data dataloader.py l889 l891. causes exit epoch additional 5 seconds. reproduce steps reproduce behavior expected behavior exit basically free pytorch 1.1, takes 5s pytorch 1.2. environment additional context suggest fix recover previous lines around https github.com pytorch pytorch blob master torch utils data dataloader.py l889 l891. example, following code fix problem"
pytorch,25690,"two machines. one p100 another 4 k80s. found following code output differently two machines. means seeding make output dropout reproducible different devices. design? make output reproducible different devices, may trouble comparing different models. output p100 machine always output k80 machine always",1,dropout behaves differently different devices,"dropout behaves differently different devices two machines. one p100 another 4 k80s. found following code output differently two machines. means seeding make output dropout reproducible different devices. design? make output reproducible different devices, may trouble comparing different models. output p100 machine always output k80 machine always"
pytorch,10851,"issue description processing large amount output lines 100000 lines , profiler.table spend several minutes. think , new lines added end string, result copy operation whole string. best practise may using .",1,profiler.table slow,"profiler.table slow issue description processing large amount output lines 100000 lines , profiler.table spend several minutes. think , new lines added end string, result copy operation whole string. best practise may using ."
pytorch,22127,"bug reproduce cpu memory leak model forwarding loop define model simple write test program expected behavior look system monitor, related python process's memory stable. fact memory increase time. environment pytorch version 1.1 os linux osx installed pytorch python version 3.6 3.7 relevant information using gc.collect , increasing becomes slow, however help since want use lstm layers quickly memory additional context writing reinforcement learning program, every episode loops model forwardings. memory leak problem vital.",1,cpu memory leak model forwarding loop,"cpu memory leak model forwarding loop bug reproduce cpu memory leak model forwarding loop define model simple write test program expected behavior look system monitor, related python process's memory stable. fact memory increase time. environment pytorch version 1.1 os linux osx installed pytorch python version 3.6 3.7 relevant information using gc.collect , increasing becomes slow, however help since want use lstm layers quickly memory additional context writing reinforcement learning program, every episode loops model forwardings. memory leak problem vital."
pytorch,29809,"bug training cpu simple one layer cnn variable width batches, standard text classification , memory usage increases significantly call, quickly causes memory. reproduce running , get oom error 30 iterations memory profiler output https github.com pytorch pytorch files 3846948 memory profiler output.txt clearly shows steady memory increase never decrease call, 190mb per call average. expected behavior would expect memory usage increase slightly first step, stabilize increase linearly. environment pytorch version 1.3.1 cpu debug build cuda used build pytorch none os ubuntu 18.04.3 lts gcc version ubuntu 7.4.0 1ubuntu1 18.04.1 7.4.0 cmake version could collect python version 3.6 cuda available cuda runtime version cuda gpu models configuration cuda nvidia driver version cuda cudnn version cuda versions relevant libraries pip3 numpy 1.17.4 pip3 torch 1.3.1 cpu conda could collect additional context things tested, following advice multiples related issues found https github.com pytorch vision issues 984, https github.com pytorch pytorch issues 5285 others 1. check references model output kept accross iterations e.g. loss 2. remove anything possible narrow issue data loader, tqdm, backward, optimization, . kept forward pass dummy data. 3. use exact issue happens. 4. random fixed size tensors replacing , issue disappears. however, cnn text, unfortunately, need batches variable width... 5. remove line, issue disappears well, problem data generation itelf. 6. tried following pytorch versions exact issue pip pip conda cc ezyang gchanan zou3519 jerryzh168",1,memory leak conv1d cpu,"memory leak conv1d cpu bug training cpu simple one layer cnn variable width batches, standard text classification , memory usage increases significantly call, quickly causes memory. reproduce running , get oom error 30 iterations memory profiler output https github.com pytorch pytorch files 3846948 memory profiler output.txt clearly shows steady memory increase never decrease call, 190mb per call average. expected behavior would expect memory usage increase slightly first step, stabilize increase linearly. environment pytorch version 1.3.1 cpu debug build cuda used build pytorch none os ubuntu 18.04.3 lts gcc version ubuntu 7.4.0 1ubuntu1 18.04.1 7.4.0 cmake version could collect python version 3.6 cuda available cuda runtime version cuda gpu models configuration cuda nvidia driver version cuda cudnn version cuda versions relevant libraries pip3 numpy 1.17.4 pip3 torch 1.3.1 cpu conda could collect additional context things tested, following advice multiples related issues found https github.com pytorch vision issues 984, https github.com pytorch pytorch issues 5285 others 1. check references model output kept accross iterations e.g. loss 2. remove anything possible narrow issue data loader, tqdm, backward, optimization, . kept forward pass dummy data. 3. use exact issue happens. 4. random fixed size tensors replacing , issue disappears. however, cnn text, unfortunately, need batches variable width... 5. remove line, issue disappears well, problem data generation itelf. 6. tried following pytorch versions exact issue pip pip conda cc ezyang gchanan zou3519 jerryzh168"
pytorch,8818,"issue description running mpi backend cluster, job fails exit hangs unresponsively essentially node cluster runs script becomes unresponsive try terminate job. varies run run, happens often enough problematic. minimal reproduction script. high level steps 1. initialize multiple dataloader workers 2. communicate model parameters 3. update model parameters 4. run forward backward pass code example single machine gpus, run issues.py system info pytorch built source version 0.5.0a0 f8c18e0 debug build cuda used build pytorch 9.0.176 os ubuntu 16.04.4 lts gcc version ubuntu 5.5.0 12ubuntu1 16.04 5.5.0 20171010 cmake version version 3.11.1 python version 3.6 cuda available yes cuda runtime version 9.0.176 gpu models configuration gpu 0 8 v100 nvidia voltas nvidia driver version 384.81 cudnn version cudnn v7.0 cuda.9.0 versions relevant libraries openmpi 3.0.0 gcc.5.4.0 nccl 2.2.12 1 cuda.9.0 pip numpy 1.14.3 pip torch 0.5.0a0 f8c18e0 pip torchvision 0.2.1 conda magma cuda90 2.3.0 1 pytorch conda torch 0.5.0 conda torchvision 0.2.1 cc pietern mrshenli pritamdamania87 zhaojuanmao satgera rohan varma gqchen aazzolini osalpekar jiayisuse agolynski scipioneer h huang mrzzd",1,mpi causing job hang unresponsive external termination signals,"mpi causing job hang unresponsive external termination signals issue description running mpi backend cluster, job fails exit hangs unresponsively essentially node cluster runs script becomes unresponsive try terminate job. varies run run, happens often enough problematic. minimal reproduction script. high level steps 1. initialize multiple dataloader workers 2. communicate model parameters 3. update model parameters 4. run forward backward pass code example single machine gpus, run issues.py system info pytorch built source version 0.5.0a0 f8c18e0 debug build cuda used build pytorch 9.0.176 os ubuntu 16.04.4 lts gcc version ubuntu 5.5.0 12ubuntu1 16.04 5.5.0 20171010 cmake version version 3.11.1 python version 3.6 cuda available yes cuda runtime version 9.0.176 gpu models configuration gpu 0 8 v100 nvidia voltas nvidia driver version 384.81 cudnn version cudnn v7.0 cuda.9.0 versions relevant libraries openmpi 3.0.0 gcc.5.4.0 nccl 2.2.12 1 cuda.9.0 pip numpy 1.14.3 pip torch 0.5.0a0 f8c18e0 pip torchvision 0.2.1 conda magma cuda90 2.3.0 1 pytorch conda torch 0.5.0 conda torchvision 0.2.1 cc pietern mrshenli pritamdamania87 zhaojuanmao satgera rohan varma gqchen aazzolini osalpekar jiayisuse agolynski scipioneer h huang mrzzd"
pytorch,4211,similar 537. first call .cuda take long time. take even model.cuda . pytorch install conda . gpu gtx 1080 cuda9.0.,1,initial .cuda slow occur latest cuda9 version pytorch,initial .cuda slow occur latest cuda9 version pytorch similar 537. first call .cuda take long time. take even model.cuda . pytorch install conda . gpu gtx 1080 cuda9.0.
pytorch,23156,"bug niche bug, might cause troubles advanced users like use masking filter nan losses. simply put, nan losses masked using , performing sum losses produce valid gradients assuming gradient graph smooth everywhere except masked losses . reproduce steps reproduce behavior define environment follows. backpropagating gradients simple layer. performing forward inference linear layer propagating gradients works intended involved. suppose one rows. performing backwards computation sum matrix produce gradients. expected. however, using index slicing leaf nodes mask problematic gradient graphs not. check whether masking final losses works cases involved. accumulated gradient latter case less former index slicing computation graph prevented third row affecting gradient computation. environment collecting environment information... pytorch version 1.0.0 debug build cuda used build pytorch none os mac osx 10.14.5 gcc version could collect cmake version could collect python version 3.7 cuda available cuda runtime version cuda gpu models configuration cuda nvidia driver version cuda cudnn version cuda versions relevant libraries pip numpy 1.16.3 pip torch 1.0.0 conda torch 1.0.0 additional context none.",1,pruning nan values gradient graph still produces nan gradients.,"pruning nan values gradient graph still produces nan gradients. bug niche bug, might cause troubles advanced users like use masking filter nan losses. simply put, nan losses masked using , performing sum losses produce valid gradients assuming gradient graph smooth everywhere except masked losses . reproduce steps reproduce behavior define environment follows. backpropagating gradients simple layer. performing forward inference linear layer propagating gradients works intended involved. suppose one rows. performing backwards computation sum matrix produce gradients. expected. however, using index slicing leaf nodes mask problematic gradient graphs not. check whether masking final losses works cases involved. accumulated gradient latter case less former index slicing computation graph prevented third row affecting gradient computation. environment collecting environment information... pytorch version 1.0.0 debug build cuda used build pytorch none os mac osx 10.14.5 gcc version could collect cmake version could collect python version 3.7 cuda available cuda runtime version cuda gpu models configuration cuda nvidia driver version cuda cudnn version cuda versions relevant libraries pip numpy 1.16.3 pip torch 1.0.0 conda torch 1.0.0 additional context none."
pytorch,15482,"create cnn work images. network u net type segmentation, architecture saves intermediate calculations. leads conclusion small amount memory need either reduce size image, number layers, size batch. ! default https user images.githubusercontent.com 37583380 50356634 ccf4b080 0563 11e9 824a 44f7ef669087.png give network single image small size, memory becomes slightly larger, logical. ! default https user images.githubusercontent.com 37583380 50356532 5d7ec100 0563 11e9 95d2 9c7e27bd1558.png restart image, amount allocated memory change, occupied part memory cache increases. ! default https user images.githubusercontent.com 37583380 50356677 ed246f80 0563 11e9 8595 8eb10b651868.png error text runtimeerror traceback recent call last 1 test np.random.sample 1, 3, 1280, 1280 0.5 2 net torch.floattensor test .to device 3 out.shape e python anaconda lib site packages torch nn modules module.py call self, input, kwargs 487 result self. slow forward input, kwargs 488 else 489 result self.forward input, kwargs 490 hook self. forward hooks.values 491 hook result hook self, input, result forward self, x 50 x self.maxpool d1 51 52 d2 self.down2 x 53 x self.maxpool d2 54 e python anaconda lib site packages torch nn modules module.py call self, input, kwargs 487 result self. slow forward input, kwargs 488 else 489 result self.forward input, kwargs 490 hook self. forward hooks.values 491 hook result hook self, input, result e python anaconda lib site packages torch nn modules container.py forward self, input 90 def forward self, input 91 module self. modules.values 92 input module input 93 return input 94 e python anaconda lib site packages torch nn modules module.py call self, input, kwargs 487 result self. slow forward input, kwargs 488 else 489 result self.forward input, kwargs 490 hook self. forward hooks.values 491 hook result hook self, input, result e python anaconda lib site packages torch nn modules activation.py forward self, input 48 weak script method 49 def forward self, input 50 return f.threshold input, self.threshold, self.value, self.inplace 51 52 def extra repr self e python anaconda lib site packages torch nn functional.py threshold input, threshold, value, inplace 838 result vf.threshold input, threshold, value 839 else 840 result vf.threshold input, threshold, value 841 return result 842 runtimeerror cuda memory. tried allocate 100.00 mib gpu 0 2.00 gib total capacity 1.34 gib already allocated 5.20 mib free 12.75 mib cached large image give network image smaller get error ! default https user images.githubusercontent.com 37583380 50356722 2230c200 0564 11e9 8fba 54855c20de9f.png things memory become like ! default https user images.githubusercontent.com 37583380 50356785 67ed8a80 0564 11e9 8a9c 895b453410ba.png without restarting jupyter notebook work, memory clear out.",1,gpu allocated memory released,"gpu allocated memory released create cnn work images. network u net type segmentation, architecture saves intermediate calculations. leads conclusion small amount memory need either reduce size image, number layers, size batch. ! default https user images.githubusercontent.com 37583380 50356634 ccf4b080 0563 11e9 824a 44f7ef669087.png give network single image small size, memory becomes slightly larger, logical. ! default https user images.githubusercontent.com 37583380 50356532 5d7ec100 0563 11e9 95d2 9c7e27bd1558.png restart image, amount allocated memory change, occupied part memory cache increases. ! default https user images.githubusercontent.com 37583380 50356677 ed246f80 0563 11e9 8595 8eb10b651868.png error text runtimeerror traceback recent call last 1 test np.random.sample 1, 3, 1280, 1280 0.5 2 net torch.floattensor test .to device 3 out.shape e python anaconda lib site packages torch nn modules module.py call self, input, kwargs 487 result self. slow forward input, kwargs 488 else 489 result self.forward input, kwargs 490 hook self. forward hooks.values 491 hook result hook self, input, result forward self, x 50 x self.maxpool d1 51 52 d2 self.down2 x 53 x self.maxpool d2 54 e python anaconda lib site packages torch nn modules module.py call self, input, kwargs 487 result self. slow forward input, kwargs 488 else 489 result self.forward input, kwargs 490 hook self. forward hooks.values 491 hook result hook self, input, result e python anaconda lib site packages torch nn modules container.py forward self, input 90 def forward self, input 91 module self. modules.values 92 input module input 93 return input 94 e python anaconda lib site packages torch nn modules module.py call self, input, kwargs 487 result self. slow forward input, kwargs 488 else 489 result self.forward input, kwargs 490 hook self. forward hooks.values 491 hook result hook self, input, result e python anaconda lib site packages torch nn modules activation.py forward self, input 48 weak script method 49 def forward self, input 50 return f.threshold input, self.threshold, self.value, self.inplace 51 52 def extra repr self e python anaconda lib site packages torch nn functional.py threshold input, threshold, value, inplace 838 result vf.threshold input, threshold, value 839 else 840 result vf.threshold input, threshold, value 841 return result 842 runtimeerror cuda memory. tried allocate 100.00 mib gpu 0 2.00 gib total capacity 1.34 gib already allocated 5.20 mib free 12.75 mib cached large image give network image smaller get error ! default https user images.githubusercontent.com 37583380 50356722 2230c200 0564 11e9 8fba 54855c20de9f.png things memory become like ! default https user images.githubusercontent.com 37583380 50356785 67ed8a80 0564 11e9 8a9c 895b453410ba.png without restarting jupyter notebook work, memory clear out."
pytorch,2180,"bcewithlogitsloss, weights unnormalized used, classnllloss weights normalized used. discussion around found https github.com pytorch pytorch commit 67968cb60b1d3021834594967d4140a36a8213e3 commitcomment 23155134 . discrepancy expected? else issue send pr resolve this.",1,discrepancy bcewithlogitsloss classnllloss,"discrepancy bcewithlogitsloss classnllloss bcewithlogitsloss, weights unnormalized used, classnllloss weights normalized used. discussion around found https github.com pytorch pytorch commit 67968cb60b1d3021834594967d4140a36a8213e3 commitcomment 23155134 . discrepancy expected? else issue send pr resolve this."
pytorch,9873,"testing acceleration effect cpu decomposing convolution layers, found pytorch slow cannot utilize multicores cpu. output cpu usage 100 200 147.48149728775024 16.699654817581177 ! pytorch https user images.githubusercontent.com 10665923 43258302 eca30248 9104 11e8 8045 49d8d63819a4.png however, keras tf back end faster multi threaded. output cpu usage 800 1600 26.393059253692627 13.783706188201904 ! keras https user images.githubusercontent.com 10665923 43258314 f57cf11c 9104 11e8 8cfd c094a798df48.png system info pytorch 0.4 ubuntu 16.04 cc vitalyfedyunin ngimel",1,"pytorch slow using cpu, cannot utilize multicore cpu","pytorch slow using cpu, cannot utilize multicore cpu testing acceleration effect cpu decomposing convolution layers, found pytorch slow cannot utilize multicores cpu. output cpu usage 100 200 147.48149728775024 16.699654817581177 ! pytorch https user images.githubusercontent.com 10665923 43258302 eca30248 9104 11e8 8045 49d8d63819a4.png however, keras tf back end faster multi threaded. output cpu usage 800 1600 26.393059253692627 13.783706188201904 ! keras https user images.githubusercontent.com 10665923 43258314 f57cf11c 9104 11e8 8cfd c094a798df48.png system info pytorch 0.4 ubuntu 16.04 cc vitalyfedyunin ngimel"
pytorch,25243,possible cause nan loss pytorch1.2 load pretrained model trained pytorch0.4? pytorch1.2 support sync bn pytorch0.4 not. tested random init load pretrained model nan loss appear,1,load pretrained model trained torch0.4 got nan loss torch1.2,load pretrained model trained torch0.4 got nan loss torch1.2 possible cause nan loss pytorch1.2 load pretrained model trained pytorch0.4? pytorch1.2 support sync bn pytorch0.4 not. tested random init load pretrained model nan loss appear
pytorch,30365,"bug there's 150x gap performance torchscript ops versus straight python c . looping 100k numbers takes 2 seconds instead 18ms better. please see benchmarks https github.com divyekapoor ml op benchmarks reproduce https github.com divyekapoor ml op benchmarks steps reproduce behavior 1. clone repo 1. make torchbench see related tensorflow issue context https github.com tensorflow tensorflow issues 34500 expected behavior fizzbuzz iteration counts 100000 raw latency ms per run latency usec python multiplier c multiplier pytorch python 4007 40.07 222.61 23851 pytorch torchscript python loaded torchscript 2830 28.3 157.22 16845 pytorch torchscript c native 255 2.55 14.17 1518 pytorch torchscript c native aten tensors 252 2.52 14.00 1500 raw python 18 0.18 1.00 107 raw c 0.168 0.00168 0.01 1 performance similar raw python expected behavior. environment please copy paste output environment collection script https raw.githubusercontent.com pytorch pytorch master torch utils collect env.py fill checklist manually . get script run pytorch version e.g., 1.0 1.3 os e.g., linux mac os x installed pytorch , , source pip build command used compiling source na python version 3.7 cuda cudnn version na gpu models configuration na relevant information see performance tables github repo. additional context code cc suo",1,torchscript performance 150x gap torchscript native python,"torchscript performance 150x gap torchscript native python bug there's 150x gap performance torchscript ops versus straight python c . looping 100k numbers takes 2 seconds instead 18ms better. please see benchmarks https github.com divyekapoor ml op benchmarks reproduce https github.com divyekapoor ml op benchmarks steps reproduce behavior 1. clone repo 1. make torchbench see related tensorflow issue context https github.com tensorflow tensorflow issues 34500 expected behavior fizzbuzz iteration counts 100000 raw latency ms per run latency usec python multiplier c multiplier pytorch python 4007 40.07 222.61 23851 pytorch torchscript python loaded torchscript 2830 28.3 157.22 16845 pytorch torchscript c native 255 2.55 14.17 1518 pytorch torchscript c native aten tensors 252 2.52 14.00 1500 raw python 18 0.18 1.00 107 raw c 0.168 0.00168 0.01 1 performance similar raw python expected behavior. environment please copy paste output environment collection script https raw.githubusercontent.com pytorch pytorch master torch utils collect env.py fill checklist manually . get script run pytorch version e.g., 1.0 1.3 os e.g., linux mac os x installed pytorch , , source pip build command used compiling source na python version 3.7 cuda cudnn version na gpu models configuration na relevant information see performance tables github repo. additional context code cc suo"
pytorch,20125,"bug aws p3.16xlarge machine, using mixed precision training https github.com nvidia megatron lm checked https github.com cybertronai transformer xl run adaptive softmax https github.com cybertronai transformer xl blob master mem transformer.py l477 8 gpus apex mixed precision https github.com cybertronai transformer xl blob master fp16 opt.py , nccl hangs. remove , works expected reducing batch size avoid oom . remove also works fine, definitely mixed precision issue. sorry dup 11672 able figure out. reproduce steps reproduce behavior p3.16xlarge logs https github.com nvidia nccl files 3144052 combined.txt found figure coming from? something code account bug ddp apex? expected behavior hang silently. environment please copy paste output environment collection script https raw.githubusercontent.com pytorch pytorch master torch utils collect env.py fill checklist manually . additional context",1,nccl hang pytorch distributed data parallel mixed precision training,"nccl hang pytorch distributed data parallel mixed precision training bug aws p3.16xlarge machine, using mixed precision training https github.com nvidia megatron lm checked https github.com cybertronai transformer xl run adaptive softmax https github.com cybertronai transformer xl blob master mem transformer.py l477 8 gpus apex mixed precision https github.com cybertronai transformer xl blob master fp16 opt.py , nccl hangs. remove , works expected reducing batch size avoid oom . remove also works fine, definitely mixed precision issue. sorry dup 11672 able figure out. reproduce steps reproduce behavior p3.16xlarge logs https github.com nvidia nccl files 3144052 combined.txt found figure coming from? something code account bug ddp apex? expected behavior hang silently. environment please copy paste output environment collection script https raw.githubusercontent.com pytorch pytorch master torch utils collect env.py fill checklist manually . additional context"
pytorch,160,"following code outputs 4, 0.4, 4 instead 4, 0.4, 0.04 seem errors loss functions well, like smoothl1loss.",1,loss functions return wrong values,"loss functions return wrong values following code outputs 4, 0.4, 4 instead 4, 0.4, 0.04 seem errors loss functions well, like smoothl1loss."
pytorch,5801,"pytorch version '0.3.0' code without cuda, gradients loss cuda gradients returned kldivloss scaled number class, loss same.",1,kldivloss behaves differently cpu gpu,"kldivloss behaves differently cpu gpu pytorch version '0.3.0' code without cuda, gradients loss cuda gradients returned kldivloss scaled number class, loss same."
pytorch,26797,"bug switching cudnn backend ctcloss since fully reproducible, however, turns exact model used work pytorch's cuda backend ctc loss failed. simple example, found there's huge difference gradient direction magnitude two backends. sure bug pytorch cudnn, far know, tensorflow also used ctc cudnn similar issue. thanks advance. reproduce behavior environment cc ezyang gchanan zou3519",1,inconsistent gradient different backend ctcloss,"inconsistent gradient different backend ctcloss bug switching cudnn backend ctcloss since fully reproducible, however, turns exact model used work pytorch's cuda backend ctc loss failed. simple example, found there's huge difference gradient direction magnitude two backends. sure bug pytorch cudnn, far know, tensorflow also used ctc cudnn similar issue. thanks advance. reproduce behavior environment cc ezyang gchanan zou3519"
pytorch,20146,"bug using weight decay libtorch, cpu memory usage slowly increasing. reproduce used docker container nvidia cuda 9.2 cudnn7 devel ubuntu18.04 , stable libtorch 1.1 cuda9.0. phenomenon reproduced using mnist example pytorch example repository. rewrite examples cpp mnist mnist.cpp l.148 speed increase slow, may better increase number epochs. happens cpu learning gpu learning. environment os ubuntu 18.04.2 lts gcc version ubuntu 7.3.0 27ubuntu1 18.04 7.3.0 cmake version version 3.10.2 cuda runtime version 9.2.148 gpu models configuration gpu 0 geforce rtx 2080 ti nvidia driver version 410.104 cudnn version usr lib x86 64 linux gnu libcudnn.so.7.4.1 additional context phenomenon happens also cuda10.0 libtorch nightly build cuda10.0.",1,cpu memory leak using weight decay libtorch,"cpu memory leak using weight decay libtorch bug using weight decay libtorch, cpu memory usage slowly increasing. reproduce used docker container nvidia cuda 9.2 cudnn7 devel ubuntu18.04 , stable libtorch 1.1 cuda9.0. phenomenon reproduced using mnist example pytorch example repository. rewrite examples cpp mnist mnist.cpp l.148 speed increase slow, may better increase number epochs. happens cpu learning gpu learning. environment os ubuntu 18.04.2 lts gcc version ubuntu 7.3.0 27ubuntu1 18.04 7.3.0 cmake version version 3.10.2 cuda runtime version 9.2.148 gpu models configuration gpu 0 geforce rtx 2080 ti nvidia driver version 410.104 cudnn version usr lib x86 64 linux gnu libcudnn.so.7.4.1 additional context phenomenon happens also cuda10.0 libtorch nightly build cuda10.0."
pytorch,5812,"os ubuntu16.04 pytorch version 0.3.1 pytorchnet version 0.0.1 installed pytorch conda, pip, source conda python version 3.6.4 cuda cudnn version cuda 9.0.176 cudnn 7.0.5.15 script reproduce bug error messages set 64, epoch 1, ! screenshot 2018 03 15 20 23 31 https user images.githubusercontent.com 9991443 37463453 30928920 2890 11e8 93d0 d0dbad043997.png , could see memory grow 3 times ! screenshot 2018 03 15 20 24 00 https user images.githubusercontent.com 9991443 37463507 5f8d2992 2890 11e8 9f7d b6a754314c6a.png furthermore, change 1000, epoch 1, , could see memory use 715mb , strange , could see memory change",1,memory leaky dataloader,"memory leaky dataloader os ubuntu16.04 pytorch version 0.3.1 pytorchnet version 0.0.1 installed pytorch conda, pip, source conda python version 3.6.4 cuda cudnn version cuda 9.0.176 cudnn 7.0.5.15 script reproduce bug error messages set 64, epoch 1, ! screenshot 2018 03 15 20 23 31 https user images.githubusercontent.com 9991443 37463453 30928920 2890 11e8 93d0 d0dbad043997.png , could see memory grow 3 times ! screenshot 2018 03 15 20 24 00 https user images.githubusercontent.com 9991443 37463507 5f8d2992 2890 11e8 9f7d b6a754314c6a.png furthermore, change 1000, epoch 1, , could see memory use 715mb , strange , could see memory change"
pytorch,12501,"bug hi, trying obtain model prediction gpu running another piece code parallel cpu . since streaming data, instantiate separate std thread std async every time call workspace runnet, causes gpu memory leak noticeable unless streaming data. however, thread used maintained worker thread thread id , leak occur. thank you, reproduce steps reproduce behavior 1. given pre trained model, please use following sample code reproduce issue expected behavior expecting memory leak executing threads different thread ids. environment additional context",1,c calling workspace runnet prediction different thread time causes gpu memory leak,"c calling workspace runnet prediction different thread time causes gpu memory leak bug hi, trying obtain model prediction gpu running another piece code parallel cpu . since streaming data, instantiate separate std thread std async every time call workspace runnet, causes gpu memory leak noticeable unless streaming data. however, thread used maintained worker thread thread id , leak occur. thank you, reproduce steps reproduce behavior 1. given pre trained model, please use following sample code reproduce issue expected behavior expecting memory leak executing threads different thread ids. environment additional context"
pytorch,19163,"context use case es small network multiprocessing. turns becomes extremely slow cpu. reproduce effect, code attached benchmark completely frozen waiting minutes, shows 80 cores cpu 100 busy",1,layernorm slow almost frozen cpu multiprocessing,"layernorm slow almost frozen cpu multiprocessing context use case es small network multiprocessing. turns becomes extremely slow cpu. reproduce effect, code attached benchmark completely frozen waiting minutes, shows 80 cores cpu 100 busy"
pytorch,1253,"may related issue 1184 believe something else working version , using micro batches split mini batches smaller batches calculate backprop loss micro batch, accumulating mini batch. noticed multiple times code goes memory several micro batches. seems strange micro batch require amount memory. maybe something fully understand here, anyone explain me, happy ! anyway, tried look bit attached smallest code snippet came demonstrating problem around 100 lines . essentially, training network images, using micro batches reduce memory. however, first sometimes second micro batch, memory usage increases stays micro batches. using functional approach mini micro batches, memory usage increase way, believe fact may 'lazy' memory freeing involved freeing necessary . torch memory.zip https github.com pytorch pytorch files 919384 torch memory.zip",1,memory usage increasing first batch possibly freeing potentially free memory,"memory usage increasing first batch possibly freeing potentially free memory may related issue 1184 believe something else working version , using micro batches split mini batches smaller batches calculate backprop loss micro batch, accumulating mini batch. noticed multiple times code goes memory several micro batches. seems strange micro batch require amount memory. maybe something fully understand here, anyone explain me, happy ! anyway, tried look bit attached smallest code snippet came demonstrating problem around 100 lines . essentially, training network images, using micro batches reduce memory. however, first sometimes second micro batch, memory usage increases stays micro batches. using functional approach mini micro batches, memory usage increase way, believe fact may 'lazy' memory freeing involved freeing necessary . torch memory.zip https github.com pytorch pytorch files 919384 torch memory.zip"
pytorch,5351,minimal example output environment,1,wrong automatic gradient linear layer autograd gradcheck,wrong automatic gradient linear layer autograd gradcheck minimal example output environment
pytorch,29429,"bug version torch 1.3.1 torchvision 0.4.1 notes nllloss reduce true seem work float16. also, training model loss1 float16 seem decrease loss. model trains fine loss1 float32 possible related 14878 code reproduce",1,nllloss reduce true returning nan float16,"nllloss reduce true returning nan float16 bug version torch 1.3.1 torchvision 0.4.1 notes nllloss reduce true seem work float16. also, training model loss1 float16 seem decrease loss. model trains fine loss1 float32 possible related 14878 code reproduce"
pytorch,13045,"bug number machines threadripper cpus, 2 nvidia gpus, 1070ti cards 1080 1080ti one titanxp, displayed behavior, switching using data parallel, training would fail, i.e. accuracy would go up. first saw code base, also happens imagnet example pytorch examples repo reproduce steps reproduce behavior 1. run imagnet example examples repo pytorch dataparallel error messages found dmesg log 1118468.873266 nvidia 0000 0a 00.0 amd vi event logged io page fault domain 0x000f address 0x00000000ea13a000 flags 0x0020 1118468.942145 nvidia 0000 0a 00.0 amd vi event logged io page fault domain 0x000f address 0x00000000ea139068 flags 0x0020 1118468.942189 nvidia 0000 0a 00.0 amd vi event logged io page fault domain 0x000f address 0x00000000d0000040 flags 0x0020 1118468.942227 nvidia 0000 0a 00.0 amd vi event logged io page fault domain 0x000f address 0x00000000d00007c0 flags 0x0020 1118468.942265 nvidia 0000 0a 00.0 amd vi event logged io page fault domain 0x000f address 0x00000000d0001040 flags 0x0020 1118468.942303 nvidia 0000 0a 00.0 amd vi event logged io page fault domain 0x000f address 0x00000000d0000f40 flags 0x0020 1118468.942340 nvidia 0000 0a 00.0 amd vi event logged io page fault domain 0x000f address 0x00000000d00016c0 flags 0x0020 1118468.942377 nvidia 0000 0a 00.0 amd vi event logged io page fault domain 0x000f address 0x00000000d0002040 flags 0x0020 1118468.942414 nvidia 0000 0a 00.0 amd vi event logged io page fault domain 0x000f address 0x00000000d0001e40 flags 0x0020 1118468.942452 nvidia 0000 0a 00.0 amd vi event logged io page fault domain 0x000f address 0x00000000d00025c0 flags 0x0020 1118468.942489 amd vi event logged io page fault device 0a 00.0 domain 0x000f address 0x00000000d0003040 flags 0x0020 1118468.942525 amd vi event logged io page fault device 0a 00.0 domain 0x000f address 0x00000000d0002d40 flags 0x0020 1118468.942560 amd vi event logged io page fault device 0a 00.0 domain 0x000f address 0x00000000d00034c0 flags 0x0020 1118468.942596 amd vi event logged io page fault device 0a 00.0 domain 0x000f address 0x00000000d0004040 flags 0x0020 1118468.942632 amd vi event logged io page fault device 0a 00.0 domain 0x000f address 0x00000000d0003c40 flags 0x0020 1118468.942667 amd vi event logged io page fault device 0a 00.0 domain 0x000f address 0x00000000d00043c0 flags 0x0020 1118468.942703 amd vi event logged io page fault device 0a 00.0 domain 0x000f address 0x00000000d0005040 flags 0x0020 1118468.942739 amd vi event logged io page fault device 0a 00.0 domain 0x000f address 0x00000000d0004b40 flags 0x0020 1118468.942774 amd vi event logged io page fault device 0a 00.0 domain 0x000f address 0x00000000d00052c0 flags 0x0020 expected behavior accuracy would pick cases, never picked validation set. managed work around problem turning iommu bios. environment pytorch version 0.4.1 debug build cuda used build pytorch 9.0.176 os ubuntu 16.04.5 lts gcc version ubuntu 5.4.0 6ubuntu1 16.04.10 5.4.0 20160609 cmake version version 3.5.1 python version 2.7 cuda available yes cuda runtime version could collect gpu models configuration gpu 0 geforce gtx 1080 ti gpu 1 geforce gtx 1080 ti nvidia driver version 384.130 cudnn version probably one following usr local cuda 9.0 targets x86 64 linux lib libcudnn.so.7.0.5 usr local cuda 9.0 targets x86 64 linux lib libcudnn static.a",1,dataparallel working nvidia gpus amd cpus,"dataparallel working nvidia gpus amd cpus bug number machines threadripper cpus, 2 nvidia gpus, 1070ti cards 1080 1080ti one titanxp, displayed behavior, switching using data parallel, training would fail, i.e. accuracy would go up. first saw code base, also happens imagnet example pytorch examples repo reproduce steps reproduce behavior 1. run imagnet example examples repo pytorch dataparallel error messages found dmesg log 1118468.873266 nvidia 0000 0a 00.0 amd vi event logged io page fault domain 0x000f address 0x00000000ea13a000 flags 0x0020 1118468.942145 nvidia 0000 0a 00.0 amd vi event logged io page fault domain 0x000f address 0x00000000ea139068 flags 0x0020 1118468.942189 nvidia 0000 0a 00.0 amd vi event logged io page fault domain 0x000f address 0x00000000d0000040 flags 0x0020 1118468.942227 nvidia 0000 0a 00.0 amd vi event logged io page fault domain 0x000f address 0x00000000d00007c0 flags 0x0020 1118468.942265 nvidia 0000 0a 00.0 amd vi event logged io page fault domain 0x000f address 0x00000000d0001040 flags 0x0020 1118468.942303 nvidia 0000 0a 00.0 amd vi event logged io page fault domain 0x000f address 0x00000000d0000f40 flags 0x0020 1118468.942340 nvidia 0000 0a 00.0 amd vi event logged io page fault domain 0x000f address 0x00000000d00016c0 flags 0x0020 1118468.942377 nvidia 0000 0a 00.0 amd vi event logged io page fault domain 0x000f address 0x00000000d0002040 flags 0x0020 1118468.942414 nvidia 0000 0a 00.0 amd vi event logged io page fault domain 0x000f address 0x00000000d0001e40 flags 0x0020 1118468.942452 nvidia 0000 0a 00.0 amd vi event logged io page fault domain 0x000f address 0x00000000d00025c0 flags 0x0020 1118468.942489 amd vi event logged io page fault device 0a 00.0 domain 0x000f address 0x00000000d0003040 flags 0x0020 1118468.942525 amd vi event logged io page fault device 0a 00.0 domain 0x000f address 0x00000000d0002d40 flags 0x0020 1118468.942560 amd vi event logged io page fault device 0a 00.0 domain 0x000f address 0x00000000d00034c0 flags 0x0020 1118468.942596 amd vi event logged io page fault device 0a 00.0 domain 0x000f address 0x00000000d0004040 flags 0x0020 1118468.942632 amd vi event logged io page fault device 0a 00.0 domain 0x000f address 0x00000000d0003c40 flags 0x0020 1118468.942667 amd vi event logged io page fault device 0a 00.0 domain 0x000f address 0x00000000d00043c0 flags 0x0020 1118468.942703 amd vi event logged io page fault device 0a 00.0 domain 0x000f address 0x00000000d0005040 flags 0x0020 1118468.942739 amd vi event logged io page fault device 0a 00.0 domain 0x000f address 0x00000000d0004b40 flags 0x0020 1118468.942774 amd vi event logged io page fault device 0a 00.0 domain 0x000f address 0x00000000d00052c0 flags 0x0020 expected behavior accuracy would pick cases, never picked validation set. managed work around problem turning iommu bios. environment pytorch version 0.4.1 debug build cuda used build pytorch 9.0.176 os ubuntu 16.04.5 lts gcc version ubuntu 5.4.0 6ubuntu1 16.04.10 5.4.0 20160609 cmake version version 3.5.1 python version 2.7 cuda available yes cuda runtime version could collect gpu models configuration gpu 0 geforce gtx 1080 ti gpu 1 geforce gtx 1080 ti nvidia driver version 384.130 cudnn version probably one following usr local cuda 9.0 targets x86 64 linux lib libcudnn.so.7.0.5 usr local cuda 9.0 targets x86 64 linux lib libcudnn static.a"
pytorch,30968,bug categorical.sample shape calls shape times . slow want get many samples large number classes. reproduce steps reproduce behavior outputs expected behavior slower single call torch.multinomial outputs environment pytorch version 1.3.1 debug build cuda used build pytorch 10.0.130 os ubuntu 18.04.1 lts gcc version ubuntu 7.4.0 1ubuntu1 18.04.1 7.4.0 cmake version version 3.10.2 python version 3.7 cuda available yes cuda runtime version 10.0.130 gpu models configuration gpu 0 quadro gp100 gpu 1 quadro gp100 nvidia driver version 410.79 cudnn version could collect versions relevant libraries pip numpy 1.16.4 pip numpysane 0.17 pip torch 1.3.1 conda blas 1.0 mkl conda faiss gpu 1.6.0 py37h1a5d453 0 pytorch conda mkl 2019.4 intel 243 intel conda mkl fft 1.0.12 py37ha843d7b 0 conda mkl random 1.0.2 py37hd81dba3 0 conda pytorch 1.3.1 py3.7 cuda10.0.130 cudnn7.6.3 0 pytorch cc vincentqb fritzo neerajprad alicanb vishwakftw,1,categorical.sample slow,categorical.sample slow bug categorical.sample shape calls shape times . slow want get many samples large number classes. reproduce steps reproduce behavior outputs expected behavior slower single call torch.multinomial outputs environment pytorch version 1.3.1 debug build cuda used build pytorch 10.0.130 os ubuntu 18.04.1 lts gcc version ubuntu 7.4.0 1ubuntu1 18.04.1 7.4.0 cmake version version 3.10.2 python version 3.7 cuda available yes cuda runtime version 10.0.130 gpu models configuration gpu 0 quadro gp100 gpu 1 quadro gp100 nvidia driver version 410.79 cudnn version could collect versions relevant libraries pip numpy 1.16.4 pip numpysane 0.17 pip torch 1.3.1 conda blas 1.0 mkl conda faiss gpu 1.6.0 py37h1a5d453 0 pytorch conda mkl 2019.4 intel 243 intel conda mkl fft 1.0.12 py37ha843d7b 0 conda mkl random 1.0.2 py37hd81dba3 0 conda pytorch 1.3.1 py3.7 cuda10.0.130 cudnn7.6.3 0 pytorch cc vincentqb fritzo neerajprad alicanb vishwakftw
pytorch,27902,"bug pytorch appears issue threads causes properly warm models executed different threads succession. using multi instances model gpu, provide speedup cases even slower single instance. reproduce steps reproduce behavior simple libtorch code snippet reproduces issue. short run model resnet50 model torchvision traced produce torchscript version . code run titian v batch size 1. 1. create model main thread 2. launch thread runs model n times loop, reporting runtime time 3. launch n threads run model once, reporting runtime time expected behavior environment debug build cuda used build pytorch 10.1.243 os ubuntu 18.04.3 lts gcc version ubuntu 7.4.0 1ubuntu1 18.04.1 7.4.0 cmake version version 3.14.0 python version 3.6 cuda available cuda runtime version 10.1.243 gpu models configuration could collect nvidia driver version could collect cudnn version usr lib x86 64 linux gnu libcudnn.so.7.6.3 versions relevant libraries pip msgpack numpy 0.4.3.2 pip numpy 1.16.4 pip torch 1.2.0a0 afb7a16 pip torchtext 0.4.0 pip torchvision 0.3.0a0 conda magma cuda100 2.5.0 1 local conda mkl 2019.1 144 conda mkl include 2019.1 144 conda nomkl 3.0 0 conda torch 1.2.0a0 afb7a16 pypi 0 pypi conda torchtext 0.4.0 pypi 0 pypi conda torchvision 0.3.0a0 pypi 0 pypi cc ezyang gchanan zou3519 suo",1,slowdown due thread specific model caching,"slowdown due thread specific model caching bug pytorch appears issue threads causes properly warm models executed different threads succession. using multi instances model gpu, provide speedup cases even slower single instance. reproduce steps reproduce behavior simple libtorch code snippet reproduces issue. short run model resnet50 model torchvision traced produce torchscript version . code run titian v batch size 1. 1. create model main thread 2. launch thread runs model n times loop, reporting runtime time 3. launch n threads run model once, reporting runtime time expected behavior environment debug build cuda used build pytorch 10.1.243 os ubuntu 18.04.3 lts gcc version ubuntu 7.4.0 1ubuntu1 18.04.1 7.4.0 cmake version version 3.14.0 python version 3.6 cuda available cuda runtime version 10.1.243 gpu models configuration could collect nvidia driver version could collect cudnn version usr lib x86 64 linux gnu libcudnn.so.7.6.3 versions relevant libraries pip msgpack numpy 0.4.3.2 pip numpy 1.16.4 pip torch 1.2.0a0 afb7a16 pip torchtext 0.4.0 pip torchvision 0.3.0a0 conda magma cuda100 2.5.0 1 local conda mkl 2019.1 144 conda mkl include 2019.1 144 conda nomkl 3.0 0 conda torch 1.2.0a0 afb7a16 pypi 0 pypi conda torchtext 0.4.0 pypi 0 pypi conda torchvision 0.3.0a0 pypi 0 pypi cc ezyang gchanan zou3519 suo"
pytorch,18175,"found sentence https caffe2.ai docs distributed training.html assuming successfully built caffe2 system least one gpu, preferably test distributed features. since experiments cpu performance, wonder use distributed training cpu cluster?",1,caffe2 caffe2 support distributed training cpu?,"caffe2 caffe2 support distributed training cpu? found sentence https caffe2.ai docs distributed training.html assuming successfully built caffe2 system least one gpu, preferably test distributed features. since experiments cpu performance, wonder use distributed training cpu cluster?"
pytorch,5388,tensor variable merge perf regression indexing 1 tensor 6a2afe3 tensor variable merge note indexing python list even faster affects things like https github.com pytorch examples blob 4ef2d4d0c8524372d0047e050065edcac665ce1a word language model data.py l39 l46,1,perf regression indexing 1 tensor,perf regression indexing 1 tensor tensor variable merge perf regression indexing 1 tensor 6a2afe3 tensor variable merge note indexing python list even faster affects things like https github.com pytorch examples blob 4ef2d4d0c8524372d0047e050065edcac665ce1a word language model data.py l39 l46
pytorch,26386,model trained fp32. try use .half change layers inputs fp16. actuallyit indeed accelerate inference. acceleration effect far away twice fp32. platform nvidia tx2. compute capability 6.2. supports fp16 well. want ask whether fp16 cannot twice fast fp32 pytorch. looking reply.thank you.,1,pytorch inference fp16,pytorch inference fp16 model trained fp32. try use .half change layers inputs fp16. actuallyit indeed accelerate inference. acceleration effect far away twice fp32. platform nvidia tx2. compute capability 6.2. supports fp16 well. want ask whether fp16 cannot twice fast fp32 pytorch. looking reply.thank you.
pytorch,27926,"feature single scalar increments 1 every time called layer set true. current implementation stores single element buffer resides device rest parameters buffers. request update storage moved host, despite residence rest parameters buffers. motivation bn layer accelerators gpus , every forward call updates triggers single element kernel launch, introducing unnecessary host overhead could hurt end 2 end perf cpu bounded workload. last attempt move host device gives 0 11 performance gain across common problem sizes. 26550 pitch need way would resides device backward compatible save load modules. involves relaxing checks python tests, assumes values buffers parameters passed reference. alternatives implementation 26550 give full backward compatibility failing tests loading reference know easily without big hammer rewriting inherited . support save load module, well allow assignment using tensor scalar.",1,num batches tracked update batchnorm forward single scalar update host regardless residence layer,"num batches tracked update batchnorm forward single scalar update host regardless residence layer feature single scalar increments 1 every time called layer set true. current implementation stores single element buffer resides device rest parameters buffers. request update storage moved host, despite residence rest parameters buffers. motivation bn layer accelerators gpus , every forward call updates triggers single element kernel launch, introducing unnecessary host overhead could hurt end 2 end perf cpu bounded workload. last attempt move host device gives 0 11 performance gain across common problem sizes. 26550 pitch need way would resides device backward compatible save load modules. involves relaxing checks python tests, assumes values buffers parameters passed reference. alternatives implementation 26550 give full backward compatibility failing tests loading reference know easily without big hammer rewriting inherited . support save load module, well allow assignment using tensor scalar."
pytorch,3863,"model multiple outputs and, therefore, multiple losses. training accumulate losses using retain graph. something along lines input, output target dictionaries respective data different inputs losses. using adam optimization. noticed number epochs, running time epoch goes suddenly 7sec 34sec. also noticed slowdown cpu usage computer test yet gpu . memory usage seem increase. profiled code saw output cprofile normal epoch slow epoch tested adaptive losses like adagrad, can't see issue. seems related line code adam.step ideas happening? seems like suddenly size accumulated gradient explodes, can't see why. cc vincentqb vitalyfedyunin ngimel",1,considerable slowdown adam.step number epochs multiple losses,"considerable slowdown adam.step number epochs multiple losses model multiple outputs and, therefore, multiple losses. training accumulate losses using retain graph. something along lines input, output target dictionaries respective data different inputs losses. using adam optimization. noticed number epochs, running time epoch goes suddenly 7sec 34sec. also noticed slowdown cpu usage computer test yet gpu . memory usage seem increase. profiled code saw output cprofile normal epoch slow epoch tested adaptive losses like adagrad, can't see issue. seems related line code adam.step ideas happening? seems like suddenly size accumulated gradient explodes, can't see why. cc vincentqb vitalyfedyunin ngimel"
pytorch,4893,"recently profiled one models surprised find softmax layers attention mechanism expensive entries column. original code looked something like experimentation, tried adding transposes code increased overall model speed around 10 , matrix multiplication top would expect . could considered performance bug? wonder way softmax cuda code comparable speed regardless softmax dimension. sorry sample code moment actual code deeply embedded current project doing. system info os linux pytorch version 0.3.0 installed pytorch conda, pip, source conda python version 3.6 cuda cudnn version cuda 9 gpu models configuration k80",1,gpu softmax last dimension 3d tensor slow,"gpu softmax last dimension 3d tensor slow recently profiled one models surprised find softmax layers attention mechanism expensive entries column. original code looked something like experimentation, tried adding transposes code increased overall model speed around 10 , matrix multiplication top would expect . could considered performance bug? wonder way softmax cuda code comparable speed regardless softmax dimension. sorry sample code moment actual code deeply embedded current project doing. system info os linux pytorch version 0.3.0 installed pytorch conda, pip, source conda python version 3.6 cuda cudnn version cuda 9 gpu models configuration k80"
pytorch,3869,"recent project using elegant tool,pytorch. use dropout2d batchnorm2d network.the trained model performance well test phase using model.eval . however,when use model.eval , result pool. really feel confused.i sincerely hope developer help solving confusion. thank much!",1,dropout2d batchnorm2d's model.eval result poor,"dropout2d batchnorm2d's model.eval result poor recent project using elegant tool,pytorch. use dropout2d batchnorm2d network.the trained model performance well test phase using model.eval . however,when use model.eval , result pool. really feel confused.i sincerely hope developer help solving confusion. thank much!"
pytorch,5406,sample output also master,1,batchnorm behaves different train eval,batchnorm behaves different train eval sample output also master
pytorch,18722,"use data data iter vgg16 network, pytorch's loss beame 12.3 keras's loss became 4.2 pytorch loss log train loss 15.2963 val loss 8.0108 train loss 12.4626 val loss 9.5972 train loss 12.4143 val loss 6.9568 train loss 12.3993 val loss 8.3055 train loss 12.3956 val loss 6.8054 keras loss epoch 1 20 402 402 183s 455ms step loss 11.6960 epoch 2 20 402 402 181s 449ms step loss 6.6525 epoch 3 20 402 402 181s 450ms step loss 5.4451 epoch 4 20 402 402 186s 463ms step loss 4.9515 epoch 5 20 402 402 186s 462ms step loss 4.6679 epoch 6 20 402 402 186s 462ms step loss 4.4298 main code fallows pytorch model nn.sequential list make model 'vgg16', num classes 1, pretrained true, input size input size .children 0 , nn.adaptiveavgpool2d 1 , flatten ,nn.sequential nn.linear 512, 1024, bias true ,nn.relu , nn.linear 1024, 1, bias true .cuda dataloaders 'train' train dl, 'val' valid dl train model model, dataloaders, nn.l1loss , optim.adam model.parameters ,lr 0.001 , 21 keras base model vgg16 weights 'imagenet', include top false, input shape input shape 1 , input shape 0 , 3 model globalaveragepooling2d base model.output model dense 1024, activation 'relu' model model dense 1, activation 'linear' model model model inputs base model.input, outputs model model base model keras.callbacks.earlystopping monitor 'val loss', patience 0, verbose 0, mode 'auto' model.compile keras.optimizers.adam 0.001 , 'mape', metrics 'acc', 'mae', 'mse' model.fit generator train iter, len train bs 1, 20,validation data valid iter, validation steps len valid bs, callbacks early stop",1,pytorch can't good keras,"pytorch can't good keras use data data iter vgg16 network, pytorch's loss beame 12.3 keras's loss became 4.2 pytorch loss log train loss 15.2963 val loss 8.0108 train loss 12.4626 val loss 9.5972 train loss 12.4143 val loss 6.9568 train loss 12.3993 val loss 8.3055 train loss 12.3956 val loss 6.8054 keras loss epoch 1 20 402 402 183s 455ms step loss 11.6960 epoch 2 20 402 402 181s 449ms step loss 6.6525 epoch 3 20 402 402 181s 450ms step loss 5.4451 epoch 4 20 402 402 186s 463ms step loss 4.9515 epoch 5 20 402 402 186s 462ms step loss 4.6679 epoch 6 20 402 402 186s 462ms step loss 4.4298 main code fallows pytorch model nn.sequential list make model 'vgg16', num classes 1, pretrained true, input size input size .children 0 , nn.adaptiveavgpool2d 1 , flatten ,nn.sequential nn.linear 512, 1024, bias true ,nn.relu , nn.linear 1024, 1, bias true .cuda dataloaders 'train' train dl, 'val' valid dl train model model, dataloaders, nn.l1loss , optim.adam model.parameters ,lr 0.001 , 21 keras base model vgg16 weights 'imagenet', include top false, input shape input shape 1 , input shape 0 , 3 model globalaveragepooling2d base model.output model dense 1024, activation 'relu' model model dense 1, activation 'linear' model model model inputs base model.input, outputs model model base model keras.callbacks.earlystopping monitor 'val loss', patience 0, verbose 0, mode 'auto' model.compile keras.optimizers.adam 0.001 , 'mape', metrics 'acc', 'mae', 'mse' model.fit generator train iter, len train bs 1, 20,validation data valid iter, validation steps len valid bs, callbacks early stop"
pytorch,17703,"training hangs using distributeddataparallel two pod nodes. problem occasionally happens reproduced time. pods started k8s using flannel network. physical nodes connected 10ge ethernet. training started normally, hanged middle training process. ! image https user images.githubusercontent.com 41627739 53853400 4860ec80 4000 11e9 9929 678b7547f630.png output again. training process exit servel hours. found gpu utilization always 100 two nodes. ! image https user images.githubusercontent.com 41627739 53852922 a096ef00 3ffe 11e9 828c 6d38e8d5dd68.png added nccl debug warn training command, found warn follows. pod worker0 0d9gnb 15 91 3 include socket.h 398 nccl warn call write failed connection reset peer. worker0 0d9gnb 15 91 3 transport.cu 153 nccl warn transport.cu 153 2 proxy thread error pod worker0 0d9gnb 15 91 3 include socket.h 398 nccl warn call recv failed connection reset peer. worker0 0d9gnb 15 91 3 transport.cu 153 nccl warn transport.cu 153 2 proxy thread error environment training code pull https github.com pytorch examples tree master imagenet. model resnet. used 8 v100s train 4 node . batch size set 8, 32 128 per gpu. network rate 500 mb synchronize gradients. pytorch version e.g., 1.0 1.0 0.4.1 os e.g., linux ubunut 16.04 installed pytorch , , source pip build command used compiling source none python version 3.6 cuda cudnn version cuda 9.0 cudnn 7.1.2 gpu models configuration v100 nvlink relevant information",1,training hangs using distributeddataparallel two pod two nodes,"training hangs using distributeddataparallel two pod two nodes training hangs using distributeddataparallel two pod nodes. problem occasionally happens reproduced time. pods started k8s using flannel network. physical nodes connected 10ge ethernet. training started normally, hanged middle training process. ! image https user images.githubusercontent.com 41627739 53853400 4860ec80 4000 11e9 9929 678b7547f630.png output again. training process exit servel hours. found gpu utilization always 100 two nodes. ! image https user images.githubusercontent.com 41627739 53852922 a096ef00 3ffe 11e9 828c 6d38e8d5dd68.png added nccl debug warn training command, found warn follows. pod worker0 0d9gnb 15 91 3 include socket.h 398 nccl warn call write failed connection reset peer. worker0 0d9gnb 15 91 3 transport.cu 153 nccl warn transport.cu 153 2 proxy thread error pod worker0 0d9gnb 15 91 3 include socket.h 398 nccl warn call recv failed connection reset peer. worker0 0d9gnb 15 91 3 transport.cu 153 nccl warn transport.cu 153 2 proxy thread error environment training code pull https github.com pytorch examples tree master imagenet. model resnet. used 8 v100s train 4 node . batch size set 8, 32 128 per gpu. network rate 500 mb synchronize gradients. pytorch version e.g., 1.0 1.0 0.4.1 os e.g., linux ubunut 16.04 installed pytorch , , source pip build command used compiling source none python version 3.6 cuda cudnn version cuda 9.0 cudnn 7.1.2 gpu models configuration v100 nvlink relevant information"
pytorch,27946,bug,1,torch.mean imprecise large tensors,torch.mean imprecise large tensors bug
pytorch,25904,"bug using function torch.exp makes python hang. test functions like torch.sum , torch.abs , functions return right results. reproduce steps reproduce behavior python hang, python cannot closed using ctrl c ubuntu 16.04. expected behavior 2 tensor 1, 2 environment collecting environment information... pytorch version 1.2.0 debug build cuda used build pytorch 10.0.130 os ubuntu 16.04.5 lts gcc version ubuntu 5.3.1 14ubuntu2 5.3.1 20160413 cmake version version 3.5.1 python version 3.6 cuda available yes cuda runtime version 9.0.176 gpu models configuration gpu 0 geforce gtx 1080 ti gpu 1 geforce gtx 1080 ti nvidia driver version 410.93 cudnn version could collect versions relevant libraries pip numpy 1.16.2 pip numpydoc 0.9.1 pip torch 1.2.0 pip torch cluster 1.4.2 pip torch geometric 1.2.1 pip torch scatter 1.2.0 pip torch sparse 0.4.0 pip torch spline conv 1.1.0 pip torchsummaryx 1.3.0 pip torchvision 0.4.0a0 6b959ee conda blas 1.0 mkl conda cuda90 1.0 h6433d27 0 pytorch conda mkl 2018.0.3 1 conda mkl service 1.1.2 py36h90e4bf4 5 conda torch 1.2.0 pypi 0 pypi conda torch cluster 1.4.2 pypi 0 pypi conda torch geometric 1.2.1 pypi 0 pypi conda torch scatter 1.2.0 pypi 0 pypi conda torch sparse 0.4.0 pypi 0 pypi conda torch spline conv 1.1.0 pypi 0 pypi conda torchsummaryx 1.3.0 pypi 0 pypi conda torchvision 0.2.2.post2 pypi 0 pypi",1,python hang using torch.exp,"python hang using torch.exp bug using function torch.exp makes python hang. test functions like torch.sum , torch.abs , functions return right results. reproduce steps reproduce behavior python hang, python cannot closed using ctrl c ubuntu 16.04. expected behavior 2 tensor 1, 2 environment collecting environment information... pytorch version 1.2.0 debug build cuda used build pytorch 10.0.130 os ubuntu 16.04.5 lts gcc version ubuntu 5.3.1 14ubuntu2 5.3.1 20160413 cmake version version 3.5.1 python version 3.6 cuda available yes cuda runtime version 9.0.176 gpu models configuration gpu 0 geforce gtx 1080 ti gpu 1 geforce gtx 1080 ti nvidia driver version 410.93 cudnn version could collect versions relevant libraries pip numpy 1.16.2 pip numpydoc 0.9.1 pip torch 1.2.0 pip torch cluster 1.4.2 pip torch geometric 1.2.1 pip torch scatter 1.2.0 pip torch sparse 0.4.0 pip torch spline conv 1.1.0 pip torchsummaryx 1.3.0 pip torchvision 0.4.0a0 6b959ee conda blas 1.0 mkl conda cuda90 1.0 h6433d27 0 pytorch conda mkl 2018.0.3 1 conda mkl service 1.1.2 py36h90e4bf4 5 conda torch 1.2.0 pypi 0 pypi conda torch cluster 1.4.2 pypi 0 pypi conda torch geometric 1.2.1 pypi 0 pypi conda torch scatter 1.2.0 pypi 0 pypi conda torch sparse 0.4.0 pypi 0 pypi conda torch spline conv 1.1.0 pypi 0 pypi conda torchsummaryx 1.3.0 pypi 0 pypi conda torchvision 0.2.2.post2 pypi 0 pypi"
pytorch,5426,"looking feasibility using ray parameter server combined single machine nn implementation. bottleneck using pytorch result ray calls come numpy array created top mmaped memory https groups.google.com forum !msg ray dev pnsxwi isyi vhwnjgafawaj , pytorch arrays slow, working 2.5 gb sec, speed single threaded memcpy. regular numpy array turned pytorch gpu tensors 8 11 gb sec, depending whether memory page locked. memcpy necessary? done multi threaded way? related issue ray side https github.com ray project ray issues 1614 https github.com diux dev cluster blob master yuxin numpy tf numpy benchmark.py pytorch 0.3.0.post4 repro instructions like https github.com pytorch pytorch issues 5412 issuecomment 368658892",1,torch.from numpy slow mmap'ped numpy arrays,"torch.from numpy slow mmap'ped numpy arrays looking feasibility using ray parameter server combined single machine nn implementation. bottleneck using pytorch result ray calls come numpy array created top mmaped memory https groups.google.com forum !msg ray dev pnsxwi isyi vhwnjgafawaj , pytorch arrays slow, working 2.5 gb sec, speed single threaded memcpy. regular numpy array turned pytorch gpu tensors 8 11 gb sec, depending whether memory page locked. memcpy necessary? done multi threaded way? related issue ray side https github.com ray project ray issues 1614 https github.com diux dev cluster blob master yuxin numpy tf numpy benchmark.py pytorch 0.3.0.post4 repro instructions like https github.com pytorch pytorch issues 5412 issuecomment 368658892"
pytorch,24373,bug reproduce code prints expect 100x slower . environment,1,torch.as tensor slow,torch.as tensor slow bug reproduce code prints expect 100x slower . environment
pytorch,23862,"bug using torch conv2d huge slowdown speed using specific weight tensor coming pretrained state dict. adding 1 subsequently subtracting 1 tensor state dict get huge improvement factor 5 machine . code example compare pretrained weight adding subtracting 1. also observed problem 1.0.1 generates problematic tensor many tensors like pretrained state dict, unique reproduce reproduce run following code read printouts console. expected behavior would expect speed conv2d identical whether using w1 w2. environment pytorch version 1.1 os ubuntu 18.04.2 lts installed pytorch python version 3.7 cuda available cuda runtime version cuda gpu models configuration cuda nvidia driver version cuda cudnn version cuda versions relevant libraries certifi 2019.6.16 numpy 1.17.0 pip 19.1.1 setuptools 41.0.1 torch 1.1.0 wheel 0.33.4",1,nn.functional.conv2d factor 5 slower using specific weight tensor cpu.,"nn.functional.conv2d factor 5 slower using specific weight tensor cpu. bug using torch conv2d huge slowdown speed using specific weight tensor coming pretrained state dict. adding 1 subsequently subtracting 1 tensor state dict get huge improvement factor 5 machine . code example compare pretrained weight adding subtracting 1. also observed problem 1.0.1 generates problematic tensor many tensors like pretrained state dict, unique reproduce reproduce run following code read printouts console. expected behavior would expect speed conv2d identical whether using w1 w2. environment pytorch version 1.1 os ubuntu 18.04.2 lts installed pytorch python version 3.7 cuda available cuda runtime version cuda gpu models configuration cuda nvidia driver version cuda cudnn version cuda versions relevant libraries certifi 2019.6.16 numpy 1.17.0 pip 19.1.1 setuptools 41.0.1 torch 1.1.0 wheel 0.33.4"
pytorch,5436,,1,slower performance eq fill compared numpy,slower performance eq fill compared numpy 
pytorch,23871,repro run resnext101 32x8d quantized conv instead float conv measure elapsed time. possible causes layout transform quantized conv2d calls.,1,quantized conv 6x slower float cpu,quantized conv 6x slower float cpu repro run resnext101 32x8d quantized conv instead float conv measure elapsed time. possible causes layout transform quantized conv2d calls.
pytorch,21828,"bug small 1d tensors smaller 4000 , setting element zero accessor faster using make zeros. benchmarking code uses https github.com google benchmark results see, small 1d tensors make zero sometime almost 3x slower using accessor. 2d tensors, make zeros still slower small tensors. environment pytorch version 1.1.0a0 de582e2 debug build cuda used build pytorch 10.0.130 os ubuntu 18.04.2 lts gcc version ubuntu 7.4.0 1ubuntu1 18.04.1 7.4.0 cmake version version 3.14.0 python version 3.7 cuda available yes cuda runtime version 10.0.130 gpu models configuration gpu 0 geforce gtx titan x nvidia driver version 418.56 cudnn version probably one following usr lib x86 64 linux gnu libcudnn.so.6.0.21 usr lib x86 64 linux gnu libcudnn.so.7.5.1 versions relevant libraries pip3 numpy 1.13.3 conda tflow select 2.3.0 mkl conda blas 1.0 mkl conda magma cuda10 2.4.0 1 cpbotha conda mkl 2019.3 199 conda mkl include 2019.3 199 conda mkl fft 1.0.12 py37ha843d7b 0 conda mkl random 1.0.2 py37hd81dba3 0 conda mkldnn 0.16.1 0 mingfeima conda tensorflow 1.13.1 mkl py37h54b294f 0 conda tensorflow base 1.13.1 mkl py37h7ce6ba3 0 conda torch 1.1.0a0 de582e2 pypi 0 pypi",1,torch zeros slow small tensors c,"torch zeros slow small tensors c bug small 1d tensors smaller 4000 , setting element zero accessor faster using make zeros. benchmarking code uses https github.com google benchmark results see, small 1d tensors make zero sometime almost 3x slower using accessor. 2d tensors, make zeros still slower small tensors. environment pytorch version 1.1.0a0 de582e2 debug build cuda used build pytorch 10.0.130 os ubuntu 18.04.2 lts gcc version ubuntu 7.4.0 1ubuntu1 18.04.1 7.4.0 cmake version version 3.14.0 python version 3.7 cuda available yes cuda runtime version 10.0.130 gpu models configuration gpu 0 geforce gtx titan x nvidia driver version 418.56 cudnn version probably one following usr lib x86 64 linux gnu libcudnn.so.6.0.21 usr lib x86 64 linux gnu libcudnn.so.7.5.1 versions relevant libraries pip3 numpy 1.13.3 conda tflow select 2.3.0 mkl conda blas 1.0 mkl conda magma cuda10 2.4.0 1 cpbotha conda mkl 2019.3 199 conda mkl include 2019.3 199 conda mkl fft 1.0.12 py37ha843d7b 0 conda mkl random 1.0.2 py37hd81dba3 0 conda mkldnn 0.16.1 0 mingfeima conda tensorflow 1.13.1 mkl py37h54b294f 0 conda tensorflow base 1.13.1 mkl py37h7ce6ba3 0 conda torch 1.1.0a0 de582e2 pypi 0 pypi"
pytorch,17738,"bug inconsistent results torch.argmax tensors duplicated values reproduce another machine pytorch 0.4.1, cuda 9.0 expected behavior torch.argmax returns value tensors data it, regardless devices factors . environment pytorch version 0.4.1.post2 debug build cuda used build pytorch 9.0.176 os ubuntu 16.04.5 lts gcc version ubuntu 5.4.0 6ubuntu1 16.04.10 5.4.0 20160609 cmake version version 3.5.1 python version 3.6 cuda available yes cuda runtime version 9.0.176 gpu models configuration gpu 0 geforce gtx 1080 ti gpu 1 geforce gtx 1080 ti nvidia driver version 410.79 cudnn version usr local cuda 9.0 targets x86 64 linux lib libcudnn.so.7.3.1 versions relevant libraries pip3 msgpack numpy 0.4.4.1 pip3 numpy 1.15.0 pip3 torch 0.4.1.post2 pip3 torchtext 0.3.1 pip3 torchvision 0.2.1 conda blas 1.0 mkl conda mkl 2018.0.3 1 conda mkl fft 1.0.4 py36h4414c95 1 conda mkl random 1.0.1 py36h4414c95 1 conda mxnet mkl 1.3.0.post0 conda pytorch 0.4.1 py36 py35 py27 9.0.176 7.1.2 2 pytorch conda torchtext 0.3.1 conda torchvision 0.2.1 py36 0 additional context would helpful documentation describes torch.argmax returns last first index tensor holds duplicated values.",1,inconsistent results torch.argmax tensors duplicated values,"inconsistent results torch.argmax tensors duplicated values bug inconsistent results torch.argmax tensors duplicated values reproduce another machine pytorch 0.4.1, cuda 9.0 expected behavior torch.argmax returns value tensors data it, regardless devices factors . environment pytorch version 0.4.1.post2 debug build cuda used build pytorch 9.0.176 os ubuntu 16.04.5 lts gcc version ubuntu 5.4.0 6ubuntu1 16.04.10 5.4.0 20160609 cmake version version 3.5.1 python version 3.6 cuda available yes cuda runtime version 9.0.176 gpu models configuration gpu 0 geforce gtx 1080 ti gpu 1 geforce gtx 1080 ti nvidia driver version 410.79 cudnn version usr local cuda 9.0 targets x86 64 linux lib libcudnn.so.7.3.1 versions relevant libraries pip3 msgpack numpy 0.4.4.1 pip3 numpy 1.15.0 pip3 torch 0.4.1.post2 pip3 torchtext 0.3.1 pip3 torchvision 0.2.1 conda blas 1.0 mkl conda mkl 2018.0.3 1 conda mkl fft 1.0.4 py36h4414c95 1 conda mkl random 1.0.1 py36h4414c95 1 conda mxnet mkl 1.3.0.post0 conda pytorch 0.4.1 py36 py35 py27 9.0.176 7.1.2 2 pytorch conda torchtext 0.3.1 conda torchvision 0.2.1 py36 0 additional context would helpful documentation describes torch.argmax returns last first index tensor holds duplicated values."
pytorch,847,"hi, created simple model consisting two 1 layer nn competing other. so, loss function based nn outputs. similar gan. problem simple test sample case, loss function decreasing. using non stochastic optimizer eliminate randomness. pseudo code explanation n1 model net1 dimension n1, dimension 1 layer nn sigmoid n2 model net2 dimension n2, dimension 1 layer nn sigmoid n1 optimizer torch.optim.lbfgs n1 model.parameters , lr 0.01,max iter 50 n2 optimizer torch.optim.lbfgs n2 model.parameters , lr 0.01, max iter 50 range iter x n1 variable torch.from numpy load input nn1 batch size x n2 variable torch.from numpy load input nn2 batch size def closure reset grad n1 model.parameters reset grad n2.parameters n1 n1 model x n1 n2 n2 model x n2 n1 params getparams n1 model n2 param getparams n2 model loss loss function n1, n2, n1 params, n2 param loss function, mean square error regularizer loss.backward retain variables true return loss n1 optimizer.step closure def clos reset grad n1 model.parameters reset grad n2 model.parameters n1 n1 model x n1 n2 n2 model x n2 n1 params getparams n1 model n2 param getparams n2 model loss loss function n1, n2, n1 params, n2 param loss function, mean square error regularizer loss.backward return loss n2 optimizer.step clos definition loss function def loss function n1 output, n2 output, n1 parm, n2 param sm torch.pow n1 output n2 output, 2 reg torch.norm n1 parm,2 torch.norm n2 param,2 torch.sum sm 1 reg return plot loss function, oscillation expect decrease training.",1,loss decreasing,"loss decreasing hi, created simple model consisting two 1 layer nn competing other. so, loss function based nn outputs. similar gan. problem simple test sample case, loss function decreasing. using non stochastic optimizer eliminate randomness. pseudo code explanation n1 model net1 dimension n1, dimension 1 layer nn sigmoid n2 model net2 dimension n2, dimension 1 layer nn sigmoid n1 optimizer torch.optim.lbfgs n1 model.parameters , lr 0.01,max iter 50 n2 optimizer torch.optim.lbfgs n2 model.parameters , lr 0.01, max iter 50 range iter x n1 variable torch.from numpy load input nn1 batch size x n2 variable torch.from numpy load input nn2 batch size def closure reset grad n1 model.parameters reset grad n2.parameters n1 n1 model x n1 n2 n2 model x n2 n1 params getparams n1 model n2 param getparams n2 model loss loss function n1, n2, n1 params, n2 param loss function, mean square error regularizer loss.backward retain variables true return loss n1 optimizer.step closure def clos reset grad n1 model.parameters reset grad n2 model.parameters n1 n1 model x n1 n2 n2 model x n2 n1 params getparams n1 model n2 param getparams n2 model loss loss function n1, n2, n1 params, n2 param loss function, mean square error regularizer loss.backward return loss n2 optimizer.step clos definition loss function def loss function n1 output, n2 output, n1 parm, n2 param sm torch.pow n1 output n2 output, 2 reg torch.norm n1 parm,2 torch.norm n2 param,2 torch.sum sm 1 reg return plot loss function, oscillation expect decrease training."
pytorch,17745,"run problem 14870 . cannot reopen issue, opened new issue. gpus empty, except 11mb memory used process running . code easy reproduce nmax utilizationmax utilizationmax memory usagen per processmax utilizationmax memory usage hangs. works fine .",1,"distributed data parallel, gloo backend works, nccl deadlock","distributed data parallel, gloo backend works, nccl deadlock run problem 14870 . cannot reopen issue, opened new issue. gpus empty, except 11mb memory used process running . code easy reproduce nmax utilizationmax utilizationmax memory usagen per processmax utilizationmax memory usage hangs. works fine ."
pytorch,22866,bug pytorch 1.0.1 uses lot cpu cores making tensor numpy array numpy array processed np.transpose. bug appears pytorch 1.0.0. nightly build bug. reproduce steps reproduce behavior 1. install pytorch 1.0.1 2. run following code 3. open htop enjoy 2500 cpu utilization process running code. code without np.transpose works fine 100 cpu utilization abnormal behavior torch.permute call tensor.cuda . torch.permute tensor.cuda works fine 100 cpu utilization expected behavior expecting utilization 100 . environment additional context tried code server. server cpu usage increased drastically still shows 200 utilization np.transpose 100 without.,1,high cpu usage torch.tensor,high cpu usage torch.tensor bug pytorch 1.0.1 uses lot cpu cores making tensor numpy array numpy array processed np.transpose. bug appears pytorch 1.0.0. nightly build bug. reproduce steps reproduce behavior 1. install pytorch 1.0.1 2. run following code 3. open htop enjoy 2500 cpu utilization process running code. code without np.transpose works fine 100 cpu utilization abnormal behavior torch.permute call tensor.cuda . torch.permute tensor.cuda works fine 100 cpu utilization expected behavior expecting utilization 100 . environment additional context tried code server. server cpu usage increased drastically still shows 200 utilization np.transpose 100 without.
pytorch,28503,"bug torch.einsum seems inconsistent behavior half precision tensors, illustrated snippet minimal non working example see example above, results get fp16 significantly different fp32 0.000 also inconsistent computation highlighted comments. reproduce see snipped above. expected behavior running computation multiple times give result everytime input. environment please copy paste output environment collection script https raw.githubusercontent.com pytorch pytorch master torch utils collect env.py fill checklist manually . get script run pytorch version e.g., 1.0 1.3.0 os e.g., linux ubuntu 18.04 installed pytorch , , source conda build command used compiling source n python version 3.7.4 cuda cudnn version 10.2 7.5 gpu models configuration code run single 2080 ti relevant information additional context",1,inconsistent behaviour einsum fp16,"inconsistent behaviour einsum fp16 bug torch.einsum seems inconsistent behavior half precision tensors, illustrated snippet minimal non working example see example above, results get fp16 significantly different fp32 0.000 also inconsistent computation highlighted comments. reproduce see snipped above. expected behavior running computation multiple times give result everytime input. environment please copy paste output environment collection script https raw.githubusercontent.com pytorch pytorch master torch utils collect env.py fill checklist manually . get script run pytorch version e.g., 1.0 1.3.0 os e.g., linux ubuntu 18.04 installed pytorch , , source conda build command used compiling source n python version 3.7.4 cuda cudnn version 10.2 7.5 gpu models configuration code run single 2080 ti relevant information additional context"
pytorch,14687,"bug advanced indexing slower numpy. reproduce steps reproduce behavior expected behavior pytorch fast numpy advanced indexing tensors require grad. environment pytorch version e.g., 1.0 1.0.0.dev20181202 os e.g., linux ubuntu 18.04.1 lts installed pytorch , , source conda install pytorch nightly c pytorch python version 3.7.0 cc mruberry rgommers heitorschueroff vitalyfedyunin ngimel",1,advanced indexing slower numpy,"advanced indexing slower numpy bug advanced indexing slower numpy. reproduce steps reproduce behavior expected behavior pytorch fast numpy advanced indexing tensors require grad. environment pytorch version e.g., 1.0 1.0.0.dev20181202 os e.g., linux ubuntu 18.04.1 lts installed pytorch , , source conda install pytorch nightly c pytorch python version 3.7.0 cc mruberry rgommers heitorschueroff vitalyfedyunin ngimel"
pytorch,18786,"questions help trying test costing time main procedures train step, order accelerate training. found become slow executing codes results load batch dataloader convert cuda i, b, target enumerate train loader start time.time target cuda target.cuda input b.cuda .half time.time cost start print cost output model input loss criterion output 0 , target cuda loss.backward model input loss.backward torch.cuda.synchronize .cuda backward . discussion forum https discuss.pytorch.org",1,tensor.cuda slow invoking model input loss.backward,"tensor.cuda slow invoking model input loss.backward questions help trying test costing time main procedures train step, order accelerate training. found become slow executing codes results load batch dataloader convert cuda i, b, target enumerate train loader start time.time target cuda target.cuda input b.cuda .half time.time cost start print cost output model input loss criterion output 0 , target cuda loss.backward model input loss.backward torch.cuda.synchronize .cuda backward . discussion forum https discuss.pytorch.org"
pytorch,18277,bug taking large cpu tensor results severe precision loss. would like know following behaviour known normal. reproduce produces expected behavior close match cpu gpu versions mean operation. environment pytorch version 1.0.1.post2 debug build cuda used build pytorch 9.0.176 os ubuntu 18.04.1 lts gcc version ubuntu 7.3.0 27ubuntu1 18.04 7.3.0 cmake version version 3.10.2 python version 3.7 cuda available yes cuda runtime version could collect gpu models configuration gpu 0 geforce gtx 1080 ti gpu 1 geforce gtx 1080 ti nvidia driver version 390.116 cudnn version could collect versions relevant libraries pip3 numpy 1.16.2 conda blas 1.0 mkl conda mkl 2019.1 144 conda mkl service 1.1.2 py37he904b0f 5 conda mkl fft 1.0.6 py37hd81dba3 0 conda mkl random 1.0.2 py37hd81dba3 0 conda pytorch 1.0.1 py3.7 cuda9.0.176 cudnn7.4.2 2 pytorch conda torchvision 0.2.1 py 2 pytorch,1,precision loss large tensor averaging cpu,precision loss large tensor averaging cpu bug taking large cpu tensor results severe precision loss. would like know following behaviour known normal. reproduce produces expected behavior close match cpu gpu versions mean operation. environment pytorch version 1.0.1.post2 debug build cuda used build pytorch 9.0.176 os ubuntu 18.04.1 lts gcc version ubuntu 7.3.0 27ubuntu1 18.04 7.3.0 cmake version version 3.10.2 python version 3.7 cuda available yes cuda runtime version could collect gpu models configuration gpu 0 geforce gtx 1080 ti gpu 1 geforce gtx 1080 ti nvidia driver version 390.116 cudnn version could collect versions relevant libraries pip3 numpy 1.16.2 conda blas 1.0 mkl conda mkl 2019.1 144 conda mkl service 1.1.2 py37he904b0f 5 conda mkl fft 1.0.6 py37hd81dba3 0 conda mkl random 1.0.2 py37hd81dba3 0 conda pytorch 1.0.1 py3.7 cuda9.0.176 cudnn7.4.2 2 pytorch conda torchvision 0.2.1 py 2 pytorch
pytorch,19827,"bug issue certain gpu operations hang. appear multi gpu problem, reproduce single gpu. seem reliably effect operations change around order cache certain results gpu minimize copies could cause hang move another command. reproduce current code reproduce problem fairly reliably following command e.g. indexing tensor stored gpu. point seeing commands, usually copying something onto gpu, mentioned changing using gpu caching certain results limit copies reordering things, happening line. time taken command several different images here's numbers mean. first telling line printing, ignored. next number long took execute offending line seconds. images comparable size. newline separates image. see first image executes lines without problem. next one, first line slow takes 30s execute. trend continues rest images. environment additional context first thought something garbage collection gpu, something multi gpu iommu problem people had, sure anymore, especially since reproduce single gpu.",1,random gpu operations hang,"random gpu operations hang bug issue certain gpu operations hang. appear multi gpu problem, reproduce single gpu. seem reliably effect operations change around order cache certain results gpu minimize copies could cause hang move another command. reproduce current code reproduce problem fairly reliably following command e.g. indexing tensor stored gpu. point seeing commands, usually copying something onto gpu, mentioned changing using gpu caching certain results limit copies reordering things, happening line. time taken command several different images here's numbers mean. first telling line printing, ignored. next number long took execute offending line seconds. images comparable size. newline separates image. see first image executes lines without problem. next one, first line slow takes 30s execute. trend continues rest images. environment additional context first thought something garbage collection gpu, something multi gpu iommu problem people had, sure anymore, especially since reproduce single gpu."
pytorch,3958,"pr https github.com pytorch pytorch pull 3666 introduces 10 performance regression cudnn convolution dispatch path. single smoking gun, needs fast old cudnn code was. spent morning trying fix avail, remind fix next release. cc csarofeen ptrblck",1,10 performance regression cudnn convolution dispatch path,"10 performance regression cudnn convolution dispatch path pr https github.com pytorch pytorch pull 3666 introduces 10 performance regression cudnn convolution dispatch path. single smoking gun, needs fast old cudnn code was. spent morning trying fix avail, remind fix next release. cc csarofeen ptrblck"
pytorch,9592,issue description convolutions raise exception small input shape . code example suppose following convolution code raises . quite expected. change 2 gives . expected behavior? guess formula output shape division stride check performed taking operation. system info installed pytorch conda os linux pytorch version 0.4 python version 3.6 code run cpu.,1,nn.conv incostintent error depending stride,nn.conv incostintent error depending stride issue description convolutions raise exception small input shape . code example suppose following convolution code raises . quite expected. change 2 gives . expected behavior? guess formula output shape division stride check performed taking operation. system info installed pytorch conda os linux pytorch version 0.4 python version 3.6 code run cpu.
pytorch,11647,issue description fairly optimized cnn blstm crf tagger https github.com dpressel baseline blob master python baseline pytorch tagger model.py crf defined https github.com dpressel baseline blob f0204432076c166ce3d6672705826f33aec95dbe python baseline pytorch torchy.py l669 . pytorch using cuda cudnn run single epoch conll 2003 ner task 21.41 0.28 thing change version pytorch current conda install single epoch takes 27.99 0.26 models run gpu. pytorch forums told post https discuss.pytorch.org large preformance regression 0 4 1 25037 code example please try provide minimal example repro bug. assuming pytorch installed system info 0.4.0 collecting environment information... pytorch version 0.4.0 debug build cuda used build pytorch 9.0.176 os ubuntu 16.04.3 lts gcc version ubuntu 5.4.0 6ubuntu1 16.04.10 5.4.0 20160609 cmake version version 3.5.1 python version 3.6 cuda available yes cuda runtime version 9.0.176 gpu models configuration gpu 0 geforce gtx 1070 max q design nvidia driver version 384.130 cudnn version probably one following usr local cuda 9.0 cudnn 7 lib64 libcudnn.so usr local cuda 9.0 cudnn 7 lib64 libcudnn.so.7 usr local cuda 9.0 cudnn 7 lib64 libcudnn.so.7.0.5 usr local cuda 9.0 cudnn 7 lib64 libcudnn.so.7.1.2 usr local cuda 9.0 cudnn 7 lib64 libcudnn static.a usr local cuda 9.0 lib64 libcudnn.so usr local cuda 9.0 lib64 libcudnn.so.7 usr local cuda 9.0 lib64 libcudnn.so.7.0.5 usr local cuda 9.0 lib64 libcudnn.so.7.1.2 usr local cuda 9.0 lib64 libcudnn.so.7.2.1 usr local cuda 9.0 lib64 libcudnn static.a versions relevant libraries pip numpy 1.14.3 pip torch 0.4.0 pip torchfile 0.1.0 pip torchvision 0.2.1 conda cuda90 1.0 h6433d27 0 pytorch conda pytorch 0.4.0 py36hdf912b8 0 conda torchfile 0.1.0 conda torchvision 0.2.1 py36 1 pytorch 0.4.1 collecting environment information... pytorch version 0.4.1.post2 debug build cuda used build pytorch 9.0.176 os ubuntu 16.04.3 lts gcc version ubuntu 5.4.0 6ubuntu1 16.04.10 5.4.0 20160609 cmake version version 3.5.1 python version 3.6 cuda available yes cuda runtime version 9.0.176 gpu models configuration gpu 0 geforce gtx 1070 max q design nvidia driver version 384.130 cudnn version probably one following usr local cuda 9.0 cudnn 7 lib64 libcudnn.so usr local cuda 9.0 cudnn 7 lib64 libcudnn.so.7 usr local cuda 9.0 cudnn 7 lib64 libcudnn.so.7.0.5 usr local cuda 9.0 cudnn 7 lib64 libcudnn.so.7.1.2 usr local cuda 9.0 cudnn 7 lib64 libcudnn static.a usr local cuda 9.0 lib64 libcudnn.so usr local cuda 9.0 lib64 libcudnn.so.7 usr local cuda 9.0 lib64 libcudnn.so.7.0.5 usr local cuda 9.0 lib64 libcudnn.so.7.1.2 usr local cuda 9.0 lib64 libcudnn.so.7.2.1 usr local cuda 9.0 lib64 libcudnn static.a versions relevant libraries pip numpy 1.14.3 pip torch 0.4.1.post2 pip torchfile 0.1.0 pip torchvision 0.2.1 conda cuda90 1.0 h6433d27 0 pytorch conda pytorch 0.4.1 py36 py35 py27 9.0.176 7.1.2 2 pytorch conda torchfile 0.1.0 conda torchvision 0.2.1 py36 1 pytorch,1,performance speed regression 0.4.0 0.4.1,performance speed regression 0.4.0 0.4.1 issue description fairly optimized cnn blstm crf tagger https github.com dpressel baseline blob master python baseline pytorch tagger model.py crf defined https github.com dpressel baseline blob f0204432076c166ce3d6672705826f33aec95dbe python baseline pytorch torchy.py l669 . pytorch using cuda cudnn run single epoch conll 2003 ner task 21.41 0.28 thing change version pytorch current conda install single epoch takes 27.99 0.26 models run gpu. pytorch forums told post https discuss.pytorch.org large preformance regression 0 4 1 25037 code example please try provide minimal example repro bug. assuming pytorch installed system info 0.4.0 collecting environment information... pytorch version 0.4.0 debug build cuda used build pytorch 9.0.176 os ubuntu 16.04.3 lts gcc version ubuntu 5.4.0 6ubuntu1 16.04.10 5.4.0 20160609 cmake version version 3.5.1 python version 3.6 cuda available yes cuda runtime version 9.0.176 gpu models configuration gpu 0 geforce gtx 1070 max q design nvidia driver version 384.130 cudnn version probably one following usr local cuda 9.0 cudnn 7 lib64 libcudnn.so usr local cuda 9.0 cudnn 7 lib64 libcudnn.so.7 usr local cuda 9.0 cudnn 7 lib64 libcudnn.so.7.0.5 usr local cuda 9.0 cudnn 7 lib64 libcudnn.so.7.1.2 usr local cuda 9.0 cudnn 7 lib64 libcudnn static.a usr local cuda 9.0 lib64 libcudnn.so usr local cuda 9.0 lib64 libcudnn.so.7 usr local cuda 9.0 lib64 libcudnn.so.7.0.5 usr local cuda 9.0 lib64 libcudnn.so.7.1.2 usr local cuda 9.0 lib64 libcudnn.so.7.2.1 usr local cuda 9.0 lib64 libcudnn static.a versions relevant libraries pip numpy 1.14.3 pip torch 0.4.0 pip torchfile 0.1.0 pip torchvision 0.2.1 conda cuda90 1.0 h6433d27 0 pytorch conda pytorch 0.4.0 py36hdf912b8 0 conda torchfile 0.1.0 conda torchvision 0.2.1 py36 1 pytorch 0.4.1 collecting environment information... pytorch version 0.4.1.post2 debug build cuda used build pytorch 9.0.176 os ubuntu 16.04.3 lts gcc version ubuntu 5.4.0 6ubuntu1 16.04.10 5.4.0 20160609 cmake version version 3.5.1 python version 3.6 cuda available yes cuda runtime version 9.0.176 gpu models configuration gpu 0 geforce gtx 1070 max q design nvidia driver version 384.130 cudnn version probably one following usr local cuda 9.0 cudnn 7 lib64 libcudnn.so usr local cuda 9.0 cudnn 7 lib64 libcudnn.so.7 usr local cuda 9.0 cudnn 7 lib64 libcudnn.so.7.0.5 usr local cuda 9.0 cudnn 7 lib64 libcudnn.so.7.1.2 usr local cuda 9.0 cudnn 7 lib64 libcudnn static.a usr local cuda 9.0 lib64 libcudnn.so usr local cuda 9.0 lib64 libcudnn.so.7 usr local cuda 9.0 lib64 libcudnn.so.7.0.5 usr local cuda 9.0 lib64 libcudnn.so.7.1.2 usr local cuda 9.0 lib64 libcudnn.so.7.2.1 usr local cuda 9.0 lib64 libcudnn static.a versions relevant libraries pip numpy 1.14.3 pip torch 0.4.1.post2 pip torchfile 0.1.0 pip torchvision 0.2.1 conda cuda90 1.0 h6433d27 0 pytorch conda pytorch 0.4.1 py36 py35 py27 9.0.176 7.1.2 2 pytorch conda torchfile 0.1.0 conda torchvision 0.2.1 py36 1 pytorch
pytorch,23425,"bug hello. training multi task network cluster using 8 gpu . every iteration, process one image per gpu. suppose respectively. case, could zero one gpu, means loss computed. return zero tensor. situation happens, code hang on. pytorch version 1.1.0 os e.g., linux ubuntu installed pytorch , , source docker conda python version 3.6 cuda cudnn version cuda 9.0 gpu models configuration 1080ti relevant information",1,hanging one gpu node return zero loss context distributed data parallel training,"hanging one gpu node return zero loss context distributed data parallel training bug hello. training multi task network cluster using 8 gpu . every iteration, process one image per gpu. suppose respectively. case, could zero one gpu, means loss computed. return zero tensor. situation happens, code hang on. pytorch version 1.1.0 os e.g., linux ubuntu installed pytorch , , source docker conda python version 3.6 cuda cudnn version cuda 9.0 gpu models configuration 1080ti relevant information"
pytorch,16258,"bug trying convert format cv mat torch tensor using c libtorch , one crucial step permute shape tensor. however found cases permute function failed manage memory image. reproduce provide code reproduce bug 1. image version 1. simpler matrix version code would output wrong change permute line output correct note permute failed managed memory, always returned correct shape permuted tensor. expected behavior two different usage permute behave consistently permute memory shape environment please copy paste output environment collection script https raw.githubusercontent.com pytorch pytorch master torch utils collect env.py fill checklist manually . get script run pytorch version e.g., 1.0 v1.0.0 os e.g., linux linux arm installed pytorch , , source source, following step issue 15138 https raw.githubusercontent.com pytorch pytorch master torch utils collect env.py build command used compiling source python setup.py build python version 3.5.2 cuda cudnn version libcudnn.so.7.0.5 gpu models configuration relevant information additional context",1,inconsistent behavior permute different calling order,"inconsistent behavior permute different calling order bug trying convert format cv mat torch tensor using c libtorch , one crucial step permute shape tensor. however found cases permute function failed manage memory image. reproduce provide code reproduce bug 1. image version 1. simpler matrix version code would output wrong change permute line output correct note permute failed managed memory, always returned correct shape permuted tensor. expected behavior two different usage permute behave consistently permute memory shape environment please copy paste output environment collection script https raw.githubusercontent.com pytorch pytorch master torch utils collect env.py fill checklist manually . get script run pytorch version e.g., 1.0 v1.0.0 os e.g., linux linux arm installed pytorch , , source source, following step issue 15138 https raw.githubusercontent.com pytorch pytorch master torch utils collect env.py build command used compiling source python setup.py build python version 3.5.2 cuda cudnn version libcudnn.so.7.0.5 gpu models configuration relevant information additional context"
pytorch,20355,"training hanging call backwards might wrong looks like hangs one threads tries acquire lock. setup following pytorch version 1.0.0post2 start 4 python processes machine using multiprocessing spawn, happens popen process uses different gpu different data data loaded memory prior start training use torch.distributed nccl synchronise training communication different processes happens via nccl observe random times, one processes hangs others wait call reduce. unfortunately, hard reproduce, take arbitrary amount time hang sometimes hangs 10min, sometimes hangs 10h . help appreciated thanks. moment hangs, threads frozen process state gdb info threads id target id frame 16 thread 0x7f22711b9700 lwp 23578 python 0x00007f22d64fff0d poll lib64 libc.so.6 15 thread 0x7f2270943700 lwp 23614 python 0x00007f22d650c03f accept4 lib64 libc.so.6 14 thread 0x7f2265fff700 lwp 23644 python 0x00007f22d64fff0d poll lib64 libc.so.6 13 thread 0x7f22617fe700 lwp 30293 python 0x00007f22d70f5995 pthread cond wait glibc 2.3.2 lib64 libpthread.so.0 12 thread 0x7f22609ed800 lwp 30294 python 0x00007f22d70f5995 pthread cond wait glibc 2.3.2 lib64 libpthread.so.0 11 thread 0x7f22605eb880 lwp 30295 python 0x00007f22d70f5995 pthread cond wait glibc 2.3.2 lib64 libpthread.so.0 10 thread 0x7f224dffe900 lwp 30296 python 0x00007f22d70f5995 pthread cond wait glibc 2.3.2 lib64 libpthread.so.0 9 thread 0x7f224dbfc980 lwp 30297 python 0x00007f22d70f5995 pthread cond wait glibc 2.3.2 lib64 libpthread.so.0 8 thread 0x7f224d7faa00 lwp 30298 python 0x00007f22d70f5995 pthread cond wait glibc 2.3.2 lib64 libpthread.so.0 7 thread 0x7f224d3f8a80 lwp 30299 python 0x00007f22d70f5995 pthread cond wait glibc 2.3.2 lib64 libpthread.so.0 6 thread 0x7f221aee5700 lwp 68788 python 0x00007f22d70f851d lll lock wait lib64 libpthread.so.0 5 thread 0x7f221a6e4700 lwp 68789 python 0x00007f22d70f5995 pthread cond wait glibc 2.3.2 lib64 libpthread.so.0 4 thread 0x7f2219ee3700 lwp 68790 python 0x00007f22d70f5995 pthread cond wait glibc 2.3.2 lib64 libpthread.so.0 3 thread 0x7f22196e2700 lwp 68791 python 0x00007f22d70f5995 pthread cond wait glibc 2.3.2 lib64 libpthread.so.0 2 thread 0x7f2218ee1700 lwp 68792 python 0x00007f22d70f5995 pthread cond wait glibc 2.3.2 lib64 libpthread.so.0 1 thread 0x7f22d750f740 lwp 23478 python 0x00007f22d70f5995 pthread cond wait glibc 2.3.2 lib64 libpthread.so.0 stack trace per thread below. thread 1 0 0x00007f22d70f5995 pthread cond wait glibc 2.3.2 lib64 libpthread.so.0 1 0x00007f22bfb9656f gthread cond wait mutex , cond opt conda conda bld compilers linux 64 1534514838838 work .build x86 64 conda cos6 linux gnu build build cc gcc final x86 64 conda cos6 linux gnu libstdc v3 include x86 64 conda cos6 linux gnu bits gthr default.h 877 2 std condition variable wait , lock opt conda conda bld compilers linux 64 1534514838838 work .build x86 64 conda cos6 linux gnu src gcc libstdc v3 src c 11 condition variable.cc 53 3 0x00007f2278eb08e3 torch autograd engine execute std vector const , std vector const , bool, bool, std vector const opt anaconda3 lib python3.7 site packages torch lib libtorch.so.1 4 0x00007f22bc696a0c torch autograd python pythonengine execute std vector const , std vector const , bool, bool, std vector const opt anaconda3 lib python3.7 site packages torch lib libtorch python.so 5 0x00007f22bc69722c thpengine run backward thpengine , object , object opt anaconda3 lib python3.7 site packages torch lib libtorch python.so threads 2 5 0 0x00007f22d70f5995 pthread cond wait glibc 2.3.2 lib64 libpthread.so.0 1 0x00007f22bfb9656f gthread cond wait mutex , cond opt conda conda bld compilers linux 64 1534514838838 work .build x86 64 conda cos6 linux gnu build build cc gcc final x86 64 conda cos6 linux gnu libstdc v3 include x86 64 conda cos6 linux gnu bits gthr default.h 877 2 std condition variable wait , lock opt conda conda bld compilers linux 64 1534514838838 work .build x86 64 conda cos6 linux gnu src gcc libstdc v3 src c 11 condition variable.cc 53 3 0x00007f2278eac92b torch autograd readyqueue pop opt anaconda3 lib python3.7 site packages torch lib libtorch.so.1 4 0x00007f2278eaf083 torch autograd engine thread main torch autograd graphtask opt anaconda3 lib python3.7 site packages torch lib libtorch.so.1 5 0x00007f2278eaba77 torch autograd engine thread init int opt anaconda3 lib python3.7 site packages torch lib libtorch.so.1 6 0x00007f22bc6968aa torch autograd python pythonengine thread init int opt anaconda3 lib python3.7 site packages torch lib libtorch python.so 7 0x00007f22bfb9a678 std execute native thread routine compat p opt conda conda bld compilers linux 64 1534514838838 work .build x86 64 conda cos6 linux gnu src gcc libstdc v3 src c 11 thread.cc 94 8 0x00007f22d70f1e25 start thread lib64 libpthread.so.0 9 0x00007f22d650abad clone lib64 libc.so.6 thread 6 0 0x00007f22d70f851d lll lock wait lib64 libpthread.so.0 1 0x00007f22d70f61a0 pthread cond broadcast glibc 2.3.2 lib64 libpthread.so.0 2 0x00007f22bfb96594 gthread cond broadcast cond opt conda conda bld compilers linux 64 1534514838838 work .build x86 64 conda cos6 linux gnu build build cc gcc final x86 64 conda cos6 linux gnu libstdc v3 include x86 64 conda cos6 linux gnu bits gthr default.h 865 3 std condition variable notify opt conda conda bld compilers linux 64 1534514838838 work .build x86 64 conda cos6 linux gnu src gcc libstdc v3 src c 11 condition variable.cc 73 4 0x00007f2278eaf167 torch autograd engine thread main torch autograd graphtask opt anaconda3 lib python3.7 site packages torch lib libtorch.so.1 5 0x00007f2278eaba77 torch autograd engine thread init int opt anaconda3 lib python3.7 site packages torch lib libtorch.so.1 6 0x00007f22bc6968aa torch autograd python pythonengine thread init int opt anaconda3 lib python3.7 site packages torch lib libtorch python.so 7 0x00007f22bfb9a678 std execute native thread routine compat p opt conda conda bld compilers linux 64 1534514838838 work .build x86 64 conda cos6 linux gnu src gcc libstdc v3 src c 11 thread.cc 94 8 0x00007f22d70f1e25 start thread lib64 libpthread.so.0 9 0x00007f22d650abad clone lib64 libc.so.6 thread 7 13 0 0x00007f22d70f5995 pthread cond wait glibc 2.3.2 lib64 libpthread.so.0 1 0x00007f22cb364725 kmp suspend 64 opt anaconda3 lib python3.7 site packages numpy .. .. .. libiomp5.so 2 0x00007f22cb2d5e4e bool internal 25 src kmp barrier cpp 3dc39ea5 kmp wait template kmp info , kmp flag 64 , void opt anaconda3 lib python3.7 site packages numpy .. .. .. libiomp5.so 3 0x00007f22cb2d9398 internal 25 src kmp barrier cpp 3dc39ea5 kmp hyper barrier release barrier type, kmp info , int, int, int, void opt anaconda3 lib python3.7 site packages numpy .. .. .. libiomp5.so 4 0x00007f22cb2df4d2 kmp fork barrier int, int opt anaconda3 lib python3.7 site packages numpy .. .. .. libiomp5.so 5 0x00007f22cb320d8e kmp launch thread opt anaconda3 lib python3.7 site packages numpy .. .. .. libiomp5.so 6 0x00007f22cb360571 internal 26 src z linux util cpp 51eec780 kmp launch worker void opt anaconda3 lib python3.7 site packages numpy .. .. .. libiomp5.so 7 0x00007f22d70f1e25 start thread lib64 libpthread.so.0 8 0x00007f22d650abad clone lib64 libc.so.6 thread 14 0 0x00007f22d64fff0d poll lib64 libc.so.6 1 0x00007f22beb8a323 ?? lib64 libcuda.so.1 2 0x00007f22bebecacd ?? lib64 libcuda.so.1 3 0x00007f22beb8c988 ?? lib64 libcuda.so.1 4 0x00007f22d70f1e25 start thread lib64 libpthread.so.0 5 0x00007f22d650abad clone lib64 libc.so.6 thread 15 0 0x00007f22d650c03f accept4 lib64 libc.so.6 1 0x00007f22beb8b2ca ?? lib64 libcuda.so.1 2 0x00007f22beb7d8dd ?? lib64 libcuda.so.1 3 0x00007f22beb8c988 ?? lib64 libcuda.so.1 4 0x00007f22d70f1e25 start thread lib64 libpthread.so.0 5 0x00007f22d650abad clone lib64 libc.so.6 thread 16 0 0x00007f22d64fff0d poll lib64 libc.so.6 1 0x00007f22bca13879 c10d tcpstoredaemon run opt anaconda3 lib python3.7 site packages torch lib libtorch python.so 2 0x00007f22bfb9a678 std execute native thread routine compat p opt conda conda bld compilers linux 64 1534514838838 work .build x86 64 conda cos6 linux gnu src gcc libstdc v3 src c 11 thread.cc 94 3 0x00007f22d70f1e25 start thread lib64 libpthread.so.0 4 0x00007f22d650abad clone lib64 libc.so.6",1,backwards hangs,"backwards hangs training hanging call backwards might wrong looks like hangs one threads tries acquire lock. setup following pytorch version 1.0.0post2 start 4 python processes machine using multiprocessing spawn, happens popen process uses different gpu different data data loaded memory prior start training use torch.distributed nccl synchronise training communication different processes happens via nccl observe random times, one processes hangs others wait call reduce. unfortunately, hard reproduce, take arbitrary amount time hang sometimes hangs 10min, sometimes hangs 10h . help appreciated thanks. moment hangs, threads frozen process state gdb info threads id target id frame 16 thread 0x7f22711b9700 lwp 23578 python 0x00007f22d64fff0d poll lib64 libc.so.6 15 thread 0x7f2270943700 lwp 23614 python 0x00007f22d650c03f accept4 lib64 libc.so.6 14 thread 0x7f2265fff700 lwp 23644 python 0x00007f22d64fff0d poll lib64 libc.so.6 13 thread 0x7f22617fe700 lwp 30293 python 0x00007f22d70f5995 pthread cond wait glibc 2.3.2 lib64 libpthread.so.0 12 thread 0x7f22609ed800 lwp 30294 python 0x00007f22d70f5995 pthread cond wait glibc 2.3.2 lib64 libpthread.so.0 11 thread 0x7f22605eb880 lwp 30295 python 0x00007f22d70f5995 pthread cond wait glibc 2.3.2 lib64 libpthread.so.0 10 thread 0x7f224dffe900 lwp 30296 python 0x00007f22d70f5995 pthread cond wait glibc 2.3.2 lib64 libpthread.so.0 9 thread 0x7f224dbfc980 lwp 30297 python 0x00007f22d70f5995 pthread cond wait glibc 2.3.2 lib64 libpthread.so.0 8 thread 0x7f224d7faa00 lwp 30298 python 0x00007f22d70f5995 pthread cond wait glibc 2.3.2 lib64 libpthread.so.0 7 thread 0x7f224d3f8a80 lwp 30299 python 0x00007f22d70f5995 pthread cond wait glibc 2.3.2 lib64 libpthread.so.0 6 thread 0x7f221aee5700 lwp 68788 python 0x00007f22d70f851d lll lock wait lib64 libpthread.so.0 5 thread 0x7f221a6e4700 lwp 68789 python 0x00007f22d70f5995 pthread cond wait glibc 2.3.2 lib64 libpthread.so.0 4 thread 0x7f2219ee3700 lwp 68790 python 0x00007f22d70f5995 pthread cond wait glibc 2.3.2 lib64 libpthread.so.0 3 thread 0x7f22196e2700 lwp 68791 python 0x00007f22d70f5995 pthread cond wait glibc 2.3.2 lib64 libpthread.so.0 2 thread 0x7f2218ee1700 lwp 68792 python 0x00007f22d70f5995 pthread cond wait glibc 2.3.2 lib64 libpthread.so.0 1 thread 0x7f22d750f740 lwp 23478 python 0x00007f22d70f5995 pthread cond wait glibc 2.3.2 lib64 libpthread.so.0 stack trace per thread below. thread 1 0 0x00007f22d70f5995 pthread cond wait glibc 2.3.2 lib64 libpthread.so.0 1 0x00007f22bfb9656f gthread cond wait mutex , cond opt conda conda bld compilers linux 64 1534514838838 work .build x86 64 conda cos6 linux gnu build build cc gcc final x86 64 conda cos6 linux gnu libstdc v3 include x86 64 conda cos6 linux gnu bits gthr default.h 877 2 std condition variable wait , lock opt conda conda bld compilers linux 64 1534514838838 work .build x86 64 conda cos6 linux gnu src gcc libstdc v3 src c 11 condition variable.cc 53 3 0x00007f2278eb08e3 torch autograd engine execute std vector const , std vector const , bool, bool, std vector const opt anaconda3 lib python3.7 site packages torch lib libtorch.so.1 4 0x00007f22bc696a0c torch autograd python pythonengine execute std vector const , std vector const , bool, bool, std vector const opt anaconda3 lib python3.7 site packages torch lib libtorch python.so 5 0x00007f22bc69722c thpengine run backward thpengine , object , object opt anaconda3 lib python3.7 site packages torch lib libtorch python.so threads 2 5 0 0x00007f22d70f5995 pthread cond wait glibc 2.3.2 lib64 libpthread.so.0 1 0x00007f22bfb9656f gthread cond wait mutex , cond opt conda conda bld compilers linux 64 1534514838838 work .build x86 64 conda cos6 linux gnu build build cc gcc final x86 64 conda cos6 linux gnu libstdc v3 include x86 64 conda cos6 linux gnu bits gthr default.h 877 2 std condition variable wait , lock opt conda conda bld compilers linux 64 1534514838838 work .build x86 64 conda cos6 linux gnu src gcc libstdc v3 src c 11 condition variable.cc 53 3 0x00007f2278eac92b torch autograd readyqueue pop opt anaconda3 lib python3.7 site packages torch lib libtorch.so.1 4 0x00007f2278eaf083 torch autograd engine thread main torch autograd graphtask opt anaconda3 lib python3.7 site packages torch lib libtorch.so.1 5 0x00007f2278eaba77 torch autograd engine thread init int opt anaconda3 lib python3.7 site packages torch lib libtorch.so.1 6 0x00007f22bc6968aa torch autograd python pythonengine thread init int opt anaconda3 lib python3.7 site packages torch lib libtorch python.so 7 0x00007f22bfb9a678 std execute native thread routine compat p opt conda conda bld compilers linux 64 1534514838838 work .build x86 64 conda cos6 linux gnu src gcc libstdc v3 src c 11 thread.cc 94 8 0x00007f22d70f1e25 start thread lib64 libpthread.so.0 9 0x00007f22d650abad clone lib64 libc.so.6 thread 6 0 0x00007f22d70f851d lll lock wait lib64 libpthread.so.0 1 0x00007f22d70f61a0 pthread cond broadcast glibc 2.3.2 lib64 libpthread.so.0 2 0x00007f22bfb96594 gthread cond broadcast cond opt conda conda bld compilers linux 64 1534514838838 work .build x86 64 conda cos6 linux gnu build build cc gcc final x86 64 conda cos6 linux gnu libstdc v3 include x86 64 conda cos6 linux gnu bits gthr default.h 865 3 std condition variable notify opt conda conda bld compilers linux 64 1534514838838 work .build x86 64 conda cos6 linux gnu src gcc libstdc v3 src c 11 condition variable.cc 73 4 0x00007f2278eaf167 torch autograd engine thread main torch autograd graphtask opt anaconda3 lib python3.7 site packages torch lib libtorch.so.1 5 0x00007f2278eaba77 torch autograd engine thread init int opt anaconda3 lib python3.7 site packages torch lib libtorch.so.1 6 0x00007f22bc6968aa torch autograd python pythonengine thread init int opt anaconda3 lib python3.7 site packages torch lib libtorch python.so 7 0x00007f22bfb9a678 std execute native thread routine compat p opt conda conda bld compilers linux 64 1534514838838 work .build x86 64 conda cos6 linux gnu src gcc libstdc v3 src c 11 thread.cc 94 8 0x00007f22d70f1e25 start thread lib64 libpthread.so.0 9 0x00007f22d650abad clone lib64 libc.so.6 thread 7 13 0 0x00007f22d70f5995 pthread cond wait glibc 2.3.2 lib64 libpthread.so.0 1 0x00007f22cb364725 kmp suspend 64 opt anaconda3 lib python3.7 site packages numpy .. .. .. libiomp5.so 2 0x00007f22cb2d5e4e bool internal 25 src kmp barrier cpp 3dc39ea5 kmp wait template kmp info , kmp flag 64 , void opt anaconda3 lib python3.7 site packages numpy .. .. .. libiomp5.so 3 0x00007f22cb2d9398 internal 25 src kmp barrier cpp 3dc39ea5 kmp hyper barrier release barrier type, kmp info , int, int, int, void opt anaconda3 lib python3.7 site packages numpy .. .. .. libiomp5.so 4 0x00007f22cb2df4d2 kmp fork barrier int, int opt anaconda3 lib python3.7 site packages numpy .. .. .. libiomp5.so 5 0x00007f22cb320d8e kmp launch thread opt anaconda3 lib python3.7 site packages numpy .. .. .. libiomp5.so 6 0x00007f22cb360571 internal 26 src z linux util cpp 51eec780 kmp launch worker void opt anaconda3 lib python3.7 site packages numpy .. .. .. libiomp5.so 7 0x00007f22d70f1e25 start thread lib64 libpthread.so.0 8 0x00007f22d650abad clone lib64 libc.so.6 thread 14 0 0x00007f22d64fff0d poll lib64 libc.so.6 1 0x00007f22beb8a323 ?? lib64 libcuda.so.1 2 0x00007f22bebecacd ?? lib64 libcuda.so.1 3 0x00007f22beb8c988 ?? lib64 libcuda.so.1 4 0x00007f22d70f1e25 start thread lib64 libpthread.so.0 5 0x00007f22d650abad clone lib64 libc.so.6 thread 15 0 0x00007f22d650c03f accept4 lib64 libc.so.6 1 0x00007f22beb8b2ca ?? lib64 libcuda.so.1 2 0x00007f22beb7d8dd ?? lib64 libcuda.so.1 3 0x00007f22beb8c988 ?? lib64 libcuda.so.1 4 0x00007f22d70f1e25 start thread lib64 libpthread.so.0 5 0x00007f22d650abad clone lib64 libc.so.6 thread 16 0 0x00007f22d64fff0d poll lib64 libc.so.6 1 0x00007f22bca13879 c10d tcpstoredaemon run opt anaconda3 lib python3.7 site packages torch lib libtorch python.so 2 0x00007f22bfb9a678 std execute native thread routine compat p opt conda conda bld compilers linux 64 1534514838838 work .build x86 64 conda cos6 linux gnu src gcc libstdc v3 src c 11 thread.cc 94 3 0x00007f22d70f1e25 start thread lib64 libpthread.so.0 4 0x00007f22d650abad clone lib64 libc.so.6"
pytorch,27016,"issue description comparing outcomes torch.mm torch.einsum matrix multiplication, results consistent. result also produced numpy. code example minimal example here. true true true true true true true true true true true true true true true true true true true true true true true true true true true true true true true true tensor true, true, true, true , false, true, false, true , true, false, true, true , false, true, true, true true true true true false true false true true false true true false true true true course expect trues. system info pytorch version 1.2.0 debug build cuda used build pytorch 10.0.130 os red hat enterprise linux release 8.0 ootpa gcc version gcc 8.2.1 20180905 red hat 8.2.1 3 cmake version version 3.11.4 python version 3.7 cuda available yes cuda runtime version 10.1.168 gpu models configuration gpu 0 geforce rtx 2080 ti nvidia driver version 430.14 cudnn version could collect versions relevant libraries pip numpy 1.16.2 pip numpydoc 0.8.0 pip torch 1.2.0 conda pytorch select 0.2 gpu 0 conda blas 1.0 mkl conda mkl 2019.4 243 conda mkl service 2.3.0 py37he904b0f 0 conda mkl fft 1.0.10 py37ha843d7b 0 conda mkl random 1.0.2 py37hd81dba3 0 conda pytorch 1.2.0 cuda100py37h938c94c 0",1,inconsistent einsum torch.mm,"inconsistent einsum torch.mm issue description comparing outcomes torch.mm torch.einsum matrix multiplication, results consistent. result also produced numpy. code example minimal example here. true true true true true true true true true true true true true true true true true true true true true true true true true true true true true true true true tensor true, true, true, true , false, true, false, true , true, false, true, true , false, true, true, true true true true true false true false true true false true true false true true true course expect trues. system info pytorch version 1.2.0 debug build cuda used build pytorch 10.0.130 os red hat enterprise linux release 8.0 ootpa gcc version gcc 8.2.1 20180905 red hat 8.2.1 3 cmake version version 3.11.4 python version 3.7 cuda available yes cuda runtime version 10.1.168 gpu models configuration gpu 0 geforce rtx 2080 ti nvidia driver version 430.14 cudnn version could collect versions relevant libraries pip numpy 1.16.2 pip numpydoc 0.8.0 pip torch 1.2.0 conda pytorch select 0.2 gpu 0 conda blas 1.0 mkl conda mkl 2019.4 243 conda mkl service 2.3.0 py37he904b0f 0 conda mkl fft 1.0.10 py37ha843d7b 0 conda mkl random 1.0.2 py37hd81dba3 0 conda pytorch 1.2.0 cuda100py37h938c94c 0"
pytorch,12683,"would nice gpu version triangular solve. needed preconditioned stochastic gradient optimizer paper https ieeexplore.ieee.org document 7875097 , possibly reason tf implementation runs 3x faster pytorch version here's microbenchmark milliseconds 8000x8000 triangular solve https github.com yaroslavvb newton blob master benchmark triangular.py cc vitalyfedyunin jianyuh nikitaved pearu mruberry heitorschueroff walterddr ivanyashchuk xwang233 ngimel",1,cpu triangular solve slow,"cpu triangular solve slow would nice gpu version triangular solve. needed preconditioned stochastic gradient optimizer paper https ieeexplore.ieee.org document 7875097 , possibly reason tf implementation runs 3x faster pytorch version here's microbenchmark milliseconds 8000x8000 triangular solve https github.com yaroslavvb newton blob master benchmark triangular.py cc vitalyfedyunin jianyuh nikitaved pearu mruberry heitorschueroff walterddr ivanyashchuk xwang233 ngimel"
pytorch,13716,"try use depthwise convolution reduce parameters model. however found depthwise convolutions slow cpu, 4x 5x normal 3x3 convolution, input channel output channel 256. way speed process? version pytorch 0.4.1. cc vitalyfedyunin ngimel",1,depthwise convolution slow cpu,"depthwise convolution slow cpu try use depthwise convolution reduce parameters model. however found depthwise convolutions slow cpu, 4x 5x normal 3x3 convolution, input channel output channel 256. way speed process? version pytorch 0.4.1. cc vitalyfedyunin ngimel"
pytorch,14231,"bug integer array indexing appears slower applying flattened tensor, using flattened indexes, reshaping expected output size. reproduce reproduced gist https gist.github.com gngdb 3d4f5aa27ee5199b0d4b997ffe21a6b4 expected behavior faster way implement integer array indexing, default. might though, uses integer array indexing speed difference might hold. environment pytorch version 0.4.1.post2 debug build cuda used build pytorch 9.0.176 os scientific linux release 7.5 nitrogen gcc version gcc 4.8.5 20150623 red hat 4.8.5 28 cmake version version 2.8.12.2 python version 3.6 cuda available yes cuda runtime version 8.0.44 gpu models configuration gpu 0 titan x pascal gpu 1 titan x pascal gpu 2 titan x pascal gpu 3 titan x pascal nvidia driver version 390.87 cudnn version could collect versions relevant libraries pip numpy 1.15.2 pip torch 0.4.1.post2 pip torchvision 0.2.1 conda pytorch 0.4.1 py36 py35 py27 9.0.176 7.1.2 2 pytorch conda torchvision 0.2.1 py36 1 pytorch",1,"index select flat tensor faster integer array indexing, even including reshaping","index select flat tensor faster integer array indexing, even including reshaping bug integer array indexing appears slower applying flattened tensor, using flattened indexes, reshaping expected output size. reproduce reproduced gist https gist.github.com gngdb 3d4f5aa27ee5199b0d4b997ffe21a6b4 expected behavior faster way implement integer array indexing, default. might though, uses integer array indexing speed difference might hold. environment pytorch version 0.4.1.post2 debug build cuda used build pytorch 9.0.176 os scientific linux release 7.5 nitrogen gcc version gcc 4.8.5 20150623 red hat 4.8.5 28 cmake version version 2.8.12.2 python version 3.6 cuda available yes cuda runtime version 8.0.44 gpu models configuration gpu 0 titan x pascal gpu 1 titan x pascal gpu 2 titan x pascal gpu 3 titan x pascal nvidia driver version 390.87 cudnn version could collect versions relevant libraries pip numpy 1.15.2 pip torch 0.4.1.post2 pip torchvision 0.2.1 conda pytorch 0.4.1 py36 py35 py27 9.0.176 7.1.2 2 pytorch conda torchvision 0.2.1 py36 1 pytorch"
pytorch,13722,"bug using particular architecture, pytorch throwing cuda oome much faster batch size 10k tensorflow runs smoothly batch size 200k 500k . reproduce steps reproduce behavior wrote two almost identical implementations problematic nn architecture pytorch tensorflow. pytorch version tensorflow run scripts 11gb gpu see cuda oome pytorch loss.backward line , tensorflow script. expected behavior would expect adequate gpu memory usage simple architecture small input size 10k numbers , case tensorflow, pytorch. environment environment google colaboratory, issue two machines pytorch version 0.4.1 debug build cuda used build pytorch 9.2.148 os ubuntu 18.04.1 lts gcc version ubuntu 7.3.0 27ubuntu1 18.04 7.3.0 cmake version could collect python version 3.6 cuda available yes cuda runtime version 9.2.148 gpu models configuration gpu 0 tesla k80 nvidia driver version 396.44 cudnn version probably one following usr lib x86 64 linux gnu libcudnn.so.7.3.1 versions relevant libraries pip could collect conda could collect",1,extremely high gpu memory usage simple architecture,"extremely high gpu memory usage simple architecture bug using particular architecture, pytorch throwing cuda oome much faster batch size 10k tensorflow runs smoothly batch size 200k 500k . reproduce steps reproduce behavior wrote two almost identical implementations problematic nn architecture pytorch tensorflow. pytorch version tensorflow run scripts 11gb gpu see cuda oome pytorch loss.backward line , tensorflow script. expected behavior would expect adequate gpu memory usage simple architecture small input size 10k numbers , case tensorflow, pytorch. environment environment google colaboratory, issue two machines pytorch version 0.4.1 debug build cuda used build pytorch 9.2.148 os ubuntu 18.04.1 lts gcc version ubuntu 7.3.0 27ubuntu1 18.04 7.3.0 cmake version could collect python version 3.6 cuda available yes cuda runtime version 9.2.148 gpu models configuration gpu 0 tesla k80 nvidia driver version 396.44 cudnn version probably one following usr lib x86 64 linux gnu libcudnn.so.7.3.1 versions relevant libraries pip could collect conda could collect"
pytorch,2973,"multiply binary operator two tensor variables, noticed weird behaviour depending shapes variables x.size 1,10 y.size 10 , x .size 1,10 expected b x.size 10 y.size 1,10 , x .size 1,10 expected c x.size 10,1 y.size 10 , x .size 10,10 unexpected x.size 10 y.size 10,1 , x .size 10,10 unexpected e x.size 10,1 y.size 1,10 , x .size 10,10 expected cases c really easily throw many perhaps debugging, cases happen loss function reduces output single scalar. expectation broadcasting first find matching axis, wonder current implementation behaviour expected designed certain goal mind.",1,broadcasting inconsistency?,"broadcasting inconsistency? multiply binary operator two tensor variables, noticed weird behaviour depending shapes variables x.size 1,10 y.size 10 , x .size 1,10 expected b x.size 10 y.size 1,10 , x .size 1,10 expected c x.size 10,1 y.size 10 , x .size 10,10 unexpected x.size 10 y.size 10,1 , x .size 10,10 unexpected e x.size 10,1 y.size 1,10 , x .size 10,10 expected cases c really easily throw many perhaps debugging, cases happen loss function reduces output single scalar. expectation broadcasting first find matching axis, wonder current implementation behaviour expected designed certain goal mind."
pytorch,21926,"bug indexing tensors storing element batch separately read individually disk. output model tensor size 40,100,256,256 . index one element store 100,256,256 dimensional tensor using torch.save takes exact time takes save whole 40,100,256,256 object. hand, .numpy process numpy much faster. may thing torch handles indexed sub tensors. separate object, one? dummy code reproduces issue pytorch 1.0.1. reproduce expected behavior expect torch numpy different. environment pytorch version 1.0.1 debug build cuda used build pytorch 10.0.130 os ubuntu 14.04.6 lts gcc version gcc 5.2.0 cmake version version 3.10.20171205 gd06b8 python version 3.6 cuda available yes cuda runtime version 10.1.105 gpu models configuration gpu 0 titan xp gpu 1 titan xp gpu 2 titan xp gpu 3 titan xp nvidia driver version 418.56 cudnn version could collect versions relevant libraries pip3 numpy 1.15.2 conda blas 1.0 mkl conda mkl 2019.1 144 conda mkl fft 1.0.10 py36ha843d7b 0 conda mkl random 1.0.2 py36hd81dba3 0 conda pytorch 1.0.1 cuda100py36he554f03 0 conda torchfile 0.1.0 conda torchnet 0.0.4 conda torchvision 0.2.1 py36 0 additional context saving indexed sub tensors important storing features images etc. due large dataset sizes, things usually done batches, may important extract element batch processed in.",1,bug saving indexed torch tensors makes much slower numpy.,"bug saving indexed torch tensors makes much slower numpy. bug indexing tensors storing element batch separately read individually disk. output model tensor size 40,100,256,256 . index one element store 100,256,256 dimensional tensor using torch.save takes exact time takes save whole 40,100,256,256 object. hand, .numpy process numpy much faster. may thing torch handles indexed sub tensors. separate object, one? dummy code reproduces issue pytorch 1.0.1. reproduce expected behavior expect torch numpy different. environment pytorch version 1.0.1 debug build cuda used build pytorch 10.0.130 os ubuntu 14.04.6 lts gcc version gcc 5.2.0 cmake version version 3.10.20171205 gd06b8 python version 3.6 cuda available yes cuda runtime version 10.1.105 gpu models configuration gpu 0 titan xp gpu 1 titan xp gpu 2 titan xp gpu 3 titan xp nvidia driver version 418.56 cudnn version could collect versions relevant libraries pip3 numpy 1.15.2 conda blas 1.0 mkl conda mkl 2019.1 144 conda mkl fft 1.0.10 py36ha843d7b 0 conda mkl random 1.0.2 py36hd81dba3 0 conda pytorch 1.0.1 cuda100py36he554f03 0 conda torchfile 0.1.0 conda torchnet 0.0.4 conda torchvision 0.2.1 py36 0 additional context saving indexed sub tensors important storing features images etc. due large dataset sizes, things usually done batches, may important extract element batch processed in."
pytorch,22961,"bug model historically trained 1080ti, recently discovered training speed much worse almost 2x slower 2080ti. rest setup nvidia driver cpu networking two. profiled script using , discovered 2080ti, way much time 70 spent function ideas problem could be? attached two profiles case helpful. 1080ti.log https github.com pytorch pytorch files 3400744 1080ti.log 2080ti.log https github.com pytorch pytorch files 3400745 2080ti.log pytorch version e.g., 1.0 1.1.0 os e.g., linux ubuntu 16.04 installed pytorch , , source pip build command used compiling source python version 3.6 cuda cudnn version 10.0 7.4 gpu models configuration 1080ti 2080ti, nvidia driver 410.78 relevant information",1,performance much worse 2080ti 1080ti,"performance much worse 2080ti 1080ti bug model historically trained 1080ti, recently discovered training speed much worse almost 2x slower 2080ti. rest setup nvidia driver cpu networking two. profiled script using , discovered 2080ti, way much time 70 spent function ideas problem could be? attached two profiles case helpful. 1080ti.log https github.com pytorch pytorch files 3400744 1080ti.log 2080ti.log https github.com pytorch pytorch files 3400745 2080ti.log pytorch version e.g., 1.0 1.1.0 os e.g., linux ubuntu 16.04 installed pytorch , , source pip build command used compiling source python version 3.6 cuda cudnn version 10.0 7.4 gpu models configuration 1080ti 2080ti, nvidia driver 410.78 relevant information"
pytorch,10681,"hi guys, tested simple example nn.dataparallel use multiple gpus, got hang. hangs try forward data model. nvidia smi gives tried solution https github.com pytorch pytorch issues 1637 issuecomment 338268158 , work. use cuda 9.1.85 pytorch 0.4.1 installed pip python 2.7.13 debian 4.9.110 3 deb9u1 2018 08 03 x86 64 gnu linux titan v cards ideas solve issue? let nvidia folks see issue?",1,nn.dataparallel hangs pytorch 0.4.1 cuda 9.1.85 titan v,"nn.dataparallel hangs pytorch 0.4.1 cuda 9.1.85 titan v hi guys, tested simple example nn.dataparallel use multiple gpus, got hang. hangs try forward data model. nvidia smi gives tried solution https github.com pytorch pytorch issues 1637 issuecomment 338268158 , work. use cuda 9.1.85 pytorch 0.4.1 installed pip python 2.7.13 debian 4.9.110 3 deb9u1 2018 08 03 x86 64 gnu linux titan v cards ideas solve issue? let nvidia folks see issue?"
pytorch,17350,"bug torch.nn.crossentropyloss output deterministic results segmentation outputs labels, using reduction 'none'. happens gpu. cpu give consistent behavior. reproduce output expected behavior believe expected behavior reduction 'sum' 'mean' consistent 'none' option use numpy reduction . environment additional context ran amazon k80 instance p2.xlarge . know seem like tiny error result two training sessions segmentation network identical parameters, initialization, order image batches, random seed, etc' produce identical results. problematic want investigate specific training session.",1,torch.nn.crossentropyloss reduction sum mean deterministic segmentation outputs labels,"torch.nn.crossentropyloss reduction sum mean deterministic segmentation outputs labels bug torch.nn.crossentropyloss output deterministic results segmentation outputs labels, using reduction 'none'. happens gpu. cpu give consistent behavior. reproduce output expected behavior believe expected behavior reduction 'sum' 'mean' consistent 'none' option use numpy reduction . environment additional context ran amazon k80 instance p2.xlarge . know seem like tiny error result two training sessions segmentation network identical parameters, initialization, order image batches, random seed, etc' produce identical results. problematic want investigate specific training session."
pytorch,3018,"installed pytorch head source cuda 9 cudnn 7 using gcc 5.4 ubuntu 16.04. seemed build install without issue, however python process hangs try move tensor gpu using cuda method. following snippet also hangs. 2 maxwell titan x 2 pascal 1080 ti cards attached machine.",1,cuda initialization hangs cuda 9,"cuda initialization hangs cuda 9 installed pytorch head source cuda 9 cudnn 7 using gcc 5.4 ubuntu 16.04. seemed build install without issue, however python process hangs try move tensor gpu using cuda method. following snippet also hangs. 2 maxwell titan x 2 pascal 1080 ti cards attached machine."
pytorch,3021,"platform c4.4xlarge ec2 instance 16 vcpus os fedora cloud 26 1.5 version pytorch v0.2.0 description running 4 workloads cores x8 slower pinning workload 4 different cpus. reproduce following yields 220 ms sequence utilizing cpus fully following yields 1850 ms sequence moreover, cpus 100 utilized... motivation wanted scale training multi core systems, single workload utilized cpus fully, thought utilizing rest resources running multiple workloads.",1,cpu scaling issue,"cpu scaling issue platform c4.4xlarge ec2 instance 16 vcpus os fedora cloud 26 1.5 version pytorch v0.2.0 description running 4 workloads cores x8 slower pinning workload 4 different cpus. reproduce following yields 220 ms sequence utilizing cpus fully following yields 1850 ms sequence moreover, cpus 100 utilized... motivation wanted scale training multi core systems, single workload utilized cpus fully, thought utilizing rest resources running multiple workloads."
pytorch,12238,"questions help please note issue tracker help form issue closed. hi , compiled caffe2 cpu inference python 3, get following warning question compile caffe2 optimization passes ? also noticed inference model uses one core cpu inference , compiled caffe2 nnpack enabled wich make use multi core bad set correct engine use convolution ops , that's nnpack used model used faster rcnn fpn 101 converted .pb detectron, inference time reported cpu image size 800x800 5.8 seconds thanks",1,caffe2 compiled without optimization passes.,"caffe2 compiled without optimization passes. questions help please note issue tracker help form issue closed. hi , compiled caffe2 cpu inference python 3, get following warning question compile caffe2 optimization passes ? also noticed inference model uses one core cpu inference , compiled caffe2 nnpack enabled wich make use multi core bad set correct engine use convolution ops , that's nnpack used model used faster rcnn fpn 101 converted .pb detectron, inference time reported cpu image size 800x800 5.8 seconds thanks"
pytorch,19407,"issue description seems quite slow depending input. example, sorting tensor calling make miserably slow. note issue exists even non pathological cases input fully sorted. originally observed beam search implementation seemed run faster greedy decoding times. code example minimal example highlighting issue. framed terms greedy decoding algorithm following timing results number different gpu configurations. note input sorted always slower , otherwise faster. speed difference definitely architecture dependent. additionally, much stable terms speed regardless sorting. timings titan x pascal timings 1080ti finally timings running google colab tesla t4 system info google colab system",1,torch.max slow inputs,"torch.max slow inputs issue description seems quite slow depending input. example, sorting tensor calling make miserably slow. note issue exists even non pathological cases input fully sorted. originally observed beam search implementation seemed run faster greedy decoding times. code example minimal example highlighting issue. framed terms greedy decoding algorithm following timing results number different gpu configurations. note input sorted always slower , otherwise faster. speed difference definitely architecture dependent. additionally, much stable terms speed regardless sorting. timings titan x pascal timings 1080ti finally timings running google colab tesla t4 system info google colab system"
pytorch,21462,"bug using large kernel size 1024 instance gpu, cudnn implementation slow gets slower increase kernel size. thought using fft apparently not. using fft, computation time independent kernel size, kernel anyway padded length input. tried benchmarking set . implementation using fft significantly faster especially using stride 1. find hereafter code fft based convolution implementation use profiling. implementation within 5e 5 reference implementation random weights input. kernel size 1024, 64 channels, stride 1 input length 64000, default implementation 20 times slower fft based one. using kernel size 2048, 40 times slower. reproduce steps reproduce behavior 1. copy code 2. run torch.cuda.synchronize expected behavior using stride 1 large kernel size, fft implementation much faster default one. fft one takes 160ms whatever size kernel, versus 3.3 seconds resp 6.7 default one kernel size 1024 resp 2048 . large strides, cudnn implementation competitive faster expected fft interest want convolution positions . would expect cudnn provide fast implementation large kernels low stride, especially useful audio filters implementation . talking around me, people surprised announced fft based implementation added cudnn. environment collecting environment information... pytorch version 1.1.0 debug build cuda used build pytorch 10.0.130 os ubuntu 18.04.1 lts gcc version ubuntu 7.3.0 27ubuntu1 18.04 7.3.0 cmake version version 3.13.4 python version 3.7 cuda available yes cuda runtime version could collect gpu models configuration gpu 0 quadro gp100 gpu 1 quadro gp100 nvidia driver version 410.79 cudnn version could collect versions relevant libraries pip3 numpy 1.15.4 pip3 torch 1.1.0 pip3 torchvision 0.2.2 conda blas 1.0 mkl conda mkl 2018.0.3 1 conda mkl fft 1.0.6 py37h7dd41cf 0 conda mkl random 1.0.1 py37h4414c95 1 conda pytorch 1.1.0 py3.7 cuda10.0.130 cudnn7.5.1 0 pytorch conda torchvision 0.2.2 py 3 pytorch cc csarofeen ptrblck mruberry peterbell10 vitalyfedyunin ngimel",1,"slow convolution large kernels, using fft","slow convolution large kernels, using fft bug using large kernel size 1024 instance gpu, cudnn implementation slow gets slower increase kernel size. thought using fft apparently not. using fft, computation time independent kernel size, kernel anyway padded length input. tried benchmarking set . implementation using fft significantly faster especially using stride 1. find hereafter code fft based convolution implementation use profiling. implementation within 5e 5 reference implementation random weights input. kernel size 1024, 64 channels, stride 1 input length 64000, default implementation 20 times slower fft based one. using kernel size 2048, 40 times slower. reproduce steps reproduce behavior 1. copy code 2. run torch.cuda.synchronize expected behavior using stride 1 large kernel size, fft implementation much faster default one. fft one takes 160ms whatever size kernel, versus 3.3 seconds resp 6.7 default one kernel size 1024 resp 2048 . large strides, cudnn implementation competitive faster expected fft interest want convolution positions . would expect cudnn provide fast implementation large kernels low stride, especially useful audio filters implementation . talking around me, people surprised announced fft based implementation added cudnn. environment collecting environment information... pytorch version 1.1.0 debug build cuda used build pytorch 10.0.130 os ubuntu 18.04.1 lts gcc version ubuntu 7.3.0 27ubuntu1 18.04 7.3.0 cmake version version 3.13.4 python version 3.7 cuda available yes cuda runtime version could collect gpu models configuration gpu 0 quadro gp100 gpu 1 quadro gp100 nvidia driver version 410.79 cudnn version could collect versions relevant libraries pip3 numpy 1.15.4 pip3 torch 1.1.0 pip3 torchvision 0.2.2 conda blas 1.0 mkl conda mkl 2018.0.3 1 conda mkl fft 1.0.6 py37h7dd41cf 0 conda mkl random 1.0.1 py37h4414c95 1 conda pytorch 1.1.0 py3.7 cuda10.0.130 cudnn7.5.1 0 pytorch conda torchvision 0.2.2 py 3 pytorch cc csarofeen ptrblck mruberry peterbell10 vitalyfedyunin ngimel"
pytorch,2520,using ddp module process always hangs. believe reduction threads loop. https github.com pytorch pytorch blob 4c69697d2acbbe8e418a0921464c09aaf09da82a torch nn parallel distributed.py l313 l315 repro code,1,distributeddataparallel exit properly leaves reduction threads running,distributeddataparallel exit properly leaves reduction threads running using ddp module process always hangs. believe reduction threads loop. https github.com pytorch pytorch blob 4c69697d2acbbe8e418a0921464c09aaf09da82a torch nn parallel distributed.py l313 l315 repro code
pytorch,8154,"issue description updating pytorch 0.3.1 0.4 yields significant performance drop small feedforward networks, trained cpu . effect happens linux virtual machine windows. attached code executed windows 10 machine intel i7 7700hq. linux test done virtual machine. computer equipped intel i7 4702mq. average runtime 10 repetitions linux 26.141655 pytorch 0.3.1 32.092158 0.4 23 increase average runtime 10 repetitions windows 12.577887 pytorch 0.3.1 17.759720 0.4 41 increase observations time difference 0.3.1 0.4 seems less constant windows linux. also profiling suggests dataloader next takes 8 instead 4 previously, major part increased time weirdly, different calls script, early stopping clause activates sometimes despire setting pytorch seed avoid effects due randomness code example see results data script used pytorchspeedtest.zip https github.com pytorch pytorch files 2071136 pytorchspeedtest.zip system info collect env yet adapted windows, therefore information missing. using pip 10.x latest conda pytorch version 0.4.0 debug build cuda used build pytorch 9.1 os microsoft windows 10 home gcc version could collect cmake version could collect python version 3.6 cuda available yes cuda runtime version 9.1.85 gpu models configuration could collect nvidia driver version could collect cudnn version could collect versions relevant libraries pip could collect conda could collect pytorch caffe2 pytorch installed pytorch conda, pip, source conda windows 0.3.1. via conda install c peterjc123 pytorch cpu os windows 10 ubuntu 16.04 pytorch version 0.3.1 0.4 python version 3.6 bottleneck profiling 0.4 cprofile 0.3.1",1,performance drop small models trained cpu 0.3.1 0.4,"performance drop small models trained cpu 0.3.1 0.4 issue description updating pytorch 0.3.1 0.4 yields significant performance drop small feedforward networks, trained cpu . effect happens linux virtual machine windows. attached code executed windows 10 machine intel i7 7700hq. linux test done virtual machine. computer equipped intel i7 4702mq. average runtime 10 repetitions linux 26.141655 pytorch 0.3.1 32.092158 0.4 23 increase average runtime 10 repetitions windows 12.577887 pytorch 0.3.1 17.759720 0.4 41 increase observations time difference 0.3.1 0.4 seems less constant windows linux. also profiling suggests dataloader next takes 8 instead 4 previously, major part increased time weirdly, different calls script, early stopping clause activates sometimes despire setting pytorch seed avoid effects due randomness code example see results data script used pytorchspeedtest.zip https github.com pytorch pytorch files 2071136 pytorchspeedtest.zip system info collect env yet adapted windows, therefore information missing. using pip 10.x latest conda pytorch version 0.4.0 debug build cuda used build pytorch 9.1 os microsoft windows 10 home gcc version could collect cmake version could collect python version 3.6 cuda available yes cuda runtime version 9.1.85 gpu models configuration could collect nvidia driver version could collect cudnn version could collect versions relevant libraries pip could collect conda could collect pytorch caffe2 pytorch installed pytorch conda, pip, source conda windows 0.3.1. via conda install c peterjc123 pytorch cpu os windows 10 ubuntu 16.04 pytorch version 0.3.1 0.4 python version 3.6 bottleneck profiling 0.4 cprofile 0.3.1"
pytorch,1509,"hello, new pytorch meet strange gpu memory behavior training cnn model semantic segmentation. batchsize 1, totally 100 image label pairs trainset, thus 100 iterations per epoch. however gpu memory consumption increases lot first several iterations training . platform gtx titan x 12g , cuda 7.5, cudnn 5.0 torch.backends.cudnn.enabled false torch.backends.cudnn.benchmark false gpu memory consumption 2934m 4413m 4433m 4537m 4537m 4537m first six iterations. torch.backends.cudnn.enabled true torch.backends.cudnn.benchmark true gpu memory consumption 1686m 1791m 1791m 1791m 1791m 1791m first six iterations. gpu memory consumption increases training, especially, increases largely cudnn? opinion, gpu memory consumption increase cnn build starts training anyone meet problem? could anyone give help? code snippet",1,gpu memory consumption increases training,"gpu memory consumption increases training hello, new pytorch meet strange gpu memory behavior training cnn model semantic segmentation. batchsize 1, totally 100 image label pairs trainset, thus 100 iterations per epoch. however gpu memory consumption increases lot first several iterations training . platform gtx titan x 12g , cuda 7.5, cudnn 5.0 torch.backends.cudnn.enabled false torch.backends.cudnn.benchmark false gpu memory consumption 2934m 4413m 4433m 4537m 4537m 4537m first six iterations. torch.backends.cudnn.enabled true torch.backends.cudnn.benchmark true gpu memory consumption 1686m 1791m 1791m 1791m 1791m 1791m first six iterations. gpu memory consumption increases training, especially, increases largely cudnn? opinion, gpu memory consumption increase cnn build starts training anyone meet problem? could anyone give help? code snippet"
pytorch,6126,"network then, created network training loops initial output large, became inf finally nan. happening?",1,inf nan loss,"inf nan loss network then, created network training loops initial output large, became inf finally nan. happening?"
pytorch,4081,testing resnet 50 noticed batch 128 gpu longer fits multi gpu. seeing around 20 gpu memory used see . 'old version' commit hash https github.com pytorch pytorch commit 50009144c02155be6afd4570e93f453e73904a8e 'new version' commit hash https github.com pytorch pytorch commit 7ddcb91c7f84c3da8cc9f7fba28a3ae9ecd6cc45,1,20 gpu memory usage resnet,20 gpu memory usage resnet testing resnet 50 noticed batch 128 gpu longer fits multi gpu. seeing around 20 gpu memory used see . 'old version' commit hash https github.com pytorch pytorch commit 50009144c02155be6afd4570e93f453e73904a8e 'new version' commit hash https github.com pytorch pytorch commit 7ddcb91c7f84c3da8cc9f7fba28a3ae9ecd6cc45
pytorch,17914,"bug pytorch's implementation argmin function returns incorrect maybe rather unexpected results using argmin min function tensors dimensions minimal value appears multiple times. pytorch picks last ! index said value, whereas one would assume first occurrence reported. inconsistent numpy's, eigen's, c stl etc. reproduce pytorch expected behavior numpy environment pytorch version 1.0.0 debug build cuda used build pytorch 9.0.176 os fedora release 27 twenty seven gcc version gcc 7.3.1 20180712 red hat 7.3.1 6 cmake version version 3.11.2 python version 3.6 cuda available cuda runtime version cuda gpu models configuration cuda nvidia driver version cuda cudnn version cuda versions relevant libraries pip3 numpy 1.14.6 pip3 torch 1.0.0 conda could collect additional context see numpy's argmin documentation https docs.scipy.org doc numpy reference generated numpy.argmin.html",1,incorrect behaviour min argmin,"incorrect behaviour min argmin bug pytorch's implementation argmin function returns incorrect maybe rather unexpected results using argmin min function tensors dimensions minimal value appears multiple times. pytorch picks last ! index said value, whereas one would assume first occurrence reported. inconsistent numpy's, eigen's, c stl etc. reproduce pytorch expected behavior numpy environment pytorch version 1.0.0 debug build cuda used build pytorch 9.0.176 os fedora release 27 twenty seven gcc version gcc 7.3.1 20180712 red hat 7.3.1 6 cmake version version 3.11.2 python version 3.6 cuda available cuda runtime version cuda gpu models configuration cuda nvidia driver version cuda cudnn version cuda versions relevant libraries pip3 numpy 1.14.6 pip3 torch 1.0.0 conda could collect additional context see numpy's argmin documentation https docs.scipy.org doc numpy reference generated numpy.argmin.html"
pytorch,25878,"bug silently fails model saved loaded different gpu devices. result, model remains randomly initialized state. occur pytorch 1.1 occur pytorch 1.2. reproduce steps reproduce behavior following code demonstrates bug 1. define functions creating simple model. loading model asserting loaded weights reflect weights checkpoint 2. create model, place gpu 0 save state dictionary 3. demonstrate model loads expected gpu 0 4. demonstrate model loads expected cpu 5. demonstrate model fails load gpu 1, retains randomly initialized weights. expected behavior environment please copy paste output environment collection script https raw.githubusercontent.com pytorch pytorch master torch utils collect env.py fill checklist manually . get script run pytorch version 1.2.0 debug build cuda used build pytorch 10.0.130 os ubuntu 16.04.6 lts gcc version ubuntu 5.4.0 6ubuntu1 16.04.11 5.4.0 20160609 cmake version version 3.5.1 python version 3.7 cuda available yes cuda runtime version could collect gpu models configuration gpu 0 tesla v100 pcie 16gb gpu 1 tesla v100 pcie 16gb gpu 2 tesla v100 pcie 16gb gpu 3 tesla v100 pcie 16gb gpu 4 tesla v100 pcie 16gb gpu 5 tesla v100 pcie 16gb gpu 6 tesla v100 pcie 16gb gpu 7 tesla v100 pcie 16gb nvidia driver version 418.67 cudnn version usr lib x86 64 linux gnu libcudnn.so.7.6.1 versions relevant libraries pip numpy 1.16.4 pip torch 1.2.0 pip torchvision 0.4.0a0 6b959ee conda blas 1.0 mkl conda mkl 2019.4 243 conda mkl service 2.0.2 py37h7b6447c 0 conda mkl fft 1.0.14 py37ha843d7b 0 conda mkl random 1.0.2 py37hd81dba3 0 conda pytorch 1.2.0 py3.7 cuda10.0.130 cudnn7.6.2 0 pytorch conda torchvision 0.4.0 pypi 0 pypi additional context cc ezyang gchanan zou3519",0,model weights silently fail load model different gpu saved,"model weights silently fail load model different gpu saved bug silently fails model saved loaded different gpu devices. result, model remains randomly initialized state. occur pytorch 1.1 occur pytorch 1.2. reproduce steps reproduce behavior following code demonstrates bug 1. define functions creating simple model. loading model asserting loaded weights reflect weights checkpoint 2. create model, place gpu 0 save state dictionary 3. demonstrate model loads expected gpu 0 4. demonstrate model loads expected cpu 5. demonstrate model fails load gpu 1, retains randomly initialized weights. expected behavior environment please copy paste output environment collection script https raw.githubusercontent.com pytorch pytorch master torch utils collect env.py fill checklist manually . get script run pytorch version 1.2.0 debug build cuda used build pytorch 10.0.130 os ubuntu 16.04.6 lts gcc version ubuntu 5.4.0 6ubuntu1 16.04.11 5.4.0 20160609 cmake version version 3.5.1 python version 3.7 cuda available yes cuda runtime version could collect gpu models configuration gpu 0 tesla v100 pcie 16gb gpu 1 tesla v100 pcie 16gb gpu 2 tesla v100 pcie 16gb gpu 3 tesla v100 pcie 16gb gpu 4 tesla v100 pcie 16gb gpu 5 tesla v100 pcie 16gb gpu 6 tesla v100 pcie 16gb gpu 7 tesla v100 pcie 16gb nvidia driver version 418.67 cudnn version usr lib x86 64 linux gnu libcudnn.so.7.6.1 versions relevant libraries pip numpy 1.16.4 pip torch 1.2.0 pip torchvision 0.4.0a0 6b959ee conda blas 1.0 mkl conda mkl 2019.4 243 conda mkl service 2.0.2 py37h7b6447c 0 conda mkl fft 1.0.14 py37ha843d7b 0 conda mkl random 1.0.2 py37hd81dba3 0 conda pytorch 1.2.0 py3.7 cuda10.0.130 cudnn7.6.2 0 pytorch conda torchvision 0.4.0 pypi 0 pypi additional context cc ezyang gchanan zou3519"
pytorch,27094,"docs mention traced, namedtuples cannot traced cc suo",0,jit document types traced,"jit document types traced docs mention traced, namedtuples cannot traced cc suo"
pytorch,20230,bug cannot create tensor using list tuples. reproduce steps reproduce behavior gives following error expected behavior give error. works pure python. environment pytorch 1.1.0 additional context workaround use list lists instead.,0,jit torch.tensor support list tuples,jit torch.tensor support list tuples bug cannot create tensor using list tuples. reproduce steps reproduce behavior gives following error expected behavior give error. works pure python. environment pytorch 1.1.0 additional context workaround use list lists instead.
pytorch,18053,bug following simple script https gist.github.com vlasenkov b3aa7c12570fe0056fca3421453470ca crashes following traceback reproduce run code single gpu. expected behavior script successfully finishes. environment,0,cudnn error using 3d convolutions,cudnn error using 3d convolutions bug following simple script https gist.github.com vlasenkov b3aa7c12570fe0056fca3421453470ca crashes following traceback reproduce run code single gpu. expected behavior script successfully finishes. environment
pytorch,25150,"bug pytorch 1.3.0 source installation necessary, one cuda 9.0. update version, root access. reproduce done everything say tutorial. result following output cmakererror main' checksymbolexists.c .text 0x1b undefined reference expected behavior environment pytorch version source debug build n cuda used build pytorch 9.0 os debian gnu linux 9.9 stretch gcc version debian 6.3.0 18 deb9u1 6.3.0 20170516 cmake version version 3.14.0 python version 3.7 cuda available yes cuda runtime version could collect gpu models configuration gpu 0 geforce gtx 1080 gpu 1 nvs 310 nvidia driver version 384.111 cudnn version could collect versions relevant libraries pip3 numpy 1.17.0 pip3 torch 1.2.0 pip3 torchvision 0.4.0 conda blas 1.0 mkl conda magma cuda90 2.5.0 1 pytorch conda mkl 2019.4 243 conda mkl include 2019.4 243 conda mkl service 2.0.2 py37h7b6447c 0 conda mkl fft 1.0.14 py37ha843d7b 0 conda mkl random 1.0.2 py37hd81dba3 0 additional context",0,problems install python source,"problems install python source bug pytorch 1.3.0 source installation necessary, one cuda 9.0. update version, root access. reproduce done everything say tutorial. result following output cmakererror main' checksymbolexists.c .text 0x1b undefined reference expected behavior environment pytorch version source debug build n cuda used build pytorch 9.0 os debian gnu linux 9.9 stretch gcc version debian 6.3.0 18 deb9u1 6.3.0 20170516 cmake version version 3.14.0 python version 3.7 cuda available yes cuda runtime version could collect gpu models configuration gpu 0 geforce gtx 1080 gpu 1 nvs 310 nvidia driver version 384.111 cudnn version could collect versions relevant libraries pip3 numpy 1.17.0 pip3 torch 1.2.0 pip3 torchvision 0.4.0 conda blas 1.0 mkl conda magma cuda90 2.5.0 1 pytorch conda mkl 2019.4 243 conda mkl include 2019.4 243 conda mkl service 2.0.2 py37h7b6447c 0 conda mkl fft 1.0.14 py37ha843d7b 0 conda mkl random 1.0.2 py37hd81dba3 0 additional context"
pytorch,19149,"pretty ridiculous that, 1d input like , still requires specify operating dimension 0. particular error, saying needs dim argument. works. suggestion that, least 1d inputs, default dim operate set 0. problem , name two. cc jlin27 mruberry rgommers",0,make operators like logsumexp cumsum operate dimension 0 default least 1d arrays,"make operators like logsumexp cumsum operate dimension 0 default least 1d arrays pretty ridiculous that, 1d input like , still requires specify operating dimension 0. particular error, saying needs dim argument. works. suggestion that, least 1d inputs, default dim operate set 0. problem , name two. cc jlin27 mruberry rgommers"
pytorch,16527,"cudart python ctypes binding libcudart.so library. previously used streams events unused. initialization load cudart sometimes fail. example, zou3519 ran",0,delete cudart torch cuda init .py,"delete cudart torch cuda init .py cudart python ctypes binding libcudart.so library. previously used streams events unused. initialization load cudart sometimes fail. example, zou3519 ran"
pytorch,7342,"currently, easy way change decay momentum lr parameter constructing optimizer. example, hard stop parameter decayed iteration. maybe support getter setter api like . cc vincentqb",0,feature request pytorch flexible optimizer api,"feature request pytorch flexible optimizer api currently, easy way change decay momentum lr parameter constructing optimizer. example, hard stop parameter decayed iteration. maybe support getter setter api like . cc vincentqb"
pytorch,13618,nan,0,test dist broadcast coalesced nccl timeout ci,test dist broadcast coalesced nccl timeout ci nan
pytorch,27421,currently base abstract sampler class https github.com pytorch pytorch blob master torch utils data sampler.py l17 declared common argument saved . many samplers take argument indeed that. exist samplers even take argument. two proposed suggestions 1 add abstract class 2 remove method base abstract class completely cc ssnl,0,dataloader sampler abstract constructor api minor proposal,dataloader sampler abstract constructor api minor proposal currently base abstract sampler class https github.com pytorch pytorch blob master torch utils data sampler.py l17 declared common argument saved . many samplers take argument indeed that. exist samplers even take argument. two proposed suggestions 1 add abstract class 2 remove method base abstract class completely cc ssnl
pytorch,22120,"bug print torch. version 1.1.0 print torch.version.cuda 9.0.176 os linux 18.04 installed pytorch , , source try them, try compile source. python version 3.6 cuda cudnn version 7.6.0 hi, try convert .pth weights yolact project onnx, running following code get following error",0,convert pth onnx,"convert pth onnx bug print torch. version 1.1.0 print torch.version.cuda 9.0.176 os linux 18.04 installed pytorch , , source try them, try compile source. python version 3.6 cuda cudnn version 7.6.0 hi, try convert .pth weights yolact project onnx, running following code get following error"
pytorch,2576,"reported moustapha6c. following fails, many categories cc ezyang gchanan zou3519 bdhirsh jbschlosser anjali411 ngimel fritzo neerajprad alicanb vishwakftw nikitaved vincentqb",0,cuda multinomial limited 2 24 categories,"cuda multinomial limited 2 24 categories reported moustapha6c. following fails, many categories cc ezyang gchanan zou3519 bdhirsh jbschlosser anjali411 ngimel fritzo neerajprad alicanb vishwakftw nikitaved vincentqb"
pytorch,14320,"bug caffe2 git master fails build fbgemm enabled , giving following error error seems caused recent commit https github.com pytorch pytorch commit fb8c3d62feacf5c3d39f6fb034db944a89a0bcf4. building fine commits it. reproduce steps reproduce behavior 1. mkdir build 1. cd build 1. cmake options https bpaste.net show ca5a7eb99368 1. make j1 expected behavior succesful build fbgemm enabled. environment pytorch version git master currently https github.com pytorch pytorch commit f79fb58744ba70970de652e46ea039b03e9ce9ff os arch linux x86 64 installed pytorch source build command used compiling source already shown python version 3.7.1 cuda cudnn version enabled example gpu models configuration enabled example compiler gcc 8.2.1 relevant information error occurs enabling cuda using gcc 7.3.1 additional context builds fine disabling fbgemm .",0,caffe2 fails build fbgemm enabled,"caffe2 fails build fbgemm enabled bug caffe2 git master fails build fbgemm enabled , giving following error error seems caused recent commit https github.com pytorch pytorch commit fb8c3d62feacf5c3d39f6fb034db944a89a0bcf4. building fine commits it. reproduce steps reproduce behavior 1. mkdir build 1. cd build 1. cmake options https bpaste.net show ca5a7eb99368 1. make j1 expected behavior succesful build fbgemm enabled. environment pytorch version git master currently https github.com pytorch pytorch commit f79fb58744ba70970de652e46ea039b03e9ce9ff os arch linux x86 64 installed pytorch source build command used compiling source already shown python version 3.7.1 cuda cudnn version enabled example gpu models configuration enabled example compiler gcc 8.2.1 relevant information error occurs enabling cuda using gcc 7.3.1 additional context builds fine disabling fbgemm ."
pytorch,15617,"feature transposed version locally connected layer, similar transposed version convolution layer without weight sharing. motivation similar necessity transposed convolution layer, locally connected layer issue 499, pr 1583 also transposed version. cc alband mruberry jbschlosser",0,feature request transposed locally connected layer,"feature request transposed locally connected layer feature transposed version locally connected layer, similar transposed version convolution layer without weight sharing. motivation similar necessity transposed convolution layer, locally connected layer issue 499, pr 1583 also transposed version. cc alband mruberry jbschlosser"
pytorch,7480,want run caffe2 model multiple cpus evaluation . way directly? maybe requires openblas? think possible use multiprocessing call runnet different workspaces.,0,caffe2 use multiple cpus?,caffe2 use multiple cpus? want run caffe2 model multiple cpus evaluation . way directly? maybe requires openblas? think possible use multiprocessing call runnet different workspaces.
pytorch,20009,"bug getting error converting saved torch model onnx format torch.onnx.export trained model, dummy input, sentiment.onnx typeerror forward missing 1 required positional argument 'hidden' torch version '1.0.1.post2' reproduce steps reproduce behavior torch.autograd import variable import torch.onnx load trained model file net save sentimentrnn vocab size, output size, embedding dim, hidden dim, n layers trained model net save trained model.load state dict torch.load 'sentiment.pth' export trained model onnx dummy input variable torch.randn 1, 1, 28, 28 one black white 28 x 28 picture input model torch.onnx.export trained model, dummy input, sentiment.onnx expected behavior get onnx format model getting error",0,issue exporting torch model onnx format,"issue exporting torch model onnx format bug getting error converting saved torch model onnx format torch.onnx.export trained model, dummy input, sentiment.onnx typeerror forward missing 1 required positional argument 'hidden' torch version '1.0.1.post2' reproduce steps reproduce behavior torch.autograd import variable import torch.onnx load trained model file net save sentimentrnn vocab size, output size, embedding dim, hidden dim, n layers trained model net save trained model.load state dict torch.load 'sentiment.pth' export trained model onnx dummy input variable torch.randn 1, 1, 28, 28 one black white 28 x 28 picture input model torch.onnx.export trained model, dummy input, sentiment.onnx expected behavior get onnx format model getting error"
pytorch,30139,"so, trying convert torchscript model onnx. scripting tracing works creation graph, fails model converted onnx here's error traceback recent call last file yolo2script.py , line 8, torch.onnx.export model scripted, dummy input, yolov3.onnx , example outputs model dummy input file usr local lib python3.6 dist packages torch onnx init .py , line 143, export strip doc string, dynamic axes, keep initializers inputs file usr local lib python3.6 dist packages torch onnx utils.py , line 66, export dynamic axes dynamic axes, keep initializers inputs keep initializers inputs file usr local lib python3.6 dist packages torch onnx utils.py , line 382, export fixed batch size fixed batch size file usr local lib python3.6 dist packages torch onnx utils.py , line 235, model graph method graph, params model.forward. lowered graph runtimeerror istensor internal assert failed pytorch aten src aten core ivalue inl.h 90, please report bug pytorch. expected tensor got int totensor pytorch aten src aten core ivalue inl.h 90 frame 0 c10 error error c10 sourcelocation, std string const 0x33 0x7f83210f0813 usr local lib python3.6 dist packages torch lib libc10.so frame 1 0x1876251 0x7f82bdc7d251 usr local lib python3.6 dist packages torch lib libtorch.so frame 2 torch jit script method lowered graph 0x161 0x7f82c0420771 usr local lib python3.6 dist packages torch lib libtorch.so frame 3 0x5af698 0x7f832229b698 usr local lib python3.6 dist packages torch lib libtorch python.so frame 4 0x2110f4 0x7f8321efd0f4 usr local lib python3.6 dist packages torch lib libtorch python.so frame 6 python3 0x4f88ba frame 8 python3 0x4f6128 frame 9 python3 0x4f7d60 frame 10 python3 0x4f876d frame 12 python3 0x4f6128 frame 13 python3 0x4f7d60 frame 14 python3 0x4f876d frame 16 python3 0x4f6128 frame 17 python3 0x4f7d60 frame 18 python3 0x4f876d frame 20 python3 0x4f6128 frame 21 python3 0x4f7d60 frame 22 python3 0x4f876d frame 24 python3 0x4f6128 frame 26 python3 0x6415b2 frame 31 libc start main 0xe7 0x7f83267afb97 lib x86 64 linux gnu libc.so.6 here's system collecting environment information... pytorch version 1.3.1 debug build cuda used build pytorch 10.1.243 os ubuntu 18.04.2 lts gcc version ubuntu 7.4.0 1ubuntu1 18.04.1 7.4.0 cmake version version 3.16.0 rc3 python version 3.6 cuda available yes cuda runtime version 10.0.130 gpu models configuration gpu 0 geforce gtx 1060 max q design nvidia driver version 418.87.00 cudnn version usr lib x86 64 linux gnu libcudnn.so.7.6.2 versions relevant libraries pip3 efficientnet pytorch 0.4.0 pip3 numpy 1.17.4 pip3 torch 1.3.1 pip3 torch2trt 0.0.2 pip3 torchetl 0.3.9 pip3 torchvision 0.4.2 conda could collect need model source code, provide asap. thank ! cc houseroad spandantiwari lara hdr bowenbao neginraoof",0,"runtimeerror istensor internal assert failed pytorch aten src aten core ivalue inl.h 90, please report bug pytorch. expected tensor got int totensor pytorch aten src aten core ivalue inl.h 90","runtimeerror istensor internal assert failed pytorch aten src aten core ivalue inl.h 90, please report bug pytorch. expected tensor got int totensor pytorch aten src aten core ivalue inl.h 90 so, trying convert torchscript model onnx. scripting tracing works creation graph, fails model converted onnx here's error traceback recent call last file yolo2script.py , line 8, torch.onnx.export model scripted, dummy input, yolov3.onnx , example outputs model dummy input file usr local lib python3.6 dist packages torch onnx init .py , line 143, export strip doc string, dynamic axes, keep initializers inputs file usr local lib python3.6 dist packages torch onnx utils.py , line 66, export dynamic axes dynamic axes, keep initializers inputs keep initializers inputs file usr local lib python3.6 dist packages torch onnx utils.py , line 382, export fixed batch size fixed batch size file usr local lib python3.6 dist packages torch onnx utils.py , line 235, model graph method graph, params model.forward. lowered graph runtimeerror istensor internal assert failed pytorch aten src aten core ivalue inl.h 90, please report bug pytorch. expected tensor got int totensor pytorch aten src aten core ivalue inl.h 90 frame 0 c10 error error c10 sourcelocation, std string const 0x33 0x7f83210f0813 usr local lib python3.6 dist packages torch lib libc10.so frame 1 0x1876251 0x7f82bdc7d251 usr local lib python3.6 dist packages torch lib libtorch.so frame 2 torch jit script method lowered graph 0x161 0x7f82c0420771 usr local lib python3.6 dist packages torch lib libtorch.so frame 3 0x5af698 0x7f832229b698 usr local lib python3.6 dist packages torch lib libtorch python.so frame 4 0x2110f4 0x7f8321efd0f4 usr local lib python3.6 dist packages torch lib libtorch python.so frame 6 python3 0x4f88ba frame 8 python3 0x4f6128 frame 9 python3 0x4f7d60 frame 10 python3 0x4f876d frame 12 python3 0x4f6128 frame 13 python3 0x4f7d60 frame 14 python3 0x4f876d frame 16 python3 0x4f6128 frame 17 python3 0x4f7d60 frame 18 python3 0x4f876d frame 20 python3 0x4f6128 frame 21 python3 0x4f7d60 frame 22 python3 0x4f876d frame 24 python3 0x4f6128 frame 26 python3 0x6415b2 frame 31 libc start main 0xe7 0x7f83267afb97 lib x86 64 linux gnu libc.so.6 here's system collecting environment information... pytorch version 1.3.1 debug build cuda used build pytorch 10.1.243 os ubuntu 18.04.2 lts gcc version ubuntu 7.4.0 1ubuntu1 18.04.1 7.4.0 cmake version version 3.16.0 rc3 python version 3.6 cuda available yes cuda runtime version 10.0.130 gpu models configuration gpu 0 geforce gtx 1060 max q design nvidia driver version 418.87.00 cudnn version usr lib x86 64 linux gnu libcudnn.so.7.6.2 versions relevant libraries pip3 efficientnet pytorch 0.4.0 pip3 numpy 1.17.4 pip3 torch 1.3.1 pip3 torch2trt 0.0.2 pip3 torchetl 0.3.9 pip3 torchvision 0.4.2 conda could collect need model source code, provide asap. thank ! cc houseroad spandantiwari lara hdr bowenbao neginraoof"
pytorch,24608,porting th operators essential code simplicity performance reasons. porting guides q available umbrella issue 24507 feel free add vitalyfedyunin reviewer get prioritized review.,0,migrate nll loss2d forward th aten cuda,migrate nll loss2d forward th aten cuda porting th operators essential code simplicity performance reasons. porting guides q available umbrella issue 24507 feel free add vitalyfedyunin reviewer get prioritized review.
pytorch,17632,questions help looking caffe2 c script supports latest version. find good resource. anyone provide caffe2 c example script classification. thanks.,0,caffe2 c script classification object detection cmakelists.txt,caffe2 c script classification object detection cmakelists.txt questions help looking caffe2 c script supports latest version. find good resource. anyone provide caffe2 c example script classification. thanks.
pytorch,11982,"would great wheels pytorch debug symbols published least made available via https ci.pytorch.org , users investigate native crashes without going whole error prone process building pytorch themselves. reference, asked question forums first, got answer https discuss.pytorch.org pytorch wheel debug symbols 25169 . cc ezyang seemethere malfet walterddr",0,feature request publish wheels debug symbols,"feature request publish wheels debug symbols would great wheels pytorch debug symbols published least made available via https ci.pytorch.org , users investigate native crashes without going whole error prone process building pytorch themselves. reference, asked question forums first, got answer https discuss.pytorch.org pytorch wheel debug symbols 25169 . cc ezyang seemethere malfet walterddr"
pytorch,24625,porting th operators essential code simplicity performance reasons. porting guides q available umbrella issue 24507 feel free add vitalyfedyunin reviewer get prioritized review.,0,migrate sigmoid backward th aten cuda,migrate sigmoid backward th aten cuda porting th operators essential code simplicity performance reasons. porting guides q available umbrella issue 24507 feel free add vitalyfedyunin reviewer get prioritized review.
pytorch,7764,"instructions https github.com pytorch pytorch source compiling source say use following command macos try compile command, fails following error able get compile using following command instead system info pytorch caffe2 pytorch installed pytorch conda, pip, source source build command used compiling source see os macos 10.13.4 pytorch version latest code repository python version 3.6.1",0,incorrect compilation instructions mac,"incorrect compilation instructions mac instructions https github.com pytorch pytorch source compiling source say use following command macos try compile command, fails following error able get compile using following command instead system info pytorch caffe2 pytorch installed pytorch conda, pip, source source build command used compiling source see os macos 10.13.4 pytorch version latest code repository python version 3.6.1"
pytorch,1434,"building source, got error. system osx 10.12.4, conda, cuda 8.0. thoughts? thanks!",0,member named 'shared ptr' namespace 'std',"member named 'shared ptr' namespace 'std' building source, got error. system osx 10.12.4, conda, cuda 8.0. thoughts? thanks!"
pytorch,22764,"returns real imaginary components last dimension. two frequent functions consume numpy . abs similar regular , meanwhile complex tensor support developed, could 's job. numpy also , although accepts two arrays. may two versions one accepting two arrays, another accepting one array dim. essentially, version , specialized two element vectors only.",0,feature request torch.hypot,"feature request torch.hypot returns real imaginary components last dimension. two frequent functions consume numpy . abs similar regular , meanwhile complex tensor support developed, could 's job. numpy also , although accepts two arrays. may two versions one accepting two arrays, another accepting one array dim. essentially, version , specialized two element vectors only."
pytorch,13491,"feature would useful library numerical ode solvers compatible pytorch. motivation neural networks increasingly combined odes. example, modelling odes form part probabilistic model, defining derivative neural network neural odes . may want get speedup using gpu pytorch this, useful able differentiate algorithms too. pitch proposing create module contains half dozen common ode solvers. instance, could call calculate points equation",0,numerical ode solvers,"numerical ode solvers feature would useful library numerical ode solvers compatible pytorch. motivation neural networks increasingly combined odes. example, modelling odes form part probabilistic model, defining derivative neural network neural odes . may want get speedup using gpu pytorch this, useful able differentiate algorithms too. pitch proposing create module contains half dozen common ode solvers. instance, could call calculate points equation"
pytorch,6894,issue description unfriendly exception output input data size last two dims upsample param 'size' value code example output home user .pyenv versions 3.6.4 envs shinerio python3.6.4 env bin python' free invalid pointer 0x00007fbea6fdfd40 backtrace lib x86 64 linux gnu libc.so.6 0x777e5 0x7fbed7b017e5 lib x86 64 linux gnu libc.so.6 0x8037a 0x7fbed7b0a37a lib x86 64 linux gnu libc.so.6 cfree 0x4c 0x7fbed7b0e53c home user .pyenv versions 3.6.4 envs shinerio python3.6.4 env lib python3.6 site packages torch thnn thnn.cpython 36m x86 64 linux gnu.so 0x2f367 0x7fbe764f6367 home user .pyenv versions 3.6.4 envs shinerio python3.6.4 env bin python pycfunction fastcallkeywords 0x279 0x4aefe9 home user .pyenv versions 3.6.4 envs shinerio python3.6.4 env bin python 0x54060e home user .pyenv versions 3.6.4 envs shinerio python3.6.4 env bin python pyeval evalframedefault 0x102d 0x54268d home user .pyenv versions 3.6.4 envs shinerio python3.6.4 env bin python 0x540275 home user .pyenv versions 3.6.4 envs shinerio python3.6.4 env bin python pyeval evalcodeex 0x3e 0x54118e home user .pyenv versions 3.6.4 envs shinerio python3.6.4 env bin python 0x485828 home user .pyenv versions 3.6.4 envs shinerio python3.6.4 env bin python pyobject call 0x5c 0x45322c home user .pyenv versions 3.6.4 envs shinerio python3.6.4 env lib python3.6 site packages torch c.cpython 36m x86 64 linux gnu.so z17thpfunction applyp7 objects0 0x271 0x7fbea5ae7ac1 7fbed899f000 7fbed89a6000 r 00000000 08 02 17172406 usr lib x86 64 linux gnu gconv gconv modules.cache 7fbed89a6000 7fbed89a7000 r p 00025000 08 02 8912913 lib x86 64 linux gnu ld 2.23.so 7fbed89a7000 7fbed89a8000 rw p 00026000 08 02 8912913 lib x86 64 linux gnu ld 2.23.so 7fbed89a8000 7fbed89a9000 rw p 00000000 00 00 0 7ffc0354d000 7ffc0356e000 rw p 00000000 00 00 0 stack 7ffc03575000 7ffc03578000 r p 00000000 00 00 0 vvar 7ffc03578000 7ffc0357a000 r xp 00000000 00 00 0 vdso ffffffffff600000 ffffffffff601000 r xp 00000000 00 00 0 vsyscall process finished exit code 1 system info pytorch version 0.3.1 debug build cuda used build pytorch 8.0.61 os ubuntu 16.04.2 lts gcc version ubuntu 5.4.0 6ubuntu1 16.04.9 5.4.0 20160609 cmake version version 3.5.1 python version 3.6 cuda available yes cuda runtime version could collect gpu models configuration gpu 0 tesla m40 24gb nvidia driver version 384.111 cudnn version probably one following usr local cuda 9.0 lib64 libcudnn.so usr local cuda 9.0 lib64 libcudnn.so.7 usr local cuda 9.0 lib64 libcudnn.so.7.0.5 usr local cuda 9.0 lib64 libcudnn static.a versions relevant libraries pip3 numpy 1.14.2 conda could collect,0,maybe bug torch.nn.upsample,maybe bug torch.nn.upsample issue description unfriendly exception output input data size last two dims upsample param 'size' value code example output home user .pyenv versions 3.6.4 envs shinerio python3.6.4 env bin python' free invalid pointer 0x00007fbea6fdfd40 backtrace lib x86 64 linux gnu libc.so.6 0x777e5 0x7fbed7b017e5 lib x86 64 linux gnu libc.so.6 0x8037a 0x7fbed7b0a37a lib x86 64 linux gnu libc.so.6 cfree 0x4c 0x7fbed7b0e53c home user .pyenv versions 3.6.4 envs shinerio python3.6.4 env lib python3.6 site packages torch thnn thnn.cpython 36m x86 64 linux gnu.so 0x2f367 0x7fbe764f6367 home user .pyenv versions 3.6.4 envs shinerio python3.6.4 env bin python pycfunction fastcallkeywords 0x279 0x4aefe9 home user .pyenv versions 3.6.4 envs shinerio python3.6.4 env bin python 0x54060e home user .pyenv versions 3.6.4 envs shinerio python3.6.4 env bin python pyeval evalframedefault 0x102d 0x54268d home user .pyenv versions 3.6.4 envs shinerio python3.6.4 env bin python 0x540275 home user .pyenv versions 3.6.4 envs shinerio python3.6.4 env bin python pyeval evalcodeex 0x3e 0x54118e home user .pyenv versions 3.6.4 envs shinerio python3.6.4 env bin python 0x485828 home user .pyenv versions 3.6.4 envs shinerio python3.6.4 env bin python pyobject call 0x5c 0x45322c home user .pyenv versions 3.6.4 envs shinerio python3.6.4 env lib python3.6 site packages torch c.cpython 36m x86 64 linux gnu.so z17thpfunction applyp7 objects0 0x271 0x7fbea5ae7ac1 7fbed899f000 7fbed89a6000 r 00000000 08 02 17172406 usr lib x86 64 linux gnu gconv gconv modules.cache 7fbed89a6000 7fbed89a7000 r p 00025000 08 02 8912913 lib x86 64 linux gnu ld 2.23.so 7fbed89a7000 7fbed89a8000 rw p 00026000 08 02 8912913 lib x86 64 linux gnu ld 2.23.so 7fbed89a8000 7fbed89a9000 rw p 00000000 00 00 0 7ffc0354d000 7ffc0356e000 rw p 00000000 00 00 0 stack 7ffc03575000 7ffc03578000 r p 00000000 00 00 0 vvar 7ffc03578000 7ffc0357a000 r xp 00000000 00 00 0 vdso ffffffffff600000 ffffffffff601000 r xp 00000000 00 00 0 vsyscall process finished exit code 1 system info pytorch version 0.3.1 debug build cuda used build pytorch 8.0.61 os ubuntu 16.04.2 lts gcc version ubuntu 5.4.0 6ubuntu1 16.04.9 5.4.0 20160609 cmake version version 3.5.1 python version 3.6 cuda available yes cuda runtime version could collect gpu models configuration gpu 0 tesla m40 24gb nvidia driver version 384.111 cudnn version probably one following usr local cuda 9.0 lib64 libcudnn.so usr local cuda 9.0 lib64 libcudnn.so.7 usr local cuda 9.0 lib64 libcudnn.so.7.0.5 usr local cuda 9.0 lib64 libcudnn static.a versions relevant libraries pip3 numpy 1.14.2 conda could collect
pytorch,6064,methods returning values generate weird error messages instance produces expected? omitting return statement valid return 0 values someone might print debugging point written return yet value work.,0,jit script handling 'void' returns,jit script handling 'void' returns methods returning values generate weird error messages instance produces expected? omitting return statement valid return 0 values someone might print debugging point written return yet value work.
pytorch,17484,"bug instantiation custom module, parameters register initialized .to 'cuda' function call parameter level. reproduce hand, .to 'cuda' called wrapped tensor, everything works expected. os ubuntu 16.04.5 lts gcc version ubuntu 5.4.0 6ubuntu1 16.04.11 5.4.0 20160609 cmake version could collect python version 3.5 cuda available n cuda runtime version 7.5.17 gpu models configuration gpu 0 geforce gtx 1070 nvidia driver version 384.130 cudnn version could collect versions relevant libraries pip msgpack numpy 0.4.1 pip numpy 1.13.3 pip torch 1.0.0 pip torchvision 0.2.1 conda could collect",0,parameter registering .to device used,"parameter registering .to device used bug instantiation custom module, parameters register initialized .to 'cuda' function call parameter level. reproduce hand, .to 'cuda' called wrapped tensor, everything works expected. os ubuntu 16.04.5 lts gcc version ubuntu 5.4.0 6ubuntu1 16.04.11 5.4.0 20160609 cmake version could collect python version 3.5 cuda available n cuda runtime version 7.5.17 gpu models configuration gpu 0 geforce gtx 1070 nvidia driver version 384.130 cudnn version could collect versions relevant libraries pip msgpack numpy 0.4.1 pip numpy 1.13.3 pip torch 1.0.0 pip torchvision 0.2.1 conda could collect"
pytorch,2523,"currently, cudnnsetstream calls missing bindings, fix them. context https discuss.pytorch.org torch nn context cuda stream 6304 3",0,cudnn bindings respecting current stream,"cudnn bindings respecting current stream currently, cudnnsetstream calls missing bindings, fix them. context https discuss.pytorch.org torch nn context cuda stream 6304 3"
pytorch,19201,"currently .t7 file contains arrays raw text, want reduce number entries there. edit file? far, display file's contents using",0,edit .t7 file?,"edit .t7 file? currently .t7 file contains arrays raw text, want reduce number entries there. edit file? far, display file's contents using"
pytorch,15028,"hi. noticed that, pytorch installed, pkg info file says license unknown. tools rely file obtain correct license information. possible put correct license information it? example,",0,license unavailable installed package,"license unavailable installed package hi. noticed that, pytorch installed, pkg info file says license unknown. tools rely file obtain correct license information. possible put correct license information it? example,"
pytorch,4115,would interested extending alias multinomial sampling pytorch. c cuda version already present https github.com torch torch7 blob aed31711c6b8846b8337a263a7f9f998697994e7 doc maths.md res torchmultinomialaliasoutput state linked believe could useful people want use multinomial sampling static distribution.,0,feature request alias multinomial,feature request alias multinomial would interested extending alias multinomial sampling pytorch. c cuda version already present https github.com torch torch7 blob aed31711c6b8846b8337a263a7f9f998697994e7 doc maths.md res torchmultinomialaliasoutput state linked believe could useful people want use multinomial sampling static distribution.
pytorch,3462,"above, matrix large numbers, every element same, added matrix small numbers affect ordering small matrix . hence same. however, different rest, meaning adding large numbers input makes behave weirdly. , presumably reason. getting weird results beam search implementation pytorch, culprit. beam search sort list large negative numbers log probabilities candidate translation sentences . anyone suggest quick workaround? thanks!",0,sort topk behave weirdly given large numbers,"sort topk behave weirdly given large numbers above, matrix large numbers, every element same, added matrix small numbers affect ordering small matrix . hence same. however, different rest, meaning adding large numbers input makes behave weirdly. , presumably reason. getting weird results beam search implementation pytorch, culprit. beam search sort list large negative numbers log probabilities candidate translation sentences . anyone suggest quick workaround? thanks!"
pytorch,15620,cuda runtime error 11 invalid argumen pytorch torch lib thc generic thctensorcopy.c 70,0,cuda,cuda cuda runtime error 11 invalid argumen pytorch torch lib thc generic thctensorcopy.c 70
pytorch,3131,", keyword argument named , tensor variable, keyword argument .",0,cuda methods module tensor variable inconsistent,"cuda methods module tensor variable inconsistent , keyword argument named , tensor variable, keyword argument ."
pytorch,11504,"issue description normal distribution hence distributions use sampler throwing runtime exception jit seems recent regression . isolated issue following code snippet throws exception pytorch master, breaking many pytorch models. cc. zou3519, apaszke, fritzo. code example following code returns sample normal distribution, throws runtime exception system info",0,jit tracer throws runtime exception torch.normal,"jit tracer throws runtime exception torch.normal issue description normal distribution hence distributions use sampler throwing runtime exception jit seems recent regression . isolated issue following code snippet throws exception pytorch master, breaking many pytorch models. cc. zou3519, apaszke, fritzo. code example following code returns sample normal distribution, throws runtime exception system info"
pytorch,260,"think would useful cuda.set default device pytorch, gpu 0 always default one.",0,define default gpu device,"define default gpu device think would useful cuda.set default device pytorch, gpu 0 always default one."
pytorch,13494,"bug special backward multiple times since saved variable, nothing really destroyed first backward. behavior bad might silently backward multiple times without noticing, grad accumulates. reproduce zdevito told briefly fix, send pr soon.",0,throws error backward sum twice,"throws error backward sum twice bug special backward multiple times since saved variable, nothing really destroyed first backward. behavior bad might silently backward multiple times without noticing, grad accumulates. reproduce zdevito told briefly fix, send pr soon."
pytorch,11737,"issue description using torch.utils.cpp extension.load, returned module always puts tensors gpu 0 'cuda 0' causes problems model moved another gpu. example used code https github.com mapillary inplace abn see also mapillary inplace abn 52 code example system info please copy paste output environment collection script https raw.githubusercontent.com pytorch pytorch master torch utils collect env.py fill checklist manually . pytorch version 0.4.0 debug build cuda used build pytorch 8.0.61 os ubuntu 16.04.4 lts gcc version ubuntu 4.9.4 2ubuntu1 16.04 4.9.4 cmake version version 3.12.0 python version 3.5 cuda available yes cuda runtime version 9.0.176 gpu models configuration gpu 0 geforce gtx 1080 ti gpu 1 geforce gtx 1080 ti gpu 2 geforce gtx 1080 ti gpu 3 geforce gtx 1080 ti nvidia driver version 384.130 cudnn version probably one following usr local cuda 8.0 lib64 libcudnn.so usr local cuda 8.0 lib64 libcudnn.so.6 usr local cuda 8.0 lib64 libcudnn.so.6.0.21 usr local cuda 8.0 lib64 libcudnn static.a usr local cuda 9.0 lib64 libcudnn.so.7.0.5 usr local cuda 9.0 lib64 libcudnn static.a",0,torch.utils.cpp extension.load change device moving model,"torch.utils.cpp extension.load change device moving model issue description using torch.utils.cpp extension.load, returned module always puts tensors gpu 0 'cuda 0' causes problems model moved another gpu. example used code https github.com mapillary inplace abn see also mapillary inplace abn 52 code example system info please copy paste output environment collection script https raw.githubusercontent.com pytorch pytorch master torch utils collect env.py fill checklist manually . pytorch version 0.4.0 debug build cuda used build pytorch 8.0.61 os ubuntu 16.04.4 lts gcc version ubuntu 4.9.4 2ubuntu1 16.04 4.9.4 cmake version version 3.12.0 python version 3.5 cuda available yes cuda runtime version 9.0.176 gpu models configuration gpu 0 geforce gtx 1080 ti gpu 1 geforce gtx 1080 ti gpu 2 geforce gtx 1080 ti gpu 3 geforce gtx 1080 ti nvidia driver version 384.130 cudnn version probably one following usr local cuda 8.0 lib64 libcudnn.so usr local cuda 8.0 lib64 libcudnn.so.6 usr local cuda 8.0 lib64 libcudnn.so.6.0.21 usr local cuda 8.0 lib64 libcudnn static.a usr local cuda 9.0 lib64 libcudnn.so.7.0.5 usr local cuda 9.0 lib64 libcudnn static.a"
pytorch,3124,"3d project found pytorch conv3d, pooling3d batchnorm3d, find softmax3d. anyone softmax3d layer?",0,pytorch softmax3d,"pytorch softmax3d 3d project found pytorch conv3d, pooling3d batchnorm3d, find softmax3d. anyone softmax3d layer?"
pytorch,12510,"issue description getting bunch invalid escape sequence n compiling source. coming paths unescaped backslashes them. trying resolve meantime real pain code example invalid escape sequence n policy cmp0010 set bad variable reference syntax error. run cmake help policy cmp0010 policy details. use cmake policy command set policy suppress warning. warning project developers. use wno dev suppress it. cmake warning dev caffe2 gpu generated thcstoragecopy.cu.obj.release.cmake 178 execute process syntax error cmake code c users user pytorchrc1 pytorch build caffe2 cmakefiles caffe2 gpu.dir aten src thc caffe2 gpu generated thcstoragecopy.cu.obj.release.cmake 178 parsing string c program files nvidia gpu computing toolkit cuda v9.0 bin nvcc.exe cudacc c users user pytorchrc1 pytorch aten src thc thcstoragecopy.cu c users user pytorchrc1 pytorch bui ld caffe2 cmakefiles caffe2 gpu.dir aten src thc caffe2 gpu generated thcstoragecopy.cu.obj.nvcc depend ccbin c program files x86 microsoft visual studio 2017 community vc tools msvc 14.11. 25503 bin hostx64 x64 m64 dcaffe2 gpu exports dcaffe2 build main lib dnominmax crt secure deprecate 1 duse msc atomics 1 dth blas mkl openmp noforce manifest xcompiler , wd4267 , wd4251 , wd4522 , wd4522 , wd4838 , wd4305 , wd4244 , wd4190 , wd4101 , wd4996 , wd4275 , eha , donnx namespace onnx torch , openmp , mp , bigobj , dhave avx cpu definition , dhav e avx2 cpu definition , md , o2 , ob2 , mp , bigobj donnx namespace onnx torch gencode arch compute 30,code sm 30 xcudafe diag suppress cc clobber ignored xcudafe diag suppress integ er sign change xcudafe diag suppress useless using declaration xcudafe diag suppress set used xcompiler md expt relaxed constexpr expt extended lambda xcompiler wd4819 xcomp iler wd4503 xcompiler wd4190 xcompiler wd4244 xcompiler wd4251 xcompiler wd4275 xcompiler wd4522 wno deprecated gpu targets expt extended lambda gencode arch compute 30,code sm 30 dcuda fp16 1 cuda half operators cuda half conversions cuda half2 operators dnvcc ic program files nvidia gpu computing toolkit cuda v9.0 include ic users yacin e pytorchrc1 pytorch aten src ic users user pytorchrc1 pytorch build ic users user pytorchrc1 pytorch ic users user pytorchrc1 pytorch third party protobuf src ic program files x86 intelswtools compilers libraries windows mkl include ic users user pytorchrc1 pytorch cmake .. third party eigen ic users user pytorchrc1 pytorch cmake .. third party pybind11 include ic users user pytorchrc1 pytorch cmake .. third party cub ic users user pytorchrc1 pytorch third party onnx ic users user pytorchrc1 pytorch build third party onnx ic users user p ytorchrc1 pytorch build caffe2 aten src th ic users user pytorchrc1 pytorch aten src th ic users user pytorchrc1 pytorch aten src thc ic users user pytorchrc1 pytorch build caffe2 aten src thc ic users user pytorchrc1 pytorch aten src thcunn ic users user pytorchrc1 pytorch aten src aten cuda ic users user pytorchrc1 pytorch build caffe2 aten src ic users user p ytorchrc1 pytorch build aten src ic users user pytorchrc1 pytorch aten src thnn ic users user pytorchrc1 pytorch aten .. third party catch single include ic users user pytorchrc1 pytor ch build caffe2 aten src aten ic users user pytorchrc1 pytorch aten src aten .. ic nvidia cuda 9.0 cudnn v7.3 include invalid escape sequence n policy cmp0010 set bad variable reference syntax error. run cmake help policy cmp0010 policy details. use cmake policy command set policy suppress warning. call stack recent call first caffe2 gpu generated thcstoragecopy.cu.obj.release.cmake 203 cuda execute process warning project developers. use wno dev suppress it. system info pytorch caffe2 pytorch installed pytorch conda, pip, source source build command used compiling source python setup.py build f os microsoft windows 10 pro x64 version 10.0.17134 build 17134 pytorch version python version 3.6.5 cuda cudnn version 9.0patch4 7.3 gpu models configuration nvidia quadro k2100m gcc version compiling source n cmake version one vs described compilation instruction versions relevant libraries",0,feature request fix windows cmake scripts deal backlashes paths,"feature request fix windows cmake scripts deal backlashes paths issue description getting bunch invalid escape sequence n compiling source. coming paths unescaped backslashes them. trying resolve meantime real pain code example invalid escape sequence n policy cmp0010 set bad variable reference syntax error. run cmake help policy cmp0010 policy details. use cmake policy command set policy suppress warning. warning project developers. use wno dev suppress it. cmake warning dev caffe2 gpu generated thcstoragecopy.cu.obj.release.cmake 178 execute process syntax error cmake code c users user pytorchrc1 pytorch build caffe2 cmakefiles caffe2 gpu.dir aten src thc caffe2 gpu generated thcstoragecopy.cu.obj.release.cmake 178 parsing string c program files nvidia gpu computing toolkit cuda v9.0 bin nvcc.exe cudacc c users user pytorchrc1 pytorch aten src thc thcstoragecopy.cu c users user pytorchrc1 pytorch bui ld caffe2 cmakefiles caffe2 gpu.dir aten src thc caffe2 gpu generated thcstoragecopy.cu.obj.nvcc depend ccbin c program files x86 microsoft visual studio 2017 community vc tools msvc 14.11. 25503 bin hostx64 x64 m64 dcaffe2 gpu exports dcaffe2 build main lib dnominmax crt secure deprecate 1 duse msc atomics 1 dth blas mkl openmp noforce manifest xcompiler , wd4267 , wd4251 , wd4522 , wd4522 , wd4838 , wd4305 , wd4244 , wd4190 , wd4101 , wd4996 , wd4275 , eha , donnx namespace onnx torch , openmp , mp , bigobj , dhave avx cpu definition , dhav e avx2 cpu definition , md , o2 , ob2 , mp , bigobj donnx namespace onnx torch gencode arch compute 30,code sm 30 xcudafe diag suppress cc clobber ignored xcudafe diag suppress integ er sign change xcudafe diag suppress useless using declaration xcudafe diag suppress set used xcompiler md expt relaxed constexpr expt extended lambda xcompiler wd4819 xcomp iler wd4503 xcompiler wd4190 xcompiler wd4244 xcompiler wd4251 xcompiler wd4275 xcompiler wd4522 wno deprecated gpu targets expt extended lambda gencode arch compute 30,code sm 30 dcuda fp16 1 cuda half operators cuda half conversions cuda half2 operators dnvcc ic program files nvidia gpu computing toolkit cuda v9.0 include ic users yacin e pytorchrc1 pytorch aten src ic users user pytorchrc1 pytorch build ic users user pytorchrc1 pytorch ic users user pytorchrc1 pytorch third party protobuf src ic program files x86 intelswtools compilers libraries windows mkl include ic users user pytorchrc1 pytorch cmake .. third party eigen ic users user pytorchrc1 pytorch cmake .. third party pybind11 include ic users user pytorchrc1 pytorch cmake .. third party cub ic users user pytorchrc1 pytorch third party onnx ic users user pytorchrc1 pytorch build third party onnx ic users user p ytorchrc1 pytorch build caffe2 aten src th ic users user pytorchrc1 pytorch aten src th ic users user pytorchrc1 pytorch aten src thc ic users user pytorchrc1 pytorch build caffe2 aten src thc ic users user pytorchrc1 pytorch aten src thcunn ic users user pytorchrc1 pytorch aten src aten cuda ic users user pytorchrc1 pytorch build caffe2 aten src ic users user p ytorchrc1 pytorch build aten src ic users user pytorchrc1 pytorch aten src thnn ic users user pytorchrc1 pytorch aten .. third party catch single include ic users user pytorchrc1 pytor ch build caffe2 aten src aten ic users user pytorchrc1 pytorch aten src aten .. ic nvidia cuda 9.0 cudnn v7.3 include invalid escape sequence n policy cmp0010 set bad variable reference syntax error. run cmake help policy cmp0010 policy details. use cmake policy command set policy suppress warning. call stack recent call first caffe2 gpu generated thcstoragecopy.cu.obj.release.cmake 203 cuda execute process warning project developers. use wno dev suppress it. system info pytorch caffe2 pytorch installed pytorch conda, pip, source source build command used compiling source python setup.py build f os microsoft windows 10 pro x64 version 10.0.17134 build 17134 pytorch version python version 3.6.5 cuda cudnn version 9.0patch4 7.3 gpu models configuration nvidia quadro k2100m gcc version compiling source n cmake version one vs described compilation instruction versions relevant libraries"
pytorch,26142,bug reproduce reproducer cc suo,0,torchscript fails compile methods misindented comments,torchscript fails compile methods misindented comments bug reproduce reproducer cc suo
pytorch,14954,"issue description data1 bryanleoliu pytorch torch lib c10d processgroupgloo.cpp member function c10d algorithmentry c10d processgroupgloo checkout const c10d algorithmkey data1 bryanleoliu pytorch torch lib c10d processgroupgloo.cpp 445 21 warning comparison signed unsigned integer expressions wsign compare vec.size ! cachenumalgorithmentries 42 linking cxx static library libc10d.a 42 built target c10d 47 building nvcc device object test cmakefiles c10d cuda test.dir c10d cuda test generated cudatest.cu.o scanning dependencies target processgroupmpitest scanning dependencies target tcpstoretest scanning dependencies target filestoretest 52 building cxx object test cmakefiles filestoretest.dir filestoretest.cpp.o 57 building cxx object test cmakefiles tcpstoretest.dir tcpstoretest.cpp.o 61 building cxx object test cmakefiles processgroupmpitest.dir processgroupmpitest.cpp.o 66 linking cxx executable filestoretest data1 bryanleoliu pytorch torch lib c10d test processgroupmpitest.cpp function void testallreduce int data1 bryanleoliu pytorch torch lib c10d test processgroupmpitest.cpp 19 57 warning tensor ones const type , intlist deprecated declared data1 bryanleoliu pytorch torch lib tmp install include aten functions.h 3936 wdeprecated declarations auto tensor ones cpu kfloat , 16, 16 data1 bryanleoliu pytorch torch lib c10d test processgroupmpitest.cpp function void testbroadcast int data1 bryanleoliu pytorch torch lib c10d test processgroupmpitest.cpp 61 59 warning tensor ones const type , intlist deprecated declared data1 bryanleoliu pytorch torch lib tmp install include aten functions.h 3936 wdeprecated declarations auto tensor ones cpu kfloat , 16, 16 data1 bryanleoliu pytorch torch lib c10d test processgroupmpitest.cpp 64 60 warning tensor zeros const type , intlist deprecated declared data1 bryanleoliu pytorch torch lib tmp install include aten functions.h 4374 wdeprecated declarations auto tensor zeros cpu kfloat , 16, 16 data1 bryanleoliu pytorch torch lib tmp install lib libcaffe2.so undefined reference vmslog2' collect2 error ld returned 1 exit status make 2 test filestoretest error 1 make 1 test cmakefiles filestoretest.dir error 2 make 1 waiting unfinished jobs.... 71 linking cxx executable tcpstoretest 76 linking cxx executable processgroupmpitest data1 bryanleoliu pytorch torch lib tmp install lib libcaffe2.so undefined reference vmslog2' collect2 error ld returned 1 exit status make 2 test tcpstoretest error 1 make 1 test cmakefiles tcpstoretest.dir error 2 data1 bryanleoliu pytorch torch lib tmp install lib libcaffe2.so undefined reference vmslog2' collect2 error ld returned 1 exit status make 2 test processgroupmpitest error 1 make 1 test cmakefiles processgroupmpitest.dir error 2 scanning dependencies target c10d cuda test 80 linking cxx static library libc10d cuda test.a 80 built target c10d cuda test make error 2 failed run 'bash tools build pytorch libs.sh use cuda use nnpack use mkldnn nccl caffe2 nanopb libshm gloo thd c10d' system info pytorch caffe2 pytorch installed pytorch conda, pip, source source build command used compiling source python setup.py install os centos 7 pytorch version 0.4.1 python version 3.6 cuda cudnn version 9.0 7.3 gpu models configuration 8 nvidia p40 gcc version compiling source 4.8.5 cmake version 3.12.0 versions relevant libraries cc malfet",0,undefined reference vmdlog2',"undefined reference vmdlog2' issue description data1 bryanleoliu pytorch torch lib c10d processgroupgloo.cpp member function c10d algorithmentry c10d processgroupgloo checkout const c10d algorithmkey data1 bryanleoliu pytorch torch lib c10d processgroupgloo.cpp 445 21 warning comparison signed unsigned integer expressions wsign compare vec.size ! cachenumalgorithmentries 42 linking cxx static library libc10d.a 42 built target c10d 47 building nvcc device object test cmakefiles c10d cuda test.dir c10d cuda test generated cudatest.cu.o scanning dependencies target processgroupmpitest scanning dependencies target tcpstoretest scanning dependencies target filestoretest 52 building cxx object test cmakefiles filestoretest.dir filestoretest.cpp.o 57 building cxx object test cmakefiles tcpstoretest.dir tcpstoretest.cpp.o 61 building cxx object test cmakefiles processgroupmpitest.dir processgroupmpitest.cpp.o 66 linking cxx executable filestoretest data1 bryanleoliu pytorch torch lib c10d test processgroupmpitest.cpp function void testallreduce int data1 bryanleoliu pytorch torch lib c10d test processgroupmpitest.cpp 19 57 warning tensor ones const type , intlist deprecated declared data1 bryanleoliu pytorch torch lib tmp install include aten functions.h 3936 wdeprecated declarations auto tensor ones cpu kfloat , 16, 16 data1 bryanleoliu pytorch torch lib c10d test processgroupmpitest.cpp function void testbroadcast int data1 bryanleoliu pytorch torch lib c10d test processgroupmpitest.cpp 61 59 warning tensor ones const type , intlist deprecated declared data1 bryanleoliu pytorch torch lib tmp install include aten functions.h 3936 wdeprecated declarations auto tensor ones cpu kfloat , 16, 16 data1 bryanleoliu pytorch torch lib c10d test processgroupmpitest.cpp 64 60 warning tensor zeros const type , intlist deprecated declared data1 bryanleoliu pytorch torch lib tmp install include aten functions.h 4374 wdeprecated declarations auto tensor zeros cpu kfloat , 16, 16 data1 bryanleoliu pytorch torch lib tmp install lib libcaffe2.so undefined reference vmslog2' collect2 error ld returned 1 exit status make 2 test filestoretest error 1 make 1 test cmakefiles filestoretest.dir error 2 make 1 waiting unfinished jobs.... 71 linking cxx executable tcpstoretest 76 linking cxx executable processgroupmpitest data1 bryanleoliu pytorch torch lib tmp install lib libcaffe2.so undefined reference vmslog2' collect2 error ld returned 1 exit status make 2 test tcpstoretest error 1 make 1 test cmakefiles tcpstoretest.dir error 2 data1 bryanleoliu pytorch torch lib tmp install lib libcaffe2.so undefined reference vmslog2' collect2 error ld returned 1 exit status make 2 test processgroupmpitest error 1 make 1 test cmakefiles processgroupmpitest.dir error 2 scanning dependencies target c10d cuda test 80 linking cxx static library libc10d cuda test.a 80 built target c10d cuda test make error 2 failed run 'bash tools build pytorch libs.sh use cuda use nnpack use mkldnn nccl caffe2 nanopb libshm gloo thd c10d' system info pytorch caffe2 pytorch installed pytorch conda, pip, source source build command used compiling source python setup.py install os centos 7 pytorch version 0.4.1 python version 3.6 cuda cudnn version 9.0 7.3 gpu models configuration 8 nvidia p40 gcc version compiling source 4.8.5 cmake version 3.12.0 versions relevant libraries cc malfet"
pytorch,31095,"problem baobablyh pietern working pr 28883. trying create pinned memory tensors reducer constructor https github.com pytorch pytorch blob 4902a08e44e1e8dbcced99b9c18321cf9bb644d5 torch csrc distributed c10d reducer.cpp l113 l116 however, observed that, one process created pinned memory tensor successfully, another process stuck line above. specifically, stuck creating pinned memory storage stack traces attached end https github.com pytorch pytorch blob 4902a08e44e1e8dbcced99b9c18321cf9bb644d5 aten src aten native memory.cpp l24 baobablyh also discovered that, adding 3 seconds sleep time reducer constructioin, problem disappear. so, symptom likes like something ready. might caused invoking async copy one process another process unready pinned memory tensor sure . ngimel mruberry know could cause problem? way synchronize pinned memory ready? reproduce 1. fetch 28883 2. comment https github.com pytorch pytorch blob 4902a08e44e1e8dbcced99b9c18321cf9bb644d5 torch nn parallel distributed.py l308 l310 3. run trace process 1 process 2 environment cc ezyang gchanan zou3519 ngimel pietern mrshenli pritamdamania87 zhaojuanmao satgera rohan varma gqchen aazzolini xush6528",0,pin memory stuck ddp reducer constructor,"pin memory stuck ddp reducer constructor problem baobablyh pietern working pr 28883. trying create pinned memory tensors reducer constructor https github.com pytorch pytorch blob 4902a08e44e1e8dbcced99b9c18321cf9bb644d5 torch csrc distributed c10d reducer.cpp l113 l116 however, observed that, one process created pinned memory tensor successfully, another process stuck line above. specifically, stuck creating pinned memory storage stack traces attached end https github.com pytorch pytorch blob 4902a08e44e1e8dbcced99b9c18321cf9bb644d5 aten src aten native memory.cpp l24 baobablyh also discovered that, adding 3 seconds sleep time reducer constructioin, problem disappear. so, symptom likes like something ready. might caused invoking async copy one process another process unready pinned memory tensor sure . ngimel mruberry know could cause problem? way synchronize pinned memory ready? reproduce 1. fetch 28883 2. comment https github.com pytorch pytorch blob 4902a08e44e1e8dbcced99b9c18321cf9bb644d5 torch nn parallel distributed.py l308 l310 3. run trace process 1 process 2 environment cc ezyang gchanan zou3519 ngimel pietern mrshenli pritamdamania87 zhaojuanmao satgera rohan varma gqchen aazzolini xush6528"
pytorch,31410,program exit print 'closed' try close . program exit . python 3.7.5 pytorch 1.3.1 tensorboard 2.0.0 os ubuntu win10,0,can't close torch.utils.tensorboard.summarywriter del,can't close torch.utils.tensorboard.summarywriter del program exit print 'closed' try close . program exit . python 3.7.5 pytorch 1.3.1 tensorboard 2.0.0 os ubuntu win10
pytorch,23641,questions help please note issue tracker help form issue closed. set listed resources available website https pytorch.org resources . primary means support discussion forum discussion forum https discuss.pytorch.org,0,"hi, could please tell different deconvtranspose pytorch caffe?","hi, could please tell different deconvtranspose pytorch caffe? questions help please note issue tracker help form issue closed. set listed resources available website https pytorch.org resources . primary means support discussion forum discussion forum https discuss.pytorch.org"
pytorch,3652,"testing non empty tensor statement unconditionally raise runtimeerror stating bool value non empty torch.bytetensor objects ambiguous . there's one value, bool value pretty straightforward. think?",0,fix far fetched boolean ambiguity byte tensor one value it.,"fix far fetched boolean ambiguity byte tensor one value it. testing non empty tensor statement unconditionally raise runtimeerror stating bool value non empty torch.bytetensor objects ambiguous . there's one value, bool value pretty straightforward. think?"
pytorch,8716,nan,0,jit traced script modules properly inlined script functions,jit traced script modules properly inlined script functions nan
pytorch,26726,cc suo,0,jit recursive script pick torchscript class attributes,jit recursive script pick torchscript class attributes cc suo
pytorch,30015,bug run model torchscript torchscripted model fails running time multihead attention driazati reproduce following code sample expected behavior environment bento classyvision env cc suo,0,jit fails multihead attention,jit fails multihead attention bug run model torchscript torchscripted model fails running time multihead attention driazati reproduce following code sample expected behavior environment bento classyvision env cc suo
pytorch,3195,"happens, example, import dynet importing pytorch. code 3113 tries take log 0 throws exception. guess dynet setting ftz daz . see https software.intel.com en us node 523328",0,pytorch throws exception import denormals flushed zero,"pytorch throws exception import denormals flushed zero happens, example, import dynet importing pytorch. code 3113 tries take log 0 throws exception. guess dynet setting ftz daz . see https software.intel.com en us node 523328"
pytorch,31560,"functions export model using onnx torch.onnx.export model, dummy input, alexnet.onnx , verbose true, input names input names, output names output names model multiple inputs multiple outputs? export model? model interface roughly follows model yolo segone model initial model, model name cuda true model.cuda model.eval mask out1mask out2 model.seg sub net3d conv9 roia, conv7 roia, conv5 roia, conv3 roia",0,model multiple inputs multiple outputs?,"model multiple inputs multiple outputs? functions export model using onnx torch.onnx.export model, dummy input, alexnet.onnx , verbose true, input names input names, output names output names model multiple inputs multiple outputs? export model? model interface roughly follows model yolo segone model initial model, model name cuda true model.cuda model.eval mask out1mask out2 model.seg sub net3d conv9 roia, conv7 roia, conv5 roia, conv3 roia"
pytorch,15916,"specifically need pytorch 0.2.0 4 test project built version https github.com oawiles x2face can't find anywhere https pytorch.org get started previous versions , work. way install specific version?",0,download pytorch 0.2.0 4,"download pytorch 0.2.0 4 specifically need pytorch 0.2.0 4 test project built version https github.com oawiles x2face can't find anywhere https pytorch.org get started previous versions , work. way install specific version?"
pytorch,22296,"feature hopefully, support data parallel multiple heterogeneous gpu. motivation currently working big model would like use speed training. however, use two different gpu computer, i.e, gtx 1060 gtx 1080, train model, 1060 becomes bottleneck slows whole training process. pitch great automatically assign tasks according computation power support manually specifying mini batch size gpu.",0,support data parallel heterogeneous gpu,"support data parallel heterogeneous gpu feature hopefully, support data parallel multiple heterogeneous gpu. motivation currently working big model would like use speed training. however, use two different gpu computer, i.e, gtx 1060 gtx 1080, train model, 1060 becomes bottleneck slows whole training process. pitch great automatically assign tasks according computation power support manually specifying mini batch size gpu."
pytorch,22780,bug build pytorch libtorch tbb support failing reproduce steps reproduce behavior 1. set environment variables use tbb 2. build expected behavior builds successfully. building openmp working environment. environment please copy paste output environment collection script https raw.githubusercontent.com pytorch pytorch master torch utils collect env.py fill checklist manually . get script run collecting environment information... pytorch version n debug build n cuda used build pytorch n os ubuntu 14.04.6 lts gcc version ubuntu 4.8.4 2ubuntu1 14.04.4 4.8.4 cmake version version 3.11.1 python version 2.7 cuda available n cuda runtime version 10.0.130 gpu models configuration could collect nvidia driver version could collect cudnn version probably one following usr lib x86 64 linux gnu libcudnn.so.5.1.5 usr lib x86 64 linux gnu libcudnn.so.6.0.21 versions relevant libraries pip numpy 1.8.2 conda could collect additional context attached cmakeoutput cmakeerror files. cmakeerror.log https github.com pytorch pytorch files 3384330 cmakeerror.log cmakeoutput.log https github.com pytorch pytorch files 3384331 cmakeoutput.log,0,build pytorch libtorch tbb support failing,build pytorch libtorch tbb support failing bug build pytorch libtorch tbb support failing reproduce steps reproduce behavior 1. set environment variables use tbb 2. build expected behavior builds successfully. building openmp working environment. environment please copy paste output environment collection script https raw.githubusercontent.com pytorch pytorch master torch utils collect env.py fill checklist manually . get script run collecting environment information... pytorch version n debug build n cuda used build pytorch n os ubuntu 14.04.6 lts gcc version ubuntu 4.8.4 2ubuntu1 14.04.4 4.8.4 cmake version version 3.11.1 python version 2.7 cuda available n cuda runtime version 10.0.130 gpu models configuration could collect nvidia driver version could collect cudnn version probably one following usr lib x86 64 linux gnu libcudnn.so.5.1.5 usr lib x86 64 linux gnu libcudnn.so.6.0.21 versions relevant libraries pip numpy 1.8.2 conda could collect additional context attached cmakeoutput cmakeerror files. cmakeerror.log https github.com pytorch pytorch files 3384330 cmakeerror.log cmakeoutput.log https github.com pytorch pytorch files 3384331 cmakeoutput.log
pytorch,12210,returns following,0,packagesnotfounderror unable install torchvision anaconda prompt windows 10,packagesnotfounderror unable install torchvision anaconda prompt windows 10 returns following
pytorch,17699,"bug using following script break error reproduce steps reproduce behavior 1. run script expected behavior error environment please copy paste output environment collection script https raw.githubusercontent.com pytorch pytorch master torch utils collect env.py fill checklist manually . get script run pytorch version e.g., 1.0 1.0.1.post2 os e.g., linux mac osx 10.14.3 installed pytorch , , source conda build command used compiling source python version 3.6 cuda cudnn version gpu models configuration relevant information additional context",0,nn.embedding broken half,"nn.embedding broken half bug using following script break error reproduce steps reproduce behavior 1. run script expected behavior error environment please copy paste output environment collection script https raw.githubusercontent.com pytorch pytorch master torch utils collect env.py fill checklist manually . get script run pytorch version e.g., 1.0 1.0.1.post2 os e.g., linux mac osx 10.14.3 installed pytorch , , source conda build command used compiling source python version 3.6 cuda cudnn version gpu models configuration relevant information additional context"
pytorch,6111,"issue submission checklist, collect lot information gotten mechanically. simple way collecting info script uploading github.",0,report bug script,"report bug script issue submission checklist, collect lot information gotten mechanically. simple way collecting info script uploading github."
pytorch,598,"hi, something strange step maybe something understand . define module takes 3 inputs, size 3, right ? case backward hook point view case, print grad input throught hook function, composed two elements... could tell wrong ? seem correctly computed cc ezyang gchanan zou3519 ssnl alband gqchen",0,problem backward hook function,"problem backward hook function hi, something strange step maybe something understand . define module takes 3 inputs, size 3, right ? case backward hook point view case, print grad input throught hook function, composed two elements... could tell wrong ? seem correctly computed cc ezyang gchanan zou3519 ssnl alband gqchen"
pytorch,27703,"bug v1.3.0 doc changes seeing test failure fail test torch main .testdoccoverage reproduce steps reproduce behavior 1. git checkout upstream master 2. python . test test docs coverage.py assertionerror items second set first 'real' 'imag' 'conj' 'angle' lists tensor methods documented tensors.rst python different. forget add new thing tensors.rst, whitelist things want document? fail test torch main .testdoccoverage traceback recent call last file . test test docs coverage.py , line 73, test torch want document?''' assertionerror items first set second 'promote types' 'result type' 'can cast' items second set first 'quantize per tensor' 'quantize per channel' lists functions documented torch.rst python different. forget add new thing torch.rst, whitelist things want document? ran 2 tests 0.003s failed failures 2 expected behavior git checkout pr fix doc test failure gottbrath ubuntu pytorch doc python . test test docs coverage.py .. ran 2 tests 0.003s ok cc ezyang",0,fail test torch main .testdoccoverage,"fail test torch main .testdoccoverage bug v1.3.0 doc changes seeing test failure fail test torch main .testdoccoverage reproduce steps reproduce behavior 1. git checkout upstream master 2. python . test test docs coverage.py assertionerror items second set first 'real' 'imag' 'conj' 'angle' lists tensor methods documented tensors.rst python different. forget add new thing tensors.rst, whitelist things want document? fail test torch main .testdoccoverage traceback recent call last file . test test docs coverage.py , line 73, test torch want document?''' assertionerror items first set second 'promote types' 'result type' 'can cast' items second set first 'quantize per tensor' 'quantize per channel' lists functions documented torch.rst python different. forget add new thing torch.rst, whitelist things want document? ran 2 tests 0.003s failed failures 2 expected behavior git checkout pr fix doc test failure gottbrath ubuntu pytorch doc python . test test docs coverage.py .. ran 2 tests 0.003s ok cc ezyang"
pytorch,8369,"question would like help support, please ask forums https discuss.pytorch.org . submitting feature request, please preface title feature request . submitting bug report, please fill following details. issue description got something wrong torch 0.4.0 rnn. like errors ram. code example please try provide minimal example repro bug. error messages stack traces also helpful. home python3 bin python3.6' double free corruption fasttop 0x00007fe73000aa50 backtrace lib64 libc.so.6 0x7c619 0x7fe86b938619 usr local nvidia lib64 libcuda.so.1 0x1dedcf 0x7fe860e4ddcf usr local nvidia lib64 libcuda.so.1 0xf6ebb 0x7fe860d65ebb usr local nvidia lib64 libcuda.so.1 custreamcreate 0x5b 0x7fe860e9672b home python3 lib python3.6 site packages torch lib libaten.so 0x3258786 0x7fe801f1c786 home python3 lib python3.6 site packages torch lib libaten.so 0x328dec4 0x7fe801f51ec4 home python3 lib python3.6 site packages torch lib libaten.so zn17rnnbackwardfilterifffe4initep12cudnncontextp14cudnnrnnstructi11perfoptions 0x3b0 0x7fe807792d10 home python3 lib python3.6 site packages torch lib libaten.so cudnnrnnbackwardweights 0xed1 0x7fe807791f01 home python3 lib python3.6 site packages torch lib libaten.so zn2at6native26 cudnn rnn backward weighterkns 6tensorens 8arrayrefis1 eels3 s3 s3 s3 lllbdbbns4 ilees3 s3 0xa7e 0x7fe8000a0e5e home python3 lib python3.6 site packages torch lib libaten.so zn2at6native19 cudnn rnn backwarderkns 6tensorens 8arrayrefis1 eels3 s3 s3 s3 s3 s3 s3 lllbdbbns4 ilees3 s3 st5arrayiblm4ee 0x22f 0x7fe8000a388f home python3 lib python3.6 site packages torch lib libaten.so znk2at4type19 cudnn rnn backwarderkns 6tensorens 8arrayrefis1 eels3 s3 s3 s3 s3 s3 s3 lllbdbbns4 ilees3 s3 st5arrayiblm4ee 0x9f 0x7fe80030061f home python3 lib python3.6 site packages torch c.cpython 36m x86 64 linux gnu.so znk5torch8autograd12variabletype19 cudnn rnn backwarderkn2at6tensorens2 8arrayrefis3 eels5 s5 s5 s5 s5 s5 s5 lllbdbbns6 ilees5 s5 st5arrayiblm4ee 0x43b 0x7fe824eead1b home python3 lib python3.6 site packages torch c.cpython 36m x86 64 linux gnu.so zn5torch8autograd9generated16cudnnrnnbackward5applyerkst6vectorins0 8variableesais4 ee 0x6c6 0x7fe824fb7f56 home python3 lib python3.6 site packages torch c.cpython 36m x86 64 linux gnu.so zn5torch8autograd6engine17evaluate functionerns0 12functiontaske 0x3e2 0x7fe824e46b62 home python3 lib python3.6 site packages torch c.cpython 36m x86 64 linux gnu.so zn5torch8autograd6engine11thread mainepns0 9graphtaske 0xe5 0x7fe824e47b75 home python3 lib python3.6 site packages torch c.cpython 36m x86 64 linux gnu.so zn5torch8autograd6engine11thread initei 0x5e 0x7fe824e4402e home python3 lib python3.6 site packages torch c.cpython 36m x86 64 linux gnu.so zn5torch8autograd6python12pythonengine11thread initei 0x2a 0x7fe824e71a8a lib64 libstdc .so.6 0xb52b0 0x7fe859a602b0 lib64 libpthread.so.0 0x7e25 0x7fe86c38fe25 lib64 libc.so.6 clone 0x6d 0x7fe86b9b434d memory map ollecting environment information... pytorch version 0.4.0 debug build cuda used build pytorch 8.0.61 os centos linux 7 core gcc version gcc 4.8.5 20150623 red hat 4.8.5 16 cmake version version 2.8.12.2 python version 3.6 cuda available yes cuda runtime version 8.0.61 gpu models configuration gpu 0 tesla p40 gpu 1 tesla p40 gpu 2 tesla p40 gpu 3 tesla p40 nvidia driver version 390.30 cudnn version probably one following usr local cuda 8.0 targets x86 64 linux lib libcudnn.so.6.0.21 usr local cuda 8.0 targets x86 64 linux lib libcudnn static.a versions relevant libraries pip3 numpy 1.14.2 pip3 torch 0.4.0 pip3 torchvision 0.2.1 conda could collect way, dataset size 300 g. cudnn error something wrong pytorch?",0,something wrong torch 0.4.0 rnn?,"something wrong torch 0.4.0 rnn? question would like help support, please ask forums https discuss.pytorch.org . submitting feature request, please preface title feature request . submitting bug report, please fill following details. issue description got something wrong torch 0.4.0 rnn. like errors ram. code example please try provide minimal example repro bug. error messages stack traces also helpful. home python3 bin python3.6' double free corruption fasttop 0x00007fe73000aa50 backtrace lib64 libc.so.6 0x7c619 0x7fe86b938619 usr local nvidia lib64 libcuda.so.1 0x1dedcf 0x7fe860e4ddcf usr local nvidia lib64 libcuda.so.1 0xf6ebb 0x7fe860d65ebb usr local nvidia lib64 libcuda.so.1 custreamcreate 0x5b 0x7fe860e9672b home python3 lib python3.6 site packages torch lib libaten.so 0x3258786 0x7fe801f1c786 home python3 lib python3.6 site packages torch lib libaten.so 0x328dec4 0x7fe801f51ec4 home python3 lib python3.6 site packages torch lib libaten.so zn17rnnbackwardfilterifffe4initep12cudnncontextp14cudnnrnnstructi11perfoptions 0x3b0 0x7fe807792d10 home python3 lib python3.6 site packages torch lib libaten.so cudnnrnnbackwardweights 0xed1 0x7fe807791f01 home python3 lib python3.6 site packages torch lib libaten.so zn2at6native26 cudnn rnn backward weighterkns 6tensorens 8arrayrefis1 eels3 s3 s3 s3 lllbdbbns4 ilees3 s3 0xa7e 0x7fe8000a0e5e home python3 lib python3.6 site packages torch lib libaten.so zn2at6native19 cudnn rnn backwarderkns 6tensorens 8arrayrefis1 eels3 s3 s3 s3 s3 s3 s3 lllbdbbns4 ilees3 s3 st5arrayiblm4ee 0x22f 0x7fe8000a388f home python3 lib python3.6 site packages torch lib libaten.so znk2at4type19 cudnn rnn backwarderkns 6tensorens 8arrayrefis1 eels3 s3 s3 s3 s3 s3 s3 lllbdbbns4 ilees3 s3 st5arrayiblm4ee 0x9f 0x7fe80030061f home python3 lib python3.6 site packages torch c.cpython 36m x86 64 linux gnu.so znk5torch8autograd12variabletype19 cudnn rnn backwarderkn2at6tensorens2 8arrayrefis3 eels5 s5 s5 s5 s5 s5 s5 lllbdbbns6 ilees5 s5 st5arrayiblm4ee 0x43b 0x7fe824eead1b home python3 lib python3.6 site packages torch c.cpython 36m x86 64 linux gnu.so zn5torch8autograd9generated16cudnnrnnbackward5applyerkst6vectorins0 8variableesais4 ee 0x6c6 0x7fe824fb7f56 home python3 lib python3.6 site packages torch c.cpython 36m x86 64 linux gnu.so zn5torch8autograd6engine17evaluate functionerns0 12functiontaske 0x3e2 0x7fe824e46b62 home python3 lib python3.6 site packages torch c.cpython 36m x86 64 linux gnu.so zn5torch8autograd6engine11thread mainepns0 9graphtaske 0xe5 0x7fe824e47b75 home python3 lib python3.6 site packages torch c.cpython 36m x86 64 linux gnu.so zn5torch8autograd6engine11thread initei 0x5e 0x7fe824e4402e home python3 lib python3.6 site packages torch c.cpython 36m x86 64 linux gnu.so zn5torch8autograd6python12pythonengine11thread initei 0x2a 0x7fe824e71a8a lib64 libstdc .so.6 0xb52b0 0x7fe859a602b0 lib64 libpthread.so.0 0x7e25 0x7fe86c38fe25 lib64 libc.so.6 clone 0x6d 0x7fe86b9b434d memory map ollecting environment information... pytorch version 0.4.0 debug build cuda used build pytorch 8.0.61 os centos linux 7 core gcc version gcc 4.8.5 20150623 red hat 4.8.5 16 cmake version version 2.8.12.2 python version 3.6 cuda available yes cuda runtime version 8.0.61 gpu models configuration gpu 0 tesla p40 gpu 1 tesla p40 gpu 2 tesla p40 gpu 3 tesla p40 nvidia driver version 390.30 cudnn version probably one following usr local cuda 8.0 targets x86 64 linux lib libcudnn.so.6.0.21 usr local cuda 8.0 targets x86 64 linux lib libcudnn static.a versions relevant libraries pip3 numpy 1.14.2 pip3 torch 0.4.0 pip3 torchvision 0.2.1 conda could collect way, dataset size 300 g. cudnn error something wrong pytorch?"
pytorch,29246,"bug trying export model created pytorch onnx. model created function then, one convolutional block model swapped different kind block, like try export updated model, like error occurs reason, pytorch fails export model, otherwise fully functional. blocks swapped, network exported successfully. reproduce get code temporary repository created issue run file, get following error expected behavior expect model exported, even convolutional blocks swapped another type block. ideally, would want swap blocks model, first one, example given. environment pytorch version 1.2.0 os ubuntu 19.04 installed pytorch pip3 python version 3.7.3 cuda cudnn version cuda gpu models configuration cc houseroad spandantiwari lara hdr bowenbao neginraoof",0,replacing convolutional blocks causes exporting onnx fail,"replacing convolutional blocks causes exporting onnx fail bug trying export model created pytorch onnx. model created function then, one convolutional block model swapped different kind block, like try export updated model, like error occurs reason, pytorch fails export model, otherwise fully functional. blocks swapped, network exported successfully. reproduce get code temporary repository created issue run file, get following error expected behavior expect model exported, even convolutional blocks swapped another type block. ideally, would want swap blocks model, first one, example given. environment pytorch version 1.2.0 os ubuntu 19.04 installed pytorch pip3 python version 3.7.3 cuda cudnn version cuda gpu models configuration cc houseroad spandantiwari lara hdr bowenbao neginraoof"
pytorch,2126,,0,changed thetorchvision.utils source code long int,changed thetorchvision.utils source code long int 
pytorch,18671,see test result https fburl.com testinfra 6yuycydh,0,jit crash unpickler code,jit crash unpickler code see test result https fburl.com testinfra 6yuycydh
pytorch,15567,right errors saying sequences must sorted. users may aware pass function avoid sorting requirement intend onnx export would great error message suggested instead.,0,"improve error messages pack padded sequence ..., enforced sorted true","improve error messages pack padded sequence ..., enforced sorted true right errors saying sequences must sorted. users may aware pass function avoid sorting requirement intend onnx export would great error message suggested instead."
pytorch,25515,"bug use torch.onnx.export , met opset support problem reproduce steps reproduce behavior 1. pytorch inference file 2. change output onnx.export environment pytorch version e.g., 1.0 1.2 os e.g., linux centos installed pytorch , , source conda build command used compiling source python version 3.7.3 cuda cudnn version 10 7.6 relevant information pytorch model trained pytorch 1.0.0",0,onnx export failed aten operator upsample bilinear2d,"onnx export failed aten operator upsample bilinear2d bug use torch.onnx.export , met opset support problem reproduce steps reproduce behavior 1. pytorch inference file 2. change output onnx.export environment pytorch version e.g., 1.0 1.2 os e.g., linux centos installed pytorch , , source conda build command used compiling source python version 3.7.3 cuda cudnn version 10 7.6 relevant information pytorch model trained pytorch 1.0.0"
pytorch,16392,"bug found inconsistent behavior tensor.squeeze , could intentional so, can't understand reason it. reproduce 1 2 second example, hoped get glad know whether bug, not, why. cheers! shiran",0,squeeze operation edge case,"squeeze operation edge case bug found inconsistent behavior tensor.squeeze , could intentional so, can't understand reason it. reproduce 1 2 second example, hoped get glad know whether bug, not, why. cheers! shiran"
pytorch,3981,"piece test code following conditions meet, second order gradient cannot computed 1. want compute second order grad first order grad 2. network contains convolution fc layer functions normally 3. computation gpu cpu, everything fine error add fix anything.",0,errors computing second order gradients gpu,"errors computing second order gradients gpu piece test code following conditions meet, second order gradient cannot computed 1. want compute second order grad first order grad 2. network contains convolution fc layer functions normally 3. computation gpu cpu, everything fine error add fix anything."
pytorch,23393,bug issue trying execute bunch code. code below.. pytorch version 1.0 linux pip3 install torchvision build command used compil python version 3.7,0,runtimeerror set storage allowed tensor created .data .detach,runtimeerror set storage allowed tensor created .data .detach bug issue trying execute bunch code. code below.. pytorch version 1.0 linux pip3 install torchvision build command used compil python version 3.7
pytorch,3657,http pytorch.org docs master link switch doc dead. http pytorch.org docs 0.2.0 old versions still available link ok there.,0,link previous version doc master doc,link previous version doc master doc http pytorch.org docs master link switch doc dead. http pytorch.org docs 0.2.0 old versions still available link ok there.
pytorch,4403,pytorch docstorch.nn.rnn parts show variable weight ih l k shape input size x hidden sizehowever hidden size x input size.,0,pytorch docsrnn weight ih l k shape error,pytorch docsrnn weight ih l k shape error pytorch docstorch.nn.rnn parts show variable weight ih l k shape input size x hidden sizehowever hidden size x input size.
pytorch,4673,"according code, difference used gpu memory convolution 4809 mb 1345 mb 3.5 gb . much since inp 536mb embed0 134 mb. experiments issue fixed delete nn.batchnorm2d block. reason strange behavior? ps turned cudnn avoid another bug convtranspose3d 4344 pyton2.7, pytorch 0.3 pip , cuda 8, cudnn 7",0,gpu memory consumption cudnn.enabled false,"gpu memory consumption cudnn.enabled false according code, difference used gpu memory convolution 4809 mb 1345 mb 3.5 gb . much since inp 536mb embed0 134 mb. experiments issue fixed delete nn.batchnorm2d block. reason strange behavior? ps turned cudnn avoid another bug convtranspose3d 4344 pyton2.7, pytorch 0.3 pip , cuda 8, cudnn 7"
pytorch,16646,"build net resnet18 ctc loss, run loss.backward , error python 17267,0x700007df0000 malloc error object 0x7fb849370210 incorrect checksum freed object find method resolve it? note training date size equal, mean widht image variable, height imgage 32, label length variable net.py coding utf 8 import torch import torch.nn nn import torch.nn.functional f lstm class bilstm nn.module def init self, num in, num hidden, num super bilstm, self . init self.lstm nn.lstm input size num in, hidden size num hidden, num layers 2, bidirectional true, batch first false self.embeding nn.linear num hidden 2, num def forward self, input n, c, h, w input.size input input.permute 0, 3, 2, 1 input input.reshape n, w, h c input input.permute 1, 0, 2 recurrent, self.lstm input put self.embeding recurrent return put class residualblock nn.module def init self, inchannel, outchannel, stride 1, 1 super residualblock, self . init self.left nn.sequential nn.conv2d inchannel, outchannel, kernel size 3, stride stride, padding 1, bias false , nn.batchnorm2d outchannel , nn.relu inplace true , nn.conv2d outchannel, outchannel, kernel size 3, stride 1, padding 1, bias false , nn.batchnorm2d outchannel self.shortcut nn.sequential stride ! 1, 1 inchannel ! outchannel self.shortcut nn.sequential nn.conv2d inchannel, outchannel, kernel size 1, stride stride, bias false , nn.batchnorm2d outchannel def forward self, x self.left x self.shortcut x f.relu return class resnet nn.module def init self, residualblock, num classes 10, test false super resnet, self . init self.inchannel 64 self.conv1 nn.sequential nn.conv2d 3, 64, kernel size 3, stride 1, padding 1, bias false , nn.batchnorm2d 64 , nn.relu self.layer1 self.make layer residualblock, 64, 2, stride 1, 1 self.layer2 self.make layer residualblock, 128, 2, stride 2, 2 self.layer3 self.make layer residualblock, 256, 2, stride 2, 2 self.layer4 self.make layer residualblock, 512, 2, stride 2, 1 self.rnn nn.sequential bidirectionallstm 512, 512, 512 , bidirectionallstm 512, 512, num classes , self.rnn bilstm 512, 512, num classes self.fc nn.linear 512, num classes self.test test def make layer self, block, channels, num blocks, stride strides stride 1, 1 num blocks 1 layers stride strides layers.append block self.inchannel, channels, stride self.inchannel channels return nn.sequential layers def forward self, x self.conv1 x self.layer1 self.layer2 self.layer3 self.layer4 f.avg pool2d out, 4, 1 self.test return out.view out.size 0 , 1 self.fc self.rnn return def resnet18 return resnet residualblock train code def train batch self, epoch self.model.train batch idx, img, label, width, lens enumerate self.train loader self.optimizer.zero grad preds self.model img log probs preds.log softmax 2 .detach .requires grad loss self.criterion log probs, label, width, lens print loss loss.backward self.optimizer.step batch idx 1 self.log interval 0 lr self.scheduler.get lr 0 src str self.label convert.get str label lables label preds size torch.inttensor preds.size 0 preds.size 1 , preds preds.max 2 preds preds.squeeze 2 preds preds.transpose 1, 0 sim preds self.label convert.decode preds, preds size, raw false n correct 0 pred, target zip sim preds, src str pred target n correct 1 distance value util.edit distance src str, sim preds localtime datetime.datetime.now .strftime h print ' ttrain epoch tloss .6f, acc .2f, error .2f, lr .4f' str localtime , epoch, batch idx, loss, n correct, distance value, lr batch idx 1 self.save interval 0 new save name 'resnet18 d.pth' epoch, batch idx 1 torch.save self.state, new save name util.update file self.save path, new save name new save name 'resnet18 d.pth' epoch torch.save self.state, new save name util.update file self.save path, new save name",0,loss.backward cause malloc error object 0x7fb849370210 incorrect checksum freed object,"loss.backward cause malloc error object 0x7fb849370210 incorrect checksum freed object build net resnet18 ctc loss, run loss.backward , error python 17267,0x700007df0000 malloc error object 0x7fb849370210 incorrect checksum freed object find method resolve it? note training date size equal, mean widht image variable, height imgage 32, label length variable net.py coding utf 8 import torch import torch.nn nn import torch.nn.functional f lstm class bilstm nn.module def init self, num in, num hidden, num super bilstm, self . init self.lstm nn.lstm input size num in, hidden size num hidden, num layers 2, bidirectional true, batch first false self.embeding nn.linear num hidden 2, num def forward self, input n, c, h, w input.size input input.permute 0, 3, 2, 1 input input.reshape n, w, h c input input.permute 1, 0, 2 recurrent, self.lstm input put self.embeding recurrent return put class residualblock nn.module def init self, inchannel, outchannel, stride 1, 1 super residualblock, self . init self.left nn.sequential nn.conv2d inchannel, outchannel, kernel size 3, stride stride, padding 1, bias false , nn.batchnorm2d outchannel , nn.relu inplace true , nn.conv2d outchannel, outchannel, kernel size 3, stride 1, padding 1, bias false , nn.batchnorm2d outchannel self.shortcut nn.sequential stride ! 1, 1 inchannel ! outchannel self.shortcut nn.sequential nn.conv2d inchannel, outchannel, kernel size 1, stride stride, bias false , nn.batchnorm2d outchannel def forward self, x self.left x self.shortcut x f.relu return class resnet nn.module def init self, residualblock, num classes 10, test false super resnet, self . init self.inchannel 64 self.conv1 nn.sequential nn.conv2d 3, 64, kernel size 3, stride 1, padding 1, bias false , nn.batchnorm2d 64 , nn.relu self.layer1 self.make layer residualblock, 64, 2, stride 1, 1 self.layer2 self.make layer residualblock, 128, 2, stride 2, 2 self.layer3 self.make layer residualblock, 256, 2, stride 2, 2 self.layer4 self.make layer residualblock, 512, 2, stride 2, 1 self.rnn nn.sequential bidirectionallstm 512, 512, 512 , bidirectionallstm 512, 512, num classes , self.rnn bilstm 512, 512, num classes self.fc nn.linear 512, num classes self.test test def make layer self, block, channels, num blocks, stride strides stride 1, 1 num blocks 1 layers stride strides layers.append block self.inchannel, channels, stride self.inchannel channels return nn.sequential layers def forward self, x self.conv1 x self.layer1 self.layer2 self.layer3 self.layer4 f.avg pool2d out, 4, 1 self.test return out.view out.size 0 , 1 self.fc self.rnn return def resnet18 return resnet residualblock train code def train batch self, epoch self.model.train batch idx, img, label, width, lens enumerate self.train loader self.optimizer.zero grad preds self.model img log probs preds.log softmax 2 .detach .requires grad loss self.criterion log probs, label, width, lens print loss loss.backward self.optimizer.step batch idx 1 self.log interval 0 lr self.scheduler.get lr 0 src str self.label convert.get str label lables label preds size torch.inttensor preds.size 0 preds.size 1 , preds preds.max 2 preds preds.squeeze 2 preds preds.transpose 1, 0 sim preds self.label convert.decode preds, preds size, raw false n correct 0 pred, target zip sim preds, src str pred target n correct 1 distance value util.edit distance src str, sim preds localtime datetime.datetime.now .strftime h print ' ttrain epoch tloss .6f, acc .2f, error .2f, lr .4f' str localtime , epoch, batch idx, loss, n correct, distance value, lr batch idx 1 self.save interval 0 new save name 'resnet18 d.pth' epoch, batch idx 1 torch.save self.state, new save name util.update file self.save path, new save name new save name 'resnet18 d.pth' epoch torch.save self.state, new save name util.update file self.save path, new save name"
pytorch,21249,"hello, ever encountered pytorch official website fine tuning maskrcnn, less code, data set mask pedestrian detection blackin references detection , number helper functions simplify training evaluating detection models. here, use references detection engine.py, references detection utils.py references detection transforms.py. copy folder use here.there less code",0,code incomplete data set missing,"code incomplete data set missing hello, ever encountered pytorch official website fine tuning maskrcnn, less code, data set mask pedestrian detection blackin references detection , number helper functions simplify training evaluating detection models. here, use references detection engine.py, references detection utils.py references detection transforms.py. copy folder use here.there less code"
pytorch,1114,"would nice absence remove module https github.com pytorch pytorch issues 358 , probably low priority.",0,sequential support better slicing,"sequential support better slicing would nice absence remove module https github.com pytorch pytorch issues 358 , probably low priority."
pytorch,19253,"bug backward torch.pow traced module produces error input cuda. runtimeerror expected tensor cuda backend, got tensor cpu backend checking arguments cuda tensor apply4 checkbackend .. aten src aten tensorutils.cpp 202 backward defined torch csrc jit symbolic script.cpp. think reason backend first argument torch.where always cpu. reproduce following model one https github.com pytorch examples blob master mnist main.py inserted torch.pow . environment pytorch version 1.1.0a0 7e73783 debug build cuda used build pytorch 9.2.88 os centos linux release 7.5.1804 core gcc version gcc 4.8.5 20150623 red hat 4.8.5 28 cmake version version 3.12.2 python version 3.7 cuda available yes cuda runtime version 9.2.88 gpu models configuration gpu 0 tesla p100 pcie 16gb gpu 1 tesla p100 pcie 16gb nvidia driver version 396.26 cudnn version could collect versions relevant libraries pip numpy 1.15.4 pip numpydoc 0.8.0 pip torch 1.1.0a0 7e73783 pip torchvision 0.2.2.post3 conda blas 1.0 mkl conda magma cuda90 2.5.0 1 pytorch conda magma cuda92 2.5.0 1 pytorch conda mkl 2019.1 144 conda mkl include 2019.3 199 conda mkl service 1.1.2 py37he904b0f 5 conda mkl fft 1.0.6 py37hd81dba3 0 conda mkl random 1.0.2 py37hd81dba3 0 conda torch 1.1.0a0 7e73783 dev 0 conda torchvision 0.2.2.post3 pypi 0 pypi",0,torch.pow script module produces error,"torch.pow script module produces error bug backward torch.pow traced module produces error input cuda. runtimeerror expected tensor cuda backend, got tensor cpu backend checking arguments cuda tensor apply4 checkbackend .. aten src aten tensorutils.cpp 202 backward defined torch csrc jit symbolic script.cpp. think reason backend first argument torch.where always cpu. reproduce following model one https github.com pytorch examples blob master mnist main.py inserted torch.pow . environment pytorch version 1.1.0a0 7e73783 debug build cuda used build pytorch 9.2.88 os centos linux release 7.5.1804 core gcc version gcc 4.8.5 20150623 red hat 4.8.5 28 cmake version version 3.12.2 python version 3.7 cuda available yes cuda runtime version 9.2.88 gpu models configuration gpu 0 tesla p100 pcie 16gb gpu 1 tesla p100 pcie 16gb nvidia driver version 396.26 cudnn version could collect versions relevant libraries pip numpy 1.15.4 pip numpydoc 0.8.0 pip torch 1.1.0a0 7e73783 pip torchvision 0.2.2.post3 conda blas 1.0 mkl conda magma cuda90 2.5.0 1 pytorch conda magma cuda92 2.5.0 1 pytorch conda mkl 2019.1 144 conda mkl include 2019.3 199 conda mkl service 1.1.2 py37he904b0f 5 conda mkl fft 1.0.6 py37hd81dba3 0 conda mkl random 1.0.2 py37hd81dba3 0 conda torch 1.1.0a0 7e73783 dev 0 conda torchvision 0.2.2.post3 pypi 0 pypi"
pytorch,13574,"seems internally input wrapped another list, error confusing.",0,dist.all reduce tensor throws confusing error message,"dist.all reduce tensor throws confusing error message seems internally input wrapped another list, error confusing."
pytorch,13409,"example, following fail unknown builtin op , even though underscore bound. suggest ops similar names can't find matching schema. error message could look like aten masked fillaten masked fill ? torch.jit.script def foo mask torch.rand 1 .byte a.masked fill mask, 1 return",0,jit add suggestions unknown builtin op error messages,"jit add suggestions unknown builtin op error messages example, following fail unknown builtin op , even though underscore bound. suggest ops similar names can't find matching schema. error message could look like aten masked fillaten masked fill ? torch.jit.script def foo mask torch.rand 1 .byte a.masked fill mask, 1 return"
pytorch,3293,"according colesbury, however, look like ndarray passed tensor correct codepath. cc aszlam",0,tensor constructor passed numpy ndarray check type.,"tensor constructor passed numpy ndarray check type. according colesbury, however, look like ndarray passed tensor correct codepath. cc aszlam"
pytorch,27954,repro output cc suo jerryzh168 jianyuh dzhulgakov raghuramank100,0,quantization dynamic lstm serialize properly traced,quantization dynamic lstm serialize properly traced repro output cc suo jerryzh168 jianyuh dzhulgakov raghuramank100
pytorch,5826,"os ubuntu 16.04.3 lts gnu linux 4.4.0 1038 aws x86 64 pytorch version 0.4.0a0 7f864bb source , 0.3.1 pip . installed pytorch conda, pip, source pip source python version python2 python3 cuda cudnn version cuda 9, cudnn v7 gpu models configuration tesla k80 gcc version compiling source gcc version 5.4.0 20160609 ubuntu 5.4.0 6ubuntu1 16.04.5 using pip version python2 3 , getting errors using l bfgs adam optimizers running optimization adam running optimization l bfgs feval function run run code latest github version, get error happens completely different area sure error still related pip issues, something got changed broken, etc...",0,"userwarning tensor1 broadcastable self, number elements. falling back deprecated pointwise behavior","userwarning tensor1 broadcastable self, number elements. falling back deprecated pointwise behavior os ubuntu 16.04.3 lts gnu linux 4.4.0 1038 aws x86 64 pytorch version 0.4.0a0 7f864bb source , 0.3.1 pip . installed pytorch conda, pip, source pip source python version python2 python3 cuda cudnn version cuda 9, cudnn v7 gpu models configuration tesla k80 gcc version compiling source gcc version 5.4.0 20160609 ubuntu 5.4.0 6ubuntu1 16.04.5 using pip version python2 3 , getting errors using l bfgs adam optimizers running optimization adam running optimization l bfgs feval function run run code latest github version, get error happens completely different area sure error still related pip issues, something got changed broken, etc..."
pytorch,9054,throws error ubsan error line https github.com pytorch pytorch blob master torch csrc ptrwrapper.cpp l49,0,ubsan error call function thdrequest free pointer incorrect function type 'void void ',ubsan error call function thdrequest free pointer incorrect function type 'void void ' throws error ubsan error line https github.com pytorch pytorch blob master torch csrc ptrwrapper.cpp l49
pytorch,27037,"feature add support operation. motivation useful function window operations convolution. current implementation, supports constant padding mode, automatically fills margins zeros. use case need use edge padding mode. pitch want argument exactly like convolution operation. would also make api coherent. alternatives guess alternative manually copy cell values padded area.",0,padding mode support functional.unfold,"padding mode support functional.unfold feature add support operation. motivation useful function window operations convolution. current implementation, supports constant padding mode, automatically fills margins zeros. use case need use edge padding mode. pitch want argument exactly like convolution operation. would also make api coherent. alternatives guess alternative manually copy cell values padded area."
pytorch,7721,"installed clean os 10.12.6 macbook pro late 2013 gt 750m video card. trying get work weeks, tried every tip find. following error message running install running build deps cuda 0 cuda w h c u cuda 1 shift rocm 0 nnpack w h r c nnpack 0 nnpack w h n n p c k nnpack 1 shift mkldnn 0 aten w h k l n n gloo ibverbs 0 aten w h g l b v e r b distributed mw 0 aten w h r b u e w cmake install 'make install' user cflags user ldflags n '' n '' n '' dirname tools build pytorch libs.sh cd tools .. pwd printf ' q n' users hsr pytorch pwd users hsr pytorch base dir users hsr pytorch torch lib dir users hsr pytorch torch lib install dir users hsr pytorch torch lib tmp install third party dir users hsr pytorch third party cmake version cmake c flags ' dth index base 0 users hsr pytorch torch lib tmp install include users hsr pytorch torch lib tmp install include th users hsr pytorch torch lib tmp install include thc users hsr pytorch torch lib tmp install include ths users hsr pytorch torch lib tmp install include thcs users hsr pytorch torch lib tmp install include thnn users hsr pytorch torch lib tmp install include thcunn ' c flags ' dth index base 0 users hsr pytorch torch lib tmp install include users hsr pytorch torch lib tmp install include th users hsr pytorch torch lib tmp install include thc users hsr pytorch torch lib tmp install include ths users hsr pytorch torch lib tmp install include thcs users hsr pytorch torch lib tmp install include thnn users hsr pytorch torch lib tmp install include thcunn dompi skip mpicxx 1' ldflags ' l users hsr pytorch torch lib tmp install lib ' ld postfix .so.1 ld postfix unversioned .so uname darwin r w n ldflags ' l users hsr pytorch torch lib tmp install lib wl, rpath, loader path' ld postfix .1.dylib ld postfix unversioned .dylib cpp flags ' std c 11 ' gloo flags thd flags nccl root dir users hsr pytorch torch lib tmp install 1 eq 1 gloo flags ' duse cuda 1 dnccl root dir users hsr pytorch torch lib tmp install' 0 eq 1 0 eq 1 cwrap files ' users hsr pytorch torch lib aten declarations.cwrap users hsr pytorch torch lib thnn generic thnn.h users hsr pytorch torch lib thcunn generic thcunn.h users hsr pytorch torch lib aten nn.yaml' cuda nvcc flags ' dth index base 0 users hsr pytorch torch lib tmp install include users hsr pytorch torch lib tmp install include th users hsr pytorch torch lib tmp install include thc users hsr pytorch torch lib tmp install include ths users hsr pytorch torch lib tmp install include thcs users hsr pytorch torch lib tmp install include thnn users hsr pytorch torch lib tmp install include thcunn dompi skip mpicxx 1' '' eq 1 ' ' z 8 ' ' build type release n '' n '' echo 'building release mode' building release mode mkdir p torch lib tmp install arg ' ' aten n c c l aten g l aten e n pushd users hsr pytorch aten pytorch aten pytorch build aten mkdir p build pushd build pytorch aten build pytorch aten pytorch cmake .. dcmake build type release dno cuda 0 dno nnpack 0 dcudnn include dir usr local cuda include dcudnn lib dir usr local cuda lib dcudnn library usr local cuda lib libcudnn.7.dylib dno mkldnn 1 dmkldnn include dir dmkldnn lib dir dmkldnn library daten contrib 1 dcmake install prefix users hsr pytorch torch lib tmp install dcmake export compile commands 1 dcmake c flags dcmake cxx flags dcmake exe linker flags dcmake shared linker flags dwith rocm 0 autodetected cuda architecture 3.0 found cuda fp16 support, compiling torch.cudahalftensor removing dndebug compile flags magma found. compiling without magma support could find hardware support neon machine. omap3 processor machine. omap4 processor machine. sse2 found sse3 found avx found avx2 found atomics using gcc intrinsics checking mkl intel lp64 mkl intel thread mkl core iomp5 pthread library mkl intel lp64 users hsr anaconda3 envs kate lib libmkl intel lp64.dylib library mkl intel thread users hsr anaconda3 envs kate lib libmkl intel thread.dylib library mkl core users hsr anaconda3 envs kate lib libmkl core.dylib library iomp5 users hsr anaconda3 envs kate lib libiomp5.dylib library pthread usr lib libpthread.dylib library usr lib libm.dylib mkl library found found library blas api mkl . found library lapack api. mkl found cudnn v7.0.4 include usr local cuda include, library usr local cuda lib libcudnn.7.dylib disabling mkldnn mkldnn set cmake deprecation warning src aten cmakelists.txt 25 cmake policy old behavior policy cmp0026 removed future version cmake. cmake policies 7 manual explains old behaviors policies deprecated policy set old specific short term circumstances. projects ported new behavior rely setting policy old. using python found users hsr anaconda3 envs kate bin python tbb using libc . could find openmp c missing openmp c flags openmp c lib names could find openmp cxx missing openmp cxx flags openmp cxx lib names could find openmp missing openmp c found openmp cxx found configuring build sleef v3.2 target system darwin 16.7.0 target processor x86 64 host system darwin 16.7.0 host processor x86 64 detected c compiler appleclang library developer commandlinetools usr bin clang using option compile libsleef building shared libs mpfr lib mpfr notfound gmp libgmp notfound running travis 0 compiler supports openmp disable contrib aten contrib set configuring done generating done build files written users hsr pytorch aten build make install j8 0 built target mkdisp 0 built target common 2 built target mkalias 2 built target mkrename 6 built target cpuinfo 6 built target cuda aten files generated 6 built target aten files generated scanning dependencies target tbb static 6 built target mkmasked gnuabi 6 built target arraymap 6 built target mkrename gnuabi 6 built target renamedsp256.h generated 7 built target dispavx.c generated 8 built target headers 9 built target renamesse2.h generated 9 built target renameavx.h generated 9 built target renamefma4.h generated 9 built target renamesse4.h generated 9 built target renameavx2.h generated 10 built target dispsse.c generated 10 built target renameavx2128.h generated 10 built target renamedsp128.h generated scanning dependencies target sleefavx2 scanning dependencies target sleefavx2128 11 built target sleefsse2 11 built target dispavx obj 11 built target sleefavx 11 built target sleeffma4 11 built target sleefsse4 11 building c object sleef src libm cmakefiles sleefavx2.dir sleefsimdsp.c.o 11 building c object sleef src libm cmakefiles sleefavx2.dir sleefsimddp.c.o scanning dependencies target dispsse obj 11 building c object sleef src libm cmakefiles sleefavx2128.dir sleefsimdsp.c.o 12 building c object sleef src libm cmakefiles sleefavx2128.dir sleefsimddp.c.o 12 building c object sleef src libm cmakefiles dispsse obj.dir dispsse.c.o 12 building cxx object src aten cpu tbb cmakefiles tbb static.dir users hsr pytorch third party tbb src tbb arena.cpp.o 12 building cxx object src aten cpu tbb cmakefiles tbb static.dir users hsr pytorch third party tbb src tbb concurrent vector.cpp.o 13 building cxx object src aten cpu tbb cmakefiles tbb static.dir users hsr pytorch third party tbb src tbb condition variable.cpp.o file included users hsr pytorch third party tbb src tbb arena.cpp 23 file included users hsr pytorch third party tbb src tbb scheduler.h 26 users hsr pytorch third party tbb src tbb mailbox.h 102 52 error unknown type name 'isolation tag' task proxy internal pop tbb isolation expr isolation tag isolation users hsr pytorch third party tbb src tbb mailbox.h 108 27 error use undeclared identifier 'no isolation' mean 'isolation'? isolation ! isolation isolation users hsr pytorch third party tbb src tbb mailbox.h 102 66 note 'isolation' declared task proxy internal pop tbb isolation expr isolation tag isolation users hsr pytorch third party tbb src tbb mailbox.h 109 36 error member named 'isolation' 'tbb internal task prefix' curr prefix .isolation ! isolation users hsr pytorch third party tbb src tbb mailbox.h 206 44 error unknown type name 'isolation tag' task proxy pop tbb isolation expr isolation tag isolation users hsr pytorch third party tbb src tbb concurrent vector.cpp 123 9 error use undeclared identifier 'enforce segment allocated' enforce segment allocated s.load hard rec... file included users hsr pytorch third party tbb src tbb arena.cpp 23 users hsr pytorch third party tbb src tbb scheduler.h 237 43 error unknown type name 'isolation tag' task get task tbb isolation expr isolation tag isolation users hsr pytorch third party tbb src tbb concurrent vector.cpp 188 13 error use undeclared identifier 'enforce segment allocated' enforce segment allocated s.load users hsr pytorch third party tbb src tbb scheduler.h 246 31 error unknown type name 'isolation tag' task get task size t, isolation tag isolation, bool tasks omitted users hsr pytorch third party tbb src tbb scheduler.h 257 51 error unknown type name 'isolation tag' task get mailbox task tbb isolation expr isolation tag isolation users hsr pytorch third party tbb src tbb scheduler.h 265 75 error unknown type name 'isolation tag' ...steal task tbb isolation arg arena slot victim arena slot, isolatio... users hsr pytorch third party tbb src tbb concurrent vector.cpp 273 9 error use undeclared identifier 'enforce segment allocated' enforce segment allocated array0 initial segment users hsr pytorch third party tbb src tbb scheduler.h 366 115 error unknown type name 'isolation tag' tbb atomic reference count completion ref count, isolation tag isolat... users hsr pytorch third party tbb src tbb scheduler.h 424 47 error unknown type name 'isolation tag' task reload tasks tbb isolation expr isolation tag isolation 13 building cxx object src aten cpu tbb cmakefiles tbb static.dir users hsr pytorch third party tbb src tbb critical section.cpp.o users hsr pytorch third party tbb src tbb scheduler.h 426 127 error unknown type name 'isolation tag' tbb isolation arg intptr top priority, isolation tag isolation users hsr pytorch third party tbb src tbb concurrent vector.cpp 391 9 error use undeclared identifier 'enforce segment allocated' enforce segment allocated segment k .load v... users hsr pytorch third party tbb src tbb scheduler.h 430 52 error unknown type name 'isolation tag' task winnow task pool tbb isolation expr isolation tag isolation users hsr pytorch third party tbb src tbb scheduler.h 434 88 error unknown type name 'isolation tag' ...size h0 , tbb isolation arg size t0, isolation tag isolation users hsr pytorch third party tbb src tbb concurrent vector.cpp 409 13 error use undeclared identifier 'enforce segment allocated' enforce segment allocated segment k .load users hsr pytorch third party tbb src tbb concurrent vector.cpp 466 9 error use undeclared identifier 'enforce segment allocated' enforce segment allocated segment .load 6 errors generated. make 2 src aten cpu tbb cmakefiles tbb static.dir users hsr pytorch third party tbb src tbb concurrent vector.cpp.o error 1 make 2 waiting unfinished jobs.... file included users hsr pytorch third party tbb src tbb arena.cpp 28 users hsr anaconda3 envs kate include tbb internal flow graph impl.h 25 2 error include internal file directly use public tbb headers instead. error include internal file directly use public tbb heade... users hsr anaconda3 envs kate include tbb internal flow graph impl.h 102 31 error use undeclared identifier 'continue msg' class function body leaf public fu... users hsr anaconda3 envs kate include tbb internal flow graph impl.h 125 38 error use undeclared identifier 'continue msg' class function body leaf public function users hsr anaconda3 envs kate include tbb internal flow graph impl.h 148 31 error use undeclared identifier 'continue msg' class function body leaf public functio... users hsr anaconda3 envs kate include tbb internal flow graph impl.h 208 40 error unknown class name 'task' mean 'tbb task'? class forward task bypass public task tbb task users hsr anaconda3 envs kate include tbb task.h 542 7 note 'tbb task' declared class task tbb task base access interface5 internal task base file included users hsr pytorch third party tbb src tbb arena.cpp 28 users hsr anaconda3 envs kate include tbb internal flow graph impl.h 218 29 error use undeclared identifier 'successfully enqueued' new task successfully enqueued new task null fatal error many errors emitted, stopping ferror limit 13 built target sleefavx2128 14 built target sleefavx2 20 errors generated. make 2 src aten cpu tbb cmakefiles tbb static.dir users hsr pytorch third party tbb src tbb arena.cpp.o error 1 14 built target dispsse obj scanning dependencies target sleef 15 building c object sleef src libm cmakefiles sleef.dir sleefdp.c.o 15 building c object sleef src libm cmakefiles sleef.dir sleefsp.c.o 15 building c object sleef src libm cmakefiles sleef.dir sleefld.c.o make 1 src aten cpu tbb cmakefiles tbb static.dir error 2 make 1 waiting unfinished jobs.... 15 linking c static library .. .. lib libsleef.a 15 built target sleef make error 2 help appreciated, thanks time.",0,"error compiling source os x 10.12, cuda 9.0, cudnn 7.0.4","error compiling source os x 10.12, cuda 9.0, cudnn 7.0.4 installed clean os 10.12.6 macbook pro late 2013 gt 750m video card. trying get work weeks, tried every tip find. following error message running install running build deps cuda 0 cuda w h c u cuda 1 shift rocm 0 nnpack w h r c nnpack 0 nnpack w h n n p c k nnpack 1 shift mkldnn 0 aten w h k l n n gloo ibverbs 0 aten w h g l b v e r b distributed mw 0 aten w h r b u e w cmake install 'make install' user cflags user ldflags n '' n '' n '' dirname tools build pytorch libs.sh cd tools .. pwd printf ' q n' users hsr pytorch pwd users hsr pytorch base dir users hsr pytorch torch lib dir users hsr pytorch torch lib install dir users hsr pytorch torch lib tmp install third party dir users hsr pytorch third party cmake version cmake c flags ' dth index base 0 users hsr pytorch torch lib tmp install include users hsr pytorch torch lib tmp install include th users hsr pytorch torch lib tmp install include thc users hsr pytorch torch lib tmp install include ths users hsr pytorch torch lib tmp install include thcs users hsr pytorch torch lib tmp install include thnn users hsr pytorch torch lib tmp install include thcunn ' c flags ' dth index base 0 users hsr pytorch torch lib tmp install include users hsr pytorch torch lib tmp install include th users hsr pytorch torch lib tmp install include thc users hsr pytorch torch lib tmp install include ths users hsr pytorch torch lib tmp install include thcs users hsr pytorch torch lib tmp install include thnn users hsr pytorch torch lib tmp install include thcunn dompi skip mpicxx 1' ldflags ' l users hsr pytorch torch lib tmp install lib ' ld postfix .so.1 ld postfix unversioned .so uname darwin r w n ldflags ' l users hsr pytorch torch lib tmp install lib wl, rpath, loader path' ld postfix .1.dylib ld postfix unversioned .dylib cpp flags ' std c 11 ' gloo flags thd flags nccl root dir users hsr pytorch torch lib tmp install 1 eq 1 gloo flags ' duse cuda 1 dnccl root dir users hsr pytorch torch lib tmp install' 0 eq 1 0 eq 1 cwrap files ' users hsr pytorch torch lib aten declarations.cwrap users hsr pytorch torch lib thnn generic thnn.h users hsr pytorch torch lib thcunn generic thcunn.h users hsr pytorch torch lib aten nn.yaml' cuda nvcc flags ' dth index base 0 users hsr pytorch torch lib tmp install include users hsr pytorch torch lib tmp install include th users hsr pytorch torch lib tmp install include thc users hsr pytorch torch lib tmp install include ths users hsr pytorch torch lib tmp install include thcs users hsr pytorch torch lib tmp install include thnn users hsr pytorch torch lib tmp install include thcunn dompi skip mpicxx 1' '' eq 1 ' ' z 8 ' ' build type release n '' n '' echo 'building release mode' building release mode mkdir p torch lib tmp install arg ' ' aten n c c l aten g l aten e n pushd users hsr pytorch aten pytorch aten pytorch build aten mkdir p build pushd build pytorch aten build pytorch aten pytorch cmake .. dcmake build type release dno cuda 0 dno nnpack 0 dcudnn include dir usr local cuda include dcudnn lib dir usr local cuda lib dcudnn library usr local cuda lib libcudnn.7.dylib dno mkldnn 1 dmkldnn include dir dmkldnn lib dir dmkldnn library daten contrib 1 dcmake install prefix users hsr pytorch torch lib tmp install dcmake export compile commands 1 dcmake c flags dcmake cxx flags dcmake exe linker flags dcmake shared linker flags dwith rocm 0 autodetected cuda architecture 3.0 found cuda fp16 support, compiling torch.cudahalftensor removing dndebug compile flags magma found. compiling without magma support could find hardware support neon machine. omap3 processor machine. omap4 processor machine. sse2 found sse3 found avx found avx2 found atomics using gcc intrinsics checking mkl intel lp64 mkl intel thread mkl core iomp5 pthread library mkl intel lp64 users hsr anaconda3 envs kate lib libmkl intel lp64.dylib library mkl intel thread users hsr anaconda3 envs kate lib libmkl intel thread.dylib library mkl core users hsr anaconda3 envs kate lib libmkl core.dylib library iomp5 users hsr anaconda3 envs kate lib libiomp5.dylib library pthread usr lib libpthread.dylib library usr lib libm.dylib mkl library found found library blas api mkl . found library lapack api. mkl found cudnn v7.0.4 include usr local cuda include, library usr local cuda lib libcudnn.7.dylib disabling mkldnn mkldnn set cmake deprecation warning src aten cmakelists.txt 25 cmake policy old behavior policy cmp0026 removed future version cmake. cmake policies 7 manual explains old behaviors policies deprecated policy set old specific short term circumstances. projects ported new behavior rely setting policy old. using python found users hsr anaconda3 envs kate bin python tbb using libc . could find openmp c missing openmp c flags openmp c lib names could find openmp cxx missing openmp cxx flags openmp cxx lib names could find openmp missing openmp c found openmp cxx found configuring build sleef v3.2 target system darwin 16.7.0 target processor x86 64 host system darwin 16.7.0 host processor x86 64 detected c compiler appleclang library developer commandlinetools usr bin clang using option compile libsleef building shared libs mpfr lib mpfr notfound gmp libgmp notfound running travis 0 compiler supports openmp disable contrib aten contrib set configuring done generating done build files written users hsr pytorch aten build make install j8 0 built target mkdisp 0 built target common 2 built target mkalias 2 built target mkrename 6 built target cpuinfo 6 built target cuda aten files generated 6 built target aten files generated scanning dependencies target tbb static 6 built target mkmasked gnuabi 6 built target arraymap 6 built target mkrename gnuabi 6 built target renamedsp256.h generated 7 built target dispavx.c generated 8 built target headers 9 built target renamesse2.h generated 9 built target renameavx.h generated 9 built target renamefma4.h generated 9 built target renamesse4.h generated 9 built target renameavx2.h generated 10 built target dispsse.c generated 10 built target renameavx2128.h generated 10 built target renamedsp128.h generated scanning dependencies target sleefavx2 scanning dependencies target sleefavx2128 11 built target sleefsse2 11 built target dispavx obj 11 built target sleefavx 11 built target sleeffma4 11 built target sleefsse4 11 building c object sleef src libm cmakefiles sleefavx2.dir sleefsimdsp.c.o 11 building c object sleef src libm cmakefiles sleefavx2.dir sleefsimddp.c.o scanning dependencies target dispsse obj 11 building c object sleef src libm cmakefiles sleefavx2128.dir sleefsimdsp.c.o 12 building c object sleef src libm cmakefiles sleefavx2128.dir sleefsimddp.c.o 12 building c object sleef src libm cmakefiles dispsse obj.dir dispsse.c.o 12 building cxx object src aten cpu tbb cmakefiles tbb static.dir users hsr pytorch third party tbb src tbb arena.cpp.o 12 building cxx object src aten cpu tbb cmakefiles tbb static.dir users hsr pytorch third party tbb src tbb concurrent vector.cpp.o 13 building cxx object src aten cpu tbb cmakefiles tbb static.dir users hsr pytorch third party tbb src tbb condition variable.cpp.o file included users hsr pytorch third party tbb src tbb arena.cpp 23 file included users hsr pytorch third party tbb src tbb scheduler.h 26 users hsr pytorch third party tbb src tbb mailbox.h 102 52 error unknown type name 'isolation tag' task proxy internal pop tbb isolation expr isolation tag isolation users hsr pytorch third party tbb src tbb mailbox.h 108 27 error use undeclared identifier 'no isolation' mean 'isolation'? isolation ! isolation isolation users hsr pytorch third party tbb src tbb mailbox.h 102 66 note 'isolation' declared task proxy internal pop tbb isolation expr isolation tag isolation users hsr pytorch third party tbb src tbb mailbox.h 109 36 error member named 'isolation' 'tbb internal task prefix' curr prefix .isolation ! isolation users hsr pytorch third party tbb src tbb mailbox.h 206 44 error unknown type name 'isolation tag' task proxy pop tbb isolation expr isolation tag isolation users hsr pytorch third party tbb src tbb concurrent vector.cpp 123 9 error use undeclared identifier 'enforce segment allocated' enforce segment allocated s.load hard rec... file included users hsr pytorch third party tbb src tbb arena.cpp 23 users hsr pytorch third party tbb src tbb scheduler.h 237 43 error unknown type name 'isolation tag' task get task tbb isolation expr isolation tag isolation users hsr pytorch third party tbb src tbb concurrent vector.cpp 188 13 error use undeclared identifier 'enforce segment allocated' enforce segment allocated s.load users hsr pytorch third party tbb src tbb scheduler.h 246 31 error unknown type name 'isolation tag' task get task size t, isolation tag isolation, bool tasks omitted users hsr pytorch third party tbb src tbb scheduler.h 257 51 error unknown type name 'isolation tag' task get mailbox task tbb isolation expr isolation tag isolation users hsr pytorch third party tbb src tbb scheduler.h 265 75 error unknown type name 'isolation tag' ...steal task tbb isolation arg arena slot victim arena slot, isolatio... users hsr pytorch third party tbb src tbb concurrent vector.cpp 273 9 error use undeclared identifier 'enforce segment allocated' enforce segment allocated array0 initial segment users hsr pytorch third party tbb src tbb scheduler.h 366 115 error unknown type name 'isolation tag' tbb atomic reference count completion ref count, isolation tag isolat... users hsr pytorch third party tbb src tbb scheduler.h 424 47 error unknown type name 'isolation tag' task reload tasks tbb isolation expr isolation tag isolation 13 building cxx object src aten cpu tbb cmakefiles tbb static.dir users hsr pytorch third party tbb src tbb critical section.cpp.o users hsr pytorch third party tbb src tbb scheduler.h 426 127 error unknown type name 'isolation tag' tbb isolation arg intptr top priority, isolation tag isolation users hsr pytorch third party tbb src tbb concurrent vector.cpp 391 9 error use undeclared identifier 'enforce segment allocated' enforce segment allocated segment k .load v... users hsr pytorch third party tbb src tbb scheduler.h 430 52 error unknown type name 'isolation tag' task winnow task pool tbb isolation expr isolation tag isolation users hsr pytorch third party tbb src tbb scheduler.h 434 88 error unknown type name 'isolation tag' ...size h0 , tbb isolation arg size t0, isolation tag isolation users hsr pytorch third party tbb src tbb concurrent vector.cpp 409 13 error use undeclared identifier 'enforce segment allocated' enforce segment allocated segment k .load users hsr pytorch third party tbb src tbb concurrent vector.cpp 466 9 error use undeclared identifier 'enforce segment allocated' enforce segment allocated segment .load 6 errors generated. make 2 src aten cpu tbb cmakefiles tbb static.dir users hsr pytorch third party tbb src tbb concurrent vector.cpp.o error 1 make 2 waiting unfinished jobs.... file included users hsr pytorch third party tbb src tbb arena.cpp 28 users hsr anaconda3 envs kate include tbb internal flow graph impl.h 25 2 error include internal file directly use public tbb headers instead. error include internal file directly use public tbb heade... users hsr anaconda3 envs kate include tbb internal flow graph impl.h 102 31 error use undeclared identifier 'continue msg' class function body leaf public fu... users hsr anaconda3 envs kate include tbb internal flow graph impl.h 125 38 error use undeclared identifier 'continue msg' class function body leaf public function users hsr anaconda3 envs kate include tbb internal flow graph impl.h 148 31 error use undeclared identifier 'continue msg' class function body leaf public functio... users hsr anaconda3 envs kate include tbb internal flow graph impl.h 208 40 error unknown class name 'task' mean 'tbb task'? class forward task bypass public task tbb task users hsr anaconda3 envs kate include tbb task.h 542 7 note 'tbb task' declared class task tbb task base access interface5 internal task base file included users hsr pytorch third party tbb src tbb arena.cpp 28 users hsr anaconda3 envs kate include tbb internal flow graph impl.h 218 29 error use undeclared identifier 'successfully enqueued' new task successfully enqueued new task null fatal error many errors emitted, stopping ferror limit 13 built target sleefavx2128 14 built target sleefavx2 20 errors generated. make 2 src aten cpu tbb cmakefiles tbb static.dir users hsr pytorch third party tbb src tbb arena.cpp.o error 1 14 built target dispsse obj scanning dependencies target sleef 15 building c object sleef src libm cmakefiles sleef.dir sleefdp.c.o 15 building c object sleef src libm cmakefiles sleef.dir sleefsp.c.o 15 building c object sleef src libm cmakefiles sleef.dir sleefld.c.o make 1 src aten cpu tbb cmakefiles tbb static.dir error 2 make 1 waiting unfinished jobs.... 15 linking c static library .. .. lib libsleef.a 15 built target sleef make error 2 help appreciated, thanks time."
pytorch,22050,"currently using windows 7, pytorch version 1.1.0, seems read error fixed 1.1.0? environment collecting environment information... pytorch version 1.1.0 debug build cuda used build pytorch 9.0 os microsoft windows 7 professional gcc version tdm64 1 5.1.0 cmake version version 3.7.0 rc1 python version 3.6 cuda available yes cuda runtime version 10.0.130 gpu models configuration gpu 0 geforce gtx 1070 nvidia driver version 411.31 cudnn version c program files nvidia gpu computing toolkit cuda v10.0 bin cudnn64 7.dll versions relevant libraries pip3 numpy 1.16.4 pip3 torch 1.1.0 pip3 torchsummary 1.5.1 pip3 torchvision 0.3.0 conda could collect traceback multiple versions cuda installed compatibility tensorflow, keras pytorch. thank you. script running script using https pytorch.org tutorials beginner transfer learning tutorial.html",0,runtimeerror cudnn error cudnn status internal error 1.1.0,"runtimeerror cudnn error cudnn status internal error 1.1.0 currently using windows 7, pytorch version 1.1.0, seems read error fixed 1.1.0? environment collecting environment information... pytorch version 1.1.0 debug build cuda used build pytorch 9.0 os microsoft windows 7 professional gcc version tdm64 1 5.1.0 cmake version version 3.7.0 rc1 python version 3.6 cuda available yes cuda runtime version 10.0.130 gpu models configuration gpu 0 geforce gtx 1070 nvidia driver version 411.31 cudnn version c program files nvidia gpu computing toolkit cuda v10.0 bin cudnn64 7.dll versions relevant libraries pip3 numpy 1.16.4 pip3 torch 1.1.0 pip3 torchsummary 1.5.1 pip3 torchvision 0.3.0 conda could collect traceback multiple versions cuda installed compatibility tensorflow, keras pytorch. thank you. script running script using https pytorch.org tutorials beginner transfer learning tutorial.html"
pytorch,14373,14294 failing test behavior therefore skipped . stack pr part removes deprecated code simplifications. easier change behavior synchronize strongly stack 14294 merged.,0,c10d barrier synchronize previously kicked work processgroupgloo,c10d barrier synchronize previously kicked work processgroupgloo 14294 failing test behavior therefore skipped . stack pr part removes deprecated code simplifications. easier change behavior synchronize strongly stack 14294 merged.
pytorch,16027,"feature readable error message index error nn.embedding cuda motivation nn.embedding received tensor containing larger values , raise error follows however, nn.embedding exists cuda, raises error message follows error message hard know error occurred. spent lot time find reason, created feature request.",0,readable error message index error nn.embedding cuda,"readable error message index error nn.embedding cuda feature readable error message index error nn.embedding cuda motivation nn.embedding received tensor containing larger values , raise error follows however, nn.embedding exists cuda, raises error message follows error message hard know error occurred. spent lot time find reason, created feature request."
pytorch,4655,"building pytorch docker get following error build stops cloned latest master ubuntu 16.04 docker version 17.09.1 ce, build 19e2cf6",0,error cuda9 docker build,"error cuda9 docker build building pytorch docker get following error build stops cloned latest master ubuntu 16.04 docker version 17.09.1 ce, build 19e2cf6"
pytorch,23466,"bug using pytorch without issue, found today importing torch, threw error. possibly regarding mkl library. could figure why. reproduce steps reproduce behavior 1. 1. 1. error message expected behavior clearly, happening. environment please copy paste output environment collection script https raw.githubusercontent.com pytorch pytorch master torch utils collect env.py fill checklist manually . get script run pytorch version e.g., 1.0 1.1.0 although shown above, version installed conda os e.g., linux installed pytorch , , source build command used compiling source python version cuda cudnn version gpu models configuration relevant information additional context idea happened probably using julia trying install ubuntu mac. maybe somehow messed environment?",0,import failure macos,"import failure macos bug using pytorch without issue, found today importing torch, threw error. possibly regarding mkl library. could figure why. reproduce steps reproduce behavior 1. 1. 1. error message expected behavior clearly, happening. environment please copy paste output environment collection script https raw.githubusercontent.com pytorch pytorch master torch utils collect env.py fill checklist manually . get script run pytorch version e.g., 1.0 1.1.0 although shown above, version installed conda os e.g., linux installed pytorch , , source build command used compiling source python version cuda cudnn version gpu models configuration relevant information additional context idea happened probably using julia trying install ubuntu mac. maybe somehow messed environment?"
pytorch,7857,could logistic distribution pytorch? https github.com tensorflow tensorflow blob r1.8 tensorflow contrib distributions python ops logistic.py cc fritzo neerajprad alicanb nikitaved brianjo mruberry,0,feature request logistic distribution,feature request logistic distribution could logistic distribution pytorch? https github.com tensorflow tensorflow blob r1.8 tensorflow contrib distributions python ops logistic.py cc fritzo neerajprad alicanb nikitaved brianjo mruberry
pytorch,28902,example c python cc yf225,0,"c torch tensor default gives double tensor, different python torch.tensor behavior","c torch tensor default gives double tensor, different python torch.tensor behavior example c python cc yf225"
pytorch,27793,support qnnpack dynamic quantization. rnn classes rnncell classes linear cc jerryzh168 jianyuh dzhulgakov,0,quantization qnnpack engine dynamic quantization,quantization qnnpack engine dynamic quantization support qnnpack dynamic quantization. rnn classes rnncell classes linear cc jerryzh168 jianyuh dzhulgakov
pytorch,24062,"feature extend initial implementation use qconfig dict, need scoping support, cc zolotukhinm",0,quant use qconfig dict graph mode,"quant use qconfig dict graph mode feature extend initial implementation use qconfig dict, need scoping support, cc zolotukhinm"
pytorch,27295,create table listing methods work quantized tensors subset tensor methods work .,0,documentation torch.tensor operations work qtensors,documentation torch.tensor operations work qtensors create table listing methods work quantized tensors subset tensor methods work .
pytorch,25512,"bug installed cuda cudnn via following tutorial https docs.nvidia.com deeplearning sdk cudnn install index.html , then, installed pytorch ubuntu 18.04 according pytorch website, selecting stable 1.2 , linux, conda, python3.7, cuda10.0, using virtualenv. then, go import import torch traceback recent call last file , line 1, modulenotfounderror module named 'torch' rebooted, made sure path variable included anaconda home dan anaconda3 bin home dan anaconda3 envs torch 1 2 0 bin home dan anaconda3 condabin usr local sbin usr local bin usr sbin usr bin sbin bin usr games usr local games snap bin usr local cuda 10.1 bin usr local cuda 10.1 bin sure pointing correct python alias python ' home dan anaconda3 bin python' upon seeing advice different ticket conda upgrade cudatoolkit , that, work. decided maybe since cuda version 10.1, would alter install command conda install pytorch torchvision cudatoolkit 10.0 c pytorch say cudatoolkit10.1. rolled pytorch back 1.0, cuda 9.0, generally work, put everything back that. pytorch version e.g., 1.0 1.2 os e.g., linux ubuntu 18.04 installed pytorch , , source conda install pytorch torchvision cudatoolkit 10.0 c pytorch correct virtualenv python version 3.7.3, conda version 4.6.11 cuda cudnn version 10.1 7.6.3 gpu models configuration nvidia 2080 ti rtx relevant information conda list name version build channel cudatoolkit 10.1.168 0 pytorch 1.2.0 cpu py37h00be3c6 0 torchvision 0.4.0 cuda100py37hecfc37a 0 help would greatly appreciated.",0,can't import torch installing conda ubuntu,"can't import torch installing conda ubuntu bug installed cuda cudnn via following tutorial https docs.nvidia.com deeplearning sdk cudnn install index.html , then, installed pytorch ubuntu 18.04 according pytorch website, selecting stable 1.2 , linux, conda, python3.7, cuda10.0, using virtualenv. then, go import import torch traceback recent call last file , line 1, modulenotfounderror module named 'torch' rebooted, made sure path variable included anaconda home dan anaconda3 bin home dan anaconda3 envs torch 1 2 0 bin home dan anaconda3 condabin usr local sbin usr local bin usr sbin usr bin sbin bin usr games usr local games snap bin usr local cuda 10.1 bin usr local cuda 10.1 bin sure pointing correct python alias python ' home dan anaconda3 bin python' upon seeing advice different ticket conda upgrade cudatoolkit , that, work. decided maybe since cuda version 10.1, would alter install command conda install pytorch torchvision cudatoolkit 10.0 c pytorch say cudatoolkit10.1. rolled pytorch back 1.0, cuda 9.0, generally work, put everything back that. pytorch version e.g., 1.0 1.2 os e.g., linux ubuntu 18.04 installed pytorch , , source conda install pytorch torchvision cudatoolkit 10.0 c pytorch correct virtualenv python version 3.7.3, conda version 4.6.11 cuda cudnn version 10.1 7.6.3 gpu models configuration nvidia 2080 ti rtx relevant information conda list name version build channel cudatoolkit 10.1.168 0 pytorch 1.2.0 cpu py37h00be3c6 0 torchvision 0.4.0 cuda100py37hecfc37a 0 help would greatly appreciated."
pytorch,3395,"starting commit https github.com pytorch pytorch commit 50e51eaa7fa5c252c2f4508cb5984734050177f1 linking nvrtc libcuda cuda builds. two consequences pytorch cuda builds done gpu enabled machines, libcuda.so comes nvidia driver pytorch binaries libcuda.so link dependency run cpu machines. need fix this, instead dlopen nvrtc, ship next set binaries run cpu cuda machines like previous releases .",0,dlopen libnvrtc instead dynamically linking,"dlopen libnvrtc instead dynamically linking starting commit https github.com pytorch pytorch commit 50e51eaa7fa5c252c2f4508cb5984734050177f1 linking nvrtc libcuda cuda builds. two consequences pytorch cuda builds done gpu enabled machines, libcuda.so comes nvidia driver pytorch binaries libcuda.so link dependency run cpu machines. need fix this, instead dlopen nvrtc, ship next set binaries run cpu cuda machines like previous releases ."
pytorch,25117,"successfully installed nvidia driver cudatoolkit via conda. however, able use cuda pytorch even though installed successfully . previously, using pytorch cuda 8.0, wanted upgrade. removed purge cuda updated nvidia drivers 4.10 via ppa ubuntu 16.04 everything worked smoothly. output output since wanted conda manage cuda version, installed cudatoolkit conda env python 3.6 again, everything installs perfectly. run using cuda fails. get following error message restarted, removed irrelevant environment variables may caused issues ld library path , removed conda, reinstalled, tried cuda 9.2, nothing works. sure issue could be. ideas? searched bit, found https discuss.pytorch.org found nvidia driver system 35063 pytorch thread. since completely removed cuda system problem, think somehow may related. edit surprising given error, following issue https github.com pytorch pytorch issues 4546 , checked",0,nvidia driver cudatoolkit installed properly check driver fails,"nvidia driver cudatoolkit installed properly check driver fails successfully installed nvidia driver cudatoolkit via conda. however, able use cuda pytorch even though installed successfully . previously, using pytorch cuda 8.0, wanted upgrade. removed purge cuda updated nvidia drivers 4.10 via ppa ubuntu 16.04 everything worked smoothly. output output since wanted conda manage cuda version, installed cudatoolkit conda env python 3.6 again, everything installs perfectly. run using cuda fails. get following error message restarted, removed irrelevant environment variables may caused issues ld library path , removed conda, reinstalled, tried cuda 9.2, nothing works. sure issue could be. ideas? searched bit, found https discuss.pytorch.org found nvidia driver system 35063 pytorch thread. since completely removed cuda system problem, think somehow may related. edit surprising given error, following issue https github.com pytorch pytorch issues 4546 , checked"
pytorch,27352,"checkout branch https github.com gottbrath pytorch tree quantization 1 3 doc docs , run make html docs, look index.html, old quantization docs show even though index.rst https github.com gottbrath pytorch blob quantization 1 3 doc docs source index.rst lists quantization.rst. testing research looks like quantization.rst multiple h1 headers. would need decrease headers except first one level. ! image https user images.githubusercontent.com 8042156 66173258 1b130600 e604 11e9 914c 7f3e2b874aa7.png",0,torch quant docs showing index.html,"torch quant docs showing index.html checkout branch https github.com gottbrath pytorch tree quantization 1 3 doc docs , run make html docs, look index.html, old quantization docs show even though index.rst https github.com gottbrath pytorch blob quantization 1 3 doc docs source index.rst lists quantization.rst. testing research looks like quantization.rst multiple h1 headers. would need decrease headers except first one level. ! image https user images.githubusercontent.com 8042156 66173258 1b130600 e604 11e9 914c 7f3e2b874aa7.png"
pytorch,15866,"traceback recent call last file home tcl pycharmprojects caffe2 mnist caffe2.py , line 278, workspace.createnet train model.net, overwrite false file usr local lib python2.7 dist packages caffe2 python workspace.py , line 171, createnet stringifyproto net , overwrite, file usr local lib python2.7 dist packages caffe2 python workspace.py , line 197, callwithexceptionintercept return func args, kwargs runtimeerror enforce fail cast op.cc 69 . casting half cpu supported yet frame 0 c10 throwenforcenotmet char const , int, char const , std cxx11 basic string, std allocator const , void const 0x78 0x7f234572e408 usr local lib python2.7 dist packages caffe2 python .. .. torch lib libc10.so frame 1 0x1636612 0x7f2346f74612 usr local lib python2.7 dist packages caffe2 python .. .. torch lib libcaffe2.so frame 2 0x163e3c8 0x7f2346f7c3c8 usr local lib python2.7 dist packages caffe2 python .. .. torch lib libcaffe2.so frame 3 0x163e55e 0x7f2346f7c55e usr local lib python2.7 dist packages caffe2 python .. .. torch lib libcaffe2.so frame 4 std function handler caffe2 operatordef const , caffe2 workspace , std unique ptr caffe2 operatordef const , caffe2 workspace invoke std data const , caffe2 operatordef const , caffe2 workspace 0x23 0x7f2348971803 usr local lib python2.7 dist packages caffe2 python caffe2 pybind11 state.so frame 5 0x13efc1c 0x7f2346d2dc1c usr local lib python2.7 dist packages caffe2 python .. .. torch lib libcaffe2.so frame 6 0x13f2aaa 0x7f2346d30aaa usr local lib python2.7 dist packages caffe2 python .. .. torch lib libcaffe2.so frame 7 caffe2 createoperator caffe2 operatordef const , caffe2 workspace , int 0x3b9 0x7f2346d30f29 usr local lib python2.7 dist packages caffe2 python .. .. torch lib libcaffe2.so frame 8 caffe2 simplenet simplenet std shared ptr const , caffe2 workspace 0x295 0x7f2346d27b75 usr local lib python2.7 dist packages caffe2 python .. .. torch lib libcaffe2.so frame 9 0x13ebf2e 0x7f2346d29f2e usr local lib python2.7 dist packages caffe2 python .. .. torch lib libcaffe2.so frame 10 0x13ce563 0x7f2346d0c563 usr local lib python2.7 dist packages caffe2 python .. .. torch lib libcaffe2.so frame 11 caffe2 createnet std shared ptr const , caffe2 workspace 0x47d 0x7f2346cffc4d usr local lib python2.7 dist packages caffe2 python .. .. torch lib libcaffe2.so frame 12 caffe2 workspace createnet std shared ptr const , bool 0xfd 0x7f2346d6e1dd usr local lib python2.7 dist packages caffe2 python .. .. torch lib libcaffe2.so frame 13 caffe2 workspace createnet caffe2 netdef const , bool 0x91 0x7f2346d6f371 usr local lib python2.7 dist packages caffe2 python .. .. torch lib libcaffe2.so frame 14 0x4e616 0x7f234896a616 usr local lib python2.7 dist packages caffe2 python caffe2 pybind11 state.so frame 15 0x4e8dc 0x7f234896a8dc usr local lib python2.7 dist packages caffe2 python caffe2 pybind11 state.so frame 16 0x86831 0x7f23489a2831 usr local lib python2.7 dist packages caffe2 python caffe2 pybind11 state.so frame 17 pyeval evalframeex 0x695b 0x55e76d2878db usr bin python2.7 frame 18 pyeval evalcodeex 0x6da 0x55e76d27ed0a usr bin python2.7 frame 19 pyeval evalframeex 0x5cb8 0x55e76d286c38 usr bin python2.7 frame 20 pyeval evalcodeex 0x6da 0x55e76d27ed0a usr bin python2.7 frame 21 pyeval evalframeex 0x567e 0x55e76d2865fe usr bin python2.7 frame 22 pyeval evalcodeex 0x6da 0x55e76d27ed0a usr bin python2.7 frame 23 pyeval evalcode 0x19 0x55e76d27e629 usr bin python2.7 frame 24 0x12461f 0x55e76d2af61f usr bin python2.7 frame 25 pyrun fileexflags 0x82 0x55e76d2aa322 usr bin python2.7 frame 26 pyrun simplefileexflags 0x18d 0x55e76d2a967d usr bin python2.7 frame 27 py main 0x68b 0x55e76d2581ab usr bin python2.7 frame 28 libc start main 0xe7 0x7f2367bf4b97 lib x86 64 linux gnu libc.so.6 frame 29 start 0x2a 0x55e76d257a2a usr bin python2.7",0,casting half cpu supported yet,"casting half cpu supported yet traceback recent call last file home tcl pycharmprojects caffe2 mnist caffe2.py , line 278, workspace.createnet train model.net, overwrite false file usr local lib python2.7 dist packages caffe2 python workspace.py , line 171, createnet stringifyproto net , overwrite, file usr local lib python2.7 dist packages caffe2 python workspace.py , line 197, callwithexceptionintercept return func args, kwargs runtimeerror enforce fail cast op.cc 69 . casting half cpu supported yet frame 0 c10 throwenforcenotmet char const , int, char const , std cxx11 basic string, std allocator const , void const 0x78 0x7f234572e408 usr local lib python2.7 dist packages caffe2 python .. .. torch lib libc10.so frame 1 0x1636612 0x7f2346f74612 usr local lib python2.7 dist packages caffe2 python .. .. torch lib libcaffe2.so frame 2 0x163e3c8 0x7f2346f7c3c8 usr local lib python2.7 dist packages caffe2 python .. .. torch lib libcaffe2.so frame 3 0x163e55e 0x7f2346f7c55e usr local lib python2.7 dist packages caffe2 python .. .. torch lib libcaffe2.so frame 4 std function handler caffe2 operatordef const , caffe2 workspace , std unique ptr caffe2 operatordef const , caffe2 workspace invoke std data const , caffe2 operatordef const , caffe2 workspace 0x23 0x7f2348971803 usr local lib python2.7 dist packages caffe2 python caffe2 pybind11 state.so frame 5 0x13efc1c 0x7f2346d2dc1c usr local lib python2.7 dist packages caffe2 python .. .. torch lib libcaffe2.so frame 6 0x13f2aaa 0x7f2346d30aaa usr local lib python2.7 dist packages caffe2 python .. .. torch lib libcaffe2.so frame 7 caffe2 createoperator caffe2 operatordef const , caffe2 workspace , int 0x3b9 0x7f2346d30f29 usr local lib python2.7 dist packages caffe2 python .. .. torch lib libcaffe2.so frame 8 caffe2 simplenet simplenet std shared ptr const , caffe2 workspace 0x295 0x7f2346d27b75 usr local lib python2.7 dist packages caffe2 python .. .. torch lib libcaffe2.so frame 9 0x13ebf2e 0x7f2346d29f2e usr local lib python2.7 dist packages caffe2 python .. .. torch lib libcaffe2.so frame 10 0x13ce563 0x7f2346d0c563 usr local lib python2.7 dist packages caffe2 python .. .. torch lib libcaffe2.so frame 11 caffe2 createnet std shared ptr const , caffe2 workspace 0x47d 0x7f2346cffc4d usr local lib python2.7 dist packages caffe2 python .. .. torch lib libcaffe2.so frame 12 caffe2 workspace createnet std shared ptr const , bool 0xfd 0x7f2346d6e1dd usr local lib python2.7 dist packages caffe2 python .. .. torch lib libcaffe2.so frame 13 caffe2 workspace createnet caffe2 netdef const , bool 0x91 0x7f2346d6f371 usr local lib python2.7 dist packages caffe2 python .. .. torch lib libcaffe2.so frame 14 0x4e616 0x7f234896a616 usr local lib python2.7 dist packages caffe2 python caffe2 pybind11 state.so frame 15 0x4e8dc 0x7f234896a8dc usr local lib python2.7 dist packages caffe2 python caffe2 pybind11 state.so frame 16 0x86831 0x7f23489a2831 usr local lib python2.7 dist packages caffe2 python caffe2 pybind11 state.so frame 17 pyeval evalframeex 0x695b 0x55e76d2878db usr bin python2.7 frame 18 pyeval evalcodeex 0x6da 0x55e76d27ed0a usr bin python2.7 frame 19 pyeval evalframeex 0x5cb8 0x55e76d286c38 usr bin python2.7 frame 20 pyeval evalcodeex 0x6da 0x55e76d27ed0a usr bin python2.7 frame 21 pyeval evalframeex 0x567e 0x55e76d2865fe usr bin python2.7 frame 22 pyeval evalcodeex 0x6da 0x55e76d27ed0a usr bin python2.7 frame 23 pyeval evalcode 0x19 0x55e76d27e629 usr bin python2.7 frame 24 0x12461f 0x55e76d2af61f usr bin python2.7 frame 25 pyrun fileexflags 0x82 0x55e76d2aa322 usr bin python2.7 frame 26 pyrun simplefileexflags 0x18d 0x55e76d2a967d usr bin python2.7 frame 27 py main 0x68b 0x55e76d2581ab usr bin python2.7 frame 28 libc start main 0xe7 0x7f2367bf4b97 lib x86 64 linux gnu libc.so.6 frame 29 start 0x2a 0x55e76d257a2a usr bin python2.7"
pytorch,8181,see https github.com pytorch pytorch pull 7411 backstory,0,stop using pip import main,stop using pip import main see https github.com pytorch pytorch pull 7411 backstory
pytorch,11907,"hi, like many, installed pytorch, says installing package 'pytorch' yet called 'torch' attempt import python. cuts standard practices entire python world. please remedy. thanks!",0,please rename package pytorch stay consistent entire python world,"please rename package pytorch stay consistent entire python world hi, like many, installed pytorch, says installing package 'pytorch' yet called 'torch' attempt import python. cuts standard practices entire python world. please remedy. thanks!"
pytorch,17882,nccl 2.4 functions detect errors abort running kernels. required implement timeouts force workers raise error terminate workers fail. see https docs.nvidia.com deeplearning sdk nccl developer guide docs api comms.html https devblogs.nvidia.com massively scale deep learning training nccl 2 4 example.,0,processgroupnccl error timeout handling,processgroupnccl error timeout handling nccl 2.4 functions detect errors abort running kernels. required implement timeouts force workers raise error terminate workers fail. see https docs.nvidia.com deeplearning sdk nccl developer guide docs api comms.html https devblogs.nvidia.com massively scale deep learning training nccl 2 4 example.
pytorch,1280,"specify multiple gpu ids train network using fine. however, comes time run validation dataset trained network, pytorch throws error using list gpu ids unless gpu id list starts id 0. here's sample function cause error train network using function executes problem. however, train network using throw error.",0,multi gpu forward pass fails first gpu id 0,"multi gpu forward pass fails first gpu id 0 specify multiple gpu ids train network using fine. however, comes time run validation dataset trained network, pytorch throws error using list gpu ids unless gpu id list starts id 0. here's sample function cause error train network using function executes problem. however, train network using throw error."
pytorch,21922,bug using cuda aware mpi cuda device causes segmentation fault. managed bypass specific use case setting using within pytorch. reproduce steps reproduce behavior mpirun np 4 oversubscribe host 127.0.0.1 python test.py segmentation fault expected behavior crash environment pytorch version 1.1.0 debug build cuda used build pytorch 10.0.130 os ubuntu 16.04.6 lts gcc version ubuntu 5.4.0 6ubuntu1 16.04.11 5.4.0 20160609 cmake version version 3.14.0 python version 3.6 cuda available yes cuda runtime version 10.0.130 gpu models configuration gpu 0 tesla p100 pcie 16gb gpu 1 tesla p100 pcie 16gb nvidia driver version 418.67 cudnn version could collect versions relevant libraries pip3 numpy 1.16.4 pip3 numpydoc 0.8.0 pip3 torch 1.1.0 pip3 torchvision 0.3.0a0 c94a158 conda blas 1.0 mkl conda libmklml 2018.0.3 0 conda magma cuda100 2.5.0 1 pytorch conda mkl 2019.4 243 conda mkl dnn 0.14 h6bb024c 0 conda mkl include 2019.4 243 conda mkl service 2.0.2 py36h7b6447c 0 conda mkl fft 1.0.12 py36ha843d7b 0 conda mkl random 1.0.2 py36hd81dba3 0 conda torch 1.1.0 pypi 0 pypi conda torchvision 0.3.0a0 c94a158 pypi 0 pypi mpi 3.0.0 cuda aware,0,segmentation fault using reduce cuda 1 mpi,segmentation fault using reduce cuda 1 mpi bug using cuda aware mpi cuda device causes segmentation fault. managed bypass specific use case setting using within pytorch. reproduce steps reproduce behavior mpirun np 4 oversubscribe host 127.0.0.1 python test.py segmentation fault expected behavior crash environment pytorch version 1.1.0 debug build cuda used build pytorch 10.0.130 os ubuntu 16.04.6 lts gcc version ubuntu 5.4.0 6ubuntu1 16.04.11 5.4.0 20160609 cmake version version 3.14.0 python version 3.6 cuda available yes cuda runtime version 10.0.130 gpu models configuration gpu 0 tesla p100 pcie 16gb gpu 1 tesla p100 pcie 16gb nvidia driver version 418.67 cudnn version could collect versions relevant libraries pip3 numpy 1.16.4 pip3 numpydoc 0.8.0 pip3 torch 1.1.0 pip3 torchvision 0.3.0a0 c94a158 conda blas 1.0 mkl conda libmklml 2018.0.3 0 conda magma cuda100 2.5.0 1 pytorch conda mkl 2019.4 243 conda mkl dnn 0.14 h6bb024c 0 conda mkl include 2019.4 243 conda mkl service 2.0.2 py36h7b6447c 0 conda mkl fft 1.0.12 py36ha843d7b 0 conda mkl random 1.0.2 py36hd81dba3 0 conda torch 1.1.0 pypi 0 pypi conda torchvision 0.3.0a0 c94a158 pypi 0 pypi mpi 3.0.0 cuda aware
pytorch,20101,bug tracing nn.sequential nn.conv2d torch 1.1.0 reproduce steps reproduce behavior raises following error expected behavior expected convert without issues environment pytorch version 1.1.0 debug build cuda used build pytorch 9.0.176 os ubuntu 18.04.2 lts gcc version ubuntu 7.3.0 27ubuntu1 18.04 7.3.0 cmake version version 3.10.2 python version 3.6 cuda available yes cuda runtime version 9.1.85 cudnn version usr local cuda 9.0 targets x86 64 linux lib libcudnn.so.7.0.5 versions relevant libraries pip3 numpy 1.16.2 pip3 numpy image widget 2019.1.6 pip3 torch 1.1.0 pip3 torchfile 0.1.0 pip3 torchvision 0.2.1 conda could collect,0,jit tracing error nn.sequential nn.conv2d torch 1.1.0,jit tracing error nn.sequential nn.conv2d torch 1.1.0 bug tracing nn.sequential nn.conv2d torch 1.1.0 reproduce steps reproduce behavior raises following error expected behavior expected convert without issues environment pytorch version 1.1.0 debug build cuda used build pytorch 9.0.176 os ubuntu 18.04.2 lts gcc version ubuntu 7.3.0 27ubuntu1 18.04 7.3.0 cmake version version 3.10.2 python version 3.6 cuda available yes cuda runtime version 9.1.85 cudnn version usr local cuda 9.0 targets x86 64 linux lib libcudnn.so.7.0.5 versions relevant libraries pip3 numpy 1.16.2 pip3 numpy image widget 2019.1.6 pip3 torch 1.1.0 pip3 torchfile 0.1.0 pip3 torchvision 0.2.1 conda could collect
pytorch,9261,"hi again, one, sure whether cupy pytorch issue, either, think run memory high. could refcounting thing things going scope properly. case would cupy issue. best regards thomas",0,memory leak pytorch dlpack cupy,"memory leak pytorch dlpack cupy hi again, one, sure whether cupy pytorch issue, either, think run memory high. could refcounting thing things going scope properly. case would cupy issue. best regards thomas"
pytorch,1626,"continued https github.com pytorch pytorch issues 1624 running tests debug build python3.6.1. following tests testnn failed gc problems test variable sequence cuda test cuda rnn fused test rnn initial hidden state test rnn cpu vs cudnn dropout test rnn cpu vs cudnn dropout test rnn dropout state test rnn change dropout error gc gc.gc refs 1 ! 0' failed. 0 0x00007ffff712c1d7 raise lib64 libc.so.6 1 0x00007ffff712d8c8 abort lib64 libc.so.6 2 0x00007ffff7125146 assert fail base lib64 libc.so.6 3 0x00007ffff71251f2 lib64 libc.so.6 4 0x0000000000435f63 visit decref op 0x7fffc006e4f8, data modules gcmodule.c 380 5 0x00007fffecd7c20d thpvariable traverse thpvariable , visitproc, void self self entry 0x7fffc00781f0, visit visit entry 0x435efd , arg arg entry 0x0 torch csrc autograd python variable.cpp 92 6 0x00000000004a86ce subtype traverse self 0x7fffc00781f0, visit 0x435efd , arg 0x0 objects typeobject.c 1021 7 0x00000000004356ee subtract refs containers containers entry 0x885d20 modules gcmodule.c 399 8 0x0000000000436809 collect generation generation entry 0, n collected n collected entry 0x7fffffff96b8, n uncollectable n uncollectable entry 0x7fffffff96b0, nofail nofail entry 0 modules gcmodule.c 956 9 0x0000000000436c47 collect callback generation 0 modules gcmodule.c 1128 10 0x0000000000436d3f collect generations modules gcmodule.c 1151 11 0x0000000000436eac pyobject gc alloc use calloc use calloc entry 0, basicsize basicsize entry 72 modules gcmodule.c 1726 12 0x0000000000437382 pyobject gc malloc basicsize basicsize entry 72 modules gcmodule.c 1736 13 0x00000000004a8443 pytype genericalloc type 0x7fffed9eb400 , nitems 2 objects typeobject.c 936 14 0x00007fffecd3083f thpsize new int, long dim 2, sizes 0x4547f5c0 torch csrc size.cpp 16 15 0x00007fffed1c2102 thcpdoubletensor size object , object , object self , args 0x7ffff7fa0058, kwargs 0x0 data users gchanan pytorch6 torch csrc generic tensormethods.cpp 650 16 0x00000000004950a9 pycfunction fastcalldict func obj func obj entry 0x7fffc28df328, args args entry 0x4547e598, nargs nargs entry 0, kwargs kwargs entry 0x0 objects methodobject.c 231 17 0x000000000049545a pycfunction fastcallkeywords func func entry 0x7fffc28df328, stack stack entry 0x4547e598, nargs nargs entry 0, kwnames kwnames entry 0x0 objects methodobject.c 295 18 0x0000000000530f41 call function pp stack pp stack entry 0x7fffffff9908, oparg oparg entry 0, kwnames kwnames entry 0x0 python ceval.c 4798 19 0x000000000053a5b6 pyeval evalframedefault f , throwflag python ceval.c 3284 20 0x0000000000530124 pyeval evalframeex f f entry 0x4547e3e8, throwflag throwflag entry 0 python ceval.c 718 21 0x0000000000530b4b pyeval evalcodewithname co 0x7fffc2406e80, globals , locals locals entry 0x0, args args entry 0x7fffc008c480, argcount 4, kwnames kwnames entry 0x0, kwargs kwargs entry 0x8, kwcount kwcount entry 0, kwstep kwstep entry 2, defs defs entry 0x0, defcount defcount entry 0, kwdefs kwdefs entry 0x0, closure closure entry 0x0, name name entry 0x0, qualname qualname entry 0x0 python ceval.c 4128 22 0x0000000000531168 pyeval evalcodeex co , globals , locals locals entry 0x0, args args entry 0x7fffc008c480, argcount , kws kws entry 0x0, kwcount kwcount entry 0, defs defs entry 0x0, defcount defcount entry 0, kwdefs 0x0, closure 0x0 python ceval.c 4149 23 0x000000000047386f function call func 0x7fffc23a6840, arg 0x7fffc008c458, kw 0x0 objects funcobject.c 604 24 0x0000000000444e8c pyobject call func func entry 0x7fffc23a6840, args args entry 0x7fffc008c458, kwargs kwargs entry 0x0 objects abstract.c 2246 25 0x00000000005316b0 pyeval callobjectwithkeywords func 0x7fffc23a6840, args 0x7fffc008c458, kwargs kwargs entry 0x0 python ceval.c 4718 26 0x0000000000444c69 pyobject callobject , objects abstract.c 2172 27 0x00007fffecd74eb3 thpfunction apply object , object cls 0x11c9178, inputs 0x7fffc1da3418 torch csrc autograd python function.cpp 722 28 0x00000000004950b8 pycfunction fastcalldict func obj func obj entry 0x7fffc28d61c0, args args entry 0x4547bf70, nargs nargs entry 3, kwargs kwargs entry 0x0 objects methodobject.c 234 29 0x000000000049545a pycfunction fastcallkeywords func func entry 0x7fffc28d61c0, stack stack entry 0x4547bf70, nargs nargs entry 3, kwnames kwnames entry 0x0 objects methodobject.c 295 30 0x0000000000530f41 call function pp stack pp stack entry 0x7fffffff9e58, oparg oparg entry 3, kwnames kwnames entry 0x0 python ceval.c 4798 31 0x000000000053a5b6 pyeval evalframedefault f , throwflag python ceval.c 3284 32 0x0000000000530124 pyeval evalframeex f f entry 0x4547bdc8, throwflag throwflag entry 0 python ceval.c 718 33 0x00000000005301ec pyfunction fastcall co co entry 0x7fffc31e2640, args 0x7fffffffa038, args entry 0x7fffffffa020, nargs nargs entry 3, globals globals entry 0x7fffc31e1e68 python ceval.c 4880 type continue, q quit 34 0x000000000053c110 pyfunction fastcalldict func func entry 0x7fffc23f61c8, args args entry 0x7fffffffa020, nargs nargs entry 3, kwargs kwargs entry 0x0 python ceval.c 4982 35 0x0000000000445042 pyobject fastcalldict func func entry 0x7fffc23f61c8, args args entry 0x7fffffffa020, nargs nargs entry 3, kwargs kwargs entry 0x0 objects abstract.c 2295 36 0x0000000000445345 pyobject call prepend func 0x7fffc23f61c8, obj 0x7fffc0085e28, args 0x7fffc22eb988, kwargs 0x0 objects abstract.c 2358 37 0x000000000045ee6b method call method , args , kwargs objects classobject.c 317 38 0x0000000000444e8c pyobject call func func entry 0x7fffc23963d8, args args entry 0x7fffc22eb988, kwargs kwargs entry 0x0 objects abstract.c 2246 39 0x00000000004af68e call method , nameid nameid entry 0x8a5740 , format format entry 0x635a54 oo objects typeobject.c 1453 40 0x00000000004af814 slot mp ass subscript self , key , value objects typeobject.c 5944 41 0x00000000004444f0 pyobject setitem entry 0x7fffc0085e28, key key entry 0x7fffc22eb838, value value entry 0x7fffc0089b80 objects abstract.c 180 42 0x00000000005345cc pyeval evalframedefault f , throwflag python ceval.c 1727 43 0x0000000000530124 pyeval evalframeex f f entry 0x4547ed58, throwflag throwflag entry 0 python ceval.c 718 44 0x0000000000530b4b pyeval evalcodewithname co 0x7fffc1f6b700, globals , locals locals entry 0x0, args , argcount 1, kwnames 0x0, kwargs 0x307b1470, kwcount 0, kwstep kwstep entry 1, defs 0x7fffc1f69d18, defcount defcount entry 1, kwdefs kwdefs entry 0x0, closure closure entry 0x0, name name entry 0x7fffefef55e0, qualname qualname entry 0x7fffefef55e0 python ceval.c 4128 45 0x0000000000530da7 fast function func func entry 0x7fffc1f8a110, stack , nargs nargs entry 1, kwnames kwnames entry 0x0 python ceval.c 4939 46 0x0000000000530ffe call function pp stack pp stack entry 0x7fffffffa4e8, oparg oparg entry 1, kwnames kwnames entry 0x0 python ceval.c 4819 47 0x000000000053a5b6 pyeval evalframedefault f , throwflag python ceval.c 3284 48 0x0000000000530124 pyeval evalframeex f f entry 0x307b1208, throwflag throwflag entry 0 python ceval.c 718 49 0x00000000005301ec pyfunction fastcall co , args 0x7fffc231ff18, nargs nargs entry 2, globals python ceval.c 4880 50 0x0000000000530cf7 fast function func func entry 0x7fffc1dc39b0, stack , nargs nargs entry 2, kwnames kwnames entry 0x0 python ceval.c 4915 51 0x0000000000530ffe call function pp stack pp stack entry 0x7fffffffa6e8, oparg oparg entry 1, kwnames kwnames entry 0x0 python ceval.c 4819 52 0x000000000053a5b6 pyeval evalframedefault f , throwflag python ceval.c 3284 53 0x0000000000530124 pyeval evalframeex f f entry 0x7fffc231fd78, throwflag throwflag entry 0 python ceval.c 718 54 0x00000000005301ec pyfunction fastcall co , args 0x307b0898, nargs nargs entry 1, globals python ceval.c 4880 55 0x0000000000530cf7 fast function func func entry 0x7fffc1dc3b20, stack , nargs nargs entry 1, kwnames kwnames entry 0x0 python ceval.c 4915 56 0x0000000000530ffe call function pp stack pp stack entry 0x7fffffffa8e8, oparg oparg entry 0, kwnames kwnames entry 0x0 python ceval.c 4819 57 0x000000000053a5b6 pyeval evalframedefault f , throwflag python ceval.c 3284 58 0x0000000000530124 pyeval evalframeex f f entry 0x307b0698, throwflag throwflag entry 0 python ceval.c 718 59 0x0000000000530b4b pyeval evalcodewithname co co entry 0x7fffede08340, globals globals entry 0x7fffede77670, locals locals entry 0x0, args args entry 0x7fffffffab20, argcount argcount entry 2, kwnames kwnames entry 0x7ffff7fa0080, kwargs kwargs entry 0x7ffff7fa0088, kwcount kwcount entry 0, kwstep kwstep entry 2, defs 0x7fffede0ff88, defcount 1, kwdefs 0x0, closure 0x0, name 0x7fffede06860, qualname 0x7fffede054a0 python ceval.c 4128 60 0x000000000053c29d pyfunction fastcalldict func func entry 0x7fffedd4f280, args args entry 0x7fffffffab20, nargs nargs entry 2, kwargs kwargs entry 0x7fffc00e09b8 python ceval.c 5031 61 0x0000000000445042 pyobject fastcalldict func func entry 0x7fffedd4f280, args args entry 0x7fffffffab20, nargs nargs entry 2, kwargs kwargs entry 0x7fffc00e09b8 objects abstract.c 2295 62 0x0000000000445345 pyobject call prepend func 0x7fffedd4f280, obj 0x7fffc00d0e28, args 0x7fffc00e1ae8, kwargs 0x7fffc00e09b8 objects abstract.c 2358 63 0x000000000045ee6b method call method , args , kwargs objects classobject.c 317 64 0x0000000000444e8c pyobject call func 0x7fffeff04838, args 0x7fffc00e1ae8, kwargs 0x7fffc00e09b8 objects abstract.c 2246 65 0x000000000052f6b9 call core func func entry 0x7fffeff04838, callargs callargs entry 0x7fffc00e1ae8, kwdict kwdict entry 0x7fffc00e09b8 python ceval.c 5067 66 0x000000000053aad1 pyeval evalframedefault f , throwflag python ceval.c 3366 67 0x0000000000530124 pyeval evalframeex f f entry 0x7fffc0064250, throwflag throwflag entry 0 python ceval.c 718 68 0x0000000000530b4b pyeval evalcodewithname co co entry 0x7fffede084c0, globals globals entry 0x7fffede77670, locals locals entry 0x0, args args entry 0x7fffffffaec0, argcount argcount entry 2, kwnames kwnames entry 0x0, kwargs kwargs entry 0x8, kwcount kwcount entry 0, kwstep kwstep entry 2, defs 0x0, defcount 0, kwdefs 0x0, closure 0x0, name 0x7ffff7fa3270, qualname 0x7fffede04ce8 python ceval.c 4128 type continue, q quit 69 0x000000000053c29d pyfunction fastcalldict func func entry 0x7fffedd4f3f0, args args entry 0x7fffffffaec0, nargs nargs entry 2, kwargs kwargs entry 0x0 python ceval.c 5031 70 0x0000000000445042 pyobject fastcalldict func func entry 0x7fffedd4f3f0, args args entry 0x7fffffffaec0, nargs nargs entry 2, kwargs kwargs entry 0x0 objects abstract.c 2295 71 0x0000000000445345 pyobject call prepend func 0x7fffedd4f3f0, obj 0x7fffc00d0e28, args 0x7fffc00e1b50, kwargs 0x0 objects abstract.c 2358 72 0x000000000045ee6b method call method , args , kwargs objects classobject.c 317 73 0x0000000000444e8c pyobject call func func entry 0x7fffede3c6e8, args args entry 0x7fffc00e1b50, kwargs kwargs entry 0x0 objects abstract.c 2246 74 0x00000000004b1e1d slot tp call self , args 0x7fffc00e1b50, kwds 0x0 objects typeobject.c 6167 75 0x00000000004450b1 pyobject fastcalldict func func entry 0x7fffc00d0e28, args args entry 0x307af060, nargs nargs entry 1, kwargs kwargs entry 0x0 objects abstract.c 2316 76 0x00000000004456e5 pyobject fastcallkeywords func func entry 0x7fffc00d0e28, stack 0x307af060, nargs nargs entry 1, kwnames kwnames entry 0x0 objects abstract.c 2480 77 0x0000000000531011 call function pp stack pp stack entry 0x7fffffffb0c8, oparg oparg entry 1, kwnames kwnames entry 0x0 python ceval.c 4822 78 0x000000000053a5b6 pyeval evalframedefault f , throwflag python ceval.c 3284 79 0x0000000000530124 pyeval evalframeex f f entry 0x307aee98, throwflag throwflag entry 0 python ceval.c 718 80 0x0000000000530b4b pyeval evalcodewithname co co entry 0x7fffede1e880, globals globals entry 0x7fffedeaad78, locals locals entry 0x0, args args entry 0x7fffffffb300, argcount argcount entry 2, kwnames kwnames entry 0x7ffff7fa0080, kwargs kwargs entry 0x7ffff7fa0088, kwcount kwcount entry 0, kwstep kwstep entry 2, defs 0x7fffedd4d5c8, defcount 1, kwdefs 0x0, closure 0x0, name 0x7fffede06860, qualname 0x7fffedd50970 python ceval.c 4128 81 0x000000000053c29d pyfunction fastcalldict func func entry 0x7fffedd54eb8, args args entry 0x7fffffffb300, nargs nargs entry 2, kwargs kwargs entry 0x7fffc00e0a30 python ceval.c 5031 82 0x0000000000445042 pyobject fastcalldict func func entry 0x7fffedd54eb8, args args entry 0x7fffffffb300, nargs nargs entry 2, kwargs kwargs entry 0x7fffc00e0a30 objects abstract.c 2295 83 0x0000000000445345 pyobject call prepend func 0x7fffedd54eb8, obj 0x7fffc00e1400, args 0x7fffc00e1cf0, kwargs 0x7fffc00e0a30 objects abstract.c 2358 84 0x000000000045ee6b method call method , args , kwargs objects classobject.c 317 85 0x0000000000444e8c pyobject call func 0x7ffff064cf38, args 0x7fffc00e1cf0, kwargs 0x7fffc00e0a30 objects abstract.c 2246 86 0x000000000052f6b9 call core func func entry 0x7ffff064cf38, callargs callargs entry 0x7fffc00e1cf0, kwdict kwdict entry 0x7fffc00e0a30 python ceval.c 5067 87 0x000000000053aad1 pyeval evalframedefault f , throwflag python ceval.c 3366 88 0x0000000000530124 pyeval evalframeex f f entry 0x7fffc00d9e20, throwflag throwflag entry 0 python ceval.c 718 89 0x0000000000530b4b pyeval evalcodewithname co co entry 0x7fffede14a00, globals globals entry 0x7fffedeaad78, locals locals entry 0x0, args args entry 0x7fffffffb6a0, argcount argcount entry 2, kwnames kwnames entry 0x0, kwargs kwargs entry 0x8, kwcount kwcount entry 0, kwstep kwstep entry 2, defs 0x0, defcount 0, kwdefs 0x0, closure 0x0, name 0x7ffff7fa3270, qualname 0x7fffedd55838 python ceval.c 4128 90 0x000000000053c29d pyfunction fastcalldict func func entry 0x7fffedd54d48, args args entry 0x7fffffffb6a0, nargs nargs entry 2, kwargs kwargs entry 0x0 python ceval.c 5031 91 0x0000000000445042 pyobject fastcalldict func func entry 0x7fffedd54d48, args args entry 0x7fffffffb6a0, nargs nargs entry 2, kwargs kwargs entry 0x0 objects abstract.c 2295 92 0x0000000000445345 pyobject call prepend func 0x7fffedd54d48, obj 0x7fffc00e1400, args 0x7fffc00d0e90, kwargs 0x0 objects abstract.c 2358 93 0x000000000045ee6b method call method , args , kwargs objects classobject.c 317 94 0x0000000000444e8c pyobject call func func entry 0x7fffedee0288, args args entry 0x7fffc00d0e90, kwargs kwargs entry 0x0 objects abstract.c 2246 95 0x00000000004b1e1d slot tp call self , args 0x7fffc00d0e90, kwds 0x0 objects typeobject.c 6167 96 0x00000000004450b1 pyobject fastcalldict func func entry 0x7fffc00e1400, args args entry 0x307ae830, nargs nargs entry 1, kwargs kwargs entry 0x0 objects abstract.c 2316 97 0x00000000004456e5 pyobject fastcallkeywords func func entry 0x7fffc00e1400, stack 0x307ae830, nargs nargs entry 1, kwnames kwnames entry 0x0 objects abstract.c 2480 98 0x0000000000531011 call function pp stack pp stack entry 0x7fffffffb8a8, oparg oparg entry 1, kwnames kwnames entry 0x0 python ceval.c 4822 99 0x000000000053a5b6 pyeval evalframedefault f , throwflag python ceval.c 3284 100 0x0000000000530124 pyeval evalframeex f f entry 0x307ae668, throwflag throwflag entry 0 python ceval.c 718 101 0x0000000000530b4b pyeval evalcodewithname co co entry 0x7fffede1e880, globals globals entry 0x7fffedeaad78, locals locals entry 0x0, args args entry 0x7fffffffbae0, argcount argcount entry 2, kwnames kwnames entry 0x7ffff7fa0080, kwargs kwargs entry 0x7ffff7fa0088, kwcount kwcount entry 0, kwstep kwstep entry 2, defs 0x7fffedd4d5c8, defcount 1, kwdefs 0x0, closure 0x0, name 0x7fffede06860, qualname 0x7fffedd50970 python ceval.c 4128 102 0x000000000053c29d pyfunction fastcalldict func func entry 0x7fffedd54eb8, args args entry 0x7fffffffbae0, nargs nargs entry 2, kwargs kwargs entry 0x7fffc00e0058 python ceval.c 5031 103 0x0000000000445042 pyobject fastcalldict func func entry 0x7fffedd54eb8, args args entry 0x7fffffffbae0, nargs nargs entry 2, kwargs kwargs entry 0x7fffc00e0058 objects abstract.c 2295 type continue, q quit 104 0x0000000000445345 pyobject call prepend func 0x7fffedd54eb8, obj 0x7fffc00d0dc0, args 0x7fffc00d0ae8, kwargs 0x7fffc00e0058 objects abstract.c 2358 105 0x000000000045ee6b method call method , args , kwargs objects classobject.c 317 106 0x0000000000444e8c pyobject call func 0x7fffede3c7c8, args 0x7fffc00d0ae8, kwargs 0x7fffc00e0058 objects abstract.c 2246 107 0x000000000052f6b9 call core func func entry 0x7fffede3c7c8, callargs callargs entry 0x7fffc00d0ae8, kwdict kwdict entry 0x7fffc00e0058 python ceval.c 5067 108 0x000000000053aad1 pyeval evalframedefault f , throwflag python ceval.c 3366 109 0x0000000000530124 pyeval evalframeex f f entry 0x7fffc00d9c28, throwflag throwflag entry 0 python ceval.c 718 110 0x0000000000530b4b pyeval evalcodewithname co co entry 0x7fffede14a00, globals globals entry 0x7fffedeaad78, locals locals entry 0x0, args args entry 0x7fffffffbe80, argcount argcount entry 2, kwnames kwnames entry 0x0, kwargs kwargs entry 0x8, kwcount kwcount entry 0, kwstep kwstep entry 2, defs 0x0, defcount 0, kwdefs 0x0, closure 0x0, name 0x7ffff7fa3270, qualname 0x7fffedd55838 python ceval.c 4128 111 0x000000000053c29d pyfunction fastcalldict func func entry 0x7fffedd54d48, args args entry 0x7fffffffbe80, nargs nargs entry 2, kwargs kwargs entry 0x0 python ceval.c 5031 112 0x0000000000445042 pyobject fastcalldict func func entry 0x7fffedd54d48, args args entry 0x7fffffffbe80, nargs nargs entry 2, kwargs kwargs entry 0x0 objects abstract.c 2295 113 0x0000000000445345 pyobject call prepend func 0x7fffedd54d48, obj 0x7fffc00d0dc0, args 0x7fffc00d0bb8, kwargs 0x0 objects abstract.c 2358 114 0x000000000045ee6b method call method , args , kwargs objects classobject.c 317 115 0x0000000000444e8c pyobject call func func entry 0x7ffff0655e58, args args entry 0x7fffc00d0bb8, kwargs kwargs entry 0x0 objects abstract.c 2246 116 0x00000000004b1e1d slot tp call self , args 0x7fffc00d0bb8, kwds 0x0 objects typeobject.c 6167 117 0x00000000004450b1 pyobject fastcalldict func func entry 0x7fffc00d0dc0, args args entry 0x307ad4b0, nargs nargs entry 1, kwargs kwargs entry 0x0 objects abstract.c 2316 118 0x00000000004456e5 pyobject fastcallkeywords func func entry 0x7fffc00d0dc0, stack 0x307ad4b0, nargs nargs entry 1, kwnames kwnames entry 0x0 objects abstract.c 2480 119 0x0000000000531011 call function pp stack pp stack entry 0x7fffffffc088, oparg oparg entry 1, kwnames kwnames entry 0x0 python ceval.c 4822 120 0x000000000053a5b6 pyeval evalframedefault f , throwflag python ceval.c 3284 121 0x0000000000530124 pyeval evalframeex f f entry 0x307ad298, throwflag throwflag entry 0 python ceval.c 718 122 0x00000000005301ec pyfunction fastcall co , args 0x307acf50, nargs nargs entry 2, globals python ceval.c 4880 123 0x0000000000530cf7 fast function func func entry 0x7fffedca1110, stack , nargs nargs entry 2, kwnames kwnames entry 0x0 python ceval.c 4915 124 0x0000000000530ffe call function pp stack pp stack entry 0x7fffffffc288, oparg oparg entry 1, kwnames kwnames entry 0x0 python ceval.c 4819 125 0x000000000053a5b6 pyeval evalframedefault f , throwflag python ceval.c 3284 126 0x0000000000530124 pyeval evalframeex f f entry 0x307acda8, throwflag throwflag entry 0 python ceval.c 718 127 0x00000000005301ec pyfunction fastcall co , args 0x307a77b8, nargs nargs entry 1, globals python ceval.c 4880 128 0x0000000000530cf7 fast function func func entry 0x7fffedca1a68, stack , nargs nargs entry 1, kwnames kwnames entry 0x0 python ceval.c 4915 129 0x0000000000530ffe call function pp stack pp stack entry 0x7fffffffc488, oparg oparg entry 0, kwnames kwnames entry 0x0 python ceval.c 4819 130 0x000000000053a5b6 pyeval evalframedefault f , throwflag python ceval.c 3284 131 0x0000000000530124 pyeval evalframeex f f entry 0x307a75b8, throwflag throwflag entry 0 python ceval.c 718 132 0x0000000000530b4b pyeval evalcodewithname co co entry 0x7fffedceb400, globals globals entry 0x7fffedd4c9b8, locals locals entry 0x0, args args entry 0x7fffffffc6c0, argcount argcount entry 1, kwnames kwnames entry 0x7fffc22e5e10, kwargs kwargs entry 0x7fffc22e5e18, kwcount 2, kwcount entry 1, kwstep kwstep entry 2, defs 0x7fffedca12a8, defcount 11, kwdefs 0x7fffedcefd78, closure 0x0, name 0x7ffff7fa3430, qualname 0x7fffedcef478 python ceval.c 4128 133 0x000000000053c29d pyfunction fastcalldict func func entry 0x7fffedca1338, args args entry 0x7fffffffc6c0, nargs nargs entry 1, kwargs kwargs entry 0x7fffc00d5238 python ceval.c 5031 134 0x0000000000445042 pyobject fastcalldict func func entry 0x7fffedca1338, args args entry 0x7fffffffc6c0, nargs nargs entry 1, kwargs kwargs entry 0x7fffc00d5238 objects abstract.c 2295 135 0x0000000000445345 pyobject call prepend func 0x7fffedca1338, obj 0x7fffc00d09b0, args 0x7ffff7fa0058, kwargs 0x7fffc00d5238 objects abstract.c 2358 136 0x000000000045ee6b method call method , args , kwargs objects classobject.c 317 137 0x0000000000444e8c pyobject call func func entry 0x7ffff064ce58, args args entry 0x7ffff7fa0058, kwargs kwargs entry 0x7fffc00d5238 objects abstract.c 2246 138 0x00000000004b1a5c slot tp init self , args 0x7ffff7fa0058, kwds 0x7fffc00d5238 objects typeobject.c 6380 139 0x00000000004ada4d type call type 0xb5e1c8, args 0x7ffff7fa0058, kwds 0x7fffc00d5238 objects typeobject.c 915 140 0x00000000004450b1 pyobject fastcalldict func func entry 0xb5e1c8, args args entry 0x3079d738, nargs nargs entry 0, kwargs kwargs entry 0x7fffc00d5238 objects abstract.c 2316 type continue, q quit 141 0x00000000004456e5 pyobject fastcallkeywords func func entry 0xb5e1c8, stack 0x3079d738, nargs nargs entry 0, kwnames kwnames entry 0x7fffc1f43260 objects abstract.c 2480 142 0x0000000000531011 call function pp stack pp stack entry 0x7fffffffc900, oparg , kwnames kwnames entry 0x7fffc1f43260 python ceval.c 4822 143 0x000000000053a6d4 pyeval evalframedefault f , throwflag python ceval.c 3300 144 0x0000000000530124 pyeval evalframeex f f entry 0x3079d588, throwflag throwflag entry 0 python ceval.c 718 145 0x00000000005301ec pyfunction fastcall co , args 0xa1a588, nargs nargs entry 0, globals python ceval.c 4880 146 0x0000000000530cf7 fast function func func entry 0x7fffc1d9e3f0, stack , nargs nargs entry 0, kwnames kwnames entry 0x0 python ceval.c 4915 147 0x0000000000530ffe call function pp stack pp stack entry 0x7fffffffcaf8, oparg oparg entry 0, kwnames kwnames entry 0x0 python ceval.c 4819 148 0x000000000053a5b6 pyeval evalframedefault f , throwflag python ceval.c 3284 149 0x0000000000530124 pyeval evalframeex f f entry 0xa1a3f8, throwflag throwflag entry 0 python ceval.c 718 150 0x0000000000530b4b pyeval evalcodewithname co co entry 0x7ffff0a6c4c0, globals globals entry 0x7ffff7eaac88, locals locals entry 0x7ffff7eaac88, args args entry 0x0, argcount argcount entry 0, kwnames kwnames entry 0x0, kwargs kwargs entry 0x8, kwcount kwcount entry 0, kwstep kwstep entry 2, defs defs entry 0x0, defcount defcount entry 0, kwdefs kwdefs entry 0x0, closure closure entry 0x0, name name entry 0x0, qualname qualname entry 0x0 python ceval.c 4128 151 0x0000000000531168 pyeval evalcodeex co co entry 0x7ffff0a6c4c0, globals globals entry 0x7ffff7eaac88, locals locals entry 0x7ffff7eaac88, args args entry 0x0, argcount argcount entry 0, kws kws entry 0x0, kwcount kwcount entry 0, defs defs entry 0x0, defcount defcount entry 0, kwdefs kwdefs entry 0x0, closure closure entry 0x0 python ceval.c 4149 152 0x00000000005311b2 pyeval evalcode co co entry 0x7ffff0a6c4c0, globals globals entry 0x7ffff7eaac88, locals locals entry 0x7ffff7eaac88 python ceval.c 695 153 0x00000000004231c4 run mod mod mod entry 0xab1fe0, filename filename entry 0x7ffff0b5f660, globals globals entry 0x7ffff7eaac88, locals locals entry 0x7ffff7eaac88, flags flags entry 0x7fffffffcea0, arena arena entry 0x7ffff0b94520 python pythonrun.c 980 154 0x0000000000425a44 pyrun fileexflags fp fp entry 0x9501a0, filename str filename str entry 0x7ffff7efaeb0 test test nn.py , start start entry 257, globals globals entry 0x7ffff7eaac88, locals locals entry 0x7ffff7eaac88, closeit closeit entry 1, flags flags entry 0x7fffffffcea0 python pythonrun.c 933 155 0x0000000000425ddb pyrun simplefileexflags fp fp entry 0x9501a0, filename , filename entry 0x7ffff7efaeb0 test test nn.py , closeit closeit entry 1, flags flags entry 0x7fffffffcea0 python pythonrun.c 396 156 0x0000000000425f37 pyrun anyfileexflags fp fp entry 0x9501a0, filename 0x7ffff7efaeb0 test test nn.py , closeit closeit entry 1, flags flags entry 0x7fffffffcea0 python pythonrun.c 80 157 0x00000000004349d9 run file fp fp entry 0x9501a0, filename filename entry 0x917300 l test test nn.py , p cf p cf entry 0x7fffffffcea0 modules main.c 338 158 0x0000000000435583 py main argc argc entry 3, argv argv entry 0x916010 modules main.c 809 159 0x000000000041d21a main argc 3, argv 0x7fffffffd018 . programs python.c 69 tests test run test.sh pass, although run magma tests installed outside conda environment.",0,modules gcmodule.c 380 visit decref assertion gc gc.gc refs 1 ! 0' failed.,"modules gcmodule.c 380 visit decref assertion gc gc.gc refs 1 ! 0' failed. continued https github.com pytorch pytorch issues 1624 running tests debug build python3.6.1. following tests testnn failed gc problems test variable sequence cuda test cuda rnn fused test rnn initial hidden state test rnn cpu vs cudnn dropout test rnn cpu vs cudnn dropout test rnn dropout state test rnn change dropout error gc gc.gc refs 1 ! 0' failed. 0 0x00007ffff712c1d7 raise lib64 libc.so.6 1 0x00007ffff712d8c8 abort lib64 libc.so.6 2 0x00007ffff7125146 assert fail base lib64 libc.so.6 3 0x00007ffff71251f2 lib64 libc.so.6 4 0x0000000000435f63 visit decref op 0x7fffc006e4f8, data modules gcmodule.c 380 5 0x00007fffecd7c20d thpvariable traverse thpvariable , visitproc, void self self entry 0x7fffc00781f0, visit visit entry 0x435efd , arg arg entry 0x0 torch csrc autograd python variable.cpp 92 6 0x00000000004a86ce subtype traverse self 0x7fffc00781f0, visit 0x435efd , arg 0x0 objects typeobject.c 1021 7 0x00000000004356ee subtract refs containers containers entry 0x885d20 modules gcmodule.c 399 8 0x0000000000436809 collect generation generation entry 0, n collected n collected entry 0x7fffffff96b8, n uncollectable n uncollectable entry 0x7fffffff96b0, nofail nofail entry 0 modules gcmodule.c 956 9 0x0000000000436c47 collect callback generation 0 modules gcmodule.c 1128 10 0x0000000000436d3f collect generations modules gcmodule.c 1151 11 0x0000000000436eac pyobject gc alloc use calloc use calloc entry 0, basicsize basicsize entry 72 modules gcmodule.c 1726 12 0x0000000000437382 pyobject gc malloc basicsize basicsize entry 72 modules gcmodule.c 1736 13 0x00000000004a8443 pytype genericalloc type 0x7fffed9eb400 , nitems 2 objects typeobject.c 936 14 0x00007fffecd3083f thpsize new int, long dim 2, sizes 0x4547f5c0 torch csrc size.cpp 16 15 0x00007fffed1c2102 thcpdoubletensor size object , object , object self , args 0x7ffff7fa0058, kwargs 0x0 data users gchanan pytorch6 torch csrc generic tensormethods.cpp 650 16 0x00000000004950a9 pycfunction fastcalldict func obj func obj entry 0x7fffc28df328, args args entry 0x4547e598, nargs nargs entry 0, kwargs kwargs entry 0x0 objects methodobject.c 231 17 0x000000000049545a pycfunction fastcallkeywords func func entry 0x7fffc28df328, stack stack entry 0x4547e598, nargs nargs entry 0, kwnames kwnames entry 0x0 objects methodobject.c 295 18 0x0000000000530f41 call function pp stack pp stack entry 0x7fffffff9908, oparg oparg entry 0, kwnames kwnames entry 0x0 python ceval.c 4798 19 0x000000000053a5b6 pyeval evalframedefault f , throwflag python ceval.c 3284 20 0x0000000000530124 pyeval evalframeex f f entry 0x4547e3e8, throwflag throwflag entry 0 python ceval.c 718 21 0x0000000000530b4b pyeval evalcodewithname co 0x7fffc2406e80, globals , locals locals entry 0x0, args args entry 0x7fffc008c480, argcount 4, kwnames kwnames entry 0x0, kwargs kwargs entry 0x8, kwcount kwcount entry 0, kwstep kwstep entry 2, defs defs entry 0x0, defcount defcount entry 0, kwdefs kwdefs entry 0x0, closure closure entry 0x0, name name entry 0x0, qualname qualname entry 0x0 python ceval.c 4128 22 0x0000000000531168 pyeval evalcodeex co , globals , locals locals entry 0x0, args args entry 0x7fffc008c480, argcount , kws kws entry 0x0, kwcount kwcount entry 0, defs defs entry 0x0, defcount defcount entry 0, kwdefs 0x0, closure 0x0 python ceval.c 4149 23 0x000000000047386f function call func 0x7fffc23a6840, arg 0x7fffc008c458, kw 0x0 objects funcobject.c 604 24 0x0000000000444e8c pyobject call func func entry 0x7fffc23a6840, args args entry 0x7fffc008c458, kwargs kwargs entry 0x0 objects abstract.c 2246 25 0x00000000005316b0 pyeval callobjectwithkeywords func 0x7fffc23a6840, args 0x7fffc008c458, kwargs kwargs entry 0x0 python ceval.c 4718 26 0x0000000000444c69 pyobject callobject , objects abstract.c 2172 27 0x00007fffecd74eb3 thpfunction apply object , object cls 0x11c9178, inputs 0x7fffc1da3418 torch csrc autograd python function.cpp 722 28 0x00000000004950b8 pycfunction fastcalldict func obj func obj entry 0x7fffc28d61c0, args args entry 0x4547bf70, nargs nargs entry 3, kwargs kwargs entry 0x0 objects methodobject.c 234 29 0x000000000049545a pycfunction fastcallkeywords func func entry 0x7fffc28d61c0, stack stack entry 0x4547bf70, nargs nargs entry 3, kwnames kwnames entry 0x0 objects methodobject.c 295 30 0x0000000000530f41 call function pp stack pp stack entry 0x7fffffff9e58, oparg oparg entry 3, kwnames kwnames entry 0x0 python ceval.c 4798 31 0x000000000053a5b6 pyeval evalframedefault f , throwflag python ceval.c 3284 32 0x0000000000530124 pyeval evalframeex f f entry 0x4547bdc8, throwflag throwflag entry 0 python ceval.c 718 33 0x00000000005301ec pyfunction fastcall co co entry 0x7fffc31e2640, args 0x7fffffffa038, args entry 0x7fffffffa020, nargs nargs entry 3, globals globals entry 0x7fffc31e1e68 python ceval.c 4880 type continue, q quit 34 0x000000000053c110 pyfunction fastcalldict func func entry 0x7fffc23f61c8, args args entry 0x7fffffffa020, nargs nargs entry 3, kwargs kwargs entry 0x0 python ceval.c 4982 35 0x0000000000445042 pyobject fastcalldict func func entry 0x7fffc23f61c8, args args entry 0x7fffffffa020, nargs nargs entry 3, kwargs kwargs entry 0x0 objects abstract.c 2295 36 0x0000000000445345 pyobject call prepend func 0x7fffc23f61c8, obj 0x7fffc0085e28, args 0x7fffc22eb988, kwargs 0x0 objects abstract.c 2358 37 0x000000000045ee6b method call method , args , kwargs objects classobject.c 317 38 0x0000000000444e8c pyobject call func func entry 0x7fffc23963d8, args args entry 0x7fffc22eb988, kwargs kwargs entry 0x0 objects abstract.c 2246 39 0x00000000004af68e call method , nameid nameid entry 0x8a5740 , format format entry 0x635a54 oo objects typeobject.c 1453 40 0x00000000004af814 slot mp ass subscript self , key , value objects typeobject.c 5944 41 0x00000000004444f0 pyobject setitem entry 0x7fffc0085e28, key key entry 0x7fffc22eb838, value value entry 0x7fffc0089b80 objects abstract.c 180 42 0x00000000005345cc pyeval evalframedefault f , throwflag python ceval.c 1727 43 0x0000000000530124 pyeval evalframeex f f entry 0x4547ed58, throwflag throwflag entry 0 python ceval.c 718 44 0x0000000000530b4b pyeval evalcodewithname co 0x7fffc1f6b700, globals , locals locals entry 0x0, args , argcount 1, kwnames 0x0, kwargs 0x307b1470, kwcount 0, kwstep kwstep entry 1, defs 0x7fffc1f69d18, defcount defcount entry 1, kwdefs kwdefs entry 0x0, closure closure entry 0x0, name name entry 0x7fffefef55e0, qualname qualname entry 0x7fffefef55e0 python ceval.c 4128 45 0x0000000000530da7 fast function func func entry 0x7fffc1f8a110, stack , nargs nargs entry 1, kwnames kwnames entry 0x0 python ceval.c 4939 46 0x0000000000530ffe call function pp stack pp stack entry 0x7fffffffa4e8, oparg oparg entry 1, kwnames kwnames entry 0x0 python ceval.c 4819 47 0x000000000053a5b6 pyeval evalframedefault f , throwflag python ceval.c 3284 48 0x0000000000530124 pyeval evalframeex f f entry 0x307b1208, throwflag throwflag entry 0 python ceval.c 718 49 0x00000000005301ec pyfunction fastcall co , args 0x7fffc231ff18, nargs nargs entry 2, globals python ceval.c 4880 50 0x0000000000530cf7 fast function func func entry 0x7fffc1dc39b0, stack , nargs nargs entry 2, kwnames kwnames entry 0x0 python ceval.c 4915 51 0x0000000000530ffe call function pp stack pp stack entry 0x7fffffffa6e8, oparg oparg entry 1, kwnames kwnames entry 0x0 python ceval.c 4819 52 0x000000000053a5b6 pyeval evalframedefault f , throwflag python ceval.c 3284 53 0x0000000000530124 pyeval evalframeex f f entry 0x7fffc231fd78, throwflag throwflag entry 0 python ceval.c 718 54 0x00000000005301ec pyfunction fastcall co , args 0x307b0898, nargs nargs entry 1, globals python ceval.c 4880 55 0x0000000000530cf7 fast function func func entry 0x7fffc1dc3b20, stack , nargs nargs entry 1, kwnames kwnames entry 0x0 python ceval.c 4915 56 0x0000000000530ffe call function pp stack pp stack entry 0x7fffffffa8e8, oparg oparg entry 0, kwnames kwnames entry 0x0 python ceval.c 4819 57 0x000000000053a5b6 pyeval evalframedefault f , throwflag python ceval.c 3284 58 0x0000000000530124 pyeval evalframeex f f entry 0x307b0698, throwflag throwflag entry 0 python ceval.c 718 59 0x0000000000530b4b pyeval evalcodewithname co co entry 0x7fffede08340, globals globals entry 0x7fffede77670, locals locals entry 0x0, args args entry 0x7fffffffab20, argcount argcount entry 2, kwnames kwnames entry 0x7ffff7fa0080, kwargs kwargs entry 0x7ffff7fa0088, kwcount kwcount entry 0, kwstep kwstep entry 2, defs 0x7fffede0ff88, defcount 1, kwdefs 0x0, closure 0x0, name 0x7fffede06860, qualname 0x7fffede054a0 python ceval.c 4128 60 0x000000000053c29d pyfunction fastcalldict func func entry 0x7fffedd4f280, args args entry 0x7fffffffab20, nargs nargs entry 2, kwargs kwargs entry 0x7fffc00e09b8 python ceval.c 5031 61 0x0000000000445042 pyobject fastcalldict func func entry 0x7fffedd4f280, args args entry 0x7fffffffab20, nargs nargs entry 2, kwargs kwargs entry 0x7fffc00e09b8 objects abstract.c 2295 62 0x0000000000445345 pyobject call prepend func 0x7fffedd4f280, obj 0x7fffc00d0e28, args 0x7fffc00e1ae8, kwargs 0x7fffc00e09b8 objects abstract.c 2358 63 0x000000000045ee6b method call method , args , kwargs objects classobject.c 317 64 0x0000000000444e8c pyobject call func 0x7fffeff04838, args 0x7fffc00e1ae8, kwargs 0x7fffc00e09b8 objects abstract.c 2246 65 0x000000000052f6b9 call core func func entry 0x7fffeff04838, callargs callargs entry 0x7fffc00e1ae8, kwdict kwdict entry 0x7fffc00e09b8 python ceval.c 5067 66 0x000000000053aad1 pyeval evalframedefault f , throwflag python ceval.c 3366 67 0x0000000000530124 pyeval evalframeex f f entry 0x7fffc0064250, throwflag throwflag entry 0 python ceval.c 718 68 0x0000000000530b4b pyeval evalcodewithname co co entry 0x7fffede084c0, globals globals entry 0x7fffede77670, locals locals entry 0x0, args args entry 0x7fffffffaec0, argcount argcount entry 2, kwnames kwnames entry 0x0, kwargs kwargs entry 0x8, kwcount kwcount entry 0, kwstep kwstep entry 2, defs 0x0, defcount 0, kwdefs 0x0, closure 0x0, name 0x7ffff7fa3270, qualname 0x7fffede04ce8 python ceval.c 4128 type continue, q quit 69 0x000000000053c29d pyfunction fastcalldict func func entry 0x7fffedd4f3f0, args args entry 0x7fffffffaec0, nargs nargs entry 2, kwargs kwargs entry 0x0 python ceval.c 5031 70 0x0000000000445042 pyobject fastcalldict func func entry 0x7fffedd4f3f0, args args entry 0x7fffffffaec0, nargs nargs entry 2, kwargs kwargs entry 0x0 objects abstract.c 2295 71 0x0000000000445345 pyobject call prepend func 0x7fffedd4f3f0, obj 0x7fffc00d0e28, args 0x7fffc00e1b50, kwargs 0x0 objects abstract.c 2358 72 0x000000000045ee6b method call method , args , kwargs objects classobject.c 317 73 0x0000000000444e8c pyobject call func func entry 0x7fffede3c6e8, args args entry 0x7fffc00e1b50, kwargs kwargs entry 0x0 objects abstract.c 2246 74 0x00000000004b1e1d slot tp call self , args 0x7fffc00e1b50, kwds 0x0 objects typeobject.c 6167 75 0x00000000004450b1 pyobject fastcalldict func func entry 0x7fffc00d0e28, args args entry 0x307af060, nargs nargs entry 1, kwargs kwargs entry 0x0 objects abstract.c 2316 76 0x00000000004456e5 pyobject fastcallkeywords func func entry 0x7fffc00d0e28, stack 0x307af060, nargs nargs entry 1, kwnames kwnames entry 0x0 objects abstract.c 2480 77 0x0000000000531011 call function pp stack pp stack entry 0x7fffffffb0c8, oparg oparg entry 1, kwnames kwnames entry 0x0 python ceval.c 4822 78 0x000000000053a5b6 pyeval evalframedefault f , throwflag python ceval.c 3284 79 0x0000000000530124 pyeval evalframeex f f entry 0x307aee98, throwflag throwflag entry 0 python ceval.c 718 80 0x0000000000530b4b pyeval evalcodewithname co co entry 0x7fffede1e880, globals globals entry 0x7fffedeaad78, locals locals entry 0x0, args args entry 0x7fffffffb300, argcount argcount entry 2, kwnames kwnames entry 0x7ffff7fa0080, kwargs kwargs entry 0x7ffff7fa0088, kwcount kwcount entry 0, kwstep kwstep entry 2, defs 0x7fffedd4d5c8, defcount 1, kwdefs 0x0, closure 0x0, name 0x7fffede06860, qualname 0x7fffedd50970 python ceval.c 4128 81 0x000000000053c29d pyfunction fastcalldict func func entry 0x7fffedd54eb8, args args entry 0x7fffffffb300, nargs nargs entry 2, kwargs kwargs entry 0x7fffc00e0a30 python ceval.c 5031 82 0x0000000000445042 pyobject fastcalldict func func entry 0x7fffedd54eb8, args args entry 0x7fffffffb300, nargs nargs entry 2, kwargs kwargs entry 0x7fffc00e0a30 objects abstract.c 2295 83 0x0000000000445345 pyobject call prepend func 0x7fffedd54eb8, obj 0x7fffc00e1400, args 0x7fffc00e1cf0, kwargs 0x7fffc00e0a30 objects abstract.c 2358 84 0x000000000045ee6b method call method , args , kwargs objects classobject.c 317 85 0x0000000000444e8c pyobject call func 0x7ffff064cf38, args 0x7fffc00e1cf0, kwargs 0x7fffc00e0a30 objects abstract.c 2246 86 0x000000000052f6b9 call core func func entry 0x7ffff064cf38, callargs callargs entry 0x7fffc00e1cf0, kwdict kwdict entry 0x7fffc00e0a30 python ceval.c 5067 87 0x000000000053aad1 pyeval evalframedefault f , throwflag python ceval.c 3366 88 0x0000000000530124 pyeval evalframeex f f entry 0x7fffc00d9e20, throwflag throwflag entry 0 python ceval.c 718 89 0x0000000000530b4b pyeval evalcodewithname co co entry 0x7fffede14a00, globals globals entry 0x7fffedeaad78, locals locals entry 0x0, args args entry 0x7fffffffb6a0, argcount argcount entry 2, kwnames kwnames entry 0x0, kwargs kwargs entry 0x8, kwcount kwcount entry 0, kwstep kwstep entry 2, defs 0x0, defcount 0, kwdefs 0x0, closure 0x0, name 0x7ffff7fa3270, qualname 0x7fffedd55838 python ceval.c 4128 90 0x000000000053c29d pyfunction fastcalldict func func entry 0x7fffedd54d48, args args entry 0x7fffffffb6a0, nargs nargs entry 2, kwargs kwargs entry 0x0 python ceval.c 5031 91 0x0000000000445042 pyobject fastcalldict func func entry 0x7fffedd54d48, args args entry 0x7fffffffb6a0, nargs nargs entry 2, kwargs kwargs entry 0x0 objects abstract.c 2295 92 0x0000000000445345 pyobject call prepend func 0x7fffedd54d48, obj 0x7fffc00e1400, args 0x7fffc00d0e90, kwargs 0x0 objects abstract.c 2358 93 0x000000000045ee6b method call method , args , kwargs objects classobject.c 317 94 0x0000000000444e8c pyobject call func func entry 0x7fffedee0288, args args entry 0x7fffc00d0e90, kwargs kwargs entry 0x0 objects abstract.c 2246 95 0x00000000004b1e1d slot tp call self , args 0x7fffc00d0e90, kwds 0x0 objects typeobject.c 6167 96 0x00000000004450b1 pyobject fastcalldict func func entry 0x7fffc00e1400, args args entry 0x307ae830, nargs nargs entry 1, kwargs kwargs entry 0x0 objects abstract.c 2316 97 0x00000000004456e5 pyobject fastcallkeywords func func entry 0x7fffc00e1400, stack 0x307ae830, nargs nargs entry 1, kwnames kwnames entry 0x0 objects abstract.c 2480 98 0x0000000000531011 call function pp stack pp stack entry 0x7fffffffb8a8, oparg oparg entry 1, kwnames kwnames entry 0x0 python ceval.c 4822 99 0x000000000053a5b6 pyeval evalframedefault f , throwflag python ceval.c 3284 100 0x0000000000530124 pyeval evalframeex f f entry 0x307ae668, throwflag throwflag entry 0 python ceval.c 718 101 0x0000000000530b4b pyeval evalcodewithname co co entry 0x7fffede1e880, globals globals entry 0x7fffedeaad78, locals locals entry 0x0, args args entry 0x7fffffffbae0, argcount argcount entry 2, kwnames kwnames entry 0x7ffff7fa0080, kwargs kwargs entry 0x7ffff7fa0088, kwcount kwcount entry 0, kwstep kwstep entry 2, defs 0x7fffedd4d5c8, defcount 1, kwdefs 0x0, closure 0x0, name 0x7fffede06860, qualname 0x7fffedd50970 python ceval.c 4128 102 0x000000000053c29d pyfunction fastcalldict func func entry 0x7fffedd54eb8, args args entry 0x7fffffffbae0, nargs nargs entry 2, kwargs kwargs entry 0x7fffc00e0058 python ceval.c 5031 103 0x0000000000445042 pyobject fastcalldict func func entry 0x7fffedd54eb8, args args entry 0x7fffffffbae0, nargs nargs entry 2, kwargs kwargs entry 0x7fffc00e0058 objects abstract.c 2295 type continue, q quit 104 0x0000000000445345 pyobject call prepend func 0x7fffedd54eb8, obj 0x7fffc00d0dc0, args 0x7fffc00d0ae8, kwargs 0x7fffc00e0058 objects abstract.c 2358 105 0x000000000045ee6b method call method , args , kwargs objects classobject.c 317 106 0x0000000000444e8c pyobject call func 0x7fffede3c7c8, args 0x7fffc00d0ae8, kwargs 0x7fffc00e0058 objects abstract.c 2246 107 0x000000000052f6b9 call core func func entry 0x7fffede3c7c8, callargs callargs entry 0x7fffc00d0ae8, kwdict kwdict entry 0x7fffc00e0058 python ceval.c 5067 108 0x000000000053aad1 pyeval evalframedefault f , throwflag python ceval.c 3366 109 0x0000000000530124 pyeval evalframeex f f entry 0x7fffc00d9c28, throwflag throwflag entry 0 python ceval.c 718 110 0x0000000000530b4b pyeval evalcodewithname co co entry 0x7fffede14a00, globals globals entry 0x7fffedeaad78, locals locals entry 0x0, args args entry 0x7fffffffbe80, argcount argcount entry 2, kwnames kwnames entry 0x0, kwargs kwargs entry 0x8, kwcount kwcount entry 0, kwstep kwstep entry 2, defs 0x0, defcount 0, kwdefs 0x0, closure 0x0, name 0x7ffff7fa3270, qualname 0x7fffedd55838 python ceval.c 4128 111 0x000000000053c29d pyfunction fastcalldict func func entry 0x7fffedd54d48, args args entry 0x7fffffffbe80, nargs nargs entry 2, kwargs kwargs entry 0x0 python ceval.c 5031 112 0x0000000000445042 pyobject fastcalldict func func entry 0x7fffedd54d48, args args entry 0x7fffffffbe80, nargs nargs entry 2, kwargs kwargs entry 0x0 objects abstract.c 2295 113 0x0000000000445345 pyobject call prepend func 0x7fffedd54d48, obj 0x7fffc00d0dc0, args 0x7fffc00d0bb8, kwargs 0x0 objects abstract.c 2358 114 0x000000000045ee6b method call method , args , kwargs objects classobject.c 317 115 0x0000000000444e8c pyobject call func func entry 0x7ffff0655e58, args args entry 0x7fffc00d0bb8, kwargs kwargs entry 0x0 objects abstract.c 2246 116 0x00000000004b1e1d slot tp call self , args 0x7fffc00d0bb8, kwds 0x0 objects typeobject.c 6167 117 0x00000000004450b1 pyobject fastcalldict func func entry 0x7fffc00d0dc0, args args entry 0x307ad4b0, nargs nargs entry 1, kwargs kwargs entry 0x0 objects abstract.c 2316 118 0x00000000004456e5 pyobject fastcallkeywords func func entry 0x7fffc00d0dc0, stack 0x307ad4b0, nargs nargs entry 1, kwnames kwnames entry 0x0 objects abstract.c 2480 119 0x0000000000531011 call function pp stack pp stack entry 0x7fffffffc088, oparg oparg entry 1, kwnames kwnames entry 0x0 python ceval.c 4822 120 0x000000000053a5b6 pyeval evalframedefault f , throwflag python ceval.c 3284 121 0x0000000000530124 pyeval evalframeex f f entry 0x307ad298, throwflag throwflag entry 0 python ceval.c 718 122 0x00000000005301ec pyfunction fastcall co , args 0x307acf50, nargs nargs entry 2, globals python ceval.c 4880 123 0x0000000000530cf7 fast function func func entry 0x7fffedca1110, stack , nargs nargs entry 2, kwnames kwnames entry 0x0 python ceval.c 4915 124 0x0000000000530ffe call function pp stack pp stack entry 0x7fffffffc288, oparg oparg entry 1, kwnames kwnames entry 0x0 python ceval.c 4819 125 0x000000000053a5b6 pyeval evalframedefault f , throwflag python ceval.c 3284 126 0x0000000000530124 pyeval evalframeex f f entry 0x307acda8, throwflag throwflag entry 0 python ceval.c 718 127 0x00000000005301ec pyfunction fastcall co , args 0x307a77b8, nargs nargs entry 1, globals python ceval.c 4880 128 0x0000000000530cf7 fast function func func entry 0x7fffedca1a68, stack , nargs nargs entry 1, kwnames kwnames entry 0x0 python ceval.c 4915 129 0x0000000000530ffe call function pp stack pp stack entry 0x7fffffffc488, oparg oparg entry 0, kwnames kwnames entry 0x0 python ceval.c 4819 130 0x000000000053a5b6 pyeval evalframedefault f , throwflag python ceval.c 3284 131 0x0000000000530124 pyeval evalframeex f f entry 0x307a75b8, throwflag throwflag entry 0 python ceval.c 718 132 0x0000000000530b4b pyeval evalcodewithname co co entry 0x7fffedceb400, globals globals entry 0x7fffedd4c9b8, locals locals entry 0x0, args args entry 0x7fffffffc6c0, argcount argcount entry 1, kwnames kwnames entry 0x7fffc22e5e10, kwargs kwargs entry 0x7fffc22e5e18, kwcount 2, kwcount entry 1, kwstep kwstep entry 2, defs 0x7fffedca12a8, defcount 11, kwdefs 0x7fffedcefd78, closure 0x0, name 0x7ffff7fa3430, qualname 0x7fffedcef478 python ceval.c 4128 133 0x000000000053c29d pyfunction fastcalldict func func entry 0x7fffedca1338, args args entry 0x7fffffffc6c0, nargs nargs entry 1, kwargs kwargs entry 0x7fffc00d5238 python ceval.c 5031 134 0x0000000000445042 pyobject fastcalldict func func entry 0x7fffedca1338, args args entry 0x7fffffffc6c0, nargs nargs entry 1, kwargs kwargs entry 0x7fffc00d5238 objects abstract.c 2295 135 0x0000000000445345 pyobject call prepend func 0x7fffedca1338, obj 0x7fffc00d09b0, args 0x7ffff7fa0058, kwargs 0x7fffc00d5238 objects abstract.c 2358 136 0x000000000045ee6b method call method , args , kwargs objects classobject.c 317 137 0x0000000000444e8c pyobject call func func entry 0x7ffff064ce58, args args entry 0x7ffff7fa0058, kwargs kwargs entry 0x7fffc00d5238 objects abstract.c 2246 138 0x00000000004b1a5c slot tp init self , args 0x7ffff7fa0058, kwds 0x7fffc00d5238 objects typeobject.c 6380 139 0x00000000004ada4d type call type 0xb5e1c8, args 0x7ffff7fa0058, kwds 0x7fffc00d5238 objects typeobject.c 915 140 0x00000000004450b1 pyobject fastcalldict func func entry 0xb5e1c8, args args entry 0x3079d738, nargs nargs entry 0, kwargs kwargs entry 0x7fffc00d5238 objects abstract.c 2316 type continue, q quit 141 0x00000000004456e5 pyobject fastcallkeywords func func entry 0xb5e1c8, stack 0x3079d738, nargs nargs entry 0, kwnames kwnames entry 0x7fffc1f43260 objects abstract.c 2480 142 0x0000000000531011 call function pp stack pp stack entry 0x7fffffffc900, oparg , kwnames kwnames entry 0x7fffc1f43260 python ceval.c 4822 143 0x000000000053a6d4 pyeval evalframedefault f , throwflag python ceval.c 3300 144 0x0000000000530124 pyeval evalframeex f f entry 0x3079d588, throwflag throwflag entry 0 python ceval.c 718 145 0x00000000005301ec pyfunction fastcall co , args 0xa1a588, nargs nargs entry 0, globals python ceval.c 4880 146 0x0000000000530cf7 fast function func func entry 0x7fffc1d9e3f0, stack , nargs nargs entry 0, kwnames kwnames entry 0x0 python ceval.c 4915 147 0x0000000000530ffe call function pp stack pp stack entry 0x7fffffffcaf8, oparg oparg entry 0, kwnames kwnames entry 0x0 python ceval.c 4819 148 0x000000000053a5b6 pyeval evalframedefault f , throwflag python ceval.c 3284 149 0x0000000000530124 pyeval evalframeex f f entry 0xa1a3f8, throwflag throwflag entry 0 python ceval.c 718 150 0x0000000000530b4b pyeval evalcodewithname co co entry 0x7ffff0a6c4c0, globals globals entry 0x7ffff7eaac88, locals locals entry 0x7ffff7eaac88, args args entry 0x0, argcount argcount entry 0, kwnames kwnames entry 0x0, kwargs kwargs entry 0x8, kwcount kwcount entry 0, kwstep kwstep entry 2, defs defs entry 0x0, defcount defcount entry 0, kwdefs kwdefs entry 0x0, closure closure entry 0x0, name name entry 0x0, qualname qualname entry 0x0 python ceval.c 4128 151 0x0000000000531168 pyeval evalcodeex co co entry 0x7ffff0a6c4c0, globals globals entry 0x7ffff7eaac88, locals locals entry 0x7ffff7eaac88, args args entry 0x0, argcount argcount entry 0, kws kws entry 0x0, kwcount kwcount entry 0, defs defs entry 0x0, defcount defcount entry 0, kwdefs kwdefs entry 0x0, closure closure entry 0x0 python ceval.c 4149 152 0x00000000005311b2 pyeval evalcode co co entry 0x7ffff0a6c4c0, globals globals entry 0x7ffff7eaac88, locals locals entry 0x7ffff7eaac88 python ceval.c 695 153 0x00000000004231c4 run mod mod mod entry 0xab1fe0, filename filename entry 0x7ffff0b5f660, globals globals entry 0x7ffff7eaac88, locals locals entry 0x7ffff7eaac88, flags flags entry 0x7fffffffcea0, arena arena entry 0x7ffff0b94520 python pythonrun.c 980 154 0x0000000000425a44 pyrun fileexflags fp fp entry 0x9501a0, filename str filename str entry 0x7ffff7efaeb0 test test nn.py , start start entry 257, globals globals entry 0x7ffff7eaac88, locals locals entry 0x7ffff7eaac88, closeit closeit entry 1, flags flags entry 0x7fffffffcea0 python pythonrun.c 933 155 0x0000000000425ddb pyrun simplefileexflags fp fp entry 0x9501a0, filename , filename entry 0x7ffff7efaeb0 test test nn.py , closeit closeit entry 1, flags flags entry 0x7fffffffcea0 python pythonrun.c 396 156 0x0000000000425f37 pyrun anyfileexflags fp fp entry 0x9501a0, filename 0x7ffff7efaeb0 test test nn.py , closeit closeit entry 1, flags flags entry 0x7fffffffcea0 python pythonrun.c 80 157 0x00000000004349d9 run file fp fp entry 0x9501a0, filename filename entry 0x917300 l test test nn.py , p cf p cf entry 0x7fffffffcea0 modules main.c 338 158 0x0000000000435583 py main argc argc entry 3, argv argv entry 0x916010 modules main.c 809 159 0x000000000041d21a main argc 3, argv 0x7fffffffd018 . programs python.c 69 tests test run test.sh pass, although run magma tests installed outside conda environment."
pytorch,17560,"another powerful general purpose optimizer called adabound trains fast adam good sgd, see https github.com luolc adabound https github.com luolc adabound . appears work better adam several tasks including nlp cv, slightly higher accuracy smoothier learning curve. ! image https user images.githubusercontent.com 10172392 53534724 08e75b80 3b3b 11e9 87b7 debe21b3908e.png think would great useful addition, thanks!",0,please test add adabound optimizer next stable release,"please test add adabound optimizer next stable release another powerful general purpose optimizer called adabound trains fast adam good sgd, see https github.com luolc adabound https github.com luolc adabound . appears work better adam several tasks including nlp cv, slightly higher accuracy smoothier learning curve. ! image https user images.githubusercontent.com 10172392 53534724 08e75b80 3b3b 11e9 87b7 debe21b3908e.png think would great useful addition, thanks!"
pytorch,6932,error coming implementing pytorch faster rcnn repository. solutions please??,0,exception nameerror global name 'filenotfounderror' defined,exception nameerror global name 'filenotfounderror' defined error coming implementing pytorch faster rcnn repository. solutions please??
pytorch,23366,"lookahead optimizer https arxiv.org pdf 1907.08610.pdf paper show new optimizer save time, improve accuracy sgd work cv nlp. think may worth try.",0,pytorch feature request lookahead optimizer,"pytorch feature request lookahead optimizer lookahead optimizer https arxiv.org pdf 1907.08610.pdf paper show new optimizer save time, improve accuracy sgd work cv nlp. think may worth try."
pytorch,24826,transformer implementation docs https pytorch.org docs stable nn.html?highlight transformer torch.nn.transformer state implement original paper fail acknowledge implement following layer norm default normalization option. positional encodings embeddings encoding decoding step fine implemented directly module making clear original paper would helpful. cc ezyang gchanan zou3519 bdhirsh jbschlosser anjali411 brianjo mruberry alband zhangguanheng66 jlin27,0,transformer lack embedding layer positional encodings,transformer lack embedding layer positional encodings transformer implementation docs https pytorch.org docs stable nn.html?highlight transformer torch.nn.transformer state implement original paper fail acknowledge implement following layer norm default normalization option. positional encodings embeddings encoding decoding step fine implemented directly module making clear original paper would helpful. cc ezyang gchanan zou3519 bdhirsh jbschlosser anjali411 brianjo mruberry alband zhangguanheng66 jlin27
pytorch,26218,"feature use 1e 8 default use 16 bit, round zero, thus get stability gains. cc vincentqb",0,default adam epsilon 1e 7 fp16,"default adam epsilon 1e 7 fp16 feature use 1e 8 default use 16 bit, round zero, thus get stability gains. cc vincentqb"
pytorch,13929,"feature taking current look jit optimization framework couple feature missing. structure captures post pass analysis optimization pass. analysis fundamental optimization allow us understand necessary passes better. example pytorch simple analysis per pass calculated number nodes computational graph changed could introduce features fixed point optimization. could mature even immutable passes analysis passes outline order apply passes maximize performance. currently trivial implementation optimization simply runs passes linear order https github.com pytorch pytorch blob master torch csrc jit graph executor.cpp l458 . another feature pass managers. pytorch implicitly defining families optimization passes e.g. peepholeoptimizeimpl implements family peephole optimization . explicit oo implementation pass managers substantially clean code. simplifying implementation passes increase agility. there's lot code reuse happening respect iterating graph checking pattern apply transform. could implement type's passes e.g. predicatepass patternmatchpass, looppass , could simplify code reduce code reuse. motivation hoping introducing concepts pytorch's jit could mature optimization framework increase performance pytorch general. pitch approach features implemented onnx optimization framework https github.com onnx onnx tree master onnx optimizer . agree pytorch needs features start working this.",0,maturing jit optimization framework,"maturing jit optimization framework feature taking current look jit optimization framework couple feature missing. structure captures post pass analysis optimization pass. analysis fundamental optimization allow us understand necessary passes better. example pytorch simple analysis per pass calculated number nodes computational graph changed could introduce features fixed point optimization. could mature even immutable passes analysis passes outline order apply passes maximize performance. currently trivial implementation optimization simply runs passes linear order https github.com pytorch pytorch blob master torch csrc jit graph executor.cpp l458 . another feature pass managers. pytorch implicitly defining families optimization passes e.g. peepholeoptimizeimpl implements family peephole optimization . explicit oo implementation pass managers substantially clean code. simplifying implementation passes increase agility. there's lot code reuse happening respect iterating graph checking pattern apply transform. could implement type's passes e.g. predicatepass patternmatchpass, looppass , could simplify code reduce code reuse. motivation hoping introducing concepts pytorch's jit could mature optimization framework increase performance pytorch general. pitch approach features implemented onnx optimization framework https github.com onnx onnx tree master onnx optimizer . agree pytorch needs features start working this."
pytorch,2470,seems like broken version 0.2.0.1. please note need pass argument reproduce bug. parameterless working correctly. example,0,var dim std dim broken floattensor,var dim std dim broken floattensor seems like broken version 0.2.0.1. please note need pass argument reproduce bug. parameterless working correctly. example
pytorch,15676,"feature tests c vec256 classes. motivation currently, vec256 code tested indirectly operators use it. led insufficient test coverage number recent bugs vec256 operators use it. add sufficient test infrastructure easily add test cases new functions vec256. important tests cover different data types float, double, int32, int64, etc. instruction sets generic, avx, avx2 . require changes cmake since current test cases enable avx avx2 instruction sets. cc vitalyfedyunin",0,tests vec256 classes,"tests vec256 classes feature tests c vec256 classes. motivation currently, vec256 code tested indirectly operators use it. led insufficient test coverage number recent bugs vec256 operators use it. add sufficient test infrastructure easily add test cases new functions vec256. important tests cover different data types float, double, int32, int64, etc. instruction sets generic, avx, avx2 . require changes cmake since current test cases enable avx avx2 instruction sets. cc vitalyfedyunin"
pytorch,6968,"issue description seems stacking currently pytorch 0.4 supported tensors type see code example. know intentional reason hence label issue bug , find issue documentation pointing that. since methods, like , depend case implicitly method , would great either stacking halftensors could get implemented, documentation could updated contain kind hint. code example pytorch 0.3.1, one gets basically error one tries thing",0,feature request implement torch.stack cat also torch.halftensor,"feature request implement torch.stack cat also torch.halftensor issue description seems stacking currently pytorch 0.4 supported tensors type see code example. know intentional reason hence label issue bug , find issue documentation pointing that. since methods, like , depend case implicitly method , would great either stacking halftensors could get implemented, documentation could updated contain kind hint. code example pytorch 0.3.1, one gets basically error one tries thing"
pytorch,6164,"pytorch feature requests future roadmap hello everyone thank great project. would like point things find important future growth pytorch. 1. easy name conventions throughout library. believe things renamed resonates easier remember. also lower entry bar new people exposed pytorch. example mean anything soon use brain already forgotten it. sane naming good seems universal convention pretty much since major libraries using it. would also love see something like , , , , , etc. 2. model dissection. often find scenario would like get output complex model different levels. example 3. dataset preprocessing transformation pipeline switch cpu gpu many times find need data preprocessing gpu especially scenario abundance. choice moment inexistent. flag user easily switch cpu gpu upon request order chose data preprocessing take place. 4. addition negative indexing tensors. 5. integration applicability already pre existing tools. tools already pre exist nice box support integration them. instance tensorboard great moment need rely third party developed plugins work. instead would better pytorch ability dump logs consumed tensorboard directly. 5. serving models completely inexistent moment writing. anyone else please feel free add anything else might think important future directions. basically watched whole tf dev summit one advantage pytorch might tf dynamic models that's gonna eager mode. since folks tf faster release cycles fail iterate faster concepts ideas. lot people nagging language choices horrific since iterated lot brought sane level . things make wonder future directions pytorch heading? thank you!",0,feature requests future roadmap,"feature requests future roadmap pytorch feature requests future roadmap hello everyone thank great project. would like point things find important future growth pytorch. 1. easy name conventions throughout library. believe things renamed resonates easier remember. also lower entry bar new people exposed pytorch. example mean anything soon use brain already forgotten it. sane naming good seems universal convention pretty much since major libraries using it. would also love see something like , , , , , etc. 2. model dissection. often find scenario would like get output complex model different levels. example 3. dataset preprocessing transformation pipeline switch cpu gpu many times find need data preprocessing gpu especially scenario abundance. choice moment inexistent. flag user easily switch cpu gpu upon request order chose data preprocessing take place. 4. addition negative indexing tensors. 5. integration applicability already pre existing tools. tools already pre exist nice box support integration them. instance tensorboard great moment need rely third party developed plugins work. instead would better pytorch ability dump logs consumed tensorboard directly. 5. serving models completely inexistent moment writing. anyone else please feel free add anything else might think important future directions. basically watched whole tf dev summit one advantage pytorch might tf dynamic models that's gonna eager mode. since folks tf faster release cycles fail iterate faster concepts ideas. lot people nagging language choices horrific since iterated lot brought sane level . things make wonder future directions pytorch heading? thank you!"
pytorch,25172,"bug https github.com pytorch pytorch blob v1.2.0 test test cuda.py l2224 fails ppc64le. following test https github.com pytorch pytorch blob v1.2.0 test test torch.py l6558 testing different possible combinations, failures seen combinations tests fine . went commented assert statements printed tensors compared combinations reproduce steps reproduce behavior 1. build source ppc64le 1. run tests expected behavior test pass environment pytorch version 1.2.0 debug build cuda used build pytorch 10.1.105 os red hat enterprise linux gcc version gcc 8.2.0 cmake version version 3.13.3 python version 3.7 cuda available yes cuda runtime version 10.1.105 gpu models configuration gpu 0 tesla v100 sxm2 16gb gpu 1 tesla v100 sxm2 16gb nvidia driver version 418.67 cudnn version 7.4.2.24 magma version 7.5.1 versions relevant libraries pip3 numpy 1.16.2 pip3 torch 1.2.0 conda could collect pytorch built source build command pytorch build version 1.2.0 pytorch build number 1 verbose 1 ldflags ldflags ldl cflags cxxflags use ffmpeg use gloo ibverbs 1 use gflags use glog cudnn lib dir ebrootcudnn lib64 cudnn include dir ebrootcudnn include torch cuda arch list 3.5 6.0 7.0 rds bear apps devel 2019a branfosj eb pytorch el7 el7 power9 software python 3.7.2 gcccore 8.2.0 bin python setup.py build",0,test det logdet slogdet batched test cuda.py fails ppc64le,"test det logdet slogdet batched test cuda.py fails ppc64le bug https github.com pytorch pytorch blob v1.2.0 test test cuda.py l2224 fails ppc64le. following test https github.com pytorch pytorch blob v1.2.0 test test torch.py l6558 testing different possible combinations, failures seen combinations tests fine . went commented assert statements printed tensors compared combinations reproduce steps reproduce behavior 1. build source ppc64le 1. run tests expected behavior test pass environment pytorch version 1.2.0 debug build cuda used build pytorch 10.1.105 os red hat enterprise linux gcc version gcc 8.2.0 cmake version version 3.13.3 python version 3.7 cuda available yes cuda runtime version 10.1.105 gpu models configuration gpu 0 tesla v100 sxm2 16gb gpu 1 tesla v100 sxm2 16gb nvidia driver version 418.67 cudnn version 7.4.2.24 magma version 7.5.1 versions relevant libraries pip3 numpy 1.16.2 pip3 torch 1.2.0 conda could collect pytorch built source build command pytorch build version 1.2.0 pytorch build number 1 verbose 1 ldflags ldflags ldl cflags cxxflags use ffmpeg use gloo ibverbs 1 use gflags use glog cudnn lib dir ebrootcudnn lib64 cudnn include dir ebrootcudnn include torch cuda arch list 3.5 6.0 7.0 rds bear apps devel 2019a branfosj eb pytorch el7 el7 power9 software python 3.7.2 gcccore 8.2.0 bin python setup.py build"
pytorch,20855,"code run well pytorch 0.4 . pytorch1.0.1, got error following. locate question resolve it? thanks much. file g 20190215 backup bsandnieliantoimage optimizer expanglesplit faceseg splitbs faceexprecong.py , line 215, optimizer loss.backward file e program files python35 lib site packages torch tensor.py , line 102, backward torch.autograd.backward self, gradient, retain graph, create graph file e program files python35 lib site packages torch autograd init .py , line 90, backward allow unreachable true allow unreachable flag runtimeerror one variables needed gradient computation modified inplace operation",0,pytorch0.4 pytorch1.0.1 runtimeerror one variables needed gradient computation modified inplace operation,"pytorch0.4 pytorch1.0.1 runtimeerror one variables needed gradient computation modified inplace operation code run well pytorch 0.4 . pytorch1.0.1, got error following. locate question resolve it? thanks much. file g 20190215 backup bsandnieliantoimage optimizer expanglesplit faceseg splitbs faceexprecong.py , line 215, optimizer loss.backward file e program files python35 lib site packages torch tensor.py , line 102, backward torch.autograd.backward self, gradient, retain graph, create graph file e program files python35 lib site packages torch autograd init .py , line 90, backward allow unreachable true allow unreachable flag runtimeerror one variables needed gradient computation modified inplace operation"
pytorch,6258,"cpuhrsch 6192 disables imprecise vectorized function avx mathfunc.h vml https software.intel.com en us mkl developer reference c vm mathematical functions vector math library mkl, performance precision promised. long installed conda, vml used aten. mlwoo preparing code previously, job got interrupted since hospital lately performance comparison avx mathfunc vml exp. though performance improvement much, precision guaranteed. looks ok you, continue job, mlwoo finish therapy anyway.",0,"feature request use mkl vml.h exp, log vectorization cpu","feature request use mkl vml.h exp, log vectorization cpu cpuhrsch 6192 disables imprecise vectorized function avx mathfunc.h vml https software.intel.com en us mkl developer reference c vm mathematical functions vector math library mkl, performance precision promised. long installed conda, vml used aten. mlwoo preparing code previously, job got interrupted since hospital lately performance comparison avx mathfunc vml exp. though performance improvement much, precision guaranteed. looks ok you, continue job, mlwoo finish therapy anyway."
pytorch,630,paper http www.jmlr.org papers volume3 gers02a gers02a.pdf . peephole connections seem help learning precise timings events.,0,add peephole connections lstms?,add peephole connections lstms? paper http www.jmlr.org papers volume3 gers02a gers02a.pdf . peephole connections seem help learning precise timings events.
pytorch,4540,"running slightly modified code edsr baseline network https github.com thstkdgus35 edsr pytorch training first epoch goes well, test stage loading model raises error strange gpu memory moment utilised half. here's nvidia smi output moment peak memory usage test function also use volatile true option. could problem? os win10 gpu 1060 6g",0,cuda memory gpu memory utilized half,"cuda memory gpu memory utilized half running slightly modified code edsr baseline network https github.com thstkdgus35 edsr pytorch training first epoch goes well, test stage loading model raises error strange gpu memory moment utilised half. here's nvidia smi output moment peak memory usage test function also use volatile true option. could problem? os win10 gpu 1060 6g"
pytorch,15436,"bug quite new pytorch exactly sure bug error end. attempting build simple logisticregression module dataset sklearn reason, keep getting this. runtimeerror expected object scalar type double got scalar type float argument 2 'mat2' tried convert x variables still seems think float reason. reproduce steps reproduce behavior error generated runtimeerror traceback recent call last 10 optimizer.zero grad 11 12 outputs model train 13 loss criterion outputs, test 14 miniconda3 envs mlbook lib python3.6 site packages torch nn modules module.py call self, input, kwargs 487 result self. slow forward input, kwargs 488 else 489 result self.forward input, kwargs 490 hook self. forward hooks.values 491 hook result hook self, input, result forward self, x 5 6 def forward self, x 7 self.layer x 8 return miniconda3 envs mlbook lib python3.6 site packages torch nn modules module.py call self, input, kwargs 487 result self. slow forward input, kwargs 488 else 489 result self.forward input, kwargs 490 hook self. forward hooks.values 491 hook result hook self, input, result miniconda3 envs mlbook lib python3.6 site packages torch nn modules linear.py forward self, input 65 weak script method 66 def forward self, input 67 return f.linear input, self.weight, self.bias 68 69 def extra repr self miniconda3 envs mlbook lib python3.6 site packages torch nn functional.py linear input, weight, bias 1352 ret torch.addmm torch.jit. unwrap optional bias , input, weight.t 1353 else 1354 output input.matmul weight.t 1355 bias none 1356 output torch.jit. unwrap optional bias runtimeerror expected object scalar type double got scalar type float argument 2 'mat2' expected behavior train test data network environment please copy paste output environment collection script https raw.githubusercontent.com pytorch pytorch master torch utils collect env.py fill checklist manually . get script run pytorch version 1.0.0 debug build cuda used build pytorch none os mac osx 10.14.3 gcc version could collect cmake version version 3.13.2 python version 3.6 cuda available cuda runtime version cuda gpu models configuration cuda nvidia driver version cuda cudnn version cuda versions relevant libraries pip numpy 1.14.0 pip numpydoc 0.7.0 pip torch 1.0.0 pip torchvision 0.2.1 conda blas 1.0 mkl conda mkl 2018.0.1 hfbd8650 4 conda mkl service 1.1.2 py36h7ea6df4 4 conda pytorch 1.0.0 py3.6 1 pytorch conda torchvision 0.2.1 py 2 pytorch additional context attempt check datatype point, get respectively.",0,type conversions,"type conversions bug quite new pytorch exactly sure bug error end. attempting build simple logisticregression module dataset sklearn reason, keep getting this. runtimeerror expected object scalar type double got scalar type float argument 2 'mat2' tried convert x variables still seems think float reason. reproduce steps reproduce behavior error generated runtimeerror traceback recent call last 10 optimizer.zero grad 11 12 outputs model train 13 loss criterion outputs, test 14 miniconda3 envs mlbook lib python3.6 site packages torch nn modules module.py call self, input, kwargs 487 result self. slow forward input, kwargs 488 else 489 result self.forward input, kwargs 490 hook self. forward hooks.values 491 hook result hook self, input, result forward self, x 5 6 def forward self, x 7 self.layer x 8 return miniconda3 envs mlbook lib python3.6 site packages torch nn modules module.py call self, input, kwargs 487 result self. slow forward input, kwargs 488 else 489 result self.forward input, kwargs 490 hook self. forward hooks.values 491 hook result hook self, input, result miniconda3 envs mlbook lib python3.6 site packages torch nn modules linear.py forward self, input 65 weak script method 66 def forward self, input 67 return f.linear input, self.weight, self.bias 68 69 def extra repr self miniconda3 envs mlbook lib python3.6 site packages torch nn functional.py linear input, weight, bias 1352 ret torch.addmm torch.jit. unwrap optional bias , input, weight.t 1353 else 1354 output input.matmul weight.t 1355 bias none 1356 output torch.jit. unwrap optional bias runtimeerror expected object scalar type double got scalar type float argument 2 'mat2' expected behavior train test data network environment please copy paste output environment collection script https raw.githubusercontent.com pytorch pytorch master torch utils collect env.py fill checklist manually . get script run pytorch version 1.0.0 debug build cuda used build pytorch none os mac osx 10.14.3 gcc version could collect cmake version version 3.13.2 python version 3.6 cuda available cuda runtime version cuda gpu models configuration cuda nvidia driver version cuda cudnn version cuda versions relevant libraries pip numpy 1.14.0 pip numpydoc 0.7.0 pip torch 1.0.0 pip torchvision 0.2.1 conda blas 1.0 mkl conda mkl 2018.0.1 hfbd8650 4 conda mkl service 1.1.2 py36h7ea6df4 4 conda pytorch 1.0.0 py3.6 1 pytorch conda torchvision 0.2.1 py 2 pytorch additional context attempt check datatype point, get respectively."
pytorch,20029,"bug cuda 10.1 rtx gpu. try even simpliest code like nums torch.randn 2,2 .cuda , show error torch.cuda.is available true. idea?",0,runtimeerror cuda error memory,"runtimeerror cuda error memory bug cuda 10.1 rtx gpu. try even simpliest code like nums torch.randn 2,2 .cuda , show error torch.cuda.is available true. idea?"
pytorch,23070,"bug file sts main.py , line 196, model, optimizer, loss train model, train iter, optimizer, criterion file sts main.py , line 72, train optimizer.step file home ubuntu anaconda3 lib python3.6 site packages torch optim adam.py , line 85, step beta1, beta2 group 'betas' keyerror 'betas'",0,adam.py keyerror 'betas',"adam.py keyerror 'betas' bug file sts main.py , line 196, model, optimizer, loss train model, train iter, optimizer, criterion file sts main.py , line 72, train optimizer.step file home ubuntu anaconda3 lib python3.6 site packages torch optim adam.py , line 85, step beta1, beta2 group 'betas' keyerror 'betas'"
pytorch,4894,os ubuntu 14.04 pytorch version 0.3.0.post4 installed pytorch conda python version 3.6.3 cuda version 8 script reproduce error,0,inconsistent size error using pin memory torch.unbind,inconsistent size error using pin memory torch.unbind os ubuntu 14.04 pytorch version 0.3.0.post4 installed pytorch conda python version 3.6.3 cuda version 8 script reproduce error
pytorch,31528,"bug using distributeddataparallel openmpi ucx certain tested models getting error ddp constructor believe model broadcast observed error using resnet50 model error instead used small single layer cnn. reproduce script https github.com sparticlesteve nersc pytorch build blob 911fc67b6667d3c6e3be972169e30e34c1a33af5 test ddp.py submit via slurm single node job 8 mpi ranks 8 v100 gpus, something like full log stack trace https gist.github.com sparticlesteve 7307694f89329c277e16e452b524fefa environment pytorch version 1.3.1 debug build openmpi 4.0.1 ucx 1.6 cuda used build pytorch 10.1.168 os opensuse leap 15.0 gcc version gcc 7.3.0 20180125 cray inc. cmake version version 3.14.0 python version 3.6 cuda available yes cuda runtime version 10.1.168 gpu models configuration gpu 0 tesla v100 sxm2 16gb gpu 1 tesla v100 sxm2 16gb gpu 2 tesla v100 sxm2 16gb gpu 3 tesla v100 sxm2 16gb gpu 4 tesla v100 sxm2 16gb gpu 5 tesla v100 sxm2 16gb gpu 6 tesla v100 sxm2 16gb gpu 7 tesla v100 sxm2 16gb nvidia driver version 440.33.01 cudnn version could collect cc ngimel pietern mrshenli pritamdamania87 zhaojuanmao satgera rohan varma gqchen aazzolini xush6528",0,cuctxgetdevice error seg fault ddp openmpi,"cuctxgetdevice error seg fault ddp openmpi bug using distributeddataparallel openmpi ucx certain tested models getting error ddp constructor believe model broadcast observed error using resnet50 model error instead used small single layer cnn. reproduce script https github.com sparticlesteve nersc pytorch build blob 911fc67b6667d3c6e3be972169e30e34c1a33af5 test ddp.py submit via slurm single node job 8 mpi ranks 8 v100 gpus, something like full log stack trace https gist.github.com sparticlesteve 7307694f89329c277e16e452b524fefa environment pytorch version 1.3.1 debug build openmpi 4.0.1 ucx 1.6 cuda used build pytorch 10.1.168 os opensuse leap 15.0 gcc version gcc 7.3.0 20180125 cray inc. cmake version version 3.14.0 python version 3.6 cuda available yes cuda runtime version 10.1.168 gpu models configuration gpu 0 tesla v100 sxm2 16gb gpu 1 tesla v100 sxm2 16gb gpu 2 tesla v100 sxm2 16gb gpu 3 tesla v100 sxm2 16gb gpu 4 tesla v100 sxm2 16gb gpu 5 tesla v100 sxm2 16gb gpu 6 tesla v100 sxm2 16gb gpu 7 tesla v100 sxm2 16gb nvidia driver version 440.33.01 cudnn version could collect cc ngimel pietern mrshenli pritamdamania87 zhaojuanmao satgera rohan varma gqchen aazzolini xush6528"
pytorch,18701,"bug pytorch debug build lib debug caffe2 debug.lib fatal error lnk1248 image size 10007fe9e exceeds maximum allowable size ffffffff reproduce steps reproduce behavior 1. compile latest master windows static expected behavior links environment pytorch version e.g., 1.0 master os e.g., linux windows installed pytorch , , source source build command used compiling source cmake duse cuda dbuild shared libs python version 3.7 cuda cudnn version 10.1 gpu models configuration gtx 1060 relevant information additional context issue 1.0.1 perhaps size gone threshold. appears msvc 4gb code data file size limit. sure there's workaround. although, microsoft says split library https developercommunity.visualstudio.com content problem 332991 static lib 4gb file size limit lnk1248 image size.html",0,msvc caffe2 debug static size big lnk1248,"msvc caffe2 debug static size big lnk1248 bug pytorch debug build lib debug caffe2 debug.lib fatal error lnk1248 image size 10007fe9e exceeds maximum allowable size ffffffff reproduce steps reproduce behavior 1. compile latest master windows static expected behavior links environment pytorch version e.g., 1.0 master os e.g., linux windows installed pytorch , , source source build command used compiling source cmake duse cuda dbuild shared libs python version 3.7 cuda cudnn version 10.1 gpu models configuration gtx 1060 relevant information additional context issue 1.0.1 perhaps size gone threshold. appears msvc 4gb code data file size limit. sure there's workaround. although, microsoft says split library https developercommunity.visualstudio.com content problem 332991 static lib 4gb file size limit lnk1248 image size.html"
pytorch,1357,error happens specific conditions. change view input tensor use weight converted numpy array. output functional conv2d far scale e12 so.,0,convnd c implementation check tensor types,convnd c implementation check tensor types error happens specific conditions. change view input tensor use weight converted numpy array. output functional conv2d far scale e12 so.
pytorch,5938,"trying run code https github.com yunjey pytorch tutorial blob 4b67434961a64ba5e19e63d44f0b9979d2a9aa11 tutorials 01 basics feedforward neural network main gpu.py . training progress perfectbut accuracy print outand error came like code tried python3.5 win10it could ran perfectlybut somehow use cudaso turn ubuntuis python version problem os ubuntu16.04 pytorch version 0.4.0a0 7cbbc0b installed pytorch conda, pip, source source python version 2.7 cuda cudnn version 9.0 gcc version compiling source gcc ubuntu 5.4.0 6ubuntu1 16.04.4 5.4.0 20160609 know question asked cause pytorch's issus, can't access stackoverflow, slow chinese internet .so appericated someone could help this.thx",0,runtimeerror value cannot converted type uint8 without overflow 10000,"runtimeerror value cannot converted type uint8 without overflow 10000 trying run code https github.com yunjey pytorch tutorial blob 4b67434961a64ba5e19e63d44f0b9979d2a9aa11 tutorials 01 basics feedforward neural network main gpu.py . training progress perfectbut accuracy print outand error came like code tried python3.5 win10it could ran perfectlybut somehow use cudaso turn ubuntuis python version problem os ubuntu16.04 pytorch version 0.4.0a0 7cbbc0b installed pytorch conda, pip, source source python version 2.7 cuda cudnn version 9.0 gcc version compiling source gcc ubuntu 5.4.0 6ubuntu1 16.04.4 5.4.0 20160609 know question asked cause pytorch's issus, can't access stackoverflow, slow chinese internet .so appericated someone could help this.thx"
pytorch,14864,".pth files used python list additional package search paths https docs.python.org 3 library site.html pth files loaded text files python interpreter. point pytorch model pth file placed along sources, caused hang python startup trying parse big binary file list paths . maybe .pt?",0,discussion recommend different file extension models .pth special extension python,"discussion recommend different file extension models .pth special extension python .pth files used python list additional package search paths https docs.python.org 3 library site.html pth files loaded text files python interpreter. point pytorch model pth file placed along sources, caused hang python startup trying parse big binary file list paths . maybe .pt?"
pytorch,2677,"code x variable torch.rand 8, 1, 5, 5 , requires grad true conv nn.conv2d 1, 1, 3 conv x final torch.sum print 'y.grad', y.grad final.backward print 'y.grad', y.grad print 'y.grad', y.requires grad however, output y.grad none y.grad none y.grad true y.requires grad true, second y.grad output gradients instead none? x, gradient showed normally backward",0,"require grad true, printed none","require grad true, printed none code x variable torch.rand 8, 1, 5, 5 , requires grad true conv nn.conv2d 1, 1, 3 conv x final torch.sum print 'y.grad', y.grad final.backward print 'y.grad', y.grad print 'y.grad', y.requires grad however, output y.grad none y.grad none y.grad true y.requires grad true, second y.grad output gradients instead none? x, gradient showed normally backward"
pytorch,4387,"problem arise run 1epoch, works well first train epoch well followed 1st validate epoch. batch size 8 work number 2why happened ? file home luhongchao anaconda2 lib python2.7 site packages torch utils data dataload er.py , line 301, iter return dataloaderiter self file home luhongchao anaconda2 lib python2.7 site packages torch utils data dataload er.py , line 158, init w.start file home luhongchao anaconda2 lib python2.7 multiprocessing process.py , line 130, start self. popen popen self file home luhongchao anaconda2 lib python2.7 multiprocessing forking.py , line 121, init self.pid os.fork oserror errno 12 cannot allocate memory",0,cannot allocate memory,"cannot allocate memory problem arise run 1epoch, works well first train epoch well followed 1st validate epoch. batch size 8 work number 2why happened ? file home luhongchao anaconda2 lib python2.7 site packages torch utils data dataload er.py , line 301, iter return dataloaderiter self file home luhongchao anaconda2 lib python2.7 site packages torch utils data dataload er.py , line 158, init w.start file home luhongchao anaconda2 lib python2.7 multiprocessing process.py , line 130, start self. popen popen self file home luhongchao anaconda2 lib python2.7 multiprocessing forking.py , line 121, init self.pid os.fork oserror errno 12 cannot allocate memory"
pytorch,12887,"hi. using pytorch 0.5.0a0 09896d1 code returning segmentation fault . instead , works. also, problem occur . edit efficient way reproduce bug",0,segmentation fault summing uint8 tensor,"segmentation fault summing uint8 tensor hi. using pytorch 0.5.0a0 09896d1 code returning segmentation fault . instead , works. also, problem occur . edit efficient way reproduce bug"
pytorch,6437,"lot operators defined double precision. planning experimenting double precision calculations add necessary. would interest submitting pr request them? reason currently enabled? drawback larger library file size slower compilation time suppose much. thank you, svet",0,caffe2 double precision operators?,"caffe2 double precision operators? lot operators defined double precision. planning experimenting double precision calculations add necessary. would interest submitting pr request them? reason currently enabled? drawback larger library file size slower compilation time suppose much. thank you, svet"
pytorch,2351,"hi, use pytroch train cnn model cifar 10 entropy sgd https github.com ucla vision entropy sgd tree master python . log v0.1.12 2 train 1 0.9442 33.32 13.62s train 2 0.8139 28.38 14.39s train 3 0.7006 24.22 13.76s train 4 0.6254 21.68 13.71s train 5 0.5720 19.96 13.63s log v0.2.0 1 train 1 0.2238 7.84 294.03s train 2 0.1349 4.66 291.71s train 3 0.0950 3.26 291.67s train 4 0.0715 2.41 290.50s train 5 0.0558 1.89 295.84s code difference version pytroch. performance two gap? p.s. generate pip resource v.0.1.12 ?",0,performance v0.2.0 20x slower v0.1.12,"performance v0.2.0 20x slower v0.1.12 hi, use pytroch train cnn model cifar 10 entropy sgd https github.com ucla vision entropy sgd tree master python . log v0.1.12 2 train 1 0.9442 33.32 13.62s train 2 0.8139 28.38 14.39s train 3 0.7006 24.22 13.76s train 4 0.6254 21.68 13.71s train 5 0.5720 19.96 13.63s log v0.2.0 1 train 1 0.2238 7.84 294.03s train 2 0.1349 4.66 291.71s train 3 0.0950 3.26 291.67s train 4 0.0715 2.41 290.50s train 5 0.0558 1.89 295.84s code difference version pytroch. performance two gap? p.s. generate pip resource v.0.1.12 ?"
pytorch,12013,"support tensors zero size, believe would handy support accepting batches size 0 functions. non exhaustive list functions would good supporting x x x x handling losses bit trickier, generally involves computing , results due 0 0 division. expect 0 loss empty batches make sense, that's debatable might worth postponing decision. cc ezyang gchanan zou3519 alband mruberry",0,feature request make nn layers accept empty batch size,"feature request make nn layers accept empty batch size support tensors zero size, believe would handy support accepting batches size 0 functions. non exhaustive list functions would good supporting x x x x handling losses bit trickier, generally involves computing , results due 0 0 division. expect 0 loss empty batches make sense, that's debatable might worth postponing decision. cc ezyang gchanan zou3519 alband mruberry"
pytorch,30798,"implement nn.unfold 5d tensor? implementing operation 3d image. found need nn.unfold function process. now, pytorch official implementation latest release version. want implement official release code form myself. little confused relationship implementation part c cuda code. could one give suggestions files refer corresponding implementation nn.unfold 4d version...",0,implement nn.unfold 5d tensor?,"implement nn.unfold 5d tensor? implement nn.unfold 5d tensor? implementing operation 3d image. found need nn.unfold function process. now, pytorch official implementation latest release version. want implement official release code form myself. little confused relationship implementation part c cuda code. could one give suggestions files refer corresponding implementation nn.unfold 4d version..."
pytorch,1128,"taking list tensors, function pad them? useful feature process sequences different lengths process mini batches. think documentation pad packed sequence torch.nn.utils.rnn.pack padded sequence little confusing http pytorch.org docs nn.html?highlight pad packed torch.nn.utils.rnn.pad packed sequence maybe adding examples could help explanation.",0,pad list tensors,"pad list tensors taking list tensors, function pad them? useful feature process sequences different lengths process mini batches. think documentation pad packed sequence torch.nn.utils.rnn.pack padded sequence little confusing http pytorch.org docs nn.html?highlight pad packed torch.nn.utils.rnn.pad packed sequence maybe adding examples could help explanation."
pytorch,28404,"questions help tried quantizate shuffle model 'static quantization eager mode pytorch'. comes forward propagation, time assess model losses, failed cc jerryzh168 jianyuh dzhulgakov raghuramank100",0,quantization error function registered schema aten,"quantization error function registered schema aten questions help tried quantizate shuffle model 'static quantization eager mode pytorch'. comes forward propagation, time assess model losses, failed cc jerryzh168 jianyuh dzhulgakov raghuramank100"
pytorch,5331,"hi, saw error last days. gpu cpu environment. test precision assertion correct ?",0,ppc64le fail test set flush denormal main .testtorch,"ppc64le fail test set flush denormal main .testtorch hi, saw error last days. gpu cpu environment. test precision assertion correct ?"
pytorch,23756,inplace batchnorm seems developed mapillary https github.com mapillary inplace abn would nice addition core pytorch memory savings . cc vincentqb fritzo neerajprad alicanb vishwakftw ssnl,0,feature request core api invertible flow like ops,feature request core api invertible flow like ops inplace batchnorm seems developed mapillary https github.com mapillary inplace abn would nice addition core pytorch memory savings . cc vincentqb fritzo neerajprad alicanb vishwakftw ssnl
pytorch,145,would possible feed embedding layer inttensors rather longtensors only?,0,embeddings layer inttensor cuda.inttensor inputs,embeddings layer inttensor cuda.inttensor inputs would possible feed embedding layer inttensors rather longtensors only?
pytorch,5302,"upgraded v0.3.1 using conda since experienced really slow performance first time call .cuda . tried solutions listed https discuss.pytorch.org model cuda takes long time 102 20 https discuss.pytorch.org model cuda takes long time 102 20 luck. tried installing latest version source still experienced slowdown. help would hugely appreciated! info os centos 7.4.1708 pytorch version tried 0.3.1 conda 0.4.0a0 6279367 source installed pytorch conda, pip, source conda, source python version 3.6.3 cuda cudnn version 8.0.61, 7.0.2 gpu models configuration 1080ti gcc version compiling source 7.2.0",0,slow first .cuda call,"slow first .cuda call upgraded v0.3.1 using conda since experienced really slow performance first time call .cuda . tried solutions listed https discuss.pytorch.org model cuda takes long time 102 20 https discuss.pytorch.org model cuda takes long time 102 20 luck. tried installing latest version source still experienced slowdown. help would hugely appreciated! info os centos 7.4.1708 pytorch version tried 0.3.1 conda 0.4.0a0 6279367 source installed pytorch conda, pip, source conda, source python version 3.6.3 cuda cudnn version 8.0.61, 7.0.2 gpu models configuration 1080ti gcc version compiling source 7.2.0"
pytorch,9356,"forums https discuss.pytorch.org . submitting bug report, please fill following details. using linux machine ubuntu . ubuntu version 16.04 lts nvidia 930mx nvidia driver version 384.184 issue description runtimeerror cuda runtime error 8 invalid device function opt conda conda bld pytorch 1518243271935 work torch lib thcunn generic threshold.cu 34 provide short description. code example code segnet , detection vessel retina. please try provide minimal example repro bug. error messages stack traces also helpful. system info please copy paste output environment collection script https raw.githubusercontent.com pytorch pytorch master torch utils collect env.py fill checklist manually . get script run pytorch caffe2 installed pytorch conda, pip, source conda build command used compiling source os ubuntu 16.04 pytorch version 0.3.1u python version 3.6 cuda cudnn version 7.5 ! issue pytorch https user images.githubusercontent.com 24300927 42598377 a3d0db30 8579 11e8 9db3 886d6a9e498c.png gpu models configuration gcc version compiling source cmake version versions relevant libraries",0,runtimeerror cuda runtime error 8 invalid device function opt conda conda bld pytorch 1518243271935 work torch lib thcunn generic threshold.cu 34,"runtimeerror cuda runtime error 8 invalid device function opt conda conda bld pytorch 1518243271935 work torch lib thcunn generic threshold.cu 34 forums https discuss.pytorch.org . submitting bug report, please fill following details. using linux machine ubuntu . ubuntu version 16.04 lts nvidia 930mx nvidia driver version 384.184 issue description runtimeerror cuda runtime error 8 invalid device function opt conda conda bld pytorch 1518243271935 work torch lib thcunn generic threshold.cu 34 provide short description. code example code segnet , detection vessel retina. please try provide minimal example repro bug. error messages stack traces also helpful. system info please copy paste output environment collection script https raw.githubusercontent.com pytorch pytorch master torch utils collect env.py fill checklist manually . get script run pytorch caffe2 installed pytorch conda, pip, source conda build command used compiling source os ubuntu 16.04 pytorch version 0.3.1u python version 3.6 cuda cudnn version 7.5 ! issue pytorch https user images.githubusercontent.com 24300927 42598377 a3d0db30 8579 11e8 9db3 886d6a9e498c.png gpu models configuration gcc version compiling source cmake version versions relevant libraries"
pytorch,28781,"questions help hi new pytorch try install pytorch rocm step step following https rocm documentation.readthedocs.io en latest deep learning deep learning.html building pytorch rocm output pytorch test rocm 1 python test run test.py verbose correct ,but got error like test lerp tensor weights, test unused output device cuda failed could anyone help figure please hardware os ubuntu 16.04 motherboard rog crosshair vi hero cpu 2700x gpu rx480 2",0,run run test.py error,"run run test.py error questions help hi new pytorch try install pytorch rocm step step following https rocm documentation.readthedocs.io en latest deep learning deep learning.html building pytorch rocm output pytorch test rocm 1 python test run test.py verbose correct ,but got error like test lerp tensor weights, test unused output device cuda failed could anyone help figure please hardware os ubuntu 16.04 motherboard rog crosshair vi hero cpu 2700x gpu rx480 2"
pytorch,4302,results 'torch.halftensor' object attribute 'mean' 0.4.0a0 5f7c550,0,'torch.halftensor' object attribute 'mean','torch.halftensor' object attribute 'mean' results 'torch.halftensor' object attribute 'mean' 0.4.0a0 5f7c550
pytorch,5790,"would nice hookable weights, per layer parameters operated upon determined dynamically forward pass. following short list techniques could benefit functionality quantization 1 https arxiv.org abs 1609.07061 pruning 2 https arxiv.org abs 1510.00149 3 https arxiv.org abs 1707.06168 4 https arxiv.org abs 1802.00124 stochastic gradient estimation 5 https arxiv.org abs 1711.00123 6 https arxiv.org abs 1703.07370 dropconnect 7 https cs.nyu.edu wanli dropc regularization 8 https openreview.net forum?id h1y8hhg0b manifold tangent classifier 9 https papers.nips.cc paper 4409 manifold tangent classifier.pdf begun work framework https github.com castorini candle , code research oriented production ready. think community would benefit official support hookable weights. think would also interesting extensible backprop framework, sense allowing computation different user defined quantities across computation graph. example, hessian diagonals computed efficiently 10 http yann.lecun.com exdb publis pdf lecun 90b.pdf using backprop like approach. thanks reading. cc alband mruberry jbschlosser jerryzh168 jianyuh dzhulgakov raghuramank100 jamesr66a vkuzo",0,add hookable weights,"add hookable weights would nice hookable weights, per layer parameters operated upon determined dynamically forward pass. following short list techniques could benefit functionality quantization 1 https arxiv.org abs 1609.07061 pruning 2 https arxiv.org abs 1510.00149 3 https arxiv.org abs 1707.06168 4 https arxiv.org abs 1802.00124 stochastic gradient estimation 5 https arxiv.org abs 1711.00123 6 https arxiv.org abs 1703.07370 dropconnect 7 https cs.nyu.edu wanli dropc regularization 8 https openreview.net forum?id h1y8hhg0b manifold tangent classifier 9 https papers.nips.cc paper 4409 manifold tangent classifier.pdf begun work framework https github.com castorini candle , code research oriented production ready. think community would benefit official support hookable weights. think would also interesting extensible backprop framework, sense allowing computation different user defined quantities across computation graph. example, hessian diagonals computed efficiently 10 http yann.lecun.com exdb publis pdf lecun 90b.pdf using backprop like approach. thanks reading. cc alband mruberry jbschlosser jerryzh168 jianyuh dzhulgakov raghuramank100 jamesr66a vkuzo"
pytorch,25481,"needed incorporating curvature information optimization there's pytorch implementation symmetric matrix square root op https github.com msubhransu matrix sqrt use pytorch backward pass only, use scipy forward pass hacked something together mirroring cc vincentqb vishwakftw jianyuh nikitaved pearu mruberry heitorschueroff ssnl",0,feature request symmetric matrix square root,"feature request symmetric matrix square root needed incorporating curvature information optimization there's pytorch implementation symmetric matrix square root op https github.com msubhransu matrix sqrt use pytorch backward pass only, use scipy forward pass hacked something together mirroring cc vincentqb vishwakftw jianyuh nikitaved pearu mruberry heitorschueroff ssnl"
pytorch,28444,"bug reproduce steps reproduce behavior ran following code 1. conda create detectron2 python 3.7 2. conda activate detectron2 3. conda install numpy ninja pyyaml mkl mkl include setuptools cmake cffi typing 4. git clone recursive https github.com pytorch pytorch 5. cd pytorch 6. export cmake prefix path conda prefix dirname conda .. 7. python setup.py install gave following stdout sorry wall text! wall wno unused wno attributes wno unused result wno psabi ffp contract fno math errno fno trapping math nethome ebj26 apps pytorch third party nccl nccl src' compiling misc nvmlwrap.cc nethome ebj26 apps pytorch build nccl obj misc nvmlwrap.o compiling misc rings.cc nethome ebj26 apps pytorch build nccl obj misc rings.o compiling init.cc nethome ebj26 apps pytorch build nccl obj init.o compiling misc group.cc nethome ebj26 apps pytorch build nccl obj misc group.o grabbing include nccl net.h nethome ebj26 apps pytorch build nccl include nccl net.h compiling misc ibvwrap.cc nethome ebj26 apps pytorch build nccl obj misc ibvwrap.o compiling collectives reduce scatter.cc nethome ebj26 apps pytorch build nccl obj collectives reduce scatter.o compiling channel.cc nethome ebj26 apps pytorch build nccl obj channel.o compiling collectives gather.cc nethome ebj26 apps pytorch build nccl obj collectives gather.o compiling transport p2p.cc nethome ebj26 apps pytorch build nccl obj transport p2p.o compiling enqueue.cc nethome ebj26 apps pytorch build nccl obj enqueue.o compiling bootstrap.cc nethome ebj26 apps pytorch build nccl obj bootstrap.o compiling misc argcheck.cc nethome ebj26 apps pytorch build nccl obj misc argcheck.o compiling transport shm.cc nethome ebj26 apps pytorch build nccl obj transport shm.o compiling transport net.cc nethome ebj26 apps pytorch build nccl obj transport net.o compiling misc topo.cc nethome ebj26 apps pytorch build nccl obj misc topo.o compiling transport net ib.cc nethome ebj26 apps pytorch build nccl obj transport net ib.o compiling collectives broadcast.cc nethome ebj26 apps pytorch build nccl obj collectives broadcast.o compiling transport.cc nethome ebj26 apps pytorch build nccl obj transport.o compiling transport net socket.cc nethome ebj26 apps pytorch build nccl obj transport net socket.o compiling misc utils.cc nethome ebj26 apps pytorch build nccl obj misc utils.o compiling misc trees.cc nethome ebj26 apps pytorch build nccl obj misc trees.o compiling collectives reduce.cc nethome ebj26 apps pytorch build nccl obj collectives reduce.o compiling collectives reduce.cc nethome ebj26 apps pytorch build nccl obj collectives reduce.o generating nccl.h.in nethome ebj26 apps pytorch build nccl include nccl.h generating nccl.pc.in nethome ebj26 apps pytorch build nccl lib pkgconfig nccl.pc make 2 entering directory nethome ebj26 apps pytorch third party nccl nccl src collectives device' make 2 entering directory nethome ebj26 apps pytorch third party nccl nccl src collectives device' linking libnccl.so.2.4.8 nethome ebj26 apps pytorch build nccl lib libnccl.so.2.4.8 make src.build segmentation fault core dumped 200 3081 building cxx object third party protobuf cmake cmakefiles libprotobuf.dir src google protobuf descriptor.cc.o ninja build stopped subcommand failed. traceback recent call last file setup.py , line 759, build deps file setup.py , line 311, build deps cmake cmake file nethome ebj26 apps pytorch tools build pytorch libs.py , line 59, build caffe2 cmake.build env file nethome ebj26 apps pytorch tools setup helpers cmake.py , line 334, build self.run build args, env file nethome ebj26 apps pytorch tools setup helpers cmake.py , line 142, run check call command, cwd self.build dir, env env file nethome ebj26 apps anaconda3 envs detectron2 lib python3.7 subprocess.py , line 347, check call raise calledprocesserror retcode, cmd subprocess.calledprocesserror command ' 'cmake', ' build', '.', ' target', 'install', ' config', 'release', ' ', ' j', '16' ' returned non zero exit status 1. pytorch version n debug build n cuda used build pytorch n os centos release 6.5 final gcc version gcc 5.5.0 cmake version version 3.14.0 python version 3.7 cuda available n cuda runtime version could collect gpu models configuration could collect nvidia driver version could collect cudnn version could collect versions relevant libraries pip numpy 1.17.2 conda blas 1.0 mkl conda mkl 2019.4 243 conda mkl include 2019.4 243 conda mkl service 2.3.0 py37he904b0f 0 conda mkl fft 1.0.14 py37ha843d7b 0 conda mkl random 1.1.0 py37hd6b4f25 0 additional context help would greatly appreciated!",0,problem installing source centos 6.5,"problem installing source centos 6.5 bug reproduce steps reproduce behavior ran following code 1. conda create detectron2 python 3.7 2. conda activate detectron2 3. conda install numpy ninja pyyaml mkl mkl include setuptools cmake cffi typing 4. git clone recursive https github.com pytorch pytorch 5. cd pytorch 6. export cmake prefix path conda prefix dirname conda .. 7. python setup.py install gave following stdout sorry wall text! wall wno unused wno attributes wno unused result wno psabi ffp contract fno math errno fno trapping math nethome ebj26 apps pytorch third party nccl nccl src' compiling misc nvmlwrap.cc nethome ebj26 apps pytorch build nccl obj misc nvmlwrap.o compiling misc rings.cc nethome ebj26 apps pytorch build nccl obj misc rings.o compiling init.cc nethome ebj26 apps pytorch build nccl obj init.o compiling misc group.cc nethome ebj26 apps pytorch build nccl obj misc group.o grabbing include nccl net.h nethome ebj26 apps pytorch build nccl include nccl net.h compiling misc ibvwrap.cc nethome ebj26 apps pytorch build nccl obj misc ibvwrap.o compiling collectives reduce scatter.cc nethome ebj26 apps pytorch build nccl obj collectives reduce scatter.o compiling channel.cc nethome ebj26 apps pytorch build nccl obj channel.o compiling collectives gather.cc nethome ebj26 apps pytorch build nccl obj collectives gather.o compiling transport p2p.cc nethome ebj26 apps pytorch build nccl obj transport p2p.o compiling enqueue.cc nethome ebj26 apps pytorch build nccl obj enqueue.o compiling bootstrap.cc nethome ebj26 apps pytorch build nccl obj bootstrap.o compiling misc argcheck.cc nethome ebj26 apps pytorch build nccl obj misc argcheck.o compiling transport shm.cc nethome ebj26 apps pytorch build nccl obj transport shm.o compiling transport net.cc nethome ebj26 apps pytorch build nccl obj transport net.o compiling misc topo.cc nethome ebj26 apps pytorch build nccl obj misc topo.o compiling transport net ib.cc nethome ebj26 apps pytorch build nccl obj transport net ib.o compiling collectives broadcast.cc nethome ebj26 apps pytorch build nccl obj collectives broadcast.o compiling transport.cc nethome ebj26 apps pytorch build nccl obj transport.o compiling transport net socket.cc nethome ebj26 apps pytorch build nccl obj transport net socket.o compiling misc utils.cc nethome ebj26 apps pytorch build nccl obj misc utils.o compiling misc trees.cc nethome ebj26 apps pytorch build nccl obj misc trees.o compiling collectives reduce.cc nethome ebj26 apps pytorch build nccl obj collectives reduce.o compiling collectives reduce.cc nethome ebj26 apps pytorch build nccl obj collectives reduce.o generating nccl.h.in nethome ebj26 apps pytorch build nccl include nccl.h generating nccl.pc.in nethome ebj26 apps pytorch build nccl lib pkgconfig nccl.pc make 2 entering directory nethome ebj26 apps pytorch third party nccl nccl src collectives device' make 2 entering directory nethome ebj26 apps pytorch third party nccl nccl src collectives device' linking libnccl.so.2.4.8 nethome ebj26 apps pytorch build nccl lib libnccl.so.2.4.8 make src.build segmentation fault core dumped 200 3081 building cxx object third party protobuf cmake cmakefiles libprotobuf.dir src google protobuf descriptor.cc.o ninja build stopped subcommand failed. traceback recent call last file setup.py , line 759, build deps file setup.py , line 311, build deps cmake cmake file nethome ebj26 apps pytorch tools build pytorch libs.py , line 59, build caffe2 cmake.build env file nethome ebj26 apps pytorch tools setup helpers cmake.py , line 334, build self.run build args, env file nethome ebj26 apps pytorch tools setup helpers cmake.py , line 142, run check call command, cwd self.build dir, env env file nethome ebj26 apps anaconda3 envs detectron2 lib python3.7 subprocess.py , line 347, check call raise calledprocesserror retcode, cmd subprocess.calledprocesserror command ' 'cmake', ' build', '.', ' target', 'install', ' config', 'release', ' ', ' j', '16' ' returned non zero exit status 1. pytorch version n debug build n cuda used build pytorch n os centos release 6.5 final gcc version gcc 5.5.0 cmake version version 3.14.0 python version 3.7 cuda available n cuda runtime version could collect gpu models configuration could collect nvidia driver version could collect cudnn version could collect versions relevant libraries pip numpy 1.17.2 conda blas 1.0 mkl conda mkl 2019.4 243 conda mkl include 2019.4 243 conda mkl service 2.3.0 py37he904b0f 0 conda mkl fft 1.0.14 py37ha843d7b 0 conda mkl random 1.1.0 py37hd6b4f25 0 additional context help would greatly appreciated!"
pytorch,20402,"tensor nn.module pruning tensor method util sparsify tensors model, according various pruning techniques literature. motivation state art deep learning techniques rely parametrized models hard deploy. contrary, biological neural networks known use efficient sparse connectivity. important identify best techniques compress models reducing number parameters them, order reduce memory, battery, hardware consumption without sacrificing accuracy, deploy lightweight models device, guarantee privacy private device computation. research front, pruning used investigate differences learning dynamics parametrized parametrized networks, study role lucky sparse subnetworks initializations lottery tickets 1 https arxiv.org abs 1803.03635 , destructive neural architecture search technique, others. goal feature harmonizing pruning practices providing standard interface pytorch. target audience researchers, engineering product teams. pitch minimalist api, deeper flexibility power users. tensor level, could look follows place method return type size one acts on. place pruning supported via . model level, require bit thinking follow similar api patterns. important pruning methods make sense parameters model pruning conv kernels ! pruning biases ! pruning rnns ! pruning presence batch norm, etc. . first, sensible, well documented default behavior average user's api, call defaults pruning pytorch prepackaged modules linear, conv, recurrent layers sensible, expected way. power users though would probably want prune custom layers, prune different layer types layers different depths using different pruning methods pruning method parameters. could specified via dictionary, maps parameter names contained pruning method parameters similar tensor operations, model level pruning could return copy model, act model place. pruning methods used post training implementation training loop agnostic user take care writing training loop decide prune pruned object initialize retrain, finetune, etc. . alternatives depending live within codebase, could also look like . personally would prefer first option kwargs parameters pruning method itself, tensor acts pruning methods also take data applied , last option line how, say, implemented. perhaps, module level application, following example implementation https pytorch.org docs stable modules torch nn utils weight norm.html using hooks make pytorch y, know want sacrifice ability act directly tensor part module. would go ? open suggestions here. cc alband mruberry jbschlosser",0,tensor nn.module pruning,"tensor nn.module pruning tensor nn.module pruning tensor method util sparsify tensors model, according various pruning techniques literature. motivation state art deep learning techniques rely parametrized models hard deploy. contrary, biological neural networks known use efficient sparse connectivity. important identify best techniques compress models reducing number parameters them, order reduce memory, battery, hardware consumption without sacrificing accuracy, deploy lightweight models device, guarantee privacy private device computation. research front, pruning used investigate differences learning dynamics parametrized parametrized networks, study role lucky sparse subnetworks initializations lottery tickets 1 https arxiv.org abs 1803.03635 , destructive neural architecture search technique, others. goal feature harmonizing pruning practices providing standard interface pytorch. target audience researchers, engineering product teams. pitch minimalist api, deeper flexibility power users. tensor level, could look follows place method return type size one acts on. place pruning supported via . model level, require bit thinking follow similar api patterns. important pruning methods make sense parameters model pruning conv kernels ! pruning biases ! pruning rnns ! pruning presence batch norm, etc. . first, sensible, well documented default behavior average user's api, call defaults pruning pytorch prepackaged modules linear, conv, recurrent layers sensible, expected way. power users though would probably want prune custom layers, prune different layer types layers different depths using different pruning methods pruning method parameters. could specified via dictionary, maps parameter names contained pruning method parameters similar tensor operations, model level pruning could return copy model, act model place. pruning methods used post training implementation training loop agnostic user take care writing training loop decide prune pruned object initialize retrain, finetune, etc. . alternatives depending live within codebase, could also look like . personally would prefer first option kwargs parameters pruning method itself, tensor acts pruning methods also take data applied , last option line how, say, implemented. perhaps, module level application, following example implementation https pytorch.org docs stable modules torch nn utils weight norm.html using hooks make pytorch y, know want sacrifice ability act directly tensor part module. would go ? open suggestions here. cc alband mruberry jbschlosser"
pytorch,21004,"bug trying run fasterrcnn newly released torchvision 0.3 runs ptx jit compilation failed error. fails google cloud instance using gpu, fail using cpu reproduce steps reproduce behavior simple code fails error output expected behavior run environment additional context",0,ptx jit compilation failed running fasterrcnn,"ptx jit compilation failed running fasterrcnn bug trying run fasterrcnn newly released torchvision 0.3 runs ptx jit compilation failed error. fails google cloud instance using gpu, fail using cpu reproduce steps reproduce behavior simple code fails error output expected behavior run environment additional context"
pytorch,14365,"bug using nightly build 1.0.0.dev20181123. issue similar 13569 . instantiate two nn.embedding, dataparallel max norm 1.0, get following assert remove self.lut dummy, issue disappears. reproduce output 1.0.0.dev20181123 home software lm stash amitoj pytorch1.0 local lib python2.7 site packages torch nn parallel data parallel.py 25 userwarning imbalance gpus. may want exclude gpu 0 less 75 memory cores gpu 1. setting device ids argument dataparallel, setting cuda visible devices environment variable. warnings.warn imbalance warn.format device ids min pos , device ids max pos ok1 ok1 ok2 traceback recent call last file error train.py , line 25, main file error train.py , line 22, main output model src file home software lm stash amitoj pytorch1.0 local lib python2.7 site packages torch nn modules module.py , line 479, call result self.forward input, kwargs file home software lm stash amitoj pytorch1.0 local lib python2.7 site packages torch nn parallel data parallel.py , line 143, forward outputs self.parallel apply replicas, inputs, kwargs file home software lm stash amitoj pytorch1.0 local lib python2.7 site packages torch nn parallel data parallel.py , line 153, parallel apply return parallel apply replicas, inputs, kwargs, self.device ids len replicas file home software lm stash amitoj pytorch1.0 local lib python2.7 site packages torch nn parallel parallel apply.py , line 83, parallel apply raise output runtimeerror output nr 0 assert failed pytorch torch csrc autograd variable.cpp 196, please report bug pytorch. additional context observations 1. comment self.lut dummy, issue disappears. though real model, using . 2. max norm removed, issue disappears again. 3. set os.environ cuda visible devices 0 1 , again, code works fine. cc ezyang gchanan ssnl alband",0,assertion fails using dataparallel two nn.embedding,"assertion fails using dataparallel two nn.embedding bug using nightly build 1.0.0.dev20181123. issue similar 13569 . instantiate two nn.embedding, dataparallel max norm 1.0, get following assert remove self.lut dummy, issue disappears. reproduce output 1.0.0.dev20181123 home software lm stash amitoj pytorch1.0 local lib python2.7 site packages torch nn parallel data parallel.py 25 userwarning imbalance gpus. may want exclude gpu 0 less 75 memory cores gpu 1. setting device ids argument dataparallel, setting cuda visible devices environment variable. warnings.warn imbalance warn.format device ids min pos , device ids max pos ok1 ok1 ok2 traceback recent call last file error train.py , line 25, main file error train.py , line 22, main output model src file home software lm stash amitoj pytorch1.0 local lib python2.7 site packages torch nn modules module.py , line 479, call result self.forward input, kwargs file home software lm stash amitoj pytorch1.0 local lib python2.7 site packages torch nn parallel data parallel.py , line 143, forward outputs self.parallel apply replicas, inputs, kwargs file home software lm stash amitoj pytorch1.0 local lib python2.7 site packages torch nn parallel data parallel.py , line 153, parallel apply return parallel apply replicas, inputs, kwargs, self.device ids len replicas file home software lm stash amitoj pytorch1.0 local lib python2.7 site packages torch nn parallel parallel apply.py , line 83, parallel apply raise output runtimeerror output nr 0 assert failed pytorch torch csrc autograd variable.cpp 196, please report bug pytorch. additional context observations 1. comment self.lut dummy, issue disappears. though real model, using . 2. max norm removed, issue disappears again. 3. set os.environ cuda visible devices 0 1 , again, code works fine. cc ezyang gchanan ssnl alband"
pytorch,13850,"code works perfectly ipython jupyter notebook environment. training gan asynchronous loading data imagefolder dataloader. behaves expected. export code .py execute vscode inside conda environment started notebook memory quickly goes reaches . ram goes 100 aswell . using syntax windows using works epoch time increases 22x. everything 1 leads aforementioned ooms. theres issue copying ipython code vscode run .py file there, happening? system information pytorch version 0.4.1 debug build cuda used build pytorch 9.2 os microsoft windows 10 home gcc version could collect cmake version could collect python version 3.7 cuda available yes cuda runtime version 9.2.148 gpu models configuration gpu 0 geforce rtx 2070 nvidia driver version 416.81 cudnn version could collect versions relevant libraries pip could collect conda cuda92 1.0 0 pytorch conda pytorch 0.4.1 py37 cuda92 cudnn7he774522 1 cuda92 pytorch conda torchvision 0.2.1",0,num workers 0 leading oom non ipython environment,"num workers 0 leading oom non ipython environment code works perfectly ipython jupyter notebook environment. training gan asynchronous loading data imagefolder dataloader. behaves expected. export code .py execute vscode inside conda environment started notebook memory quickly goes reaches . ram goes 100 aswell . using syntax windows using works epoch time increases 22x. everything 1 leads aforementioned ooms. theres issue copying ipython code vscode run .py file there, happening? system information pytorch version 0.4.1 debug build cuda used build pytorch 9.2 os microsoft windows 10 home gcc version could collect cmake version could collect python version 3.7 cuda available yes cuda runtime version 9.2.148 gpu models configuration gpu 0 geforce rtx 2070 nvidia driver version 416.81 cudnn version could collect versions relevant libraries pip could collect conda cuda92 1.0 0 pytorch conda pytorch 0.4.1 py37 cuda92 cudnn7he774522 1 cuda92 pytorch conda torchvision 0.2.1"
pytorch,16437,"bug using half precision convolution pytorch nightly 20190125 , torch.backends.cudnn.deterministic true, cudnn status bad param runtime error occurs found using latest pytorch 1.0, using fp32, use torch.backends.cudnn.deterministic false script runs without problem. reproduce steps reproduce behavior expected behavior run without error environment pytorch version 1.0.0.dev20190125 debug build cuda used build pytorch 10.0.130 os ubuntu 16.04.5 lts gcc version ubuntu 5.4.0 6ubuntu1 16.04.10 5.4.0 20160609 cmake version version 3.5.1 python version 3.6 cuda available yes cuda runtime version 10.0.130 gpu models configuration gpu 0 tesla v100 pcie 32gb nvidia driver version 410.79 cudnn version probably one following usr lib x86 64 linux gnu libcudnn.so.7.4.1 usr lib x86 64 linux gnu libcudnn static v7.a versions relevant libraries pip could collect conda could collect",0,cudnn error using half precision convolution,"cudnn error using half precision convolution bug using half precision convolution pytorch nightly 20190125 , torch.backends.cudnn.deterministic true, cudnn status bad param runtime error occurs found using latest pytorch 1.0, using fp32, use torch.backends.cudnn.deterministic false script runs without problem. reproduce steps reproduce behavior expected behavior run without error environment pytorch version 1.0.0.dev20190125 debug build cuda used build pytorch 10.0.130 os ubuntu 16.04.5 lts gcc version ubuntu 5.4.0 6ubuntu1 16.04.10 5.4.0 20160609 cmake version version 3.5.1 python version 3.6 cuda available yes cuda runtime version 10.0.130 gpu models configuration gpu 0 tesla v100 pcie 32gb nvidia driver version 410.79 cudnn version probably one following usr lib x86 64 linux gnu libcudnn.so.7.4.1 usr lib x86 64 linux gnu libcudnn static v7.a versions relevant libraries pip could collect conda could collect"
pytorch,9172,"issue description permuting axis tensor using option breaks kernel tested jupyter backward pass. kernel restart, error provided. code example code automatically breaks kernel jupyter notebook run jupyterlab . note following code worked well. system info pytorch version 0.3.1 debug build cuda used build pytorch 9.0.176 os ubuntu 16.04.3 lts gcc version ubuntu 5.4.0 6ubuntu1 16.04.10 5.4.0 20160609 cmake version version 3.5.1 python version 3.5 cuda available yes cuda runtime version 9.0.176 gpu models configuration gpu 0 tesla k80 nvidia driver version 384.111 cudnn version probably one following usr local cuda 8.0 lib64 libcudnn.so.6.0.21 usr local cuda 8.0 lib64 libcudnn static.a usr local cuda 9.0 lib64 libcudnn.so.7.0.5 usr local cuda 9.0 lib64 libcudnn static.a usr local lib python3.5 dist packages torch lib libcudnn 7b07b0f1.so.7.0.5 versions relevant libraries pip3 msgpack numpy 0.4.1 pip3 numpy 1.14.0 pip3 torch 0.3.1 pip3 torchvision 0.2.0 conda could collect",0,kernel breaks backward pass .permute 1 option,"kernel breaks backward pass .permute 1 option issue description permuting axis tensor using option breaks kernel tested jupyter backward pass. kernel restart, error provided. code example code automatically breaks kernel jupyter notebook run jupyterlab . note following code worked well. system info pytorch version 0.3.1 debug build cuda used build pytorch 9.0.176 os ubuntu 16.04.3 lts gcc version ubuntu 5.4.0 6ubuntu1 16.04.10 5.4.0 20160609 cmake version version 3.5.1 python version 3.5 cuda available yes cuda runtime version 9.0.176 gpu models configuration gpu 0 tesla k80 nvidia driver version 384.111 cudnn version probably one following usr local cuda 8.0 lib64 libcudnn.so.6.0.21 usr local cuda 8.0 lib64 libcudnn static.a usr local cuda 9.0 lib64 libcudnn.so.7.0.5 usr local cuda 9.0 lib64 libcudnn static.a usr local lib python3.5 dist packages torch lib libcudnn 7b07b0f1.so.7.0.5 versions relevant libraries pip3 msgpack numpy 0.4.1 pip3 numpy 1.14.0 pip3 torch 0.3.1 pip3 torchvision 0.2.0 conda could collect"
pytorch,31698,questions help please note issue tracker help form issue closed. set listed resources available website https pytorch.org resources . primary means support discussion forum discussion forum https discuss.pytorch.org,0,can't iter dataset imported hdf5,can't iter dataset imported hdf5 questions help please note issue tracker help form issue closed. set listed resources available website https pytorch.org resources . primary means support discussion forum discussion forum https discuss.pytorch.org
pytorch,473,happens index tensor contains duplicate elements. allowed index copy raise exception happens fix test. otherwise need fix autograd op. here's example cc ezyang ssnl alband,0,autograd indexcopy broken,autograd indexcopy broken happens index tensor contains duplicate elements. allowed index copy raise exception happens fix test. otherwise need fix autograd op. here's example cc ezyang ssnl alband
pytorch,25775,bug broken yet works. error message confusing well. ! screenshot 2019 09 06 11 11 08 https user images.githubusercontent.com 5674597 64439027 1efc4880 d097 11e9 8c00 eb420962d78e.png,0,mean sum dtype arg matching gives bad error message positional dtype arg,mean sum dtype arg matching gives bad error message positional dtype arg bug broken yet works. error message confusing well. ! screenshot 2019 09 06 11 11 08 https user images.githubusercontent.com 5674597 64439027 1efc4880 d097 11e9 8c00 eb420962d78e.png
pytorch,31317,"far understand, time stft natively supported onnx https github.com onnx onnx blob master docs operators.md stft exported conv1d op precomputed windowed fourier basis https github.com nvidia mellotron blob master stft.py useful tracking exporting speech recongition models. done pure pytorch version https github.com vadimkantorov convasr blob master models.py l315 l334 seems work cc houseroad spandantiwari lara hdr bowenbao neginraoof",0,feature request onnx export torch.stft conv1d till onnx supports stft op,"feature request onnx export torch.stft conv1d till onnx supports stft op far understand, time stft natively supported onnx https github.com onnx onnx blob master docs operators.md stft exported conv1d op precomputed windowed fourier basis https github.com nvidia mellotron blob master stft.py useful tracking exporting speech recongition models. done pure pytorch version https github.com vadimkantorov convasr blob master models.py l315 l334 seems work cc houseroad spandantiwari lara hdr bowenbao neginraoof"
pytorch,22277,see 22032 20866 rationale 22073 example.,0,handle intarrayref expansions aten,handle intarrayref expansions aten see 22032 20866 rationale 22073 example.
pytorch,22049,bug use multiple gpu loss calculated part parameters. get following errors. use one gpu works well. reproduce steps reproduce behavior define network loss depends part parameters. get find unused parameters truetorch.nn.parallel.distributeddataparallelforwardforwardforward expected behavior environment pytorch version 1.2.0.dev20190620 cuda used build pytorch 9.0.176 os centos linux release 7.5.1804 core gcc version crosstool ng 1.23.0.449 a04d0 7.3.0 cmake version version 2.8.12.2 python version 3.6 cuda available yes cuda runtime version could collect gpu models configuration gpu 0 geforce gtx 1080 gpu 1 geforce gtx 1080 gpu 2 geforce gtx 1080 gpu 3 geforce gtx 1080 nvidia driver version 396.26 cudnn version could collect versions relevant libraries pip3 msgpack numpy 0.4.3.2 pip3 numpy 1.15.4 pip3 pytorch pretrained bert 0.4.0 pip3 torch 1.0.1.post2 pip3 torchfile 0.1.0 pip3 torchtext 0.4.0 pip3 torchvision nightly 0.2.1 conda pytorch pretrained bert 0.6.2 pypi 0 pypi conda torch nightly 1.2.0.dev20190620 pypi 0 pypi conda torchfile 0.1.0 pypi 0 pypi conda torchtext 0.4.0 pypi 0 pypi,0,cannot update part parameters distributeddataparallel.,cannot update part parameters distributeddataparallel. bug use multiple gpu loss calculated part parameters. get following errors. use one gpu works well. reproduce steps reproduce behavior define network loss depends part parameters. get find unused parameters truetorch.nn.parallel.distributeddataparallelforwardforwardforward expected behavior environment pytorch version 1.2.0.dev20190620 cuda used build pytorch 9.0.176 os centos linux release 7.5.1804 core gcc version crosstool ng 1.23.0.449 a04d0 7.3.0 cmake version version 2.8.12.2 python version 3.6 cuda available yes cuda runtime version could collect gpu models configuration gpu 0 geforce gtx 1080 gpu 1 geforce gtx 1080 gpu 2 geforce gtx 1080 gpu 3 geforce gtx 1080 nvidia driver version 396.26 cudnn version could collect versions relevant libraries pip3 msgpack numpy 0.4.3.2 pip3 numpy 1.15.4 pip3 pytorch pretrained bert 0.4.0 pip3 torch 1.0.1.post2 pip3 torchfile 0.1.0 pip3 torchtext 0.4.0 pip3 torchvision nightly 0.2.1 conda pytorch pretrained bert 0.6.2 pypi 0 pypi conda torch nightly 1.2.0.dev20190620 pypi 0 pypi conda torchfile 0.1.0 pypi 0 pypi conda torchtext 0.4.0 pypi 0 pypi
pytorch,22013,"bug indexing tensor 2d list indices seems fail sometimes, critical point number indices less 32. reproduce fails expected behavior expected indexing work number indices. problematic actual code size data varies. environment pytorch version 1.1.0 debug build cuda used build pytorch 10.0.130 os ubuntu 16.04.6 lts gcc version ubuntu 5.4.0 6ubuntu1 16.04.11 5.4.0 20160609 cmake version could collect python version 3.6 cuda available cuda runtime version cuda gpu models configuration cuda nvidia driver version cuda cudnn version cuda versions relevant libraries pip3 numpy 1.16.4 pip3 pytorch pretrained bert 0.6.2 pip3 torch 1.1.0 conda could collect additional context cc mruberry rgommers heitorschueroff",0,mysterious tensor indexing problem,"mysterious tensor indexing problem bug indexing tensor 2d list indices seems fail sometimes, critical point number indices less 32. reproduce fails expected behavior expected indexing work number indices. problematic actual code size data varies. environment pytorch version 1.1.0 debug build cuda used build pytorch 10.0.130 os ubuntu 16.04.6 lts gcc version ubuntu 5.4.0 6ubuntu1 16.04.11 5.4.0 20160609 cmake version could collect python version 3.6 cuda available cuda runtime version cuda gpu models configuration cuda nvidia driver version cuda cudnn version cuda versions relevant libraries pip3 numpy 1.16.4 pip3 pytorch pretrained bert 0.6.2 pip3 torch 1.1.0 conda could collect additional context cc mruberry rgommers heitorschueroff"
pytorch,28306,"bug reports issues exporting models . reproduce steps reproduce behavior enable case 2 case 4 run example errors reported torch.onnx exporter case 2 cases 3 4 expected behavior onnx randomuniform export must supported torch onnx model must generated cases example script. environment pytorch version 3.6 os e.g., linux ubuntu 18.04.3 lts installed pytorch , , source pip build command used compiling source na python version 3.6 cuda cudnn version cuda 10.1 cudnn 7.6.3 gpu models configuration gpu 0 geforce gtx 1070 ti gpu 1 titan v relevant information pip torch 1.3.0 pip torchvision 0.4.1 additional context na cc houseroad spandantiwari lara hdr bowenbao neginraoof",0,torch onnx export broken randomuniform randomuniformlike,"torch onnx export broken randomuniform randomuniformlike bug reports issues exporting models . reproduce steps reproduce behavior enable case 2 case 4 run example errors reported torch.onnx exporter case 2 cases 3 4 expected behavior onnx randomuniform export must supported torch onnx model must generated cases example script. environment pytorch version 3.6 os e.g., linux ubuntu 18.04.3 lts installed pytorch , , source pip build command used compiling source na python version 3.6 cuda cudnn version cuda 10.1 cudnn 7.6.3 gpu models configuration gpu 0 geforce gtx 1070 ti gpu 1 titan v relevant information pip torch 1.3.0 pip torchvision 0.4.1 additional context na cc houseroad spandantiwari lara hdr bowenbao neginraoof"
pytorch,14726,"feature corresponding c api prepare prediction net motivation python api able load predictor models metanetdef format. https github.com pytorch pytorch blob edb88b5f3af03718b443d015f195faa1832ce95b caffe2 python predictor predictor exporter.py l127 however, corresponding c api missing. pitch python world, able export load models metanetdef format, however, want productionize model load c , api missing. alternatives right now, alternative export init predict nets seperately protobufs load c .",0,caffe2 corresponding c api prepare prediction net,"caffe2 corresponding c api prepare prediction net feature corresponding c api prepare prediction net motivation python api able load predictor models metanetdef format. https github.com pytorch pytorch blob edb88b5f3af03718b443d015f195faa1832ce95b caffe2 python predictor predictor exporter.py l127 however, corresponding c api missing. pitch python world, able export load models metanetdef format, however, want productionize model load c , api missing. alternatives right now, alternative export init predict nets seperately protobufs load c ."
pytorch,19126,"bug backprop weights generated torch. weight norm zero filled yields nan gradients. see way add eta norm prevent this. reproduce steps reproduce behavior encountering nan's backprop training network weight normalization. seemingly related thread sounds like advice add eta norm, case norm generated pytorch's c implementation see obvious way this. expected behavior expect way generate non nan gradients weight norm weights zero filled environment pytorch version e.g., 1.0 1.0.0 os e.g., linux mac centos installed pytorch , , source pip build command used compiling source python version 3.6 cuda cudnn version none gpu models configuration none relevant information additional context cc alband mruberry",0,weight norm support eta returns nan zero weights,"weight norm support eta returns nan zero weights bug backprop weights generated torch. weight norm zero filled yields nan gradients. see way add eta norm prevent this. reproduce steps reproduce behavior encountering nan's backprop training network weight normalization. seemingly related thread sounds like advice add eta norm, case norm generated pytorch's c implementation see obvious way this. expected behavior expect way generate non nan gradients weight norm weights zero filled environment pytorch version e.g., 1.0 1.0.0 os e.g., linux mac centos installed pytorch , , source pip build command used compiling source python version 3.6 cuda cudnn version none gpu models configuration none relevant information additional context cc alband mruberry"
pytorch,14685,"bug getting following error use squaredl2distance operator output layer cnn network 1st attempt 28train net tried fix error converting float even though already float32, see using cast operator 2nd attempt however, got following error furthermore, created lmdb training dataset stores image data uint8 label multivalue float64 key 00000001 image data shape 210, 280, 3 type uint8 indicators shape 14, type float64 fix error? reproduce steps reproduce behavior 1. project consists two files trainer creator. run trainer calls train function creator. trainer.py cnncreator dpnet dpnet.py expected behavior execute squaredl2distance output layer cnn network environment please copy paste output environment collection script https raw.githubusercontent.com pytorch pytorch master torch utils collect env.py fill checklist manually . get script run pytorch version e.g., 1.0 caffer2 tag v0.4.0 os e.g., linux ubuntu 16.04 installed pytorch conda, pip, source build source tag v0.4.0 build command used compiling source python version python 2.7 cuda cudnn version 8.0 7.0.5 gpu models configuration gtx 1050 relevant information additional context",0,caffe2 exception creating gradient cast squaredl2distance output layer cnn network,"caffe2 exception creating gradient cast squaredl2distance output layer cnn network bug getting following error use squaredl2distance operator output layer cnn network 1st attempt 28train net tried fix error converting float even though already float32, see using cast operator 2nd attempt however, got following error furthermore, created lmdb training dataset stores image data uint8 label multivalue float64 key 00000001 image data shape 210, 280, 3 type uint8 indicators shape 14, type float64 fix error? reproduce steps reproduce behavior 1. project consists two files trainer creator. run trainer calls train function creator. trainer.py cnncreator dpnet dpnet.py expected behavior execute squaredl2distance output layer cnn network environment please copy paste output environment collection script https raw.githubusercontent.com pytorch pytorch master torch utils collect env.py fill checklist manually . get script run pytorch version e.g., 1.0 caffer2 tag v0.4.0 os e.g., linux ubuntu 16.04 installed pytorch conda, pip, source build source tag v0.4.0 build command used compiling source python version python 2.7 cuda cudnn version 8.0 7.0.5 gpu models configuration gtx 1050 relevant information additional context"
pytorch,21683,"feature importing onnx models pytorch. motivation almost frameworks already support this. importing onnx models pytorch makes pytorch much flexible. pitch , function created take onnx model outputs pytorch model.",0,import onnx model pytorch,"import onnx model pytorch feature importing onnx models pytorch. motivation almost frameworks already support this. importing onnx models pytorch makes pytorch much flexible. pitch , function created take onnx model outputs pytorch model."
pytorch,31300,"bug manual link libmkldnn.so, want use functions inside mkldnn, pytorch get runtime error manual link libmkldnn.so reproduce steps reproduce behavior get runtime error expected behavior runtime error environment pytorch version 1.3.0 debug build cuda used build pytorch 10.1.243 os ubuntu 16.04.3 lts gcc version ubuntu 5.4.0 6ubuntu1 16.04.12 5.4.0 20160609 cmake version version 3.5.1 python version 3.7 cuda available cuda runtime version cuda gpu models configuration cuda nvidia driver version cuda cudnn version cuda versions relevant libraries pip3 numpy 1.17.3 pip3 torch 1.3.0 pip3 torchvision 0.4.1 conda could collect cc gujinghui penghuicheng xiaobingsuper jianyuh",0,torch runtime error manual link libmkldnn.so,"torch runtime error manual link libmkldnn.so bug manual link libmkldnn.so, want use functions inside mkldnn, pytorch get runtime error manual link libmkldnn.so reproduce steps reproduce behavior get runtime error expected behavior runtime error environment pytorch version 1.3.0 debug build cuda used build pytorch 10.1.243 os ubuntu 16.04.3 lts gcc version ubuntu 5.4.0 6ubuntu1 16.04.12 5.4.0 20160609 cmake version version 3.5.1 python version 3.7 cuda available cuda runtime version cuda gpu models configuration cuda nvidia driver version cuda cudnn version cuda versions relevant libraries pip3 numpy 1.17.3 pip3 torch 1.3.0 pip3 torchvision 0.4.1 conda could collect cc gujinghui penghuicheng xiaobingsuper jianyuh"
pytorch,28249,"feature add scopes within torchscript model get profile information per scope forward backward pass. motivation distributed execution model parallelism want able measure exact cpu time spent different parts torchscript module shard model reasonable manner different machines. high level components represent shardable units execution want use autograd profiler profile per component. pitch want add two custom methods torch describe scope. making c allows us use torchscript well python. torchscript pseudocode python sugar methods would add scope information recordfunction thus provide autograd profiler. https github.com pytorch pytorch blob master torch csrc autograd record function.h current record function implementation appears track lineage backwards functions forward pass via sequence nr easy walk recordfunction tree extract scope information. https github.com pytorch pytorch blob master torch csrc autograd function.h l104 l116 https github.com pytorch pytorch blob master tools autograd gen variable type.py l238 one notable thing sequence nr currently thread local. heard, autograd backward pass multithreaded thus likely need extend sequence nr include thread id well current thread local sequence nr. alternatives there's ways running python environment w register backward hook custom nn.module however there's equivalent way torchscript since support register backward hook. autograd backward pass also run multithreaded understanding would break approach anyways.",0,add scopes autograd profiler,"add scopes autograd profiler feature add scopes within torchscript model get profile information per scope forward backward pass. motivation distributed execution model parallelism want able measure exact cpu time spent different parts torchscript module shard model reasonable manner different machines. high level components represent shardable units execution want use autograd profiler profile per component. pitch want add two custom methods torch describe scope. making c allows us use torchscript well python. torchscript pseudocode python sugar methods would add scope information recordfunction thus provide autograd profiler. https github.com pytorch pytorch blob master torch csrc autograd record function.h current record function implementation appears track lineage backwards functions forward pass via sequence nr easy walk recordfunction tree extract scope information. https github.com pytorch pytorch blob master torch csrc autograd function.h l104 l116 https github.com pytorch pytorch blob master tools autograd gen variable type.py l238 one notable thing sequence nr currently thread local. heard, autograd backward pass multithreaded thus likely need extend sequence nr include thread id well current thread local sequence nr. alternatives there's ways running python environment w register backward hook custom nn.module however there's equivalent way torchscript since support register backward hook. autograd backward pass also run multithreaded understanding would break approach anyways."
pytorch,28884,"feature think shoud add 3d attn mask. motivation example, translation task, want let every word target sentence focus different words source sentence, attn mask shape tgt len, scr len , ok! that's fine, that's exactly now. different sentencesthe target word position may focus different source words, broadcast unsuitable case attn mask shape batch size, tgt len, scr len . question also nn.transfomers. think necessary fix. cc alband mruberry jbschlosser zhangguanheng66",0,attn mask 3d tensor nn.multiheadattention?,"attn mask 3d tensor nn.multiheadattention? feature think shoud add 3d attn mask. motivation example, translation task, want let every word target sentence focus different words source sentence, attn mask shape tgt len, scr len , ok! that's fine, that's exactly now. different sentencesthe target word position may focus different source words, broadcast unsuitable case attn mask shape batch size, tgt len, scr len . question also nn.transfomers. think necessary fix. cc alband mruberry jbschlosser zhangguanheng66"
pytorch,11532,example calls emit e.g. specialized rank tensor called,0,jit tracer slicing shape specialized tensor rank,jit tracer slicing shape specialized tensor rank example calls emit e.g. specialized rank tensor called
pytorch,20591,"feature user defined type, would nice ignore annotation similar https github.com pytorch pytorch pull 16055 allows user write python methods used training debugging like cc suo",0,ignore annotation user defined type,"ignore annotation user defined type feature user defined type, would nice ignore annotation similar https github.com pytorch pytorch pull 16055 allows user write python methods used training debugging like cc suo"
pytorch,7214,"seeing today paths taken build environment, belong rpath since paths shipped application. paths may contain versions dt needed libraries want use pytorch, however rpath precedence everything else, libraries get picked, way around that. example, let's say libcublas provided version want since old, newest version , way point since ignored. caused https github.com pytorch pytorch pull 3255 cc malfet seemethere walterddr",0,put system paths rpath,"put system paths rpath seeing today paths taken build environment, belong rpath since paths shipped application. paths may contain versions dt needed libraries want use pytorch, however rpath precedence everything else, libraries get picked, way around that. example, let's say libcublas provided version want since old, newest version , way point since ignored. caused https github.com pytorch pytorch pull 3255 cc malfet seemethere walterddr"
pytorch,12855,"firstly, run model crnn model pytorch gpu. code result cost time 0.002339872884750366 secondly, convert pytorch model onnx convert onnx pb caffe2. code thirdly,i run model caffe2 gpu. code run time per runnet 0.005980247449874878 but, speed slower pytorch ! weird! one konws happens!",0,model caffe2 runs much slower pytorch gpu mode !!!!,"model caffe2 runs much slower pytorch gpu mode !!!! firstly, run model crnn model pytorch gpu. code result cost time 0.002339872884750366 secondly, convert pytorch model onnx convert onnx pb caffe2. code thirdly,i run model caffe2 gpu. code run time per runnet 0.005980247449874878 but, speed slower pytorch ! weird! one konws happens!"
pytorch,28882,"necessary syntax sugar following use case implement sending message owner, triggering owner run locally, returning immediately another rref output. cc pietern mrshenli pritamdamania87 zhaojuanmao satgera rohan varma gqchen aazzolini xush6528",0,support rref . call args invokes t. call args owner,"support rref . call args invokes t. call args owner necessary syntax sugar following use case implement sending message owner, triggering owner run locally, returning immediately another rref output. cc pietern mrshenli pritamdamania87 zhaojuanmao satgera rohan varma gqchen aazzolini xush6528"
pytorch,30413,"feature provide functional alternative every stateful component supports it. some, like deprecated . motivation people usually import modules submodules used often short intuitive names. one name often encountered , vein importing . current plan move math functions permanently away somewhat awkwardly splits api. 1. every operation name knowledge . 2. almost every operation exposed cleanly functional manner name . don't. pitch move 'mathy' functions class counterparts expose aliases basic fundamental ones . get ugly territory know exactly argue methods currently , , co definitely belong . alternatives flip implementation exporting tale impls exports . additional context e.g. https github.com pytorch pytorch issues 6245 deprecate torch.nn.functional.tanh?",0,nn.functional maintain api parity nn possible,"nn.functional maintain api parity nn possible feature provide functional alternative every stateful component supports it. some, like deprecated . motivation people usually import modules submodules used often short intuitive names. one name often encountered , vein importing . current plan move math functions permanently away somewhat awkwardly splits api. 1. every operation name knowledge . 2. almost every operation exposed cleanly functional manner name . don't. pitch move 'mathy' functions class counterparts expose aliases basic fundamental ones . get ugly territory know exactly argue methods currently , , co definitely belong . alternatives flip implementation exporting tale impls exports . additional context e.g. https github.com pytorch pytorch issues 6245 deprecate torch.nn.functional.tanh?"
pytorch,21459,"runtimeerror cublas runtime error gpu program failed execute pytorch aten src thc thcblas.cu 411 gpu 2080ti,cuda 10.0 slove problem?thank you.",0,runtimeerror cublas runtime error,"runtimeerror cublas runtime error runtimeerror cublas runtime error gpu program failed execute pytorch aten src thc thcblas.cu 411 gpu 2080ti,cuda 10.0 slove problem?thank you."
pytorch,9853,reference inputs threshold sample https ci.pytorch.org jenkins job caffe2 builds job py2 cuda9.0 cudnn7 aten ubuntu16.04 test 4954 consoletext,0,testsequenceops.test gather padding failing,testsequenceops.test gather padding failing reference inputs threshold sample https ci.pytorch.org jenkins job caffe2 builds job py2 cuda9.0 cudnn7 aten ubuntu16.04 test 4954 consoletext
pytorch,2129,"recent discussion discuss truncated normal initializer link https discuss.pytorch.org implementing truncated normal initializer 4778 15 . implemented one own, code basically paraphrasing tensorflow code, feel like may cause problem. feel good add this, submit pr tell ok paraphrase what's alternative.",0,feature request truncated normal initializer sampler,"feature request truncated normal initializer sampler recent discussion discuss truncated normal initializer link https discuss.pytorch.org implementing truncated normal initializer 4778 15 . implemented one own, code basically paraphrasing tensorflow code, feel like may cause problem. feel good add this, submit pr tell ok paraphrase what's alternative."
pytorch,7569,"facebookresearch detectron, see conv layer add conv rpn fpn model.conv bl in, 'conv rpn fpn' slvl, dim in, dim out, kernel 3, pad 1, stride 1, weight init gauss fill 0.01 , bias init const fill 0.0 however, find parameter statement lr mult decay mult like caffe conv layer. could please give example? thanks lot! question would like help support, please ask forums https discuss.pytorch.org . submitting feature request, please preface title feature request . submitting bug report, please fill following details. issue description provide short description. code example please try provide minimal example repro bug. error messages stack traces also helpful. system info please copy paste output environment collection script https raw.githubusercontent.com pytorch pytorch master torch utils collect env.py fill checklist manually . get script run pytorch caffe2 installed pytorch conda, pip, source build command used compiling source os pytorch version python version cuda cudnn version gpu models configuration gcc version compiling source cmake version versions relevant libraries",0,caffe2 set lr mult decay mult conv layer?,"caffe2 set lr mult decay mult conv layer? facebookresearch detectron, see conv layer add conv rpn fpn model.conv bl in, 'conv rpn fpn' slvl, dim in, dim out, kernel 3, pad 1, stride 1, weight init gauss fill 0.01 , bias init const fill 0.0 however, find parameter statement lr mult decay mult like caffe conv layer. could please give example? thanks lot! question would like help support, please ask forums https discuss.pytorch.org . submitting feature request, please preface title feature request . submitting bug report, please fill following details. issue description provide short description. code example please try provide minimal example repro bug. error messages stack traces also helpful. system info please copy paste output environment collection script https raw.githubusercontent.com pytorch pytorch master torch utils collect env.py fill checklist manually . get script run pytorch caffe2 installed pytorch conda, pip, source build command used compiling source os pytorch version python version cuda cudnn version gpu models configuration gcc version compiling source cmake version versions relevant libraries"
pytorch,14844,feature negative indexing nn.embedding inputs motivation found allow negative indexing padding. allow negative indexing inputs well? example cc alband mruberry,0,negative indexing nn.embedding inputs,negative indexing nn.embedding inputs feature negative indexing nn.embedding inputs motivation found allow negative indexing padding. allow negative indexing inputs well? example cc alband mruberry
pytorch,9484,"hi, gpu cuda, cudnn, nccl. os ubuntu 16.04, followed tutorial install coffe2 gpu support conda install c caffe2 caffe2 cuda9.0 cudnn7 installation finished successfully command python2 c 'from caffe2.python import workspace print workspace.numcudadevices ' returns warning root caffe2 python run gpu support. run cpu mode. warning root debug message libcurand.so.9.0 cannot open shared object file file directory segmentation fault core dumped idea solve it?? tnx nvcc version nvcc nvidia r cuda compiler driver copyright c 2005 2018 nvidia corporation built tue jun 12 23 07 04 cdt 2018 cuda compilation tools, release 9.2, v9.2.148 conda v conda 4.5.8 nvidia smi tue jul 17 01 58 01 2018 nvidia smi 384.130 driver version 384.130 gpu name persistence bus id disp.a volatile uncorr. ecc fan temp perf pwr usage cap memory usage gpu util compute m. 0 quadro m520 00000000 02 00.0 n n 41c p0 n n 298mib 2002mib 2 default processes gpu memory gpu pid type process name usage 0 21578 g usr lib xorg xorg 234mib 0 22157 g compiz 62mib",0,warning root caffe2 python run gpu support. run cpu mode. warning root debug message libcurand.so.9.0 cannot open shared object file file directory segmentation fault core dumped,"warning root caffe2 python run gpu support. run cpu mode. warning root debug message libcurand.so.9.0 cannot open shared object file file directory segmentation fault core dumped hi, gpu cuda, cudnn, nccl. os ubuntu 16.04, followed tutorial install coffe2 gpu support conda install c caffe2 caffe2 cuda9.0 cudnn7 installation finished successfully command python2 c 'from caffe2.python import workspace print workspace.numcudadevices ' returns warning root caffe2 python run gpu support. run cpu mode. warning root debug message libcurand.so.9.0 cannot open shared object file file directory segmentation fault core dumped idea solve it?? tnx nvcc version nvcc nvidia r cuda compiler driver copyright c 2005 2018 nvidia corporation built tue jun 12 23 07 04 cdt 2018 cuda compilation tools, release 9.2, v9.2.148 conda v conda 4.5.8 nvidia smi tue jul 17 01 58 01 2018 nvidia smi 384.130 driver version 384.130 gpu name persistence bus id disp.a volatile uncorr. ecc fan temp perf pwr usage cap memory usage gpu util compute m. 0 quadro m520 00000000 02 00.0 n n 41c p0 n n 298mib 2002mib 2 default processes gpu memory gpu pid type process name usage 0 21578 g usr lib xorg xorg 234mib 0 22157 g compiz 62mib"
pytorch,15260,"bug runtime gru multi gpu environment, runtimeerror expected hidden size 3, 64, 12 , got 3, 16, 12 first, second, third arguments number gru layers, batch size number hidden units respectively. model run well single gpu environment. pytorch forum, question dataparallel lstm gru wrong hidden batch size 8 gpus asked. one solution set batch first true gru. error still setting. correct solution store hidden state inside model rather return like code block user anodynecodeasher newcomer wrong code sample please help fix bug. thanks lot. environment pytorch version 1,0 os ubuntu 16.04 installed pytorch python version 3.6 cuda cudnn version 9.0 cc zou3519",0,multigpu gru,"multigpu gru bug runtime gru multi gpu environment, runtimeerror expected hidden size 3, 64, 12 , got 3, 16, 12 first, second, third arguments number gru layers, batch size number hidden units respectively. model run well single gpu environment. pytorch forum, question dataparallel lstm gru wrong hidden batch size 8 gpus asked. one solution set batch first true gru. error still setting. correct solution store hidden state inside model rather return like code block user anodynecodeasher newcomer wrong code sample please help fix bug. thanks lot. environment pytorch version 1,0 os ubuntu 16.04 installed pytorch python version 3.6 cuda cudnn version 9.0 cc zou3519"
pytorch,4689,"currently use magma solvers, uses mix cpu gpu. wonder cusolver faster, avoid cpu path entirely. cc ngimel want confirm, cusolver dense wont use cpu right? cc ngimel vincentqb vishwakftw ssnl jianyuh",0,switch cuda svd qr using cusolver,"switch cuda svd qr using cusolver currently use magma solvers, uses mix cpu gpu. wonder cusolver faster, avoid cpu path entirely. cc ngimel want confirm, cusolver dense wont use cpu right? cc ngimel vincentqb vishwakftw ssnl jianyuh"
pytorch,29842,pytorch linux xenial cuda9 cudnn7 py3 test https app.circleci.com jobs github pytorch pytorch 3600910 suo cc ezyang gchanan zou3519 jerryzh168 suo,0,ci timeout running test async grad guard grad jit.test async.testasync,ci timeout running test async grad guard grad jit.test async.testasync pytorch linux xenial cuda9 cudnn7 py3 test https app.circleci.com jobs github pytorch pytorch 3600910 suo cc ezyang gchanan zou3519 jerryzh168 suo
pytorch,23512,"per discussion https github.com pytorch pytorch pull 23323 issuecomment 515168182 currently plainly additional build options passed setup.py clean tree configuration configuration build invoke setup.py plainly rebuild reconfiguration without rebuild unreliable , edit cmakecache.txt run cmake directly rebuild without reconfiguration invoke setup.py plainly, build options persist reconfiguration rebuild unreliable , edit cmakecache.txt invoke setup.py plainly need make two unreliable spots reliable .",0,build reconfiguration consistently honor env variables,"build reconfiguration consistently honor env variables per discussion https github.com pytorch pytorch pull 23323 issuecomment 515168182 currently plainly additional build options passed setup.py clean tree configuration configuration build invoke setup.py plainly rebuild reconfiguration without rebuild unreliable , edit cmakecache.txt run cmake directly rebuild without reconfiguration invoke setup.py plainly, build options persist reconfiguration rebuild unreliable , edit cmakecache.txt invoke setup.py plainly need make two unreliable spots reliable ."
pytorch,20704,function torch csrc nn type checks.h used anymore already killed torch.legacy.nn normal nn ops go aten bindings right now. proper removal function would also involve removing caller functions.,0,remove unpack torch csrc nn type checks.h caller functions codebase,remove unpack torch csrc nn type checks.h caller functions codebase function torch csrc nn type checks.h used anymore already killed torch.legacy.nn normal nn ops go aten bindings right now. proper removal function would also involve removing caller functions.
pytorch,17850,"bug pytorch fails build issue related mkl dnn. previously built mkl dnn ing github repo. error trace build command environment pytorch version e.g., 1.0 master branch os e.g., linux debian stretch installed pytorch , , source source build command used compiling source see python version 3.6.5 cuda cudnn version na gpu models configuration na relevant information na",0,build fails caffe2 cmakefiles caffe2.dir aten src aten native mkldnn conv.cpp.o possibly mkl dnn issue,"build fails caffe2 cmakefiles caffe2.dir aten src aten native mkldnn conv.cpp.o possibly mkl dnn issue bug pytorch fails build issue related mkl dnn. previously built mkl dnn ing github repo. error trace build command environment pytorch version e.g., 1.0 master branch os e.g., linux debian stretch installed pytorch , , source source build command used compiling source see python version 3.6.5 cuda cudnn version na gpu models configuration na relevant information na"
pytorch,28206,"bug reproduce steps reproduce behavior 1.run script see trace take bug 3 example expected behavior writer.add graph run normally. environment please copy paste output environment collection script https raw.githubusercontent.com pytorch pytorch master torch utils collect env.py fill checklist manually . get script run collecting environment information... pytorch version 1.3.0 debug build cuda used build pytorch none os mac osx 10.14.6 gcc version could collect cmake version could collect python version 3.7 cuda available cuda runtime version cuda gpu models configuration cuda nvidia driver version cuda cudnn version cuda versions relevant libraries pip numpy 1.17.2 pip torch 1.3.0 pip torchvision 0.4.1 conda torch 1.3.0 pypi 0 pypi conda torchvision 0.4.1 pypi 0 pypi additional context 1.tensorboardx.summarywriter.add graph bug torch.utils.tensorboard 2.besides bug, hope add graph could accept tuple positional arguments, also dict keyword arguments model.forward 's input",0,torch.utils.tensorboard.summarywriter.add graph support non tensor inputs,"torch.utils.tensorboard.summarywriter.add graph support non tensor inputs bug reproduce steps reproduce behavior 1.run script see trace take bug 3 example expected behavior writer.add graph run normally. environment please copy paste output environment collection script https raw.githubusercontent.com pytorch pytorch master torch utils collect env.py fill checklist manually . get script run collecting environment information... pytorch version 1.3.0 debug build cuda used build pytorch none os mac osx 10.14.6 gcc version could collect cmake version could collect python version 3.7 cuda available cuda runtime version cuda gpu models configuration cuda nvidia driver version cuda cudnn version cuda versions relevant libraries pip numpy 1.17.2 pip torch 1.3.0 pip torchvision 0.4.1 conda torch 1.3.0 pypi 0 pypi conda torchvision 0.4.1 pypi 0 pypi additional context 1.tensorboardx.summarywriter.add graph bug torch.utils.tensorboard 2.besides bug, hope add graph could accept tuple positional arguments, also dict keyword arguments model.forward 's input"
pytorch,14839,"tested, evidence https github.com pytorch pytorch pull 14356 cc ezyang seemethere malfet walterddr pytorch pytorch dev infra",0,test opencv4 ci,"test opencv4 ci tested, evidence https github.com pytorch pytorch pull 14356 cc ezyang seemethere malfet walterddr pytorch pytorch dev infra"
pytorch,28323,currently hypothesis semi random ci. need fix seed something constant. cc jerryzh168 jianyuh dzhulgakov raghuramank100,0,quantization fix seed hypothesis ci,quantization fix seed hypothesis ci currently hypothesis semi random ci. need fix seed something constant. cc jerryzh168 jianyuh dzhulgakov raghuramank100
pytorch,6257,"first mentioned here, turns non existent. https github.com pytorch pytorch pull 6225 pullrequestreview 108887248 operation would simply add new index new value sparse tensor. cc vincentqb aocsa nikitaved pearu mruberry",0,feature request adding nonzero element place sparse tensor,"feature request adding nonzero element place sparse tensor first mentioned here, turns non existent. https github.com pytorch pytorch pull 6225 pullrequestreview 108887248 operation would simply add new index new value sparse tensor. cc vincentqb aocsa nikitaved pearu mruberry"
pytorch,21015,"trying build pytorch source cpu cluster backend gloo. installing pytorch, got information install summay cluster, network interface eno1 represents ethernet, ib0 represents infiniband. set environment variable , distributed pytorch works fine. set , cause error. do? thanks.",0,use infiniband cpu cluster backend gloo?,"use infiniband cpu cluster backend gloo? trying build pytorch source cpu cluster backend gloo. installing pytorch, got information install summay cluster, network interface eno1 represents ethernet, ib0 represents infiniband. set environment variable , distributed pytorch works fine. set , cause error. do? thanks."
pytorch,21135,"pytorch can't built installed mkl library pytorch version master branch os centos 7.5 installed intel . built scratch, build system find installed libraries. found mkl related module, . related enviroments set default path following way, build installed libraries. effective method build installed . build installed order build installed accept environments command line, modify following accept environments command line, file modified following tools build pytorch libs.py b tools build pytorch libs.py 228,6 228,18 def run cmake version, os.getenv 'mkl tbb' cmake defines cmake args, intel mkl tbb check env flag 'mkl tbb' os.getenv 'intel compiler dir' cmake defines cmake args, intel compiler dir os.getenv 'intel compiler dir' os.getenv 'intel mkl dir' cmake defines cmake args, intel mkl dir os.getenv 'intel mkl dir' os.getenv 'intel mkl sequential' cmake defines cmake args, intel mkl sequential check env flag 'intel mkl sequential' os.getenv 'intel mkl tbb' cmake defines cmake args, intel mkl tbb check env flag 'intel mkl tbb' mkldnn threading os.getenv 'mkldnn threading' mkldnn threading cmake defines cmake args, mkldnn threading mkldnn threading",0,cmake build can't build pytorch install mkl library,"cmake build can't build pytorch install mkl library pytorch can't built installed mkl library pytorch version master branch os centos 7.5 installed intel . built scratch, build system find installed libraries. found mkl related module, . related enviroments set default path following way, build installed libraries. effective method build installed . build installed order build installed accept environments command line, modify following accept environments command line, file modified following tools build pytorch libs.py b tools build pytorch libs.py 228,6 228,18 def run cmake version, os.getenv 'mkl tbb' cmake defines cmake args, intel mkl tbb check env flag 'mkl tbb' os.getenv 'intel compiler dir' cmake defines cmake args, intel compiler dir os.getenv 'intel compiler dir' os.getenv 'intel mkl dir' cmake defines cmake args, intel mkl dir os.getenv 'intel mkl dir' os.getenv 'intel mkl sequential' cmake defines cmake args, intel mkl sequential check env flag 'intel mkl sequential' os.getenv 'intel mkl tbb' cmake defines cmake args, intel mkl tbb check env flag 'intel mkl tbb' mkldnn threading os.getenv 'mkldnn threading' mkldnn threading cmake defines cmake args, mkldnn threading mkldnn threading"
pytorch,16316,bug tried install pytorch using pipenv see https github.com pypa pipenv issues 3476 . put fancy thing supposed open browser. not. instead output believed installation broken. reproduce steps reproduce behavior 1. pipenv install pytorch see something like expected behavior want people installing pypi put there. break dependency management tools. environment collecting environment information... pytorch version n debug build n cuda used build pytorch n os ubuntu 18.04 lts gcc version ubuntu 7.3.0 16ubuntu3 7.3.0 cmake version version 3.5.1 python version 3.7 cuda available n cuda runtime version could collect gpu models configuration could collect nvidia driver version could collect cudnn version could collect versions relevant libraries pip could collect conda could collect additional context understand desire helpful. trying make non interactive activity dependency management interactive activity inevitably going result situations like this. please make installation fail promptly noisily.,0,please remove pytorch pypi,please remove pytorch pypi bug tried install pytorch using pipenv see https github.com pypa pipenv issues 3476 . put fancy thing supposed open browser. not. instead output believed installation broken. reproduce steps reproduce behavior 1. pipenv install pytorch see something like expected behavior want people installing pypi put there. break dependency management tools. environment collecting environment information... pytorch version n debug build n cuda used build pytorch n os ubuntu 18.04 lts gcc version ubuntu 7.3.0 16ubuntu3 7.3.0 cmake version version 3.5.1 python version 3.7 cuda available n cuda runtime version could collect gpu models configuration could collect nvidia driver version could collect cudnn version could collect versions relevant libraries pip could collect conda could collect additional context understand desire helpful. trying make non interactive activity dependency management interactive activity inevitably going result situations like this. please make installation fail promptly noisily.
pytorch,31596,"bug peculiar bug encountering. using mtcnn face detector https github.com treb1en insightface pytorch pytorch 1.3.1 cpu face detection works fine api called main thread. forked new process initialized mtcnn well face detector model mobilefacenet , hangs first net forward call mtcnn . load state dict mobilefacenet commented out, works fine. reproduce steps reproduce behavior 1. clone https github.com treb1en insightface pytorch 2. try following code 3. test function called main thread, code works fine. 4. line removed, code works fine environment pkg version python 3.7.4 torch 1.3.1 cpu pip pillow 5.4.1 opencv python 4.1.1.26 os ubuntu 18.04, linux 4.15.0 72 generic",0,pytorch forward hangs multiprocess environment,"pytorch forward hangs multiprocess environment bug peculiar bug encountering. using mtcnn face detector https github.com treb1en insightface pytorch pytorch 1.3.1 cpu face detection works fine api called main thread. forked new process initialized mtcnn well face detector model mobilefacenet , hangs first net forward call mtcnn . load state dict mobilefacenet commented out, works fine. reproduce steps reproduce behavior 1. clone https github.com treb1en insightface pytorch 2. try following code 3. test function called main thread, code works fine. 4. line removed, code works fine environment pkg version python 3.7.4 torch 1.3.1 cpu pip pillow 5.4.1 opencv python 4.1.1.26 os ubuntu 18.04, linux 4.15.0 72 generic"
pytorch,13304,"bug hello great programmers using fair's platform detectron training e2e mask rcnn r 101 fpn 3x gn.yaml config file, faced issue indicated report one bug pytorch. 1generalized rcnn additional context way, e2e mask rcnn r 50 fpn 1x.yaml config works fine me. waiting response, thank .",0,assert failed opt conda conda bld pytorch nightly 1539602533843 work aten src aten core blob.h 79,"assert failed opt conda conda bld pytorch nightly 1539602533843 work aten src aten core blob.h 79 bug hello great programmers using fair's platform detectron training e2e mask rcnn r 101 fpn 3x gn.yaml config file, faced issue indicated report one bug pytorch. 1generalized rcnn additional context way, e2e mask rcnn r 50 fpn 1x.yaml config works fine me. waiting response, thank ."
pytorch,6998,"trying reload modules cause crashes python 2.7 crash ubuntu release, error messages debug build . type tp flags 1l 9 ' failed. aborted core dumped torch import autograd reload autograd terminate called throwing instance 'std runtime error' generic type cannot initialize type profilerevent object name already defined aborted core dumped colesbury see obvious reason this? look details?",0,pytorch handling python reload properly,"pytorch handling python reload properly trying reload modules cause crashes python 2.7 crash ubuntu release, error messages debug build . type tp flags 1l 9 ' failed. aborted core dumped torch import autograd reload autograd terminate called throwing instance 'std runtime error' generic type cannot initialize type profilerevent object name already defined aborted core dumped colesbury see obvious reason this? look details?"
pytorch,3990,previous instances https github.com pytorch pytorch issues 3018 affected bug server working upgraded cuda 8 cuda 9. dutifully rebuilt pytorch forgot uninstall magma cuda80 instant hang cuda initialization.,0,raise error using magma built wrong version cuda,raise error using magma built wrong version cuda previous instances https github.com pytorch pytorch issues 3018 affected bug server working upgraded cuda 8 cuda 9. dutifully rebuilt pytorch forgot uninstall magma cuda80 instant hang cuda initialization.
pytorch,12646,getting error message please see attached run command python setup.py install ! pytorch question https user images.githubusercontent.com 8035201 46946663 b2551580 d0a2 11e8 9f9e afd34fd2454d.png . followed office guide pytorch https caffe2.ai docs getting started.html?platform ubuntu configuration compile. weird thing print coffee2 sucessfully ! coffe2 https user images.githubusercontent.com 8035201 46946767 03fda000 d0a3 11e8 8de5 4ccbba7c3d36.png would like ask anybody knows fix this?,0,caffe2 installation inside pytorch,caffe2 installation inside pytorch getting error message please see attached run command python setup.py install ! pytorch question https user images.githubusercontent.com 8035201 46946663 b2551580 d0a2 11e8 9f9e afd34fd2454d.png . followed office guide pytorch https caffe2.ai docs getting started.html?platform ubuntu configuration compile. weird thing print coffee2 sucessfully ! coffe2 https user images.githubusercontent.com 8035201 46946767 03fda000 d0a3 11e8 8de5 4ccbba7c3d36.png would like ask anybody knows fix this?
pytorch,8837,"noticed odd behavior attempting write scheduler based . write custom get lr work based self.last epoch, impossible differentiate 0th 1st epoch. minimal working example results output see last epoch asked set learning rate based last epoch 0 twice. lrscheduler class takes last epoch argument, knows set lr previous epoch. default last epoch 1, first epoch 0 epoch run yet. construction calls , means step function sets learning rate epoch 0. last epoch reset 1 immediately after, next call step also sets learning rate epoch 0. fix would simply remove 1 , might break existing implementations expect set negative number. think intuitive implementation class might track current epoch rather previous one. would backwards incompatible change, think would improve overall quality torch. willing give implementation shot sounds like good idea maintainers. cc ezyang alband zou3519 gqchen pearu nikitaved soulitzer mruberry jbschlosser vincentqb",0,inconsistency implementation lrscheduler,"inconsistency implementation lrscheduler noticed odd behavior attempting write scheduler based . write custom get lr work based self.last epoch, impossible differentiate 0th 1st epoch. minimal working example results output see last epoch asked set learning rate based last epoch 0 twice. lrscheduler class takes last epoch argument, knows set lr previous epoch. default last epoch 1, first epoch 0 epoch run yet. construction calls , means step function sets learning rate epoch 0. last epoch reset 1 immediately after, next call step also sets learning rate epoch 0. fix would simply remove 1 , might break existing implementations expect set negative number. think intuitive implementation class might track current epoch rather previous one. would backwards incompatible change, think would improve overall quality torch. willing give implementation shot sounds like good idea maintainers. cc ezyang alband zou3519 gqchen pearu nikitaved soulitzer mruberry jbschlosser vincentqb"
pytorch,8430,"load pretrained model, runtime error occurred missing key state dict bn.num batches tracked every batchnorm2d layer",0,batchnorm2d track running stats,"batchnorm2d track running stats load pretrained model, runtime error occurred missing key state dict bn.num batches tracked every batchnorm2d layer"
pytorch,19160,"documentation commonly used evaluating model. confusion exists whether setting also sets https discuss.pytorch.org model eval torch set grad enabled train effect grad history 17183 5?u ataraxy would accept pr suggests usage other, words like evaluating model's performance, using module.eval may also useful. evaluating model's performance, using autograd.no grad may also useful.",0,suggest model.eval torch.no grad vice versa,"suggest model.eval torch.no grad vice versa documentation commonly used evaluating model. confusion exists whether setting also sets https discuss.pytorch.org model eval torch set grad enabled train effect grad history 17183 5?u ataraxy would accept pr suggests usage other, words like evaluating model's performance, using module.eval may also useful. evaluating model's performance, using autograd.no grad may also useful."
pytorch,24045,repro output cc ezyang gchanan zou3519,0,"torch. save,load data corruption serializing module get,set state","torch. save,load data corruption serializing module get,set state repro output cc ezyang gchanan zou3519"
pytorch,5157,"parameter seems incorrectly defined using multi dimensional input target. related forum thread https discuss.pytorch.org binary cross entropy weights 13299 . documentation defines given, tensor size nbatch . however, example throws error workaround tensor match number dimensions internally, called fails first code snippet. second code snippet applied weighting batch element, fine. automatically unsqueeze weight tensor, input target multi dimensional? also, handle class weighting? pass 2 weights tensor, code successfully runs, apply class weighting, might mislead users workaround would wanted behavior parameter class batch weighting? cases issues moment opinion. would like fix issue, would like hear opinions right behavior. class weighting would consistent loss functions like , maybe batch weighting common use case . pytorch version installed source",0,bceloss weight parameter shape incorrect,"bceloss weight parameter shape incorrect parameter seems incorrectly defined using multi dimensional input target. related forum thread https discuss.pytorch.org binary cross entropy weights 13299 . documentation defines given, tensor size nbatch . however, example throws error workaround tensor match number dimensions internally, called fails first code snippet. second code snippet applied weighting batch element, fine. automatically unsqueeze weight tensor, input target multi dimensional? also, handle class weighting? pass 2 weights tensor, code successfully runs, apply class weighting, might mislead users workaround would wanted behavior parameter class batch weighting? cases issues moment opinion. would like fix issue, would like hear opinions right behavior. class weighting would consistent loss functions like , maybe batch weighting common use case . pytorch version installed source"
pytorch,7944,"issue description following code produce error tcp distributed backend error bit misleading, since sizes tensors types different. think would better give message along lines happy submit pr others agree original message changed.",0,better error message datachanneltcp receive,"better error message datachanneltcp receive issue description following code produce error tcp distributed backend error bit misleading, since sizes tensors types different. think would better give message along lines happy submit pr others agree original message changed."
pytorch,12181,"per pytorch caffe2 readme asking here. would like use existing network definition weights model zoo backbone new network. specific example architecture squeezenet, new network simply different shape top parameterized layers 'conv10 w', 'conv10 b' , accommodate different set classes imagenet. unfortunately, clear documentation, tutorials, examples achieve . os notes built caffe2 opencv source current master, python2.7.12 virtualenv, cuda 9.0, cudnn 7.0. wrote script based https nbviewer.jupyter.org gist kyamagu 6cff70840c10ca374e069a3a7eb00cb4 dogs vs cats.ipynb think https gist.github.com johncorring d735675e75add96fbdfbcc40fa00f3ba get following error message traceback recent call last file dogsvscats.py , line 184, shtyp workspace.infershapesandtypes train model.net file home john code pytorch build caffe2 python workspace.py , line 258, infershapesandtypes blobdesc prototxt c.infer shapes types workspace net protos memoryerror std bad alloc helpful especially since cross referencing caffe2 docs yield anything . comment offending line try continue training recieve seg fault narrowed coming line 204, workspace.runnet train model.net . lldb returns following stack trace thread 1 tid 9130, 0x00007fffaa112240 libcaffe2.sovoid caffe2 math copymatrix int, int, float const , int, int, float , int, int, caffe2 cpucontext 208 frame 1 0x00007fffaa11392f libcaffe2.socaffe2 convop runondevicewithordernchw lambda caffe2 tensor 1 operator caffe2 tensor const 1169 frame 3 0x00007fffaa3f77f8 libcaffe2.socaffe2 convpoolopbase runondevice 301 frame 5 0x00007fffa9fb52e5 libcaffe2.socaffe2 simplenet run 460 frame 7 0x00007fffaa0aeb8a libcaffe2.sovoid pybind11 cpp function initialize, std allocator const , int, bool 21 , bool, std cxx11 basic string, std allocator const , int, bool, pybind11 name, pybind11 scope, pybind11 sibling caffe2 python addglobalmethods pybind11 module lambda std cxx11 basic string, std allocator const , int, bool 21 , bool std cxx11 basic string, std allocator const , int, bool , pybind11 name const , pybind11 scope const , pybind11 sibling const lambda pybind11 detail function call 3 fun pybind11 detail function call 311 frame 9 0x00007fffab160220 caffe2 pybind11 state gpu.sopyeval evalframeex 29342 frame 11 0x00000000004b9ab6 pythonpyeval evalframeex 24639 frame 13 0x00000000004b9ab6 pythonpyeval evalframeex 22711 frame 15 0x00000000004b9ab6 python??? 63 frame 17 0x00000000004e5422 pythonpyrun simplefileexflags 390 frame 19 0x0000000000493ae2 python libc start main main python start 41",0,network surgery transfer fails,"network surgery transfer fails per pytorch caffe2 readme asking here. would like use existing network definition weights model zoo backbone new network. specific example architecture squeezenet, new network simply different shape top parameterized layers 'conv10 w', 'conv10 b' , accommodate different set classes imagenet. unfortunately, clear documentation, tutorials, examples achieve . os notes built caffe2 opencv source current master, python2.7.12 virtualenv, cuda 9.0, cudnn 7.0. wrote script based https nbviewer.jupyter.org gist kyamagu 6cff70840c10ca374e069a3a7eb00cb4 dogs vs cats.ipynb think https gist.github.com johncorring d735675e75add96fbdfbcc40fa00f3ba get following error message traceback recent call last file dogsvscats.py , line 184, shtyp workspace.infershapesandtypes train model.net file home john code pytorch build caffe2 python workspace.py , line 258, infershapesandtypes blobdesc prototxt c.infer shapes types workspace net protos memoryerror std bad alloc helpful especially since cross referencing caffe2 docs yield anything . comment offending line try continue training recieve seg fault narrowed coming line 204, workspace.runnet train model.net . lldb returns following stack trace thread 1 tid 9130, 0x00007fffaa112240 libcaffe2.sovoid caffe2 math copymatrix int, int, float const , int, int, float , int, int, caffe2 cpucontext 208 frame 1 0x00007fffaa11392f libcaffe2.socaffe2 convop runondevicewithordernchw lambda caffe2 tensor 1 operator caffe2 tensor const 1169 frame 3 0x00007fffaa3f77f8 libcaffe2.socaffe2 convpoolopbase runondevice 301 frame 5 0x00007fffa9fb52e5 libcaffe2.socaffe2 simplenet run 460 frame 7 0x00007fffaa0aeb8a libcaffe2.sovoid pybind11 cpp function initialize, std allocator const , int, bool 21 , bool, std cxx11 basic string, std allocator const , int, bool, pybind11 name, pybind11 scope, pybind11 sibling caffe2 python addglobalmethods pybind11 module lambda std cxx11 basic string, std allocator const , int, bool 21 , bool std cxx11 basic string, std allocator const , int, bool , pybind11 name const , pybind11 scope const , pybind11 sibling const lambda pybind11 detail function call 3 fun pybind11 detail function call 311 frame 9 0x00007fffab160220 caffe2 pybind11 state gpu.sopyeval evalframeex 29342 frame 11 0x00000000004b9ab6 pythonpyeval evalframeex 24639 frame 13 0x00000000004b9ab6 pythonpyeval evalframeex 22711 frame 15 0x00000000004b9ab6 python??? 63 frame 17 0x00000000004e5422 pythonpyrun simplefileexflags 390 frame 19 0x0000000000493ae2 python libc start main main python start 41"
pytorch,18434,reported jph00 new tensor legacy constructor supported jit helpful know error message non legacy alternative is.,0,improve jit error message legacy constructor,improve jit error message legacy constructor reported jph00 new tensor legacy constructor supported jit helpful know error message non legacy alternative is.
pytorch,13636,"use torch.backends.cudnn.benchmark true conv3d, returned error runtimeerror cudnn error cudnn status internal error tired rf .nv work. error disappears torch.backends.cudnn.benchmark false . ubuntu 16.04.4 lts tesla p100 cuda 9.0 cudnn 7.1 pytorch 1.0.0a0 d03c6ba source 0.4.1 pip tried.",0,conv3d cudnn error,"conv3d cudnn error use torch.backends.cudnn.benchmark true conv3d, returned error runtimeerror cudnn error cudnn status internal error tired rf .nv work. error disappears torch.backends.cudnn.benchmark false . ubuntu 16.04.4 lts tesla p100 cuda 9.0 cudnn 7.1 pytorch 1.0.0a0 d03c6ba source 0.4.1 pip tried."
pytorch,7343,leaks . discovered running code 7270 cc ezyang alband zou3519 gqchen pearu nikitaved,0,memory leak pytorch .backward create graph true,memory leak pytorch .backward create graph true leaks . discovered running code 7270 cc ezyang alband zou3519 gqchen pearu nikitaved
pytorch,12322,"bug want import caffe2.pytho.core, get segmentation fault. followed install guide ubuntu 16 prebuilt binaries also installed nccl, browsing past issues can't find solution. please help reproduce steps reproduce behavior 1. install nccl 2.3.5 2. conda install c caffe2 caffe2 cuda9.0 cudnn7 3. import caffe2.python.core error environment followed https caffe2.ai docs getting started.html?platform ubuntu configuration prebuilt cmake installed anaconda can't find cmake output",0,caffe2 segmentation fault core dumped import caffe2.python.core,"caffe2 segmentation fault core dumped import caffe2.python.core bug want import caffe2.pytho.core, get segmentation fault. followed install guide ubuntu 16 prebuilt binaries also installed nccl, browsing past issues can't find solution. please help reproduce steps reproduce behavior 1. install nccl 2.3.5 2. conda install c caffe2 caffe2 cuda9.0 cudnn7 3. import caffe2.python.core error environment followed https caffe2.ai docs getting started.html?platform ubuntu configuration prebuilt cmake installed anaconda can't find cmake output"
pytorch,934,"current design , set , 8 batches data prepared advance, worker works one batch. efficient especially one batch large expensive process normally need 1 2 batches ahead time . go design workers work one batch time? cc ssnl",0,dataloader parallels elements vs batches,"dataloader parallels elements vs batches current design , set , 8 batches data prepared advance, worker works one batch. efficient especially one batch large expensive process normally need 1 2 batches ahead time . go design workers work one batch time? cc ssnl"
pytorch,1362,"standard loss function implementing this, even though pretty common. perfectly willing implement myself, nobody else feels like it. terribly complicated. would structure something like n, c c num classes n 0 targets c 1 api tensorflow also uses parameter setting switch negative sampling objective nce. worth doing? something already exists? something worth implementing?",0,feature request noise contrastive estimation negative sampling,"feature request noise contrastive estimation negative sampling standard loss function implementing this, even though pretty common. perfectly willing implement myself, nobody else feels like it. terribly complicated. would structure something like n, c c num classes n 0 targets c 1 api tensorflow also uses parameter setting switch negative sampling objective nce. worth doing? something already exists? something worth implementing?"
pytorch,13711,"bug train , test model, batchnorm different different gpus, broadcast eval ? reproduce steps reproduce behavior 1. 1. 1. expected behavior environment please copy paste output environment collection script https raw.githubusercontent.com pytorch pytorch master torch utils collect env.py fill checklist manually . get script run pytorch version e.g., 1.0 os e.g., linux installed pytorch , , source build command used compiling source python version cuda cudnn version gpu models configuration relevant information additional context",0,"eval , batchnorm running mean different different gpus","eval , batchnorm running mean different different gpus bug train , test model, batchnorm different different gpus, broadcast eval ? reproduce steps reproduce behavior 1. 1. 1. expected behavior environment please copy paste output environment collection script https raw.githubusercontent.com pytorch pytorch master torch utils collect env.py fill checklist manually . get script run pytorch version e.g., 1.0 os e.g., linux installed pytorch , , source build command used compiling source python version cuda cudnn version gpu models configuration relevant information additional context"
pytorch,30987,"even though documented, many users still use it. leads many bugs user code. remove completely prevent this. expected steps add new api make shallow copy share version? cat unbind function share version counter enough. remove use .data internal code blocky https github.com pytorch pytorch pull 30258 issuecomment 558344600 blocked ones done 31479 use requires new api mentioned above. x 31480 simple ones done 31481 31482 x ignored add warning every call actually remove ? cc ezyang ssnl alband zou3519 gqchen",0,remove .data,"remove .data even though documented, many users still use it. leads many bugs user code. remove completely prevent this. expected steps add new api make shallow copy share version? cat unbind function share version counter enough. remove use .data internal code blocky https github.com pytorch pytorch pull 30258 issuecomment 558344600 blocked ones done 31479 use requires new api mentioned above. x 31480 simple ones done 31481 31482 x ignored add warning every call actually remove ? cc ezyang ssnl alband zou3519 gqchen"
pytorch,9983,"add matrix power implemented numpy.linalg.matrix power https docs.scipy.org doc numpy reference generated numpy.linalg.matrix power.html , matrix exponential implemented scipy.linalg.expm https docs.scipy.org doc scipy 0.15.1 reference generated scipy.linalg.expm.html , matrix logarithm implemented scipy.linalg.logm https docs.scipy.org doc scipy 0.15.1 reference generated scipy.linalg.logm.html . x matrix power x matrix exp matrix log matrix sqrt cc ezyang gchanan zou3519 bdhirsh jbschlosser anjali411 jianyuh nikitaved pearu mruberry heitorschueroff walterddr ivanyashchuk xwang233 lezcano rgommers",0,feature request add matrix functions,"feature request add matrix functions add matrix power implemented numpy.linalg.matrix power https docs.scipy.org doc numpy reference generated numpy.linalg.matrix power.html , matrix exponential implemented scipy.linalg.expm https docs.scipy.org doc scipy 0.15.1 reference generated scipy.linalg.expm.html , matrix logarithm implemented scipy.linalg.logm https docs.scipy.org doc scipy 0.15.1 reference generated scipy.linalg.logm.html . x matrix power x matrix exp matrix log matrix sqrt cc ezyang gchanan zou3519 bdhirsh jbschlosser anjali411 jianyuh nikitaved pearu mruberry heitorschueroff walterddr ivanyashchuk xwang233 lezcano rgommers"
pytorch,30965,"targetting correct issue time, sorry noise bug per pep 563 postponed evaluation annotations https www.python.org dev peps pep 0563 , typing annotations automatically evaluated definition time starting python 3.7 using . solution avoid using directly https github.com pytorch pytorch blob master torch jit recursive.py l74 call https docs.python.org 3.7 library typing.html typing.get type hints future call also correctly evaluated. make pr? testcase fails traceback cc suo",0,jit breaks postponed annotations,"jit breaks postponed annotations targetting correct issue time, sorry noise bug per pep 563 postponed evaluation annotations https www.python.org dev peps pep 0563 , typing annotations automatically evaluated definition time starting python 3.7 using . solution avoid using directly https github.com pytorch pytorch blob master torch jit recursive.py l74 call https docs.python.org 3.7 library typing.html typing.get type hints future call also correctly evaluated. make pr? testcase fails traceback cc suo"
pytorch,11850,"solution solution found problem, maybe bug caffe2 mkl dnn enabled first compiling pytorch. turn mkl recompile pytorch. mkl dnn enabled. issue description trying accelerate caffe2 inference mkl dnn. mkl dnn lib detected mkl operators compiled mkl dnn cannot found installation since returns false. also, try enable mkl changing cmakelists.txt acutally cause error system info pytorch version 1.0.0a0 98aebed debug build cuda used build pytorch none os ubuntu 18.04.1 lts gcc version ubuntu 7.3.0 16ubuntu3 7.3.0 cmake version version 3.10.2 python version 2.7 cuda available cuda runtime version cuda gpu models configuration cuda nvidia driver version cuda cudnn version cuda versions relevant libraries pip numpy 1.15.1 pip torch 1.0.0a0 98aebed conda could collect",0,caffe2 bug cannot enable mkl dnn,"caffe2 bug cannot enable mkl dnn solution solution found problem, maybe bug caffe2 mkl dnn enabled first compiling pytorch. turn mkl recompile pytorch. mkl dnn enabled. issue description trying accelerate caffe2 inference mkl dnn. mkl dnn lib detected mkl operators compiled mkl dnn cannot found installation since returns false. also, try enable mkl changing cmakelists.txt acutally cause error system info pytorch version 1.0.0a0 98aebed debug build cuda used build pytorch none os ubuntu 18.04.1 lts gcc version ubuntu 7.3.0 16ubuntu3 7.3.0 cmake version version 3.10.2 python version 2.7 cuda available cuda runtime version cuda gpu models configuration cuda nvidia driver version cuda cudnn version cuda versions relevant libraries pip numpy 1.15.1 pip torch 1.0.0a0 98aebed conda could collect"
pytorch,10170,issue description failed build caffe2 code example cmake 3.11.3 system info caffe2 summary general cmake version 3.11.3 cmake command c program files cmake bin cmake.exe git version v0.1.11 9628 gf908b2b91 dirty system windows c compiler c program files x86 microsoft visual studio 14.0 vc bin x86 amd64 cl.exe c compiler version 19.0.24215.1 blas eigen cxx flags dwin32 windows w3 gr ehsc donnx namespace onnx c2 mp bigobj build type release compile definitions cmake prefix path cmake install prefix c dev pytorch build install build caffe2 build aten build binary build custom protobuf link local protobuf build docs build python build shared libs build test use asan use aten use cuda cuda static link use cudnn cuda version 8.0 cudnn version 7.0.5 cuda root directory c program files nvidia gpu computing toolkit cuda v8.0 cuda library c program files nvidia gpu computing toolkit cuda v8.0 lib x64 cuda.lib cudart library c program files nvidia gpu computing toolkit cuda v8.0 lib x64 cudart static.lib cublas library c program files nvidia gpu computing toolkit cuda v8.0 lib x64 cublas.lib c program files nvidia gpu computing toolkit cuda v8.0 lib x64 cublas device.lib cufft library c program files nvidia gpu computing toolkit cuda v8.0 lib x64 cufft.lib curand library c program files nvidia gpu computing toolkit cuda v8.0 lib x64 curand.lib cudnn library c program files nvidia gpu computing toolkit cuda v8.0 lib x64 cudnn.lib nvrtc c program files nvidia gpu computing toolkit cuda v8.0 lib x64 nvrtc.lib cuda include path c program files nvidia gpu computing toolkit cuda v8.0 include nvcc executable c program files nvidia gpu computing toolkit cuda v8.0 bin nvcc.exe cuda host compiler vcinstalldir bin use tensorrt use rocm use eigen blas use ffmpeg use gflags use glog use gloo use leveldb use lite proto use lmdb use metal use mkl use mobile opengl use mpi use nccl use nervana gpu use nnpack use observers use opencl use opencv use openmp use prof use redis use rocksdb use zmq public dependencies threads threads gflags glog glog private dependencies cpuinfo onnxifi loader configuring done generating done,0,build error,build error issue description failed build caffe2 code example cmake 3.11.3 system info caffe2 summary general cmake version 3.11.3 cmake command c program files cmake bin cmake.exe git version v0.1.11 9628 gf908b2b91 dirty system windows c compiler c program files x86 microsoft visual studio 14.0 vc bin x86 amd64 cl.exe c compiler version 19.0.24215.1 blas eigen cxx flags dwin32 windows w3 gr ehsc donnx namespace onnx c2 mp bigobj build type release compile definitions cmake prefix path cmake install prefix c dev pytorch build install build caffe2 build aten build binary build custom protobuf link local protobuf build docs build python build shared libs build test use asan use aten use cuda cuda static link use cudnn cuda version 8.0 cudnn version 7.0.5 cuda root directory c program files nvidia gpu computing toolkit cuda v8.0 cuda library c program files nvidia gpu computing toolkit cuda v8.0 lib x64 cuda.lib cudart library c program files nvidia gpu computing toolkit cuda v8.0 lib x64 cudart static.lib cublas library c program files nvidia gpu computing toolkit cuda v8.0 lib x64 cublas.lib c program files nvidia gpu computing toolkit cuda v8.0 lib x64 cublas device.lib cufft library c program files nvidia gpu computing toolkit cuda v8.0 lib x64 cufft.lib curand library c program files nvidia gpu computing toolkit cuda v8.0 lib x64 curand.lib cudnn library c program files nvidia gpu computing toolkit cuda v8.0 lib x64 cudnn.lib nvrtc c program files nvidia gpu computing toolkit cuda v8.0 lib x64 nvrtc.lib cuda include path c program files nvidia gpu computing toolkit cuda v8.0 include nvcc executable c program files nvidia gpu computing toolkit cuda v8.0 bin nvcc.exe cuda host compiler vcinstalldir bin use tensorrt use rocm use eigen blas use ffmpeg use gflags use glog use gloo use leveldb use lite proto use lmdb use metal use mkl use mobile opengl use mpi use nccl use nervana gpu use nnpack use observers use opencl use opencv use openmp use prof use redis use rocksdb use zmq public dependencies threads threads gflags glog glog private dependencies cpuinfo onnxifi loader configuring done generating done
pytorch,18496,"bug implementation optional seem always compilable .cu files. compiling commit https github.com gchanan pytorch commit 6666ff1083a55e90b230d2269c13a3d30af8c0f4 gives following error bit digging problem seems constexpr functions, e.g. https github.com pytorch pytorch blob 654e59fcac4a9d4bf0b48306e1d7f7be5b7e40b1 c10 util optional.h l600 l604 basically, nvcc compile .cu file, . seem possible constraint bad optional access derives constructors. note applies calls . calls seem fail reasons dig into. passes fine, solution use fix issue.",0,can't compile c10 optional values operator .cu file,"can't compile c10 optional values operator .cu file bug implementation optional seem always compilable .cu files. compiling commit https github.com gchanan pytorch commit 6666ff1083a55e90b230d2269c13a3d30af8c0f4 gives following error bit digging problem seems constexpr functions, e.g. https github.com pytorch pytorch blob 654e59fcac4a9d4bf0b48306e1d7f7be5b7e40b1 c10 util optional.h l600 l604 basically, nvcc compile .cu file, . seem possible constraint bad optional access derives constructors. note applies calls . calls seem fail reasons dig into. passes fine, solution use fix issue."
pytorch,1051,"running variables accumulated non master gpus lost, next step running variables accumulated master gpu broadcast non master gpus. convergence wise make much difference , still bug. also, device ids dataparallel checked contain values, otherwise stuff like break https github.com pytorch pytorch blob master torch backends cudnn rnn.py l38 l43 creating replicas gpu actually intended supported, referenced lines create dictionary indexed current device constitute bug. issues ultimately caused well defined behaviour dataparallel respect stateful modules, batchnorm recurrent nets cudnn backend examples. cc ngimel alband mruberry",0,dataparallel correctly handle running variables batch norm,"dataparallel correctly handle running variables batch norm running variables accumulated non master gpus lost, next step running variables accumulated master gpu broadcast non master gpus. convergence wise make much difference , still bug. also, device ids dataparallel checked contain values, otherwise stuff like break https github.com pytorch pytorch blob master torch backends cudnn rnn.py l38 l43 creating replicas gpu actually intended supported, referenced lines create dictionary indexed current device constitute bug. issues ultimately caused well defined behaviour dataparallel respect stateful modules, batchnorm recurrent nets cudnn backend examples. cc ngimel alband mruberry"
pytorch,4181,"ticket track plan refactor fused rnn api makes use cudnn implementation rnns . difficult? number factors make fused rnns unusual, compared differentiable operations pytorch requires unusually large, structured series weights. differentiable operators fixed number weights, rnn entire sequence must weights every layer rnn. make matters worse, layer needs one two tensors weight bias. operator pytorch behaves like this. cudnn requires weights inputs packed particular way. required packing operations frequently reported users extremely confusing aspect pytorch. weights vary depending type rnn, hidden tensors. lstm, hx cx rnns, hx needed. rnn dropout stateful api, requires dropout descriptors passed invocations handle randomness. want do? desired goals rnn refactor make cudnn rnn available aten design ideas. aten api take two tensors, , undefined tensor non lstm networks. cc csarofeen ptrblck",0,fused rnn refactor plan,"fused rnn refactor plan ticket track plan refactor fused rnn api makes use cudnn implementation rnns . difficult? number factors make fused rnns unusual, compared differentiable operations pytorch requires unusually large, structured series weights. differentiable operators fixed number weights, rnn entire sequence must weights every layer rnn. make matters worse, layer needs one two tensors weight bias. operator pytorch behaves like this. cudnn requires weights inputs packed particular way. required packing operations frequently reported users extremely confusing aspect pytorch. weights vary depending type rnn, hidden tensors. lstm, hx cx rnns, hx needed. rnn dropout stateful api, requires dropout descriptors passed invocations handle randomness. want do? desired goals rnn refactor make cudnn rnn available aten design ideas. aten api take two tensors, , undefined tensor non lstm networks. cc csarofeen ptrblck"
pytorch,25381,"feature way see code builtin function method motivation consider scenario 1 using google colab, want see implementation nn.dropout 2 ctrl click nn.dropout, takes dropout.py, find dropout class, returns 3 so, ctrl click f.dropout, takes functional.py, see 4 now, stuck, builtin, way look code, search github. 5 thing happens want see implementation nn.crossmaplrn2d, redirects torch.group norm, builtin. 6 similarly view, permute, way understand builtins becomes difficult raised issue google colab, provide way look builtin function method, told no, there's way get directly source case. particular, .so, could cython file, could another kind c extension built via vanilla c c code, swig'd in, something else completely . installed package there's way us trace back source, cases like torch distributed .whl file source included. pitch alternatives additional context",0,regarding builtin function method,"regarding builtin function method feature way see code builtin function method motivation consider scenario 1 using google colab, want see implementation nn.dropout 2 ctrl click nn.dropout, takes dropout.py, find dropout class, returns 3 so, ctrl click f.dropout, takes functional.py, see 4 now, stuck, builtin, way look code, search github. 5 thing happens want see implementation nn.crossmaplrn2d, redirects torch.group norm, builtin. 6 similarly view, permute, way understand builtins becomes difficult raised issue google colab, provide way look builtin function method, told no, there's way get directly source case. particular, .so, could cython file, could another kind c extension built via vanilla c c code, swig'd in, something else completely . installed package there's way us trace back source, cases like torch distributed .whl file source included. pitch alternatives additional context"
pytorch,24915,feature want build unified data pipeline interface offers building blocks others build following objectives standardize datasets across domains. offer flexible building blocks combine obtain datasets. enable datasets fit memory. share code among domains. facilitate parallel loading processing data. decouple data loading preprocessing transformation. offer static typing datasets motivation domains currently non standard dataset structure may also download data. duplicate efforts adds complexity user. common bottleneck generating datasets reading data. want offer interface enables reading data running initial preprocessing maximizing available computing resources utilization. may want leverage specialize libraries nvidia dali. additional information torch.utils.data https github.com pytorch pytorch blob master torch utils data tf.data https www.tensorflow.org beta guide data e.g. uses dictionary data point iteration fast.ai's basic data https docs.fast.ai basic data.html data block https docs.fast.ai data block.html tnt https github.com pytorch tnt blob master torchnet dataset dataset.py torchnet https github.com torchnet torchnet tree master dataset torchdata https pypi.org project torchdata datasets pytorch text 624 pytorch text 610 pytorch audio 303 new datasets domains pytorch vision 1193 wants select metadata return internal overview https fb.quip.com vlwwa35cmq0t torchtext https fb.quip.com lncwasc1cuzt core https fb.quip.com b0peacndlzee torchvision https fb.quip.com wgsuapsce6xn safe datasets https github.com msamogh nonechucks dataloader torchaudio background iterator https github.com pytorch audio blob master torchaudio datasets utils.py l314 24915 wants use worker processes fastdataloader https github.com pytorch pytorch issues 15849 issuecomment 573921048 python 3.8 shared memory https docs.python.org 3 library multiprocessing.shared memory.html internal torchdata https fb.quip.com ekjjasyqmg7x gil https docs.google.com document 1injp79dwtiyj xgvu65y2r k2hel6t2l1xgdfktu4rw edit experiment https fb.quip.com imvlaodyjfai dataloader iterable https fb.workplace.com groups 2162019300778793 permalink 3398854433474998 features 12672 wants move collate fn functionality datasets 26547 wants distributed random sampling 28743 sampler iterable datasets pytorch vision 1315 wants apply instance random transform sequence many images cc ssnl fmassa zhangguanheng66 vincentqb mrshenli,0,shared dataset functionality,shared dataset functionality feature want build unified data pipeline interface offers building blocks others build following objectives standardize datasets across domains. offer flexible building blocks combine obtain datasets. enable datasets fit memory. share code among domains. facilitate parallel loading processing data. decouple data loading preprocessing transformation. offer static typing datasets motivation domains currently non standard dataset structure may also download data. duplicate efforts adds complexity user. common bottleneck generating datasets reading data. want offer interface enables reading data running initial preprocessing maximizing available computing resources utilization. may want leverage specialize libraries nvidia dali. additional information torch.utils.data https github.com pytorch pytorch blob master torch utils data tf.data https www.tensorflow.org beta guide data e.g. uses dictionary data point iteration fast.ai's basic data https docs.fast.ai basic data.html data block https docs.fast.ai data block.html tnt https github.com pytorch tnt blob master torchnet dataset dataset.py torchnet https github.com torchnet torchnet tree master dataset torchdata https pypi.org project torchdata datasets pytorch text 624 pytorch text 610 pytorch audio 303 new datasets domains pytorch vision 1193 wants select metadata return internal overview https fb.quip.com vlwwa35cmq0t torchtext https fb.quip.com lncwasc1cuzt core https fb.quip.com b0peacndlzee torchvision https fb.quip.com wgsuapsce6xn safe datasets https github.com msamogh nonechucks dataloader torchaudio background iterator https github.com pytorch audio blob master torchaudio datasets utils.py l314 24915 wants use worker processes fastdataloader https github.com pytorch pytorch issues 15849 issuecomment 573921048 python 3.8 shared memory https docs.python.org 3 library multiprocessing.shared memory.html internal torchdata https fb.quip.com ekjjasyqmg7x gil https docs.google.com document 1injp79dwtiyj xgvu65y2r k2hel6t2l1xgdfktu4rw edit experiment https fb.quip.com imvlaodyjfai dataloader iterable https fb.workplace.com groups 2162019300778793 permalink 3398854433474998 features 12672 wants move collate fn functionality datasets 26547 wants distributed random sampling 28743 sampler iterable datasets pytorch vision 1315 wants apply instance random transform sequence many images cc ssnl fmassa zhangguanheng66 vincentqb mrshenli
pytorch,28245,"feature context model parallel https github.com pytorch pytorch issues 23110 motivation applications using complex distributed primitives like rpc, rref distributed autograd, debugging issues cumbersome. way exposing metrics applications. could simply api returns information various things. full list metrics needs decided, although examples could number owner rrefs, number user rrefs, rpc latency, distributed autograd latency etc. cc ezyang gchanan zou3519 jerryzh168 pietern mrshenli pritamdamania87 zhaojuanmao satgera gqchen aazzolini rohan varma xush6528",0,pytorch rpc expose critical metrics application.,"pytorch rpc expose critical metrics application. feature context model parallel https github.com pytorch pytorch issues 23110 motivation applications using complex distributed primitives like rpc, rref distributed autograd, debugging issues cumbersome. way exposing metrics applications. could simply api returns information various things. full list metrics needs decided, although examples could number owner rrefs, number user rrefs, rpc latency, distributed autograd latency etc. cc ezyang gchanan zou3519 jerryzh168 pietern mrshenli pritamdamania87 zhaojuanmao satgera gqchen aazzolini rohan varma xush6528"
pytorch,2001,keras gives fine visualization model convenient comes debugging network. try implement something like pytorch? cc ezyang gchanan zou3519 bdhirsh jbschlosser alband mruberry,0,implement similar pytorch function model.summary keras?,implement similar pytorch function model.summary keras? keras gives fine visualization model convenient comes debugging network. try implement something like pytorch? cc ezyang gchanan zou3519 bdhirsh jbschlosser alband mruberry
pytorch,18998,feature simple method transforming pil images directly torch tensor. motivation frustrating use transforms simple conversion pil image torch tensors time easy get tensor numpy pitch giving pil images type returns torch tensor alternatives considered two 1. transforms 2. convert first numpy torch,0,torch.from pil request ?,torch.from pil request ? feature simple method transforming pil images directly torch tensor. motivation frustrating use transforms simple conversion pil image torch tensors time easy get tensor numpy pitch giving pil images type returns torch tensor alternatives considered two 1. transforms 2. convert first numpy torch
pytorch,15771,"hello, compiling generated sources caffe2.pb.cc caffe2.proto getting implicit conversion error protobuf version v3.5.2 compiler cc aarch64 linux android clang cxx aarch64 linux android clang thank you,",0,implicit conversion error caffe2,"implicit conversion error caffe2 hello, compiling generated sources caffe2.pb.cc caffe2.proto getting implicit conversion error protobuf version v3.5.2 compiler cc aarch64 linux android clang cxx aarch64 linux android clang thank you,"
pytorch,24770,porting th operators essential code simplicity performance reasons. porting guides q available umbrella issue 24507 feel free add vitalyfedyunin reviewer get prioritized review.,0,migrate sort th aten cpu,migrate sort th aten cpu porting th operators essential code simplicity performance reasons. porting guides q available umbrella issue 24507 feel free add vitalyfedyunin reviewer get prioritized review.
pytorch,27694,"model contain custom layer, deployed directly c side using jit mechanism libtorch library. model contains custom c cuda layer. deploying model c side, need compile custom c cuda layers libtorch library? thank you!!! cc suo yf225",0,deployment training model c end,"deployment training model c end model contain custom layer, deployed directly c side using jit mechanism libtorch library. model contains custom c cuda layer. deploying model c side, need compile custom c cuda layers libtorch library? thank you!!! cc suo yf225"
pytorch,28515,"finally good reason merge libc10.so libtorch.so corresponding libc10 cuda.so libtorch cuda.so, etc. trying devirtualize access autogradmeta, tensorimpl lives c10 autogradmeta lives torch, cannot destructor would cross dynamic library boundary. absorbing c10 torch able this. dangers putting c10 libtorch might push windows build max library size. see https github.com pytorch pytorch issues 27215 alternatives thought move tensorimpl back aten core keep virtual interface autogradmeta alternatives work move autogradmeta c10. autogradmeta must declared tensor contains field tensor, cannot replace field public api tensor returns mutable reference. though, might possible fix moving tensorimpl c10 cc ezyang gchanan zou3519 jerryzh168 dzhulgakov smessmer",0,get rid libc10.so,"get rid libc10.so finally good reason merge libc10.so libtorch.so corresponding libc10 cuda.so libtorch cuda.so, etc. trying devirtualize access autogradmeta, tensorimpl lives c10 autogradmeta lives torch, cannot destructor would cross dynamic library boundary. absorbing c10 torch able this. dangers putting c10 libtorch might push windows build max library size. see https github.com pytorch pytorch issues 27215 alternatives thought move tensorimpl back aten core keep virtual interface autogradmeta alternatives work move autogradmeta c10. autogradmeta must declared tensor contains field tensor, cannot replace field public api tensor returns mutable reference. though, might possible fix moving tensorimpl c10 cc ezyang gchanan zou3519 jerryzh168 dzhulgakov smessmer"
pytorch,25417,"jfc4050 added cpu 24949. current version flattens input tensors one, allreduce tensor. works, even if, say, process 0 provides tensor size 4 process 1 provides two tensors size 2 each. reasonable shortcut avoid using additional communications check tensor sizes, good add size checking default mode, asking users explicitly set indeed want skip that. cc pietern mrshenli pritamdamania87 zhaojuanmao satgera",0,add mode check input tensor sizes allreduce coalesced,"add mode check input tensor sizes allreduce coalesced jfc4050 added cpu 24949. current version flattens input tensors one, allreduce tensor. works, even if, say, process 0 provides tensor size 4 process 1 provides two tensors size 2 each. reasonable shortcut avoid using additional communications check tensor sizes, good add size checking default mode, asking users explicitly set indeed want skip that. cc pietern mrshenli pritamdamania87 zhaojuanmao satgera"
pytorch,11980,"issue description currently, using random split function https github.com pytorch pytorch blob master torch utils data dataset.py parameters need given dataset list contains lengths splits produced means user calculate upfront add function parameters like better second parameter list contains percentages users wants split happen, like . result change would cleaner code user also believer natural way creating splits. would like pick pull request! cc vitalyfedyunin ejguan",0,enhancement increase user friendliness dataset.random split,"enhancement increase user friendliness dataset.random split issue description currently, using random split function https github.com pytorch pytorch blob master torch utils data dataset.py parameters need given dataset list contains lengths splits produced means user calculate upfront add function parameters like better second parameter list contains percentages users wants split happen, like . result change would cleaner code user also believer natural way creating splits. would like pick pull request! cc vitalyfedyunin ejguan"
pytorch,26714,"issue description spectral norm used okay. used ,there model network .cuda environment pytorch version e.g., 1.0 1.1.0 python version 3.6.8 cuda cudnn version cuda9.0",0,spectral norm used rnn causes parameter types mismatch gpu,"spectral norm used rnn causes parameter types mismatch gpu issue description spectral norm used okay. used ,there model network .cuda environment pytorch version e.g., 1.0 1.1.0 python version 3.6.8 cuda cudnn version cuda9.0"
pytorch,24600,porting th operators essential code simplicity performance reasons. porting guides q available umbrella issue 24507 feel free add vitalyfedyunin reviewer get prioritized review.,0,migrate multi margin loss th aten cuda,migrate multi margin loss th aten cuda porting th operators essential code simplicity performance reasons. porting guides q available umbrella issue 24507 feel free add vitalyfedyunin reviewer get prioritized review.
pytorch,29692,stochastic functions accept . well.,0,fr add generator kwarg support torch.randn torch.rand,fr add generator kwarg support torch.randn torch.rand stochastic functions accept . well.
pytorch,28938,"documentation documentation pages viewed different versions using versions page https pytorch.org docs versions.html . however, navigating tutorials , versions cannot selected. problem experimental parts s.a. quantization , update tutorials master, cannot viewed.",0,cannot select version tutorials page,"cannot select version tutorials page documentation documentation pages viewed different versions using versions page https pytorch.org docs versions.html . however, navigating tutorials , versions cannot selected. problem experimental parts s.a. quantization , update tutorials master, cannot viewed."
pytorch,10714,"currently, fc fctranspose accepts 1d bias. fc's implementation broadcast create 2d bias 1d bias, could provide whole 2d bias. then, better optimization onnx gemm op caffe2 backend. try implement idea create pull request later. houseroad bddppq",0,feature request caffe2 extend fc fctranspose op handle 2d bias.,"feature request caffe2 extend fc fctranspose op handle 2d bias. currently, fc fctranspose accepts 1d bias. fc's implementation broadcast create 2d bias 1d bias, could provide whole 2d bias. then, better optimization onnx gemm op caffe2 backend. try implement idea create pull request later. houseroad bddppq"
pytorch,14672,"bug scipy, try calculate log probability value outside given distribution's support, scipy return . uniform distribution torch follows behavior, shown however, behavior extend distributions constrained support, exponential, beta, gamma, discrete distributions geometric. four distributions tested unsure more. discrepancy behavior expected? please let know missed something. system info collecting environment information... pytorch version 0.4.1 debug build cuda used build pytorch 9.0.176 os ubuntu 18.10 gcc version ubuntu 8.2.0 7ubuntu1 8.2.0 cmake version version 3.12.1 python version 3.7 cuda available yes cuda runtime version could collect gpu models configuration gpu 0 geforce gtx 960m nvidia driver version 415.13 cudnn version could collect versions relevant libraries pip numpy 1.15.4 pip numpydoc 0.8.0 pip torch 0.4.1 conda pytorch 0.4.1 py37ha74772b 0 cc fritzo neerajprad alicanb nikitaved",0,inconsistent behavior log prob values outside support,"inconsistent behavior log prob values outside support bug scipy, try calculate log probability value outside given distribution's support, scipy return . uniform distribution torch follows behavior, shown however, behavior extend distributions constrained support, exponential, beta, gamma, discrete distributions geometric. four distributions tested unsure more. discrepancy behavior expected? please let know missed something. system info collecting environment information... pytorch version 0.4.1 debug build cuda used build pytorch 9.0.176 os ubuntu 18.10 gcc version ubuntu 8.2.0 7ubuntu1 8.2.0 cmake version version 3.12.1 python version 3.7 cuda available yes cuda runtime version could collect gpu models configuration gpu 0 geforce gtx 960m nvidia driver version 415.13 cudnn version could collect versions relevant libraries pip numpy 1.15.4 pip numpydoc 0.8.0 pip torch 0.4.1 conda pytorch 0.4.1 py37ha74772b 0 cc fritzo neerajprad alicanb nikitaved"
pytorch,14112,descriptions examples methods listed doc page much easier new comers learn use sparse improve doc little bit cc aocsa nikitaved pearu mruberry ivanyashchuk,0,sparse add descriptions examples methods torch.sparse doc page,sparse add descriptions examples methods torch.sparse doc page descriptions examples methods listed doc page much easier new comers learn use sparse improve doc little bit cc aocsa nikitaved pearu mruberry ivanyashchuk
pytorch,20997,"bug reducelronplateau fail add new parameter group optimizer. function raise list index range error reproduce steps reproduce behavior 1. initialize optimizer 1. initialize scheduler 1. add parameter group optimizer 1. raise error learning rate changed expected behavior add parameter group optimizer, attribute scheduler updated avoid error. environment pytorch version 1.1.0",0,reducelronplateau fail add new parameter group optimizer,"reducelronplateau fail add new parameter group optimizer bug reducelronplateau fail add new parameter group optimizer. function raise list index range error reproduce steps reproduce behavior 1. initialize optimizer 1. initialize scheduler 1. add parameter group optimizer 1. raise error learning rate changed expected behavior add parameter group optimizer, attribute scheduler updated avoid error. environment pytorch version 1.1.0"
pytorch,22687,"22681 get unhelpful error message says cuda kernels missing, say missing. would good dispatch stub gave information. maybe pass string constant template parameter update macro stringify struct name pass in. cc cpuhrsch vitalyfedyunin colesbury",0,dispatchstub report operator failed find kernel,"dispatchstub report operator failed find kernel 22681 get unhelpful error message says cuda kernels missing, say missing. would good dispatch stub gave information. maybe pass string constant template parameter update macro stringify struct name pass in. cc cpuhrsch vitalyfedyunin colesbury"
pytorch,27406,cc suo,0,jit string frontend support default arg values,jit string frontend support default arg values cc suo
pytorch,15163,"size average true reduce true, loss x,class weights class x class n",0,doc error torch.nn.nllloss,"doc error torch.nn.nllloss size average true reduce true, loss x,class weights class x class n"
pytorch,17126,"feature allow callables notably scripted functions scriptmodules passed scripted functions currently fails . motivation attempting tease fusions larger codebases case maskrcnn benchmark blocks code may scriptable, subsets blocks may be. concrete example, module contains loop sets inputs modules, seen. scriptmodule, like able add fused group that's already generated within scriptmodule. however, several things prevent outermost module scripted. pulling relevant code separate function like allow desired fusion, currently supported. pitch allow callables least scriptmodules passed arguments scripted functions called. alternatives specific use case disappears scripting supports certain level python, would expect use cases. additional context proxy code wrote make sure error seeing larger application. fails . cc suo",0,support callables scripted functions,"support callables scripted functions feature allow callables notably scripted functions scriptmodules passed scripted functions currently fails . motivation attempting tease fusions larger codebases case maskrcnn benchmark blocks code may scriptable, subsets blocks may be. concrete example, module contains loop sets inputs modules, seen. scriptmodule, like able add fused group that's already generated within scriptmodule. however, several things prevent outermost module scripted. pulling relevant code separate function like allow desired fusion, currently supported. pitch allow callables least scriptmodules passed arguments scripted functions called. alternatives specific use case disappears scripting supports certain level python, would expect use cases. additional context proxy code wrote make sure error seeing larger application. fails . cc suo"
pytorch,27034,"traceback recent call last file usr vmz master 1 tools train net.py , line 586, main file usr vmz master 1 tools train net.py , line 581, main train args file usr vmz master 1 tools train net.py , line 404, train workspace.createnet test model.net file root pytorch build caffe2 python workspace.py , line 181, createnet stringifyproto net , overwrite, file root pytorch build caffe2 python workspace.py , line 215, callwithexceptionintercept return func args, kwargs runtimeerror enforce fail context.h 48 option.device type proto cpu. 1 vs 0 frame 0 c10 throwenforcenotmet char const, int, char const, std cxx11 basic string const , void const 0x78 0x7fed19c32178 usr local lib libc10.so frame 1 0x2686d70 0x7fecdcf03d70 usr local lib libtorch.so frame 2 0x2723fec 0x7fecdcfa0fec usr local lib libtorch.so frame 3 0x3aff5ee 0x7fecde37c5ee usr local lib libtorch.so frame 4 std function handler caffe2 operatordef const , caffe2 workspace , std unique ptr caffe2 operatordef const , caffe2 workspace invoke std data const , caffe2 operatordef const , caffe2 workspace 0x23 0x7fed1a4b5433 root pytorch build caffe2 python caffe2 pybind11 state gpu.so frame 5 0x236c25c 0x7fecdcbe925c usr local lib libtorch.so frame 6 caffe2 createoperator caffe2 operatordef const , caffe2 workspace , int 0x328 0x7fecdcbea528 usr local lib libtorch.so frame 7 caffe2 dag utils prepareoperatornodes std shared ptr const , caffe2 workspace 0x2ad 0x7fecdcbda06d usr local lib libtorch.so frame 8 caffe2 asyncnetbase asyncnetbase std shared ptr const , caffe2 workspace 0x24d 0x7fecdcbb670d usr local lib libtorch.so frame 9 caffe2 asyncschedulingnet asyncschedulingnet std shared ptr const , caffe2 workspace 0x9 0x7fecdcbbb5b9 usr local lib libtorch.so frame 10 0x23410ae 0x7fecdcbbe0ae usr local lib libtorch.so frame 11 std function handler std shared ptr const , caffe2 workspace , std unique ptr std shared ptr const , caffe2 workspace invoke std data const , std shared ptr const , caffe2 workspace 0x23 0x7fecdcbbdf83 usr local lib libtorch.so frame 12 caffe2 createnet std shared ptr const , caffe2 workspace 0x4a5 0x7fecdcbb0495 usr local lib libtorch.so frame 13 caffe2 workspace createnet std shared ptr const , bool 0x103 0x7fecdcc2fe23 usr local lib libtorch.so frame 14 caffe2 workspace createnet caffe2 netdef const , bool 0x91 0x7fecdcc30d61 usr local lib libtorch.so frame 15 0x57906 0x7fed1a4ad906 root pytorch build caffe2 python caffe2 pybind11 state gpu.so frame 16 0x57bd2 0x7fed1a4adbd2 root pytorch build caffe2 python caffe2 pybind11 state gpu.so frame 17 0x99e3d 0x7fed1a4efe3d root pytorch build caffe2 python caffe2 pybind11 state gpu.so frame 33 libc start main 0xe7 0x7fed1e897b97 lib x86 64 linux gnu libc.so.6",0,runtimeerror enforce fail context.h 48 option.device type proto cpu. 1vs0,"runtimeerror enforce fail context.h 48 option.device type proto cpu. 1vs0 traceback recent call last file usr vmz master 1 tools train net.py , line 586, main file usr vmz master 1 tools train net.py , line 581, main train args file usr vmz master 1 tools train net.py , line 404, train workspace.createnet test model.net file root pytorch build caffe2 python workspace.py , line 181, createnet stringifyproto net , overwrite, file root pytorch build caffe2 python workspace.py , line 215, callwithexceptionintercept return func args, kwargs runtimeerror enforce fail context.h 48 option.device type proto cpu. 1 vs 0 frame 0 c10 throwenforcenotmet char const, int, char const, std cxx11 basic string const , void const 0x78 0x7fed19c32178 usr local lib libc10.so frame 1 0x2686d70 0x7fecdcf03d70 usr local lib libtorch.so frame 2 0x2723fec 0x7fecdcfa0fec usr local lib libtorch.so frame 3 0x3aff5ee 0x7fecde37c5ee usr local lib libtorch.so frame 4 std function handler caffe2 operatordef const , caffe2 workspace , std unique ptr caffe2 operatordef const , caffe2 workspace invoke std data const , caffe2 operatordef const , caffe2 workspace 0x23 0x7fed1a4b5433 root pytorch build caffe2 python caffe2 pybind11 state gpu.so frame 5 0x236c25c 0x7fecdcbe925c usr local lib libtorch.so frame 6 caffe2 createoperator caffe2 operatordef const , caffe2 workspace , int 0x328 0x7fecdcbea528 usr local lib libtorch.so frame 7 caffe2 dag utils prepareoperatornodes std shared ptr const , caffe2 workspace 0x2ad 0x7fecdcbda06d usr local lib libtorch.so frame 8 caffe2 asyncnetbase asyncnetbase std shared ptr const , caffe2 workspace 0x24d 0x7fecdcbb670d usr local lib libtorch.so frame 9 caffe2 asyncschedulingnet asyncschedulingnet std shared ptr const , caffe2 workspace 0x9 0x7fecdcbbb5b9 usr local lib libtorch.so frame 10 0x23410ae 0x7fecdcbbe0ae usr local lib libtorch.so frame 11 std function handler std shared ptr const , caffe2 workspace , std unique ptr std shared ptr const , caffe2 workspace invoke std data const , std shared ptr const , caffe2 workspace 0x23 0x7fecdcbbdf83 usr local lib libtorch.so frame 12 caffe2 createnet std shared ptr const , caffe2 workspace 0x4a5 0x7fecdcbb0495 usr local lib libtorch.so frame 13 caffe2 workspace createnet std shared ptr const , bool 0x103 0x7fecdcc2fe23 usr local lib libtorch.so frame 14 caffe2 workspace createnet caffe2 netdef const , bool 0x91 0x7fecdcc30d61 usr local lib libtorch.so frame 15 0x57906 0x7fed1a4ad906 root pytorch build caffe2 python caffe2 pybind11 state gpu.so frame 16 0x57bd2 0x7fed1a4adbd2 root pytorch build caffe2 python caffe2 pybind11 state gpu.so frame 17 0x99e3d 0x7fed1a4efe3d root pytorch build caffe2 python caffe2 pybind11 state gpu.so frame 33 libc start main 0xe7 0x7fed1e897b97 lib x86 64 linux gnu libc.so.6"
pytorch,23054,"bug concatdataset returns different error messages setting range plus index minus index. reproduce expected behavior think better x 100 x 100 error message. way solve chage lines https github.com pytorch pytorch blob master torch utils data dataset.py l197 l200 like this. environment pytorch version e.g., 1.0 1.1.0 os e.g., linux ubuntu 18.04 installed pytorch , , source pip python version python 3.7.3 cuda cudnn version none gpu models configuration none",0,concatdataset returns different error messages setting range plus index minus index.,"concatdataset returns different error messages setting range plus index minus index. bug concatdataset returns different error messages setting range plus index minus index. reproduce expected behavior think better x 100 x 100 error message. way solve chage lines https github.com pytorch pytorch blob master torch utils data dataset.py l197 l200 like this. environment pytorch version e.g., 1.0 1.1.0 os e.g., linux ubuntu 18.04 installed pytorch , , source pip python version python 3.7.3 cuda cudnn version none gpu models configuration none"
pytorch,31557,"upd summary long discussion discoverability 1. f.ctc loss produce inf loss presented invalid unalignable examples 2. invalid examples may generated official usage code example one extremely unlucky one twists dimension sizes little bit 3. presented invalid examples, sum mean reduction modes default cause whole batch loss inf proposals 1. docs warn clearly conditions valid examples 2. docs warn clearly official usage example may produce invalid examples fix official code example 3. enable zero infinity true default least reduction modes sum mean original issue description docs specify targets can't blank. consecutive labels supported, think explicitly mentioned docs. maybe docs specify workaround encode consecutive valued targets given blank separate allowed docs . current docs",0,docs f.ctc loss docs warn clearly invalid inf causing inputs zero infinity become enabled default,"docs f.ctc loss docs warn clearly invalid inf causing inputs zero infinity become enabled default upd summary long discussion discoverability 1. f.ctc loss produce inf loss presented invalid unalignable examples 2. invalid examples may generated official usage code example one extremely unlucky one twists dimension sizes little bit 3. presented invalid examples, sum mean reduction modes default cause whole batch loss inf proposals 1. docs warn clearly conditions valid examples 2. docs warn clearly official usage example may produce invalid examples fix official code example 3. enable zero infinity true default least reduction modes sum mean original issue description docs specify targets can't blank. consecutive labels supported, think explicitly mentioned docs. maybe docs specify workaround encode consecutive valued targets given blank separate allowed docs . current docs"
pytorch,9945,seems toggle many warnings caffe2 fails.,0,werror 1 work full caffe2,werror 1 work full caffe2 seems toggle many warnings caffe2 fails.
pytorch,20165,feature think useful pass threshold tensor threshold function. way compute backprop threshold tensor use training process. right threshold function takes non tensor threshold,0,torch.nn.threshold cannot accept tensor threshold,torch.nn.threshold cannot accept tensor threshold feature think useful pass threshold tensor threshold function. way compute backprop threshold tensor use training process. right threshold function takes non tensor threshold
pytorch,6265,"init net file generated mobile exporter function code calling information, fragment information generated init net file 146 giventensorfill shape0 values 6 ? ?r file style torch.py , line 186, init net, predict net mobile exporter.export c2 workspace, c2 model, c2 model.external input file home wguo lib temp1 caffe2 build caffe2 python predictor mobile exporter.py , line 86, export add tensor init net, blob name, blob file home wguo lib temp1 caffe2 build caffe2 python predictor mobile exporter.py , line 53, add tensor utils.makeargument values , values ,",0,caffe2 mobile exporter init net code calling information,"caffe2 mobile exporter init net code calling information init net file generated mobile exporter function code calling information, fragment information generated init net file 146 giventensorfill shape0 values 6 ? ?r file style torch.py , line 186, init net, predict net mobile exporter.export c2 workspace, c2 model, c2 model.external input file home wguo lib temp1 caffe2 build caffe2 python predictor mobile exporter.py , line 86, export add tensor init net, blob name, blob file home wguo lib temp1 caffe2 build caffe2 python predictor mobile exporter.py , line 53, add tensor utils.makeargument values , values ,"
pytorch,18645,"visual studio2017 reference directory pytorch libtorch include torch csrc api include pytorch libtorch include library directory pytorch libtorch lib .... 101 document rnn.h. there's mistake.have error cast function type illegal pytorch https download.pytorch.org libtorch cu100 libtorch win shared deps latest.zip cuda 10.0 windwos 10 visual studio 2017 release x64 cl.exe 14.16.27023 think compiler error. this, windows installation tutorials, compare difference. select compilation environment version.",0,c windows compile error,"c windows compile error visual studio2017 reference directory pytorch libtorch include torch csrc api include pytorch libtorch include library directory pytorch libtorch lib .... 101 document rnn.h. there's mistake.have error cast function type illegal pytorch https download.pytorch.org libtorch cu100 libtorch win shared deps latest.zip cuda 10.0 windwos 10 visual studio 2017 release x64 cl.exe 14.16.27023 think compiler error. this, windows installation tutorials, compare difference. select compilation environment version."
pytorch,19969,"bug tl dr traced non pretrained resnet18 model python saved .pt. successfully loaded .pt c program. model inference seems work. c program exits, encounter segmentation fault. reproduce code snippets minimally reproduce issue 1 first, generate model running following python script. successful, see 0 print out. able run code problems. 2 c program run generates segmentation fault. run, run c program, here's output receive things tried tried running c code cpu so, replacing . result segmentation fault. tried running python c code cpu, gpu simple feedforward net 30 inputs 20 hiddens relu 2 outputs sigmoid result segmentation fault. running cpu, see issues. gpu, however here's get expected behavior running hard.pt, expect code run without segfaults. environment pytorch version e.g., 1.0 1.1.0.dev20190425 os e.g., linux red hat enterprise linux server 7.4 maipo installed pytorch , , source build command used compiling source python version cuda cudnn version 8.0.44 7 gpu models configuration versions relevant libraries pip numpy 1.15.4 pip numpydoc 0.8.0 pip torch 1.0.1.post2 pip torchvision 0.2.2 conda blas 1.0 mkl conda mkl 2019.1 144 conda mkl service 1.1.2 py37he904b0f 5 conda mkl fft 1.0.6 py37hd81dba3 0 conda mkl random 1.0.2 py37hd81dba3 0 conda pytorch 1.0.1 py3.7 cuda8.0.61 cudnn7.1.2 2 pytorch conda torchvision 0.2.2 py 3 pytorch additional context tried following instructions resolve issue https github.com pytorch pytorch issues 12705 seen success tried building pytorch source linking c code generated libs, work either. cc ezyang gchanan zou3519 ngimel",0,libtorch segmentation fault rhel 7 easy reproduce,"libtorch segmentation fault rhel 7 easy reproduce bug tl dr traced non pretrained resnet18 model python saved .pt. successfully loaded .pt c program. model inference seems work. c program exits, encounter segmentation fault. reproduce code snippets minimally reproduce issue 1 first, generate model running following python script. successful, see 0 print out. able run code problems. 2 c program run generates segmentation fault. run, run c program, here's output receive things tried tried running c code cpu so, replacing . result segmentation fault. tried running python c code cpu, gpu simple feedforward net 30 inputs 20 hiddens relu 2 outputs sigmoid result segmentation fault. running cpu, see issues. gpu, however here's get expected behavior running hard.pt, expect code run without segfaults. environment pytorch version e.g., 1.0 1.1.0.dev20190425 os e.g., linux red hat enterprise linux server 7.4 maipo installed pytorch , , source build command used compiling source python version cuda cudnn version 8.0.44 7 gpu models configuration versions relevant libraries pip numpy 1.15.4 pip numpydoc 0.8.0 pip torch 1.0.1.post2 pip torchvision 0.2.2 conda blas 1.0 mkl conda mkl 2019.1 144 conda mkl service 1.1.2 py37he904b0f 5 conda mkl fft 1.0.6 py37hd81dba3 0 conda mkl random 1.0.2 py37hd81dba3 0 conda pytorch 1.0.1 py3.7 cuda8.0.61 cudnn7.1.2 2 pytorch conda torchvision 0.2.2 py 3 pytorch additional context tried following instructions resolve issue https github.com pytorch pytorch issues 12705 seen success tried building pytorch source linking c code generated libs, work either. cc ezyang gchanan zou3519 ngimel"
pytorch,11389,"issue description use expanded tensors distribution parameters many cases dynamically broadcast parameters runtime. working related https github.com pytorch pytorch pull 11341 pr, noticed sampling slow using expanded tensors distribution parameters. narrowed slowdown native torch samplers. likely expected behavior, raises question whether anything inside distributions ensure parameter tensors contiguous, so, b conditions ensure contiguity always default, controllable user via optional keyword argument. use instance draw multiple samples, worth one time cost calling distribution parameters think, given relatively low overhead , probably make default . profiling code cc. fritzo, vishwakftw cc fritzo neerajprad alicanb vishwakftw nikitaved",0,distributions torch distribution samplers slow expanded parameters,"distributions torch distribution samplers slow expanded parameters issue description use expanded tensors distribution parameters many cases dynamically broadcast parameters runtime. working related https github.com pytorch pytorch pull 11341 pr, noticed sampling slow using expanded tensors distribution parameters. narrowed slowdown native torch samplers. likely expected behavior, raises question whether anything inside distributions ensure parameter tensors contiguous, so, b conditions ensure contiguity always default, controllable user via optional keyword argument. use instance draw multiple samples, worth one time cost calling distribution parameters think, given relatively low overhead , probably make default . profiling code cc. fritzo, vishwakftw cc fritzo neerajprad alicanb vishwakftw nikitaved"
pytorch,15284,"hi, wondering whether could update functions like third argument also scalar value?",0,index add scalar values instead tensors,"index add scalar values instead tensors hi, wondering whether could update functions like third argument also scalar value?"
pytorch,14659,"bug reproduce steps reproduce behavior 1. followed tutorial code https pytorch.org tutorials beginner data loading tutorial.html 1. run without change anything, error comes out. even download official code, error still exist. 1. seems like cannot enumerate trainloader. enumerate trainloader , error appears module multiprocessing.util' attribute ' flush std streams expected behavior environment pytorch version 0.4.1 debug build cuda used build pytorch 8.0.61 os ubuntu 16.04 lts gcc version ubuntu 4.9.3 13ubuntu2 4.9.3 cmake version version 3.5.1 python version 3.6 cuda available yes cuda runtime version could collect gpu models configuration gpu 0 tesla p100 pcie 16gb gpu 1 tesla p100 pcie 16gb gpu 2 tesla p100 pcie 16gb gpu 3 tesla p100 pcie 16gb gpu 4 tesla p100 pcie 16gb gpu 5 tesla p100 pcie 16gb gpu 6 tesla p100 pcie 16gb gpu 7 tesla p100 pcie 16gb nvidia driver version 384.90 cudnn version probably one following usr local matlab r2016b bin glnxa64 libcudnn.so.4.0.7 versions relevant libraries pip numpy 1.15.4 pip torch 0.4.1 pip torchvision 0.2.1 conda cuda80 1.0 h205658b 0 pytorch conda pytorch 0.4.1 py36 cuda8.0.61 cudnn7.1.2 1 cuda80 pytorch conda torchvision 0.2.1 py36 1 pytorch additional context attributeerror traceback recent call last 21 plt.title 'batch dataloader' 22 23 batch, sample batched enumerate dataloader 24 print batch, sample batched 'image' .size , 25 sample batched 'landmarks' .size anaconda3 envs pytorch lib python3.6 site packages torch utils data dataloader.py iter self 499 500 def iter self 501 return dataloaderiter self 502 503 def len self anaconda3 envs pytorch lib python3.6 site packages torch utils data dataloader.py init self, loader 287 w self.workers 288 w.daemon true ensure worker exits process exit 289 w.start 290 291 update worker pids id self , tuple w.pid w self.workers anaconda3 envs pytorch lib python3.6 multiprocessing process.py start self 103 'daemonic processes allowed children' 104 cleanup 105 self. popen self. popen self 106 self. sentinel self. popen.sentinel 107 avoid refcycle target function holds indirect anaconda3 envs pytorch lib python3.6 multiprocessing context.py popen process obj 221 staticmethod 222 def popen process obj 223 return default context.get context .process. popen process obj 224 225 class defaultcontext basecontext anaconda3 envs pytorch lib python3.6 multiprocessing context.py popen process obj 275 def popen process obj 276 .popen fork import popen 277 return popen process obj 278 279 class spawnprocess process.baseprocess anaconda3 envs pytorch lib python3.6 multiprocessing popen fork.py init self, process obj 15 16 def init self, process obj 17 util. flush std streams 18 self.returncode none 19 self. launch process obj attributeerror module 'multiprocessing.util' attribute ' flush std streams' test code windows, still got error errno 32 broken pipe",0,error module multiprocessing.util' attribute ' flush std streams,"error module multiprocessing.util' attribute ' flush std streams bug reproduce steps reproduce behavior 1. followed tutorial code https pytorch.org tutorials beginner data loading tutorial.html 1. run without change anything, error comes out. even download official code, error still exist. 1. seems like cannot enumerate trainloader. enumerate trainloader , error appears module multiprocessing.util' attribute ' flush std streams expected behavior environment pytorch version 0.4.1 debug build cuda used build pytorch 8.0.61 os ubuntu 16.04 lts gcc version ubuntu 4.9.3 13ubuntu2 4.9.3 cmake version version 3.5.1 python version 3.6 cuda available yes cuda runtime version could collect gpu models configuration gpu 0 tesla p100 pcie 16gb gpu 1 tesla p100 pcie 16gb gpu 2 tesla p100 pcie 16gb gpu 3 tesla p100 pcie 16gb gpu 4 tesla p100 pcie 16gb gpu 5 tesla p100 pcie 16gb gpu 6 tesla p100 pcie 16gb gpu 7 tesla p100 pcie 16gb nvidia driver version 384.90 cudnn version probably one following usr local matlab r2016b bin glnxa64 libcudnn.so.4.0.7 versions relevant libraries pip numpy 1.15.4 pip torch 0.4.1 pip torchvision 0.2.1 conda cuda80 1.0 h205658b 0 pytorch conda pytorch 0.4.1 py36 cuda8.0.61 cudnn7.1.2 1 cuda80 pytorch conda torchvision 0.2.1 py36 1 pytorch additional context attributeerror traceback recent call last 21 plt.title 'batch dataloader' 22 23 batch, sample batched enumerate dataloader 24 print batch, sample batched 'image' .size , 25 sample batched 'landmarks' .size anaconda3 envs pytorch lib python3.6 site packages torch utils data dataloader.py iter self 499 500 def iter self 501 return dataloaderiter self 502 503 def len self anaconda3 envs pytorch lib python3.6 site packages torch utils data dataloader.py init self, loader 287 w self.workers 288 w.daemon true ensure worker exits process exit 289 w.start 290 291 update worker pids id self , tuple w.pid w self.workers anaconda3 envs pytorch lib python3.6 multiprocessing process.py start self 103 'daemonic processes allowed children' 104 cleanup 105 self. popen self. popen self 106 self. sentinel self. popen.sentinel 107 avoid refcycle target function holds indirect anaconda3 envs pytorch lib python3.6 multiprocessing context.py popen process obj 221 staticmethod 222 def popen process obj 223 return default context.get context .process. popen process obj 224 225 class defaultcontext basecontext anaconda3 envs pytorch lib python3.6 multiprocessing context.py popen process obj 275 def popen process obj 276 .popen fork import popen 277 return popen process obj 278 279 class spawnprocess process.baseprocess anaconda3 envs pytorch lib python3.6 multiprocessing popen fork.py init self, process obj 15 16 def init self, process obj 17 util. flush std streams 18 self.returncode none 19 self. launch process obj attributeerror module 'multiprocessing.util' attribute ' flush std streams' test code windows, still got error errno 32 broken pipe"
pytorch,17231,"bug unit test pass ind tor.longtensor 110, 125, 235, 333, 404 .cuda nd shape tor.longtensor 10, 10, 10 .cuda xy array nd index ind, nd shape result 1, 1, 0 , 1, 2, 5 , 2, 3, 5 , 3, 3, 3 , 4, 0, 4 tortest.assert allclose result, xy however, get runtimeerror expected type torch.longtensor got torch.cuda.longtensor here's solution wrote assert function, though may bit permissive a.is cuda b.is cuda b b.cuda elif b.is cuda a.is cuda a.cuda environment pytorch version e.g., 1.0 1.0.1 os e.g., linux windows installed pytorch , , source pip python version 3.6 cuda cudnn version 0.9",0,testing.assert allclose assert tensors devices,"testing.assert allclose assert tensors devices bug unit test pass ind tor.longtensor 110, 125, 235, 333, 404 .cuda nd shape tor.longtensor 10, 10, 10 .cuda xy array nd index ind, nd shape result 1, 1, 0 , 1, 2, 5 , 2, 3, 5 , 3, 3, 3 , 4, 0, 4 tortest.assert allclose result, xy however, get runtimeerror expected type torch.longtensor got torch.cuda.longtensor here's solution wrote assert function, though may bit permissive a.is cuda b.is cuda b b.cuda elif b.is cuda a.is cuda a.cuda environment pytorch version e.g., 1.0 1.0.1 os e.g., linux windows installed pytorch , , source pip python version 3.6 cuda cudnn version 0.9"
pytorch,12609,"feature creating precompiled pytorch wheel file trimmed down, inference version. motivation right pytorch wheels average 400mb zipped 1. gb unzipped, big deal training prototyping generally wheels installed that's case productionizing using service providers like sagemaker algorithmia etc. pitch create trimmed down, potentially inference capable wheel file directly improve load time performance algorithms serverless algorithm delivery environments, could directly pytorch's ability compete hpc serverless marketplace. alternatives could also provide clear way users create wheels, simplifying documenting build process somewhat enable optional features compilation process. additional context full disclosure, employee algorithmia change would make life much easier smile cc malfet seemethere walterddr",0,request stripped inference pytorch wheels,"request stripped inference pytorch wheels feature creating precompiled pytorch wheel file trimmed down, inference version. motivation right pytorch wheels average 400mb zipped 1. gb unzipped, big deal training prototyping generally wheels installed that's case productionizing using service providers like sagemaker algorithmia etc. pitch create trimmed down, potentially inference capable wheel file directly improve load time performance algorithms serverless algorithm delivery environments, could directly pytorch's ability compete hpc serverless marketplace. alternatives could also provide clear way users create wheels, simplifying documenting build process somewhat enable optional features compilation process. additional context full disclosure, employee algorithmia change would make life much easier smile cc malfet seemethere walterddr"
pytorch,31591,"bug reproduce steps reproduce behavior users duke opt anaconda3 bin python volumes project code pytorch2onnx.py volumes project code models anchors.py 24 tracerwarning converting tensor python integer might cause trace incorrect. can't record data flow python values, value treated constant future. means trace might generalize inputs! image shape np.array image shape volumes project code models anchors.py 35 tracerwarning torch.from numpy results registered constants trace. safely ignore warning use function create tensors constant variables would every time call function. case, might cause trace incorrect. return torch.from numpy anchors.astype np.float32 traceback recent call last file volumes project code pytorch2onnx.py , line 39, save2onnx args file volumes project code pytorch2onnx.py , line 27, save2onnx torch.onnx.export model, x, export onnx file file users duke opt anaconda3 lib python3.7 site packages torch onnx init .py , line 143, export strip doc string, dynamic axes, keep initializers inputs file users duke opt anaconda3 lib python3.7 site packages torch onnx utils.py , line 66, export dynamic axes dynamic axes, keep initializers inputs keep initializers inputs file users duke opt anaconda3 lib python3.7 site packages torch onnx utils.py , line 382, export fixed batch size fixed batch size file users duke opt anaconda3 lib python3.7 site packages torch onnx utils.py , line 249, model graph graph, torch trace get graph model model, args, training file users duke opt anaconda3 lib python3.7 site packages torch onnx utils.py , line 206, trace get graph model trace, torch out, inputs states torch.jit.get trace graph model, args, force outplace true, return inputs states true file users duke opt anaconda3 lib python3.7 site packages torch jit init .py , line 275, get trace graph return legacytracedmodule f, force outplace, return inputs, return inputs states args, kwargs file users duke opt anaconda3 lib python3.7 site packages torch nn modules module.py , line 541, call result self.forward input, kwargs file users duke opt anaconda3 lib python3.7 site packages torch jit init .py , line 356, forward torch. c. tracer exit tuple vars runtimeerror output traced region observable data dependence trace inputs probably indicates program cannot understood tracer. process finished exit code 1 code import torch import torch.onnx torch.autograd import variable models.retinanet import resnet34, resnet50 build network import build network input model '.. user data model data model epoch 1.dat' output dir 'torch onnx model.onnx' dummy input torch.randn 2, 3, 800, 800 checkpoint torch.load input model model, build network snapshot none, backend 'retinanet' model.load state dict checkpoint 'state dict' torch.onnx.export model, dummy input, output dir print 'done' expected behavior onnx exported successfully. environment python 3.7.4 pytorch '1.3.1' macos 10.15 cc suo",0,"try export pytorch model onnx, got runtimeerror output traced region observable data dependence trace inputs probably indicates program cannot understood tracer.","try export pytorch model onnx, got runtimeerror output traced region observable data dependence trace inputs probably indicates program cannot understood tracer. bug reproduce steps reproduce behavior users duke opt anaconda3 bin python volumes project code pytorch2onnx.py volumes project code models anchors.py 24 tracerwarning converting tensor python integer might cause trace incorrect. can't record data flow python values, value treated constant future. means trace might generalize inputs! image shape np.array image shape volumes project code models anchors.py 35 tracerwarning torch.from numpy results registered constants trace. safely ignore warning use function create tensors constant variables would every time call function. case, might cause trace incorrect. return torch.from numpy anchors.astype np.float32 traceback recent call last file volumes project code pytorch2onnx.py , line 39, save2onnx args file volumes project code pytorch2onnx.py , line 27, save2onnx torch.onnx.export model, x, export onnx file file users duke opt anaconda3 lib python3.7 site packages torch onnx init .py , line 143, export strip doc string, dynamic axes, keep initializers inputs file users duke opt anaconda3 lib python3.7 site packages torch onnx utils.py , line 66, export dynamic axes dynamic axes, keep initializers inputs keep initializers inputs file users duke opt anaconda3 lib python3.7 site packages torch onnx utils.py , line 382, export fixed batch size fixed batch size file users duke opt anaconda3 lib python3.7 site packages torch onnx utils.py , line 249, model graph graph, torch trace get graph model model, args, training file users duke opt anaconda3 lib python3.7 site packages torch onnx utils.py , line 206, trace get graph model trace, torch out, inputs states torch.jit.get trace graph model, args, force outplace true, return inputs states true file users duke opt anaconda3 lib python3.7 site packages torch jit init .py , line 275, get trace graph return legacytracedmodule f, force outplace, return inputs, return inputs states args, kwargs file users duke opt anaconda3 lib python3.7 site packages torch nn modules module.py , line 541, call result self.forward input, kwargs file users duke opt anaconda3 lib python3.7 site packages torch jit init .py , line 356, forward torch. c. tracer exit tuple vars runtimeerror output traced region observable data dependence trace inputs probably indicates program cannot understood tracer. process finished exit code 1 code import torch import torch.onnx torch.autograd import variable models.retinanet import resnet34, resnet50 build network import build network input model '.. user data model data model epoch 1.dat' output dir 'torch onnx model.onnx' dummy input torch.randn 2, 3, 800, 800 checkpoint torch.load input model model, build network snapshot none, backend 'retinanet' model.load state dict checkpoint 'state dict' torch.onnx.export model, dummy input, output dir print 'done' expected behavior onnx exported successfully. environment python 3.7.4 pytorch '1.3.1' macos 10.15 cc suo"
pytorch,28472,"bug even official docs cpp extensions https pytorch.org tutorials advanced cpp extension.html use get deprecation warning. reproduce warning tensor data const float deprecated wdeprecated declarations home ehazar miniconda3 envs py3 night lib python3.6 site packages torch include aten core tensorbody.h 312 1 note declared data const expected behavior provide non deprecated way cpp extensions. similarly, alternative provided cpp extensions. environment collecting environment information... pytorch version 1.4.0.dev20191018 debug build cuda used build pytorch 10.0 os ubuntu 16.04.6 lts gcc version ubuntu 5.4.0 6ubuntu1 16.04.11 5.4.0 20160609 cmake version version 3.14.0 python version 3.6 cuda available yes cuda runtime version could collect gpu models configuration gpu 0 geforce gtx 1070 nvidia driver version 418.40.04 cudnn version usr local lib libcudnn.so.5.1.10 versions relevant libraries pip numpy 1.17.2 pip torch 1.4.0.dev20191018 pip torchvision 0.5.0a0 155c504 conda could collect cc yf225",0,tensor data deprecated way suggested cpp extensions,"tensor data deprecated way suggested cpp extensions bug even official docs cpp extensions https pytorch.org tutorials advanced cpp extension.html use get deprecation warning. reproduce warning tensor data const float deprecated wdeprecated declarations home ehazar miniconda3 envs py3 night lib python3.6 site packages torch include aten core tensorbody.h 312 1 note declared data const expected behavior provide non deprecated way cpp extensions. similarly, alternative provided cpp extensions. environment collecting environment information... pytorch version 1.4.0.dev20191018 debug build cuda used build pytorch 10.0 os ubuntu 16.04.6 lts gcc version ubuntu 5.4.0 6ubuntu1 16.04.11 5.4.0 20160609 cmake version version 3.14.0 python version 3.6 cuda available yes cuda runtime version could collect gpu models configuration gpu 0 geforce gtx 1070 nvidia driver version 418.40.04 cudnn version usr local lib libcudnn.so.5.1.10 versions relevant libraries pip numpy 1.17.2 pip torch 1.4.0.dev20191018 pip torchvision 0.5.0a0 155c504 conda could collect cc yf225"
pytorch,7773,"issue description mentioned paper http yann.lecun.com exdb publis pdf jarrett iccv 09.pdf noticed local response norm present. good addition too. implementation ready create pr soon, approved. cc alband mruberry",0,feature request add local contrast normalization,"feature request add local contrast normalization issue description mentioned paper http yann.lecun.com exdb publis pdf jarrett iccv 09.pdf noticed local response norm present. good addition too. implementation ready create pr soon, approved. cc alband mruberry"
pytorch,20117,"common pattern people use build first feed support many handy methods existing . e.g., totally unnecessary provide .",0,fr rfc add sequential.append .extend,"fr rfc add sequential.append .extend common pattern people use build first feed support many handy methods existing . e.g., totally unnecessary provide ."
pytorch,8386,"right simply leaking them, nccl segfaults attempt destroy driver unloaded. one strategy fix would use handler. see 8352 context.",0,properly release nccl resources,"properly release nccl resources right simply leaking them, nccl segfaults attempt destroy driver unloaded. one strategy fix would use handler. see 8352 context."
pytorch,14366,"want clip loss using clip op like model.net.clip input,output , 0,10 dosen't work!! operators catalog caffe2 simple. anyone help me? thank much!!!",0,use model.net.clip?,"use model.net.clip? want clip loss using clip op like model.net.clip input,output , 0,10 dosen't work!! operators catalog caffe2 simple. anyone help me? thank much!!!"
pytorch,11978,"issue description hello, trying install caffe2 google colab work detectron. tried installing source however takes long 2 hours impractical colab. thus, forced install binaries using anaconda. install anaconda without much issues. however, trying follow recommendation https caffe2.ai docs faq.html caffe2 working expected anaconda install caffe2 different environment base one. manage create environment fine, however, trying activate work. particular using seems nothing. seems that's out. so, go install caffe2 using base anaconda environment. first, add code add anaconda binaries path, seems usual way assign environmental variables colab seem work. installation usual installs without issues. however, trying run test code returns understand probably pythonpath issue https github.com facebookresearch detectron blob master install.md caffe2 , however, can't seem find build folder caffe2. search caffe2 directory installing get following results search directories result get ls root anaconda2 pkgs caffe2 cuda9.0 cudnn7 0.8.dev py27 2018.08.26 lib python2.7 site packages caffe2 ls root anaconda2 pkgs caffe2 cuda9.0 cudnn7 0.8.dev py27 2018.08.26 include caffe2 ls root anaconda2 lib python2.7 site packages caffe2 ls root anaconda2 include caffe2 none build directory. finally questions caffe2 installation successful? wonder given cannot find 'build' directory. pythonpath variable issue? so, folder point it? sorry question noobish. bit stuck. thanks advance! system settings pytorch caffe2 caffe2 installed pytorch conda, pip, source conda build command used compiling source os linux, whatever colab's using. pytorch version python version 2.7 cuda cudnn version 9.0 7.0 gpu models configuration tesla k80 gcc version compiling source cmake version versions relevant libraries",0,caffe2 attempting install caffe2 google colab,"caffe2 attempting install caffe2 google colab issue description hello, trying install caffe2 google colab work detectron. tried installing source however takes long 2 hours impractical colab. thus, forced install binaries using anaconda. install anaconda without much issues. however, trying follow recommendation https caffe2.ai docs faq.html caffe2 working expected anaconda install caffe2 different environment base one. manage create environment fine, however, trying activate work. particular using seems nothing. seems that's out. so, go install caffe2 using base anaconda environment. first, add code add anaconda binaries path, seems usual way assign environmental variables colab seem work. installation usual installs without issues. however, trying run test code returns understand probably pythonpath issue https github.com facebookresearch detectron blob master install.md caffe2 , however, can't seem find build folder caffe2. search caffe2 directory installing get following results search directories result get ls root anaconda2 pkgs caffe2 cuda9.0 cudnn7 0.8.dev py27 2018.08.26 lib python2.7 site packages caffe2 ls root anaconda2 pkgs caffe2 cuda9.0 cudnn7 0.8.dev py27 2018.08.26 include caffe2 ls root anaconda2 lib python2.7 site packages caffe2 ls root anaconda2 include caffe2 none build directory. finally questions caffe2 installation successful? wonder given cannot find 'build' directory. pythonpath variable issue? so, folder point it? sorry question noobish. bit stuck. thanks advance! system settings pytorch caffe2 caffe2 installed pytorch conda, pip, source conda build command used compiling source os linux, whatever colab's using. pytorch version python version 2.7 cuda cudnn version 9.0 7.0 gpu models configuration tesla k80 gcc version compiling source cmake version versions relevant libraries"
pytorch,31277,"feature motivation pitch current nn.multiheadattention uses matrix multiplication similarity, i.e., q k.t , variants similarity available directly, example, dot product similarity, i.e., , additive similarity, , general dot product similarity, . variants also pytorch. alternatives additional context cc zhangguanheng66",0,nn.multiheadattention different similarity measures,"nn.multiheadattention different similarity measures feature motivation pitch current nn.multiheadattention uses matrix multiplication similarity, i.e., q k.t , variants similarity available directly, example, dot product similarity, i.e., , additive similarity, , general dot product similarity, . variants also pytorch. alternatives additional context cc zhangguanheng66"
pytorch,25045,"bug passing input function, gradient w.r.t. input correctly calculated cuda device. reproduce steps reproduce behavior 3. increasing tensor size running error message expected behavior environment please copy paste output environment collection script https raw.githubusercontent.com pytorch pytorch master torch utils collect env.py fill checklist manually . get script run pytorch version 1.2.0 os e.g., linux centos 7.6.1810 core installed pytorch , , source conda build command used compiling source n python version 3.6.7 cuda cudnn version v10.0.130 gpu models configuration geforce gtx 1080 relevant information additional context cc ngimel",0,distance functions f.pdist backward cuda invalid configuration,"distance functions f.pdist backward cuda invalid configuration bug passing input function, gradient w.r.t. input correctly calculated cuda device. reproduce steps reproduce behavior 3. increasing tensor size running error message expected behavior environment please copy paste output environment collection script https raw.githubusercontent.com pytorch pytorch master torch utils collect env.py fill checklist manually . get script run pytorch version 1.2.0 os e.g., linux centos 7.6.1810 core installed pytorch , , source conda build command used compiling source n python version 3.6.7 cuda cudnn version v10.0.130 gpu models configuration geforce gtx 1080 relevant information additional context cc ngimel"
pytorch,15608,"hi, spent 1 hour looking documentation tutorials website, googling cannot figure put net testing phase. would like equivalent caffe2 .",0,caffe2 switch test phase?,"caffe2 switch test phase? hi, spent 1 hour looking documentation tutorials website, googling cannot figure put net testing phase. would like equivalent caffe2 ."
pytorch,26646,"peterjc123 propose drop cuda 9.2 binary build support windows, meaning provide cuda 10.1 binaries windows. reasoning strongly prefer single ci provider build binaries. moment, circleci provider however, supports single visual studio version, new cuda 9.2. run cuda 9.2 builds circleci, would install cuda 9.2 provide cuda 10.1 also older version msvc compatible cuda 9.2. workaround cuda 9.2 builds today, currently builds azure. however, agents provided microsoft build nightlies facing problems, release github actions tied resources available resolve problems. drop source support cuda 9.2 users always welcome build source need configuration. however, building source windows quite painful, peterjc123 concerned libtorch users, rely user's environment opposed packaging cuda . windows cuda 9.2 user would unduly affected change, please comment issue. thank you! cc ezyang peterjc123",0,rfc drop windows cuda 9.2 support,"rfc drop windows cuda 9.2 support peterjc123 propose drop cuda 9.2 binary build support windows, meaning provide cuda 10.1 binaries windows. reasoning strongly prefer single ci provider build binaries. moment, circleci provider however, supports single visual studio version, new cuda 9.2. run cuda 9.2 builds circleci, would install cuda 9.2 provide cuda 10.1 also older version msvc compatible cuda 9.2. workaround cuda 9.2 builds today, currently builds azure. however, agents provided microsoft build nightlies facing problems, release github actions tied resources available resolve problems. drop source support cuda 9.2 users always welcome build source need configuration. however, building source windows quite painful, peterjc123 concerned libtorch users, rely user's environment opposed packaging cuda . windows cuda 9.2 user would unduly affected change, please comment issue. thank you! cc ezyang peterjc123"
pytorch,3025,"believe useful function similar https docs.scipy.org doc numpy 1.13.0 reference generated numpy.in1d.html, compares tensor element wise list possible values. using filter labels classes classifiers . expected behavior possible implement iterating filter, storing results tensor operator. faster implementation possible th thc current implementation n, batch x samplessamples cc mruberry rgommers heitorschueroff",0,function request np.isin,"function request np.isin believe useful function similar https docs.scipy.org doc numpy 1.13.0 reference generated numpy.in1d.html, compares tensor element wise list possible values. using filter labels classes classifiers . expected behavior possible implement iterating filter, storing results tensor operator. faster implementation possible th thc current implementation n, batch x samplessamples cc mruberry rgommers heitorschueroff"
pytorch,3158,"hi, thanks great work. would like raise issue discussion implement following features dense m,1 sparse m,n sparse m,n sparse m,n dense 1,n sparse m,n sparse n dense b,1,n sparse b,m,n features fully supported cusparse level 3 https developer.nvidia.com cusparse , cpu side, quite sure weather mkl satisfied. would like help implementation needed, could anyone give brief guideline done still need do? cc ezyang gchanan zou3519 bdhirsh jbschlosser aocsa nikitaved pearu mruberry vincentqb",0,feature request sparse dense elementwise multiplication,"feature request sparse dense elementwise multiplication hi, thanks great work. would like raise issue discussion implement following features dense m,1 sparse m,n sparse m,n sparse m,n dense 1,n sparse m,n sparse n dense b,1,n sparse b,m,n features fully supported cusparse level 3 https developer.nvidia.com cusparse , cpu side, quite sure weather mkl satisfied. would like help implementation needed, could anyone give brief guideline done still need do? cc ezyang gchanan zou3519 bdhirsh jbschlosser aocsa nikitaved pearu mruberry vincentqb"
pytorch,28549,"feature currently, easy way visualizing graph generated autodiff jit. want something could additional context autograd, jit graph defined class python correspondence, therefore python views general . need create python class could access additional information. please confirm feature sounds useful yes, assign issue work it. cc suo ezyang ssnl alband zou3519 gqchen",0,expose differentiablegraphbackward python,"expose differentiablegraphbackward python feature currently, easy way visualizing graph generated autodiff jit. want something could additional context autograd, jit graph defined class python correspondence, therefore python views general . need create python class could access additional information. please confirm feature sounds useful yes, assign issue work it. cc suo ezyang ssnl alband zou3519 gqchen"
pytorch,6171,"code sparse matrix dense matrix multiplication https github.com pytorch pytorch blob master aten src ths generic thstensormath.c l305 https github.com pytorch pytorch blob master aten src thcs generic thcstensormath.cu l58 , first computes csr representation, perform sparse matrix dense matrix multiplication based representation, destroy csr representation. repeatedly perform matrix multiplication sparse matrix, computing csr representation redundant. could resolved computing csr representation coalescing save member https github.com pytorch pytorch blob master aten src ths generic thstensor.cpp l428 https github.com pytorch pytorch blob master aten src thcs generic thcstensor.cu l38 . would enhance speed sparse matrix dense matrix multiplication. good modification? shall try make pull request one? cc vincentqb aocsa nikitaved pearu mruberry",0,compute csr representation torch.sparse.tensor.coalesce faster sparse matrix multiplication,"compute csr representation torch.sparse.tensor.coalesce faster sparse matrix multiplication code sparse matrix dense matrix multiplication https github.com pytorch pytorch blob master aten src ths generic thstensormath.c l305 https github.com pytorch pytorch blob master aten src thcs generic thcstensormath.cu l58 , first computes csr representation, perform sparse matrix dense matrix multiplication based representation, destroy csr representation. repeatedly perform matrix multiplication sparse matrix, computing csr representation redundant. could resolved computing csr representation coalescing save member https github.com pytorch pytorch blob master aten src ths generic thstensor.cpp l428 https github.com pytorch pytorch blob master aten src thcs generic thcstensor.cu l38 . would enhance speed sparse matrix dense matrix multiplication. good modification? shall try make pull request one? cc vincentqb aocsa nikitaved pearu mruberry"
pytorch,20323,would consistent numpy. cc mruberry rgommers heitorschueroff,0,support size torch.normal,support size torch.normal would consistent numpy. cc mruberry rgommers heitorschueroff
pytorch,24145,"many occurences build error azure pipelines. https dev.azure.com pytorch pytorch build results?buildid 3891 https dev.azure.com pytorch pytorch build results?buildid 3901 https dev.azure.com pytorch pytorch build results?buildid 3695 ideas, yf225?",0,sccache crashes building distribution.cu windows,"sccache crashes building distribution.cu windows many occurences build error azure pipelines. https dev.azure.com pytorch pytorch build results?buildid 3891 https dev.azure.com pytorch pytorch build results?buildid 3901 https dev.azure.com pytorch pytorch build results?buildid 3695 ideas, yf225?"
pytorch,15298,"bug batchnorm2d' doc, explanation momentum newly computed batch statistics, estimated statistics history. experiments, momentum set 0, batchnorm operates use newly computed statistics. guess maybe following explanation correct reproduce torch.tensor shape 8, 3, 299, 299 , read image model load set momentum 0 inception v3 model struct https github.com pytorch vision blob master torchvision models inception.py calculate mean var result expected behavior expected environment pytorch version 0.4.1 windows pip python version 3.6.5 cc brianjo mruberry alband jbschlosser",0,momentum problem 1 momentum correct? batchnorm2d,"momentum problem 1 momentum correct? batchnorm2d bug batchnorm2d' doc, explanation momentum newly computed batch statistics, estimated statistics history. experiments, momentum set 0, batchnorm operates use newly computed statistics. guess maybe following explanation correct reproduce torch.tensor shape 8, 3, 299, 299 , read image model load set momentum 0 inception v3 model struct https github.com pytorch vision blob master torchvision models inception.py calculate mean var result expected behavior expected environment pytorch version 0.4.1 windows pip python version 3.6.5 cc brianjo mruberry alband jbschlosser"
pytorch,25034,"feature motivation api consistent way architectures made. actions using set methods, actions using slightly different version methods. clear example, currently want alter use part existing architecture may use like 1. method 1 2. slightly different version however, time try use named version method i.e , longer thing face error process using method situations, lose information originally placed architecture, meaning longer access modules using names. source issue lies method interestingly accepts name module default. pitch allow , alikes accept modules, also accept named modules. meaning two snippets interchangeable using modules using tuple cc ssnl",0,"make add module accept tuples well change containers modulelist, sequential, etc allow","make add module accept tuples well change containers modulelist, sequential, etc allow feature motivation api consistent way architectures made. actions using set methods, actions using slightly different version methods. clear example, currently want alter use part existing architecture may use like 1. method 1 2. slightly different version however, time try use named version method i.e , longer thing face error process using method situations, lose information originally placed architecture, meaning longer access modules using names. source issue lies method interestingly accepts name module default. pitch allow , alikes accept modules, also accept named modules. meaning two snippets interchangeable using modules using tuple cc ssnl"
pytorch,14996,"bug according doc https pytorch.org docs stable torch.html?highlight torch 20as tensor torch.as tensor , input copied cuda device default tensor type cuda tensor. default none, uses current device default tensor type see torch.set default tensor type . device cpu cpu tensor types current cuda device cuda tensor types. copy cuda device input numpy array, input cpu tensor. reproduce steps reproduce behavior expected behavior device 'cuda' device cases. environment pytorch version 1.0.0 debug build cuda used build pytorch 9.0.176 os ubuntu 18.04.1 lts gcc version ubuntu 7.3.0 27ubuntu1 18.04 7.3.0 cmake version version 3.10.2 python version 3.7 cuda available yes cuda runtime version could collect gpu models configuration gpu 0 geforce gtx 1070 gpu 1 geforce gtx 1080 nvidia driver version 390.77 cudnn version could collect versions relevant libraries pip could collect conda blas 1.0 mkl conda mkl 2018.0.3 1 conda mkl fft 1.0.6 py37h7dd41cf 0 conda mkl random 1.0.1 py37h4414c95 1 conda pytorch 1.0.0 py3.7 cuda9.0.176 cudnn7.4.1 1 pytorch conda torchvision 0.2.1 py 2 pytorch cc ngimel jlin27 mruberry",0,tensor use device default tensor type,"tensor use device default tensor type bug according doc https pytorch.org docs stable torch.html?highlight torch 20as tensor torch.as tensor , input copied cuda device default tensor type cuda tensor. default none, uses current device default tensor type see torch.set default tensor type . device cpu cpu tensor types current cuda device cuda tensor types. copy cuda device input numpy array, input cpu tensor. reproduce steps reproduce behavior expected behavior device 'cuda' device cases. environment pytorch version 1.0.0 debug build cuda used build pytorch 9.0.176 os ubuntu 18.04.1 lts gcc version ubuntu 7.3.0 27ubuntu1 18.04 7.3.0 cmake version version 3.10.2 python version 3.7 cuda available yes cuda runtime version could collect gpu models configuration gpu 0 geforce gtx 1070 gpu 1 geforce gtx 1080 nvidia driver version 390.77 cudnn version could collect versions relevant libraries pip could collect conda blas 1.0 mkl conda mkl 2018.0.3 1 conda mkl fft 1.0.6 py37h7dd41cf 0 conda mkl random 1.0.1 py37h4414c95 1 conda pytorch 1.0.0 py3.7 cuda9.0.176 cudnn7.4.1 1 pytorch conda torchvision 0.2.1 py 2 pytorch cc ngimel jlin27 mruberry"
pytorch,24645,porting th operators essential code simplicity performance reasons. porting guides q available umbrella issue 24507 feel free add vitalyfedyunin reviewer get prioritized review.,0,migrate thnn conv2d forward th aten cuda,migrate thnn conv2d forward th aten cuda porting th operators essential code simplicity performance reasons. porting guides q available umbrella issue 24507 feel free add vitalyfedyunin reviewer get prioritized review.
pytorch,3542,"error message reason pytorch cloned recursive recommended submodule stores file absolute pathname run copy everything absolute pathname match hence error. apparently know problem git, see https lwn.net articles 691223 fixed v 2.9 may put note page telling people save time.",0,"minor docker build fails submodule gitdir absolute path name problem git v 2.7 2.8, fixed git v 2.9","minor docker build fails submodule gitdir absolute path name problem git v 2.7 2.8, fixed git v 2.9 error message reason pytorch cloned recursive recommended submodule stores file absolute pathname run copy everything absolute pathname match hence error. apparently know problem git, see https lwn.net articles 691223 fixed v 2.9 may put note page telling people save time."
pytorch,26551,"bug hi, torch.nn.functional.mse loss always throws warning using two tensors different shapes. legitimate reasons wanting use differently shaped tensor taking advantage standard broadcasting behaviour pytorch, needs way disable warning. reproduce expected behavior optional parameter disable warning, like , . discussed https github.com pytorch pytorch issues 16045 issuecomment 476266780 additional context legitimate cases wanting calculate mse differently shaped tensors. example need calculate difference sample subset samples. need calculate mse tensors shaped like number samples, dimension sample, 1 number samples, dimension sample, number samples subset . understand safeguards need put place avoid misleading people, discussed original issue https github.com pytorch pytorch issues 16045 broadcasting pytorch staple users implement basic functions like mse make use full power. cheers",0,way disable mse loss broadcasting warning,"way disable mse loss broadcasting warning bug hi, torch.nn.functional.mse loss always throws warning using two tensors different shapes. legitimate reasons wanting use differently shaped tensor taking advantage standard broadcasting behaviour pytorch, needs way disable warning. reproduce expected behavior optional parameter disable warning, like , . discussed https github.com pytorch pytorch issues 16045 issuecomment 476266780 additional context legitimate cases wanting calculate mse differently shaped tensors. example need calculate difference sample subset samples. need calculate mse tensors shaped like number samples, dimension sample, 1 number samples, dimension sample, number samples subset . understand safeguards need put place avoid misleading people, discussed original issue https github.com pytorch pytorch issues 16045 broadcasting pytorch staple users implement basic functions like mse make use full power. cheers"
pytorch,28329,"feature way perform functionality place. feature would either add method parameter existing , both. motivation usual reasons operations place. pitch stated above, add method parameter existing . alternatives one could use place less efficient alternative. note method functionality place .",0,place version,"place version feature way perform functionality place. feature would either add method parameter existing , both. motivation usual reasons operations place. pitch stated above, add method parameter existing . alternatives one could use place less efficient alternative. note method functionality place ."
pytorch,23301,"following configuration .pro file opencv works absolutely fine without qmake cxxflags glibcxx use cxx11 abi 0 . this, however, get following errors ! screenshot 2019 07 24 14 35 50 https user images.githubusercontent.com 17313248 61799072 b6307800 ae2a 11e9 9bb3 e8b8068c8a87.png opencv works fine qmake cxxflags glibcxx use cxx11 abi 1 well. throws different set errors ! screenshot 2019 07 24 14 38 10 https user images.githubusercontent.com 17313248 61799101 c183a380 ae2a 11e9 9da8 ff815f721dde.png setting qmake cxxflags glibcxx use cxx11 abi 0 recommended libtorch forums avoid errors above. could solution solutions work around this? newbie libtorch qt creator.",0,error using libtorch opencv qt creator,"error using libtorch opencv qt creator following configuration .pro file opencv works absolutely fine without qmake cxxflags glibcxx use cxx11 abi 0 . this, however, get following errors ! screenshot 2019 07 24 14 35 50 https user images.githubusercontent.com 17313248 61799072 b6307800 ae2a 11e9 9bb3 e8b8068c8a87.png opencv works fine qmake cxxflags glibcxx use cxx11 abi 1 well. throws different set errors ! screenshot 2019 07 24 14 38 10 https user images.githubusercontent.com 17313248 61799101 c183a380 ae2a 11e9 9da8 ff815f721dde.png setting qmake cxxflags glibcxx use cxx11 abi 0 recommended libtorch forums avoid errors above. could solution solutions work around this? newbie libtorch qt creator."
pytorch,18776,"bug trace module load script module c via libtorch, resulting behavior c depends whether flag set. calls c api runtime appear effect. reproduce note able verify issue still exists latest nightly 20190402 appears latest nightly least windows cannot run jit traced models. even simplest model gives following error 1 run python script 2 compile run c code below. 3 observe average time per call. see 0.8ms python script either 0.8 120ms c depending flag used python. either case, c sets benchmarking on. gtx 1080 b kernel run cudnn. w either setting flag, python code runs . flag on, runs taking 120ms chooses faster . flag python, c also chooses flag python, always chooses regardless flag setting c . observed choice kernel using . python script c code expected behavior would expect either 1 c setting respected choosing correct algorithm 2 least print warning overridden value flag trace time. additional info printed jit graphs generated benchmarking got following flag change flag register 17 1 instead 0. suppose hardcoding flag might happening? environment python code run linux, c code run windows pytorch version e.g., 1.0 1.0.0.dev20190311 linux, 2336f0ba0 windows os e.g., linux fedora 29, windows 10 1809 installed pytorch , , source conda pytorch nightly python version 3.7 cuda cudnn version cuda 10, cudnn 7.4.2 gpu models configuration titan rtx linux , gtx 1080 windows cc suo",0,value torch.backends.cudnn.benchmark baked jit traced modules 150x slowdown convtranspose2d jit libtorch cudnn,"value torch.backends.cudnn.benchmark baked jit traced modules 150x slowdown convtranspose2d jit libtorch cudnn bug trace module load script module c via libtorch, resulting behavior c depends whether flag set. calls c api runtime appear effect. reproduce note able verify issue still exists latest nightly 20190402 appears latest nightly least windows cannot run jit traced models. even simplest model gives following error 1 run python script 2 compile run c code below. 3 observe average time per call. see 0.8ms python script either 0.8 120ms c depending flag used python. either case, c sets benchmarking on. gtx 1080 b kernel run cudnn. w either setting flag, python code runs . flag on, runs taking 120ms chooses faster . flag python, c also chooses flag python, always chooses regardless flag setting c . observed choice kernel using . python script c code expected behavior would expect either 1 c setting respected choosing correct algorithm 2 least print warning overridden value flag trace time. additional info printed jit graphs generated benchmarking got following flag change flag register 17 1 instead 0. suppose hardcoding flag might happening? environment python code run linux, c code run windows pytorch version e.g., 1.0 1.0.0.dev20190311 linux, 2336f0ba0 windows os e.g., linux fedora 29, windows 10 1809 installed pytorch , , source conda pytorch nightly python version 3.7 cuda cudnn version cuda 10, cudnn 7.4.2 gpu models configuration titan rtx linux , gtx 1080 windows cc suo"
pytorch,20056,"bug trying create tensor many dimensions simply returns empty tensor shape dimensions passed even though contained value . reproduce steps reproduce behavior number 2's following example . output another related issue 2's. output 2's. output changes expected behavior expected see aforementioned dimensions filled possible, exception says value error. environment please copy paste output environment collection script https raw.githubusercontent.com pytorch pytorch master torch utils collect env.py fill checklist manually . get script run additional context suspect desired behavior happening tensors seem basing numpy accepts dims. also, found line https github.com pytorch pytorch blob 7ddd5d06ed07a50b94aa6b2fdffa2f667d677c4b aten src th thgeneral.cpp l184 . think integer division two elements results error.",0,creation big multidimensional array returns empty tensor.,"creation big multidimensional array returns empty tensor. bug trying create tensor many dimensions simply returns empty tensor shape dimensions passed even though contained value . reproduce steps reproduce behavior number 2's following example . output another related issue 2's. output 2's. output changes expected behavior expected see aforementioned dimensions filled possible, exception says value error. environment please copy paste output environment collection script https raw.githubusercontent.com pytorch pytorch master torch utils collect env.py fill checklist manually . get script run additional context suspect desired behavior happening tensors seem basing numpy accepts dims. also, found line https github.com pytorch pytorch blob 7ddd5d06ed07a50b94aa6b2fdffa2f667d677c4b aten src th thgeneral.cpp l184 . think integer division two elements results error."
pytorch,10746,"issue description getting lnk2001 windows . system info pytorch caffe2 c2 installed pytorch conda, pip, source src build command used compiling source cmake os win10 pytorch version master vs version compiling source 2017 cmake version 3.12",0,caffe2 failed build dispatch test. error lnk2001 unresolved external symbol,"caffe2 failed build dispatch test. error lnk2001 unresolved external symbol issue description getting lnk2001 windows . system info pytorch caffe2 c2 installed pytorch conda, pip, source src build command used compiling source cmake os win10 pytorch version master vs version compiling source 2017 cmake version 3.12"
pytorch,31657,"feature using sequence similar python abstract class class order tackle note lines 30 32 def len self len using abstract classes, see official python documentation exact abstract methods class expects, i.e. . motivation motivation three fold 1 tackling note mentioned left pytorch contributors, 2 let code auto documented since accessing code states explicitly need define two methods and, 3 using python abstract classes instead inheritting object rather deprecated practice python abstract classes capabilities . pitch implement dataset class using new python abstract classes capabilities, least discuss it. alternatives currently alternative proposals. additional context . cc ssnl",0,use sequence collections abstract classes dataset,"use sequence collections abstract classes dataset feature using sequence similar python abstract class class order tackle note lines 30 32 def len self len using abstract classes, see official python documentation exact abstract methods class expects, i.e. . motivation motivation three fold 1 tackling note mentioned left pytorch contributors, 2 let code auto documented since accessing code states explicitly need define two methods and, 3 using python abstract classes instead inheritting object rather deprecated practice python abstract classes capabilities . pitch implement dataset class using new python abstract classes capabilities, least discuss it. alternatives currently alternative proposals. additional context . cc ssnl"
pytorch,24675,porting th operators essential code simplicity performance reasons. porting guides q available umbrella issue 24507 feel free add vitalyfedyunin reviewer get prioritized review.,0,migrate multinomial alias setup th aten cpu,migrate multinomial alias setup th aten cpu porting th operators essential code simplicity performance reasons. porting guides q available umbrella issue 24507 feel free add vitalyfedyunin reviewer get prioritized review.
pytorch,215,"missing writing docs size, stride storage, storageoffset, size, stride",0,missing tensor constructors,"missing tensor constructors missing writing docs size, stride storage, storageoffset, size, stride"
pytorch,12555,https raw.githubusercontent.com pytorch pytorch master torch utils collect env.py currently misses collecting nvidia driver version. would useful that.,0,add nvidia driver version environment collection script,add nvidia driver version environment collection script https raw.githubusercontent.com pytorch pytorch master torch utils collect env.py currently misses collecting nvidia driver version. would useful that.
pytorch,10751,"primary fft equation documentation https github.com pytorch pytorch blob 8013dac43d2acb592cab75317f17d4f9c5b9eb6a torch torch docs.py l5303 however, experimentation seems 1 n factor forward transform. relevant line https github.com pytorch pytorch blob 8013dac43d2acb592cab75317f17d4f9c5b9eb6a torch torch docs.py l5303 inverse transform equation 1 n consistent behavior implementation. note, documentation accurate , , makes sense 1 n factor main equation above. note, documentation also accurate , makes sense 1 n factor main equation above. lastly, documentation mix two problems. line https github.com pytorch pytorch blob 8013dac43d2acb592cab75317f17d4f9c5b9eb6a torch torch docs.py l5493 says says multiply instead divide fft . consistent primary equation, consistent implementation.",0,docs error documentation fft normalization,"docs error documentation fft normalization primary fft equation documentation https github.com pytorch pytorch blob 8013dac43d2acb592cab75317f17d4f9c5b9eb6a torch torch docs.py l5303 however, experimentation seems 1 n factor forward transform. relevant line https github.com pytorch pytorch blob 8013dac43d2acb592cab75317f17d4f9c5b9eb6a torch torch docs.py l5303 inverse transform equation 1 n consistent behavior implementation. note, documentation accurate , , makes sense 1 n factor main equation above. note, documentation also accurate , makes sense 1 n factor main equation above. lastly, documentation mix two problems. line https github.com pytorch pytorch blob 8013dac43d2acb592cab75317f17d4f9c5b9eb6a torch torch docs.py l5493 says says multiply instead divide fft . consistent primary equation, consistent implementation."
pytorch,17397,"questions help pytorch batch normalization sgd, batch size use? per gpu batch size, total batch size gpus? example, suppose set per gpu batch size 32, use 8 gpus. pytorch batch normalization, batch size use, 32 32 x 8? concern comes facebook's paper accurate, large minibatch sgd training imagenet 1 hour . pytorch's implementation paper describes, modify parameters model accordingly not, need know. thanks!",0,pytorch count batch size sgd multigpu batch norm?,"pytorch count batch size sgd multigpu batch norm? questions help pytorch batch normalization sgd, batch size use? per gpu batch size, total batch size gpus? example, suppose set per gpu batch size 32, use 8 gpus. pytorch batch normalization, batch size use, 32 32 x 8? concern comes facebook's paper accurate, large minibatch sgd training imagenet 1 hour . pytorch's implementation paper describes, modify parameters model accordingly not, need know. thanks!"
pytorch,12608,"hi, would like get general ideas exporting onnx model model accepts input sequence output labels arguments. would set variables traverse model export onnx? thanks.",0,onnx model export issue,"onnx model export issue hi, would like get general ideas exporting onnx model model accepts input sequence output labels arguments. would set variables traverse model export onnx? thanks."
pytorch,347,added bindings 2d modules. finish up. assigning sam.,0,cudnn 3d 1d modules,cudnn 3d 1d modules added bindings 2d modules. finish up. assigning sam.
pytorch,8981,"failing diff usually looks something like thought weird buffering problem, looked carefully test, one regexes looks suspicious size short version hash varies, regex gobble less. really hardcoded wildcard periods. cc zou3519",0,intermittent failure testcollectenv.test expect,"intermittent failure testcollectenv.test expect failing diff usually looks something like thought weird buffering problem, looked carefully test, one regexes looks suspicious size short version hash varies, regex gobble less. really hardcoded wildcard periods. cc zou3519"
pytorch,6518,expect least see print default. cc gchanan,0,pytorch torch.tensor torch.tensor.dtype confusing repr,pytorch torch.tensor torch.tensor.dtype confusing repr expect least see print default. cc gchanan
pytorch,21119,"bug libtorch excellent provide cmake multiple configs windows. download release version https download.pytorch.org libtorch cu90 libtorch win shared deps latest.zip download debug version https download.pytorch.org libtorch cu90 libtorch win shared deps debug latest.zip however, quite inconvenient. usually, need copy different versions switch different configurations, annoying development. support configs inside one findtorch.cmake ?",0,one libtorch findtorch.cmake module support multi configs debug release,"one libtorch findtorch.cmake module support multi configs debug release bug libtorch excellent provide cmake multiple configs windows. download release version https download.pytorch.org libtorch cu90 libtorch win shared deps latest.zip download debug version https download.pytorch.org libtorch cu90 libtorch win shared deps debug latest.zip however, quite inconvenient. usually, need copy different versions switch different configurations, annoying development. support configs inside one findtorch.cmake ?"
pytorch,3231,"converting ndarray buffer back works expected, ndarray given works",0,.numpy conversion problem,".numpy conversion problem converting ndarray buffer back works expected, ndarray given works"
pytorch,28775,"feature pytorch compiled arch successfully, utilize related api develop 1 motivation working compiling pytorch mips64 arch, work tried add , error happened. pitch pytorch compiled arch alternatives 1 see warning info , plan fake match rule , example ultimate result also failed since . so, whether support arch, too? thanks!",0,feature request whether planning support mips64 arch environments,"feature request whether planning support mips64 arch environments feature pytorch compiled arch successfully, utilize related api develop 1 motivation working compiling pytorch mips64 arch, work tried add , error happened. pitch pytorch compiled arch alternatives 1 see warning info , plan fake match rule , example ultimate result also failed since . so, whether support arch, too? thanks!"
pytorch,1981,"commit history pyyaml, far tell, http pyyaml.org log commit history , https bitbucket.org ruamel yaml commits ruamel.yaml supports yaml 1.2, handles round trip, including comments.",0,"controversial pyyaml 6 years old, might best switch ruamel.yaml ?","controversial pyyaml 6 years old, might best switch ruamel.yaml ? commit history pyyaml, far tell, http pyyaml.org log commit history , https bitbucket.org ruamel yaml commits ruamel.yaml supports yaml 1.2, handles round trip, including comments."
pytorch,5949,allowed? considering fact changes entire layer gives dimension mismatch error,0,bug report initializing weights layer initialization,bug report initializing weights layer initialization allowed? considering fact changes entire layer gives dimension mismatch error
pytorch,12365,"current, following code gives error error message",0,"jit mytuple 0 , mytuple work jit","jit mytuple 0 , mytuple work jit current, following code gives error error message"
pytorch,15764,"bug given large amount numbers say, list containing 32768 numbers , found argsort behaves wrongly setting descend false. reproduce want calculate similarity word embeddings, managed achieve batched similarity calculation. expect similarity embedding itself, thus highest rank 1,2... accordingly, vice versa. say torch.tensor object shape batch size 32768 , want extract top similarities along last dimension. code gives result get top torch.argsort similarity, dim 1 20485, 30807, 27706, ..., 117, 6, 1 , 29628, 16835, 23989, ..., 6773, 10377, 2 , ..., 30120, 23853, 24914, ..., 118, 50, 47 , 23528, 23663, 25283, ..., 78, 77, 48 , 31738, 17062, 29731, ..., 1240, 79, 49 top8 torch.argsort similarity, dim 1, descending true tensor 21820 , 21822, 21816, ..., 10909, 10910, 32725 , 32497 , 28158, 20713, ..., 14889, 12632, 16177 , 17110 , 17460, 18861, ..., 14628, 12113, 18 , ..., 22335 , 21440, 26360, ..., 14870, 8292, 13842 , 20379 , 16479, 28973, ..., 11770, 11327, 13454 , 23388 , 19492, 16577, ..., 7874, 5219, 13230 bold part implys main problems. assume simply accuracy float numebrs. expected behavior top8 torch.argsort similarity, dim 1, descending true tensor 1 , blablabla , 2 , blablabla , ..., 47 , blablabla , 48 , blablabla , 49 , blablabla environment pytorch version 1.0 stable os e.g., linux linux installed pytorch , , source pip install python version 3.6.3",0,torch.argsort descends wrongly,"torch.argsort descends wrongly bug given large amount numbers say, list containing 32768 numbers , found argsort behaves wrongly setting descend false. reproduce want calculate similarity word embeddings, managed achieve batched similarity calculation. expect similarity embedding itself, thus highest rank 1,2... accordingly, vice versa. say torch.tensor object shape batch size 32768 , want extract top similarities along last dimension. code gives result get top torch.argsort similarity, dim 1 20485, 30807, 27706, ..., 117, 6, 1 , 29628, 16835, 23989, ..., 6773, 10377, 2 , ..., 30120, 23853, 24914, ..., 118, 50, 47 , 23528, 23663, 25283, ..., 78, 77, 48 , 31738, 17062, 29731, ..., 1240, 79, 49 top8 torch.argsort similarity, dim 1, descending true tensor 21820 , 21822, 21816, ..., 10909, 10910, 32725 , 32497 , 28158, 20713, ..., 14889, 12632, 16177 , 17110 , 17460, 18861, ..., 14628, 12113, 18 , ..., 22335 , 21440, 26360, ..., 14870, 8292, 13842 , 20379 , 16479, 28973, ..., 11770, 11327, 13454 , 23388 , 19492, 16577, ..., 7874, 5219, 13230 bold part implys main problems. assume simply accuracy float numebrs. expected behavior top8 torch.argsort similarity, dim 1, descending true tensor 1 , blablabla , 2 , blablabla , ..., 47 , blablabla , 48 , blablabla , 49 , blablabla environment pytorch version 1.0 stable os e.g., linux linux installed pytorch , , source pip install python version 3.6.3"
pytorch,3573,"one representative error cause problem aa911939a328eff55c9b28b39ed3c43507ba8a2a seems clang, changing type parameter sufficient cause template instantiation fail. maybe easiest way fix write portable version pyint fromlong friends always returns .",0,mac os x build python 3.6 fails,"mac os x build python 3.6 fails one representative error cause problem aa911939a328eff55c9b28b39ed3c43507ba8a2a seems clang, changing type parameter sufficient cause template instantiation fail. maybe easiest way fix write portable version pyint fromlong friends always returns ."
pytorch,27769,docs link cuda documentation render incorrectly ! image https user images.githubusercontent.com 5652049 66680833 414d2d00 ec26 11e9 8347 5da72167ccb5.png cc ezyang gchanan zou3519,0,nvidia cuda documentation link renders wrong,nvidia cuda documentation link renders wrong docs link cuda documentation render incorrectly ! image https user images.githubusercontent.com 5652049 66680833 414d2d00 ec26 11e9 8347 5da72167ccb5.png cc ezyang gchanan zou3519
pytorch,20675,"questions help please note issue tracker help form issue closed. set listed resources available website https pytorch.org resources . primary means support discussion forum discussion forum https discuss.pytorch.org firstly tested tensorflow gpu 1.12, works cuda gpu well. followed pytorch tutorial setup simple cifar10 net, works well cpu. tested cuda cudnn available, return true, however gpu enabled cifar10 net failed gpu exception cudnn status execution failed. inputs cudnn acceptable true traceback recent call last file python pytorch tutorial tut 4 classifier gpu.py , line 97, outputs net inputs file python python36 lib site packages torch nn modules module.py , line 489, call result self.forward input, kwargs file python pytorch tutorial tut 4 classifier gpu.py , line 62, forward x self.pool f.relu self.conv1 x file python python36 lib site packages torch nn modules module.py , line 489, call result self.forward input, kwargs file python python36 lib site packages torch nn modules conv.py , line 320, forward self.padding, self.dilation, self.groups runtimeerror cudnn error cudnn status execution failed",0,windows 10 cuda 9 cudnn 7.5 pytorch 1.1 cudnn status execution failed,"windows 10 cuda 9 cudnn 7.5 pytorch 1.1 cudnn status execution failed questions help please note issue tracker help form issue closed. set listed resources available website https pytorch.org resources . primary means support discussion forum discussion forum https discuss.pytorch.org firstly tested tensorflow gpu 1.12, works cuda gpu well. followed pytorch tutorial setup simple cifar10 net, works well cpu. tested cuda cudnn available, return true, however gpu enabled cifar10 net failed gpu exception cudnn status execution failed. inputs cudnn acceptable true traceback recent call last file python pytorch tutorial tut 4 classifier gpu.py , line 97, outputs net inputs file python python36 lib site packages torch nn modules module.py , line 489, call result self.forward input, kwargs file python pytorch tutorial tut 4 classifier gpu.py , line 62, forward x self.pool f.relu self.conv1 x file python python36 lib site packages torch nn modules module.py , line 489, call result self.forward input, kwargs file python python36 lib site packages torch nn modules conv.py , line 320, forward self.padding, self.dilation, self.groups runtimeerror cudnn error cudnn status execution failed"
pytorch,4896,values somehow zero none neural network case values normal fix this?,0,second order derivative neural network w.r.t. input encounter zero,second order derivative neural network w.r.t. input encounter zero values somehow zero none neural network case values normal fix this?
pytorch,26081,"bug compiling libtorch without fbgemm causes compile error rnn. dev pytorch aten src aten native rnn.cpp 286 57 error packedlinearweight declared scope struct exists aten src aten native quantized cpu fbgemm utils.h workaround ifdef certain sections code. see diff reproduce steps reproduce behavior 1. cmake duse fbgemm .. 2. compile error expected behavior compiles environment pytorch version e.g., 1.0 master os e.g., linux linux x64 installed pytorch , , source source build command used compiling source cmake duse fbgemm .. python version 3.6 cuda cudnn version n gpu models configuration n relevant information n cc jerryzh168 jianyuh dzhulgakov zou3519",0,rnn fails compile due use fbgemm,"rnn fails compile due use fbgemm bug compiling libtorch without fbgemm causes compile error rnn. dev pytorch aten src aten native rnn.cpp 286 57 error packedlinearweight declared scope struct exists aten src aten native quantized cpu fbgemm utils.h workaround ifdef certain sections code. see diff reproduce steps reproduce behavior 1. cmake duse fbgemm .. 2. compile error expected behavior compiles environment pytorch version e.g., 1.0 master os e.g., linux linux x64 installed pytorch , , source source build command used compiling source cmake duse fbgemm .. python version 3.6 cuda cudnn version n gpu models configuration n relevant information n cc jerryzh168 jianyuh dzhulgakov zou3519"
pytorch,17163,"might nontrivial zeros implemented terms vectorized fill cpu, work make code work half.",0,support torch.zeros halftensor half,"support torch.zeros halftensor half might nontrivial zeros implemented terms vectorized fill cpu, work make code work half."
pytorch,11989,"issue description upgrade cuda 8.0 9.0, reinstall cudnn pytorch. following error happen runtimeerror cuda error unknown error code example traceback recent call last file example.py , line 3, x torch.tensor 1., 2. , device device runtimeerror cuda error unknown error system info collecting environment information... pytorch version 0.4.1 debug build cuda used build pytorch 9.0.176 os ubuntu 16.04.5 lts gcc version ubuntu 5.4.0 6ubuntu1 16.04.10 5.4.0 20160609 cmake version version 3.5.1 python version 3.5 cuda available yes cuda runtime version 9.0.176 gpu models configuration gpu 0 titan xp nvidia driver version 384.130 cudnn version probably one following usr lib x86 64 linux gnu libcudnn.so.7.3.0 usr lib x86 64 linux gnu libcudnn static v7.a versions relevant libraries pip could collect conda could collect",0,runtimeerror cuda error unknown error,"runtimeerror cuda error unknown error issue description upgrade cuda 8.0 9.0, reinstall cudnn pytorch. following error happen runtimeerror cuda error unknown error code example traceback recent call last file example.py , line 3, x torch.tensor 1., 2. , device device runtimeerror cuda error unknown error system info collecting environment information... pytorch version 0.4.1 debug build cuda used build pytorch 9.0.176 os ubuntu 16.04.5 lts gcc version ubuntu 5.4.0 6ubuntu1 16.04.10 5.4.0 20160609 cmake version version 3.5.1 python version 3.5 cuda available yes cuda runtime version 9.0.176 gpu models configuration gpu 0 titan xp nvidia driver version 384.130 cudnn version probably one following usr lib x86 64 linux gnu libcudnn.so.7.3.0 usr lib x86 64 linux gnu libcudnn static v7.a versions relevant libraries pip could collect conda could collect"
pytorch,24718,porting th operators essential code simplicity performance reasons. porting guides q available umbrella issue 24507 feel free add vitalyfedyunin reviewer get prioritized review.,0,migrate l1 loss backward th aten cpu,migrate l1 loss backward th aten cpu porting th operators essential code simplicity performance reasons. porting guides q available umbrella issue 24507 feel free add vitalyfedyunin reviewer get prioritized review.
pytorch,3001,"minimal test case import torch x torch.autograd.variable torch.floattensor 0.1, 0.2 , 0.3, 0.4 idx torch.autograd.variable torch.longtensor 1, 0 print x idx, expected output either variable containing 0.3000 0.4000 0.1000 0.2000 torch.floattensor size 2x2 printed replace last line ...or immediate exception traceback informing advanced indices must . actual output program initially hangs. interrupt program c, following exception displayed traceback recent call last file loop.py , line 5, print x idx, file home lib python3.5 site packages torch autograd variable.py , line 76, getitem return index.apply self, key file home lib python3.5 site packages torch autograd functions tensor.py , line 16, forward result i.index ctx.index indexerror performing advanced indexing indexing objects must longtensors convertible longtensors details digging around pdb, looked like https github.com pytorch pytorch blob master torch autograd variable.py l78 tail calling via c code. identify full stack, looks like eventually made way attempt convert index https github.com pytorch pytorch blob master torch csrc generic tensor.cpp l785 . sure enough, also hangs torch.longtensor idx perhaps constructor trying treat iterable, getting stuck iterating returns sequence ? happy dig further, someone point logic initializing . python 3.5.2, gcc 5.4.0 linux, '0.2.0 3', cuda",0,trying advanced indexing using variable causes hang,"trying advanced indexing using variable causes hang minimal test case import torch x torch.autograd.variable torch.floattensor 0.1, 0.2 , 0.3, 0.4 idx torch.autograd.variable torch.longtensor 1, 0 print x idx, expected output either variable containing 0.3000 0.4000 0.1000 0.2000 torch.floattensor size 2x2 printed replace last line ...or immediate exception traceback informing advanced indices must . actual output program initially hangs. interrupt program c, following exception displayed traceback recent call last file loop.py , line 5, print x idx, file home lib python3.5 site packages torch autograd variable.py , line 76, getitem return index.apply self, key file home lib python3.5 site packages torch autograd functions tensor.py , line 16, forward result i.index ctx.index indexerror performing advanced indexing indexing objects must longtensors convertible longtensors details digging around pdb, looked like https github.com pytorch pytorch blob master torch autograd variable.py l78 tail calling via c code. identify full stack, looks like eventually made way attempt convert index https github.com pytorch pytorch blob master torch csrc generic tensor.cpp l785 . sure enough, also hangs torch.longtensor idx perhaps constructor trying treat iterable, getting stuck iterating returns sequence ? happy dig further, someone point logic initializing . python 3.5.2, gcc 5.4.0 linux, '0.2.0 3', cuda"
pytorch,15544,"questions help please note issue tracker help form issue closed. set listed resources available website https pytorch.org resources . primary means support discussion forum discussion forum https discuss.pytorch.org build pytorch v1 source code docker container without gpu, cuda. commit save docker image. run docker image another machine gpu, occur pytorch python api ok ! image https user images.githubusercontent.com 17922949 50450451 20526e80 0969 11e9 8158 029ae2700ff5.png pytorch c api bad ! image https user images.githubusercontent.com 17922949 50450462 32cca800 0969 11e9 9701 fea278df8f1f.png gcc version 4.8.4 cuda 9.0.176 nvidia driver 384.81 cudnn 7.0.3 ? hope get help!",0,python api ok c api bad,"python api ok c api bad questions help please note issue tracker help form issue closed. set listed resources available website https pytorch.org resources . primary means support discussion forum discussion forum https discuss.pytorch.org build pytorch v1 source code docker container without gpu, cuda. commit save docker image. run docker image another machine gpu, occur pytorch python api ok ! image https user images.githubusercontent.com 17922949 50450451 20526e80 0969 11e9 8158 029ae2700ff5.png pytorch c api bad ! image https user images.githubusercontent.com 17922949 50450462 32cca800 0969 11e9 9701 fea278df8f1f.png gcc version 4.8.4 cuda 9.0.176 nvidia driver 384.81 cudnn 7.0.3 ? hope get help!"
pytorch,8282,"verified discuss.pytorch https discuss.pytorch.org loaded network different shape works anyway 19398 indeed unwanted behaviour, forwarding issue description trained model among others following layer saved file state dict torch.save. then, wanted load model using load state dict, accident layer setup follows nevertheless, model loaded without error. seemed weights duplicated 32 times, verified. question consistent api documentation. found statement says load state dict would somehow fix shape inconsistencies automatically. seems documentation vs reality mismatch here. need decide one fix code example provided discuss .pytorch user ptrblck system info pytorch 0.4 release",0,loaded network load state dict different shape works anyway,"loaded network load state dict different shape works anyway verified discuss.pytorch https discuss.pytorch.org loaded network different shape works anyway 19398 indeed unwanted behaviour, forwarding issue description trained model among others following layer saved file state dict torch.save. then, wanted load model using load state dict, accident layer setup follows nevertheless, model loaded without error. seemed weights duplicated 32 times, verified. question consistent api documentation. found statement says load state dict would somehow fix shape inconsistencies automatically. seems documentation vs reality mismatch here. need decide one fix code example provided discuss .pytorch user ptrblck system info pytorch 0.4 release"
pytorch,20296,"large capacity memory needed compiling pytorch source ? got 5gb available compiling ubuntu 18.04. time compile python setup.py install , compiling program occupy capacity memory. screen stuck. while, screen becomes dark laptop reboots.... way setup parameters use less memory compiling ? many thanks.",0,capacity memory compiling pytorch source,"capacity memory compiling pytorch source large capacity memory needed compiling pytorch source ? got 5gb available compiling ubuntu 18.04. time compile python setup.py install , compiling program occupy capacity memory. screen stuck. while, screen becomes dark laptop reboots.... way setup parameters use less memory compiling ? many thanks."
pytorch,3882,"met strange memory leak tried implement improved training wasserstein gans . getting oom middle second epoch cpu gpu. memory usage seems increase batch, profiling cpu version points loop dataloader. kinda minimal example discussion https discuss.pytorch.org leaking dataloader 10418 found code runs fine torch 0.2.0 3, using https github.com pytorch pytorch tree 8ebf18b5b1d57ef16c24366649e720867f394a98 built source, cuda 8 cudnn7, supposed recently introduced bug",0,leaking dataloader,"leaking dataloader met strange memory leak tried implement improved training wasserstein gans . getting oom middle second epoch cpu gpu. memory usage seems increase batch, profiling cpu version points loop dataloader. kinda minimal example discussion https discuss.pytorch.org leaking dataloader 10418 found code runs fine torch 0.2.0 3, using https github.com pytorch pytorch tree 8ebf18b5b1d57ef16c24366649e720867f394a98 built source, cuda 8 cudnn7, supposed recently introduced bug"
pytorch,15328,"bug running issue accepts right hand side left hand side forward pass, backward pass fail. unsqueezing right hand side squeezing output gets size output , backward pass also works. reproduce expected behavior either torch.gesv require trailing singleton dimension forward pass e.g. b backward pass work singleton dimension. environment reproducible installed via conda.",0,"torch.gesv forward handles singleton dimension, backward","torch.gesv forward handles singleton dimension, backward bug running issue accepts right hand side left hand side forward pass, backward pass fail. unsqueezing right hand side squeezing output gets size output , backward pass also works. reproduce expected behavior either torch.gesv require trailing singleton dimension forward pass e.g. b backward pass work singleton dimension. environment reproducible installed via conda."
pytorch,5712,"pytorch github issues guidelines os mac pytorch version master, v0.3.1, v0.3.0, v0.2.0, v0.1.12 installed pytorch conda, pip, source python version n cuda cudnn version n gpu models configuration n gcc version compiling source n version 0.3.1 produces update goes v0.3.0 update running fails update running 0.1.12 gives this, similar latest versions different package update running gives",0,multiple versions dockerfiles fail build,"multiple versions dockerfiles fail build pytorch github issues guidelines os mac pytorch version master, v0.3.1, v0.3.0, v0.2.0, v0.1.12 installed pytorch conda, pip, source python version n cuda cudnn version n gpu models configuration n gcc version compiling source n version 0.3.1 produces update goes v0.3.0 update running fails update running 0.1.12 gives this, similar latest versions different package update running gives"
pytorch,18689,"bug trying use distributed.all gather gather gradients multi nodes. found gather function stuck error throw reproduce steps reproduce behavior 1. set env variable , save following code 2. run code btw, execute code manually , found gather quickly go through, stuck trying print tensor expected behavior process print list tensor environment please copy paste output environment collection script https raw.githubusercontent.com pytorch pytorch master torch utils collect env.py fill checklist manually . get script run pytorch version 1.0.1.post2 debug build cuda used build pytorch 9.0.176 os ubuntu 18.04.1 lts gcc version ubuntu 5.4.0 6ubuntu1 16.04.10 5.4.0 20160609 cmake version version 3.10.2 python version 3.5 cuda available yes cuda runtime version 9.0.176 gpu models configuration gpu 0 tesla k80 gpu 1 tesla k80 gpu 2 tesla k80 gpu 3 tesla k80 gpu 4 tesla k80 gpu 5 tesla k80 gpu 6 tesla k80 gpu 7 tesla k80 nvidia driver version 384.145 cudnn version usr lib x86 64 linux gnu libcudnn.so.7.3.1 versions relevant libraries pip3 numpy 1.15.3 pip3 torch 1.0.1.post2 pip3 torchvision 0.2.1 conda could collect additional context",0,distributed.all gather function stuck using nccl backend,"distributed.all gather function stuck using nccl backend bug trying use distributed.all gather gather gradients multi nodes. found gather function stuck error throw reproduce steps reproduce behavior 1. set env variable , save following code 2. run code btw, execute code manually , found gather quickly go through, stuck trying print tensor expected behavior process print list tensor environment please copy paste output environment collection script https raw.githubusercontent.com pytorch pytorch master torch utils collect env.py fill checklist manually . get script run pytorch version 1.0.1.post2 debug build cuda used build pytorch 9.0.176 os ubuntu 18.04.1 lts gcc version ubuntu 5.4.0 6ubuntu1 16.04.10 5.4.0 20160609 cmake version version 3.10.2 python version 3.5 cuda available yes cuda runtime version 9.0.176 gpu models configuration gpu 0 tesla k80 gpu 1 tesla k80 gpu 2 tesla k80 gpu 3 tesla k80 gpu 4 tesla k80 gpu 5 tesla k80 gpu 6 tesla k80 gpu 7 tesla k80 nvidia driver version 384.145 cudnn version usr lib x86 64 linux gnu libcudnn.so.7.3.1 versions relevant libraries pip3 numpy 1.15.3 pip3 torch 1.0.1.post2 pip3 torchvision 0.2.1 conda could collect additional context"
pytorch,23232,"feature support , , , general python objects. instead overloading existing api, might clearer make explicit, e.g. , , , . motivation sharing objects across processes common use case. people actually started building solutions serializing meta object tensors using tensor collective comm apis share see, 22490, 23228 . pitch python object different processes might size. needs done multiple steps 1. pickle local python object byte tensors. 2. communicate tensor sizes across processes. then, process allocates local buffer tensors using max size. 3. copy local tensor buffer tensor communicate. 4. extract results buffer tensors unpickle back python objects. cc pietern zhaojuanmao pritamdamania87",0,add collective communication apis python objects.,"add collective communication apis python objects. feature support , , , general python objects. instead overloading existing api, might clearer make explicit, e.g. , , , . motivation sharing objects across processes common use case. people actually started building solutions serializing meta object tensors using tensor collective comm apis share see, 22490, 23228 . pitch python object different processes might size. needs done multiple steps 1. pickle local python object byte tensors. 2. communicate tensor sizes across processes. then, process allocates local buffer tensors using max size. 3. copy local tensor buffer tensor communicate. 4. extract results buffer tensors unpickle back python objects. cc pietern zhaojuanmao pritamdamania87"
pytorch,14179,"bug build files written home feng pytorch build make install j12 scanning dependencies target js embed scanning dependencies target benchmark scanning dependencies target nccl external scanning dependencies target pthreadpool scanning dependencies target clog scanning dependencies target gtest scanning dependencies target gloo scanning dependencies target onnxifi dummy scanning dependencies target onnxifi loader scanning dependencies target libprotobuf lite scanning dependencies target libprotobuf 0 creating directories 'nccl external' scanning dependencies target mkldnn 1 building cxx object third party benchmark src cmakefiles benchmark.dir json reporter.cc.o 1 building cxx object third party protobuf cmake cmakefiles js embed.dir src google protobuf compiler js embed.cc.o 1 building c object confu deps clog cmakefiles clog.dir src clog.c.o 1 download step 'nccl external' 1 patch step 'nccl external' 1 update step 'nccl external' 1 configure step 'nccl external' 1 performing build step 'nccl external' 1 building c object third party onnx cmakefiles onnxifi loader.dir onnx onnxifi loader.c.o 1 building c object third party onnx cmakefiles onnxifi dummy.dir onnx onnxifi dummy.c.o 1 building c object confu deps pthreadpool cmakefiles pthreadpool.dir src threadpool pthreads.c.o make 3 warning jobserver unavailable using j1. add ' ' parent make rule. home feng pytorch third party qnnpack deps clog src clog.c function clog vlog fatal home feng pytorch third party qnnpack deps clog src clog.c 120 4 warning ignoring return value write , declared attribute warn unused result wunused result write stderr fileno, buffer, prefix chars format chars clog suffix length home feng pytorch third party qnnpack deps clog src clog.c function clog vlog error home feng pytorch third party qnnpack deps clog src clog.c 196 4 warning ignoring return value write , declared attribute warn unused result wunused result write stderr fileno, buffer, prefix chars format chars clog suffix length home feng pytorch third party qnnpack deps clog src clog.c function clog vlog warning home feng pytorch third party qnnpack deps clog src clog.c 272 4 warning ignoring return value write , declared attribute warn unused result wunused result write stderr fileno, buffer, prefix chars format chars clog suffix length home feng pytorch third party qnnpack deps clog src clog.c function clog vlog info home feng pytorch third party qnnpack deps clog src clog.c 348 4 warning ignoring return value write , declared attribute warn unused result wunused result write stdout fileno, buffer, prefix chars format chars clog suffix length home feng pytorch third party qnnpack deps clog src clog.c function clog vlog debug home feng pytorch third party qnnpack deps clog src clog.c 424 4 warning ignoring return value write , declared attribute warn unused result wunused result write stdout fileno, buffer, prefix chars format chars clog suffix length 1 building cxx object third party gloo gloo cmakefiles gloo.dir algorithm.cc.o 1 building cxx object third party protobuf cmake cmakefiles libprotobuf lite.dir src google protobuf arena.cc.o generating nccl.h.in home feng pytorch build nccl include nccl.h compiling init.cu home feng pytorch build nccl obj init.o nvcc warning 'compute 20', 'sm 20', 'sm 21' architectures deprecated, may removed future release use wno deprecated gpu targets suppress warning . 1 building cxx object third party googletest googletest cmakefiles gtest.dir src gtest all.cc.o 1 linking cxx executable .. .. .. bin js embed 1 building cxx object third party protobuf cmake cmakefiles libprotobuf lite.dir src google protobuf arenastring.cc.o 1 building cxx object third party benchmark src cmakefiles benchmark.dir string util.cc.o 1 linking c shared library .. .. lib libonnxifi dummy.so 1 linking c static library .. .. lib libpthreadpool.a 1 linking c static library .. .. lib libclog.a 1 building cxx object third party gloo gloo cmakefiles gloo.dir allgather.cc.o 1 linking c static library .. .. lib libonnxifi loader.a 1 building cxx object third party ideep mkl dnn src cmakefiles mkldnn.dir common primitive.cpp.o 1 built target onnxifi dummy 1 built target js embed 1 building cxx object third party ideep mkl dnn src cmakefiles mkldnn.dir common pooling.cpp.o 1 building cxx object third party protobuf cmake cmakefiles libprotobuf lite.dir src google protobuf extension set.cc.o 1 building cxx object third party protobuf cmake cmakefiles libprotobuf.dir src google protobuf arena.cc.o 1 built target onnxifi loader 1 built target pthreadpool 1 built target clog 1 building cxx object third party protobuf cmake cmakefiles libprotobuf.dir src google protobuf arenastring.cc.o scanning dependencies target python copy files scanning dependencies target c10 1 building cxx object c10 cmakefiles c10.dir devicetype.cpp.o 1 building cxx object third party ideep mkl dnn src cmakefiles mkldnn.dir common engine.cpp.o 1 building cxx object third party ideep mkl dnn src cmakefiles mkldnn.dir common query.cpp.o 1 building cxx object third party protobuf cmake cmakefiles libprotobuf lite.dir src google protobuf generated message table driven lite.cc.o 1 building cxx object third party gloo gloo cmakefiles gloo.dir allreduce.cc.o 1 building cxx object third party ideep mkl dnn src cmakefiles mkldnn.dir common softmax.cpp.o 1 building cxx object c10 cmakefiles c10.dir half.cpp.o 1 building cxx object third party ideep mkl dnn src cmakefiles mkldnn.dir common mkldnn debug.cpp.o 1 building cxx object third party ideep mkl dnn src cmakefiles mkldnn.dir common batch normalization.cpp.o 2 building cxx object third party protobuf cmake cmakefiles libprotobuf.dir src google protobuf extension set.cc.o 2 building cxx object third party ideep mkl dnn src cmakefiles mkldnn.dir common scratchpad.cpp.o 2 building cxx object third party benchmark src cmakefiles benchmark.dir commandlineflags.cc.o 3 building cxx object third party ideep mkl dnn src cmakefiles mkldnn.dir common primitive attr.cpp.o 3 building cxx object third party protobuf cmake cmakefiles libprotobuf.dir src google protobuf generated message table driven lite.cc.o 3 building cxx object c10 cmakefiles c10.dir device.cpp.o 3 building cxx object third party ideep mkl dnn src cmakefiles mkldnn.dir common verbose.cpp.o 3 building cxx object third party ideep mkl dnn src cmakefiles mkldnn.dir common eltwise.cpp.o 3 building cxx object third party benchmark src cmakefiles benchmark.dir sleep.cc.o 3 building cxx object third party ideep mkl dnn src cmakefiles mkldnn.dir common memory desc wrapper.cpp.o 3 building cxx object third party ideep mkl dnn src cmakefiles mkldnn.dir common primitive iterator.cpp.o 3 building cxx object third party ideep mkl dnn src cmakefiles mkldnn.dir common lrn.cpp.o 3 building cxx object third party gloo gloo cmakefiles gloo.dir allreduce local.cc.o 3 building cxx object c10 cmakefiles c10.dir stream.cpp.o 3 building cxx object third party ideep mkl dnn src cmakefiles mkldnn.dir common memory.cpp.o 3 building cxx object third party benchmark src cmakefiles benchmark.dir statistics.cc.o 3 building cxx object third party ideep mkl dnn src cmakefiles mkldnn.dir common inner product.cpp.o 3 building cxx object c10 cmakefiles c10.dir core dispatch opschema.cpp.o 3 building cxx object c10 cmakefiles c10.dir core dispatch kernelregistration.cpp.o 3 building cxx object c10 cmakefiles c10.dir core dispatch deviceid.cpp.o 3 building cxx object third party gloo gloo cmakefiles gloo.dir broadcast.cc.o 3 building cxx object third party ideep mkl dnn src cmakefiles mkldnn.dir common stream.cpp.o 3 building cxx object third party ideep mkl dnn src cmakefiles mkldnn.dir common convolution relu.cpp.o 3 building cxx object third party protobuf cmake cmakefiles libprotobuf lite.dir src google protobuf generated message util.cc.o 3 building cxx object third party protobuf cmake cmakefiles libprotobuf lite.dir src google protobuf io coded stream.cc.o 3 building cxx object third party ideep mkl dnn src cmakefiles mkldnn.dir common rnn.cpp.o 3 building cxx object third party ideep mkl dnn src cmakefiles mkldnn.dir common reorder.cpp.o 3 building cxx object c10 cmakefiles c10.dir core dispatch dispatchkey.cpp.o ptxas warning big maxrregcount value specified 96, ignored 3 building cxx object third party protobuf cmake cmakefiles libprotobuf.dir src google protobuf generated message util.cc.o 3 building cxx object third party gloo gloo cmakefiles gloo.dir context.cc.o 3 building cxx object third party ideep mkl dnn src cmakefiles mkldnn.dir common convolution.cpp.o 3 building cxx object third party benchmark src cmakefiles benchmark.dir benchmark.cc.o 3 building cxx object third party ideep mkl dnn src cmakefiles mkldnn.dir common primitive desc.cpp.o 3 building cxx object c10 cmakefiles c10.dir core dispatch dispatcher.cpp.o 3 building cxx object c10 cmakefiles c10.dir core dispatch opschemaregistration.cpp.o 3 building cxx object third party ideep mkl dnn src cmakefiles mkldnn.dir common deconvolution.cpp.o 3 building cxx object third party ideep mkl dnn src cmakefiles mkldnn.dir common utils.cpp.o 3 building cxx object third party ideep mkl dnn src cmakefiles mkldnn.dir cpu jit sse42 conv kernel f32.cpp.o 3 building cxx object third party protobuf cmake cmakefiles libprotobuf lite.dir src google protobuf io zero copy stream.cc.o 3 building cxx object third party gloo gloo cmakefiles gloo.dir gather.cc.o 3 building cxx object third party ideep mkl dnn src cmakefiles mkldnn.dir cpu cpu batch normalization utils.cpp.o init.cu 52 1 warning ncclnet initialized declared extern ncclnet ncclnet null 3 building cxx object c10 cmakefiles c10.dir core dispatch dispatchtable.cpp.o 3 building cxx object c10 cmakefiles c10.dir core dispatch layoutid.cpp.o 3 building cxx object third party gloo gloo cmakefiles gloo.dir reduce.cc.o 3 building cxx object third party protobuf cmake cmakefiles libprotobuf lite.dir src google protobuf io zero copy stream impl lite.cc.o 3 building cxx object c10 cmakefiles c10.dir impl deviceguardimplinterface.cpp.o 4 building cxx object third party protobuf cmake cmakefiles libprotobuf lite.dir src google protobuf message lite.cc.o 4 building cxx object c10 cmakefiles c10.dir util type.cpp.o 4 building cxx object third party protobuf cmake cmakefiles libprotobuf.dir src google protobuf io coded stream.cc.o 4 building cxx object third party protobuf cmake cmakefiles libprotobuf lite.dir src google protobuf repeated field.cc.o nvcc warning 'compute 20', 'sm 20', 'sm 21' architectures deprecated, may removed future release use wno deprecated gpu targets suppress warning . 4 building cxx object third party ideep mkl dnn src cmakefiles mkldnn.dir cpu ref lrn.cpp.o 4 building cxx object c10 cmakefiles c10.dir util backtrace.cpp.o 4 building cxx object c10 cmakefiles c10.dir util optional.cpp.o 4 building cxx object third party gloo gloo cmakefiles gloo.dir scatter.cc.o 4 building cxx object c10 cmakefiles c10.dir util c 17.cpp.o compiling ring.cu home feng pytorch build nccl obj ring.o nvcc warning 'compute 20', 'sm 20', 'sm 21' architectures deprecated, may removed future release use wno deprecated gpu targets suppress warning . 5 building cxx object c10 cmakefiles c10.dir util smallvector.cpp.o 5 building cxx object third party benchmark src cmakefiles benchmark.dir csv reporter.cc.o 5 building cxx object third party protobuf cmake cmakefiles libprotobuf lite.dir src google protobuf stubs atomicops internals x86 gcc.cc.o 5 building cxx object third party protobuf cmake cmakefiles libprotobuf lite.dir src google protobuf stubs atomicops internals x86 msvc.cc.o 5 building cxx object c10 cmakefiles c10.dir util leftright.cpp.o 5 building cxx object c10 cmakefiles c10.dir util flags use gflags.cpp.o 5 building cxx object third party protobuf cmake cmakefiles libprotobuf.dir src google protobuf io zero copy stream.cc.o 5 building cxx object third party ideep mkl dnn src cmakefiles mkldnn.dir cpu jit avx512 common 1x1 conv kernel.cpp.o 5 linking cxx static library .. .. .. lib libgtest.a 5 built target gtest scanning dependencies target aten cpu files gen target 5 generating .. aten src aten cpubytetype.cpp, .. aten src aten cpubytetype.h, .. aten src aten cpuchartype.cpp, .. aten src aten cpuchartype.h, .. aten src aten cpucopy.cpp, .. aten src aten cpudoubletype.cpp, .. aten src aten cpudoubletype.h, .. aten src aten cpufloattype.cpp, .. aten src aten cpufloattype.h, .. aten src aten cpugenerator.h, .. aten src aten cpuhalftype.cpp, .. aten src aten cpuhalftype.h, .. aten src aten cpuinttype.cpp, .. aten src aten cpuinttype.h, .. aten src aten cpulongtype.cpp, .. aten src aten cpulongtype.h, .. aten src aten cpushorttype.cpp, .. aten src aten cpushorttype.h, .. aten src aten declarations.yaml, .. aten src aten functions.h, .. aten src aten nativefunctions.h, .. aten src aten registercpu.cpp, .. aten src aten registercpu.h, .. aten src aten sparsecpubytetype.cpp, .. aten src aten sparsecpubytetype.h, .. aten src aten sparsecpuchartype.cpp, .. aten src aten sparsecpuchartype.h, .. aten src aten sparsecpudoubletype.cpp, .. aten src aten sparsecpudoubletype.h, .. aten src aten sparsecpufloattype.cpp, .. aten src aten sparsecpufloattype.h, .. aten src aten sparsecpuinttype.cpp, .. aten src aten sparsecpuinttype.h, .. aten src aten sparsecpulongtype.cpp, .. aten src aten sparsecpulongtype.h, .. aten src aten sparsecpushorttype.cpp, .. aten src aten sparsecpushorttype.h, .. aten src aten typedefault.cpp, .. aten src aten typedefault.h, .. aten src aten typeextendedinterface.h, .. aten src aten cudabytetype.cpp, .. aten src aten cudabytetype.h, .. aten src aten cudachartype.cpp, .. aten src aten cudachartype.h, .. aten src aten cudacopy.cpp, .. aten src aten cudadoubletype.cpp, .. aten src aten cudadoubletype.h, .. aten src aten cudafloattype.cpp, .. aten src aten cudafloattype.h, .. aten src aten cudagenerator.h, .. aten src aten cudahalftype.cpp, .. aten src aten cudahalftype.h, .. aten src aten cudainttype.cpp, .. aten src aten cudainttype.h, .. aten src aten cudalongtype.cpp, .. aten src aten cudalongtype.h, .. aten src aten cudashorttype.cpp, .. aten src aten cudashorttype.h, .. aten src aten registercuda.cpp, .. aten src aten registercuda.h, .. aten src aten sparsecudabytetype.cpp, .. aten src aten sparsecudabytetype.h, .. aten src aten sparsecudachartype.cpp, .. aten src aten sparsecudachartype.h, .. aten src aten sparsecudadoubletype.cpp, .. aten src aten sparsecudadoubletype.h, .. aten src aten sparsecudafloattype.cpp, .. aten src aten sparsecudafloattype.h, .. aten src aten sparsecudainttype.cpp, .. aten src aten sparsecudainttype.h, .. aten src aten sparsecudalongtype.cpp, .. aten src aten sparsecudalongtype.h, .. aten src aten sparsecudashorttype.cpp, .. aten src aten sparsecudashorttype.h 6 building cxx object third party gloo gloo cmakefiles gloo.dir types.cc.o 6 building cxx object third party ideep mkl dnn src cmakefiles mkldnn.dir cpu ref deconvolution.cpp.o 7 building cxx object third party ideep mkl dnn src cmakefiles mkldnn.dir cpu jit sse42 1x1 conv kernel f32.cpp.o 7 built target python copy files scanning dependencies target common 7 building cxx object third party protobuf cmake cmakefiles libprotobuf lite.dir src google protobuf stubs bytestream.cc.o 7 building c object sleef src common cmakefiles common.dir common.c.o 7 building cxx object third party protobuf cmake cmakefiles libprotobuf lite.dir src google protobuf stubs common.cc.o 7 built target common scanning dependencies target mkrename 7 building c object sleef src libm cmakefiles mkrename.dir mkrename.c.o 7 building cxx object third party gloo gloo cmakefiles gloo.dir common linux.cc.o 7 linking c executable .. .. bin mkrename 7 building cxx object third party benchmark src cmakefiles benchmark.dir reporter.cc.o 7 built target mkrename 7 building cxx object third party benchmark src cmakefiles benchmark.dir complexity.cc.o 7 building cxx object third party protobuf cmake cmakefiles libprotobuf.dir src google protobuf io zero copy stream impl lite.cc.o 7 building cxx object c10 cmakefiles c10.dir util array.cpp.o 7 building cxx object third party ideep mkl dnn src cmakefiles mkldnn.dir cpu cpu sum.cpp.o 7 building cxx object c10 cmakefiles c10.dir util logging.cpp.o 7 building cxx object third party protobuf cmake cmakefiles libprotobuf lite.dir src google protobuf stubs int128.cc.o ptxas warning big maxrregcount value specified 96, ignored 7 building cxx object third party benchmark src cmakefiles benchmark.dir counter.cc.o 7 building cxx object third party protobuf cmake cmakefiles libprotobuf.dir src google protobuf message lite.cc.o 7 building cxx object third party protobuf cmake cmakefiles libprotobuf lite.dir src google protobuf stubs io win32.cc.o 7 building cxx object third party protobuf cmake cmakefiles libprotobuf lite.dir src google protobuf stubs once.cc.o 7 building cxx object third party benchmark src cmakefiles benchmark.dir sysinfo.cc.o 7 building cxx object third party protobuf cmake cmakefiles libprotobuf lite.dir src google protobuf stubs status.cc.o 7 building cxx object third party benchmark src cmakefiles benchmark.dir colorprint.cc.o 7 building cxx object third party protobuf cmake cmakefiles libprotobuf lite.dir src google protobuf stubs statusor.cc.o nvcc warning 'compute 20', 'sm 20', 'sm 21' architectures deprecated, may removed future release use wno deprecated gpu targets suppress warning . 7 building cxx object third party gloo gloo cmakefiles gloo.dir common logging.cc.o 7 building cxx object third party gloo gloo cmakefiles gloo.dir mpi context.cc.o 7 building cxx object c10 cmakefiles c10.dir util exception.cpp.o compiling bootstrap.cu home feng pytorch build nccl obj bootstrap.o nvcc warning 'compute 20', 'sm 20', 'sm 21' architectures deprecated, may removed future release use wno deprecated gpu targets suppress warning . 7 building cxx object third party protobuf cmake cmakefiles libprotobuf.dir src google protobuf repeated field.cc.o 7 building cxx object third party protobuf cmake cmakefiles libprotobuf lite.dir src google protobuf stubs stringpiece.cc.o 7 building cxx object third party protobuf cmake cmakefiles libprotobuf lite.dir src google protobuf stubs stringprintf.cc.o 7 building cxx object third party benchmark src cmakefiles benchmark.dir timers.cc.o 7 building cxx object third party ideep mkl dnn src cmakefiles mkldnn.dir cpu jit avx512 common convolution.cpp.o 7 building cxx object third party ideep mkl dnn src cmakefiles mkldnn.dir cpu jit avx512 common conv winograd kernel f32.cpp.o 7 building cxx object third party gloo gloo cmakefiles gloo.dir rendezvous context.cc.o 7 building cxx object third party benchmark src cmakefiles benchmark.dir benchmark register.cc.o 7 building cxx object third party protobuf cmake cmakefiles libprotobuf.dir src google protobuf stubs atomicops internals x86 gcc.cc.o 7 building cxx object c10 cmakefiles c10.dir util typelist.cpp.o 7 building cxx object third party protobuf cmake cmakefiles libprotobuf lite.dir src google protobuf stubs structurally valid.cc.o 7 building cxx object third party protobuf cmake cmakefiles libprotobuf lite.dir src google protobuf stubs strutil.cc.o 7 building cxx object third party benchmark src cmakefiles benchmark.dir console reporter.cc.o 7 building cxx object c10 cmakefiles c10.dir util typeid.cpp.o 7 building cxx object third party protobuf cmake cmakefiles libprotobuf.dir src google protobuf stubs atomicops internals x86 msvc.cc.o 7 building cxx object third party gloo gloo cmakefiles gloo.dir rendezvous file store.cc.o 7 building cxx object third party gloo gloo cmakefiles gloo.dir rendezvous hash store.cc.o 7 building cxx object third party protobuf cmake cmakefiles libprotobuf lite.dir src google protobuf stubs time.cc.o ptxas warning big maxrregcount value specified 96, ignored 7 building cxx object third party protobuf cmake cmakefiles libprotobuf lite.dir src google protobuf wire format lite.cc.o 7 building cxx object third party protobuf cmake cmakefiles libprotobuf.dir src google protobuf stubs bytestream.cc.o 7 building cxx object c10 cmakefiles c10.dir util tensortypeid.cpp.o 7 building cxx object third party gloo gloo cmakefiles gloo.dir rendezvous prefix store.cc.o 7 building cxx object c10 cmakefiles c10.dir util metaprogramming.cpp.o 7 building cxx object c10 cmakefiles c10.dir util typetraits.cpp.o 7 building cxx object third party gloo gloo cmakefiles gloo.dir rendezvous store.cc.o 7 building cxx object third party gloo gloo cmakefiles gloo.dir transport address.cc.o 7 building cxx object third party gloo gloo cmakefiles gloo.dir transport buffer.cc.o 7 building cxx object c10 cmakefiles c10.dir util stringutil.cpp.o 7 building cxx object third party gloo gloo cmakefiles gloo.dir transport context.cc.o 7 building cxx object third party protobuf cmake cmakefiles libprotobuf.dir src google protobuf stubs common.cc.o 7 building cxx object c10 cmakefiles c10.dir util tensortypeidregistration.cpp.o 7 building cxx object third party gloo gloo cmakefiles gloo.dir transport device.cc.o 7 building cxx object third party gloo gloo cmakefiles gloo.dir transport pair.cc.o nvcc warning 'compute 20', 'sm 20', 'sm 21' architectures deprecated, may removed future release use wno deprecated gpu targets suppress warning . 7 building cxx object c10 cmakefiles c10.dir util flags use gflags.cpp.o 7 building cxx object third party gloo gloo cmakefiles gloo.dir transport unbound buffer.cc.o 7 building cxx object third party gloo gloo cmakefiles gloo.dir transport tcp address.cc.o 7 building cxx object third party gloo gloo cmakefiles gloo.dir transport tcp buffer.cc.o 7 building cxx object third party gloo gloo cmakefiles gloo.dir transport tcp context.cc.o 7 building cxx object third party ideep mkl dnn src cmakefiles mkldnn.dir cpu gemm inner product.cpp.o compiling transport.cu home feng pytorch build nccl obj transport.o nvcc warning 'compute 20', 'sm 20', 'sm 21' architectures deprecated, may removed future release use wno deprecated gpu targets suppress warning . 7 linking cxx static library .. .. .. lib libprotobuf lite.a 7 built target libprotobuf lite 7 building cxx object third party gloo gloo cmakefiles gloo.dir transport tcp device.cc.o scanning dependencies target mkdisp 7 building c object sleef src libm cmakefiles mkdisp.dir mkdisp.c.o 7 linking c executable .. .. bin mkdisp 7 built target mkdisp scanning dependencies target renamedsp256.h generated 7 generating renamedsp256.h 7 built target renamedsp256.h generated scanning dependencies target dispavx.c generated 7 generating dispavx.c 7 built target dispavx.c generated scanning dependencies target renamesse2.h generated 7 generating include renamesse2.h 7 building cxx object third party gloo gloo cmakefiles gloo.dir transport tcp pair.cc.o generating renamesse2.h mkrename 2 4 sse2 7 built target renamesse2.h generated scanning dependencies target renamefma4.h generated 7 generating include renamefma4.h generating renamefma4.h mkrename 4 8 fma4 7 built target renamefma4.h generated scanning dependencies target renameavx2.h generated 7 generating include renameavx2.h generating renameavx2.h mkrename 4 8 avx2 7 built target renameavx2.h generated scanning dependencies target renameavx2128.h generated 7 generating include renameavx2128.h generating renameavx2128.h mkrename 2 4 avx2128 7 built target renameavx2128.h generated scanning dependencies target renamesse4.h generated 8 generating include renamesse4.h generating renamesse4.h mkrename 2 4 sse4 8 built target renamesse4.h generated scanning dependencies target aten cuda files gen target 8 building cxx object third party protobuf cmake cmakefiles libprotobuf.dir src google protobuf stubs int128.cc.o 8 generating .. aten src aten cpubytetype.cpp, .. aten src aten cpubytetype.h, .. aten src aten cpuchartype.cpp, .. aten src aten cpuchartype.h, .. aten src aten cpucopy.cpp, .. aten src aten cpudoubletype.cpp, .. aten src aten cpudoubletype.h, .. aten src aten cpufloattype.cpp, .. aten src aten cpufloattype.h, .. aten src aten cpugenerator.h, .. aten src aten cpuhalftype.cpp, .. aten src aten cpuhalftype.h, .. aten src aten cpuinttype.cpp, .. aten src aten cpuinttype.h, .. aten src aten cpulongtype.cpp, .. aten src aten cpulongtype.h, .. aten src aten cpushorttype.cpp, .. aten src aten cpushorttype.h, .. aten src aten declarations.yaml, .. aten src aten functions.h, .. aten src aten nativefunctions.h, .. aten src aten registercpu.cpp, .. aten src aten registercpu.h, .. aten src aten sparsecpubytetype.cpp, .. aten src aten sparsecpubytetype.h, .. aten src aten sparsecpuchartype.cpp, .. aten src aten sparsecpuchartype.h, .. aten src aten sparsecpudoubletype.cpp, .. aten src aten sparsecpudoubletype.h, .. aten src aten sparsecpufloattype.cpp, .. aten src aten sparsecpufloattype.h, .. aten src aten sparsecpuinttype.cpp, .. aten src aten sparsecpuinttype.h, .. aten src aten sparsecpulongtype.cpp, .. aten src aten sparsecpulongtype.h, .. aten src aten sparsecpushorttype.cpp, .. aten src aten sparsecpushorttype.h, .. aten src aten typedefault.cpp, .. aten src aten typedefault.h, .. aten src aten typeextendedinterface.h, .. aten src aten cudabytetype.cpp, .. aten src aten cudabytetype.h, .. aten src aten cudachartype.cpp, .. aten src aten cudachartype.h, .. aten src aten cudacopy.cpp, .. aten src aten cudadoubletype.cpp, .. aten src aten cudadoubletype.h, .. aten src aten cudafloattype.cpp, .. aten src aten cudafloattype.h, .. aten src aten cudagenerator.h, .. aten src aten cudahalftype.cpp, .. aten src aten cudahalftype.h, .. aten src aten cudainttype.cpp, .. aten src aten cudainttype.h, .. aten src aten cudalongtype.cpp, .. aten src aten cudalongtype.h, .. aten src aten cudashorttype.cpp, .. aten src aten cudashorttype.h, .. aten src aten registercuda.cpp, .. aten src aten registercuda.h, .. aten src aten sparsecudabytetype.cpp, .. aten src aten sparsecudabytetype.h, .. aten src aten sparsecudachartype.cpp, .. aten src aten sparsecudachartype.h, .. aten src aten sparsecudadoubletype.cpp, .. aten src aten sparsecudadoubletype.h, .. aten src aten sparsecudafloattype.cpp, .. aten src aten sparsecudafloattype.h, .. aten src aten sparsecudainttype.cpp, .. aten src aten sparsecudainttype.h, .. aten src aten sparsecudalongtype.cpp, .. aten src aten sparsecudalongtype.h, .. aten src aten sparsecudashorttype.cpp, .. aten src aten sparsecudashorttype.h 8 building cxx object third party gloo gloo cmakefiles gloo.dir transport tcp unbound buffer.cc.o 8 linking cxx shared library .. lib libc10.so 8 built target c10 scanning dependencies target mkrename gnuabi 8 building c object sleef src libm cmakefiles mkrename gnuabi.dir mkrename gnuabi.c.o 8 linking c executable .. .. bin mkrename gnuabi 8 built target mkrename gnuabi scanning dependencies target mkmasked gnuabi 8 building c object sleef src libm cmakefiles mkmasked gnuabi.dir mkmasked gnuabi.c.o 8 linking c executable .. .. bin mkmasked gnuabi 8 built target mkmasked gnuabi scanning dependencies target mkalias 8 building c object sleef src libm cmakefiles mkalias.dir mkalias.c.o scanning dependencies target arraymap 8 building c object sleef src common cmakefiles arraymap.dir arraymap.c.o 8 linking c executable .. .. bin mkalias 8 built target mkalias 8 building cxx object third party protobuf cmake cmakefiles libprotobuf.dir src google protobuf stubs io win32.cc.o scanning dependencies target torch shm manager 8 building cxx object third party protobuf cmake cmakefiles libprotobuf.dir src google protobuf stubs once.cc.o 8 building cxx object caffe2 torch lib libshm cmakefiles torch shm manager.dir manager.cpp.o 8 building cxx object third party protobuf cmake cmakefiles libprotobuf.dir src google protobuf stubs status.cc.o 8 built target arraymap scanning dependencies target c10 utils gpu 9 building cxx object caffe2 utils cmakefiles c10 utils gpu.dir dummy.cpp.o 9 building cxx object third party ideep mkl dnn src cmakefiles mkldnn.dir cpu cpu barrier.cpp.o 9 built target c10 utils gpu scanning dependencies target c10 utils hip 9 building cxx object caffe2 utils cmakefiles c10 utils hip.dir dummy.cpp.o 9 built target c10 utils hip scanning dependencies target c10 utils cpu 9 building cxx object caffe2 utils cmakefiles c10 utils cpu.dir dummy.cpp.o 9 building cxx object third party ideep mkl dnn src cmakefiles mkldnn.dir cpu simple sum.cpp.o 9 built target c10 utils cpu scanning dependencies target cpuinfo 9 building c object confu deps cpuinfo cmakefiles cpuinfo.dir src init.c.o 9 building c object confu deps cpuinfo cmakefiles cpuinfo.dir src api.c.o 9 building c object confu deps cpuinfo cmakefiles cpuinfo.dir src x86 init.c.o 9 linking cxx static library .. .. .. lib libbenchmark.a 9 built target benchmark 9 building c object confu deps cpuinfo cmakefiles cpuinfo.dir src x86 info.c.o scanning dependencies target nnpack reference layers 9 building c object confu deps nnpack cmakefiles nnpack reference layers.dir src ref convolution output.c.o 9 building c object confu deps cpuinfo cmakefiles cpuinfo.dir src x86 vendor.c.o 9 building c object confu deps cpuinfo cmakefiles cpuinfo.dir src x86 uarch.c.o ptxas warning big maxrregcount value specified 96, ignored 9 building c object confu deps nnpack cmakefiles nnpack reference layers.dir src ref convolution input gradient.c.o 9 building cxx object third party ideep mkl dnn src cmakefiles mkldnn.dir cpu ncsp batch normalization.cpp.o 9 building c object confu deps cpuinfo cmakefiles cpuinfo.dir src x86 name.c.o 9 building c object confu deps nnpack cmakefiles nnpack reference layers.dir src ref convolution kernel.c.o 9 building cxx object third party protobuf cmake cmakefiles libprotobuf.dir src google protobuf stubs statusor.cc.o 9 building c object confu deps nnpack cmakefiles nnpack reference layers.dir src ref fully connected output.c.o 9 building c object confu deps cpuinfo cmakefiles cpuinfo.dir src x86 topology.c.o 9 building c object confu deps nnpack cmakefiles nnpack reference layers.dir src ref max pooling output.c.o 9 building c object confu deps cpuinfo cmakefiles cpuinfo.dir src x86 isa.c.o 9 building c object confu deps nnpack cmakefiles nnpack reference layers.dir src ref softmax output.c.o 9 built target aten cpu files gen target 9 building c object confu deps nnpack cmakefiles nnpack reference layers.dir src ref relu output.c.o 9 building cxx object third party ideep mkl dnn src cmakefiles mkldnn.dir cpu ref inner product.cpp.o 9 building c object confu deps cpuinfo cmakefiles cpuinfo.dir src x86 cache descriptor.c.o 9 building c object confu deps nnpack cmakefiles nnpack reference layers.dir src ref relu input gradient.c.o 9 building c object confu deps cpuinfo cmakefiles cpuinfo.dir src x86 cache init.c.o 9 linking c static library .. .. lib libnnpack reference layers.a 9 building c object confu deps cpuinfo cmakefiles cpuinfo.dir src x86 cache deterministic.c.o 10 building c object confu deps cpuinfo cmakefiles cpuinfo.dir src x86 linux init.c.o 10 built target nnpack reference layers scanning dependencies target gtest main 10 building cxx object third party googletest googletest cmakefiles gtest main.dir src gtest main.cc.o 10 building c object confu deps cpuinfo cmakefiles cpuinfo.dir src x86 linux cpuinfo.c.o 10 building c object confu deps cpuinfo cmakefiles cpuinfo.dir src linux smallfile.c.o 10 building c object confu deps cpuinfo cmakefiles cpuinfo.dir src linux multiline.c.o 10 building c object confu deps cpuinfo cmakefiles cpuinfo.dir src linux current.c.o 10 building c object confu deps cpuinfo cmakefiles cpuinfo.dir src linux cpulist.c.o 10 building c object confu deps cpuinfo cmakefiles cpuinfo.dir src linux processors.c.o scanning dependencies target benchmark main 10 building cxx object third party benchmark src cmakefiles benchmark main.dir benchmark main.cc.o 10 linking c static library .. .. lib libcpuinfo.a 10 built target cpuinfo scanning dependencies target onnxifi wrapper 10 building cxx object third party protobuf cmake cmakefiles libprotobuf.dir src google protobuf stubs stringpiece.cc.o 10 building c object third party onnx cmakefiles onnxifi wrapper.dir onnx onnxifi wrapper.c.o 10 building cxx object third party ideep mkl dnn src cmakefiles mkldnn.dir cpu simple concat.cpp.o nvcc warning 'compute 20', 'sm 20', 'sm 21' architectures deprecated, may removed future release use wno deprecated gpu targets suppress warning . 10 linking cxx executable .. .. .. .. bin torch shm manager 10 linking c shared module .. .. lib libonnxifi.so 10 built target onnxifi wrapper scanning dependencies target aten op header gen 10 generating contrib aten aten op.h 10 built target torch shm manager scanning dependencies target renameavx.h generated 10 generating include renameavx.h generating renameavx.h mkrename 4 8 avx 10 built target renameavx.h generated 11 linking cxx static library .. .. .. lib libgloo.a scanning dependencies target renamedsp128.h generated 11 generating renamedsp128.h 11 built target renamedsp128.h generated scanning dependencies target headers 12 generating .. .. .. include sleef.h 12 built target gloo scanning dependencies target dispsse.c generated 12 generating dispsse.c generating sleef.h mkrename 2 4 m128d m128 m128i m128i sse2 generating sleef.h mkrename 2 4 m128d m128 m128i m128i sse2 sse2 generating sleef.h mkrename 2 4 m128d m128 m128i m128i sse2 sse4 generating sleef.h mkrename 4 8 m256d m256 m128i struct m128i x, avx generating sleef.h mkrename 4 8 m256d m256 m128i struct m128i x, avx avx generating sleef.h mkrename 4 8 m256d m256 m128i struct m128i x, avx fma4 generating sleef.h mkrename 4 8 m256d m256 m128i m256i avx avx2 generating sleef.h mkrename 2 4 m128d m128 m128i m128i sse2 avx2128 generating sleef.h mkrename 8 16 m512d m512 m256i m512i avx512f generating sleef.h mkrename 8 16 m512d m512 m256i m512i avx512f avx512f 12 built target dispsse.c generated 12 built target headers scanning dependencies target sleefsse2 scanning dependencies target sleeffma4 12 building c object sleef src libm cmakefiles sleefsse2.dir sleefsimdsp.c.o 12 building c object sleef src libm cmakefiles sleeffma4.dir sleefsimdsp.c.o compiling misc group.cu home feng pytorch build nccl obj misc group.o nvcc warning 'compute 20', 'sm 20', 'sm 21' architectures deprecated, may removed future release use wno deprecated gpu targets suppress warning . 12 linking cxx static library .. .. .. lib libbenchmark main.a 12 linking cxx static library .. .. .. lib libgtest main.a 12 built target benchmark main scanning dependencies target sleefavx2 12 built target gtest main 12 building c object sleef src libm cmakefiles sleefavx2.dir sleefsimdsp.c.o scanning dependencies target sleefavx2128 12 building c object sleef src libm cmakefiles sleefavx2128.dir sleefsimdsp.c.o 12 building cxx object third party protobuf cmake cmakefiles libprotobuf.dir src google protobuf stubs stringprintf.cc.o 12 building c object sleef src libm cmakefiles sleeffma4.dir sleefsimddp.c.o 12 building c object sleef src libm cmakefiles sleefavx2128.dir sleefsimddp.c.o 12 building c object sleef src libm cmakefiles sleefavx2.dir sleefsimddp.c.o 12 building cxx object third party protobuf cmake cmakefiles libprotobuf.dir src google protobuf stubs structurally valid.cc.o 12 building c object sleef src libm cmakefiles sleefsse2.dir sleefsimddp.c.o 12 building cxx object third party ideep mkl dnn src cmakefiles mkldnn.dir cpu jit avx512 core u8s8s32x conv kernel.cpp.o 12 building cxx object third party ideep mkl dnn src cmakefiles mkldnn.dir cpu gemm convolution.cpp.o 12 built target sleeffma4 scanning dependencies target sleefsse4 12 building cxx object third party protobuf cmake cmakefiles libprotobuf.dir src google protobuf stubs strutil.cc.o 12 built target sleefavx2128 scanning dependencies target c10 utils gpu test 12 building c object sleef src libm cmakefiles sleefsse4.dir sleefsimdsp.c.o 12 linking cxx executable .. .. bin c10 utils gpu test 12 built target sleefavx2 scanning dependencies target c10 utils hip test 12 building cxx object caffe2 utils cmakefiles c10 utils hip test.dir dummy.cpp.o 12 linking cxx executable .. .. bin c10 utils hip test ptxas warning big maxrregcount value specified 96, ignored 12 built target c10 utils gpu test scanning dependencies target c10 utils cpu test 12 linking cxx executable .. .. bin c10 utils cpu test 12 built target c10 utils hip test scanning dependencies target qnnpack 12 building c object sleef src libm cmakefiles sleefsse4.dir sleefsimddp.c.o 12 building c object confu deps qnnpack cmakefiles qnnpack.dir src init.c.o 12 built target c10 utils cpu test 12 generating src x86 64 fma 2d fourier 8x8.py.o 12 building c object confu deps qnnpack cmakefiles qnnpack.dir src convolution.c.o 12 built target sleefsse2 scanning dependencies target c10 registry test 12 building cxx object c10 test cmakefiles c10 registry test.dir registry test.cpp.o 12 building c object confu deps qnnpack cmakefiles qnnpack.dir src deconvolution.c.o 12 building c object confu deps qnnpack cmakefiles qnnpack.dir src fully connected.c.o 12 building c object confu deps qnnpack cmakefiles qnnpack.dir src sgemm 6x8 psimd.c.o 12 building c object confu deps qnnpack cmakefiles qnnpack.dir src q8gemm 2x4c8 sse2.c.o nvcc warning 'compute 20', 'sm 20', 'sm 21' architectures deprecated, may removed future release use wno deprecated gpu targets suppress warning . 12 building c object confu deps qnnpack cmakefiles qnnpack.dir src q8gemm 4x4c2 sse2.c.o 12 building cxx object third party ideep mkl dnn src cmakefiles mkldnn.dir cpu jit uni pool kernel f32.cpp.o 12 building c object confu deps qnnpack cmakefiles qnnpack.dir src q8conv 4x4c2 sse2.c.o 12 built target sleefsse4 compiling misc nvmlwrap.cu home feng pytorch build nccl obj misc nvmlwrap.o scanning dependencies target c10 opschema test nvcc warning 'compute 20', 'sm 20', 'sm 21' architectures deprecated, may removed future release use wno deprecated gpu targets suppress warning . 12 building cxx object c10 test cmakefiles c10 opschema test.dir dispatch opschema test.cpp.o 12 building c object confu deps qnnpack cmakefiles qnnpack.dir src q8dw 9c8 sse2.c.o 12 building cxx object third party protobuf cmake cmakefiles libprotobuf.dir src google protobuf stubs time.cc.o 12 linking c static library .. .. lib libqnnpack.a 13 building cxx object third party protobuf cmake cmakefiles libprotobuf.dir src google protobuf wire format lite.cc.o 13 built target qnnpack scanning dependencies target c10 inlinestreamguard test 13 building cxx object c10 test cmakefiles c10 inlinestreamguard test.dir impl inlinestreamguard test.cpp.o 13 linking cxx executable .. .. bin c10 opschema test 13 built target c10 opschema test scanning dependencies target c10 streamguard test 13 building cxx object c10 test cmakefiles c10 streamguard test.dir streamguard test.cpp.o 13 building cxx object third party ideep mkl dnn src cmakefiles mkldnn.dir cpu gemm u8s8s32x convolution.cpp.o 13 building cxx object third party protobuf cmake cmakefiles libprotobuf.dir src google protobuf any.cc.o 13 linking cxx executable .. .. bin c10 registry test 13 built target c10 registry test scanning dependencies target c10 deviceguard test 13 linking cxx executable .. .. bin c10 streamguard test 13 building cxx object c10 test cmakefiles c10 deviceguard test.dir deviceguard test.cpp.o 13 built target c10 streamguard test scanning dependencies target c10 typetraits test 13 building cxx object c10 test cmakefiles c10 typetraits test.dir util typetraits test.cpp.o 13 building cxx object third party protobuf cmake cmakefiles libprotobuf.dir src google protobuf any.pb.cc.o ptxas warning big maxrregcount value specified 96, ignored 13 generating src x86 64 fma 2d fourier 16x16.py.o 13 built target aten cuda files gen target scanning dependencies target c10 metaprogramming test 13 building cxx object third party protobuf cmake cmakefiles libprotobuf.dir src google protobuf api.pb.cc.o 13 building cxx object c10 test cmakefiles c10 metaprogramming test.dir util metaprogramming test.cpp.o 13 building cxx object third party ideep mkl dnn src cmakefiles mkldnn.dir cpu jit avx512 core u8s8s32x 1x1 convolution.cpp.o 13 linking cxx executable .. .. bin c10 typetraits test 13 built target c10 typetraits test scanning dependencies target c10 logging test 13 building cxx object c10 test cmakefiles c10 logging test.dir logging test.cpp.o 13 building cxx object third party protobuf cmake cmakefiles libprotobuf.dir src google protobuf compiler importer.cc.o 13 building cxx object third party ideep mkl dnn src cmakefiles mkldnn.dir cpu jit avx2 gemm f32.cpp.o 14 linking cxx executable .. .. bin c10 inlinestreamguard test nvcc warning 'compute 20', 'sm 20', 'sm 21' architectures deprecated, may removed future release use wno deprecated gpu targets suppress warning . 14 built target c10 inlinestreamguard test scanning dependencies target c10 array test 14 building cxx object c10 test cmakefiles c10 array test.dir util array test.cpp.o 14 linking cxx executable .. .. bin c10 deviceguard test compiling misc ibvwrap.cu home feng pytorch build nccl obj misc ibvwrap.o nvcc warning 'compute 20', 'sm 20', 'sm 21' architectures deprecated, may removed future release use wno deprecated gpu targets suppress warning . 14 built target c10 deviceguard test scanning dependencies target c10 inlinedeviceguard test 14 building cxx object c10 test cmakefiles c10 inlinedeviceguard test.dir impl inlinedeviceguard test.cpp.o 14 linking cxx executable .. .. bin c10 metaprogramming test 14 building cxx object third party protobuf cmake cmakefiles libprotobuf.dir src google protobuf compiler parser.cc.o 14 built target c10 metaprogramming test scanning dependencies target c10 typeid test 14 linking cxx executable .. .. bin c10 array test 14 building cxx object c10 test cmakefiles c10 typeid test.dir util typeid test.cpp.o 14 built target c10 array test scanning dependencies target c10 flags test 14 building cxx object c10 test cmakefiles c10 flags test.dir flags test.cpp.o 14 building cxx object third party ideep mkl dnn src cmakefiles mkldnn.dir cpu jit uni pooling.cpp.o 14 building cxx object third party protobuf cmake cmakefiles libprotobuf.dir src google protobuf descriptor.cc.o 14 linking cxx executable .. .. bin c10 logging test 14 built target c10 logging test scanning dependencies target c10 typelist test skipping th multinomial arg generator generator skipping th normal arg generator generator skipping th normal arg generator generator skipping th normal arg generator generator skipping th tensor arg storage storage skipping th tensor arg storage storage skipping rrelu noise arg generator generator skipping rrelu noise forward arg generator generator 14 building cxx object c10 test cmakefiles c10 typelist test.dir util typelist test.cpp.o skipping thnn conv transpose2d backward arg std array std array skipping thnn conv transpose3d backward arg std array std array skipping thnn conv2d backward arg std array std array skipping thnn conv depthwise2d backward arg std array std array skipping thnn conv3d backward arg std array std array skipping thnn conv dilated2d backward arg std array std array skipping thnn conv dilated3d backward arg std array std array skipping cudnn rnn backward arg std array std array skipping cudnn init dropout state factory method skipping fused dropout arg generator generator skipping arange factory method skipping bartlett window factory method skipping bernoulli arg generator generator skipping bernoulli arg generator generator skipping blackman window factory method skipping clamp arg c10 optional scalar skipping clamp arg c10 optional scalar skipping convolution double backward arg std array std array skipping cudnn convolution backward arg std array std array skipping cudnn convolution transpose backward arg std array std array skipping cumsum arg scalartype scalartype skipping cumprod arg scalartype scalartype skipping einsum arg std string std string skipping empty factory method skipping empty like factory method skipping empty strided factory method skipping eye factory method skipping full factory method skipping full like factory method skipping hann window factory method skipping hamming window factory method skipping cufft set plan cache max size ret void void skipping cufft clear plan cache ret void void skipping linspace factory method skipping logspace factory method skipping log softmax arg scalartype scalartype skipping mean arg scalartype scalartype skipping mean arg scalartype scalartype skipping mean arg scalartype scalartype skipping mkldnn convolution backward arg std array std array skipping miopen convolution backward arg std array std array skipping miopen convolution transpose backward arg std array std array skipping native batch norm backward arg std array std array skipping ones factory method skipping ones like factory method skipping rand factory method skipping rand like factory method skipping randint factory method skipping randint like factory method skipping randn factory method skipping randn like factory method skipping randperm factory method skipping range factory method skipping rrelu arg generator generator skipping softmax arg scalartype scalartype skipping sum arg scalartype scalartype skipping sum arg scalartype scalartype skipping sum arg scalartype scalartype skipping prod arg scalartype scalartype skipping prod arg scalartype scalartype skipping prod arg scalartype scalartype skipping zeros factory method skipping zeros like factory method skipping standard gamma arg generator generator skipping poisson arg generator generator skipping sparse coo tensor factory method skipping sparse coo tensor unsafe factory method skipping sparse coo tensor dims factory method skipping sparse coo tensor dims tensors factory method skipping sparse mask arg sparsetensorref sparsetensorref skipping factory method skipping data ptr ret void void skipping multinomial arg generator generator skipping normal arg generator generator skipping normal arg generator generator skipping normal arg generator generator ptxas warning big maxrregcount value specified 96, ignored 14 built target aten op header gen scanning dependencies target sleefavx 14 building c object sleef src libm cmakefiles sleefavx.dir sleefsimdsp.c.o 14 linking cxx executable .. .. bin c10 inlinedeviceguard test 14 linking cxx executable .. .. bin c10 flags test 14 built target c10 flags test 14 built target c10 inlinedeviceguard test scanning dependencies target dispsse obj scanning dependencies target dispavx obj 14 building c object sleef src libm cmakefiles dispsse obj.dir dispsse.c.o 14 building c object sleef src libm cmakefiles dispavx obj.dir dispavx.c.o 14 linking cxx executable .. .. bin c10 typelist test nvcc warning 'compute 20', 'sm 20', 'sm 21' architectures deprecated, may removed future release use wno deprecated gpu targets suppress warning . 14 built target c10 typelist test 15 building c object sleef src libm cmakefiles sleefavx.dir sleefsimddp.c.o 15 linking cxx executable .. .. bin c10 typeid test 15 building cxx object third party ideep mkl dnn src cmakefiles mkldnn.dir cpu jit uni lrn kernel f32.cpp.o 15 built target c10 typeid test compiling misc rings.cu home feng pytorch build nccl obj misc rings.o 15 building cxx object third party ideep mkl dnn src cmakefiles mkldnn.dir cpu cpu concat.cpp.o nvcc warning 'compute 20', 'sm 20', 'sm 21' architectures deprecated, may removed future release use wno deprecated gpu targets suppress warning . 15 built target dispsse obj 15 built target dispavx obj 15 building cxx object third party ideep mkl dnn src cmakefiles mkldnn.dir cpu jit avx512 core u8s8s32x convolution.cpp.o 15 building cxx object third party protobuf cmake cmakefiles libprotobuf.dir src google protobuf descriptor.pb.cc.o 15 building cxx object third party protobuf cmake cmakefiles libprotobuf.dir src google protobuf descriptor database.cc.o 15 building cxx object third party ideep mkl dnn src cmakefiles mkldnn.dir cpu jit avx512 core i8i8 pooling.cpp.o 15 built target sleefavx scanning dependencies target sleef 15 building c object sleef src libm cmakefiles sleef.dir sleefdp.c.o 15 building cxx object third party ideep mkl dnn src cmakefiles mkldnn.dir cpu nchw pooling.cpp.o ptxas warning big maxrregcount value specified 96, ignored 16 building cxx object third party ideep mkl dnn src cmakefiles mkldnn.dir cpu cpu reducer.cpp.o 16 building c object sleef src libm cmakefiles sleef.dir sleefsp.c.o 16 building cxx object third party ideep mkl dnn src cmakefiles mkldnn.dir cpu jit avx512 common gemm f32.cpp.o 16 building cxx object third party protobuf cmake cmakefiles libprotobuf.dir src google protobuf duration.pb.cc.o nvcc warning 'compute 20', 'sm 20', 'sm 21' architectures deprecated, may removed future release use wno deprecated gpu targets suppress warning . 16 building cxx object third party ideep mkl dnn src cmakefiles mkldnn.dir cpu jit uni dw conv kernel f32.cpp.o compiling misc utils.cu home feng pytorch build nccl obj misc utils.o nvcc warning 'compute 20', 'sm 20', 'sm 21' architectures deprecated, may removed future release use wno deprecated gpu targets suppress warning . 16 building cxx object third party ideep mkl dnn src cmakefiles mkldnn.dir cpu jit uni reorder.cpp.o 16 building cxx object third party ideep mkl dnn src cmakefiles mkldnn.dir cpu cpu engine.cpp.o 16 building c object sleef src libm cmakefiles sleef.dir sleefld.c.o 16 building c object sleef src libm cmakefiles sleef.dir sleefqp.c.o 16 building cxx object third party protobuf cmake cmakefiles libprotobuf.dir src google protobuf dynamic message.cc.o 16 linking c static library .. .. lib libsleef.a 16 built target sleef 16 building cxx object third party protobuf cmake cmakefiles libprotobuf.dir src google protobuf empty.pb.cc.o ptxas warning big maxrregcount value specified 96, ignored nvcc warning 'compute 20', 'sm 20', 'sm 21' architectures deprecated, may removed future release use wno deprecated gpu targets suppress warning . 16 building cxx object third party ideep mkl dnn src cmakefiles mkldnn.dir cpu jit avx512 core convolution winograd.cpp.o compiling misc enqueue.cu home feng pytorch build nccl obj misc enqueue.o nvcc warning 'compute 20', 'sm 20', 'sm 21' architectures deprecated, may removed future release use wno deprecated gpu targets suppress warning . 16 building cxx object third party protobuf cmake cmakefiles libprotobuf.dir src google protobuf extension set heavy.cc.o 16 building cxx object third party ideep mkl dnn src cmakefiles mkldnn.dir cpu jit avx512 common lrn.cpp.o 16 building cxx object third party protobuf cmake cmakefiles libprotobuf.dir src google protobuf field mask.pb.cc.o 16 generating src x86 64 fma 2d winograd 8x8 3x3.py.o 16 building cxx object third party protobuf cmake cmakefiles libprotobuf.dir src google protobuf generated message reflection.cc.o ptxas warning big maxrregcount value specified 96, ignored 16 building cxx object third party protobuf cmake cmakefiles libprotobuf.dir src google protobuf generated message table driven.cc.o 16 building cxx object third party protobuf cmake cmakefiles libprotobuf.dir src google protobuf io gzip stream.cc.o 16 building cxx object third party ideep mkl dnn src cmakefiles mkldnn.dir cpu jit sse42 convolution.cpp.o 16 building cxx object third party protobuf cmake cmakefiles libprotobuf.dir src google protobuf io printer.cc.o 16 building cxx object third party ideep mkl dnn src cmakefiles mkldnn.dir cpu jit avx512 common convolution winograd.cpp.o 16 building cxx object third party protobuf cmake cmakefiles libprotobuf.dir src google protobuf io strtod.cc.o 16 building cxx object third party ideep mkl dnn src cmakefiles mkldnn.dir cpu jit avx2 1x1 convolution.cpp.o 16 generating src x86 64 fma blas s8gemm.py.o nvcc warning 'compute 20', 'sm 20', 'sm 21' architectures deprecated, may removed future release use wno deprecated gpu targets suppress warning . 16 building cxx object third party protobuf cmake cmakefiles libprotobuf.dir src google protobuf io tokenizer.cc.o compiling transport p2p.cu home feng pytorch build nccl obj transport p2p.o nvcc warning 'compute 20', 'sm 20', 'sm 21' architectures deprecated, may removed future release use wno deprecated gpu targets suppress warning . 16 generating src x86 64 fma blas c8gemm.py.o 16 building cxx object third party ideep mkl dnn src cmakefiles mkldnn.dir cpu jit uni eltwise.cpp.o 16 building cxx object third party protobuf cmake cmakefiles libprotobuf.dir src google protobuf io zero copy stream impl.cc.o 16 generating src x86 64 fma blas s4c6gemm.py.o 16 building cxx object third party protobuf cmake cmakefiles libprotobuf.dir src google protobuf map field.cc.o 17 building cxx object third party protobuf cmake cmakefiles libprotobuf.dir src google protobuf message.cc.o 17 building cxx object third party protobuf cmake cmakefiles libprotobuf.dir src google protobuf reflection ops.cc.o 17 building cxx object third party protobuf cmake cmakefiles libprotobuf.dir src google protobuf service.cc.o 17 generating src x86 64 fma blas conv1x1.py.o 17 building cxx object third party ideep mkl dnn src cmakefiles mkldnn.dir cpu ref pooling.cpp.o 17 building cxx object third party ideep mkl dnn src cmakefiles mkldnn.dir cpu jit avx2 conv kernel f32.cpp.o ptxas warning big maxrregcount value specified 96, ignored 17 generating src x86 64 fma blas sgemm.py.o 17 building cxx object third party protobuf cmake cmakefiles libprotobuf.dir src google protobuf source context.pb.cc.o 17 generating src x86 64 fma max pooling.py.o 17 building cxx object third party protobuf cmake cmakefiles libprotobuf.dir src google protobuf struct.pb.cc.o 17 generating src x86 64 fma relu.py.o 17 building cxx object third party ideep mkl dnn src cmakefiles mkldnn.dir cpu jit uni dw convolution.cpp.o 17 generating src x86 64 fma softmax.py.o 17 building cxx object third party protobuf cmake cmakefiles libprotobuf.dir src google protobuf stubs mathlimits.cc.o nvcc warning 'compute 20', 'sm 20', 'sm 21' architectures deprecated, may removed future release use wno deprecated gpu targets suppress warning . 17 building cxx object third party protobuf cmake cmakefiles libprotobuf.dir src google protobuf stubs substitute.cc.o 17 building cxx object third party protobuf cmake cmakefiles libprotobuf.dir src google protobuf text format.cc.o compiling transport shm.cu home feng pytorch build nccl obj transport shm.o nvcc warning 'compute 20', 'sm 20', 'sm 21' architectures deprecated, may removed future release use wno deprecated gpu targets suppress warning . 17 building cxx object third party protobuf cmake cmakefiles libprotobuf.dir src google protobuf timestamp.pb.cc.o 17 building cxx object third party ideep mkl dnn src cmakefiles mkldnn.dir cpu ref batch normalization.cpp.o 17 generating src x86 64 fma blas sdotxf.py.o 17 building cxx object third party ideep mkl dnn src cmakefiles mkldnn.dir cpu jit uni inner product.cpp.o 17 building cxx object third party protobuf cmake cmakefiles libprotobuf.dir src google protobuf type.pb.cc.o 17 building cxx object third party ideep mkl dnn src cmakefiles mkldnn.dir cpu jit uni reorder utils.cpp.o 17 generating src x86 64 fma blas shdotxf.py.o 17 building cxx object third party protobuf cmake cmakefiles libprotobuf.dir src google protobuf unknown field set.cc.o 17 building cxx object third party protobuf cmake cmakefiles libprotobuf.dir src google protobuf util delimited message util.cc.o 17 building cxx object third party ideep mkl dnn src cmakefiles mkldnn.dir cpu jit avx512 core u8s8s32x 1x1 conv kernel.cpp.o 17 building cxx object third party protobuf cmake cmakefiles libprotobuf.dir src google protobuf util field comparator.cc.o ptxas warning big maxrregcount value specified 96, ignored 17 building cxx object third party protobuf cmake cmakefiles libprotobuf.dir src google protobuf util field mask util.cc.o 17 building cxx object third party protobuf cmake cmakefiles libprotobuf.dir src google protobuf util internal datapiece.cc.o 17 building cxx object third party ideep mkl dnn src cmakefiles mkldnn.dir cpu jit uni batch normalization.cpp.o 17 building cxx object third party protobuf cmake cmakefiles libprotobuf.dir src google protobuf util internal default value objectwriter.cc.o 17 building cxx object third party ideep mkl dnn src cmakefiles mkldnn.dir cpu jit sse42 1x1 convolution.cpp.o 17 building cxx object third party protobuf cmake cmakefiles libprotobuf.dir src google protobuf util internal error listener.cc.o nvcc warning 'compute 20', 'sm 20', 'sm 21' architectures deprecated, may removed future release use wno deprecated gpu targets suppress warning . 17 building cxx object third party protobuf cmake cmakefiles libprotobuf.dir src google protobuf util internal field mask utility.cc.o compiling transport net.cu home feng pytorch build nccl obj transport net.o nvcc warning 'compute 20', 'sm 20', 'sm 21' architectures deprecated, may removed future release use wno deprecated gpu targets suppress warning . 17 building cxx object third party ideep mkl dnn src cmakefiles mkldnn.dir cpu jit avx512 core u8s8s32x wino convolution.cpp.o 17 building cxx object third party protobuf cmake cmakefiles libprotobuf.dir src google protobuf util internal json escaping.cc.o 17 building cxx object third party protobuf cmake cmakefiles libprotobuf.dir src google protobuf util internal json objectwriter.cc.o 17 building cxx object third party protobuf cmake cmakefiles libprotobuf.dir src google protobuf util internal json stream parser.cc.o 18 building cxx object third party protobuf cmake cmakefiles libprotobuf.dir src google protobuf util internal object writer.cc.o 19 building cxx object third party ideep mkl dnn src cmakefiles mkldnn.dir cpu jit avx2 convolution.cpp.o 19 building cxx object third party ideep mkl dnn src cmakefiles mkldnn.dir cpu jit avx512 common 1x1 convolution.cpp.o scanning dependencies target nnpack 19 building cxx object third party protobuf cmake cmakefiles libprotobuf.dir src google protobuf util internal proto writer.cc.o 19 building c object confu deps nnpack cmakefiles nnpack.dir src init.c.o 19 building c object confu deps nnpack cmakefiles nnpack.dir src convolution inference.c.o 19 building cxx object third party protobuf cmake cmakefiles libprotobuf.dir src google protobuf util internal protostream objectsource.cc.o 19 building c object confu deps nnpack cmakefiles nnpack.dir src fully connected inference.c.o 20 building c object confu deps nnpack cmakefiles nnpack.dir src pooling output.c.o 20 building c object confu deps nnpack cmakefiles nnpack.dir src relu output.c.o 20 building cxx object third party protobuf cmake cmakefiles libprotobuf.dir src google protobuf util internal protostream objectwriter.cc.o 20 building cxx object third party protobuf cmake cmakefiles libprotobuf.dir src google protobuf util internal type info.cc.o 20 building c object confu deps nnpack cmakefiles nnpack.dir src softmax output.c.o 20 building c object confu deps nnpack cmakefiles nnpack.dir src fully connected output.c.o 20 building c object confu deps nnpack cmakefiles nnpack.dir src relu input gradient.c.o ptxas warning big maxrregcount value specified 96, ignored 20 building c object confu deps nnpack cmakefiles nnpack.dir src convolution input gradient.c.o 20 building cxx object third party protobuf cmake cmakefiles libprotobuf.dir src google protobuf util internal type info test helper.cc.o 20 building c object confu deps nnpack cmakefiles nnpack.dir src convolution kernel gradient.c.o 20 building c object confu deps nnpack cmakefiles nnpack.dir src convolution output.c.o 20 building c object confu deps nnpack cmakefiles nnpack.dir src x86 64 fma softmax.c.o 20 linking c static library .. .. lib libnnpack.a 20 built target nnpack 20 building cxx object third party protobuf cmake cmakefiles libprotobuf.dir src google protobuf util internal utility.cc.o 20 building cxx object third party ideep mkl dnn src cmakefiles mkldnn.dir cpu cpu reorder.cpp.o 20 building cxx object third party protobuf cmake cmakefiles libprotobuf.dir src google protobuf util json util.cc.o 20 building cxx object third party protobuf cmake cmakefiles libprotobuf.dir src google protobuf util message differencer.cc.o 20 building cxx object third party protobuf cmake cmakefiles libprotobuf.dir src google protobuf util time util.cc.o nvcc warning 'compute 20', 'sm 20', 'sm 21' architectures deprecated, may removed future release use wno deprecated gpu targets suppress warning . 20 building cxx object third party protobuf cmake cmakefiles libprotobuf.dir src google protobuf util type resolver util.cc.o compiling transport net socket.cu home feng pytorch build nccl obj transport net socket.o nvcc warning 'compute 20', 'sm 20', 'sm 21' architectures deprecated, may removed future release use wno deprecated gpu targets suppress warning . 20 building cxx object third party protobuf cmake cmakefiles libprotobuf.dir src google protobuf wire format.cc.o 20 building cxx object third party protobuf cmake cmakefiles libprotobuf.dir src google protobuf wrappers.pb.cc.o 20 building cxx object third party ideep mkl dnn src cmakefiles mkldnn.dir cpu jit avx2 1x1 conv kernel f32.cpp.o 20 building cxx object third party ideep mkl dnn src cmakefiles mkldnn.dir cpu jit avx512 common conv kernel.cpp.o 20 building cxx object third party ideep mkl dnn src cmakefiles mkldnn.dir cpu gemm convolution utils.cpp.o ptxas warning big maxrregcount value specified 96, ignored 20 building cxx object third party ideep mkl dnn src cmakefiles mkldnn.dir cpu jit uni lrn.cpp.o 20 building cxx object third party ideep mkl dnn src cmakefiles mkldnn.dir cpu ref softmax.cpp.o 20 building cxx object third party ideep mkl dnn src cmakefiles mkldnn.dir cpu jit transpose src utils.cpp.o 20 building cxx object third party ideep mkl dnn src cmakefiles mkldnn.dir cpu ref rnn.cpp.o 20 building cxx object third party ideep mkl dnn src cmakefiles mkldnn.dir cpu ref eltwise.cpp.o 20 building cxx object third party ideep mkl dnn src cmakefiles mkldnn.dir cpu ref convolution.cpp.o 20 building cxx object third party ideep mkl dnn src cmakefiles mkldnn.dir cpu nspc batch normalization.cpp.o 20 linking cxx static library .. .. .. lib libprotobuf.a 20 built target libprotobuf 20 generating .. .. .. .. third party protobuf src google protobuf compiler js well known types embed.cc scanning dependencies target libprotoc 20 building cxx object third party ideep mkl dnn src cmakefiles mkldnn.dir cpu jit avx512 core conv winograd kernel f32.cpp.o nvcc warning 'compute 20', 'sm 20', 'sm 21' architectures deprecated, may removed future release use wno deprecated gpu targets suppress warning . 20 building cxx object third party protobuf cmake cmakefiles libprotoc.dir src google protobuf compiler code generator.cc.o 20 building cxx object third party protobuf cmake cmakefiles libprotoc.dir src google protobuf compiler command line interface.cc.o compiling transport net ib.cu home feng pytorch build nccl obj transport net ib.o nvcc warning 'compute 20', 'sm 20', 'sm 21' architectures deprecated, may removed future release use wno deprecated gpu targets suppress warning . 20 building cxx object third party protobuf cmake cmakefiles libprotoc.dir src google protobuf compiler cpp cpp enum.cc.o 21 building cxx object third party protobuf cmake cmakefiles libprotoc.dir src google protobuf compiler cpp cpp enum field.cc.o 21 building cxx object third party protobuf cmake cmakefiles libprotoc.dir src google protobuf compiler cpp cpp extension.cc.o 21 building cxx object third party protobuf cmake cmakefiles libprotoc.dir src google protobuf compiler cpp cpp field.cc.o 21 building cxx object third party protobuf cmake cmakefiles libprotoc.dir src google protobuf compiler cpp cpp file.cc.o ptxas warning big maxrregcount value specified 96, ignored 21 building cxx object third party protobuf cmake cmakefiles libprotoc.dir src google protobuf compiler cpp cpp generator.cc.o 21 building cxx object third party protobuf cmake cmakefiles libprotoc.dir src google protobuf compiler cpp cpp helpers.cc.o 21 building cxx object third party protobuf cmake cmakefiles libprotoc.dir src google protobuf compiler cpp cpp map field.cc.o 21 building cxx object third party protobuf cmake cmakefiles libprotoc.dir src google protobuf compiler cpp cpp message.cc.o 21 building cxx object third party protobuf cmake cmakefiles libprotoc.dir src google protobuf compiler cpp cpp message field.cc.o 21 building cxx object third party protobuf cmake cmakefiles libprotoc.dir src google protobuf compiler cpp cpp padding optimizer.cc.o 21 building cxx object third party protobuf cmake cmakefiles libprotoc.dir src google protobuf compiler cpp cpp primitive field.cc.o 21 building cxx object third party protobuf cmake cmakefiles libprotoc.dir src google protobuf compiler cpp cpp service.cc.o 21 building cxx object third party protobuf cmake cmakefiles libprotoc.dir src google protobuf compiler cpp cpp string field.cc.o nvcc warning 'compute 20', 'sm 20', 'sm 21' architectures deprecated, may removed future release use wno deprecated gpu targets suppress warning . 21 building cxx object third party protobuf cmake cmakefiles libprotoc.dir src google protobuf compiler csharp csharp doc comment.cc.o compiling collectives reduce.cu home feng pytorch build nccl obj collectives reduce.o nvcc warning 'compute 20', 'sm 20', 'sm 21' architectures deprecated, may removed future release use wno deprecated gpu targets suppress warning . 21 building cxx object third party protobuf cmake cmakefiles libprotoc.dir src google protobuf compiler csharp csharp enum.cc.o 21 building cxx object third party protobuf cmake cmakefiles libprotoc.dir src google protobuf compiler csharp csharp enum field.cc.o 21 building cxx object third party protobuf cmake cmakefiles libprotoc.dir src google protobuf compiler csharp csharp field base.cc.o 21 building cxx object third party protobuf cmake cmakefiles libprotoc.dir src google protobuf compiler csharp csharp generator.cc.o 21 building cxx object third party protobuf cmake cmakefiles libprotoc.dir src google protobuf compiler csharp csharp helpers.cc.o 21 building cxx object third party protobuf cmake cmakefiles libprotoc.dir src google protobuf compiler csharp csharp map field.cc.o 21 building cxx object third party protobuf cmake cmakefiles libprotoc.dir src google protobuf compiler csharp csharp message.cc.o 22 building cxx object third party protobuf cmake cmakefiles libprotoc.dir src google protobuf compiler csharp csharp message field.cc.o 22 building cxx object third party protobuf cmake cmakefiles libprotoc.dir src google protobuf compiler csharp csharp primitive field.cc.o 22 building cxx object third party protobuf cmake cmakefiles libprotoc.dir src google protobuf compiler csharp csharp reflection class.cc.o 22 building cxx object third party protobuf cmake cmakefiles libprotoc.dir src google protobuf compiler csharp csharp repeated enum field.cc.o 22 building cxx object third party protobuf cmake cmakefiles libprotoc.dir src google protobuf compiler csharp csharp repeated message field.cc.o ptxas warning big maxrregcount value specified 96, ignored 22 building cxx object third party protobuf cmake cmakefiles libprotoc.dir src google protobuf compiler csharp csharp repeated primitive field.cc.o 22 building cxx object third party protobuf cmake cmakefiles libprotoc.dir src google protobuf compiler csharp csharp source generator base.cc.o 22 building cxx object third party protobuf cmake cmakefiles libprotoc.dir src google protobuf compiler csharp csharp wrapper field.cc.o 22 building cxx object third party protobuf cmake cmakefiles libprotoc.dir src google protobuf compiler java java context.cc.o 22 building cxx object third party protobuf cmake cmakefiles libprotoc.dir src google protobuf compiler java java doc comment.cc.o 22 building cxx object third party protobuf cmake cmakefiles libprotoc.dir src google protobuf compiler java java enum.cc.o 22 building cxx object third party protobuf cmake cmakefiles libprotoc.dir src google protobuf compiler java java enum field.cc.o nvcc warning 'compute 20', 'sm 20', 'sm 21' architectures deprecated, may removed future release use wno deprecated gpu targets suppress warning . 22 building cxx object third party protobuf cmake cmakefiles libprotoc.dir src google protobuf compiler java java enum field lite.cc.o 22 building cxx object third party protobuf cmake cmakefiles libprotoc.dir src google protobuf compiler java java enum lite.cc.o 22 building cxx object third party protobuf cmake cmakefiles libprotoc.dir src google protobuf compiler java java extension.cc.o 22 building cxx object third party protobuf cmake cmakefiles libprotoc.dir src google protobuf compiler java java extension lite.cc.o 22 building cxx object third party protobuf cmake cmakefiles libprotoc.dir src google protobuf compiler java java field.cc.o compiling collectives gather.cu home feng pytorch build nccl obj collectives gather.o nvcc warning 'compute 20', 'sm 20', 'sm 21' architectures deprecated, may removed future release use wno deprecated gpu targets suppress warning . 22 building cxx object third party protobuf cmake cmakefiles libprotoc.dir src google protobuf compiler java java file.cc.o 22 building cxx object third party protobuf cmake cmakefiles libprotoc.dir src google protobuf compiler java java generator.cc.o 22 building cxx object third party protobuf cmake cmakefiles libprotoc.dir src google protobuf compiler java java generator factory.cc.o 22 building cxx object third party protobuf cmake cmakefiles libprotoc.dir src google protobuf compiler java java helpers.cc.o 22 building cxx object third party protobuf cmake cmakefiles libprotoc.dir src google protobuf compiler java java lazy message field.cc.o 23 building cxx object third party protobuf cmake cmakefiles libprotoc.dir src google protobuf compiler java java lazy message field lite.cc.o 23 building cxx object third party protobuf cmake cmakefiles libprotoc.dir src google protobuf compiler java java map field.cc.o 23 building cxx object third party protobuf cmake cmakefiles libprotoc.dir src google protobuf compiler java java map field lite.cc.o 23 building cxx object third party protobuf cmake cmakefiles libprotoc.dir src google protobuf compiler java java message.cc.o 23 building cxx object third party protobuf cmake cmakefiles libprotoc.dir src google protobuf compiler java java message builder.cc.o 23 building cxx object third party protobuf cmake cmakefiles libprotoc.dir src google protobuf compiler java java message builder lite.cc.o 23 building cxx object third party protobuf cmake cmakefiles libprotoc.dir src google protobuf compiler java java message field.cc.o ptxas warning big maxrregcount value specified 96, ignored 23 building cxx object third party protobuf cmake cmakefiles libprotoc.dir src google protobuf compiler java java message field lite.cc.o 23 building cxx object third party protobuf cmake cmakefiles libprotoc.dir src google protobuf compiler java java message lite.cc.o 23 building cxx object third party protobuf cmake cmakefiles libprotoc.dir src google protobuf compiler java java name resolver.cc.o nvcc warning 'compute 20', 'sm 20', 'sm 21' architectures deprecated, may removed future release use wno deprecated gpu targets suppress warning . 23 building cxx object third party protobuf cmake cmakefiles libprotoc.dir src google protobuf compiler java java primitive field.cc.o 23 building cxx object third party protobuf cmake cmakefiles libprotoc.dir src google protobuf compiler java java primitive field lite.cc.o 23 building cxx object third party protobuf cmake cmakefiles libprotoc.dir src google protobuf compiler java java service.cc.o compiling collectives broadcast.cu home feng pytorch build nccl obj collectives broadcast.o nvcc warning 'compute 20', 'sm 20', 'sm 21' architectures deprecated, may removed future release use wno deprecated gpu targets suppress warning . 23 building cxx object third party protobuf cmake cmakefiles libprotoc.dir src google protobuf compiler java java shared code generator.cc.o 23 building cxx object third party protobuf cmake cmakefiles libprotoc.dir src google protobuf compiler java java string field.cc.o 23 building cxx object third party protobuf cmake cmakefiles libprotoc.dir src google protobuf compiler java java string field lite.cc.o 23 building cxx object third party protobuf cmake cmakefiles libprotoc.dir src google protobuf compiler javanano javanano enum.cc.o 23 building cxx object third party protobuf cmake cmakefiles libprotoc.dir src google protobuf compiler javanano javanano enum field.cc.o 23 building cxx object third party protobuf cmake cmakefiles libprotoc.dir src google protobuf compiler javanano javanano extension.cc.o 23 building cxx object third party protobuf cmake cmakefiles libprotoc.dir src google protobuf compiler javanano javanano field.cc.o 23 building cxx object third party protobuf cmake cmakefiles libprotoc.dir src google protobuf compiler javanano javanano file.cc.o 24 building cxx object third party protobuf cmake cmakefiles libprotoc.dir src google protobuf compiler javanano javanano generator.cc.o 24 building cxx object third party protobuf cmake cmakefiles libprotoc.dir src google protobuf compiler javanano javanano helpers.cc.o 24 building cxx object third party protobuf cmake cmakefiles libprotoc.dir src google protobuf compiler javanano javanano map field.cc.o 24 building cxx object third party protobuf cmake cmakefiles libprotoc.dir src google protobuf compiler javanano javanano message.cc.o 24 building cxx object third party protobuf cmake cmakefiles libprotoc.dir src google protobuf compiler javanano javanano message field.cc.o ptxas warning big maxrregcount value specified 96, ignored 24 building cxx object third party protobuf cmake cmakefiles libprotoc.dir src google protobuf compiler javanano javanano primitive field.cc.o 24 building cxx object third party protobuf cmake cmakefiles libprotoc.dir src google protobuf compiler js js generator.cc.o 24 building cxx object third party protobuf cmake cmakefiles libprotoc.dir src google protobuf compiler js well known types embed.cc.o nvcc warning 'compute 20', 'sm 20', 'sm 21' architectures deprecated, may removed future release use wno deprecated gpu targets suppress warning . 24 building cxx object third party protobuf cmake cmakefiles libprotoc.dir src google protobuf compiler objectivec objectivec enum.cc.o 24 building cxx object third party protobuf cmake cmakefiles libprotoc.dir src google protobuf compiler objectivec objectivec enum field.cc.o 24 building cxx object third party protobuf cmake cmakefiles libprotoc.dir src google protobuf compiler objectivec objectivec extension.cc.o compiling collectives reduce.cu home feng pytorch build nccl obj collectives reduce.o nvcc warning 'compute 20', 'sm 20', 'sm 21' architectures deprecated, may removed future release use wno deprecated gpu targets suppress warning . 24 building cxx object third party protobuf cmake cmakefiles libprotoc.dir src google protobuf compiler objectivec objectivec field.cc.o 24 building cxx object third party protobuf cmake cmakefiles libprotoc.dir src google protobuf compiler objectivec objectivec file.cc.o 24 building cxx object third party protobuf cmake cmakefiles libprotoc.dir src google protobuf compiler objectivec objectivec generator.cc.o 24 building cxx object third party protobuf cmake cmakefiles libprotoc.dir src google protobuf compiler objectivec objectivec helpers.cc.o 24 building cxx object third party protobuf cmake cmakefiles libprotoc.dir src google protobuf compiler objectivec objectivec map field.cc.o 24 building cxx object third party protobuf cmake cmakefiles libprotoc.dir src google protobuf compiler objectivec objectivec message.cc.o 24 building cxx object third party protobuf cmake cmakefiles libprotoc.dir src google protobuf compiler objectivec objectivec message field.cc.o 24 building cxx object third party protobuf cmake cmakefiles libprotoc.dir src google protobuf compiler objectivec objectivec oneof.cc.o 24 building cxx object third party protobuf cmake cmakefiles libprotoc.dir src google protobuf compiler objectivec objectivec primitive field.cc.o 24 building cxx object third party protobuf cmake cmakefiles libprotoc.dir src google protobuf compiler php php generator.cc.o ptxas warning big maxrregcount value specified 96, ignored 24 building cxx object third party protobuf cmake cmakefiles libprotoc.dir src google protobuf compiler plugin.cc.o 25 building cxx object third party protobuf cmake cmakefiles libprotoc.dir src google protobuf compiler plugin.pb.cc.o 25 building cxx object third party protobuf cmake cmakefiles libprotoc.dir src google protobuf compiler python python generator.cc.o 25 building cxx object third party protobuf cmake cmakefiles libprotoc.dir src google protobuf compiler ruby ruby generator.cc.o 25 building cxx object third party protobuf cmake cmakefiles libprotoc.dir src google protobuf compiler subprocess.cc.o nvcc warning 'compute 20', 'sm 20', 'sm 21' architectures deprecated, may removed future release use wno deprecated gpu targets suppress warning . 25 building cxx object third party protobuf cmake cmakefiles libprotoc.dir src google protobuf compiler zip writer.cc.o compiling collectives reduce scatter.cu home feng pytorch build nccl obj collectives reduce scatter.o nvcc warning 'compute 20', 'sm 20', 'sm 21' architectures deprecated, may removed future release use wno deprecated gpu targets suppress warning . 25 linking cxx shared library .. .. .. .. lib libmkldnn.so 25 built target mkldnn ptxas warning big maxrregcount value specified 96, ignored 25 linking cxx static library .. .. .. lib libprotoc.a 25 built target libprotoc scanning dependencies target protoc 25 building cxx object third party protobuf cmake cmakefiles protoc.dir src google protobuf compiler main.cc.o nvcc warning 'compute 20', 'sm 20', 'sm 21' architectures deprecated, may removed future release use wno deprecated gpu targets suppress warning . nvcc warning 'compute 20', 'sm 20', 'sm 21' architectures deprecated, may removed future release use wno deprecated gpu targets suppress warning . 25 linking cxx executable .. .. .. bin protoc 25 built target protoc scanning dependencies target gen onnx proto 25 running c python protocol buffer compiler home feng pytorch caffe2 proto prof dag.proto 25 running c python protocol buffer compiler home feng pytorch caffe2 proto predictor consts.proto 25 running c python protocol buffer compiler home feng pytorch caffe2 proto torch.proto 25 running c python protocol buffer compiler home feng pytorch caffe2 proto caffe2.proto 25 running c python protocol buffer compiler home feng pytorch caffe2 proto hsm.proto 25 running c python protocol buffer compiler home feng pytorch caffe2 proto metanet.proto 25 running c python protocol buffer compiler home feng pytorch caffe2 proto caffe2 legacy.proto 25 running gen proto.py onnx onnx.in.proto processing home feng pytorch third party onnx onnx onnx.in.proto writing home feng pytorch build third party onnx onnx onnx onnx torch.proto compiling reduce.cu home feng pytorch build nccl obj collectives device reduce sum.o nvcc warning 'compute 20', 'sm 20', 'sm 21' architectures deprecated, may removed future release use wno deprecated gpu targets suppress warning . writing home feng pytorch build third party onnx onnx onnx onnx torch.proto3 writing home feng pytorch build third party onnx onnx onnx.pb.h generating home feng pytorch build third party onnx onnx onnx pb.py 25 running c protocol buffer compiler home feng pytorch build third party onnx onnx onnx onnx torch.proto scanning dependencies target caffe2 proto 25 building cxx object caffe2 proto cmakefiles caffe2 proto.dir caffe2 legacy.pb.cc.o 25 building cxx object caffe2 proto cmakefiles caffe2 proto.dir caffe2.pb.cc.o 25 building cxx object caffe2 proto cmakefiles caffe2 proto.dir predictor consts.pb.cc.o 25 building cxx object caffe2 proto cmakefiles caffe2 proto.dir prof dag.pb.cc.o 25 building cxx object caffe2 proto cmakefiles caffe2 proto.dir hsm.pb.cc.o 25 building cxx object caffe2 proto cmakefiles caffe2 proto.dir metanet.pb.cc.o 25 building cxx object caffe2 proto cmakefiles caffe2 proto.dir torch.pb.cc.o 25 built target gen onnx proto 25 running gen proto.py onnx onnx operators.in.proto processing home feng pytorch third party onnx onnx onnx operators.in.proto writing home feng pytorch build third party onnx onnx onnx operators onnx torch.proto writing home feng pytorch build third party onnx onnx onnx operators onnx torch.proto3 writing home feng pytorch build third party onnx onnx onnx operators.pb.h generating home feng pytorch build third party onnx onnx onnx operators pb.py 25 running c protocol buffer compiler home feng pytorch build third party onnx onnx onnx operators onnx torch.proto scanning dependencies target onnx proto 26 building cxx object third party onnx cmakefiles onnx proto.dir onnx onnx onnx torch.pb.cc.o 26 building cxx object third party onnx cmakefiles onnx proto.dir onnx onnx operators onnx torch.pb.cc.o 26 linking cxx static library .. .. lib libonnx proto.a 26 built target onnx proto scanning dependencies target onnx 26 building cxx object third party onnx cmakefiles onnx.dir onnx version converter convert.cc.o 26 building cxx object third party onnx cmakefiles onnx.dir onnx defs function.cc.o 26 building cxx object third party onnx cmakefiles onnx.dir onnx defs controlflow defs.cc.o 26 building cxx object third party onnx cmakefiles onnx.dir onnx defs data type utils.cc.o 26 building cxx object third party onnx cmakefiles onnx.dir onnx version converter helper.cc.o 26 building cxx object third party onnx cmakefiles onnx.dir caffe2 onnx torch ops schema.cc.o 26 building cxx object third party onnx cmakefiles onnx.dir onnx defs nn defs.cc.o 26 building cxx object third party onnx cmakefiles onnx.dir caffe2 onnx torch ops defs.cc.o 26 building cxx object third party onnx cmakefiles onnx.dir onnx defs nn old.cc.o 26 building cxx object third party onnx cmakefiles onnx.dir onnx defs logical defs.cc.o 26 built target caffe2 proto scanning dependencies target caffe2 perfkernels avx2 26 building cxx object caffe2 perfkernels cmakefiles caffe2 perfkernels avx2.dir embedding lookup avx2.cc.o 26 building cxx object third party onnx cmakefiles onnx.dir onnx defs logical old.cc.o 26 building cxx object third party onnx cmakefiles onnx.dir onnx defs reduction defs.cc.o 26 building cxx object third party onnx cmakefiles onnx.dir onnx defs generator defs.cc.o 26 building cxx object third party onnx cmakefiles onnx.dir onnx defs generator old.cc.o 26 building cxx object third party onnx cmakefiles onnx.dir onnx defs tensor defs.cc.o 26 building cxx object third party onnx cmakefiles onnx.dir onnx defs tensor old.cc.o 26 building cxx object third party onnx cmakefiles onnx.dir onnx defs schema.cc.o 26 building cxx object third party onnx cmakefiles onnx.dir onnx defs traditionalml defs.cc.o 26 building cxx object third party onnx cmakefiles onnx.dir onnx defs math defs.cc.o 26 building cxx object third party onnx cmakefiles onnx.dir onnx defs math old.cc.o 26 building cxx object third party onnx cmakefiles onnx.dir onnx defs rnn defs.cc.o 27 building cxx object third party onnx cmakefiles onnx.dir onnx defs rnn old.cc.o 27 building cxx object caffe2 perfkernels cmakefiles caffe2 perfkernels avx2.dir math cpu avx2.cc.o 27 building cxx object third party onnx cmakefiles onnx.dir onnx defs experiments defs.cc.o 27 building cxx object caffe2 perfkernels cmakefiles caffe2 perfkernels avx2.dir common avx2.cc.o 27 building cxx object third party onnx cmakefiles onnx.dir onnx defs experiments experiments functions.cc.o 27 building cxx object caffe2 perfkernels cmakefiles caffe2 perfkernels avx2.dir typed axpy avx2.cc.o 27 building cxx object third party onnx cmakefiles onnx.dir onnx onnxifi utils.cc.o 27 building cxx object third party onnx cmakefiles onnx.dir onnx common ir pb converter.cc.o 27 building cxx object third party onnx cmakefiles onnx.dir onnx common status.cc.o 27 building cxx object third party onnx cmakefiles onnx.dir onnx common assertions.cc.o 27 building cxx object third party onnx cmakefiles onnx.dir onnx common model helpers.cc.o 27 building cxx object third party onnx cmakefiles onnx.dir onnx common interned strings.cc.o 27 building cxx object caffe2 perfkernels cmakefiles caffe2 perfkernels avx2.dir embedding lookup fused 8bit rowwise avx2.cc.o 27 building cxx object third party onnx cmakefiles onnx.dir onnx checker.cc.o 27 building cxx object third party onnx cmakefiles onnx.dir onnx shape inference implementation.cc.o 27 building cxx object third party onnx cmakefiles onnx.dir onnx optimizer pass registry.cc.o 27 building cxx object third party onnx cmakefiles onnx.dir onnx optimizer optimize.cc.o 27 building cxx object third party onnx cmakefiles onnx.dir onnx optimizer pass manager.cc.o 27 building cxx object third party onnx cmakefiles onnx.dir onnx optimizer pass.cc.o ptxas warning big maxrregcount value specified 96, ignored scanning dependencies target caffe2 protos 27 linking cxx static library .. lib libcaffe2 protos.a 27 built target caffe2 protos scanning dependencies target caffe2 perfkernels avx 27 building cxx object caffe2 perfkernels cmakefiles caffe2 perfkernels avx.dir adagrad avx.cc.o 28 building cxx object caffe2 perfkernels cmakefiles caffe2 perfkernels avx.dir typed axpy avx.cc.o 28 building cxx object caffe2 perfkernels cmakefiles caffe2 perfkernels avx.dir common avx.cc.o 28 built target caffe2 perfkernels avx 28 built target caffe2 perfkernels avx2 28 linking cxx static library .. .. lib libonnx.a 28 built target onnx scanning dependencies target caffe2 nvcc warning 'compute 20', 'sm 20', 'sm 21' architectures deprecated, may removed future release use wno deprecated gpu targets suppress warning . compiling broadcast.cu home feng pytorch build nccl obj collectives device broadcast sum.o nvcc warning 'compute 20', 'sm 20', 'sm 21' architectures deprecated, may removed future release use wno deprecated gpu targets suppress warning . ptxas warning big maxrregcount value specified 96, ignored nvcc warning 'compute 20', 'sm 20', 'sm 21' architectures deprecated, may removed future release use wno deprecated gpu targets suppress warning . compiling reduce.cu home feng pytorch build nccl obj collectives device reduce sum.o nvcc warning 'compute 20', 'sm 20', 'sm 21' architectures deprecated, may removed future release use wno deprecated gpu targets suppress warning . 28 building cxx object caffe2 cmakefiles caffe2.dir aten src aten dlconvertor.cpp.o 28 building cxx object caffe2 cmakefiles caffe2.dir aten src aten tensorutils.cpp.o 28 building cxx object caffe2 cmakefiles caffe2.dir aten src aten cpugenerator.cpp.o 28 building cxx object caffe2 cmakefiles caffe2.dir aten src aten tensorgeometry.cpp.o 29 building cxx object caffe2 cmakefiles caffe2.dir aten src aten undefinedtype.cpp.o 29 building cxx object caffe2 cmakefiles caffe2.dir aten src aten utils.cpp.o 29 building cxx object caffe2 cmakefiles caffe2.dir aten src aten context.cpp.o 29 building cxx object caffe2 cmakefiles caffe2.dir aten src aten cpugeneral.cpp.o 29 building cxx object caffe2 cmakefiles caffe2.dir aten src aten cputypedefault.cpp.o 29 building cxx object caffe2 cmakefiles caffe2.dir aten src aten sparsetensorimpl.cpp.o 29 building cxx object caffe2 cmakefiles caffe2.dir aten src aten expandutils.cpp.o 29 building cxx object caffe2 cmakefiles caffe2.dir aten src aten detail complexhooksinterface.cpp.o 29 building cxx object caffe2 cmakefiles caffe2.dir aten src aten detail cpuguardimpl.cpp.o 29 building cxx object caffe2 cmakefiles caffe2.dir aten src aten detail cudahooksinterface.cpp.o 29 building cxx object caffe2 cmakefiles caffe2.dir aten src aten cpu flushdenormal.cpp.o 29 building cxx object caffe2 cmakefiles caffe2.dir aten src aten core storage.cpp.o 29 building cxx object caffe2 cmakefiles caffe2.dir aten src aten core context base.cpp.o 29 building cxx object caffe2 cmakefiles caffe2.dir aten src aten core thread pool.cpp.o 29 building cxx object caffe2 cmakefiles caffe2.dir aten src aten core atencoretest.cpp.o 29 building cxx object caffe2 cmakefiles caffe2.dir aten src aten core storageimpl.cpp.o 29 building cxx object caffe2 cmakefiles caffe2.dir aten src aten core formatting.cpp.o 29 building cxx object caffe2 cmakefiles caffe2.dir aten src aten core legacytypedispatch.cpp.o 29 building cxx object caffe2 cmakefiles caffe2.dir aten src aten core register symbols.cpp.o 29 building cxx object caffe2 cmakefiles caffe2.dir aten src aten core variablehooksinterface.cpp.o 29 building cxx object caffe2 cmakefiles caffe2.dir aten src aten core tensor.cpp.o 29 building cxx object caffe2 cmakefiles caffe2.dir aten src aten core interned strings.cpp.o 30 building cxx object caffe2 cmakefiles caffe2.dir aten src aten core tensoroptions.cpp.o 30 building cxx object caffe2 cmakefiles caffe2.dir aten src aten core range.cpp.o 30 building cxx object caffe2 cmakefiles caffe2.dir aten src aten core ivalue.cpp.o 30 building cxx object caffe2 cmakefiles caffe2.dir aten src aten core allocator.cpp.o 30 building cxx object caffe2 cmakefiles caffe2.dir aten src aten core atengeneral.cpp.o 30 building cxx object caffe2 cmakefiles caffe2.dir aten src aten core undefinedtensorimpl.cpp.o 30 building cxx object caffe2 cmakefiles caffe2.dir aten src aten core scalar.cpp.o 30 building cxx object caffe2 cmakefiles caffe2.dir aten src aten core blob.cpp.o 30 building cxx object caffe2 cmakefiles caffe2.dir aten src aten core intrusive ptr.cpp.o 30 building cxx object caffe2 cmakefiles caffe2.dir aten src aten core tensorimpl.cpp.o 30 building cxx object caffe2 cmakefiles caffe2.dir aten src aten core uniquevoidptr.cpp.o 30 building cxx object caffe2 cmakefiles caffe2.dir aten src aten core type.cpp.o 30 building cxx object caffe2 cmakefiles caffe2.dir aten src aten core optionsguard.cpp.o 30 building cxx object caffe2 cmakefiles caffe2.dir aten src aten native embedding.cpp.o 30 building cxx object caffe2 cmakefiles caffe2.dir aten src aten native softmax.cpp.o ptxas warning big maxrregcount value specified 96, ignored 30 building cxx object caffe2 cmakefiles caffe2.dir aten src aten native dispatchstub.cpp.o 30 building cxx object caffe2 cmakefiles caffe2.dir aten src aten native dropout.cpp.o 30 building cxx object caffe2 cmakefiles caffe2.dir aten src aten native tensorcompare.cpp.o 30 building cxx object caffe2 cmakefiles caffe2.dir aten src aten native linearalgebra.cpp.o 30 building cxx object caffe2 cmakefiles caffe2.dir aten src aten native gridsampler.cpp.o 30 building cxx object caffe2 cmakefiles caffe2.dir aten src aten native reduceops.cpp.o 31 building cxx object caffe2 cmakefiles caffe2.dir aten src aten native unaryops.cpp.o 31 building cxx object caffe2 cmakefiles caffe2.dir aten src aten native linear.cpp.o nvcc warning 'compute 20', 'sm 20', 'sm 21' architectures deprecated, may removed future release use wno deprecated gpu targets suppress warning . 31 building cxx object caffe2 cmakefiles caffe2.dir aten src aten native indexing.cpp.o 31 building cxx object caffe2 cmakefiles caffe2.dir aten src aten native pixelshuffle.cpp.o compiling gather.cu home feng pytorch build nccl obj collectives device gather sum.o nvcc warning 'compute 20', 'sm 20', 'sm 21' architectures deprecated, may removed future release use wno deprecated gpu targets suppress warning . 31 building cxx object caffe2 cmakefiles caffe2.dir aten src aten native tensoriterator.cpp.o 31 building cxx object caffe2 cmakefiles caffe2.dir aten src aten native legacybridge.cpp.o 31 building cxx object caffe2 cmakefiles caffe2.dir aten src aten native typeproperties.cpp.o 31 building cxx object caffe2 cmakefiles caffe2.dir aten src aten native copy.cpp.o 31 building cxx object caffe2 cmakefiles caffe2.dir aten src aten native loss.cpp.o 31 building cxx object caffe2 cmakefiles caffe2.dir aten src aten native embeddingbag.cpp.o 31 building cxx object caffe2 cmakefiles caffe2.dir aten src aten native normalization.cpp.o 31 building cxx object caffe2 cmakefiles caffe2.dir aten src aten native weightnorm.cpp.o 31 building cxx object caffe2 cmakefiles caffe2.dir aten src aten native resize.cpp.o 31 building cxx object caffe2 cmakefiles caffe2.dir aten src aten native convolution.cpp.o ptxas warning big maxrregcount value specified 96, ignored 31 building cxx object caffe2 cmakefiles caffe2.dir aten src aten native roipooling.cpp.o 31 building cxx object caffe2 cmakefiles caffe2.dir aten src aten native packedsequence.cpp.o 31 building cxx object caffe2 cmakefiles caffe2.dir aten src aten native legacydefinitions.cpp.o nvcc warning 'compute 20', 'sm 20', 'sm 21' architectures deprecated, may removed future release use wno deprecated gpu targets suppress warning . 31 building cxx object caffe2 cmakefiles caffe2.dir aten src aten native distance.cpp.o compiling reduce scatter.cu home feng pytorch build nccl obj collectives device reduce scatter sum.o nvcc warning 'compute 20', 'sm 20', 'sm 21' architectures deprecated, may removed future release use wno deprecated gpu targets suppress warning . 31 building cxx object caffe2 cmakefiles caffe2.dir aten src aten native tensorfactories.cpp.o 31 building cxx object caffe2 cmakefiles caffe2.dir aten src aten native convolutiontbc.cpp.o 31 building cxx object caffe2 cmakefiles caffe2.dir aten src aten native summaryops.cpp.o 31 building cxx object caffe2 cmakefiles caffe2.dir aten src aten native tensortransformations.cpp.o 32 building cxx object caffe2 cmakefiles caffe2.dir aten src aten native unique.cpp.o 32 building cxx object caffe2 cmakefiles caffe2.dir aten src aten native tensoriteratorreduce.cpp.o 32 building cxx object caffe2 cmakefiles caffe2.dir aten src aten native pooling.cpp.o 32 building cxx object caffe2 cmakefiles caffe2.dir aten src aten native distributions.cpp.o 32 building cxx object caffe2 cmakefiles caffe2.dir aten src aten native spectralops.cpp.o 32 building cxx object caffe2 cmakefiles caffe2.dir aten src aten native memory.cpp.o 32 building cxx object caffe2 cmakefiles caffe2.dir aten src aten native tensorproperties.cpp.o 32 building cxx object caffe2 cmakefiles caffe2.dir aten src aten native scalar.cpp.o 32 building cxx object caffe2 cmakefiles caffe2.dir aten src aten native tensorshape.cpp.o 32 building cxx object caffe2 cmakefiles caffe2.dir aten src aten native binaryops.cpp.o 32 building cxx object caffe2 cmakefiles caffe2.dir aten src aten native activation.cpp.o 32 building cxx object caffe2 cmakefiles caffe2.dir aten src aten native lossctc.cpp.o 32 building cxx object caffe2 cmakefiles caffe2.dir aten src aten native rnn.cpp.o 32 building cxx object caffe2 cmakefiles caffe2.dir aten src aten native constantpadnd.cpp.o 32 building cxx object caffe2 cmakefiles caffe2.dir aten src aten native tensorconversions.cpp.o 32 building cxx object caffe2 cmakefiles caffe2.dir aten src aten native batchlinearalgebra.cpp.o ptxas warning big maxrregcount value specified 96, ignored 32 building cxx object caffe2 cmakefiles caffe2.dir aten src aten native sparse sparsetensormath.cpp.o 32 building cxx object caffe2 cmakefiles caffe2.dir aten src aten native sparse sparsetensor.cpp.o 32 building cxx object caffe2 cmakefiles caffe2.dir aten src aten native mkl linearalgebra.cpp.o 32 building cxx object caffe2 cmakefiles caffe2.dir aten src aten native mkl spectralops.cpp.o 32 building cxx object caffe2 cmakefiles caffe2.dir aten src aten native mkldnn conv.cpp.o 32 building cxx object caffe2 cmakefiles caffe2.dir aten src aten cpubytetype.cpp.o 33 building cxx object caffe2 cmakefiles caffe2.dir aten src aten cpuchartype.cpp.o 33 building cxx object caffe2 cmakefiles caffe2.dir aten src aten cpucopy.cpp.o 33 building cxx object caffe2 cmakefiles caffe2.dir aten src aten cpudoubletype.cpp.o 33 building cxx object caffe2 cmakefiles caffe2.dir aten src aten cpufloattype.cpp.o compiling reduce.cu home feng pytorch build nccl obj collectives device reduce prod.o nvcc warning 'compute 20', 'sm 20', 'sm 21' architectures deprecated, may removed future release use wno deprecated gpu targets suppress warning . 33 building cxx object caffe2 cmakefiles caffe2.dir aten src aten cpuhalftype.cpp.o 33 building cxx object caffe2 cmakefiles caffe2.dir aten src aten cpuinttype.cpp.o 33 building cxx object caffe2 cmakefiles caffe2.dir aten src aten cpulongtype.cpp.o 33 building cxx object caffe2 cmakefiles caffe2.dir aten src aten cpushorttype.cpp.o 33 building cxx object caffe2 cmakefiles caffe2.dir aten src aten registercpu.cpp.o 33 building cxx object caffe2 cmakefiles caffe2.dir aten src aten sparsecpubytetype.cpp.o 33 building cxx object caffe2 cmakefiles caffe2.dir aten src aten sparsecpuchartype.cpp.o 33 building cxx object caffe2 cmakefiles caffe2.dir aten src aten sparsecpudoubletype.cpp.o 33 building cxx object caffe2 cmakefiles caffe2.dir aten src aten sparsecpufloattype.cpp.o 33 building cxx object caffe2 cmakefiles caffe2.dir aten src aten sparsecpuinttype.cpp.o 33 building cxx object caffe2 cmakefiles caffe2.dir aten src aten sparsecpulongtype.cpp.o 33 building cxx object caffe2 cmakefiles caffe2.dir aten src aten sparsecpushorttype.cpp.o 33 building cxx object caffe2 cmakefiles caffe2.dir aten src aten typedefault.cpp.o 33 building cxx object caffe2 cmakefiles caffe2.dir aten src th thgeneral.cpp.o 33 building cxx object caffe2 cmakefiles caffe2.dir aten src th thallocator.cpp.o 33 building cxx object caffe2 cmakefiles caffe2.dir aten src th thsize.cpp.o 33 building cxx object caffe2 cmakefiles caffe2.dir aten src th thstoragefunctions.cpp.o 34 building cxx object caffe2 cmakefiles caffe2.dir aten src th thtensor.cpp.o 34 building cxx object caffe2 cmakefiles caffe2.dir aten src th thtensorcopy.cpp.o 34 building cxx object caffe2 cmakefiles caffe2.dir aten src th thtensorrandom.cpp.o 34 building cxx object caffe2 cmakefiles caffe2.dir aten src th thtensormath.cpp.o 34 building cxx object caffe2 cmakefiles caffe2.dir aten src th thtensormoremath.cpp.o 34 building cxx object caffe2 cmakefiles caffe2.dir aten src th thtensorevenmoremath.cpp.o 34 building cxx object caffe2 cmakefiles caffe2.dir aten src th thtensorconv.cpp.o 34 building cxx object caffe2 cmakefiles caffe2.dir aten src th thtensorlapack.cpp.o 34 building cxx object caffe2 cmakefiles caffe2.dir aten src th thblas.cpp.o ptxas warning big maxrregcount value specified 96, ignored 34 building cxx object caffe2 cmakefiles caffe2.dir aten src th thlapack.cpp.o 34 building cxx object caffe2 cmakefiles caffe2.dir aten src th thlogadd.cpp.o 34 building cxx object caffe2 cmakefiles caffe2.dir aten src th thrandom.cpp.o 34 building cxx object caffe2 cmakefiles caffe2.dir aten src th thfile.cpp.o 34 building cxx object caffe2 cmakefiles caffe2.dir aten src th thdiskfile.cpp.o 34 building cxx object caffe2 cmakefiles caffe2.dir aten src th thmemoryfile.cpp.o 34 building cxx object caffe2 cmakefiles caffe2.dir aten src th thvector.cpp.o 34 building cxx object caffe2 cmakefiles caffe2.dir aten src th vector avx.cpp.o 34 building cxx object caffe2 cmakefiles caffe2.dir aten src th vector avx2.cpp.o 34 building cxx object caffe2 cmakefiles caffe2.dir aten src thnn init.cpp.o 34 building cxx object caffe2 cmakefiles caffe2.dir aten src aten native cpu unaryopskernel.cpp.avx2.cpp.o 34 building cxx object caffe2 cmakefiles caffe2.dir aten src aten native cpu activation.cpp.avx2.cpp.o 34 building cxx object caffe2 cmakefiles caffe2.dir aten src aten native cpu softmaxkernel.cpp.avx2.cpp.o 35 building cxx object caffe2 cmakefiles caffe2.dir aten src aten native cpu gridsamplerkernel.cpp.avx2.cpp.o compiling broadcast.cu home feng pytorch build nccl obj collectives device broadcast prod.o nvcc warning 'compute 20', 'sm 20', 'sm 21' architectures deprecated, may removed future release use wno deprecated gpu targets suppress warning . 35 building cxx object caffe2 cmakefiles caffe2.dir aten src aten native cpu reduceopskernel.cpp.avx2.cpp.o 35 building cxx object caffe2 cmakefiles caffe2.dir aten src aten native cpu binaryopskernel.cpp.avx2.cpp.o ptxas warning big maxrregcount value specified 96, ignored compiling reduce.cu home feng pytorch build nccl obj collectives device reduce prod.o nvcc warning 'compute 20', 'sm 20', 'sm 21' architectures deprecated, may removed future release use wno deprecated gpu targets suppress warning . 35 building cxx object caffe2 cmakefiles caffe2.dir aten src aten native cpu distanceopskernel.cpp.avx2.cpp.o 35 building cxx object caffe2 cmakefiles caffe2.dir aten src aten native cpu tensorcomparekernel.cpp.avx2.cpp.o 35 building cxx object caffe2 cmakefiles caffe2.dir aten src aten native cpu unaryopskernel.cpp.avx.cpp.o 35 building cxx object caffe2 cmakefiles caffe2.dir aten src aten native cpu activation.cpp.avx.cpp.o 35 building cxx object caffe2 cmakefiles caffe2.dir aten src aten native cpu softmaxkernel.cpp.avx.cpp.o 35 building cxx object caffe2 cmakefiles caffe2.dir aten src aten native cpu gridsamplerkernel.cpp.avx.cpp.o 35 building cxx object caffe2 cmakefiles caffe2.dir aten src aten native cpu reduceopskernel.cpp.avx.cpp.o 35 building cxx object caffe2 cmakefiles caffe2.dir aten src aten native cpu binaryopskernel.cpp.avx.cpp.o ptxas warning big maxrregcount value specified 96, ignored 35 building cxx object caffe2 cmakefiles caffe2.dir aten src aten native cpu distanceopskernel.cpp.avx.cpp.o 35 building cxx object caffe2 cmakefiles caffe2.dir aten src aten native cpu tensorcomparekernel.cpp.avx.cpp.o compiling gather.cu home feng pytorch build nccl obj collectives device gather prod.o nvcc warning 'compute 20', 'sm 20', 'sm 21' architectures deprecated, may removed future release use wno deprecated gpu targets suppress warning . 35 building cxx object caffe2 cmakefiles caffe2.dir aten src aten native cpu unaryopskernel.cpp.default.cpp.o 35 building cxx object caffe2 cmakefiles caffe2.dir aten src aten native cpu activation.cpp.default.cpp.o 35 building cxx object caffe2 cmakefiles caffe2.dir aten src aten native cpu softmaxkernel.cpp.default.cpp.o ptxas warning big maxrregcount value specified 96, ignored 35 building cxx object caffe2 cmakefiles caffe2.dir aten src aten native cpu gridsamplerkernel.cpp.default.cpp.o 35 building cxx object caffe2 cmakefiles caffe2.dir aten src aten native cpu reduceopskernel.cpp.default.cpp.o compiling reduce scatter.cu home feng pytorch build nccl obj collectives device reduce scatter prod.o nvcc warning 'compute 20', 'sm 20', 'sm 21' architectures deprecated, may removed future release use wno deprecated gpu targets suppress warning . 35 building cxx object caffe2 cmakefiles caffe2.dir aten src aten native cpu binaryopskernel.cpp.default.cpp.o 35 building cxx object caffe2 cmakefiles caffe2.dir aten src aten native cpu distanceopskernel.cpp.default.cpp.o 35 building cxx object caffe2 cmakefiles caffe2.dir aten src aten native cpu tensorcomparekernel.cpp.default.cpp.o 36 building cxx object caffe2 cmakefiles caffe2.dir aten src aten mkldnn runtime.cpp.o 36 building cxx object caffe2 cmakefiles caffe2.dir contrib aten aten op.cc.o 36 building cxx object caffe2 cmakefiles caffe2.dir contrib gloo allgather ops.cc.o 36 building cxx object caffe2 cmakefiles caffe2.dir contrib gloo allreduce ops.cc.o 36 building cxx object caffe2 cmakefiles caffe2.dir contrib gloo barrier ops.cc.o ptxas warning big maxrregcount value specified 96, ignored 36 building cxx object caffe2 cmakefiles caffe2.dir contrib gloo broadcast ops.cc.o 36 building cxx object caffe2 cmakefiles caffe2.dir contrib gloo common.cc.o 36 building cxx object caffe2 cmakefiles caffe2.dir contrib gloo common world ops.cc.o 36 building cxx object caffe2 cmakefiles caffe2.dir contrib gloo context.cc.o 36 building cxx object caffe2 cmakefiles caffe2.dir contrib gloo reduce scatter ops.cc.o 36 building cxx object caffe2 cmakefiles caffe2.dir contrib gloo store handler.cc.o 36 building cxx object caffe2 cmakefiles caffe2.dir contrib script lexer.cc.o 36 building cxx object caffe2 cmakefiles caffe2.dir contrib script compiler.cc.o compiling reduce.cu home feng pytorch build nccl obj collectives device reduce min.o nvcc warning 'compute 20', 'sm 20', 'sm 21' architectures deprecated, may removed future release use wno deprecated gpu targets suppress warning . 36 building cxx object caffe2 cmakefiles caffe2.dir core init intrinsics check.cc.o 36 building cxx object caffe2 cmakefiles caffe2.dir core module.cc.o 36 building cxx object caffe2 cmakefiles caffe2.dir core net dag utils.cc.o 36 building cxx object caffe2 cmakefiles caffe2.dir core event.cc.o 36 building cxx object caffe2 cmakefiles caffe2.dir core graph.cc.o 36 building cxx object caffe2 cmakefiles caffe2.dir core plan executor.cc.o 36 building cxx object caffe2 cmakefiles caffe2.dir core int8 serialization.cc.o 36 building cxx object caffe2 cmakefiles caffe2.dir core tensor int8.cc.o 36 building cxx object caffe2 cmakefiles caffe2.dir core context.cc.o 37 building cxx object caffe2 cmakefiles caffe2.dir core prof dag counters.cc.o 37 building cxx object caffe2 cmakefiles caffe2.dir core blob stats.cc.o 37 building cxx object caffe2 cmakefiles caffe2.dir core qtensor.cc.o 37 building cxx object caffe2 cmakefiles caffe2.dir core net simple.cc.o 37 building cxx object caffe2 cmakefiles caffe2.dir core memonger.cc.o 37 building cxx object caffe2 cmakefiles caffe2.dir core net.cc.o 37 building cxx object caffe2 cmakefiles caffe2.dir core transform.cc.o 37 building cxx object caffe2 cmakefiles caffe2.dir core operator c10wrapper.cc.o 37 building cxx object caffe2 cmakefiles caffe2.dir core context base.cc.o 37 building cxx object caffe2 cmakefiles caffe2.dir core common.cc.o 37 building cxx object caffe2 cmakefiles caffe2.dir core blob serialization.cc.o 37 building cxx object caffe2 cmakefiles caffe2.dir core net async tracing.cc.o 37 building cxx object caffe2 cmakefiles caffe2.dir core workspace.cc.o 37 building cxx object caffe2 cmakefiles caffe2.dir core tensor impl.cc.o 37 building cxx object caffe2 cmakefiles caffe2.dir core init omp.cc.o 37 building cxx object caffe2 cmakefiles caffe2.dir core operator.cc.o 37 building cxx object caffe2 cmakefiles caffe2.dir core init.cc.o 37 building cxx object caffe2 cmakefiles caffe2.dir core operator schema.cc.o 37 building cxx object caffe2 cmakefiles caffe2.dir core tensor.cc.o 37 building cxx object caffe2 cmakefiles caffe2.dir core net async base.cc.o 37 building cxx object caffe2 cmakefiles caffe2.dir core net simple refcount.cc.o ptxas warning big maxrregcount value specified 96, ignored 37 building cxx object caffe2 cmakefiles caffe2.dir core stats.cc.o 38 building cxx object caffe2 cmakefiles caffe2.dir core types.cc.o 38 building cxx object caffe2 cmakefiles caffe2.dir core allocator.cc.o 38 building cxx object caffe2 cmakefiles caffe2.dir core net dag.cc.o 38 building cxx object caffe2 cmakefiles caffe2.dir core numa.cc.o 38 building cxx object caffe2 cmakefiles caffe2.dir core net async scheduling.cc.o 38 building cxx object caffe2 cmakefiles caffe2.dir core qtensor serialization.cc.o 38 building cxx object caffe2 cmakefiles caffe2.dir core db.cc.o 38 building cxx object caffe2 cmakefiles caffe2.dir utils proto convert.cc.o 38 building cxx object caffe2 cmakefiles caffe2.dir utils proto wrap.cc.o 38 building cxx object caffe2 cmakefiles caffe2.dir utils proto utils.cc.o 38 building cxx object caffe2 cmakefiles caffe2.dir utils murmur hash3.cc.o 38 building cxx object caffe2 cmakefiles caffe2.dir utils smart tensor printer.cc.o 38 building cxx object caffe2 cmakefiles caffe2.dir utils signal handler.cc.o 38 building cxx object caffe2 cmakefiles caffe2.dir utils string utils.cc.o 38 building cxx object caffe2 cmakefiles caffe2.dir utils threadpool threadpool.cc.o 38 building cxx object caffe2 cmakefiles caffe2.dir utils cpuid.cc.o 38 building cxx object caffe2 cmakefiles caffe2.dir utils bench utils.cc.o 38 building cxx object caffe2 cmakefiles caffe2.dir utils math cpu.cc.o compiling broadcast.cu home feng pytorch build nccl obj collectives device broadcast min.o nvcc warning 'compute 20', 'sm 20', 'sm 21' architectures deprecated, may removed future release use wno deprecated gpu targets suppress warning . 38 building cxx object caffe2 cmakefiles caffe2.dir utils math utils.cc.o 38 building cxx object caffe2 cmakefiles caffe2.dir utils thread name.cc.o 38 building cxx object caffe2 cmakefiles caffe2.dir utils threadpool pthreadpool.cc.o 39 building cxx object caffe2 cmakefiles caffe2.dir utils threadpool pthreadpool impl.cc.o 39 building cxx object caffe2 cmakefiles caffe2.dir predictor predictor.cc.o 39 building cxx object caffe2 cmakefiles caffe2.dir predictor predictor utils.cc.o 39 building cxx object caffe2 cmakefiles caffe2.dir predictor predictor config.cc.o 39 building cxx object caffe2 cmakefiles caffe2.dir core nomnigraph tests test util.cc.o 39 building cxx object caffe2 cmakefiles caffe2.dir core nomnigraph representations neuralnet.cc.o 39 building cxx object caffe2 cmakefiles caffe2.dir db create db op.cc.o 39 building cxx object caffe2 cmakefiles caffe2.dir db protodb.cc.o 39 building cxx object caffe2 cmakefiles caffe2.dir distributed file store handler.cc.o ptxas warning big maxrregcount value specified 96, ignored compiling reduce.cu home feng pytorch build nccl obj collectives device reduce min.o nvcc warning 'compute 20', 'sm 20', 'sm 21' architectures deprecated, may removed future release use wno deprecated gpu targets suppress warning . 39 building cxx object caffe2 cmakefiles caffe2.dir distributed file store handler op.cc.o 39 building cxx object caffe2 cmakefiles caffe2.dir distributed store handler.cc.o 39 building cxx object caffe2 cmakefiles caffe2.dir distributed store ops.cc.o 39 building cxx object caffe2 cmakefiles caffe2.dir ideep utils ideep register.cc.o 39 building cxx object caffe2 cmakefiles caffe2.dir ideep operators dropout op.cc.o 39 building cxx object caffe2 cmakefiles caffe2.dir ideep operators pool op.cc.o 39 building cxx object caffe2 cmakefiles caffe2.dir ideep operators momentum sgd op.cc.o 39 building cxx object caffe2 cmakefiles caffe2.dir ideep operators local response normalization op.cc.o 39 building cxx object caffe2 cmakefiles caffe2.dir ideep operators relu op.cc.o 39 building cxx object caffe2 cmakefiles caffe2.dir ideep operators elementwise sum op.cc.o 39 building cxx object caffe2 cmakefiles caffe2.dir ideep operators squeeze op.cc.o 39 building cxx object caffe2 cmakefiles caffe2.dir ideep operators utility ops.cc.o ptxas warning big maxrregcount value specified 96, ignored 39 building cxx object caffe2 cmakefiles caffe2.dir ideep operators conv fusion op.cc.o 40 building cxx object caffe2 cmakefiles caffe2.dir ideep operators operator fallback ideep.cc.o 40 building cxx object caffe2 cmakefiles caffe2.dir ideep operators concat split op.cc.o 40 building cxx object caffe2 cmakefiles caffe2.dir ideep operators spatial batch norm op.cc.o compiling gather.cu home feng pytorch build nccl obj collectives device gather min.o nvcc warning 'compute 20', 'sm 20', 'sm 21' architectures deprecated, may removed future release use wno deprecated gpu targets suppress warning . 40 building cxx object caffe2 cmakefiles caffe2.dir ideep operators fully connected op.cc.o 40 building cxx object caffe2 cmakefiles caffe2.dir ideep operators conv op.cc.o 40 building cxx object caffe2 cmakefiles caffe2.dir mpi mpi common.cc.o ptxas warning big maxrregcount value specified 96, ignored compiling reduce scatter.cu home feng pytorch build nccl obj collectives device reduce scatter min.o nvcc warning 'compute 20', 'sm 20', 'sm 21' architectures deprecated, may removed future release use wno deprecated gpu targets suppress warning . 40 building cxx object caffe2 cmakefiles caffe2.dir mpi mpi ops.cc.o 40 building cxx object caffe2 cmakefiles caffe2.dir observers time observer.cc.o 40 building cxx object caffe2 cmakefiles caffe2.dir observers runcnt observer.cc.o 40 building cxx object caffe2 cmakefiles caffe2.dir onnx backend.cc.o 40 building cxx object caffe2 cmakefiles caffe2.dir onnx onnx exporter.cc.o 40 building cxx object caffe2 cmakefiles caffe2.dir onnx backend rep.cc.o 40 building cxx object caffe2 cmakefiles caffe2.dir onnx helper.cc.o 40 building cxx object caffe2 cmakefiles caffe2.dir onnx onnxifi init.cc.o 40 building cxx object caffe2 cmakefiles caffe2.dir onnx device.cc.o 40 building cxx object caffe2 cmakefiles caffe2.dir operators lpnorm op.cc.o 40 building cxx object caffe2 cmakefiles caffe2.dir operators gru unit op.cc.o 40 building cxx object caffe2 cmakefiles caffe2.dir operators sparse normalize op.cc.o ptxas warning big maxrregcount value specified 96, ignored 40 building cxx object caffe2 cmakefiles caffe2.dir operators op.cc.o 40 building cxx object caffe2 cmakefiles caffe2.dir operators unique ops.cc.o 40 building cxx object caffe2 cmakefiles caffe2.dir operators feed blob op.cc.o compiling reduce.cu home feng pytorch build nccl obj collectives device reduce max.o nvcc warning 'compute 20', 'sm 20', 'sm 21' architectures deprecated, may removed future release use wno deprecated gpu targets suppress warning . 40 building cxx object caffe2 cmakefiles caffe2.dir operators index ops.cc.o 41 building cxx object caffe2 cmakefiles caffe2.dir operators clip op.cc.o 41 building cxx object caffe2 cmakefiles caffe2.dir operators elementwise div gradient op.cc.o 41 building cxx object caffe2 cmakefiles caffe2.dir operators glu op.cc.o 41 building cxx object caffe2 cmakefiles caffe2.dir operators reservoir sampling.cc.o 41 building cxx object caffe2 cmakefiles caffe2.dir operators dropout op.cc.o 41 building cxx object caffe2 cmakefiles caffe2.dir operators resize op.cc.o 41 building cxx object caffe2 cmakefiles caffe2.dir operators roi align rotated gradient op.cc.o 41 building cxx object caffe2 cmakefiles caffe2.dir operators tensor protos db input.cc.o 41 building cxx object caffe2 cmakefiles caffe2.dir operators sparse dense mask op.cc.o 41 building cxx object caffe2 cmakefiles caffe2.dir operators pool op.cc.o 41 building cxx object caffe2 cmakefiles caffe2.dir operators relu n op.cc.o 41 building cxx object caffe2 cmakefiles caffe2.dir operators instance norm gradient op.cc.o 41 building cxx object caffe2 cmakefiles caffe2.dir operators reciprocal op.cc.o 41 building cxx object caffe2 cmakefiles caffe2.dir operators leaky relu op.cc.o 41 building cxx object caffe2 cmakefiles caffe2.dir operators distance op.cc.o 41 building cxx object caffe2 cmakefiles caffe2.dir operators ensure cpu output op.cc.o 41 building cxx object caffe2 cmakefiles caffe2.dir operators locally connected op util.cc.o 41 building cxx object caffe2 cmakefiles caffe2.dir operators partition ops.cc.o 41 building cxx object caffe2 cmakefiles caffe2.dir operators selu op.cc.o 41 building cxx object caffe2 cmakefiles caffe2.dir operators op.cc.o 41 building cxx object caffe2 cmakefiles caffe2.dir operators lengths reducer fused 8bit rowwise ops.cc.o ptxas warning big maxrregcount value specified 96, ignored 42 building cxx object caffe2 cmakefiles caffe2.dir operators free op.cc.o 42 building cxx object caffe2 cmakefiles caffe2.dir operators lengths top k op.cc.o 42 building cxx object caffe2 cmakefiles caffe2.dir operators batch bucketize op.cc.o 42 building cxx object caffe2 cmakefiles caffe2.dir operators heatmap max keypoint op.cc.o 42 building cxx object caffe2 cmakefiles caffe2.dir operators filler op.cc.o 42 building cxx object caffe2 cmakefiles caffe2.dir operators softsign op.cc.o 42 building cxx object caffe2 cmakefiles caffe2.dir operators conv op shared.cc.o 42 building cxx object caffe2 cmakefiles caffe2.dir operators enforce finite op.cc.o 42 building cxx object caffe2 cmakefiles caffe2.dir operators local response normalization op.cc.o 42 building cxx object caffe2 cmakefiles caffe2.dir operators moments op.cc.o 42 building cxx object caffe2 cmakefiles caffe2.dir operators sparse dense op.cc.o 42 building cxx object caffe2 cmakefiles caffe2.dir operators data couple.cc.o 42 building cxx object caffe2 cmakefiles caffe2.dir operators tt linear op.cc.o 42 building cxx object caffe2 cmakefiles caffe2.dir operators arg ops.cc.o compiling broadcast.cu home feng pytorch build nccl obj collectives device broadcast max.o nvcc warning 'compute 20', 'sm 20', 'sm 21' architectures deprecated, may removed future release use wno deprecated gpu targets suppress warning . 42 building cxx object caffe2 cmakefiles caffe2.dir operators assert op.cc.o ptxas warning big maxrregcount value specified 96, ignored compiling reduce.cu home feng pytorch build nccl obj collectives device reduce max.o nvcc warning 'compute 20', 'sm 20', 'sm 21' architectures deprecated, may removed future release use wno deprecated gpu targets suppress warning . 42 building cxx object caffe2 cmakefiles caffe2.dir operators feature maps ops.cc.o 42 building cxx object caffe2 cmakefiles caffe2.dir operators op.cc.o 42 building cxx object caffe2 cmakefiles caffe2.dir operators acos op.cc.o 42 building cxx object caffe2 cmakefiles caffe2.dir operators relu op.cc.o 42 building cxx object caffe2 cmakefiles caffe2.dir operators accumulate op.cc.o 42 building cxx object caffe2 cmakefiles caffe2.dir operators spatial softmax loss op.cc.o 42 building cxx object caffe2 cmakefiles caffe2.dir operators listwise l2r op.cc.o 43 building cxx object caffe2 cmakefiles caffe2.dir operators sigmoid op.cc.o 43 building cxx object caffe2 cmakefiles caffe2.dir operators cast op.cc.o 43 building cxx object caffe2 cmakefiles caffe2.dir operators integral image op.cc.o 43 building cxx object caffe2 cmakefiles caffe2.dir operators flexible top k.cc.o 43 building cxx object caffe2 cmakefiles caffe2.dir operators elementwise sub op.cc.o 43 building cxx object caffe2 cmakefiles caffe2.dir operators sequence ops.cc.o 43 building cxx object caffe2 cmakefiles caffe2.dir operators remove data blocks op.cc.o 43 building cxx object caffe2 cmakefiles caffe2.dir operators h softmax op.cc.o 43 building cxx object caffe2 cmakefiles caffe2.dir operators expand squeeze dims op.cc.o ptxas warning big maxrregcount value specified 96, ignored 43 building cxx object caffe2 cmakefiles caffe2.dir operators slice op.cc.o 43 building cxx object caffe2 cmakefiles caffe2.dir operators reduce front back max ops.cc.o 43 building cxx object caffe2 cmakefiles caffe2.dir operators conv gradient op.cc.o 43 building cxx object caffe2 cmakefiles caffe2.dir operators stats put ops.cc.o 43 building cxx object caffe2 cmakefiles caffe2.dir operators bbox transform op.cc.o 43 building cxx object caffe2 cmakefiles caffe2.dir operators collect distribute fpn rpn proposals op.cc.o compiling gather.cu home feng pytorch build nccl obj collectives device gather max.o nvcc warning 'compute 20', 'sm 20', 'sm 21' architectures deprecated, may removed future release use wno deprecated gpu targets suppress warning . 43 building cxx object caffe2 cmakefiles caffe2.dir operators reshape op.cc.o 43 building cxx object caffe2 cmakefiles caffe2.dir operators prepend dim op.cc.o 43 building cxx object caffe2 cmakefiles caffe2.dir operators roi align gradient op.cc.o 43 building cxx object caffe2 cmakefiles caffe2.dir operators sqrt op.cc.o ptxas warning big maxrregcount value specified 96, ignored 43 building cxx object caffe2 cmakefiles caffe2.dir operators segment reduction op.cc.o 43 building cxx object caffe2 cmakefiles caffe2.dir operators elementwise div op.cc.o compiling reduce scatter.cu home feng pytorch build nccl obj collectives device reduce scatter max.o nvcc warning 'compute 20', 'sm 20', 'sm 21' architectures deprecated, may removed future release use wno deprecated gpu targets suppress warning . 44 building cxx object caffe2 cmakefiles caffe2.dir operators byte weight dequant op.cc.o 44 building cxx object caffe2 cmakefiles caffe2.dir operators elementwise sum op.cc.o 44 building cxx object caffe2 cmakefiles caffe2.dir operators upsample op.cc.o 44 building cxx object caffe2 cmakefiles caffe2.dir operators ensure clipped op.cc.o 44 building cxx object caffe2 cmakefiles caffe2.dir operators cosh op.cc.o 44 building cxx object caffe2 cmakefiles caffe2.dir operators margin ranking criterion op.cc.o 44 building cxx object caffe2 cmakefiles caffe2.dir operators roi pool op.cc.o 44 building cxx object caffe2 cmakefiles caffe2.dir operators tanh gradient op.cc.o 44 building cxx object caffe2 cmakefiles caffe2.dir operators lp pool op.cc.o 44 building cxx object caffe2 cmakefiles caffe2.dir operators pow op.cc.o 44 building cxx object caffe2 cmakefiles caffe2.dir operators rowmul op.cc.o 44 building cxx object caffe2 cmakefiles caffe2.dir operators rsqrt op.cc.o ptxas warning big maxrregcount value specified 96, ignored 44 building cxx object caffe2 cmakefiles caffe2.dir operators roi align rotated op.cc.o 44 building cxx object caffe2 cmakefiles caffe2.dir operators bisect percentile op.cc.o 44 building cxx object caffe2 cmakefiles caffe2.dir operators percentile op.cc.o 44 building cxx object caffe2 cmakefiles caffe2.dir operators normalize op.cc.o 44 building cxx object caffe2 cmakefiles caffe2.dir operators sin op.cc.o 44 building cxx object caffe2 cmakefiles caffe2.dir operators batch moments op.cc.o 44 building cxx object caffe2 cmakefiles caffe2.dir operators lstm unit op.cc.o 44 building cxx object caffe2 cmakefiles caffe2.dir operators jsd op.cc.o nvcc warning 'compute 20', 'sm 20', 'sm 21' architectures deprecated, may removed future release use wno deprecated gpu targets suppress warning . compiling functions.cu home feng pytorch build nccl obj collectives device functions.o nvcc warning 'compute 20', 'sm 20', 'sm 21' architectures deprecated, may removed future release use wno deprecated gpu targets suppress warning . 44 building cxx object caffe2 cmakefiles caffe2.dir operators pad op.cc.o 44 building cxx object caffe2 cmakefiles caffe2.dir operators sigmoid gradient op.cc.o 45 building cxx object caffe2 cmakefiles caffe2.dir operators string ops.cc.o 45 building cxx object caffe2 cmakefiles caffe2.dir operators create scope op.cc.o ptxas warning big maxrregcount value specified 96, ignored 45 building cxx object caffe2 cmakefiles caffe2.dir operators lengths reducer rowwise 8bit ops.cc.o nvcc warning 'compute 20', 'sm 20', 'sm 21' architectures deprecated, may removed future release use wno deprecated gpu targets suppress warning . nvlink fatal internal error reference deleted section makefile 83 recipe target ' home feng pytorch build nccl obj collectives device devlink.o' failed make 5 home feng pytorch build nccl obj collectives device devlink.o error 1 makefile 45 recipe target 'devicelib' failed make 4 devicelib error 2 makefile 25 recipe target 'src.build' failed make 3 src.build error 2 cmakefiles nccl external.dir build.make 110 recipe target 'nccl external prefix src nccl external stamp nccl external build' failed make 2 nccl external prefix src nccl external stamp nccl external build error 2 cmakefiles makefile2 67 recipe target 'cmakefiles nccl external.dir all' failed make 1 cmakefiles nccl external.dir error 2 make 1 waiting unfinished jobs.... 45 building cxx object caffe2 cmakefiles caffe2.dir operators lengths reducer ops.cc.o 45 building cxx object caffe2 cmakefiles caffe2.dir operators deform conv op.cc.o 45 building cxx object caffe2 cmakefiles caffe2.dir operators gather ranges dense op.cc.o 45 building cxx object caffe2 cmakefiles caffe2.dir operators conv transpose op mobile.cc.o 45 building cxx object caffe2 cmakefiles caffe2.dir operators batch matmul op.cc.o 45 building cxx object caffe2 cmakefiles caffe2.dir operators crf viterbi op.cc.o 45 building cxx object caffe2 cmakefiles caffe2.dir operators thresholded relu op.cc.o 45 building cxx object caffe2 cmakefiles caffe2.dir operators replace nan op.cc.o 45 building cxx object caffe2 cmakefiles caffe2.dir operators mean op.cc.o 45 building cxx object caffe2 cmakefiles caffe2.dir operators stylizer ops.cc.o 45 building cxx object caffe2 cmakefiles caffe2.dir operators empty op.cc.o 45 building cxx object caffe2 cmakefiles caffe2.dir operators channel stats op.cc.o 45 building cxx object caffe2 cmakefiles caffe2.dir operators sinusoid position encoding op.cc.o 45 building cxx object caffe2 cmakefiles caffe2.dir operators apmeter op.cc.o 45 building cxx object caffe2 cmakefiles caffe2.dir operators elementwise ops.cc.o 45 building cxx object caffe2 cmakefiles caffe2.dir operators key split ops.cc.o 45 building cxx object caffe2 cmakefiles caffe2.dir operators square root divide op.cc.o 45 building cxx object caffe2 cmakefiles caffe2.dir operators reduce ops.cc.o 45 building cxx object caffe2 cmakefiles caffe2.dir operators softmax shared.cc.o 46 building cxx object caffe2 cmakefiles caffe2.dir operators elementwise mul op.cc.o 46 building cxx object caffe2 cmakefiles caffe2.dir operators roi align op.cc.o 46 building cxx object caffe2 cmakefiles caffe2.dir operators minmax gradient ops.cc.o 46 building cxx object caffe2 cmakefiles caffe2.dir operators deform conv gradient op.cc.o 46 building cxx object caffe2 cmakefiles caffe2.dir operators reduce front back sum ops.cc.o 46 building cxx object caffe2 cmakefiles caffe2.dir operators transpose op.cc.o 46 building cxx object caffe2 cmakefiles caffe2.dir operators text file reader utils.cc.o 46 building cxx object caffe2 cmakefiles caffe2.dir operators elu op.cc.o 46 building cxx object caffe2 cmakefiles caffe2.dir operators reduce front back mean ops.cc.o 46 building cxx object caffe2 cmakefiles caffe2.dir operators workspace ops.cc.o 46 building cxx object caffe2 cmakefiles caffe2.dir operators text file reader.cc.o 46 building cxx object caffe2 cmakefiles caffe2.dir operators given tensor fill op.cc.o 46 building cxx object caffe2 cmakefiles caffe2.dir operators utility ops.cc.o 46 building cxx object caffe2 cmakefiles caffe2.dir operators communicator op.cc.o 46 building cxx object caffe2 cmakefiles caffe2.dir operators find duplicate elements op.cc.o 46 building cxx object caffe2 cmakefiles caffe2.dir operators abs op.cc.o 46 building cxx object caffe2 cmakefiles caffe2.dir operators cbrt op.cc.o 46 building cxx object caffe2 cmakefiles caffe2.dir operators batch box cox op.cc.o 46 building cxx object caffe2 cmakefiles caffe2.dir operators sqr op.cc.o 46 building cxx object caffe2 cmakefiles caffe2.dir operators last n window collector.cc.o 46 building cxx object caffe2 cmakefiles caffe2.dir operators tanh op.cc.o 47 building cxx object caffe2 cmakefiles caffe2.dir operators length split op.cc.o 47 building cxx object caffe2 cmakefiles caffe2.dir operators gather fused 8bit rowwise op.cc.o 47 building cxx object caffe2 cmakefiles caffe2.dir operators onnxifi op.cc.o 47 building cxx object caffe2 cmakefiles caffe2.dir operators logit op.cc.o 47 building cxx object caffe2 cmakefiles caffe2.dir operators boolean mask ops.cc.o 47 building cxx object caffe2 cmakefiles caffe2.dir operators stop gradient.cc.o 47 building cxx object caffe2 cmakefiles caffe2.dir operators asin op.cc.o 47 building cxx object caffe2 cmakefiles caffe2.dir operators batch sparse dense op.cc.o 47 building cxx object caffe2 cmakefiles caffe2.dir operators channel shuffle op.cc.o 47 building cxx object caffe2 cmakefiles caffe2.dir operators counter ops.cc.o 47 building cxx object caffe2 cmakefiles caffe2.dir operators sinh op.cc.o 47 building cxx object caffe2 cmakefiles caffe2.dir operators elementwise mul gradient op.cc.o 47 building cxx object caffe2 cmakefiles caffe2.dir operators ngram ops.cc.o 47 building cxx object caffe2 cmakefiles caffe2.dir operators accuracy op.cc.o 47 building cxx object caffe2 cmakefiles caffe2.dir operators reduction ops.cc.o 47 building cxx object caffe2 cmakefiles caffe2.dir operators matmul op.cc.o 47 building cxx object caffe2 cmakefiles caffe2.dir operators rank loss op.cc.o 47 building cxx object caffe2 cmakefiles caffe2.dir operators softplus op.cc.o 47 building cxx object caffe2 cmakefiles caffe2.dir operators conditional op.cc.o 47 building cxx object caffe2 cmakefiles caffe2.dir operators loss op.cc.o 47 building cxx object caffe2 cmakefiles caffe2.dir operators boolean unmask ops.cc.o 47 building cxx object caffe2 cmakefiles caffe2.dir operators weighted multi sampling op.cc.o 48 building cxx object caffe2 cmakefiles caffe2.dir operators fused rowwise random quantization ops.cc.o 48 building cxx object caffe2 cmakefiles caffe2.dir operators reciprocal gradient op.cc.o 48 building cxx object caffe2 cmakefiles caffe2.dir operators softmax loss op.cc.o 48 building cxx object caffe2 cmakefiles caffe2.dir operators numpy tile op.cc.o 48 building cxx object caffe2 cmakefiles caffe2.dir operators piecewise linear transform op.cc.o 48 building cxx object caffe2 cmakefiles caffe2.dir operators space batch op.cc.o 48 building cxx object caffe2 cmakefiles caffe2.dir operators floor op.cc.o 48 building cxx object caffe2 cmakefiles caffe2.dir operators log op.cc.o 48 building cxx object caffe2 cmakefiles caffe2.dir operators elementwise logical ops.cc.o 48 building cxx object caffe2 cmakefiles caffe2.dir operators elementwise add gradient op.cc.o 48 building cxx object caffe2 cmakefiles caffe2.dir operators ceil op.cc.o 48 building cxx object caffe2 cmakefiles caffe2.dir operators index hash ops.cc.o 48 building cxx object caffe2 cmakefiles caffe2.dir operators weighted sample op.cc.o 48 building cxx object caffe2 cmakefiles caffe2.dir operators atomic ops.cc.o 48 building cxx object caffe2 cmakefiles caffe2.dir operators one hot ops.cc.o 48 building cxx object caffe2 cmakefiles caffe2.dir operators elementwise ops schema.cc.o 48 building cxx object caffe2 cmakefiles caffe2.dir operators merge id lists op.cc.o 48 building cxx object caffe2 cmakefiles caffe2.dir operators shape op.cc.o 48 building cxx object caffe2 cmakefiles caffe2.dir operators perplexity op.cc.o 48 building cxx object caffe2 cmakefiles caffe2.dir operators normalize l1 op.cc.o 48 building cxx object caffe2 cmakefiles caffe2.dir operators map ops.cc.o 49 building cxx object caffe2 cmakefiles caffe2.dir operators concat split op.cc.o 49 building cxx object caffe2 cmakefiles caffe2.dir operators load save op.cc.o 49 building cxx object caffe2 cmakefiles caffe2.dir operators find op.cc.o 49 building cxx object caffe2 cmakefiles caffe2.dir operators top k.cc.o 49 building cxx object caffe2 cmakefiles caffe2.dir operators group norm op.cc.o 49 building cxx object caffe2 cmakefiles caffe2.dir operators spatial batch norm op.cc.o 49 building cxx object caffe2 cmakefiles caffe2.dir operators tile op.cc.o 49 building cxx object caffe2 cmakefiles caffe2.dir operators box nms limit op.cc.o 49 building cxx object caffe2 cmakefiles caffe2.dir operators tan op.cc.o 49 building cxx object caffe2 cmakefiles caffe2.dir operators channel backprop stats op.cc.o 49 building cxx object caffe2 cmakefiles caffe2.dir operators fully connected op.cc.o 49 building cxx object caffe2 cmakefiles caffe2.dir operators zero gradient op.cc.o 49 building cxx object caffe2 cmakefiles caffe2.dir operators stump func op.cc.o 49 building cxx object caffe2 cmakefiles caffe2.dir operators fused rowwise 8bit conversion ops.cc.o 49 building cxx object caffe2 cmakefiles caffe2.dir operators negative op.cc.o 49 building cxx object caffe2 cmakefiles caffe2.dir operators softmax op.cc.o 49 building cxx object caffe2 cmakefiles caffe2.dir operators half float ops.cc.o 49 building cxx object caffe2 cmakefiles caffe2.dir operators ctc beam search decoder op.cc.o 49 building cxx object caffe2 cmakefiles caffe2.dir operators given tensor byte string uint8 fill op.cc.o 49 building cxx object caffe2 cmakefiles caffe2.dir operators onnx op.cc.o 49 building cxx object caffe2 cmakefiles caffe2.dir operators negate gradient op.cc.o 49 building cxx object caffe2 cmakefiles caffe2.dir operators pool gradient op.cc.o 50 building cxx object caffe2 cmakefiles caffe2.dir operators flatten op.cc.o 50 building cxx object caffe2 cmakefiles caffe2.dir operators elementwise ops utils.cc.o 50 building cxx object caffe2 cmakefiles caffe2.dir operators summarize op.cc.o 50 building cxx object caffe2 cmakefiles caffe2.dir operators minmax ops.cc.o 50 building cxx object caffe2 cmakefiles caffe2.dir operators lengths tile op.cc.o 50 building cxx object caffe2 cmakefiles caffe2.dir operators im2col op.cc.o 50 building cxx object caffe2 cmakefiles caffe2.dir operators generate proposals op.cc.o 50 building cxx object caffe2 cmakefiles caffe2.dir operators conv op.cc.o 50 building cxx object caffe2 cmakefiles caffe2.dir operators stats ops.cc.o 50 building cxx object caffe2 cmakefiles caffe2.dir operators order switch ops.cc.o 50 building cxx object caffe2 cmakefiles caffe2.dir operators reverse packed segs op.cc.o 50 building cxx object caffe2 cmakefiles caffe2.dir operators norm planar yuv op.cc.o 50 building cxx object caffe2 cmakefiles caffe2.dir operators exp op.cc.o 50 building cxx object caffe2 cmakefiles caffe2.dir operators cos op.cc.o 50 building cxx object caffe2 cmakefiles caffe2.dir operators atan op.cc.o 50 building cxx object caffe2 cmakefiles caffe2.dir operators ctc greedy decoder op.cc.o 50 building cxx object caffe2 cmakefiles caffe2.dir operators scale op.cc.o 50 building cxx object caffe2 cmakefiles caffe2.dir operators conv transpose gradient op.cc.o 50 building cxx object caffe2 cmakefiles caffe2.dir operators instance norm op.cc.o 50 building cxx object caffe2 cmakefiles caffe2.dir operators prelu op.cc.o 50 building cxx object caffe2 cmakefiles caffe2.dir operators batch gather ops.cc.o 50 building cxx object caffe2 cmakefiles caffe2.dir operators expand op.cc.o 51 building cxx object caffe2 cmakefiles caffe2.dir operators cosine embedding criterion op.cc.o 51 building cxx object caffe2 cmakefiles caffe2.dir operators pack segments.cc.o 51 building cxx object caffe2 cmakefiles caffe2.dir operators elementwise add op.cc.o 51 building cxx object caffe2 cmakefiles caffe2.dir operators multi class accuracy op.cc.o 51 building cxx object caffe2 cmakefiles caffe2.dir operators copy op.cc.o 51 building cxx object caffe2 cmakefiles caffe2.dir operators mod op.cc.o 51 building cxx object caffe2 cmakefiles caffe2.dir operators dataset ops.cc.o 51 building cxx object caffe2 cmakefiles caffe2.dir operators affine channel op.cc.o 51 building cxx object caffe2 cmakefiles caffe2.dir operators lengths pad op.cc.o 51 building cxx object caffe2 cmakefiles caffe2.dir operators spatial batch norm gradient op.cc.o 51 building cxx object caffe2 cmakefiles caffe2.dir operators cross entropy op.cc.o 51 building cxx object caffe2 cmakefiles caffe2.dir operators locally connected op.cc.o 51 building cxx object caffe2 cmakefiles caffe2.dir operators rmac regions op.cc.o 51 building cxx object caffe2 cmakefiles caffe2.dir operators variable length sequence padding.cc.o 51 building cxx object caffe2 cmakefiles caffe2.dir operators hard sigmoid op.cc.o 51 building cxx object caffe2 cmakefiles caffe2.dir operators pack rnn sequence op.cc.o 51 building cxx object caffe2 cmakefiles caffe2.dir operators fc inference.cc.o 51 building cxx object caffe2 cmakefiles caffe2.dir operators conv op eigen.cc.o 51 building cxx object caffe2 cmakefiles caffe2.dir operators gather op.cc.o 51 building cxx object caffe2 cmakefiles caffe2.dir operators swish op.cc.o 51 building cxx object caffe2 cmakefiles caffe2.dir operators elementwise sub gradient op.cc.o 52 building cxx object caffe2 cmakefiles caffe2.dir operators conv transpose op.cc.o 52 building cxx object caffe2 cmakefiles caffe2.dir operators quant decode op.cc.o 52 building cxx object caffe2 cmakefiles caffe2.dir operators elementwise linear op.cc.o 52 building cxx object caffe2 cmakefiles caffe2.dir operators layer norm op.cc.o 52 building cxx object caffe2 cmakefiles caffe2.dir operators cube op.cc.o 52 building cxx object caffe2 cmakefiles caffe2.dir operators experimental c10 schemas layer norm.cc.o 52 building cxx object caffe2 cmakefiles caffe2.dir operators experimental c10 schemas batch matmul.cc.o 52 building cxx object caffe2 cmakefiles caffe2.dir operators experimental c10 schemas flatten.cc.o 52 building cxx object caffe2 cmakefiles caffe2.dir operators experimental c10 schemas averaged loss.cc.o 52 building cxx object caffe2 cmakefiles caffe2.dir operators experimental c10 schemas add.cc.o 52 building cxx object caffe2 cmakefiles caffe2.dir operators experimental c10 schemas batch gather.cc.o 52 building cxx object caffe2 cmakefiles caffe2.dir operators experimental c10 schemas filler.cc.o 52 building cxx object caffe2 cmakefiles caffe2.dir operators experimental c10 schemas enforce finite.cc.o 52 building cxx object caffe2 cmakefiles caffe2.dir operators experimental c10 schemas expand dims.cc.o 52 building cxx object caffe2 cmakefiles caffe2.dir operators experimental c10 schemas stop gradient.cc.o 52 building cxx object caffe2 cmakefiles caffe2.dir operators experimental c10 schemas cast.cc.o 52 building cxx object caffe2 cmakefiles caffe2.dir operators experimental c10 schemas mul.cc.o 52 building cxx object caffe2 cmakefiles caffe2.dir operators experimental c10 schemas sigmoid.cc.o 52 building cxx object caffe2 cmakefiles caffe2.dir operators experimental c10 schemas relu.cc.o 52 building cxx object caffe2 cmakefiles caffe2.dir operators experimental c10 schemas fc.cc.o 52 building cxx object caffe2 cmakefiles caffe2.dir operators experimental c10 schemas sparse lengths sum.cc.o 52 building cxx object caffe2 cmakefiles caffe2.dir operators experimental c10 schemas concat.cc.o 53 building cxx object caffe2 cmakefiles caffe2.dir operators experimental c10 schemas sigmoid cross entropy logits.cc.o 53 building cxx object caffe2 cmakefiles caffe2.dir operators experimental c10 cpu expand dims cpu.cc.o 53 building cxx object caffe2 cmakefiles caffe2.dir operators experimental c10 cpu add cpu.cc.o 53 building cxx object caffe2 cmakefiles caffe2.dir operators experimental c10 cpu fc cpu.cc.o 53 building cxx object caffe2 cmakefiles caffe2.dir operators experimental c10 cpu concat cpu.cc.o 53 building cxx object caffe2 cmakefiles caffe2.dir operators experimental c10 cpu relu cpu.cc.o 53 building cxx object caffe2 cmakefiles caffe2.dir operators experimental c10 cpu batch matmul cpu.cc.o 53 building cxx object caffe2 cmakefiles caffe2.dir operators experimental c10 cpu flatten cpu.cc.o 53 building cxx object caffe2 cmakefiles caffe2.dir operators experimental c10 cpu sparse lengths sum cpu.cc.o 53 building cxx object caffe2 cmakefiles caffe2.dir operators experimental c10 cpu sigmoid cross entropy logits cpu.cc.o 53 building cxx object caffe2 cmakefiles caffe2.dir operators experimental c10 cpu sigmoid cpu.cc.o 53 building cxx object caffe2 cmakefiles caffe2.dir operators experimental c10 cpu filler cpu.cc.o 53 building cxx object caffe2 cmakefiles caffe2.dir operators experimental c10 cpu averaged loss cpu.cc.o 53 building cxx object caffe2 cmakefiles caffe2.dir operators experimental c10 cpu cast cpu.cc.o 53 building cxx object caffe2 cmakefiles caffe2.dir operators experimental c10 cpu stop gradient cpu.cc.o 53 building cxx object caffe2 cmakefiles caffe2.dir operators experimental c10 cpu batch gather cpu.cc.o 53 building cxx object caffe2 cmakefiles caffe2.dir operators experimental c10 cpu enforce finite cpu.cc.o 53 building cxx object caffe2 cmakefiles caffe2.dir operators experimental c10 cpu mul cpu.cc.o 53 building cxx object caffe2 cmakefiles caffe2.dir operators rnn recurrent network op.cc.o 53 building cxx object caffe2 cmakefiles caffe2.dir operators rnn recurrent network executor.cc.o 53 building cxx object caffe2 cmakefiles caffe2.dir operators rnn recurrent network blob fetcher op.cc.o 53 building cxx object caffe2 cmakefiles caffe2.dir operators quantized init qnnpack.cc.o 54 building cxx object caffe2 cmakefiles caffe2.dir operators quantized int8 add op.cc.o 54 building cxx object caffe2 cmakefiles caffe2.dir operators quantized int8 average pool op.cc.o 54 building cxx object caffe2 cmakefiles caffe2.dir operators quantized int8 channel shuffle op.cc.o 54 building cxx object caffe2 cmakefiles caffe2.dir operators quantized int8 concat op.cc.o 54 building cxx object caffe2 cmakefiles caffe2.dir operators quantized int8 conv op.cc.o 54 building cxx object caffe2 cmakefiles caffe2.dir operators quantized int8 conv transpose op.cc.o 54 building cxx object caffe2 cmakefiles caffe2.dir operators quantized int8 dequantize op.cc.o 54 building cxx object caffe2 cmakefiles caffe2.dir operators quantized int8 fc op.cc.o 54 building cxx object caffe2 cmakefiles caffe2.dir operators quantized int8 flatten op.cc.o 54 building cxx object caffe2 cmakefiles caffe2.dir operators quantized int8 given tensor fill op.cc.o 54 building cxx object caffe2 cmakefiles caffe2.dir operators quantized int8 leaky relu op.cc.o 54 building cxx object caffe2 cmakefiles caffe2.dir operators quantized int8 max pool op.cc.o 54 building cxx object caffe2 cmakefiles caffe2.dir operators quantized int8 quantize op.cc.o 54 building cxx object caffe2 cmakefiles caffe2.dir operators quantized int8 relu op.cc.o 54 building cxx object caffe2 cmakefiles caffe2.dir operators quantized int8 reshape op.cc.o 54 building cxx object caffe2 cmakefiles caffe2.dir operators quantized int8 resize nearest op.cc.o 54 building cxx object caffe2 cmakefiles caffe2.dir operators quantized int8 roi align op.cc.o 54 building cxx object caffe2 cmakefiles caffe2.dir operators quantized int8 slice op.cc.o 54 building cxx object caffe2 cmakefiles caffe2.dir operators quantized int8 sigmoid op.cc.o 54 building cxx object caffe2 cmakefiles caffe2.dir operators quantized int8 softmax op.cc.o 54 building cxx object caffe2 cmakefiles caffe2.dir opt distributed.cc.o 55 building cxx object caffe2 cmakefiles caffe2.dir opt optimize ideep.cc.o 55 building cxx object caffe2 cmakefiles caffe2.dir opt distributed converter.cc.o 55 building cxx object caffe2 cmakefiles caffe2.dir opt dead code elim.cc.o 55 building cxx object caffe2 cmakefiles caffe2.dir opt mobile.cc.o 55 building cxx object caffe2 cmakefiles caffe2.dir opt optimizer.cc.o 55 building cxx object caffe2 cmakefiles caffe2.dir opt sink.cc.o 55 building cxx object caffe2 cmakefiles caffe2.dir opt onnxifi transformer.cc.o 55 building cxx object caffe2 cmakefiles caffe2.dir opt passes.cc.o 55 building cxx object caffe2 cmakefiles caffe2.dir opt annotations.cc.o 55 building cxx object caffe2 cmakefiles caffe2.dir opt converter.cc.o 55 building cxx object caffe2 cmakefiles caffe2.dir opt backend cutting.cc.o 55 building cxx object caffe2 cmakefiles caffe2.dir opt fusion.cc.o 55 building cxx object caffe2 cmakefiles caffe2.dir opt device.cc.o 55 building cxx object caffe2 cmakefiles caffe2.dir perfkernels typed axpy.cc.o 55 building cxx object caffe2 cmakefiles caffe2.dir perfkernels fused 8bit rowwise embedding lookup.cc.o 55 building cxx object caffe2 cmakefiles caffe2.dir perfkernels adagrad.cc.o 55 building cxx object caffe2 cmakefiles caffe2.dir perfkernels embedding lookup.cc.o 55 building cxx object caffe2 cmakefiles caffe2.dir perfkernels math cpu base.cc.o 55 building cxx object caffe2 cmakefiles caffe2.dir queue queue ops.cc.o 55 building cxx object caffe2 cmakefiles caffe2.dir queue rebatching queue ops.cc.o 55 building cxx object caffe2 cmakefiles caffe2.dir queue rebatching queue.cc.o 55 building cxx object caffe2 cmakefiles caffe2.dir queue blobs queue.cc.o 56 building cxx object caffe2 cmakefiles caffe2.dir queue blobs queue db.cc.o 56 building cxx object caffe2 cmakefiles caffe2.dir sgd momentum sgd op.cc.o 56 building cxx object caffe2 cmakefiles caffe2.dir sgd adam op.cc.o 56 building cxx object caffe2 cmakefiles caffe2.dir sgd learning rate adaption op.cc.o 56 building cxx object caffe2 cmakefiles caffe2.dir sgd clip tensor op.cc.o 56 building cxx object caffe2 cmakefiles caffe2.dir sgd yellowfin op.cc.o 56 building cxx object caffe2 cmakefiles caffe2.dir sgd wngrad op.cc.o 56 building cxx object caffe2 cmakefiles caffe2.dir sgd gftrl op.cc.o 56 building cxx object caffe2 cmakefiles caffe2.dir sgd iter op.cc.o 56 building cxx object caffe2 cmakefiles caffe2.dir sgd learning rate op.cc.o 56 building cxx object caffe2 cmakefiles caffe2.dir sgd ftrl op.cc.o 56 building cxx object caffe2 cmakefiles caffe2.dir sgd adadelta op.cc.o 56 building cxx object caffe2 cmakefiles caffe2.dir sgd lars op.cc.o 56 building cxx object caffe2 cmakefiles caffe2.dir sgd adagrad op.cc.o 56 building cxx object caffe2 cmakefiles caffe2.dir sgd rmsprop op.cc.o 56 building cxx object caffe2 cmakefiles caffe2.dir share contrib nnpack conv op.cc.o 56 building cxx object caffe2 cmakefiles caffe2.dir share contrib depthwise depthwise3x3 conv op.cc.o 56 building cxx object caffe2 cmakefiles caffe2.dir transforms single op transform.cc.o 56 building cxx object caffe2 cmakefiles caffe2.dir transforms conv nnpack transform.cc.o 56 building cxx object caffe2 cmakefiles caffe2.dir transforms pattern net transform.cc.o 56 building cxx object caffe2 cmakefiles caffe2.dir transforms common subexpression elimination.cc.o 57 linking cxx shared library .. lib libcaffe2.so 57 built target caffe2 makefile 138 recipe target 'all' failed make error 2 failed run 'bash .. tools build pytorch libs.sh use cuda use nnpack use mkldnn use qnnpack caffe2' reproduce steps reproduce behavior 1. 1. 1. expected behavior environment please copy paste output environment collection script https raw.githubusercontent.com pytorch pytorch master torch utils collect env.py fill checklist manually . get script run pytorch version e.g., 1.0 os e.g., linux installed pytorch , , source build command used compiling source python version cuda cudnn version gpu models configuration relevant information additional context",0,failed run 'bash .. tools build pytorch libs.sh use cuda use nnpack use mkldnn use qnnpack caffe2',"failed run 'bash .. tools build pytorch libs.sh use cuda use nnpack use mkldnn use qnnpack caffe2' bug build files written home feng pytorch build make install j12 scanning dependencies target js embed scanning dependencies target benchmark scanning dependencies target nccl external scanning dependencies target pthreadpool scanning dependencies target clog scanning dependencies target gtest scanning dependencies target gloo scanning dependencies target onnxifi dummy scanning dependencies target onnxifi loader scanning dependencies target libprotobuf lite scanning dependencies target libprotobuf 0 creating directories 'nccl external' scanning dependencies target mkldnn 1 building cxx object third party benchmark src cmakefiles benchmark.dir json reporter.cc.o 1 building cxx object third party protobuf cmake cmakefiles js embed.dir src google protobuf compiler js embed.cc.o 1 building c object confu deps clog cmakefiles clog.dir src clog.c.o 1 download step 'nccl external' 1 patch step 'nccl external' 1 update step 'nccl external' 1 configure step 'nccl external' 1 performing build step 'nccl external' 1 building c object third party onnx cmakefiles onnxifi loader.dir onnx onnxifi loader.c.o 1 building c object third party onnx cmakefiles onnxifi dummy.dir onnx onnxifi dummy.c.o 1 building c object confu deps pthreadpool cmakefiles pthreadpool.dir src threadpool pthreads.c.o make 3 warning jobserver unavailable using j1. add ' ' parent make rule. home feng pytorch third party qnnpack deps clog src clog.c function clog vlog fatal home feng pytorch third party qnnpack deps clog src clog.c 120 4 warning ignoring return value write , declared attribute warn unused result wunused result write stderr fileno, buffer, prefix chars format chars clog suffix length home feng pytorch third party qnnpack deps clog src clog.c function clog vlog error home feng pytorch third party qnnpack deps clog src clog.c 196 4 warning ignoring return value write , declared attribute warn unused result wunused result write stderr fileno, buffer, prefix chars format chars clog suffix length home feng pytorch third party qnnpack deps clog src clog.c function clog vlog warning home feng pytorch third party qnnpack deps clog src clog.c 272 4 warning ignoring return value write , declared attribute warn unused result wunused result write stderr fileno, buffer, prefix chars format chars clog suffix length home feng pytorch third party qnnpack deps clog src clog.c function clog vlog info home feng pytorch third party qnnpack deps clog src clog.c 348 4 warning ignoring return value write , declared attribute warn unused result wunused result write stdout fileno, buffer, prefix chars format chars clog suffix length home feng pytorch third party qnnpack deps clog src clog.c function clog vlog debug home feng pytorch third party qnnpack deps clog src clog.c 424 4 warning ignoring return value write , declared attribute warn unused result wunused result write stdout fileno, buffer, prefix chars format chars clog suffix length 1 building cxx object third party gloo gloo cmakefiles gloo.dir algorithm.cc.o 1 building cxx object third party protobuf cmake cmakefiles libprotobuf lite.dir src google protobuf arena.cc.o generating nccl.h.in home feng pytorch build nccl include nccl.h compiling init.cu home feng pytorch build nccl obj init.o nvcc warning 'compute 20', 'sm 20', 'sm 21' architectures deprecated, may removed future release use wno deprecated gpu targets suppress warning . 1 building cxx object third party googletest googletest cmakefiles gtest.dir src gtest all.cc.o 1 linking cxx executable .. .. .. bin js embed 1 building cxx object third party protobuf cmake cmakefiles libprotobuf lite.dir src google protobuf arenastring.cc.o 1 building cxx object third party benchmark src cmakefiles benchmark.dir string util.cc.o 1 linking c shared library .. .. lib libonnxifi dummy.so 1 linking c static library .. .. lib libpthreadpool.a 1 linking c static library .. .. lib libclog.a 1 building cxx object third party gloo gloo cmakefiles gloo.dir allgather.cc.o 1 linking c static library .. .. lib libonnxifi loader.a 1 building cxx object third party ideep mkl dnn src cmakefiles mkldnn.dir common primitive.cpp.o 1 built target onnxifi dummy 1 built target js embed 1 building cxx object third party ideep mkl dnn src cmakefiles mkldnn.dir common pooling.cpp.o 1 building cxx object third party protobuf cmake cmakefiles libprotobuf lite.dir src google protobuf extension set.cc.o 1 building cxx object third party protobuf cmake cmakefiles libprotobuf.dir src google protobuf arena.cc.o 1 built target onnxifi loader 1 built target pthreadpool 1 built target clog 1 building cxx object third party protobuf cmake cmakefiles libprotobuf.dir src google protobuf arenastring.cc.o scanning dependencies target python copy files scanning dependencies target c10 1 building cxx object c10 cmakefiles c10.dir devicetype.cpp.o 1 building cxx object third party ideep mkl dnn src cmakefiles mkldnn.dir common engine.cpp.o 1 building cxx object third party ideep mkl dnn src cmakefiles mkldnn.dir common query.cpp.o 1 building cxx object third party protobuf cmake cmakefiles libprotobuf lite.dir src google protobuf generated message table driven lite.cc.o 1 building cxx object third party gloo gloo cmakefiles gloo.dir allreduce.cc.o 1 building cxx object third party ideep mkl dnn src cmakefiles mkldnn.dir common softmax.cpp.o 1 building cxx object c10 cmakefiles c10.dir half.cpp.o 1 building cxx object third party ideep mkl dnn src cmakefiles mkldnn.dir common mkldnn debug.cpp.o 1 building cxx object third party ideep mkl dnn src cmakefiles mkldnn.dir common batch normalization.cpp.o 2 building cxx object third party protobuf cmake cmakefiles libprotobuf.dir src google protobuf extension set.cc.o 2 building cxx object third party ideep mkl dnn src cmakefiles mkldnn.dir common scratchpad.cpp.o 2 building cxx object third party benchmark src cmakefiles benchmark.dir commandlineflags.cc.o 3 building cxx object third party ideep mkl dnn src cmakefiles mkldnn.dir common primitive attr.cpp.o 3 building cxx object third party protobuf cmake cmakefiles libprotobuf.dir src google protobuf generated message table driven lite.cc.o 3 building cxx object c10 cmakefiles c10.dir device.cpp.o 3 building cxx object third party ideep mkl dnn src cmakefiles mkldnn.dir common verbose.cpp.o 3 building cxx object third party ideep mkl dnn src cmakefiles mkldnn.dir common eltwise.cpp.o 3 building cxx object third party benchmark src cmakefiles benchmark.dir sleep.cc.o 3 building cxx object third party ideep mkl dnn src cmakefiles mkldnn.dir common memory desc wrapper.cpp.o 3 building cxx object third party ideep mkl dnn src cmakefiles mkldnn.dir common primitive iterator.cpp.o 3 building cxx object third party ideep mkl dnn src cmakefiles mkldnn.dir common lrn.cpp.o 3 building cxx object third party gloo gloo cmakefiles gloo.dir allreduce local.cc.o 3 building cxx object c10 cmakefiles c10.dir stream.cpp.o 3 building cxx object third party ideep mkl dnn src cmakefiles mkldnn.dir common memory.cpp.o 3 building cxx object third party benchmark src cmakefiles benchmark.dir statistics.cc.o 3 building cxx object third party ideep mkl dnn src cmakefiles mkldnn.dir common inner product.cpp.o 3 building cxx object c10 cmakefiles c10.dir core dispatch opschema.cpp.o 3 building cxx object c10 cmakefiles c10.dir core dispatch kernelregistration.cpp.o 3 building cxx object c10 cmakefiles c10.dir core dispatch deviceid.cpp.o 3 building cxx object third party gloo gloo cmakefiles gloo.dir broadcast.cc.o 3 building cxx object third party ideep mkl dnn src cmakefiles mkldnn.dir common stream.cpp.o 3 building cxx object third party ideep mkl dnn src cmakefiles mkldnn.dir common convolution relu.cpp.o 3 building cxx object third party protobuf cmake cmakefiles libprotobuf lite.dir src google protobuf generated message util.cc.o 3 building cxx object third party protobuf cmake cmakefiles libprotobuf lite.dir src google protobuf io coded stream.cc.o 3 building cxx object third party ideep mkl dnn src cmakefiles mkldnn.dir common rnn.cpp.o 3 building cxx object third party ideep mkl dnn src cmakefiles mkldnn.dir common reorder.cpp.o 3 building cxx object c10 cmakefiles c10.dir core dispatch dispatchkey.cpp.o ptxas warning big maxrregcount value specified 96, ignored 3 building cxx object third party protobuf cmake cmakefiles libprotobuf.dir src google protobuf generated message util.cc.o 3 building cxx object third party gloo gloo cmakefiles gloo.dir context.cc.o 3 building cxx object third party ideep mkl dnn src cmakefiles mkldnn.dir common convolution.cpp.o 3 building cxx object third party benchmark src cmakefiles benchmark.dir benchmark.cc.o 3 building cxx object third party ideep mkl dnn src cmakefiles mkldnn.dir common primitive desc.cpp.o 3 building cxx object c10 cmakefiles c10.dir core dispatch dispatcher.cpp.o 3 building cxx object c10 cmakefiles c10.dir core dispatch opschemaregistration.cpp.o 3 building cxx object third party ideep mkl dnn src cmakefiles mkldnn.dir common deconvolution.cpp.o 3 building cxx object third party ideep mkl dnn src cmakefiles mkldnn.dir common utils.cpp.o 3 building cxx object third party ideep mkl dnn src cmakefiles mkldnn.dir cpu jit sse42 conv kernel f32.cpp.o 3 building cxx object third party protobuf cmake cmakefiles libprotobuf lite.dir src google protobuf io zero copy stream.cc.o 3 building cxx object third party gloo gloo cmakefiles gloo.dir gather.cc.o 3 building cxx object third party ideep mkl dnn src cmakefiles mkldnn.dir cpu cpu batch normalization utils.cpp.o init.cu 52 1 warning ncclnet initialized declared extern ncclnet ncclnet null 3 building cxx object c10 cmakefiles c10.dir core dispatch dispatchtable.cpp.o 3 building cxx object c10 cmakefiles c10.dir core dispatch layoutid.cpp.o 3 building cxx object third party gloo gloo cmakefiles gloo.dir reduce.cc.o 3 building cxx object third party protobuf cmake cmakefiles libprotobuf lite.dir src google protobuf io zero copy stream impl lite.cc.o 3 building cxx object c10 cmakefiles c10.dir impl deviceguardimplinterface.cpp.o 4 building cxx object third party protobuf cmake cmakefiles libprotobuf lite.dir src google protobuf message lite.cc.o 4 building cxx object c10 cmakefiles c10.dir util type.cpp.o 4 building cxx object third party protobuf cmake cmakefiles libprotobuf.dir src google protobuf io coded stream.cc.o 4 building cxx object third party protobuf cmake cmakefiles libprotobuf lite.dir src google protobuf repeated field.cc.o nvcc warning 'compute 20', 'sm 20', 'sm 21' architectures deprecated, may removed future release use wno deprecated gpu targets suppress warning . 4 building cxx object third party ideep mkl dnn src cmakefiles mkldnn.dir cpu ref lrn.cpp.o 4 building cxx object c10 cmakefiles c10.dir util backtrace.cpp.o 4 building cxx object c10 cmakefiles c10.dir util optional.cpp.o 4 building cxx object third party gloo gloo cmakefiles gloo.dir scatter.cc.o 4 building cxx object c10 cmakefiles c10.dir util c 17.cpp.o compiling ring.cu home feng pytorch build nccl obj ring.o nvcc warning 'compute 20', 'sm 20', 'sm 21' architectures deprecated, may removed future release use wno deprecated gpu targets suppress warning . 5 building cxx object c10 cmakefiles c10.dir util smallvector.cpp.o 5 building cxx object third party benchmark src cmakefiles benchmark.dir csv reporter.cc.o 5 building cxx object third party protobuf cmake cmakefiles libprotobuf lite.dir src google protobuf stubs atomicops internals x86 gcc.cc.o 5 building cxx object third party protobuf cmake cmakefiles libprotobuf lite.dir src google protobuf stubs atomicops internals x86 msvc.cc.o 5 building cxx object c10 cmakefiles c10.dir util leftright.cpp.o 5 building cxx object c10 cmakefiles c10.dir util flags use gflags.cpp.o 5 building cxx object third party protobuf cmake cmakefiles libprotobuf.dir src google protobuf io zero copy stream.cc.o 5 building cxx object third party ideep mkl dnn src cmakefiles mkldnn.dir cpu jit avx512 common 1x1 conv kernel.cpp.o 5 linking cxx static library .. .. .. lib libgtest.a 5 built target gtest scanning dependencies target aten cpu files gen target 5 generating .. aten src aten cpubytetype.cpp, .. aten src aten cpubytetype.h, .. aten src aten cpuchartype.cpp, .. aten src aten cpuchartype.h, .. aten src aten cpucopy.cpp, .. aten src aten cpudoubletype.cpp, .. aten src aten cpudoubletype.h, .. aten src aten cpufloattype.cpp, .. aten src aten cpufloattype.h, .. aten src aten cpugenerator.h, .. aten src aten cpuhalftype.cpp, .. aten src aten cpuhalftype.h, .. aten src aten cpuinttype.cpp, .. aten src aten cpuinttype.h, .. aten src aten cpulongtype.cpp, .. aten src aten cpulongtype.h, .. aten src aten cpushorttype.cpp, .. aten src aten cpushorttype.h, .. aten src aten declarations.yaml, .. aten src aten functions.h, .. aten src aten nativefunctions.h, .. aten src aten registercpu.cpp, .. aten src aten registercpu.h, .. aten src aten sparsecpubytetype.cpp, .. aten src aten sparsecpubytetype.h, .. aten src aten sparsecpuchartype.cpp, .. aten src aten sparsecpuchartype.h, .. aten src aten sparsecpudoubletype.cpp, .. aten src aten sparsecpudoubletype.h, .. aten src aten sparsecpufloattype.cpp, .. aten src aten sparsecpufloattype.h, .. aten src aten sparsecpuinttype.cpp, .. aten src aten sparsecpuinttype.h, .. aten src aten sparsecpulongtype.cpp, .. aten src aten sparsecpulongtype.h, .. aten src aten sparsecpushorttype.cpp, .. aten src aten sparsecpushorttype.h, .. aten src aten typedefault.cpp, .. aten src aten typedefault.h, .. aten src aten typeextendedinterface.h, .. aten src aten cudabytetype.cpp, .. aten src aten cudabytetype.h, .. aten src aten cudachartype.cpp, .. aten src aten cudachartype.h, .. aten src aten cudacopy.cpp, .. aten src aten cudadoubletype.cpp, .. aten src aten cudadoubletype.h, .. aten src aten cudafloattype.cpp, .. aten src aten cudafloattype.h, .. aten src aten cudagenerator.h, .. aten src aten cudahalftype.cpp, .. aten src aten cudahalftype.h, .. aten src aten cudainttype.cpp, .. aten src aten cudainttype.h, .. aten src aten cudalongtype.cpp, .. aten src aten cudalongtype.h, .. aten src aten cudashorttype.cpp, .. aten src aten cudashorttype.h, .. aten src aten registercuda.cpp, .. aten src aten registercuda.h, .. aten src aten sparsecudabytetype.cpp, .. aten src aten sparsecudabytetype.h, .. aten src aten sparsecudachartype.cpp, .. aten src aten sparsecudachartype.h, .. aten src aten sparsecudadoubletype.cpp, .. aten src aten sparsecudadoubletype.h, .. aten src aten sparsecudafloattype.cpp, .. aten src aten sparsecudafloattype.h, .. aten src aten sparsecudainttype.cpp, .. aten src aten sparsecudainttype.h, .. aten src aten sparsecudalongtype.cpp, .. aten src aten sparsecudalongtype.h, .. aten src aten sparsecudashorttype.cpp, .. aten src aten sparsecudashorttype.h 6 building cxx object third party gloo gloo cmakefiles gloo.dir types.cc.o 6 building cxx object third party ideep mkl dnn src cmakefiles mkldnn.dir cpu ref deconvolution.cpp.o 7 building cxx object third party ideep mkl dnn src cmakefiles mkldnn.dir cpu jit sse42 1x1 conv kernel f32.cpp.o 7 built target python copy files scanning dependencies target common 7 building cxx object third party protobuf cmake cmakefiles libprotobuf lite.dir src google protobuf stubs bytestream.cc.o 7 building c object sleef src common cmakefiles common.dir common.c.o 7 building cxx object third party protobuf cmake cmakefiles libprotobuf lite.dir src google protobuf stubs common.cc.o 7 built target common scanning dependencies target mkrename 7 building c object sleef src libm cmakefiles mkrename.dir mkrename.c.o 7 building cxx object third party gloo gloo cmakefiles gloo.dir common linux.cc.o 7 linking c executable .. .. bin mkrename 7 building cxx object third party benchmark src cmakefiles benchmark.dir reporter.cc.o 7 built target mkrename 7 building cxx object third party benchmark src cmakefiles benchmark.dir complexity.cc.o 7 building cxx object third party protobuf cmake cmakefiles libprotobuf.dir src google protobuf io zero copy stream impl lite.cc.o 7 building cxx object c10 cmakefiles c10.dir util array.cpp.o 7 building cxx object third party ideep mkl dnn src cmakefiles mkldnn.dir cpu cpu sum.cpp.o 7 building cxx object c10 cmakefiles c10.dir util logging.cpp.o 7 building cxx object third party protobuf cmake cmakefiles libprotobuf lite.dir src google protobuf stubs int128.cc.o ptxas warning big maxrregcount value specified 96, ignored 7 building cxx object third party benchmark src cmakefiles benchmark.dir counter.cc.o 7 building cxx object third party protobuf cmake cmakefiles libprotobuf.dir src google protobuf message lite.cc.o 7 building cxx object third party protobuf cmake cmakefiles libprotobuf lite.dir src google protobuf stubs io win32.cc.o 7 building cxx object third party protobuf cmake cmakefiles libprotobuf lite.dir src google protobuf stubs once.cc.o 7 building cxx object third party benchmark src cmakefiles benchmark.dir sysinfo.cc.o 7 building cxx object third party protobuf cmake cmakefiles libprotobuf lite.dir src google protobuf stubs status.cc.o 7 building cxx object third party benchmark src cmakefiles benchmark.dir colorprint.cc.o 7 building cxx object third party protobuf cmake cmakefiles libprotobuf lite.dir src google protobuf stubs statusor.cc.o nvcc warning 'compute 20', 'sm 20', 'sm 21' architectures deprecated, may removed future release use wno deprecated gpu targets suppress warning . 7 building cxx object third party gloo gloo cmakefiles gloo.dir common logging.cc.o 7 building cxx object third party gloo gloo cmakefiles gloo.dir mpi context.cc.o 7 building cxx object c10 cmakefiles c10.dir util exception.cpp.o compiling bootstrap.cu home feng pytorch build nccl obj bootstrap.o nvcc warning 'compute 20', 'sm 20', 'sm 21' architectures deprecated, may removed future release use wno deprecated gpu targets suppress warning . 7 building cxx object third party protobuf cmake cmakefiles libprotobuf.dir src google protobuf repeated field.cc.o 7 building cxx object third party protobuf cmake cmakefiles libprotobuf lite.dir src google protobuf stubs stringpiece.cc.o 7 building cxx object third party protobuf cmake cmakefiles libprotobuf lite.dir src google protobuf stubs stringprintf.cc.o 7 building cxx object third party benchmark src cmakefiles benchmark.dir timers.cc.o 7 building cxx object third party ideep mkl dnn src cmakefiles mkldnn.dir cpu jit avx512 common convolution.cpp.o 7 building cxx object third party ideep mkl dnn src cmakefiles mkldnn.dir cpu jit avx512 common conv winograd kernel f32.cpp.o 7 building cxx object third party gloo gloo cmakefiles gloo.dir rendezvous context.cc.o 7 building cxx object third party benchmark src cmakefiles benchmark.dir benchmark register.cc.o 7 building cxx object third party protobuf cmake cmakefiles libprotobuf.dir src google protobuf stubs atomicops internals x86 gcc.cc.o 7 building cxx object c10 cmakefiles c10.dir util typelist.cpp.o 7 building cxx object third party protobuf cmake cmakefiles libprotobuf lite.dir src google protobuf stubs structurally valid.cc.o 7 building cxx object third party protobuf cmake cmakefiles libprotobuf lite.dir src google protobuf stubs strutil.cc.o 7 building cxx object third party benchmark src cmakefiles benchmark.dir console reporter.cc.o 7 building cxx object c10 cmakefiles c10.dir util typeid.cpp.o 7 building cxx object third party protobuf cmake cmakefiles libprotobuf.dir src google protobuf stubs atomicops internals x86 msvc.cc.o 7 building cxx object third party gloo gloo cmakefiles gloo.dir rendezvous file store.cc.o 7 building cxx object third party gloo gloo cmakefiles gloo.dir rendezvous hash store.cc.o 7 building cxx object third party protobuf cmake cmakefiles libprotobuf lite.dir src google protobuf stubs time.cc.o ptxas warning big maxrregcount value specified 96, ignored 7 building cxx object third party protobuf cmake cmakefiles libprotobuf lite.dir src google protobuf wire format lite.cc.o 7 building cxx object third party protobuf cmake cmakefiles libprotobuf.dir src google protobuf stubs bytestream.cc.o 7 building cxx object c10 cmakefiles c10.dir util tensortypeid.cpp.o 7 building cxx object third party gloo gloo cmakefiles gloo.dir rendezvous prefix store.cc.o 7 building cxx object c10 cmakefiles c10.dir util metaprogramming.cpp.o 7 building cxx object c10 cmakefiles c10.dir util typetraits.cpp.o 7 building cxx object third party gloo gloo cmakefiles gloo.dir rendezvous store.cc.o 7 building cxx object third party gloo gloo cmakefiles gloo.dir transport address.cc.o 7 building cxx object third party gloo gloo cmakefiles gloo.dir transport buffer.cc.o 7 building cxx object c10 cmakefiles c10.dir util stringutil.cpp.o 7 building cxx object third party gloo gloo cmakefiles gloo.dir transport context.cc.o 7 building cxx object third party protobuf cmake cmakefiles libprotobuf.dir src google protobuf stubs common.cc.o 7 building cxx object c10 cmakefiles c10.dir util tensortypeidregistration.cpp.o 7 building cxx object third party gloo gloo cmakefiles gloo.dir transport device.cc.o 7 building cxx object third party gloo gloo cmakefiles gloo.dir transport pair.cc.o nvcc warning 'compute 20', 'sm 20', 'sm 21' architectures deprecated, may removed future release use wno deprecated gpu targets suppress warning . 7 building cxx object c10 cmakefiles c10.dir util flags use gflags.cpp.o 7 building cxx object third party gloo gloo cmakefiles gloo.dir transport unbound buffer.cc.o 7 building cxx object third party gloo gloo cmakefiles gloo.dir transport tcp address.cc.o 7 building cxx object third party gloo gloo cmakefiles gloo.dir transport tcp buffer.cc.o 7 building cxx object third party gloo gloo cmakefiles gloo.dir transport tcp context.cc.o 7 building cxx object third party ideep mkl dnn src cmakefiles mkldnn.dir cpu gemm inner product.cpp.o compiling transport.cu home feng pytorch build nccl obj transport.o nvcc warning 'compute 20', 'sm 20', 'sm 21' architectures deprecated, may removed future release use wno deprecated gpu targets suppress warning . 7 linking cxx static library .. .. .. lib libprotobuf lite.a 7 built target libprotobuf lite 7 building cxx object third party gloo gloo cmakefiles gloo.dir transport tcp device.cc.o scanning dependencies target mkdisp 7 building c object sleef src libm cmakefiles mkdisp.dir mkdisp.c.o 7 linking c executable .. .. bin mkdisp 7 built target mkdisp scanning dependencies target renamedsp256.h generated 7 generating renamedsp256.h 7 built target renamedsp256.h generated scanning dependencies target dispavx.c generated 7 generating dispavx.c 7 built target dispavx.c generated scanning dependencies target renamesse2.h generated 7 generating include renamesse2.h 7 building cxx object third party gloo gloo cmakefiles gloo.dir transport tcp pair.cc.o generating renamesse2.h mkrename 2 4 sse2 7 built target renamesse2.h generated scanning dependencies target renamefma4.h generated 7 generating include renamefma4.h generating renamefma4.h mkrename 4 8 fma4 7 built target renamefma4.h generated scanning dependencies target renameavx2.h generated 7 generating include renameavx2.h generating renameavx2.h mkrename 4 8 avx2 7 built target renameavx2.h generated scanning dependencies target renameavx2128.h generated 7 generating include renameavx2128.h generating renameavx2128.h mkrename 2 4 avx2128 7 built target renameavx2128.h generated scanning dependencies target renamesse4.h generated 8 generating include renamesse4.h generating renamesse4.h mkrename 2 4 sse4 8 built target renamesse4.h generated scanning dependencies target aten cuda files gen target 8 building cxx object third party protobuf cmake cmakefiles libprotobuf.dir src google protobuf stubs int128.cc.o 8 generating .. aten src aten cpubytetype.cpp, .. aten src aten cpubytetype.h, .. aten src aten cpuchartype.cpp, .. aten src aten cpuchartype.h, .. aten src aten cpucopy.cpp, .. aten src aten cpudoubletype.cpp, .. aten src aten cpudoubletype.h, .. aten src aten cpufloattype.cpp, .. aten src aten cpufloattype.h, .. aten src aten cpugenerator.h, .. aten src aten cpuhalftype.cpp, .. aten src aten cpuhalftype.h, .. aten src aten cpuinttype.cpp, .. aten src aten cpuinttype.h, .. aten src aten cpulongtype.cpp, .. aten src aten cpulongtype.h, .. aten src aten cpushorttype.cpp, .. aten src aten cpushorttype.h, .. aten src aten declarations.yaml, .. aten src aten functions.h, .. aten src aten nativefunctions.h, .. aten src aten registercpu.cpp, .. aten src aten registercpu.h, .. aten src aten sparsecpubytetype.cpp, .. aten src aten sparsecpubytetype.h, .. aten src aten sparsecpuchartype.cpp, .. aten src aten sparsecpuchartype.h, .. aten src aten sparsecpudoubletype.cpp, .. aten src aten sparsecpudoubletype.h, .. aten src aten sparsecpufloattype.cpp, .. aten src aten sparsecpufloattype.h, .. aten src aten sparsecpuinttype.cpp, .. aten src aten sparsecpuinttype.h, .. aten src aten sparsecpulongtype.cpp, .. aten src aten sparsecpulongtype.h, .. aten src aten sparsecpushorttype.cpp, .. aten src aten sparsecpushorttype.h, .. aten src aten typedefault.cpp, .. aten src aten typedefault.h, .. aten src aten typeextendedinterface.h, .. aten src aten cudabytetype.cpp, .. aten src aten cudabytetype.h, .. aten src aten cudachartype.cpp, .. aten src aten cudachartype.h, .. aten src aten cudacopy.cpp, .. aten src aten cudadoubletype.cpp, .. aten src aten cudadoubletype.h, .. aten src aten cudafloattype.cpp, .. aten src aten cudafloattype.h, .. aten src aten cudagenerator.h, .. aten src aten cudahalftype.cpp, .. aten src aten cudahalftype.h, .. aten src aten cudainttype.cpp, .. aten src aten cudainttype.h, .. aten src aten cudalongtype.cpp, .. aten src aten cudalongtype.h, .. aten src aten cudashorttype.cpp, .. aten src aten cudashorttype.h, .. aten src aten registercuda.cpp, .. aten src aten registercuda.h, .. aten src aten sparsecudabytetype.cpp, .. aten src aten sparsecudabytetype.h, .. aten src aten sparsecudachartype.cpp, .. aten src aten sparsecudachartype.h, .. aten src aten sparsecudadoubletype.cpp, .. aten src aten sparsecudadoubletype.h, .. aten src aten sparsecudafloattype.cpp, .. aten src aten sparsecudafloattype.h, .. aten src aten sparsecudainttype.cpp, .. aten src aten sparsecudainttype.h, .. aten src aten sparsecudalongtype.cpp, .. aten src aten sparsecudalongtype.h, .. aten src aten sparsecudashorttype.cpp, .. aten src aten sparsecudashorttype.h 8 building cxx object third party gloo gloo cmakefiles gloo.dir transport tcp unbound buffer.cc.o 8 linking cxx shared library .. lib libc10.so 8 built target c10 scanning dependencies target mkrename gnuabi 8 building c object sleef src libm cmakefiles mkrename gnuabi.dir mkrename gnuabi.c.o 8 linking c executable .. .. bin mkrename gnuabi 8 built target mkrename gnuabi scanning dependencies target mkmasked gnuabi 8 building c object sleef src libm cmakefiles mkmasked gnuabi.dir mkmasked gnuabi.c.o 8 linking c executable .. .. bin mkmasked gnuabi 8 built target mkmasked gnuabi scanning dependencies target mkalias 8 building c object sleef src libm cmakefiles mkalias.dir mkalias.c.o scanning dependencies target arraymap 8 building c object sleef src common cmakefiles arraymap.dir arraymap.c.o 8 linking c executable .. .. bin mkalias 8 built target mkalias 8 building cxx object third party protobuf cmake cmakefiles libprotobuf.dir src google protobuf stubs io win32.cc.o scanning dependencies target torch shm manager 8 building cxx object third party protobuf cmake cmakefiles libprotobuf.dir src google protobuf stubs once.cc.o 8 building cxx object caffe2 torch lib libshm cmakefiles torch shm manager.dir manager.cpp.o 8 building cxx object third party protobuf cmake cmakefiles libprotobuf.dir src google protobuf stubs status.cc.o 8 built target arraymap scanning dependencies target c10 utils gpu 9 building cxx object caffe2 utils cmakefiles c10 utils gpu.dir dummy.cpp.o 9 building cxx object third party ideep mkl dnn src cmakefiles mkldnn.dir cpu cpu barrier.cpp.o 9 built target c10 utils gpu scanning dependencies target c10 utils hip 9 building cxx object caffe2 utils cmakefiles c10 utils hip.dir dummy.cpp.o 9 built target c10 utils hip scanning dependencies target c10 utils cpu 9 building cxx object caffe2 utils cmakefiles c10 utils cpu.dir dummy.cpp.o 9 building cxx object third party ideep mkl dnn src cmakefiles mkldnn.dir cpu simple sum.cpp.o 9 built target c10 utils cpu scanning dependencies target cpuinfo 9 building c object confu deps cpuinfo cmakefiles cpuinfo.dir src init.c.o 9 building c object confu deps cpuinfo cmakefiles cpuinfo.dir src api.c.o 9 building c object confu deps cpuinfo cmakefiles cpuinfo.dir src x86 init.c.o 9 linking cxx static library .. .. .. lib libbenchmark.a 9 built target benchmark 9 building c object confu deps cpuinfo cmakefiles cpuinfo.dir src x86 info.c.o scanning dependencies target nnpack reference layers 9 building c object confu deps nnpack cmakefiles nnpack reference layers.dir src ref convolution output.c.o 9 building c object confu deps cpuinfo cmakefiles cpuinfo.dir src x86 vendor.c.o 9 building c object confu deps cpuinfo cmakefiles cpuinfo.dir src x86 uarch.c.o ptxas warning big maxrregcount value specified 96, ignored 9 building c object confu deps nnpack cmakefiles nnpack reference layers.dir src ref convolution input gradient.c.o 9 building cxx object third party ideep mkl dnn src cmakefiles mkldnn.dir cpu ncsp batch normalization.cpp.o 9 building c object confu deps cpuinfo cmakefiles cpuinfo.dir src x86 name.c.o 9 building c object confu deps nnpack cmakefiles nnpack reference layers.dir src ref convolution kernel.c.o 9 building cxx object third party protobuf cmake cmakefiles libprotobuf.dir src google protobuf stubs statusor.cc.o 9 building c object confu deps nnpack cmakefiles nnpack reference layers.dir src ref fully connected output.c.o 9 building c object confu deps cpuinfo cmakefiles cpuinfo.dir src x86 topology.c.o 9 building c object confu deps nnpack cmakefiles nnpack reference layers.dir src ref max pooling output.c.o 9 building c object confu deps cpuinfo cmakefiles cpuinfo.dir src x86 isa.c.o 9 building c object confu deps nnpack cmakefiles nnpack reference layers.dir src ref softmax output.c.o 9 built target aten cpu files gen target 9 building c object confu deps nnpack cmakefiles nnpack reference layers.dir src ref relu output.c.o 9 building cxx object third party ideep mkl dnn src cmakefiles mkldnn.dir cpu ref inner product.cpp.o 9 building c object confu deps cpuinfo cmakefiles cpuinfo.dir src x86 cache descriptor.c.o 9 building c object confu deps nnpack cmakefiles nnpack reference layers.dir src ref relu input gradient.c.o 9 building c object confu deps cpuinfo cmakefiles cpuinfo.dir src x86 cache init.c.o 9 linking c static library .. .. lib libnnpack reference layers.a 9 building c object confu deps cpuinfo cmakefiles cpuinfo.dir src x86 cache deterministic.c.o 10 building c object confu deps cpuinfo cmakefiles cpuinfo.dir src x86 linux init.c.o 10 built target nnpack reference layers scanning dependencies target gtest main 10 building cxx object third party googletest googletest cmakefiles gtest main.dir src gtest main.cc.o 10 building c object confu deps cpuinfo cmakefiles cpuinfo.dir src x86 linux cpuinfo.c.o 10 building c object confu deps cpuinfo cmakefiles cpuinfo.dir src linux smallfile.c.o 10 building c object confu deps cpuinfo cmakefiles cpuinfo.dir src linux multiline.c.o 10 building c object confu deps cpuinfo cmakefiles cpuinfo.dir src linux current.c.o 10 building c object confu deps cpuinfo cmakefiles cpuinfo.dir src linux cpulist.c.o 10 building c object confu deps cpuinfo cmakefiles cpuinfo.dir src linux processors.c.o scanning dependencies target benchmark main 10 building cxx object third party benchmark src cmakefiles benchmark main.dir benchmark main.cc.o 10 linking c static library .. .. lib libcpuinfo.a 10 built target cpuinfo scanning dependencies target onnxifi wrapper 10 building cxx object third party protobuf cmake cmakefiles libprotobuf.dir src google protobuf stubs stringpiece.cc.o 10 building c object third party onnx cmakefiles onnxifi wrapper.dir onnx onnxifi wrapper.c.o 10 building cxx object third party ideep mkl dnn src cmakefiles mkldnn.dir cpu simple concat.cpp.o nvcc warning 'compute 20', 'sm 20', 'sm 21' architectures deprecated, may removed future release use wno deprecated gpu targets suppress warning . 10 linking cxx executable .. .. .. .. bin torch shm manager 10 linking c shared module .. .. lib libonnxifi.so 10 built target onnxifi wrapper scanning dependencies target aten op header gen 10 generating contrib aten aten op.h 10 built target torch shm manager scanning dependencies target renameavx.h generated 10 generating include renameavx.h generating renameavx.h mkrename 4 8 avx 10 built target renameavx.h generated 11 linking cxx static library .. .. .. lib libgloo.a scanning dependencies target renamedsp128.h generated 11 generating renamedsp128.h 11 built target renamedsp128.h generated scanning dependencies target headers 12 generating .. .. .. include sleef.h 12 built target gloo scanning dependencies target dispsse.c generated 12 generating dispsse.c generating sleef.h mkrename 2 4 m128d m128 m128i m128i sse2 generating sleef.h mkrename 2 4 m128d m128 m128i m128i sse2 sse2 generating sleef.h mkrename 2 4 m128d m128 m128i m128i sse2 sse4 generating sleef.h mkrename 4 8 m256d m256 m128i struct m128i x, avx generating sleef.h mkrename 4 8 m256d m256 m128i struct m128i x, avx avx generating sleef.h mkrename 4 8 m256d m256 m128i struct m128i x, avx fma4 generating sleef.h mkrename 4 8 m256d m256 m128i m256i avx avx2 generating sleef.h mkrename 2 4 m128d m128 m128i m128i sse2 avx2128 generating sleef.h mkrename 8 16 m512d m512 m256i m512i avx512f generating sleef.h mkrename 8 16 m512d m512 m256i m512i avx512f avx512f 12 built target dispsse.c generated 12 built target headers scanning dependencies target sleefsse2 scanning dependencies target sleeffma4 12 building c object sleef src libm cmakefiles sleefsse2.dir sleefsimdsp.c.o 12 building c object sleef src libm cmakefiles sleeffma4.dir sleefsimdsp.c.o compiling misc group.cu home feng pytorch build nccl obj misc group.o nvcc warning 'compute 20', 'sm 20', 'sm 21' architectures deprecated, may removed future release use wno deprecated gpu targets suppress warning . 12 linking cxx static library .. .. .. lib libbenchmark main.a 12 linking cxx static library .. .. .. lib libgtest main.a 12 built target benchmark main scanning dependencies target sleefavx2 12 built target gtest main 12 building c object sleef src libm cmakefiles sleefavx2.dir sleefsimdsp.c.o scanning dependencies target sleefavx2128 12 building c object sleef src libm cmakefiles sleefavx2128.dir sleefsimdsp.c.o 12 building cxx object third party protobuf cmake cmakefiles libprotobuf.dir src google protobuf stubs stringprintf.cc.o 12 building c object sleef src libm cmakefiles sleeffma4.dir sleefsimddp.c.o 12 building c object sleef src libm cmakefiles sleefavx2128.dir sleefsimddp.c.o 12 building c object sleef src libm cmakefiles sleefavx2.dir sleefsimddp.c.o 12 building cxx object third party protobuf cmake cmakefiles libprotobuf.dir src google protobuf stubs structurally valid.cc.o 12 building c object sleef src libm cmakefiles sleefsse2.dir sleefsimddp.c.o 12 building cxx object third party ideep mkl dnn src cmakefiles mkldnn.dir cpu jit avx512 core u8s8s32x conv kernel.cpp.o 12 building cxx object third party ideep mkl dnn src cmakefiles mkldnn.dir cpu gemm convolution.cpp.o 12 built target sleeffma4 scanning dependencies target sleefsse4 12 building cxx object third party protobuf cmake cmakefiles libprotobuf.dir src google protobuf stubs strutil.cc.o 12 built target sleefavx2128 scanning dependencies target c10 utils gpu test 12 building c object sleef src libm cmakefiles sleefsse4.dir sleefsimdsp.c.o 12 linking cxx executable .. .. bin c10 utils gpu test 12 built target sleefavx2 scanning dependencies target c10 utils hip test 12 building cxx object caffe2 utils cmakefiles c10 utils hip test.dir dummy.cpp.o 12 linking cxx executable .. .. bin c10 utils hip test ptxas warning big maxrregcount value specified 96, ignored 12 built target c10 utils gpu test scanning dependencies target c10 utils cpu test 12 linking cxx executable .. .. bin c10 utils cpu test 12 built target c10 utils hip test scanning dependencies target qnnpack 12 building c object sleef src libm cmakefiles sleefsse4.dir sleefsimddp.c.o 12 building c object confu deps qnnpack cmakefiles qnnpack.dir src init.c.o 12 built target c10 utils cpu test 12 generating src x86 64 fma 2d fourier 8x8.py.o 12 building c object confu deps qnnpack cmakefiles qnnpack.dir src convolution.c.o 12 built target sleefsse2 scanning dependencies target c10 registry test 12 building cxx object c10 test cmakefiles c10 registry test.dir registry test.cpp.o 12 building c object confu deps qnnpack cmakefiles qnnpack.dir src deconvolution.c.o 12 building c object confu deps qnnpack cmakefiles qnnpack.dir src fully connected.c.o 12 building c object confu deps qnnpack cmakefiles qnnpack.dir src sgemm 6x8 psimd.c.o 12 building c object confu deps qnnpack cmakefiles qnnpack.dir src q8gemm 2x4c8 sse2.c.o nvcc warning 'compute 20', 'sm 20', 'sm 21' architectures deprecated, may removed future release use wno deprecated gpu targets suppress warning . 12 building c object confu deps qnnpack cmakefiles qnnpack.dir src q8gemm 4x4c2 sse2.c.o 12 building cxx object third party ideep mkl dnn src cmakefiles mkldnn.dir cpu jit uni pool kernel f32.cpp.o 12 building c object confu deps qnnpack cmakefiles qnnpack.dir src q8conv 4x4c2 sse2.c.o 12 built target sleefsse4 compiling misc nvmlwrap.cu home feng pytorch build nccl obj misc nvmlwrap.o scanning dependencies target c10 opschema test nvcc warning 'compute 20', 'sm 20', 'sm 21' architectures deprecated, may removed future release use wno deprecated gpu targets suppress warning . 12 building cxx object c10 test cmakefiles c10 opschema test.dir dispatch opschema test.cpp.o 12 building c object confu deps qnnpack cmakefiles qnnpack.dir src q8dw 9c8 sse2.c.o 12 building cxx object third party protobuf cmake cmakefiles libprotobuf.dir src google protobuf stubs time.cc.o 12 linking c static library .. .. lib libqnnpack.a 13 building cxx object third party protobuf cmake cmakefiles libprotobuf.dir src google protobuf wire format lite.cc.o 13 built target qnnpack scanning dependencies target c10 inlinestreamguard test 13 building cxx object c10 test cmakefiles c10 inlinestreamguard test.dir impl inlinestreamguard test.cpp.o 13 linking cxx executable .. .. bin c10 opschema test 13 built target c10 opschema test scanning dependencies target c10 streamguard test 13 building cxx object c10 test cmakefiles c10 streamguard test.dir streamguard test.cpp.o 13 building cxx object third party ideep mkl dnn src cmakefiles mkldnn.dir cpu gemm u8s8s32x convolution.cpp.o 13 building cxx object third party protobuf cmake cmakefiles libprotobuf.dir src google protobuf any.cc.o 13 linking cxx executable .. .. bin c10 registry test 13 built target c10 registry test scanning dependencies target c10 deviceguard test 13 linking cxx executable .. .. bin c10 streamguard test 13 building cxx object c10 test cmakefiles c10 deviceguard test.dir deviceguard test.cpp.o 13 built target c10 streamguard test scanning dependencies target c10 typetraits test 13 building cxx object c10 test cmakefiles c10 typetraits test.dir util typetraits test.cpp.o 13 building cxx object third party protobuf cmake cmakefiles libprotobuf.dir src google protobuf any.pb.cc.o ptxas warning big maxrregcount value specified 96, ignored 13 generating src x86 64 fma 2d fourier 16x16.py.o 13 built target aten cuda files gen target scanning dependencies target c10 metaprogramming test 13 building cxx object third party protobuf cmake cmakefiles libprotobuf.dir src google protobuf api.pb.cc.o 13 building cxx object c10 test cmakefiles c10 metaprogramming test.dir util metaprogramming test.cpp.o 13 building cxx object third party ideep mkl dnn src cmakefiles mkldnn.dir cpu jit avx512 core u8s8s32x 1x1 convolution.cpp.o 13 linking cxx executable .. .. bin c10 typetraits test 13 built target c10 typetraits test scanning dependencies target c10 logging test 13 building cxx object c10 test cmakefiles c10 logging test.dir logging test.cpp.o 13 building cxx object third party protobuf cmake cmakefiles libprotobuf.dir src google protobuf compiler importer.cc.o 13 building cxx object third party ideep mkl dnn src cmakefiles mkldnn.dir cpu jit avx2 gemm f32.cpp.o 14 linking cxx executable .. .. bin c10 inlinestreamguard test nvcc warning 'compute 20', 'sm 20', 'sm 21' architectures deprecated, may removed future release use wno deprecated gpu targets suppress warning . 14 built target c10 inlinestreamguard test scanning dependencies target c10 array test 14 building cxx object c10 test cmakefiles c10 array test.dir util array test.cpp.o 14 linking cxx executable .. .. bin c10 deviceguard test compiling misc ibvwrap.cu home feng pytorch build nccl obj misc ibvwrap.o nvcc warning 'compute 20', 'sm 20', 'sm 21' architectures deprecated, may removed future release use wno deprecated gpu targets suppress warning . 14 built target c10 deviceguard test scanning dependencies target c10 inlinedeviceguard test 14 building cxx object c10 test cmakefiles c10 inlinedeviceguard test.dir impl inlinedeviceguard test.cpp.o 14 linking cxx executable .. .. bin c10 metaprogramming test 14 building cxx object third party protobuf cmake cmakefiles libprotobuf.dir src google protobuf compiler parser.cc.o 14 built target c10 metaprogramming test scanning dependencies target c10 typeid test 14 linking cxx executable .. .. bin c10 array test 14 building cxx object c10 test cmakefiles c10 typeid test.dir util typeid test.cpp.o 14 built target c10 array test scanning dependencies target c10 flags test 14 building cxx object c10 test cmakefiles c10 flags test.dir flags test.cpp.o 14 building cxx object third party ideep mkl dnn src cmakefiles mkldnn.dir cpu jit uni pooling.cpp.o 14 building cxx object third party protobuf cmake cmakefiles libprotobuf.dir src google protobuf descriptor.cc.o 14 linking cxx executable .. .. bin c10 logging test 14 built target c10 logging test scanning dependencies target c10 typelist test skipping th multinomial arg generator generator skipping th normal arg generator generator skipping th normal arg generator generator skipping th normal arg generator generator skipping th tensor arg storage storage skipping th tensor arg storage storage skipping rrelu noise arg generator generator skipping rrelu noise forward arg generator generator 14 building cxx object c10 test cmakefiles c10 typelist test.dir util typelist test.cpp.o skipping thnn conv transpose2d backward arg std array std array skipping thnn conv transpose3d backward arg std array std array skipping thnn conv2d backward arg std array std array skipping thnn conv depthwise2d backward arg std array std array skipping thnn conv3d backward arg std array std array skipping thnn conv dilated2d backward arg std array std array skipping thnn conv dilated3d backward arg std array std array skipping cudnn rnn backward arg std array std array skipping cudnn init dropout state factory method skipping fused dropout arg generator generator skipping arange factory method skipping bartlett window factory method skipping bernoulli arg generator generator skipping bernoulli arg generator generator skipping blackman window factory method skipping clamp arg c10 optional scalar skipping clamp arg c10 optional scalar skipping convolution double backward arg std array std array skipping cudnn convolution backward arg std array std array skipping cudnn convolution transpose backward arg std array std array skipping cumsum arg scalartype scalartype skipping cumprod arg scalartype scalartype skipping einsum arg std string std string skipping empty factory method skipping empty like factory method skipping empty strided factory method skipping eye factory method skipping full factory method skipping full like factory method skipping hann window factory method skipping hamming window factory method skipping cufft set plan cache max size ret void void skipping cufft clear plan cache ret void void skipping linspace factory method skipping logspace factory method skipping log softmax arg scalartype scalartype skipping mean arg scalartype scalartype skipping mean arg scalartype scalartype skipping mean arg scalartype scalartype skipping mkldnn convolution backward arg std array std array skipping miopen convolution backward arg std array std array skipping miopen convolution transpose backward arg std array std array skipping native batch norm backward arg std array std array skipping ones factory method skipping ones like factory method skipping rand factory method skipping rand like factory method skipping randint factory method skipping randint like factory method skipping randn factory method skipping randn like factory method skipping randperm factory method skipping range factory method skipping rrelu arg generator generator skipping softmax arg scalartype scalartype skipping sum arg scalartype scalartype skipping sum arg scalartype scalartype skipping sum arg scalartype scalartype skipping prod arg scalartype scalartype skipping prod arg scalartype scalartype skipping prod arg scalartype scalartype skipping zeros factory method skipping zeros like factory method skipping standard gamma arg generator generator skipping poisson arg generator generator skipping sparse coo tensor factory method skipping sparse coo tensor unsafe factory method skipping sparse coo tensor dims factory method skipping sparse coo tensor dims tensors factory method skipping sparse mask arg sparsetensorref sparsetensorref skipping factory method skipping data ptr ret void void skipping multinomial arg generator generator skipping normal arg generator generator skipping normal arg generator generator skipping normal arg generator generator ptxas warning big maxrregcount value specified 96, ignored 14 built target aten op header gen scanning dependencies target sleefavx 14 building c object sleef src libm cmakefiles sleefavx.dir sleefsimdsp.c.o 14 linking cxx executable .. .. bin c10 inlinedeviceguard test 14 linking cxx executable .. .. bin c10 flags test 14 built target c10 flags test 14 built target c10 inlinedeviceguard test scanning dependencies target dispsse obj scanning dependencies target dispavx obj 14 building c object sleef src libm cmakefiles dispsse obj.dir dispsse.c.o 14 building c object sleef src libm cmakefiles dispavx obj.dir dispavx.c.o 14 linking cxx executable .. .. bin c10 typelist test nvcc warning 'compute 20', 'sm 20', 'sm 21' architectures deprecated, may removed future release use wno deprecated gpu targets suppress warning . 14 built target c10 typelist test 15 building c object sleef src libm cmakefiles sleefavx.dir sleefsimddp.c.o 15 linking cxx executable .. .. bin c10 typeid test 15 building cxx object third party ideep mkl dnn src cmakefiles mkldnn.dir cpu jit uni lrn kernel f32.cpp.o 15 built target c10 typeid test compiling misc rings.cu home feng pytorch build nccl obj misc rings.o 15 building cxx object third party ideep mkl dnn src cmakefiles mkldnn.dir cpu cpu concat.cpp.o nvcc warning 'compute 20', 'sm 20', 'sm 21' architectures deprecated, may removed future release use wno deprecated gpu targets suppress warning . 15 built target dispsse obj 15 built target dispavx obj 15 building cxx object third party ideep mkl dnn src cmakefiles mkldnn.dir cpu jit avx512 core u8s8s32x convolution.cpp.o 15 building cxx object third party protobuf cmake cmakefiles libprotobuf.dir src google protobuf descriptor.pb.cc.o 15 building cxx object third party protobuf cmake cmakefiles libprotobuf.dir src google protobuf descriptor database.cc.o 15 building cxx object third party ideep mkl dnn src cmakefiles mkldnn.dir cpu jit avx512 core i8i8 pooling.cpp.o 15 built target sleefavx scanning dependencies target sleef 15 building c object sleef src libm cmakefiles sleef.dir sleefdp.c.o 15 building cxx object third party ideep mkl dnn src cmakefiles mkldnn.dir cpu nchw pooling.cpp.o ptxas warning big maxrregcount value specified 96, ignored 16 building cxx object third party ideep mkl dnn src cmakefiles mkldnn.dir cpu cpu reducer.cpp.o 16 building c object sleef src libm cmakefiles sleef.dir sleefsp.c.o 16 building cxx object third party ideep mkl dnn src cmakefiles mkldnn.dir cpu jit avx512 common gemm f32.cpp.o 16 building cxx object third party protobuf cmake cmakefiles libprotobuf.dir src google protobuf duration.pb.cc.o nvcc warning 'compute 20', 'sm 20', 'sm 21' architectures deprecated, may removed future release use wno deprecated gpu targets suppress warning . 16 building cxx object third party ideep mkl dnn src cmakefiles mkldnn.dir cpu jit uni dw conv kernel f32.cpp.o compiling misc utils.cu home feng pytorch build nccl obj misc utils.o nvcc warning 'compute 20', 'sm 20', 'sm 21' architectures deprecated, may removed future release use wno deprecated gpu targets suppress warning . 16 building cxx object third party ideep mkl dnn src cmakefiles mkldnn.dir cpu jit uni reorder.cpp.o 16 building cxx object third party ideep mkl dnn src cmakefiles mkldnn.dir cpu cpu engine.cpp.o 16 building c object sleef src libm cmakefiles sleef.dir sleefld.c.o 16 building c object sleef src libm cmakefiles sleef.dir sleefqp.c.o 16 building cxx object third party protobuf cmake cmakefiles libprotobuf.dir src google protobuf dynamic message.cc.o 16 linking c static library .. .. lib libsleef.a 16 built target sleef 16 building cxx object third party protobuf cmake cmakefiles libprotobuf.dir src google protobuf empty.pb.cc.o ptxas warning big maxrregcount value specified 96, ignored nvcc warning 'compute 20', 'sm 20', 'sm 21' architectures deprecated, may removed future release use wno deprecated gpu targets suppress warning . 16 building cxx object third party ideep mkl dnn src cmakefiles mkldnn.dir cpu jit avx512 core convolution winograd.cpp.o compiling misc enqueue.cu home feng pytorch build nccl obj misc enqueue.o nvcc warning 'compute 20', 'sm 20', 'sm 21' architectures deprecated, may removed future release use wno deprecated gpu targets suppress warning . 16 building cxx object third party protobuf cmake cmakefiles libprotobuf.dir src google protobuf extension set heavy.cc.o 16 building cxx object third party ideep mkl dnn src cmakefiles mkldnn.dir cpu jit avx512 common lrn.cpp.o 16 building cxx object third party protobuf cmake cmakefiles libprotobuf.dir src google protobuf field mask.pb.cc.o 16 generating src x86 64 fma 2d winograd 8x8 3x3.py.o 16 building cxx object third party protobuf cmake cmakefiles libprotobuf.dir src google protobuf generated message reflection.cc.o ptxas warning big maxrregcount value specified 96, ignored 16 building cxx object third party protobuf cmake cmakefiles libprotobuf.dir src google protobuf generated message table driven.cc.o 16 building cxx object third party protobuf cmake cmakefiles libprotobuf.dir src google protobuf io gzip stream.cc.o 16 building cxx object third party ideep mkl dnn src cmakefiles mkldnn.dir cpu jit sse42 convolution.cpp.o 16 building cxx object third party protobuf cmake cmakefiles libprotobuf.dir src google protobuf io printer.cc.o 16 building cxx object third party ideep mkl dnn src cmakefiles mkldnn.dir cpu jit avx512 common convolution winograd.cpp.o 16 building cxx object third party protobuf cmake cmakefiles libprotobuf.dir src google protobuf io strtod.cc.o 16 building cxx object third party ideep mkl dnn src cmakefiles mkldnn.dir cpu jit avx2 1x1 convolution.cpp.o 16 generating src x86 64 fma blas s8gemm.py.o nvcc warning 'compute 20', 'sm 20', 'sm 21' architectures deprecated, may removed future release use wno deprecated gpu targets suppress warning . 16 building cxx object third party protobuf cmake cmakefiles libprotobuf.dir src google protobuf io tokenizer.cc.o compiling transport p2p.cu home feng pytorch build nccl obj transport p2p.o nvcc warning 'compute 20', 'sm 20', 'sm 21' architectures deprecated, may removed future release use wno deprecated gpu targets suppress warning . 16 generating src x86 64 fma blas c8gemm.py.o 16 building cxx object third party ideep mkl dnn src cmakefiles mkldnn.dir cpu jit uni eltwise.cpp.o 16 building cxx object third party protobuf cmake cmakefiles libprotobuf.dir src google protobuf io zero copy stream impl.cc.o 16 generating src x86 64 fma blas s4c6gemm.py.o 16 building cxx object third party protobuf cmake cmakefiles libprotobuf.dir src google protobuf map field.cc.o 17 building cxx object third party protobuf cmake cmakefiles libprotobuf.dir src google protobuf message.cc.o 17 building cxx object third party protobuf cmake cmakefiles libprotobuf.dir src google protobuf reflection ops.cc.o 17 building cxx object third party protobuf cmake cmakefiles libprotobuf.dir src google protobuf service.cc.o 17 generating src x86 64 fma blas conv1x1.py.o 17 building cxx object third party ideep mkl dnn src cmakefiles mkldnn.dir cpu ref pooling.cpp.o 17 building cxx object third party ideep mkl dnn src cmakefiles mkldnn.dir cpu jit avx2 conv kernel f32.cpp.o ptxas warning big maxrregcount value specified 96, ignored 17 generating src x86 64 fma blas sgemm.py.o 17 building cxx object third party protobuf cmake cmakefiles libprotobuf.dir src google protobuf source context.pb.cc.o 17 generating src x86 64 fma max pooling.py.o 17 building cxx object third party protobuf cmake cmakefiles libprotobuf.dir src google protobuf struct.pb.cc.o 17 generating src x86 64 fma relu.py.o 17 building cxx object third party ideep mkl dnn src cmakefiles mkldnn.dir cpu jit uni dw convolution.cpp.o 17 generating src x86 64 fma softmax.py.o 17 building cxx object third party protobuf cmake cmakefiles libprotobuf.dir src google protobuf stubs mathlimits.cc.o nvcc warning 'compute 20', 'sm 20', 'sm 21' architectures deprecated, may removed future release use wno deprecated gpu targets suppress warning . 17 building cxx object third party protobuf cmake cmakefiles libprotobuf.dir src google protobuf stubs substitute.cc.o 17 building cxx object third party protobuf cmake cmakefiles libprotobuf.dir src google protobuf text format.cc.o compiling transport shm.cu home feng pytorch build nccl obj transport shm.o nvcc warning 'compute 20', 'sm 20', 'sm 21' architectures deprecated, may removed future release use wno deprecated gpu targets suppress warning . 17 building cxx object third party protobuf cmake cmakefiles libprotobuf.dir src google protobuf timestamp.pb.cc.o 17 building cxx object third party ideep mkl dnn src cmakefiles mkldnn.dir cpu ref batch normalization.cpp.o 17 generating src x86 64 fma blas sdotxf.py.o 17 building cxx object third party ideep mkl dnn src cmakefiles mkldnn.dir cpu jit uni inner product.cpp.o 17 building cxx object third party protobuf cmake cmakefiles libprotobuf.dir src google protobuf type.pb.cc.o 17 building cxx object third party ideep mkl dnn src cmakefiles mkldnn.dir cpu jit uni reorder utils.cpp.o 17 generating src x86 64 fma blas shdotxf.py.o 17 building cxx object third party protobuf cmake cmakefiles libprotobuf.dir src google protobuf unknown field set.cc.o 17 building cxx object third party protobuf cmake cmakefiles libprotobuf.dir src google protobuf util delimited message util.cc.o 17 building cxx object third party ideep mkl dnn src cmakefiles mkldnn.dir cpu jit avx512 core u8s8s32x 1x1 conv kernel.cpp.o 17 building cxx object third party protobuf cmake cmakefiles libprotobuf.dir src google protobuf util field comparator.cc.o ptxas warning big maxrregcount value specified 96, ignored 17 building cxx object third party protobuf cmake cmakefiles libprotobuf.dir src google protobuf util field mask util.cc.o 17 building cxx object third party protobuf cmake cmakefiles libprotobuf.dir src google protobuf util internal datapiece.cc.o 17 building cxx object third party ideep mkl dnn src cmakefiles mkldnn.dir cpu jit uni batch normalization.cpp.o 17 building cxx object third party protobuf cmake cmakefiles libprotobuf.dir src google protobuf util internal default value objectwriter.cc.o 17 building cxx object third party ideep mkl dnn src cmakefiles mkldnn.dir cpu jit sse42 1x1 convolution.cpp.o 17 building cxx object third party protobuf cmake cmakefiles libprotobuf.dir src google protobuf util internal error listener.cc.o nvcc warning 'compute 20', 'sm 20', 'sm 21' architectures deprecated, may removed future release use wno deprecated gpu targets suppress warning . 17 building cxx object third party protobuf cmake cmakefiles libprotobuf.dir src google protobuf util internal field mask utility.cc.o compiling transport net.cu home feng pytorch build nccl obj transport net.o nvcc warning 'compute 20', 'sm 20', 'sm 21' architectures deprecated, may removed future release use wno deprecated gpu targets suppress warning . 17 building cxx object third party ideep mkl dnn src cmakefiles mkldnn.dir cpu jit avx512 core u8s8s32x wino convolution.cpp.o 17 building cxx object third party protobuf cmake cmakefiles libprotobuf.dir src google protobuf util internal json escaping.cc.o 17 building cxx object third party protobuf cmake cmakefiles libprotobuf.dir src google protobuf util internal json objectwriter.cc.o 17 building cxx object third party protobuf cmake cmakefiles libprotobuf.dir src google protobuf util internal json stream parser.cc.o 18 building cxx object third party protobuf cmake cmakefiles libprotobuf.dir src google protobuf util internal object writer.cc.o 19 building cxx object third party ideep mkl dnn src cmakefiles mkldnn.dir cpu jit avx2 convolution.cpp.o 19 building cxx object third party ideep mkl dnn src cmakefiles mkldnn.dir cpu jit avx512 common 1x1 convolution.cpp.o scanning dependencies target nnpack 19 building cxx object third party protobuf cmake cmakefiles libprotobuf.dir src google protobuf util internal proto writer.cc.o 19 building c object confu deps nnpack cmakefiles nnpack.dir src init.c.o 19 building c object confu deps nnpack cmakefiles nnpack.dir src convolution inference.c.o 19 building cxx object third party protobuf cmake cmakefiles libprotobuf.dir src google protobuf util internal protostream objectsource.cc.o 19 building c object confu deps nnpack cmakefiles nnpack.dir src fully connected inference.c.o 20 building c object confu deps nnpack cmakefiles nnpack.dir src pooling output.c.o 20 building c object confu deps nnpack cmakefiles nnpack.dir src relu output.c.o 20 building cxx object third party protobuf cmake cmakefiles libprotobuf.dir src google protobuf util internal protostream objectwriter.cc.o 20 building cxx object third party protobuf cmake cmakefiles libprotobuf.dir src google protobuf util internal type info.cc.o 20 building c object confu deps nnpack cmakefiles nnpack.dir src softmax output.c.o 20 building c object confu deps nnpack cmakefiles nnpack.dir src fully connected output.c.o 20 building c object confu deps nnpack cmakefiles nnpack.dir src relu input gradient.c.o ptxas warning big maxrregcount value specified 96, ignored 20 building c object confu deps nnpack cmakefiles nnpack.dir src convolution input gradient.c.o 20 building cxx object third party protobuf cmake cmakefiles libprotobuf.dir src google protobuf util internal type info test helper.cc.o 20 building c object confu deps nnpack cmakefiles nnpack.dir src convolution kernel gradient.c.o 20 building c object confu deps nnpack cmakefiles nnpack.dir src convolution output.c.o 20 building c object confu deps nnpack cmakefiles nnpack.dir src x86 64 fma softmax.c.o 20 linking c static library .. .. lib libnnpack.a 20 built target nnpack 20 building cxx object third party protobuf cmake cmakefiles libprotobuf.dir src google protobuf util internal utility.cc.o 20 building cxx object third party ideep mkl dnn src cmakefiles mkldnn.dir cpu cpu reorder.cpp.o 20 building cxx object third party protobuf cmake cmakefiles libprotobuf.dir src google protobuf util json util.cc.o 20 building cxx object third party protobuf cmake cmakefiles libprotobuf.dir src google protobuf util message differencer.cc.o 20 building cxx object third party protobuf cmake cmakefiles libprotobuf.dir src google protobuf util time util.cc.o nvcc warning 'compute 20', 'sm 20', 'sm 21' architectures deprecated, may removed future release use wno deprecated gpu targets suppress warning . 20 building cxx object third party protobuf cmake cmakefiles libprotobuf.dir src google protobuf util type resolver util.cc.o compiling transport net socket.cu home feng pytorch build nccl obj transport net socket.o nvcc warning 'compute 20', 'sm 20', 'sm 21' architectures deprecated, may removed future release use wno deprecated gpu targets suppress warning . 20 building cxx object third party protobuf cmake cmakefiles libprotobuf.dir src google protobuf wire format.cc.o 20 building cxx object third party protobuf cmake cmakefiles libprotobuf.dir src google protobuf wrappers.pb.cc.o 20 building cxx object third party ideep mkl dnn src cmakefiles mkldnn.dir cpu jit avx2 1x1 conv kernel f32.cpp.o 20 building cxx object third party ideep mkl dnn src cmakefiles mkldnn.dir cpu jit avx512 common conv kernel.cpp.o 20 building cxx object third party ideep mkl dnn src cmakefiles mkldnn.dir cpu gemm convolution utils.cpp.o ptxas warning big maxrregcount value specified 96, ignored 20 building cxx object third party ideep mkl dnn src cmakefiles mkldnn.dir cpu jit uni lrn.cpp.o 20 building cxx object third party ideep mkl dnn src cmakefiles mkldnn.dir cpu ref softmax.cpp.o 20 building cxx object third party ideep mkl dnn src cmakefiles mkldnn.dir cpu jit transpose src utils.cpp.o 20 building cxx object third party ideep mkl dnn src cmakefiles mkldnn.dir cpu ref rnn.cpp.o 20 building cxx object third party ideep mkl dnn src cmakefiles mkldnn.dir cpu ref eltwise.cpp.o 20 building cxx object third party ideep mkl dnn src cmakefiles mkldnn.dir cpu ref convolution.cpp.o 20 building cxx object third party ideep mkl dnn src cmakefiles mkldnn.dir cpu nspc batch normalization.cpp.o 20 linking cxx static library .. .. .. lib libprotobuf.a 20 built target libprotobuf 20 generating .. .. .. .. third party protobuf src google protobuf compiler js well known types embed.cc scanning dependencies target libprotoc 20 building cxx object third party ideep mkl dnn src cmakefiles mkldnn.dir cpu jit avx512 core conv winograd kernel f32.cpp.o nvcc warning 'compute 20', 'sm 20', 'sm 21' architectures deprecated, may removed future release use wno deprecated gpu targets suppress warning . 20 building cxx object third party protobuf cmake cmakefiles libprotoc.dir src google protobuf compiler code generator.cc.o 20 building cxx object third party protobuf cmake cmakefiles libprotoc.dir src google protobuf compiler command line interface.cc.o compiling transport net ib.cu home feng pytorch build nccl obj transport net ib.o nvcc warning 'compute 20', 'sm 20', 'sm 21' architectures deprecated, may removed future release use wno deprecated gpu targets suppress warning . 20 building cxx object third party protobuf cmake cmakefiles libprotoc.dir src google protobuf compiler cpp cpp enum.cc.o 21 building cxx object third party protobuf cmake cmakefiles libprotoc.dir src google protobuf compiler cpp cpp enum field.cc.o 21 building cxx object third party protobuf cmake cmakefiles libprotoc.dir src google protobuf compiler cpp cpp extension.cc.o 21 building cxx object third party protobuf cmake cmakefiles libprotoc.dir src google protobuf compiler cpp cpp field.cc.o 21 building cxx object third party protobuf cmake cmakefiles libprotoc.dir src google protobuf compiler cpp cpp file.cc.o ptxas warning big maxrregcount value specified 96, ignored 21 building cxx object third party protobuf cmake cmakefiles libprotoc.dir src google protobuf compiler cpp cpp generator.cc.o 21 building cxx object third party protobuf cmake cmakefiles libprotoc.dir src google protobuf compiler cpp cpp helpers.cc.o 21 building cxx object third party protobuf cmake cmakefiles libprotoc.dir src google protobuf compiler cpp cpp map field.cc.o 21 building cxx object third party protobuf cmake cmakefiles libprotoc.dir src google protobuf compiler cpp cpp message.cc.o 21 building cxx object third party protobuf cmake cmakefiles libprotoc.dir src google protobuf compiler cpp cpp message field.cc.o 21 building cxx object third party protobuf cmake cmakefiles libprotoc.dir src google protobuf compiler cpp cpp padding optimizer.cc.o 21 building cxx object third party protobuf cmake cmakefiles libprotoc.dir src google protobuf compiler cpp cpp primitive field.cc.o 21 building cxx object third party protobuf cmake cmakefiles libprotoc.dir src google protobuf compiler cpp cpp service.cc.o 21 building cxx object third party protobuf cmake cmakefiles libprotoc.dir src google protobuf compiler cpp cpp string field.cc.o nvcc warning 'compute 20', 'sm 20', 'sm 21' architectures deprecated, may removed future release use wno deprecated gpu targets suppress warning . 21 building cxx object third party protobuf cmake cmakefiles libprotoc.dir src google protobuf compiler csharp csharp doc comment.cc.o compiling collectives reduce.cu home feng pytorch build nccl obj collectives reduce.o nvcc warning 'compute 20', 'sm 20', 'sm 21' architectures deprecated, may removed future release use wno deprecated gpu targets suppress warning . 21 building cxx object third party protobuf cmake cmakefiles libprotoc.dir src google protobuf compiler csharp csharp enum.cc.o 21 building cxx object third party protobuf cmake cmakefiles libprotoc.dir src google protobuf compiler csharp csharp enum field.cc.o 21 building cxx object third party protobuf cmake cmakefiles libprotoc.dir src google protobuf compiler csharp csharp field base.cc.o 21 building cxx object third party protobuf cmake cmakefiles libprotoc.dir src google protobuf compiler csharp csharp generator.cc.o 21 building cxx object third party protobuf cmake cmakefiles libprotoc.dir src google protobuf compiler csharp csharp helpers.cc.o 21 building cxx object third party protobuf cmake cmakefiles libprotoc.dir src google protobuf compiler csharp csharp map field.cc.o 21 building cxx object third party protobuf cmake cmakefiles libprotoc.dir src google protobuf compiler csharp csharp message.cc.o 22 building cxx object third party protobuf cmake cmakefiles libprotoc.dir src google protobuf compiler csharp csharp message field.cc.o 22 building cxx object third party protobuf cmake cmakefiles libprotoc.dir src google protobuf compiler csharp csharp primitive field.cc.o 22 building cxx object third party protobuf cmake cmakefiles libprotoc.dir src google protobuf compiler csharp csharp reflection class.cc.o 22 building cxx object third party protobuf cmake cmakefiles libprotoc.dir src google protobuf compiler csharp csharp repeated enum field.cc.o 22 building cxx object third party protobuf cmake cmakefiles libprotoc.dir src google protobuf compiler csharp csharp repeated message field.cc.o ptxas warning big maxrregcount value specified 96, ignored 22 building cxx object third party protobuf cmake cmakefiles libprotoc.dir src google protobuf compiler csharp csharp repeated primitive field.cc.o 22 building cxx object third party protobuf cmake cmakefiles libprotoc.dir src google protobuf compiler csharp csharp source generator base.cc.o 22 building cxx object third party protobuf cmake cmakefiles libprotoc.dir src google protobuf compiler csharp csharp wrapper field.cc.o 22 building cxx object third party protobuf cmake cmakefiles libprotoc.dir src google protobuf compiler java java context.cc.o 22 building cxx object third party protobuf cmake cmakefiles libprotoc.dir src google protobuf compiler java java doc comment.cc.o 22 building cxx object third party protobuf cmake cmakefiles libprotoc.dir src google protobuf compiler java java enum.cc.o 22 building cxx object third party protobuf cmake cmakefiles libprotoc.dir src google protobuf compiler java java enum field.cc.o nvcc warning 'compute 20', 'sm 20', 'sm 21' architectures deprecated, may removed future release use wno deprecated gpu targets suppress warning . 22 building cxx object third party protobuf cmake cmakefiles libprotoc.dir src google protobuf compiler java java enum field lite.cc.o 22 building cxx object third party protobuf cmake cmakefiles libprotoc.dir src google protobuf compiler java java enum lite.cc.o 22 building cxx object third party protobuf cmake cmakefiles libprotoc.dir src google protobuf compiler java java extension.cc.o 22 building cxx object third party protobuf cmake cmakefiles libprotoc.dir src google protobuf compiler java java extension lite.cc.o 22 building cxx object third party protobuf cmake cmakefiles libprotoc.dir src google protobuf compiler java java field.cc.o compiling collectives gather.cu home feng pytorch build nccl obj collectives gather.o nvcc warning 'compute 20', 'sm 20', 'sm 21' architectures deprecated, may removed future release use wno deprecated gpu targets suppress warning . 22 building cxx object third party protobuf cmake cmakefiles libprotoc.dir src google protobuf compiler java java file.cc.o 22 building cxx object third party protobuf cmake cmakefiles libprotoc.dir src google protobuf compiler java java generator.cc.o 22 building cxx object third party protobuf cmake cmakefiles libprotoc.dir src google protobuf compiler java java generator factory.cc.o 22 building cxx object third party protobuf cmake cmakefiles libprotoc.dir src google protobuf compiler java java helpers.cc.o 22 building cxx object third party protobuf cmake cmakefiles libprotoc.dir src google protobuf compiler java java lazy message field.cc.o 23 building cxx object third party protobuf cmake cmakefiles libprotoc.dir src google protobuf compiler java java lazy message field lite.cc.o 23 building cxx object third party protobuf cmake cmakefiles libprotoc.dir src google protobuf compiler java java map field.cc.o 23 building cxx object third party protobuf cmake cmakefiles libprotoc.dir src google protobuf compiler java java map field lite.cc.o 23 building cxx object third party protobuf cmake cmakefiles libprotoc.dir src google protobuf compiler java java message.cc.o 23 building cxx object third party protobuf cmake cmakefiles libprotoc.dir src google protobuf compiler java java message builder.cc.o 23 building cxx object third party protobuf cmake cmakefiles libprotoc.dir src google protobuf compiler java java message builder lite.cc.o 23 building cxx object third party protobuf cmake cmakefiles libprotoc.dir src google protobuf compiler java java message field.cc.o ptxas warning big maxrregcount value specified 96, ignored 23 building cxx object third party protobuf cmake cmakefiles libprotoc.dir src google protobuf compiler java java message field lite.cc.o 23 building cxx object third party protobuf cmake cmakefiles libprotoc.dir src google protobuf compiler java java message lite.cc.o 23 building cxx object third party protobuf cmake cmakefiles libprotoc.dir src google protobuf compiler java java name resolver.cc.o nvcc warning 'compute 20', 'sm 20', 'sm 21' architectures deprecated, may removed future release use wno deprecated gpu targets suppress warning . 23 building cxx object third party protobuf cmake cmakefiles libprotoc.dir src google protobuf compiler java java primitive field.cc.o 23 building cxx object third party protobuf cmake cmakefiles libprotoc.dir src google protobuf compiler java java primitive field lite.cc.o 23 building cxx object third party protobuf cmake cmakefiles libprotoc.dir src google protobuf compiler java java service.cc.o compiling collectives broadcast.cu home feng pytorch build nccl obj collectives broadcast.o nvcc warning 'compute 20', 'sm 20', 'sm 21' architectures deprecated, may removed future release use wno deprecated gpu targets suppress warning . 23 building cxx object third party protobuf cmake cmakefiles libprotoc.dir src google protobuf compiler java java shared code generator.cc.o 23 building cxx object third party protobuf cmake cmakefiles libprotoc.dir src google protobuf compiler java java string field.cc.o 23 building cxx object third party protobuf cmake cmakefiles libprotoc.dir src google protobuf compiler java java string field lite.cc.o 23 building cxx object third party protobuf cmake cmakefiles libprotoc.dir src google protobuf compiler javanano javanano enum.cc.o 23 building cxx object third party protobuf cmake cmakefiles libprotoc.dir src google protobuf compiler javanano javanano enum field.cc.o 23 building cxx object third party protobuf cmake cmakefiles libprotoc.dir src google protobuf compiler javanano javanano extension.cc.o 23 building cxx object third party protobuf cmake cmakefiles libprotoc.dir src google protobuf compiler javanano javanano field.cc.o 23 building cxx object third party protobuf cmake cmakefiles libprotoc.dir src google protobuf compiler javanano javanano file.cc.o 24 building cxx object third party protobuf cmake cmakefiles libprotoc.dir src google protobuf compiler javanano javanano generator.cc.o 24 building cxx object third party protobuf cmake cmakefiles libprotoc.dir src google protobuf compiler javanano javanano helpers.cc.o 24 building cxx object third party protobuf cmake cmakefiles libprotoc.dir src google protobuf compiler javanano javanano map field.cc.o 24 building cxx object third party protobuf cmake cmakefiles libprotoc.dir src google protobuf compiler javanano javanano message.cc.o 24 building cxx object third party protobuf cmake cmakefiles libprotoc.dir src google protobuf compiler javanano javanano message field.cc.o ptxas warning big maxrregcount value specified 96, ignored 24 building cxx object third party protobuf cmake cmakefiles libprotoc.dir src google protobuf compiler javanano javanano primitive field.cc.o 24 building cxx object third party protobuf cmake cmakefiles libprotoc.dir src google protobuf compiler js js generator.cc.o 24 building cxx object third party protobuf cmake cmakefiles libprotoc.dir src google protobuf compiler js well known types embed.cc.o nvcc warning 'compute 20', 'sm 20', 'sm 21' architectures deprecated, may removed future release use wno deprecated gpu targets suppress warning . 24 building cxx object third party protobuf cmake cmakefiles libprotoc.dir src google protobuf compiler objectivec objectivec enum.cc.o 24 building cxx object third party protobuf cmake cmakefiles libprotoc.dir src google protobuf compiler objectivec objectivec enum field.cc.o 24 building cxx object third party protobuf cmake cmakefiles libprotoc.dir src google protobuf compiler objectivec objectivec extension.cc.o compiling collectives reduce.cu home feng pytorch build nccl obj collectives reduce.o nvcc warning 'compute 20', 'sm 20', 'sm 21' architectures deprecated, may removed future release use wno deprecated gpu targets suppress warning . 24 building cxx object third party protobuf cmake cmakefiles libprotoc.dir src google protobuf compiler objectivec objectivec field.cc.o 24 building cxx object third party protobuf cmake cmakefiles libprotoc.dir src google protobuf compiler objectivec objectivec file.cc.o 24 building cxx object third party protobuf cmake cmakefiles libprotoc.dir src google protobuf compiler objectivec objectivec generator.cc.o 24 building cxx object third party protobuf cmake cmakefiles libprotoc.dir src google protobuf compiler objectivec objectivec helpers.cc.o 24 building cxx object third party protobuf cmake cmakefiles libprotoc.dir src google protobuf compiler objectivec objectivec map field.cc.o 24 building cxx object third party protobuf cmake cmakefiles libprotoc.dir src google protobuf compiler objectivec objectivec message.cc.o 24 building cxx object third party protobuf cmake cmakefiles libprotoc.dir src google protobuf compiler objectivec objectivec message field.cc.o 24 building cxx object third party protobuf cmake cmakefiles libprotoc.dir src google protobuf compiler objectivec objectivec oneof.cc.o 24 building cxx object third party protobuf cmake cmakefiles libprotoc.dir src google protobuf compiler objectivec objectivec primitive field.cc.o 24 building cxx object third party protobuf cmake cmakefiles libprotoc.dir src google protobuf compiler php php generator.cc.o ptxas warning big maxrregcount value specified 96, ignored 24 building cxx object third party protobuf cmake cmakefiles libprotoc.dir src google protobuf compiler plugin.cc.o 25 building cxx object third party protobuf cmake cmakefiles libprotoc.dir src google protobuf compiler plugin.pb.cc.o 25 building cxx object third party protobuf cmake cmakefiles libprotoc.dir src google protobuf compiler python python generator.cc.o 25 building cxx object third party protobuf cmake cmakefiles libprotoc.dir src google protobuf compiler ruby ruby generator.cc.o 25 building cxx object third party protobuf cmake cmakefiles libprotoc.dir src google protobuf compiler subprocess.cc.o nvcc warning 'compute 20', 'sm 20', 'sm 21' architectures deprecated, may removed future release use wno deprecated gpu targets suppress warning . 25 building cxx object third party protobuf cmake cmakefiles libprotoc.dir src google protobuf compiler zip writer.cc.o compiling collectives reduce scatter.cu home feng pytorch build nccl obj collectives reduce scatter.o nvcc warning 'compute 20', 'sm 20', 'sm 21' architectures deprecated, may removed future release use wno deprecated gpu targets suppress warning . 25 linking cxx shared library .. .. .. .. lib libmkldnn.so 25 built target mkldnn ptxas warning big maxrregcount value specified 96, ignored 25 linking cxx static library .. .. .. lib libprotoc.a 25 built target libprotoc scanning dependencies target protoc 25 building cxx object third party protobuf cmake cmakefiles protoc.dir src google protobuf compiler main.cc.o nvcc warning 'compute 20', 'sm 20', 'sm 21' architectures deprecated, may removed future release use wno deprecated gpu targets suppress warning . nvcc warning 'compute 20', 'sm 20', 'sm 21' architectures deprecated, may removed future release use wno deprecated gpu targets suppress warning . 25 linking cxx executable .. .. .. bin protoc 25 built target protoc scanning dependencies target gen onnx proto 25 running c python protocol buffer compiler home feng pytorch caffe2 proto prof dag.proto 25 running c python protocol buffer compiler home feng pytorch caffe2 proto predictor consts.proto 25 running c python protocol buffer compiler home feng pytorch caffe2 proto torch.proto 25 running c python protocol buffer compiler home feng pytorch caffe2 proto caffe2.proto 25 running c python protocol buffer compiler home feng pytorch caffe2 proto hsm.proto 25 running c python protocol buffer compiler home feng pytorch caffe2 proto metanet.proto 25 running c python protocol buffer compiler home feng pytorch caffe2 proto caffe2 legacy.proto 25 running gen proto.py onnx onnx.in.proto processing home feng pytorch third party onnx onnx onnx.in.proto writing home feng pytorch build third party onnx onnx onnx onnx torch.proto compiling reduce.cu home feng pytorch build nccl obj collectives device reduce sum.o nvcc warning 'compute 20', 'sm 20', 'sm 21' architectures deprecated, may removed future release use wno deprecated gpu targets suppress warning . writing home feng pytorch build third party onnx onnx onnx onnx torch.proto3 writing home feng pytorch build third party onnx onnx onnx.pb.h generating home feng pytorch build third party onnx onnx onnx pb.py 25 running c protocol buffer compiler home feng pytorch build third party onnx onnx onnx onnx torch.proto scanning dependencies target caffe2 proto 25 building cxx object caffe2 proto cmakefiles caffe2 proto.dir caffe2 legacy.pb.cc.o 25 building cxx object caffe2 proto cmakefiles caffe2 proto.dir caffe2.pb.cc.o 25 building cxx object caffe2 proto cmakefiles caffe2 proto.dir predictor consts.pb.cc.o 25 building cxx object caffe2 proto cmakefiles caffe2 proto.dir prof dag.pb.cc.o 25 building cxx object caffe2 proto cmakefiles caffe2 proto.dir hsm.pb.cc.o 25 building cxx object caffe2 proto cmakefiles caffe2 proto.dir metanet.pb.cc.o 25 building cxx object caffe2 proto cmakefiles caffe2 proto.dir torch.pb.cc.o 25 built target gen onnx proto 25 running gen proto.py onnx onnx operators.in.proto processing home feng pytorch third party onnx onnx onnx operators.in.proto writing home feng pytorch build third party onnx onnx onnx operators onnx torch.proto writing home feng pytorch build third party onnx onnx onnx operators onnx torch.proto3 writing home feng pytorch build third party onnx onnx onnx operators.pb.h generating home feng pytorch build third party onnx onnx onnx operators pb.py 25 running c protocol buffer compiler home feng pytorch build third party onnx onnx onnx operators onnx torch.proto scanning dependencies target onnx proto 26 building cxx object third party onnx cmakefiles onnx proto.dir onnx onnx onnx torch.pb.cc.o 26 building cxx object third party onnx cmakefiles onnx proto.dir onnx onnx operators onnx torch.pb.cc.o 26 linking cxx static library .. .. lib libonnx proto.a 26 built target onnx proto scanning dependencies target onnx 26 building cxx object third party onnx cmakefiles onnx.dir onnx version converter convert.cc.o 26 building cxx object third party onnx cmakefiles onnx.dir onnx defs function.cc.o 26 building cxx object third party onnx cmakefiles onnx.dir onnx defs controlflow defs.cc.o 26 building cxx object third party onnx cmakefiles onnx.dir onnx defs data type utils.cc.o 26 building cxx object third party onnx cmakefiles onnx.dir onnx version converter helper.cc.o 26 building cxx object third party onnx cmakefiles onnx.dir caffe2 onnx torch ops schema.cc.o 26 building cxx object third party onnx cmakefiles onnx.dir onnx defs nn defs.cc.o 26 building cxx object third party onnx cmakefiles onnx.dir caffe2 onnx torch ops defs.cc.o 26 building cxx object third party onnx cmakefiles onnx.dir onnx defs nn old.cc.o 26 building cxx object third party onnx cmakefiles onnx.dir onnx defs logical defs.cc.o 26 built target caffe2 proto scanning dependencies target caffe2 perfkernels avx2 26 building cxx object caffe2 perfkernels cmakefiles caffe2 perfkernels avx2.dir embedding lookup avx2.cc.o 26 building cxx object third party onnx cmakefiles onnx.dir onnx defs logical old.cc.o 26 building cxx object third party onnx cmakefiles onnx.dir onnx defs reduction defs.cc.o 26 building cxx object third party onnx cmakefiles onnx.dir onnx defs generator defs.cc.o 26 building cxx object third party onnx cmakefiles onnx.dir onnx defs generator old.cc.o 26 building cxx object third party onnx cmakefiles onnx.dir onnx defs tensor defs.cc.o 26 building cxx object third party onnx cmakefiles onnx.dir onnx defs tensor old.cc.o 26 building cxx object third party onnx cmakefiles onnx.dir onnx defs schema.cc.o 26 building cxx object third party onnx cmakefiles onnx.dir onnx defs traditionalml defs.cc.o 26 building cxx object third party onnx cmakefiles onnx.dir onnx defs math defs.cc.o 26 building cxx object third party onnx cmakefiles onnx.dir onnx defs math old.cc.o 26 building cxx object third party onnx cmakefiles onnx.dir onnx defs rnn defs.cc.o 27 building cxx object third party onnx cmakefiles onnx.dir onnx defs rnn old.cc.o 27 building cxx object caffe2 perfkernels cmakefiles caffe2 perfkernels avx2.dir math cpu avx2.cc.o 27 building cxx object third party onnx cmakefiles onnx.dir onnx defs experiments defs.cc.o 27 building cxx object caffe2 perfkernels cmakefiles caffe2 perfkernels avx2.dir common avx2.cc.o 27 building cxx object third party onnx cmakefiles onnx.dir onnx defs experiments experiments functions.cc.o 27 building cxx object caffe2 perfkernels cmakefiles caffe2 perfkernels avx2.dir typed axpy avx2.cc.o 27 building cxx object third party onnx cmakefiles onnx.dir onnx onnxifi utils.cc.o 27 building cxx object third party onnx cmakefiles onnx.dir onnx common ir pb converter.cc.o 27 building cxx object third party onnx cmakefiles onnx.dir onnx common status.cc.o 27 building cxx object third party onnx cmakefiles onnx.dir onnx common assertions.cc.o 27 building cxx object third party onnx cmakefiles onnx.dir onnx common model helpers.cc.o 27 building cxx object third party onnx cmakefiles onnx.dir onnx common interned strings.cc.o 27 building cxx object caffe2 perfkernels cmakefiles caffe2 perfkernels avx2.dir embedding lookup fused 8bit rowwise avx2.cc.o 27 building cxx object third party onnx cmakefiles onnx.dir onnx checker.cc.o 27 building cxx object third party onnx cmakefiles onnx.dir onnx shape inference implementation.cc.o 27 building cxx object third party onnx cmakefiles onnx.dir onnx optimizer pass registry.cc.o 27 building cxx object third party onnx cmakefiles onnx.dir onnx optimizer optimize.cc.o 27 building cxx object third party onnx cmakefiles onnx.dir onnx optimizer pass manager.cc.o 27 building cxx object third party onnx cmakefiles onnx.dir onnx optimizer pass.cc.o ptxas warning big maxrregcount value specified 96, ignored scanning dependencies target caffe2 protos 27 linking cxx static library .. lib libcaffe2 protos.a 27 built target caffe2 protos scanning dependencies target caffe2 perfkernels avx 27 building cxx object caffe2 perfkernels cmakefiles caffe2 perfkernels avx.dir adagrad avx.cc.o 28 building cxx object caffe2 perfkernels cmakefiles caffe2 perfkernels avx.dir typed axpy avx.cc.o 28 building cxx object caffe2 perfkernels cmakefiles caffe2 perfkernels avx.dir common avx.cc.o 28 built target caffe2 perfkernels avx 28 built target caffe2 perfkernels avx2 28 linking cxx static library .. .. lib libonnx.a 28 built target onnx scanning dependencies target caffe2 nvcc warning 'compute 20', 'sm 20', 'sm 21' architectures deprecated, may removed future release use wno deprecated gpu targets suppress warning . compiling broadcast.cu home feng pytorch build nccl obj collectives device broadcast sum.o nvcc warning 'compute 20', 'sm 20', 'sm 21' architectures deprecated, may removed future release use wno deprecated gpu targets suppress warning . ptxas warning big maxrregcount value specified 96, ignored nvcc warning 'compute 20', 'sm 20', 'sm 21' architectures deprecated, may removed future release use wno deprecated gpu targets suppress warning . compiling reduce.cu home feng pytorch build nccl obj collectives device reduce sum.o nvcc warning 'compute 20', 'sm 20', 'sm 21' architectures deprecated, may removed future release use wno deprecated gpu targets suppress warning . 28 building cxx object caffe2 cmakefiles caffe2.dir aten src aten dlconvertor.cpp.o 28 building cxx object caffe2 cmakefiles caffe2.dir aten src aten tensorutils.cpp.o 28 building cxx object caffe2 cmakefiles caffe2.dir aten src aten cpugenerator.cpp.o 28 building cxx object caffe2 cmakefiles caffe2.dir aten src aten tensorgeometry.cpp.o 29 building cxx object caffe2 cmakefiles caffe2.dir aten src aten undefinedtype.cpp.o 29 building cxx object caffe2 cmakefiles caffe2.dir aten src aten utils.cpp.o 29 building cxx object caffe2 cmakefiles caffe2.dir aten src aten context.cpp.o 29 building cxx object caffe2 cmakefiles caffe2.dir aten src aten cpugeneral.cpp.o 29 building cxx object caffe2 cmakefiles caffe2.dir aten src aten cputypedefault.cpp.o 29 building cxx object caffe2 cmakefiles caffe2.dir aten src aten sparsetensorimpl.cpp.o 29 building cxx object caffe2 cmakefiles caffe2.dir aten src aten expandutils.cpp.o 29 building cxx object caffe2 cmakefiles caffe2.dir aten src aten detail complexhooksinterface.cpp.o 29 building cxx object caffe2 cmakefiles caffe2.dir aten src aten detail cpuguardimpl.cpp.o 29 building cxx object caffe2 cmakefiles caffe2.dir aten src aten detail cudahooksinterface.cpp.o 29 building cxx object caffe2 cmakefiles caffe2.dir aten src aten cpu flushdenormal.cpp.o 29 building cxx object caffe2 cmakefiles caffe2.dir aten src aten core storage.cpp.o 29 building cxx object caffe2 cmakefiles caffe2.dir aten src aten core context base.cpp.o 29 building cxx object caffe2 cmakefiles caffe2.dir aten src aten core thread pool.cpp.o 29 building cxx object caffe2 cmakefiles caffe2.dir aten src aten core atencoretest.cpp.o 29 building cxx object caffe2 cmakefiles caffe2.dir aten src aten core storageimpl.cpp.o 29 building cxx object caffe2 cmakefiles caffe2.dir aten src aten core formatting.cpp.o 29 building cxx object caffe2 cmakefiles caffe2.dir aten src aten core legacytypedispatch.cpp.o 29 building cxx object caffe2 cmakefiles caffe2.dir aten src aten core register symbols.cpp.o 29 building cxx object caffe2 cmakefiles caffe2.dir aten src aten core variablehooksinterface.cpp.o 29 building cxx object caffe2 cmakefiles caffe2.dir aten src aten core tensor.cpp.o 29 building cxx object caffe2 cmakefiles caffe2.dir aten src aten core interned strings.cpp.o 30 building cxx object caffe2 cmakefiles caffe2.dir aten src aten core tensoroptions.cpp.o 30 building cxx object caffe2 cmakefiles caffe2.dir aten src aten core range.cpp.o 30 building cxx object caffe2 cmakefiles caffe2.dir aten src aten core ivalue.cpp.o 30 building cxx object caffe2 cmakefiles caffe2.dir aten src aten core allocator.cpp.o 30 building cxx object caffe2 cmakefiles caffe2.dir aten src aten core atengeneral.cpp.o 30 building cxx object caffe2 cmakefiles caffe2.dir aten src aten core undefinedtensorimpl.cpp.o 30 building cxx object caffe2 cmakefiles caffe2.dir aten src aten core scalar.cpp.o 30 building cxx object caffe2 cmakefiles caffe2.dir aten src aten core blob.cpp.o 30 building cxx object caffe2 cmakefiles caffe2.dir aten src aten core intrusive ptr.cpp.o 30 building cxx object caffe2 cmakefiles caffe2.dir aten src aten core tensorimpl.cpp.o 30 building cxx object caffe2 cmakefiles caffe2.dir aten src aten core uniquevoidptr.cpp.o 30 building cxx object caffe2 cmakefiles caffe2.dir aten src aten core type.cpp.o 30 building cxx object caffe2 cmakefiles caffe2.dir aten src aten core optionsguard.cpp.o 30 building cxx object caffe2 cmakefiles caffe2.dir aten src aten native embedding.cpp.o 30 building cxx object caffe2 cmakefiles caffe2.dir aten src aten native softmax.cpp.o ptxas warning big maxrregcount value specified 96, ignored 30 building cxx object caffe2 cmakefiles caffe2.dir aten src aten native dispatchstub.cpp.o 30 building cxx object caffe2 cmakefiles caffe2.dir aten src aten native dropout.cpp.o 30 building cxx object caffe2 cmakefiles caffe2.dir aten src aten native tensorcompare.cpp.o 30 building cxx object caffe2 cmakefiles caffe2.dir aten src aten native linearalgebra.cpp.o 30 building cxx object caffe2 cmakefiles caffe2.dir aten src aten native gridsampler.cpp.o 30 building cxx object caffe2 cmakefiles caffe2.dir aten src aten native reduceops.cpp.o 31 building cxx object caffe2 cmakefiles caffe2.dir aten src aten native unaryops.cpp.o 31 building cxx object caffe2 cmakefiles caffe2.dir aten src aten native linear.cpp.o nvcc warning 'compute 20', 'sm 20', 'sm 21' architectures deprecated, may removed future release use wno deprecated gpu targets suppress warning . 31 building cxx object caffe2 cmakefiles caffe2.dir aten src aten native indexing.cpp.o 31 building cxx object caffe2 cmakefiles caffe2.dir aten src aten native pixelshuffle.cpp.o compiling gather.cu home feng pytorch build nccl obj collectives device gather sum.o nvcc warning 'compute 20', 'sm 20', 'sm 21' architectures deprecated, may removed future release use wno deprecated gpu targets suppress warning . 31 building cxx object caffe2 cmakefiles caffe2.dir aten src aten native tensoriterator.cpp.o 31 building cxx object caffe2 cmakefiles caffe2.dir aten src aten native legacybridge.cpp.o 31 building cxx object caffe2 cmakefiles caffe2.dir aten src aten native typeproperties.cpp.o 31 building cxx object caffe2 cmakefiles caffe2.dir aten src aten native copy.cpp.o 31 building cxx object caffe2 cmakefiles caffe2.dir aten src aten native loss.cpp.o 31 building cxx object caffe2 cmakefiles caffe2.dir aten src aten native embeddingbag.cpp.o 31 building cxx object caffe2 cmakefiles caffe2.dir aten src aten native normalization.cpp.o 31 building cxx object caffe2 cmakefiles caffe2.dir aten src aten native weightnorm.cpp.o 31 building cxx object caffe2 cmakefiles caffe2.dir aten src aten native resize.cpp.o 31 building cxx object caffe2 cmakefiles caffe2.dir aten src aten native convolution.cpp.o ptxas warning big maxrregcount value specified 96, ignored 31 building cxx object caffe2 cmakefiles caffe2.dir aten src aten native roipooling.cpp.o 31 building cxx object caffe2 cmakefiles caffe2.dir aten src aten native packedsequence.cpp.o 31 building cxx object caffe2 cmakefiles caffe2.dir aten src aten native legacydefinitions.cpp.o nvcc warning 'compute 20', 'sm 20', 'sm 21' architectures deprecated, may removed future release use wno deprecated gpu targets suppress warning . 31 building cxx object caffe2 cmakefiles caffe2.dir aten src aten native distance.cpp.o compiling reduce scatter.cu home feng pytorch build nccl obj collectives device reduce scatter sum.o nvcc warning 'compute 20', 'sm 20', 'sm 21' architectures deprecated, may removed future release use wno deprecated gpu targets suppress warning . 31 building cxx object caffe2 cmakefiles caffe2.dir aten src aten native tensorfactories.cpp.o 31 building cxx object caffe2 cmakefiles caffe2.dir aten src aten native convolutiontbc.cpp.o 31 building cxx object caffe2 cmakefiles caffe2.dir aten src aten native summaryops.cpp.o 31 building cxx object caffe2 cmakefiles caffe2.dir aten src aten native tensortransformations.cpp.o 32 building cxx object caffe2 cmakefiles caffe2.dir aten src aten native unique.cpp.o 32 building cxx object caffe2 cmakefiles caffe2.dir aten src aten native tensoriteratorreduce.cpp.o 32 building cxx object caffe2 cmakefiles caffe2.dir aten src aten native pooling.cpp.o 32 building cxx object caffe2 cmakefiles caffe2.dir aten src aten native distributions.cpp.o 32 building cxx object caffe2 cmakefiles caffe2.dir aten src aten native spectralops.cpp.o 32 building cxx object caffe2 cmakefiles caffe2.dir aten src aten native memory.cpp.o 32 building cxx object caffe2 cmakefiles caffe2.dir aten src aten native tensorproperties.cpp.o 32 building cxx object caffe2 cmakefiles caffe2.dir aten src aten native scalar.cpp.o 32 building cxx object caffe2 cmakefiles caffe2.dir aten src aten native tensorshape.cpp.o 32 building cxx object caffe2 cmakefiles caffe2.dir aten src aten native binaryops.cpp.o 32 building cxx object caffe2 cmakefiles caffe2.dir aten src aten native activation.cpp.o 32 building cxx object caffe2 cmakefiles caffe2.dir aten src aten native lossctc.cpp.o 32 building cxx object caffe2 cmakefiles caffe2.dir aten src aten native rnn.cpp.o 32 building cxx object caffe2 cmakefiles caffe2.dir aten src aten native constantpadnd.cpp.o 32 building cxx object caffe2 cmakefiles caffe2.dir aten src aten native tensorconversions.cpp.o 32 building cxx object caffe2 cmakefiles caffe2.dir aten src aten native batchlinearalgebra.cpp.o ptxas warning big maxrregcount value specified 96, ignored 32 building cxx object caffe2 cmakefiles caffe2.dir aten src aten native sparse sparsetensormath.cpp.o 32 building cxx object caffe2 cmakefiles caffe2.dir aten src aten native sparse sparsetensor.cpp.o 32 building cxx object caffe2 cmakefiles caffe2.dir aten src aten native mkl linearalgebra.cpp.o 32 building cxx object caffe2 cmakefiles caffe2.dir aten src aten native mkl spectralops.cpp.o 32 building cxx object caffe2 cmakefiles caffe2.dir aten src aten native mkldnn conv.cpp.o 32 building cxx object caffe2 cmakefiles caffe2.dir aten src aten cpubytetype.cpp.o 33 building cxx object caffe2 cmakefiles caffe2.dir aten src aten cpuchartype.cpp.o 33 building cxx object caffe2 cmakefiles caffe2.dir aten src aten cpucopy.cpp.o 33 building cxx object caffe2 cmakefiles caffe2.dir aten src aten cpudoubletype.cpp.o 33 building cxx object caffe2 cmakefiles caffe2.dir aten src aten cpufloattype.cpp.o compiling reduce.cu home feng pytorch build nccl obj collectives device reduce prod.o nvcc warning 'compute 20', 'sm 20', 'sm 21' architectures deprecated, may removed future release use wno deprecated gpu targets suppress warning . 33 building cxx object caffe2 cmakefiles caffe2.dir aten src aten cpuhalftype.cpp.o 33 building cxx object caffe2 cmakefiles caffe2.dir aten src aten cpuinttype.cpp.o 33 building cxx object caffe2 cmakefiles caffe2.dir aten src aten cpulongtype.cpp.o 33 building cxx object caffe2 cmakefiles caffe2.dir aten src aten cpushorttype.cpp.o 33 building cxx object caffe2 cmakefiles caffe2.dir aten src aten registercpu.cpp.o 33 building cxx object caffe2 cmakefiles caffe2.dir aten src aten sparsecpubytetype.cpp.o 33 building cxx object caffe2 cmakefiles caffe2.dir aten src aten sparsecpuchartype.cpp.o 33 building cxx object caffe2 cmakefiles caffe2.dir aten src aten sparsecpudoubletype.cpp.o 33 building cxx object caffe2 cmakefiles caffe2.dir aten src aten sparsecpufloattype.cpp.o 33 building cxx object caffe2 cmakefiles caffe2.dir aten src aten sparsecpuinttype.cpp.o 33 building cxx object caffe2 cmakefiles caffe2.dir aten src aten sparsecpulongtype.cpp.o 33 building cxx object caffe2 cmakefiles caffe2.dir aten src aten sparsecpushorttype.cpp.o 33 building cxx object caffe2 cmakefiles caffe2.dir aten src aten typedefault.cpp.o 33 building cxx object caffe2 cmakefiles caffe2.dir aten src th thgeneral.cpp.o 33 building cxx object caffe2 cmakefiles caffe2.dir aten src th thallocator.cpp.o 33 building cxx object caffe2 cmakefiles caffe2.dir aten src th thsize.cpp.o 33 building cxx object caffe2 cmakefiles caffe2.dir aten src th thstoragefunctions.cpp.o 34 building cxx object caffe2 cmakefiles caffe2.dir aten src th thtensor.cpp.o 34 building cxx object caffe2 cmakefiles caffe2.dir aten src th thtensorcopy.cpp.o 34 building cxx object caffe2 cmakefiles caffe2.dir aten src th thtensorrandom.cpp.o 34 building cxx object caffe2 cmakefiles caffe2.dir aten src th thtensormath.cpp.o 34 building cxx object caffe2 cmakefiles caffe2.dir aten src th thtensormoremath.cpp.o 34 building cxx object caffe2 cmakefiles caffe2.dir aten src th thtensorevenmoremath.cpp.o 34 building cxx object caffe2 cmakefiles caffe2.dir aten src th thtensorconv.cpp.o 34 building cxx object caffe2 cmakefiles caffe2.dir aten src th thtensorlapack.cpp.o 34 building cxx object caffe2 cmakefiles caffe2.dir aten src th thblas.cpp.o ptxas warning big maxrregcount value specified 96, ignored 34 building cxx object caffe2 cmakefiles caffe2.dir aten src th thlapack.cpp.o 34 building cxx object caffe2 cmakefiles caffe2.dir aten src th thlogadd.cpp.o 34 building cxx object caffe2 cmakefiles caffe2.dir aten src th thrandom.cpp.o 34 building cxx object caffe2 cmakefiles caffe2.dir aten src th thfile.cpp.o 34 building cxx object caffe2 cmakefiles caffe2.dir aten src th thdiskfile.cpp.o 34 building cxx object caffe2 cmakefiles caffe2.dir aten src th thmemoryfile.cpp.o 34 building cxx object caffe2 cmakefiles caffe2.dir aten src th thvector.cpp.o 34 building cxx object caffe2 cmakefiles caffe2.dir aten src th vector avx.cpp.o 34 building cxx object caffe2 cmakefiles caffe2.dir aten src th vector avx2.cpp.o 34 building cxx object caffe2 cmakefiles caffe2.dir aten src thnn init.cpp.o 34 building cxx object caffe2 cmakefiles caffe2.dir aten src aten native cpu unaryopskernel.cpp.avx2.cpp.o 34 building cxx object caffe2 cmakefiles caffe2.dir aten src aten native cpu activation.cpp.avx2.cpp.o 34 building cxx object caffe2 cmakefiles caffe2.dir aten src aten native cpu softmaxkernel.cpp.avx2.cpp.o 35 building cxx object caffe2 cmakefiles caffe2.dir aten src aten native cpu gridsamplerkernel.cpp.avx2.cpp.o compiling broadcast.cu home feng pytorch build nccl obj collectives device broadcast prod.o nvcc warning 'compute 20', 'sm 20', 'sm 21' architectures deprecated, may removed future release use wno deprecated gpu targets suppress warning . 35 building cxx object caffe2 cmakefiles caffe2.dir aten src aten native cpu reduceopskernel.cpp.avx2.cpp.o 35 building cxx object caffe2 cmakefiles caffe2.dir aten src aten native cpu binaryopskernel.cpp.avx2.cpp.o ptxas warning big maxrregcount value specified 96, ignored compiling reduce.cu home feng pytorch build nccl obj collectives device reduce prod.o nvcc warning 'compute 20', 'sm 20', 'sm 21' architectures deprecated, may removed future release use wno deprecated gpu targets suppress warning . 35 building cxx object caffe2 cmakefiles caffe2.dir aten src aten native cpu distanceopskernel.cpp.avx2.cpp.o 35 building cxx object caffe2 cmakefiles caffe2.dir aten src aten native cpu tensorcomparekernel.cpp.avx2.cpp.o 35 building cxx object caffe2 cmakefiles caffe2.dir aten src aten native cpu unaryopskernel.cpp.avx.cpp.o 35 building cxx object caffe2 cmakefiles caffe2.dir aten src aten native cpu activation.cpp.avx.cpp.o 35 building cxx object caffe2 cmakefiles caffe2.dir aten src aten native cpu softmaxkernel.cpp.avx.cpp.o 35 building cxx object caffe2 cmakefiles caffe2.dir aten src aten native cpu gridsamplerkernel.cpp.avx.cpp.o 35 building cxx object caffe2 cmakefiles caffe2.dir aten src aten native cpu reduceopskernel.cpp.avx.cpp.o 35 building cxx object caffe2 cmakefiles caffe2.dir aten src aten native cpu binaryopskernel.cpp.avx.cpp.o ptxas warning big maxrregcount value specified 96, ignored 35 building cxx object caffe2 cmakefiles caffe2.dir aten src aten native cpu distanceopskernel.cpp.avx.cpp.o 35 building cxx object caffe2 cmakefiles caffe2.dir aten src aten native cpu tensorcomparekernel.cpp.avx.cpp.o compiling gather.cu home feng pytorch build nccl obj collectives device gather prod.o nvcc warning 'compute 20', 'sm 20', 'sm 21' architectures deprecated, may removed future release use wno deprecated gpu targets suppress warning . 35 building cxx object caffe2 cmakefiles caffe2.dir aten src aten native cpu unaryopskernel.cpp.default.cpp.o 35 building cxx object caffe2 cmakefiles caffe2.dir aten src aten native cpu activation.cpp.default.cpp.o 35 building cxx object caffe2 cmakefiles caffe2.dir aten src aten native cpu softmaxkernel.cpp.default.cpp.o ptxas warning big maxrregcount value specified 96, ignored 35 building cxx object caffe2 cmakefiles caffe2.dir aten src aten native cpu gridsamplerkernel.cpp.default.cpp.o 35 building cxx object caffe2 cmakefiles caffe2.dir aten src aten native cpu reduceopskernel.cpp.default.cpp.o compiling reduce scatter.cu home feng pytorch build nccl obj collectives device reduce scatter prod.o nvcc warning 'compute 20', 'sm 20', 'sm 21' architectures deprecated, may removed future release use wno deprecated gpu targets suppress warning . 35 building cxx object caffe2 cmakefiles caffe2.dir aten src aten native cpu binaryopskernel.cpp.default.cpp.o 35 building cxx object caffe2 cmakefiles caffe2.dir aten src aten native cpu distanceopskernel.cpp.default.cpp.o 35 building cxx object caffe2 cmakefiles caffe2.dir aten src aten native cpu tensorcomparekernel.cpp.default.cpp.o 36 building cxx object caffe2 cmakefiles caffe2.dir aten src aten mkldnn runtime.cpp.o 36 building cxx object caffe2 cmakefiles caffe2.dir contrib aten aten op.cc.o 36 building cxx object caffe2 cmakefiles caffe2.dir contrib gloo allgather ops.cc.o 36 building cxx object caffe2 cmakefiles caffe2.dir contrib gloo allreduce ops.cc.o 36 building cxx object caffe2 cmakefiles caffe2.dir contrib gloo barrier ops.cc.o ptxas warning big maxrregcount value specified 96, ignored 36 building cxx object caffe2 cmakefiles caffe2.dir contrib gloo broadcast ops.cc.o 36 building cxx object caffe2 cmakefiles caffe2.dir contrib gloo common.cc.o 36 building cxx object caffe2 cmakefiles caffe2.dir contrib gloo common world ops.cc.o 36 building cxx object caffe2 cmakefiles caffe2.dir contrib gloo context.cc.o 36 building cxx object caffe2 cmakefiles caffe2.dir contrib gloo reduce scatter ops.cc.o 36 building cxx object caffe2 cmakefiles caffe2.dir contrib gloo store handler.cc.o 36 building cxx object caffe2 cmakefiles caffe2.dir contrib script lexer.cc.o 36 building cxx object caffe2 cmakefiles caffe2.dir contrib script compiler.cc.o compiling reduce.cu home feng pytorch build nccl obj collectives device reduce min.o nvcc warning 'compute 20', 'sm 20', 'sm 21' architectures deprecated, may removed future release use wno deprecated gpu targets suppress warning . 36 building cxx object caffe2 cmakefiles caffe2.dir core init intrinsics check.cc.o 36 building cxx object caffe2 cmakefiles caffe2.dir core module.cc.o 36 building cxx object caffe2 cmakefiles caffe2.dir core net dag utils.cc.o 36 building cxx object caffe2 cmakefiles caffe2.dir core event.cc.o 36 building cxx object caffe2 cmakefiles caffe2.dir core graph.cc.o 36 building cxx object caffe2 cmakefiles caffe2.dir core plan executor.cc.o 36 building cxx object caffe2 cmakefiles caffe2.dir core int8 serialization.cc.o 36 building cxx object caffe2 cmakefiles caffe2.dir core tensor int8.cc.o 36 building cxx object caffe2 cmakefiles caffe2.dir core context.cc.o 37 building cxx object caffe2 cmakefiles caffe2.dir core prof dag counters.cc.o 37 building cxx object caffe2 cmakefiles caffe2.dir core blob stats.cc.o 37 building cxx object caffe2 cmakefiles caffe2.dir core qtensor.cc.o 37 building cxx object caffe2 cmakefiles caffe2.dir core net simple.cc.o 37 building cxx object caffe2 cmakefiles caffe2.dir core memonger.cc.o 37 building cxx object caffe2 cmakefiles caffe2.dir core net.cc.o 37 building cxx object caffe2 cmakefiles caffe2.dir core transform.cc.o 37 building cxx object caffe2 cmakefiles caffe2.dir core operator c10wrapper.cc.o 37 building cxx object caffe2 cmakefiles caffe2.dir core context base.cc.o 37 building cxx object caffe2 cmakefiles caffe2.dir core common.cc.o 37 building cxx object caffe2 cmakefiles caffe2.dir core blob serialization.cc.o 37 building cxx object caffe2 cmakefiles caffe2.dir core net async tracing.cc.o 37 building cxx object caffe2 cmakefiles caffe2.dir core workspace.cc.o 37 building cxx object caffe2 cmakefiles caffe2.dir core tensor impl.cc.o 37 building cxx object caffe2 cmakefiles caffe2.dir core init omp.cc.o 37 building cxx object caffe2 cmakefiles caffe2.dir core operator.cc.o 37 building cxx object caffe2 cmakefiles caffe2.dir core init.cc.o 37 building cxx object caffe2 cmakefiles caffe2.dir core operator schema.cc.o 37 building cxx object caffe2 cmakefiles caffe2.dir core tensor.cc.o 37 building cxx object caffe2 cmakefiles caffe2.dir core net async base.cc.o 37 building cxx object caffe2 cmakefiles caffe2.dir core net simple refcount.cc.o ptxas warning big maxrregcount value specified 96, ignored 37 building cxx object caffe2 cmakefiles caffe2.dir core stats.cc.o 38 building cxx object caffe2 cmakefiles caffe2.dir core types.cc.o 38 building cxx object caffe2 cmakefiles caffe2.dir core allocator.cc.o 38 building cxx object caffe2 cmakefiles caffe2.dir core net dag.cc.o 38 building cxx object caffe2 cmakefiles caffe2.dir core numa.cc.o 38 building cxx object caffe2 cmakefiles caffe2.dir core net async scheduling.cc.o 38 building cxx object caffe2 cmakefiles caffe2.dir core qtensor serialization.cc.o 38 building cxx object caffe2 cmakefiles caffe2.dir core db.cc.o 38 building cxx object caffe2 cmakefiles caffe2.dir utils proto convert.cc.o 38 building cxx object caffe2 cmakefiles caffe2.dir utils proto wrap.cc.o 38 building cxx object caffe2 cmakefiles caffe2.dir utils proto utils.cc.o 38 building cxx object caffe2 cmakefiles caffe2.dir utils murmur hash3.cc.o 38 building cxx object caffe2 cmakefiles caffe2.dir utils smart tensor printer.cc.o 38 building cxx object caffe2 cmakefiles caffe2.dir utils signal handler.cc.o 38 building cxx object caffe2 cmakefiles caffe2.dir utils string utils.cc.o 38 building cxx object caffe2 cmakefiles caffe2.dir utils threadpool threadpool.cc.o 38 building cxx object caffe2 cmakefiles caffe2.dir utils cpuid.cc.o 38 building cxx object caffe2 cmakefiles caffe2.dir utils bench utils.cc.o 38 building cxx object caffe2 cmakefiles caffe2.dir utils math cpu.cc.o compiling broadcast.cu home feng pytorch build nccl obj collectives device broadcast min.o nvcc warning 'compute 20', 'sm 20', 'sm 21' architectures deprecated, may removed future release use wno deprecated gpu targets suppress warning . 38 building cxx object caffe2 cmakefiles caffe2.dir utils math utils.cc.o 38 building cxx object caffe2 cmakefiles caffe2.dir utils thread name.cc.o 38 building cxx object caffe2 cmakefiles caffe2.dir utils threadpool pthreadpool.cc.o 39 building cxx object caffe2 cmakefiles caffe2.dir utils threadpool pthreadpool impl.cc.o 39 building cxx object caffe2 cmakefiles caffe2.dir predictor predictor.cc.o 39 building cxx object caffe2 cmakefiles caffe2.dir predictor predictor utils.cc.o 39 building cxx object caffe2 cmakefiles caffe2.dir predictor predictor config.cc.o 39 building cxx object caffe2 cmakefiles caffe2.dir core nomnigraph tests test util.cc.o 39 building cxx object caffe2 cmakefiles caffe2.dir core nomnigraph representations neuralnet.cc.o 39 building cxx object caffe2 cmakefiles caffe2.dir db create db op.cc.o 39 building cxx object caffe2 cmakefiles caffe2.dir db protodb.cc.o 39 building cxx object caffe2 cmakefiles caffe2.dir distributed file store handler.cc.o ptxas warning big maxrregcount value specified 96, ignored compiling reduce.cu home feng pytorch build nccl obj collectives device reduce min.o nvcc warning 'compute 20', 'sm 20', 'sm 21' architectures deprecated, may removed future release use wno deprecated gpu targets suppress warning . 39 building cxx object caffe2 cmakefiles caffe2.dir distributed file store handler op.cc.o 39 building cxx object caffe2 cmakefiles caffe2.dir distributed store handler.cc.o 39 building cxx object caffe2 cmakefiles caffe2.dir distributed store ops.cc.o 39 building cxx object caffe2 cmakefiles caffe2.dir ideep utils ideep register.cc.o 39 building cxx object caffe2 cmakefiles caffe2.dir ideep operators dropout op.cc.o 39 building cxx object caffe2 cmakefiles caffe2.dir ideep operators pool op.cc.o 39 building cxx object caffe2 cmakefiles caffe2.dir ideep operators momentum sgd op.cc.o 39 building cxx object caffe2 cmakefiles caffe2.dir ideep operators local response normalization op.cc.o 39 building cxx object caffe2 cmakefiles caffe2.dir ideep operators relu op.cc.o 39 building cxx object caffe2 cmakefiles caffe2.dir ideep operators elementwise sum op.cc.o 39 building cxx object caffe2 cmakefiles caffe2.dir ideep operators squeeze op.cc.o 39 building cxx object caffe2 cmakefiles caffe2.dir ideep operators utility ops.cc.o ptxas warning big maxrregcount value specified 96, ignored 39 building cxx object caffe2 cmakefiles caffe2.dir ideep operators conv fusion op.cc.o 40 building cxx object caffe2 cmakefiles caffe2.dir ideep operators operator fallback ideep.cc.o 40 building cxx object caffe2 cmakefiles caffe2.dir ideep operators concat split op.cc.o 40 building cxx object caffe2 cmakefiles caffe2.dir ideep operators spatial batch norm op.cc.o compiling gather.cu home feng pytorch build nccl obj collectives device gather min.o nvcc warning 'compute 20', 'sm 20', 'sm 21' architectures deprecated, may removed future release use wno deprecated gpu targets suppress warning . 40 building cxx object caffe2 cmakefiles caffe2.dir ideep operators fully connected op.cc.o 40 building cxx object caffe2 cmakefiles caffe2.dir ideep operators conv op.cc.o 40 building cxx object caffe2 cmakefiles caffe2.dir mpi mpi common.cc.o ptxas warning big maxrregcount value specified 96, ignored compiling reduce scatter.cu home feng pytorch build nccl obj collectives device reduce scatter min.o nvcc warning 'compute 20', 'sm 20', 'sm 21' architectures deprecated, may removed future release use wno deprecated gpu targets suppress warning . 40 building cxx object caffe2 cmakefiles caffe2.dir mpi mpi ops.cc.o 40 building cxx object caffe2 cmakefiles caffe2.dir observers time observer.cc.o 40 building cxx object caffe2 cmakefiles caffe2.dir observers runcnt observer.cc.o 40 building cxx object caffe2 cmakefiles caffe2.dir onnx backend.cc.o 40 building cxx object caffe2 cmakefiles caffe2.dir onnx onnx exporter.cc.o 40 building cxx object caffe2 cmakefiles caffe2.dir onnx backend rep.cc.o 40 building cxx object caffe2 cmakefiles caffe2.dir onnx helper.cc.o 40 building cxx object caffe2 cmakefiles caffe2.dir onnx onnxifi init.cc.o 40 building cxx object caffe2 cmakefiles caffe2.dir onnx device.cc.o 40 building cxx object caffe2 cmakefiles caffe2.dir operators lpnorm op.cc.o 40 building cxx object caffe2 cmakefiles caffe2.dir operators gru unit op.cc.o 40 building cxx object caffe2 cmakefiles caffe2.dir operators sparse normalize op.cc.o ptxas warning big maxrregcount value specified 96, ignored 40 building cxx object caffe2 cmakefiles caffe2.dir operators op.cc.o 40 building cxx object caffe2 cmakefiles caffe2.dir operators unique ops.cc.o 40 building cxx object caffe2 cmakefiles caffe2.dir operators feed blob op.cc.o compiling reduce.cu home feng pytorch build nccl obj collectives device reduce max.o nvcc warning 'compute 20', 'sm 20', 'sm 21' architectures deprecated, may removed future release use wno deprecated gpu targets suppress warning . 40 building cxx object caffe2 cmakefiles caffe2.dir operators index ops.cc.o 41 building cxx object caffe2 cmakefiles caffe2.dir operators clip op.cc.o 41 building cxx object caffe2 cmakefiles caffe2.dir operators elementwise div gradient op.cc.o 41 building cxx object caffe2 cmakefiles caffe2.dir operators glu op.cc.o 41 building cxx object caffe2 cmakefiles caffe2.dir operators reservoir sampling.cc.o 41 building cxx object caffe2 cmakefiles caffe2.dir operators dropout op.cc.o 41 building cxx object caffe2 cmakefiles caffe2.dir operators resize op.cc.o 41 building cxx object caffe2 cmakefiles caffe2.dir operators roi align rotated gradient op.cc.o 41 building cxx object caffe2 cmakefiles caffe2.dir operators tensor protos db input.cc.o 41 building cxx object caffe2 cmakefiles caffe2.dir operators sparse dense mask op.cc.o 41 building cxx object caffe2 cmakefiles caffe2.dir operators pool op.cc.o 41 building cxx object caffe2 cmakefiles caffe2.dir operators relu n op.cc.o 41 building cxx object caffe2 cmakefiles caffe2.dir operators instance norm gradient op.cc.o 41 building cxx object caffe2 cmakefiles caffe2.dir operators reciprocal op.cc.o 41 building cxx object caffe2 cmakefiles caffe2.dir operators leaky relu op.cc.o 41 building cxx object caffe2 cmakefiles caffe2.dir operators distance op.cc.o 41 building cxx object caffe2 cmakefiles caffe2.dir operators ensure cpu output op.cc.o 41 building cxx object caffe2 cmakefiles caffe2.dir operators locally connected op util.cc.o 41 building cxx object caffe2 cmakefiles caffe2.dir operators partition ops.cc.o 41 building cxx object caffe2 cmakefiles caffe2.dir operators selu op.cc.o 41 building cxx object caffe2 cmakefiles caffe2.dir operators op.cc.o 41 building cxx object caffe2 cmakefiles caffe2.dir operators lengths reducer fused 8bit rowwise ops.cc.o ptxas warning big maxrregcount value specified 96, ignored 42 building cxx object caffe2 cmakefiles caffe2.dir operators free op.cc.o 42 building cxx object caffe2 cmakefiles caffe2.dir operators lengths top k op.cc.o 42 building cxx object caffe2 cmakefiles caffe2.dir operators batch bucketize op.cc.o 42 building cxx object caffe2 cmakefiles caffe2.dir operators heatmap max keypoint op.cc.o 42 building cxx object caffe2 cmakefiles caffe2.dir operators filler op.cc.o 42 building cxx object caffe2 cmakefiles caffe2.dir operators softsign op.cc.o 42 building cxx object caffe2 cmakefiles caffe2.dir operators conv op shared.cc.o 42 building cxx object caffe2 cmakefiles caffe2.dir operators enforce finite op.cc.o 42 building cxx object caffe2 cmakefiles caffe2.dir operators local response normalization op.cc.o 42 building cxx object caffe2 cmakefiles caffe2.dir operators moments op.cc.o 42 building cxx object caffe2 cmakefiles caffe2.dir operators sparse dense op.cc.o 42 building cxx object caffe2 cmakefiles caffe2.dir operators data couple.cc.o 42 building cxx object caffe2 cmakefiles caffe2.dir operators tt linear op.cc.o 42 building cxx object caffe2 cmakefiles caffe2.dir operators arg ops.cc.o compiling broadcast.cu home feng pytorch build nccl obj collectives device broadcast max.o nvcc warning 'compute 20', 'sm 20', 'sm 21' architectures deprecated, may removed future release use wno deprecated gpu targets suppress warning . 42 building cxx object caffe2 cmakefiles caffe2.dir operators assert op.cc.o ptxas warning big maxrregcount value specified 96, ignored compiling reduce.cu home feng pytorch build nccl obj collectives device reduce max.o nvcc warning 'compute 20', 'sm 20', 'sm 21' architectures deprecated, may removed future release use wno deprecated gpu targets suppress warning . 42 building cxx object caffe2 cmakefiles caffe2.dir operators feature maps ops.cc.o 42 building cxx object caffe2 cmakefiles caffe2.dir operators op.cc.o 42 building cxx object caffe2 cmakefiles caffe2.dir operators acos op.cc.o 42 building cxx object caffe2 cmakefiles caffe2.dir operators relu op.cc.o 42 building cxx object caffe2 cmakefiles caffe2.dir operators accumulate op.cc.o 42 building cxx object caffe2 cmakefiles caffe2.dir operators spatial softmax loss op.cc.o 42 building cxx object caffe2 cmakefiles caffe2.dir operators listwise l2r op.cc.o 43 building cxx object caffe2 cmakefiles caffe2.dir operators sigmoid op.cc.o 43 building cxx object caffe2 cmakefiles caffe2.dir operators cast op.cc.o 43 building cxx object caffe2 cmakefiles caffe2.dir operators integral image op.cc.o 43 building cxx object caffe2 cmakefiles caffe2.dir operators flexible top k.cc.o 43 building cxx object caffe2 cmakefiles caffe2.dir operators elementwise sub op.cc.o 43 building cxx object caffe2 cmakefiles caffe2.dir operators sequence ops.cc.o 43 building cxx object caffe2 cmakefiles caffe2.dir operators remove data blocks op.cc.o 43 building cxx object caffe2 cmakefiles caffe2.dir operators h softmax op.cc.o 43 building cxx object caffe2 cmakefiles caffe2.dir operators expand squeeze dims op.cc.o ptxas warning big maxrregcount value specified 96, ignored 43 building cxx object caffe2 cmakefiles caffe2.dir operators slice op.cc.o 43 building cxx object caffe2 cmakefiles caffe2.dir operators reduce front back max ops.cc.o 43 building cxx object caffe2 cmakefiles caffe2.dir operators conv gradient op.cc.o 43 building cxx object caffe2 cmakefiles caffe2.dir operators stats put ops.cc.o 43 building cxx object caffe2 cmakefiles caffe2.dir operators bbox transform op.cc.o 43 building cxx object caffe2 cmakefiles caffe2.dir operators collect distribute fpn rpn proposals op.cc.o compiling gather.cu home feng pytorch build nccl obj collectives device gather max.o nvcc warning 'compute 20', 'sm 20', 'sm 21' architectures deprecated, may removed future release use wno deprecated gpu targets suppress warning . 43 building cxx object caffe2 cmakefiles caffe2.dir operators reshape op.cc.o 43 building cxx object caffe2 cmakefiles caffe2.dir operators prepend dim op.cc.o 43 building cxx object caffe2 cmakefiles caffe2.dir operators roi align gradient op.cc.o 43 building cxx object caffe2 cmakefiles caffe2.dir operators sqrt op.cc.o ptxas warning big maxrregcount value specified 96, ignored 43 building cxx object caffe2 cmakefiles caffe2.dir operators segment reduction op.cc.o 43 building cxx object caffe2 cmakefiles caffe2.dir operators elementwise div op.cc.o compiling reduce scatter.cu home feng pytorch build nccl obj collectives device reduce scatter max.o nvcc warning 'compute 20', 'sm 20', 'sm 21' architectures deprecated, may removed future release use wno deprecated gpu targets suppress warning . 44 building cxx object caffe2 cmakefiles caffe2.dir operators byte weight dequant op.cc.o 44 building cxx object caffe2 cmakefiles caffe2.dir operators elementwise sum op.cc.o 44 building cxx object caffe2 cmakefiles caffe2.dir operators upsample op.cc.o 44 building cxx object caffe2 cmakefiles caffe2.dir operators ensure clipped op.cc.o 44 building cxx object caffe2 cmakefiles caffe2.dir operators cosh op.cc.o 44 building cxx object caffe2 cmakefiles caffe2.dir operators margin ranking criterion op.cc.o 44 building cxx object caffe2 cmakefiles caffe2.dir operators roi pool op.cc.o 44 building cxx object caffe2 cmakefiles caffe2.dir operators tanh gradient op.cc.o 44 building cxx object caffe2 cmakefiles caffe2.dir operators lp pool op.cc.o 44 building cxx object caffe2 cmakefiles caffe2.dir operators pow op.cc.o 44 building cxx object caffe2 cmakefiles caffe2.dir operators rowmul op.cc.o 44 building cxx object caffe2 cmakefiles caffe2.dir operators rsqrt op.cc.o ptxas warning big maxrregcount value specified 96, ignored 44 building cxx object caffe2 cmakefiles caffe2.dir operators roi align rotated op.cc.o 44 building cxx object caffe2 cmakefiles caffe2.dir operators bisect percentile op.cc.o 44 building cxx object caffe2 cmakefiles caffe2.dir operators percentile op.cc.o 44 building cxx object caffe2 cmakefiles caffe2.dir operators normalize op.cc.o 44 building cxx object caffe2 cmakefiles caffe2.dir operators sin op.cc.o 44 building cxx object caffe2 cmakefiles caffe2.dir operators batch moments op.cc.o 44 building cxx object caffe2 cmakefiles caffe2.dir operators lstm unit op.cc.o 44 building cxx object caffe2 cmakefiles caffe2.dir operators jsd op.cc.o nvcc warning 'compute 20', 'sm 20', 'sm 21' architectures deprecated, may removed future release use wno deprecated gpu targets suppress warning . compiling functions.cu home feng pytorch build nccl obj collectives device functions.o nvcc warning 'compute 20', 'sm 20', 'sm 21' architectures deprecated, may removed future release use wno deprecated gpu targets suppress warning . 44 building cxx object caffe2 cmakefiles caffe2.dir operators pad op.cc.o 44 building cxx object caffe2 cmakefiles caffe2.dir operators sigmoid gradient op.cc.o 45 building cxx object caffe2 cmakefiles caffe2.dir operators string ops.cc.o 45 building cxx object caffe2 cmakefiles caffe2.dir operators create scope op.cc.o ptxas warning big maxrregcount value specified 96, ignored 45 building cxx object caffe2 cmakefiles caffe2.dir operators lengths reducer rowwise 8bit ops.cc.o nvcc warning 'compute 20', 'sm 20', 'sm 21' architectures deprecated, may removed future release use wno deprecated gpu targets suppress warning . nvlink fatal internal error reference deleted section makefile 83 recipe target ' home feng pytorch build nccl obj collectives device devlink.o' failed make 5 home feng pytorch build nccl obj collectives device devlink.o error 1 makefile 45 recipe target 'devicelib' failed make 4 devicelib error 2 makefile 25 recipe target 'src.build' failed make 3 src.build error 2 cmakefiles nccl external.dir build.make 110 recipe target 'nccl external prefix src nccl external stamp nccl external build' failed make 2 nccl external prefix src nccl external stamp nccl external build error 2 cmakefiles makefile2 67 recipe target 'cmakefiles nccl external.dir all' failed make 1 cmakefiles nccl external.dir error 2 make 1 waiting unfinished jobs.... 45 building cxx object caffe2 cmakefiles caffe2.dir operators lengths reducer ops.cc.o 45 building cxx object caffe2 cmakefiles caffe2.dir operators deform conv op.cc.o 45 building cxx object caffe2 cmakefiles caffe2.dir operators gather ranges dense op.cc.o 45 building cxx object caffe2 cmakefiles caffe2.dir operators conv transpose op mobile.cc.o 45 building cxx object caffe2 cmakefiles caffe2.dir operators batch matmul op.cc.o 45 building cxx object caffe2 cmakefiles caffe2.dir operators crf viterbi op.cc.o 45 building cxx object caffe2 cmakefiles caffe2.dir operators thresholded relu op.cc.o 45 building cxx object caffe2 cmakefiles caffe2.dir operators replace nan op.cc.o 45 building cxx object caffe2 cmakefiles caffe2.dir operators mean op.cc.o 45 building cxx object caffe2 cmakefiles caffe2.dir operators stylizer ops.cc.o 45 building cxx object caffe2 cmakefiles caffe2.dir operators empty op.cc.o 45 building cxx object caffe2 cmakefiles caffe2.dir operators channel stats op.cc.o 45 building cxx object caffe2 cmakefiles caffe2.dir operators sinusoid position encoding op.cc.o 45 building cxx object caffe2 cmakefiles caffe2.dir operators apmeter op.cc.o 45 building cxx object caffe2 cmakefiles caffe2.dir operators elementwise ops.cc.o 45 building cxx object caffe2 cmakefiles caffe2.dir operators key split ops.cc.o 45 building cxx object caffe2 cmakefiles caffe2.dir operators square root divide op.cc.o 45 building cxx object caffe2 cmakefiles caffe2.dir operators reduce ops.cc.o 45 building cxx object caffe2 cmakefiles caffe2.dir operators softmax shared.cc.o 46 building cxx object caffe2 cmakefiles caffe2.dir operators elementwise mul op.cc.o 46 building cxx object caffe2 cmakefiles caffe2.dir operators roi align op.cc.o 46 building cxx object caffe2 cmakefiles caffe2.dir operators minmax gradient ops.cc.o 46 building cxx object caffe2 cmakefiles caffe2.dir operators deform conv gradient op.cc.o 46 building cxx object caffe2 cmakefiles caffe2.dir operators reduce front back sum ops.cc.o 46 building cxx object caffe2 cmakefiles caffe2.dir operators transpose op.cc.o 46 building cxx object caffe2 cmakefiles caffe2.dir operators text file reader utils.cc.o 46 building cxx object caffe2 cmakefiles caffe2.dir operators elu op.cc.o 46 building cxx object caffe2 cmakefiles caffe2.dir operators reduce front back mean ops.cc.o 46 building cxx object caffe2 cmakefiles caffe2.dir operators workspace ops.cc.o 46 building cxx object caffe2 cmakefiles caffe2.dir operators text file reader.cc.o 46 building cxx object caffe2 cmakefiles caffe2.dir operators given tensor fill op.cc.o 46 building cxx object caffe2 cmakefiles caffe2.dir operators utility ops.cc.o 46 building cxx object caffe2 cmakefiles caffe2.dir operators communicator op.cc.o 46 building cxx object caffe2 cmakefiles caffe2.dir operators find duplicate elements op.cc.o 46 building cxx object caffe2 cmakefiles caffe2.dir operators abs op.cc.o 46 building cxx object caffe2 cmakefiles caffe2.dir operators cbrt op.cc.o 46 building cxx object caffe2 cmakefiles caffe2.dir operators batch box cox op.cc.o 46 building cxx object caffe2 cmakefiles caffe2.dir operators sqr op.cc.o 46 building cxx object caffe2 cmakefiles caffe2.dir operators last n window collector.cc.o 46 building cxx object caffe2 cmakefiles caffe2.dir operators tanh op.cc.o 47 building cxx object caffe2 cmakefiles caffe2.dir operators length split op.cc.o 47 building cxx object caffe2 cmakefiles caffe2.dir operators gather fused 8bit rowwise op.cc.o 47 building cxx object caffe2 cmakefiles caffe2.dir operators onnxifi op.cc.o 47 building cxx object caffe2 cmakefiles caffe2.dir operators logit op.cc.o 47 building cxx object caffe2 cmakefiles caffe2.dir operators boolean mask ops.cc.o 47 building cxx object caffe2 cmakefiles caffe2.dir operators stop gradient.cc.o 47 building cxx object caffe2 cmakefiles caffe2.dir operators asin op.cc.o 47 building cxx object caffe2 cmakefiles caffe2.dir operators batch sparse dense op.cc.o 47 building cxx object caffe2 cmakefiles caffe2.dir operators channel shuffle op.cc.o 47 building cxx object caffe2 cmakefiles caffe2.dir operators counter ops.cc.o 47 building cxx object caffe2 cmakefiles caffe2.dir operators sinh op.cc.o 47 building cxx object caffe2 cmakefiles caffe2.dir operators elementwise mul gradient op.cc.o 47 building cxx object caffe2 cmakefiles caffe2.dir operators ngram ops.cc.o 47 building cxx object caffe2 cmakefiles caffe2.dir operators accuracy op.cc.o 47 building cxx object caffe2 cmakefiles caffe2.dir operators reduction ops.cc.o 47 building cxx object caffe2 cmakefiles caffe2.dir operators matmul op.cc.o 47 building cxx object caffe2 cmakefiles caffe2.dir operators rank loss op.cc.o 47 building cxx object caffe2 cmakefiles caffe2.dir operators softplus op.cc.o 47 building cxx object caffe2 cmakefiles caffe2.dir operators conditional op.cc.o 47 building cxx object caffe2 cmakefiles caffe2.dir operators loss op.cc.o 47 building cxx object caffe2 cmakefiles caffe2.dir operators boolean unmask ops.cc.o 47 building cxx object caffe2 cmakefiles caffe2.dir operators weighted multi sampling op.cc.o 48 building cxx object caffe2 cmakefiles caffe2.dir operators fused rowwise random quantization ops.cc.o 48 building cxx object caffe2 cmakefiles caffe2.dir operators reciprocal gradient op.cc.o 48 building cxx object caffe2 cmakefiles caffe2.dir operators softmax loss op.cc.o 48 building cxx object caffe2 cmakefiles caffe2.dir operators numpy tile op.cc.o 48 building cxx object caffe2 cmakefiles caffe2.dir operators piecewise linear transform op.cc.o 48 building cxx object caffe2 cmakefiles caffe2.dir operators space batch op.cc.o 48 building cxx object caffe2 cmakefiles caffe2.dir operators floor op.cc.o 48 building cxx object caffe2 cmakefiles caffe2.dir operators log op.cc.o 48 building cxx object caffe2 cmakefiles caffe2.dir operators elementwise logical ops.cc.o 48 building cxx object caffe2 cmakefiles caffe2.dir operators elementwise add gradient op.cc.o 48 building cxx object caffe2 cmakefiles caffe2.dir operators ceil op.cc.o 48 building cxx object caffe2 cmakefiles caffe2.dir operators index hash ops.cc.o 48 building cxx object caffe2 cmakefiles caffe2.dir operators weighted sample op.cc.o 48 building cxx object caffe2 cmakefiles caffe2.dir operators atomic ops.cc.o 48 building cxx object caffe2 cmakefiles caffe2.dir operators one hot ops.cc.o 48 building cxx object caffe2 cmakefiles caffe2.dir operators elementwise ops schema.cc.o 48 building cxx object caffe2 cmakefiles caffe2.dir operators merge id lists op.cc.o 48 building cxx object caffe2 cmakefiles caffe2.dir operators shape op.cc.o 48 building cxx object caffe2 cmakefiles caffe2.dir operators perplexity op.cc.o 48 building cxx object caffe2 cmakefiles caffe2.dir operators normalize l1 op.cc.o 48 building cxx object caffe2 cmakefiles caffe2.dir operators map ops.cc.o 49 building cxx object caffe2 cmakefiles caffe2.dir operators concat split op.cc.o 49 building cxx object caffe2 cmakefiles caffe2.dir operators load save op.cc.o 49 building cxx object caffe2 cmakefiles caffe2.dir operators find op.cc.o 49 building cxx object caffe2 cmakefiles caffe2.dir operators top k.cc.o 49 building cxx object caffe2 cmakefiles caffe2.dir operators group norm op.cc.o 49 building cxx object caffe2 cmakefiles caffe2.dir operators spatial batch norm op.cc.o 49 building cxx object caffe2 cmakefiles caffe2.dir operators tile op.cc.o 49 building cxx object caffe2 cmakefiles caffe2.dir operators box nms limit op.cc.o 49 building cxx object caffe2 cmakefiles caffe2.dir operators tan op.cc.o 49 building cxx object caffe2 cmakefiles caffe2.dir operators channel backprop stats op.cc.o 49 building cxx object caffe2 cmakefiles caffe2.dir operators fully connected op.cc.o 49 building cxx object caffe2 cmakefiles caffe2.dir operators zero gradient op.cc.o 49 building cxx object caffe2 cmakefiles caffe2.dir operators stump func op.cc.o 49 building cxx object caffe2 cmakefiles caffe2.dir operators fused rowwise 8bit conversion ops.cc.o 49 building cxx object caffe2 cmakefiles caffe2.dir operators negative op.cc.o 49 building cxx object caffe2 cmakefiles caffe2.dir operators softmax op.cc.o 49 building cxx object caffe2 cmakefiles caffe2.dir operators half float ops.cc.o 49 building cxx object caffe2 cmakefiles caffe2.dir operators ctc beam search decoder op.cc.o 49 building cxx object caffe2 cmakefiles caffe2.dir operators given tensor byte string uint8 fill op.cc.o 49 building cxx object caffe2 cmakefiles caffe2.dir operators onnx op.cc.o 49 building cxx object caffe2 cmakefiles caffe2.dir operators negate gradient op.cc.o 49 building cxx object caffe2 cmakefiles caffe2.dir operators pool gradient op.cc.o 50 building cxx object caffe2 cmakefiles caffe2.dir operators flatten op.cc.o 50 building cxx object caffe2 cmakefiles caffe2.dir operators elementwise ops utils.cc.o 50 building cxx object caffe2 cmakefiles caffe2.dir operators summarize op.cc.o 50 building cxx object caffe2 cmakefiles caffe2.dir operators minmax ops.cc.o 50 building cxx object caffe2 cmakefiles caffe2.dir operators lengths tile op.cc.o 50 building cxx object caffe2 cmakefiles caffe2.dir operators im2col op.cc.o 50 building cxx object caffe2 cmakefiles caffe2.dir operators generate proposals op.cc.o 50 building cxx object caffe2 cmakefiles caffe2.dir operators conv op.cc.o 50 building cxx object caffe2 cmakefiles caffe2.dir operators stats ops.cc.o 50 building cxx object caffe2 cmakefiles caffe2.dir operators order switch ops.cc.o 50 building cxx object caffe2 cmakefiles caffe2.dir operators reverse packed segs op.cc.o 50 building cxx object caffe2 cmakefiles caffe2.dir operators norm planar yuv op.cc.o 50 building cxx object caffe2 cmakefiles caffe2.dir operators exp op.cc.o 50 building cxx object caffe2 cmakefiles caffe2.dir operators cos op.cc.o 50 building cxx object caffe2 cmakefiles caffe2.dir operators atan op.cc.o 50 building cxx object caffe2 cmakefiles caffe2.dir operators ctc greedy decoder op.cc.o 50 building cxx object caffe2 cmakefiles caffe2.dir operators scale op.cc.o 50 building cxx object caffe2 cmakefiles caffe2.dir operators conv transpose gradient op.cc.o 50 building cxx object caffe2 cmakefiles caffe2.dir operators instance norm op.cc.o 50 building cxx object caffe2 cmakefiles caffe2.dir operators prelu op.cc.o 50 building cxx object caffe2 cmakefiles caffe2.dir operators batch gather ops.cc.o 50 building cxx object caffe2 cmakefiles caffe2.dir operators expand op.cc.o 51 building cxx object caffe2 cmakefiles caffe2.dir operators cosine embedding criterion op.cc.o 51 building cxx object caffe2 cmakefiles caffe2.dir operators pack segments.cc.o 51 building cxx object caffe2 cmakefiles caffe2.dir operators elementwise add op.cc.o 51 building cxx object caffe2 cmakefiles caffe2.dir operators multi class accuracy op.cc.o 51 building cxx object caffe2 cmakefiles caffe2.dir operators copy op.cc.o 51 building cxx object caffe2 cmakefiles caffe2.dir operators mod op.cc.o 51 building cxx object caffe2 cmakefiles caffe2.dir operators dataset ops.cc.o 51 building cxx object caffe2 cmakefiles caffe2.dir operators affine channel op.cc.o 51 building cxx object caffe2 cmakefiles caffe2.dir operators lengths pad op.cc.o 51 building cxx object caffe2 cmakefiles caffe2.dir operators spatial batch norm gradient op.cc.o 51 building cxx object caffe2 cmakefiles caffe2.dir operators cross entropy op.cc.o 51 building cxx object caffe2 cmakefiles caffe2.dir operators locally connected op.cc.o 51 building cxx object caffe2 cmakefiles caffe2.dir operators rmac regions op.cc.o 51 building cxx object caffe2 cmakefiles caffe2.dir operators variable length sequence padding.cc.o 51 building cxx object caffe2 cmakefiles caffe2.dir operators hard sigmoid op.cc.o 51 building cxx object caffe2 cmakefiles caffe2.dir operators pack rnn sequence op.cc.o 51 building cxx object caffe2 cmakefiles caffe2.dir operators fc inference.cc.o 51 building cxx object caffe2 cmakefiles caffe2.dir operators conv op eigen.cc.o 51 building cxx object caffe2 cmakefiles caffe2.dir operators gather op.cc.o 51 building cxx object caffe2 cmakefiles caffe2.dir operators swish op.cc.o 51 building cxx object caffe2 cmakefiles caffe2.dir operators elementwise sub gradient op.cc.o 52 building cxx object caffe2 cmakefiles caffe2.dir operators conv transpose op.cc.o 52 building cxx object caffe2 cmakefiles caffe2.dir operators quant decode op.cc.o 52 building cxx object caffe2 cmakefiles caffe2.dir operators elementwise linear op.cc.o 52 building cxx object caffe2 cmakefiles caffe2.dir operators layer norm op.cc.o 52 building cxx object caffe2 cmakefiles caffe2.dir operators cube op.cc.o 52 building cxx object caffe2 cmakefiles caffe2.dir operators experimental c10 schemas layer norm.cc.o 52 building cxx object caffe2 cmakefiles caffe2.dir operators experimental c10 schemas batch matmul.cc.o 52 building cxx object caffe2 cmakefiles caffe2.dir operators experimental c10 schemas flatten.cc.o 52 building cxx object caffe2 cmakefiles caffe2.dir operators experimental c10 schemas averaged loss.cc.o 52 building cxx object caffe2 cmakefiles caffe2.dir operators experimental c10 schemas add.cc.o 52 building cxx object caffe2 cmakefiles caffe2.dir operators experimental c10 schemas batch gather.cc.o 52 building cxx object caffe2 cmakefiles caffe2.dir operators experimental c10 schemas filler.cc.o 52 building cxx object caffe2 cmakefiles caffe2.dir operators experimental c10 schemas enforce finite.cc.o 52 building cxx object caffe2 cmakefiles caffe2.dir operators experimental c10 schemas expand dims.cc.o 52 building cxx object caffe2 cmakefiles caffe2.dir operators experimental c10 schemas stop gradient.cc.o 52 building cxx object caffe2 cmakefiles caffe2.dir operators experimental c10 schemas cast.cc.o 52 building cxx object caffe2 cmakefiles caffe2.dir operators experimental c10 schemas mul.cc.o 52 building cxx object caffe2 cmakefiles caffe2.dir operators experimental c10 schemas sigmoid.cc.o 52 building cxx object caffe2 cmakefiles caffe2.dir operators experimental c10 schemas relu.cc.o 52 building cxx object caffe2 cmakefiles caffe2.dir operators experimental c10 schemas fc.cc.o 52 building cxx object caffe2 cmakefiles caffe2.dir operators experimental c10 schemas sparse lengths sum.cc.o 52 building cxx object caffe2 cmakefiles caffe2.dir operators experimental c10 schemas concat.cc.o 53 building cxx object caffe2 cmakefiles caffe2.dir operators experimental c10 schemas sigmoid cross entropy logits.cc.o 53 building cxx object caffe2 cmakefiles caffe2.dir operators experimental c10 cpu expand dims cpu.cc.o 53 building cxx object caffe2 cmakefiles caffe2.dir operators experimental c10 cpu add cpu.cc.o 53 building cxx object caffe2 cmakefiles caffe2.dir operators experimental c10 cpu fc cpu.cc.o 53 building cxx object caffe2 cmakefiles caffe2.dir operators experimental c10 cpu concat cpu.cc.o 53 building cxx object caffe2 cmakefiles caffe2.dir operators experimental c10 cpu relu cpu.cc.o 53 building cxx object caffe2 cmakefiles caffe2.dir operators experimental c10 cpu batch matmul cpu.cc.o 53 building cxx object caffe2 cmakefiles caffe2.dir operators experimental c10 cpu flatten cpu.cc.o 53 building cxx object caffe2 cmakefiles caffe2.dir operators experimental c10 cpu sparse lengths sum cpu.cc.o 53 building cxx object caffe2 cmakefiles caffe2.dir operators experimental c10 cpu sigmoid cross entropy logits cpu.cc.o 53 building cxx object caffe2 cmakefiles caffe2.dir operators experimental c10 cpu sigmoid cpu.cc.o 53 building cxx object caffe2 cmakefiles caffe2.dir operators experimental c10 cpu filler cpu.cc.o 53 building cxx object caffe2 cmakefiles caffe2.dir operators experimental c10 cpu averaged loss cpu.cc.o 53 building cxx object caffe2 cmakefiles caffe2.dir operators experimental c10 cpu cast cpu.cc.o 53 building cxx object caffe2 cmakefiles caffe2.dir operators experimental c10 cpu stop gradient cpu.cc.o 53 building cxx object caffe2 cmakefiles caffe2.dir operators experimental c10 cpu batch gather cpu.cc.o 53 building cxx object caffe2 cmakefiles caffe2.dir operators experimental c10 cpu enforce finite cpu.cc.o 53 building cxx object caffe2 cmakefiles caffe2.dir operators experimental c10 cpu mul cpu.cc.o 53 building cxx object caffe2 cmakefiles caffe2.dir operators rnn recurrent network op.cc.o 53 building cxx object caffe2 cmakefiles caffe2.dir operators rnn recurrent network executor.cc.o 53 building cxx object caffe2 cmakefiles caffe2.dir operators rnn recurrent network blob fetcher op.cc.o 53 building cxx object caffe2 cmakefiles caffe2.dir operators quantized init qnnpack.cc.o 54 building cxx object caffe2 cmakefiles caffe2.dir operators quantized int8 add op.cc.o 54 building cxx object caffe2 cmakefiles caffe2.dir operators quantized int8 average pool op.cc.o 54 building cxx object caffe2 cmakefiles caffe2.dir operators quantized int8 channel shuffle op.cc.o 54 building cxx object caffe2 cmakefiles caffe2.dir operators quantized int8 concat op.cc.o 54 building cxx object caffe2 cmakefiles caffe2.dir operators quantized int8 conv op.cc.o 54 building cxx object caffe2 cmakefiles caffe2.dir operators quantized int8 conv transpose op.cc.o 54 building cxx object caffe2 cmakefiles caffe2.dir operators quantized int8 dequantize op.cc.o 54 building cxx object caffe2 cmakefiles caffe2.dir operators quantized int8 fc op.cc.o 54 building cxx object caffe2 cmakefiles caffe2.dir operators quantized int8 flatten op.cc.o 54 building cxx object caffe2 cmakefiles caffe2.dir operators quantized int8 given tensor fill op.cc.o 54 building cxx object caffe2 cmakefiles caffe2.dir operators quantized int8 leaky relu op.cc.o 54 building cxx object caffe2 cmakefiles caffe2.dir operators quantized int8 max pool op.cc.o 54 building cxx object caffe2 cmakefiles caffe2.dir operators quantized int8 quantize op.cc.o 54 building cxx object caffe2 cmakefiles caffe2.dir operators quantized int8 relu op.cc.o 54 building cxx object caffe2 cmakefiles caffe2.dir operators quantized int8 reshape op.cc.o 54 building cxx object caffe2 cmakefiles caffe2.dir operators quantized int8 resize nearest op.cc.o 54 building cxx object caffe2 cmakefiles caffe2.dir operators quantized int8 roi align op.cc.o 54 building cxx object caffe2 cmakefiles caffe2.dir operators quantized int8 slice op.cc.o 54 building cxx object caffe2 cmakefiles caffe2.dir operators quantized int8 sigmoid op.cc.o 54 building cxx object caffe2 cmakefiles caffe2.dir operators quantized int8 softmax op.cc.o 54 building cxx object caffe2 cmakefiles caffe2.dir opt distributed.cc.o 55 building cxx object caffe2 cmakefiles caffe2.dir opt optimize ideep.cc.o 55 building cxx object caffe2 cmakefiles caffe2.dir opt distributed converter.cc.o 55 building cxx object caffe2 cmakefiles caffe2.dir opt dead code elim.cc.o 55 building cxx object caffe2 cmakefiles caffe2.dir opt mobile.cc.o 55 building cxx object caffe2 cmakefiles caffe2.dir opt optimizer.cc.o 55 building cxx object caffe2 cmakefiles caffe2.dir opt sink.cc.o 55 building cxx object caffe2 cmakefiles caffe2.dir opt onnxifi transformer.cc.o 55 building cxx object caffe2 cmakefiles caffe2.dir opt passes.cc.o 55 building cxx object caffe2 cmakefiles caffe2.dir opt annotations.cc.o 55 building cxx object caffe2 cmakefiles caffe2.dir opt converter.cc.o 55 building cxx object caffe2 cmakefiles caffe2.dir opt backend cutting.cc.o 55 building cxx object caffe2 cmakefiles caffe2.dir opt fusion.cc.o 55 building cxx object caffe2 cmakefiles caffe2.dir opt device.cc.o 55 building cxx object caffe2 cmakefiles caffe2.dir perfkernels typed axpy.cc.o 55 building cxx object caffe2 cmakefiles caffe2.dir perfkernels fused 8bit rowwise embedding lookup.cc.o 55 building cxx object caffe2 cmakefiles caffe2.dir perfkernels adagrad.cc.o 55 building cxx object caffe2 cmakefiles caffe2.dir perfkernels embedding lookup.cc.o 55 building cxx object caffe2 cmakefiles caffe2.dir perfkernels math cpu base.cc.o 55 building cxx object caffe2 cmakefiles caffe2.dir queue queue ops.cc.o 55 building cxx object caffe2 cmakefiles caffe2.dir queue rebatching queue ops.cc.o 55 building cxx object caffe2 cmakefiles caffe2.dir queue rebatching queue.cc.o 55 building cxx object caffe2 cmakefiles caffe2.dir queue blobs queue.cc.o 56 building cxx object caffe2 cmakefiles caffe2.dir queue blobs queue db.cc.o 56 building cxx object caffe2 cmakefiles caffe2.dir sgd momentum sgd op.cc.o 56 building cxx object caffe2 cmakefiles caffe2.dir sgd adam op.cc.o 56 building cxx object caffe2 cmakefiles caffe2.dir sgd learning rate adaption op.cc.o 56 building cxx object caffe2 cmakefiles caffe2.dir sgd clip tensor op.cc.o 56 building cxx object caffe2 cmakefiles caffe2.dir sgd yellowfin op.cc.o 56 building cxx object caffe2 cmakefiles caffe2.dir sgd wngrad op.cc.o 56 building cxx object caffe2 cmakefiles caffe2.dir sgd gftrl op.cc.o 56 building cxx object caffe2 cmakefiles caffe2.dir sgd iter op.cc.o 56 building cxx object caffe2 cmakefiles caffe2.dir sgd learning rate op.cc.o 56 building cxx object caffe2 cmakefiles caffe2.dir sgd ftrl op.cc.o 56 building cxx object caffe2 cmakefiles caffe2.dir sgd adadelta op.cc.o 56 building cxx object caffe2 cmakefiles caffe2.dir sgd lars op.cc.o 56 building cxx object caffe2 cmakefiles caffe2.dir sgd adagrad op.cc.o 56 building cxx object caffe2 cmakefiles caffe2.dir sgd rmsprop op.cc.o 56 building cxx object caffe2 cmakefiles caffe2.dir share contrib nnpack conv op.cc.o 56 building cxx object caffe2 cmakefiles caffe2.dir share contrib depthwise depthwise3x3 conv op.cc.o 56 building cxx object caffe2 cmakefiles caffe2.dir transforms single op transform.cc.o 56 building cxx object caffe2 cmakefiles caffe2.dir transforms conv nnpack transform.cc.o 56 building cxx object caffe2 cmakefiles caffe2.dir transforms pattern net transform.cc.o 56 building cxx object caffe2 cmakefiles caffe2.dir transforms common subexpression elimination.cc.o 57 linking cxx shared library .. lib libcaffe2.so 57 built target caffe2 makefile 138 recipe target 'all' failed make error 2 failed run 'bash .. tools build pytorch libs.sh use cuda use nnpack use mkldnn use qnnpack caffe2' reproduce steps reproduce behavior 1. 1. 1. expected behavior environment please copy paste output environment collection script https raw.githubusercontent.com pytorch pytorch master torch utils collect env.py fill checklist manually . get script run pytorch version e.g., 1.0 os e.g., linux installed pytorch , , source build command used compiling source python version cuda cudnn version gpu models configuration relevant information additional context"
pytorch,20579,"exception adding custom scalars tensorboard exception raised adding custom scalars using reproduce minimal code reproduce running results environment please copy paste output additional context exception message quite clear, required type , happens https github.com pytorch pytorch blob 09f22d10a695bfba8ffb3327b9920fd3358c00ee torch utils tensorboard summary.py l381 enough remove surrounding square brackets, pr coming.",0,exception adding custom scalars tensorboard,"exception adding custom scalars tensorboard exception adding custom scalars tensorboard exception raised adding custom scalars using reproduce minimal code reproduce running results environment please copy paste output additional context exception message quite clear, required type , happens https github.com pytorch pytorch blob 09f22d10a695bfba8ffb3327b9920fd3358c00ee torch utils tensorboard summary.py l381 enough remove surrounding square brackets, pr coming."
pytorch,6318,"example want code encounter issue, raise judge tensor list?",0,bug bytetensor,"bug bytetensor example want code encounter issue, raise judge tensor list?"
pytorch,7645,"issue description max pooling functions consistent max functions. example, every max pooling 1d, 2d 3d, adaptive acts same, cpu cuda. essentially, two fondamental differences max pooling values max pooling nan valid values valid values, means get ignored, , soon value, result . generally, choosing explicetely deal numpy e.g. https docs.scipy.org doc numpy 1.14.0 reference generated numpy.nanmax.html could solution, maybe related cudnn's max pooling ? code example system info built latest sources 05 17 pytorch version 0.5.0a0 331a04d debug build cuda used build pytorch 9.1.85 os ubuntu 16.04.4 lts gcc version ubuntu 5.4.0 6ubuntu1 16.04.9 5.4.0 20160609 cmake version version 3.9.4 python version 3.6 cuda available yes cuda runtime version 9.1.85 gpu models configuration gpu 0 quadro m1000m nvidia driver version 390.30 cudnn version probably one following usr lib x86 64 linux gnu libcudnn.so.7.1.3 usr lib x86 64 linux gnu libcudnn static v7.a versions relevant libraries conda magma cuda91 2.3.0 1 pytorch conda torch 0.5.0a0 331a04d conda torch 0.3.1b0 2b47480 conda torch 0.5.0a0 4251e38",0,max pooling behavior nan values,"max pooling behavior nan values issue description max pooling functions consistent max functions. example, every max pooling 1d, 2d 3d, adaptive acts same, cpu cuda. essentially, two fondamental differences max pooling values max pooling nan valid values valid values, means get ignored, , soon value, result . generally, choosing explicetely deal numpy e.g. https docs.scipy.org doc numpy 1.14.0 reference generated numpy.nanmax.html could solution, maybe related cudnn's max pooling ? code example system info built latest sources 05 17 pytorch version 0.5.0a0 331a04d debug build cuda used build pytorch 9.1.85 os ubuntu 16.04.4 lts gcc version ubuntu 5.4.0 6ubuntu1 16.04.9 5.4.0 20160609 cmake version version 3.9.4 python version 3.6 cuda available yes cuda runtime version 9.1.85 gpu models configuration gpu 0 quadro m1000m nvidia driver version 390.30 cudnn version probably one following usr lib x86 64 linux gnu libcudnn.so.7.1.3 usr lib x86 64 linux gnu libcudnn static v7.a versions relevant libraries conda magma cuda91 2.3.0 1 pytorch conda torch 0.5.0a0 331a04d conda torch 0.3.1b0 2b47480 conda torch 0.5.0a0 4251e38"
pytorch,22135,19228 update type annotations. fix this.,0,dataloader type annotation broken,dataloader type annotation broken 19228 update type annotations. fix this.
pytorch,24006,"abosolutely pytorch code use 1 gpu train model, every time results use mutli gpus train model, every time result random, pazzled!",0,result random,"result random abosolutely pytorch code use 1 gpu train model, every time results use mutli gpus train model, every time result random, pazzled!"
pytorch,22298,hi. implementing mnasnet c side torchvision ran problem. apparently using inside possible. minimal code yf225 look?,0,c can't use sequential inside sequential,c can't use sequential inside sequential hi. implementing mnasnet c side torchvision ran problem. apparently using inside possible. minimal code yf225 look?
pytorch,26189,"bug able compare boolean 0,1 exporter maybe use fale,true constants. comparing bool 0 results error https github.com pytorch pytorch issues 25805 reproduce expected behavior export 0 false constant valid onnx environment pytorch version 1.2.0 debug build cuda used build pytorch 10.0.130 os ubuntu 16.04.6 lts gcc version ubuntu 5.4.0 6ubuntu1 16.04.11 5.4.0 20160609 cmake version version 3.14.0 python version 2.7 cuda available yes cuda runtime version could collect gpu models configuration gpu 0 geforce gtx 1070 nvidia driver version 418.40.04 cudnn version usr local lib libcudnn.so.5.1.10 versions relevant libraries pip mictorch 0.0.1 pip numpy 1.16.4 pip torch 1.2.0 pip torchvision 0.4.0 conda could collect additional context work around",0,"onnx bool comparison 0,1 export false,true constants","onnx bool comparison 0,1 export false,true constants bug able compare boolean 0,1 exporter maybe use fale,true constants. comparing bool 0 results error https github.com pytorch pytorch issues 25805 reproduce expected behavior export 0 false constant valid onnx environment pytorch version 1.2.0 debug build cuda used build pytorch 10.0.130 os ubuntu 16.04.6 lts gcc version ubuntu 5.4.0 6ubuntu1 16.04.11 5.4.0 20160609 cmake version version 3.14.0 python version 2.7 cuda available yes cuda runtime version could collect gpu models configuration gpu 0 geforce gtx 1070 nvidia driver version 418.40.04 cudnn version usr local lib libcudnn.so.5.1.10 versions relevant libraries pip mictorch 0.0.1 pip numpy 1.16.4 pip torch 1.2.0 pip torchvision 0.4.0 conda could collect additional context work around"
pytorch,16288,bug jit unable append reproduce,0,jit unable append tuple tensor list,jit unable append tuple tensor list bug jit unable append reproduce
pytorch,4048,"self explanatory. currently easy way maintain dictionary attribute module whose values modules need registered. introducing moduledict class functions modulelist, except exposes dictionary interface, would resolve this. following practical benefits users 1 checkpointing much better moduledict strings keys modules values modulelist. personally run problems change order modules modulelist try load old checkpoints, checkpoint key names order dependent modulelists, breaks code. 2 combines advantages modulelist dynamic assignment modules class advantages using named attributes documentation readability sane module names checkpoints . another pro simple implement. first pass, support feature clean submit pr",0,"feature request moduledict, like modulelist","feature request moduledict, like modulelist self explanatory. currently easy way maintain dictionary attribute module whose values modules need registered. introducing moduledict class functions modulelist, except exposes dictionary interface, would resolve this. following practical benefits users 1 checkpointing much better moduledict strings keys modules values modulelist. personally run problems change order modules modulelist try load old checkpoints, checkpoint key names order dependent modulelists, breaks code. 2 combines advantages modulelist dynamic assignment modules class advantages using named attributes documentation readability sane module names checkpoints . another pro simple implement. first pass, support feature clean submit pr"
pytorch,16806,"bug execute , get run time error message tells report bug . valid argument , reasonable get error. however, error message seems suggest error handled properly source code. reproduce steps reproduce behavior 1. 2. 3. expected behavior proper error message issued. environment pytorch version e.g., 1.0 1.0.0 os e.g., linux windows 10 installed pytorch , , source pip build command used compiling source n python version 3.6.2 cuda cudnn version n gpu models configuration n relevant information n",0,storage initialized assert failed execute tensor.resize 1,"storage initialized assert failed execute tensor.resize 1 bug execute , get run time error message tells report bug . valid argument , reasonable get error. however, error message seems suggest error handled properly source code. reproduce steps reproduce behavior 1. 2. 3. expected behavior proper error message issued. environment pytorch version e.g., 1.0 1.0.0 os e.g., linux windows 10 installed pytorch , , source pip build command used compiling source n python version 3.6.2 cuda cudnn version n gpu models configuration n relevant information n"
pytorch,7396,nan,0,jit script implement train scriptmodules,jit script implement train scriptmodules nan
pytorch,27255,bug following fails compile list comprehensions iterables works tensors. cc suo,0,jit list comprehensions tensors work,jit list comprehensions tensors work bug following fails compile list comprehensions iterables works tensors. cc suo
pytorch,4872,"updated pytorch 0.3 using conda. realized memory leak ram experiments. later, tried compile source python cudnn 6... 7005, versions cudnn solved problem. possible update cudnn 7003 7005 conda pip packages?",0,memory leak cudnn 7003,"memory leak cudnn 7003 updated pytorch 0.3 using conda. realized memory leak ram experiments. later, tried compile source python cudnn 6... 7005, versions cudnn solved problem. possible update cudnn 7003 7005 conda pip packages?"
pytorch,3514,"hello again, trying concatenate two tensor together using applied tensor created numpy array. example instead try works what's wrong? cheers, alessandro",0,unable use cat torch.longtensor,"unable use cat torch.longtensor hello again, trying concatenate two tensor together using applied tensor created numpy array. example instead try works what's wrong? cheers, alessandro"
pytorch,31611,"bug looks like https github.com pytorch pytorch issues 15992 coming back split torch library. reproduce steps reproduce behavior 1. following script throws torch linked cuda gets 0 output. 2. following one working correctly gets 1 output. expected behavior behaviour keeps consistent code snippet 1 code snippet 2. environment please copy paste output environment collection script https raw.githubusercontent.com pytorch pytorch master torch utils collect env.py fill checklist manually . get script run pytorch version e.g., 1.0 master os e.g., linux windows installed pytorch , , source source build command used compiling source python setup.py build python version matter cuda cudnn version matter gpu models configuration matter relevant information additional context cc ezyang gchanan zou3519 ngimel peterjc123",0,example program using libtorch linked torch cuda use cuda,"example program using libtorch linked torch cuda use cuda bug looks like https github.com pytorch pytorch issues 15992 coming back split torch library. reproduce steps reproduce behavior 1. following script throws torch linked cuda gets 0 output. 2. following one working correctly gets 1 output. expected behavior behaviour keeps consistent code snippet 1 code snippet 2. environment please copy paste output environment collection script https raw.githubusercontent.com pytorch pytorch master torch utils collect env.py fill checklist manually . get script run pytorch version e.g., 1.0 master os e.g., linux windows installed pytorch , , source source build command used compiling source python setup.py build python version matter cuda cudnn version matter gpu models configuration matter relevant information additional context cc ezyang gchanan zou3519 ngimel peterjc123"
pytorch,2032,"................................................................................................... sorry, reopen think bug within rnn module could't get help discuss.pytorch run similar experiment one linear layer neural network output correct. reasonable explanation would rnn module keep different set parameters optimization reason? mix end batch training. following code duplicate problem. ........................................................................................................ encounter problem trying implement seq2seq familiarize new framework. issue seems related parameters sharing mini batch. setup dummy training set one mini batch. mini batch 3 data entries it. input, different outputs training data 4,5,1,7,8 , 4,5,1,7,8 , 4,5,1,7,8 , input 1 0 , 0 , 0 , input 2 1 , 3 , 5 target theory, model never learn data contradiction. however, loss reach near 0 hundred epochs. split 3 data entries 3 mini batch, model learn data set correct result. model must keeping different set parameter position mini batch? parameters updated mini batch forward backward? someone tell misunderstood something?",0,possible bug rnn module,"possible bug rnn module ................................................................................................... sorry, reopen think bug within rnn module could't get help discuss.pytorch run similar experiment one linear layer neural network output correct. reasonable explanation would rnn module keep different set parameters optimization reason? mix end batch training. following code duplicate problem. ........................................................................................................ encounter problem trying implement seq2seq familiarize new framework. issue seems related parameters sharing mini batch. setup dummy training set one mini batch. mini batch 3 data entries it. input, different outputs training data 4,5,1,7,8 , 4,5,1,7,8 , 4,5,1,7,8 , input 1 0 , 0 , 0 , input 2 1 , 3 , 5 target theory, model never learn data contradiction. however, loss reach near 0 hundred epochs. split 3 data entries 3 mini batch, model learn data set correct result. model must keeping different set parameter position mini batch? parameters updated mini batch forward backward? someone tell misunderstood something?"
pytorch,27512,"bug floordiv bound int, tensor, results incorrectly inserted cast.. cc suo",0,jit floordiv bound tensor,"jit floordiv bound tensor bug floordiv bound int, tensor, results incorrectly inserted cast.. cc suo"
pytorch,2620,,0,nn.embedding error using max norm,nn.embedding error using max norm 
pytorch,1829,"hi, built pytorch os x cuda support. trouble using cuda features seems run memory even size 1 tensor. laptop fairly old, nvidia geforce gt 650m 1024 mb. cuda version 8.0. torch version . os x 10.12.4. using python 3.5.3 anaconda.",0,cuda memory size 1 tensor,"cuda memory size 1 tensor hi, built pytorch os x cuda support. trouble using cuda features seems run memory even size 1 tensor. laptop fairly old, nvidia geforce gt 650m 1024 mb. cuda version 8.0. torch version . os x 10.12.4. using python 3.5.3 anaconda."
pytorch,2735,nan,0,nk,nk nan
pytorch,26893,bug exporting models use method work reproduce minimal example gives cc matthewfeickert kratsg,0,onnx export einsum supported,onnx export einsum supported bug exporting models use method work reproduce minimal example gives cc matthewfeickert kratsg
pytorch,24568,porting th operators essential code simplicity performance reasons. porting guides q available umbrella issue 24507 feel free add vitalyfedyunin reviewer get prioritized review.,0,migrate ge ge th aten cuda,migrate ge ge th aten cuda porting th operators essential code simplicity performance reasons. porting guides q available umbrella issue 24507 feel free add vitalyfedyunin reviewer get prioritized review.
pytorch,31248,"bug using hi, always get runtimeerror training process reproduce kl loss term loss function, assume two distributions multivariate normal distributions, calculated follows environment package version pytorch 1.3.1 ubuntu 18.04 cuda 9.2.148 cudnn 7.6.3 0 additional find kl loss falls range 1e 6 1e 5. ideas solve problem? thx",0,"runtimeerror cholesky cuda batch 0 u 6,6 zero, singular u.","runtimeerror cholesky cuda batch 0 u 6,6 zero, singular u. bug using hi, always get runtimeerror training process reproduce kl loss term loss function, assume two distributions multivariate normal distributions, calculated follows environment package version pytorch 1.3.1 ubuntu 18.04 cuda 9.2.148 cudnn 7.6.3 0 additional find kl loss falls range 1e 6 1e 5. ideas solve problem? thx"
pytorch,24686,porting th operators essential code simplicity performance reasons. porting guides q available umbrella issue 24507 feel free add vitalyfedyunin reviewer get prioritized review.,0,migrate clamp clamp th aten cpu,migrate clamp clamp th aten cpu porting th operators essential code simplicity performance reasons. porting guides q available umbrella issue 24507 feel free add vitalyfedyunin reviewer get prioritized review.
pytorch,12133,"questions help please note issue tracker help form issue closed. set listed resources available website https pytorch.org resources . primary means support discussion forum discussion forum https discuss.pytorch.org know initialize tensor const vector dims,const vector values, basecontext context , cause dont't know init context.",0,initialize caffe2 tensor,"initialize caffe2 tensor questions help please note issue tracker help form issue closed. set listed resources available website https pytorch.org resources . primary means support discussion forum discussion forum https discuss.pytorch.org know initialize tensor const vector dims,const vector values, basecontext context , cause dont't know init context."
pytorch,2490,"file 'torch lib th cmakelists.txt', line 473 477, libquadmath repeated.",0,libquadmath repeated th cmakelists.txt,"libquadmath repeated th cmakelists.txt file 'torch lib th cmakelists.txt', line 473 477, libquadmath repeated."
pytorch,5918,see https github.com pytorch pytorch pull 5911,0,lint .gitmodules aten .gitmodules synchronized,lint .gitmodules aten .gitmodules synchronized see https github.com pytorch pytorch pull 5911
pytorch,25859,"moment, attractively named macro use report warnings c . told you... using function could cause code take lock sorts performance nastiness like that? well, that's exactly macro does, calls custom warning handler that, case python extension, calls python code file python warning. bad bad bad, performance reasons, also could cause deadlock file python warning, must acquire gil . better strategy python handler must buffer warnings, normally return back python acquire gil, actually report warnings. correct place add logic probably also handle catching c exceptions converting python exceptions.",0,torch warn must take gil buffer warnings,"torch warn must take gil buffer warnings moment, attractively named macro use report warnings c . told you... using function could cause code take lock sorts performance nastiness like that? well, that's exactly macro does, calls custom warning handler that, case python extension, calls python code file python warning. bad bad bad, performance reasons, also could cause deadlock file python warning, must acquire gil . better strategy python handler must buffer warnings, normally return back python acquire gil, actually report warnings. correct place add logic probably also handle catching c exceptions converting python exceptions."
pytorch,10004,"looking https github.com pytorch pytorch blob 3609977d7f4629f75dd1f8d904ddd5b52388124f torch nn utils spectral norm.py l53 code noticed easy way make , differentiable wrt useful normalizing constant depends , , depend . starting point power iteration avoid cumulating gradients problem allowing gradients pass power iteration. adds computational overhead optional.",0,detaching gradients instead using grad spectral norm,"detaching gradients instead using grad spectral norm looking https github.com pytorch pytorch blob 3609977d7f4629f75dd1f8d904ddd5b52388124f torch nn utils spectral norm.py l53 code noticed easy way make , differentiable wrt useful normalizing constant depends , , depend . starting point power iteration avoid cumulating gradients problem allowing gradients pass power iteration. adds computational overhead optional."
pytorch,25171,"hi, studying pytorch internals, especially trying find avx implementation. avx2. built pytorch source figured avx intrinsic function called simple tensor addition. debugged gdb, set breakpoints succeeded it. result gdb. avx intrinsic called 0 now, trying find exists avx acceleration cnn, debugged really simple code pytorch tutorial https pytorch.org tutorials beginner blitz cifar10 tutorial.html sphx glr beginner blitz cifar10 tutorial py procedure above. cannot find avx implementation. wondering missed options built pytorch. googled lot, looking inside cmake system, figured yet. one noticed avx.cpp th directory. searching th, also noticed libth.so file. blog pytorch homepage https pytorch.org blog tour pytorch internals 2 backend torch vendor libraries , libth.so.1 torch lib. result directory. addition, blog https apaszke.github.io torch internals.html , simd directory , it. anyone tell missing?",0,relation avx th?,"relation avx th? hi, studying pytorch internals, especially trying find avx implementation. avx2. built pytorch source figured avx intrinsic function called simple tensor addition. debugged gdb, set breakpoints succeeded it. result gdb. avx intrinsic called 0 now, trying find exists avx acceleration cnn, debugged really simple code pytorch tutorial https pytorch.org tutorials beginner blitz cifar10 tutorial.html sphx glr beginner blitz cifar10 tutorial py procedure above. cannot find avx implementation. wondering missed options built pytorch. googled lot, looking inside cmake system, figured yet. one noticed avx.cpp th directory. searching th, also noticed libth.so file. blog pytorch homepage https pytorch.org blog tour pytorch internals 2 backend torch vendor libraries , libth.so.1 torch lib. result directory. addition, blog https apaszke.github.io torch internals.html , simd directory , it. anyone tell missing?"
pytorch,24088,"bug torch.eye work dtype torch.bool device cpu, work gpu. reproduce torch.eye 5, dtype torch.bool, device torch.device cpu traceback recent call last file , line 1, runtimeerror eye implemented 'bool' torch.eye 5, dtype torch.bool, device torch.device cuda tensor true, false, false, false, false , false, true, false, false, false , false, false, true, false, false , false, false, false, true, false , false, false, false, false, true , device 'cuda 0' expected behavior expecting torch.eye bool implemented cpu cuda. maybe torch functions issue well? environment collecting environment information... pytorch version 1.2.0 debug build cuda used build pytorch 10.0.130 os ubuntu 18.04.3 lts gcc version ubuntu 7.4.0 1ubuntu1 18.04.1 7.4.0 cmake version could collect python version 3.6 cuda available yes cuda runtime version could collect gpu models configuration gpu 0 geforce gtx 1060 nvidia driver version 430.40 cudnn version could collect versions relevant libraries pip3 numpy 1.17.0 pip3 torch 1.2.0 pip3 torchvision 0.4.0 conda could collect pytorch version e.g., 1.0 1.2 os e.g., linux ubuntu 18.04 installed pytorch , , source pip python version 3.6.8 cuda cudnn version 10.1 gpu models configuration gtx 1060 6gb additional context workaround cpu casting torch.bool calling torch.eye.",0,runtimeerror eye implemented 'bool' cpu,"runtimeerror eye implemented 'bool' cpu bug torch.eye work dtype torch.bool device cpu, work gpu. reproduce torch.eye 5, dtype torch.bool, device torch.device cpu traceback recent call last file , line 1, runtimeerror eye implemented 'bool' torch.eye 5, dtype torch.bool, device torch.device cuda tensor true, false, false, false, false , false, true, false, false, false , false, false, true, false, false , false, false, false, true, false , false, false, false, false, true , device 'cuda 0' expected behavior expecting torch.eye bool implemented cpu cuda. maybe torch functions issue well? environment collecting environment information... pytorch version 1.2.0 debug build cuda used build pytorch 10.0.130 os ubuntu 18.04.3 lts gcc version ubuntu 7.4.0 1ubuntu1 18.04.1 7.4.0 cmake version could collect python version 3.6 cuda available yes cuda runtime version could collect gpu models configuration gpu 0 geforce gtx 1060 nvidia driver version 430.40 cudnn version could collect versions relevant libraries pip3 numpy 1.17.0 pip3 torch 1.2.0 pip3 torchvision 0.4.0 conda could collect pytorch version e.g., 1.0 1.2 os e.g., linux ubuntu 18.04 installed pytorch , , source pip python version 3.6.8 cuda cudnn version 10.1 gpu models configuration gtx 1060 6gb additional context workaround cpu casting torch.bool calling torch.eye."
pytorch,29117,cc ezyang gchanan zou3519 jerryzh168 pietern mrshenli pritamdamania87 zhaojuanmao satgera gqchen aazzolini rohan varma xush6528,0,tracking issue rpc tests flaky,tracking issue rpc tests flaky cc ezyang gchanan zou3519 jerryzh168 pietern mrshenli pritamdamania87 zhaojuanmao satgera gqchen aazzolini rohan varma xush6528
pytorch,7634,"issue description running trained model github ,i watched gpu memory, found script exited memory usage got 2g. gpu 1060 6g memory ,how come? ps torch might 2g limit,but install system info runtimeerror cuda runtime err 2 memory opt conda conda bld pytorch 1501953625411 work pytorch 0.1.12 torch libthc thcstorage.cu 66 pytorch caffe2 installed pytorch conda, pip, source build command used compiling source os ubuntu 14.04 pytorch version pytorch 0.1.10 python version 2.7 cuda cudnn version 8.0 5.1 gcc version compiling source 4.9",0,cuda memory err gpu memory still 4g left,"cuda memory err gpu memory still 4g left issue description running trained model github ,i watched gpu memory, found script exited memory usage got 2g. gpu 1060 6g memory ,how come? ps torch might 2g limit,but install system info runtimeerror cuda runtime err 2 memory opt conda conda bld pytorch 1501953625411 work pytorch 0.1.12 torch libthc thcstorage.cu 66 pytorch caffe2 installed pytorch conda, pip, source build command used compiling source os ubuntu 14.04 pytorch version pytorch 0.1.10 python version 2.7 cuda cudnn version 8.0 5.1 gcc version compiling source 4.9"
pytorch,2117,"quite inconvenient user calculate shape tensor manually. think would better entitle nn.linear ability infer shape input tensor. one possible solution like numpy's reshape function, nn.linear 1, hidden size nn.linear none, hidden size means automatic inferring. preliminary implementation https gist.github.com vovallen 1420e410e3dfd368b8dc9061ad0c206a implementation create overhead training process. also think kind style used layers nn.conv. feel free add comments.",0,add dynamic infer shape nn.linear,"add dynamic infer shape nn.linear quite inconvenient user calculate shape tensor manually. think would better entitle nn.linear ability infer shape input tensor. one possible solution like numpy's reshape function, nn.linear 1, hidden size nn.linear none, hidden size means automatic inferring. preliminary implementation https gist.github.com vovallen 1420e410e3dfd368b8dc9061ad0c206a implementation create overhead training process. also think kind style used layers nn.conv. feel free add comments."
pytorch,30627,"bug custom location packaging previously set , got everything copied custom place. happens ubuntu 18.04, centos 7 seems fine build script. started see recently. builds 2 weeks ago still fine. ! image https user images.githubusercontent.com 5203025 69997152 5ba9d580 1508 11ea 98c2 fa155ffc8d4e.png environment pytorch version e.g., 1.0 master os e.g., linux ubuntu 18.04 docker installed pytorch , , source source build command used compiling source cmake ninja python version 3.6.9 system default cuda cudnn version 10.2 cc ezyang gchanan zou3519",0,everything usr local include copied.,"everything usr local include copied. bug custom location packaging previously set , got everything copied custom place. happens ubuntu 18.04, centos 7 seems fine build script. started see recently. builds 2 weeks ago still fine. ! image https user images.githubusercontent.com 5203025 69997152 5ba9d580 1508 11ea 98c2 fa155ffc8d4e.png environment pytorch version e.g., 1.0 master os e.g., linux ubuntu 18.04 docker installed pytorch , , source source build command used compiling source cmake ninja python version 3.6.9 system default cuda cudnn version 10.2 cc ezyang gchanan zou3519"
pytorch,11076,"question would like help support, please ask forums https discuss.pytorch.org . submitting feature request, please preface title feature request . submitting bug report, please fill following details. issue description provide short description. code example please try provide minimal example repro bug. error messages stack traces also helpful. system info please copy paste output environment collection script https raw.githubusercontent.com pytorch pytorch master torch utils collect env.py fill checklist manually . get script run pytorch caffe2 installed pytorch conda, pip, source build command used compiling source os pytorch version python version cuda cudnn version gpu models configuration gcc version compiling source cmake version versions relevant libraries",0,"caffe2 testing, please ignore","caffe2 testing, please ignore question would like help support, please ask forums https discuss.pytorch.org . submitting feature request, please preface title feature request . submitting bug report, please fill following details. issue description provide short description. code example please try provide minimal example repro bug. error messages stack traces also helpful. system info please copy paste output environment collection script https raw.githubusercontent.com pytorch pytorch master torch utils collect env.py fill checklist manually . get script run pytorch caffe2 installed pytorch conda, pip, source build command used compiling source os pytorch version python version cuda cudnn version gpu models configuration gcc version compiling source cmake version versions relevant libraries"
pytorch,31554,runtimeerror error dlopen dlsym libcaffe2 nvrtc.so cannot open shared object file file directory environment 1. pytorch 1.3.1 2. linux 3. pycharm,0,runtimeerror error dlopen dlsym libcaffe2 nvrtc.so cannot open shared object file file directory,runtimeerror error dlopen dlsym libcaffe2 nvrtc.so cannot open shared object file file directory runtimeerror error dlopen dlsym libcaffe2 nvrtc.so cannot open shared object file file directory environment 1. pytorch 1.3.1 2. linux 3. pycharm
pytorch,15262,"pytorch 1.0 doc used measuring error reconstruction example auto encoder. note targets numbers 0 1. import torch import torch.nn.functional f b 0.2,0.1 c 1.,3. aa torch.tensor b,requires grad true target torch.tensor c output f.binary cross entropy aa,target,reduce true,size average true z output.sum z.backward print output tensor 4.1532, grad fn target value 1 3.,not 0 1.",0,doc err torch.nn.bceloss,"doc err torch.nn.bceloss pytorch 1.0 doc used measuring error reconstruction example auto encoder. note targets numbers 0 1. import torch import torch.nn.functional f b 0.2,0.1 c 1.,3. aa torch.tensor b,requires grad true target torch.tensor c output f.binary cross entropy aa,target,reduce true,size average true z output.sum z.backward print output tensor 4.1532, grad fn target value 1 3.,not 0 1."
pytorch,22542,"bug bug appears pytorch 1.1 file lr scheduler.py line 686 lr scheduler cosineannealingwarmrestarts , parameter name mult line 686 name mul used instead python interpreter know variable mul . reproduce bug easily reproduced mult parameter cosineannealingwarmrestart scheduler set properly i.e. 1 instance int , causing python raise error line 686 mul used instead mult . following line raise error cause bug 2.0 instance int. expected behavior line 686 lr scheduler.py modified environment pytorch version e.g., 1.0 1.1 os e.g., linux linux installed pytorch , , source pip build command used compiling source python version 3.7 cuda cudnn version 10 gpu models configuration 1080 ti relevant information n",0,bug cosineannealingwarmrestarts mul instead mult,"bug cosineannealingwarmrestarts mul instead mult bug bug appears pytorch 1.1 file lr scheduler.py line 686 lr scheduler cosineannealingwarmrestarts , parameter name mult line 686 name mul used instead python interpreter know variable mul . reproduce bug easily reproduced mult parameter cosineannealingwarmrestart scheduler set properly i.e. 1 instance int , causing python raise error line 686 mul used instead mult . following line raise error cause bug 2.0 instance int. expected behavior line 686 lr scheduler.py modified environment pytorch version e.g., 1.0 1.1 os e.g., linux linux installed pytorch , , source pip build command used compiling source python version 3.7 cuda cudnn version 10 gpu models configuration 1080 ti relevant information n"
pytorch,5563,"pytorch github issues guidelines like limit issues bug reports feature requests. question would like help support, please visit forums https discuss.pytorch.org submitting feature request, please preface title feature request . submitting bug report, please include following information relevant os pytorch version installed pytorch conda, pip, source python version cuda cudnn version gpu models configuration gcc version compiling source addition, including following information also helpful us diagnose problem script reproduce bug. please try provide minimal test case possible. error messages stack traces bug context around trying ! issue1 https user images.githubusercontent.com 19254992 36958144 c011b122 205f 11e8 82c2 fe0b0e0ad4f3.png image error running terminal link bracket os 16.04 pytorch version 0.31 latest , installed using conda, python version 3.5 3.6 . cuda 8.0 cudnn 5.1 . gcc version 5.4.0 cloned repository https github.com thstkdgus35 edsr pytorch trying run demo.sh file runs main.py file imports pytorch. problem able import torch home directory python3 shell whenever go folder directory run pytorch code import torch python3 shell gives error module named torch see image details . also installed torch luajit home default root directory . might possible reason.please help reference code image. ! issue1 https user images.githubusercontent.com 19254992 36958482 2c7fe048 2062 11e8 86e8 b10f25f471cc.png",0,import error module named torch,"import error module named torch pytorch github issues guidelines like limit issues bug reports feature requests. question would like help support, please visit forums https discuss.pytorch.org submitting feature request, please preface title feature request . submitting bug report, please include following information relevant os pytorch version installed pytorch conda, pip, source python version cuda cudnn version gpu models configuration gcc version compiling source addition, including following information also helpful us diagnose problem script reproduce bug. please try provide minimal test case possible. error messages stack traces bug context around trying ! issue1 https user images.githubusercontent.com 19254992 36958144 c011b122 205f 11e8 82c2 fe0b0e0ad4f3.png image error running terminal link bracket os 16.04 pytorch version 0.31 latest , installed using conda, python version 3.5 3.6 . cuda 8.0 cudnn 5.1 . gcc version 5.4.0 cloned repository https github.com thstkdgus35 edsr pytorch trying run demo.sh file runs main.py file imports pytorch. problem able import torch home directory python3 shell whenever go folder directory run pytorch code import torch python3 shell gives error module named torch see image details . also installed torch luajit home default root directory . might possible reason.please help reference code image. ! issue1 https user images.githubusercontent.com 19254992 36958482 2c7fe048 2062 11e8 86e8 b10f25f471cc.png"
pytorch,8039,issue description code example system info detect even though installed,0,runtimeerror cuda runtime error 30 unknown error pytorch aten src thc thcgeneral.cpp 70,runtimeerror cuda runtime error 30 unknown error pytorch aten src thc thcgeneral.cpp 70 issue description code example system info detect even though installed
pytorch,3225,"snippet used work, throws error",0,subtracting bytetensor work case.,"subtracting bytetensor work case. snippet used work, throws error"
pytorch,19920,implementation ctc decoder kenlm pytorch? found 3rd party implementation longer supported pytorch now?,0,ctc decoder kenlm language model,ctc decoder kenlm language model implementation ctc decoder kenlm pytorch? found 3rd party implementation longer supported pytorch now?
pytorch,7498,deploy pytorch trained model production ?,0,production,production deploy pytorch trained model production ?
pytorch,19601,bug https circleci.com gh pytorch pytorch 1423249?utm campaign vcs integration link utm medium referral utm source github build link console,0,testjit.test cpp broken master,testjit.test cpp broken master bug https circleci.com gh pytorch pytorch 1423249?utm campaign vcs integration link utm medium referral utm source github build link console
pytorch,19024,see https discuss.pytorch.org huge loss dataparallel 40749 original report. seems regressed 0.4.1 1.0 needs debugged. cc mrshenli,0,loss explosion dataparallel wgan models,loss explosion dataparallel wgan models see https discuss.pytorch.org huge loss dataparallel 40749 original report. seems regressed 0.4.1 1.0 needs debugged. cc mrshenli
pytorch,28868,"questions help hello, currently worked freshly merged feature pytorch vision 1401 able find way make caffe2 work onnx operation set 10? way build caffe2 source opset?",0,build caffe2 onnx opset version greater 9?,"build caffe2 onnx opset version greater 9? questions help hello, currently worked freshly merged feature pytorch vision 1401 able find way make caffe2 work onnx operation set 10? way build caffe2 source opset?"
pytorch,3822,"related https github.com pytorch pytorch pull 3817. building source ubuntu 16.04, get following. reverting offending pr solves problem.",0,building source master broken,"building source master broken related https github.com pytorch pytorch pull 3817. building source ubuntu 16.04, get following. reverting offending pr solves problem."
pytorch,19742,"bug scenario redis cache set call set pickle.dumps tensor value call get pickle.dumps tensor call get pickle.dumps tensor different call hit call 1, potential miss call 2 inconsistent hits misses occur 1000 calls tensors objects repeated 9 times reproduce steps reproduce behavior skeleton 0. import redis 0. import time 0. value torch.tensor 0. start time.time 1. redis cache redis.strictredis host localhost , port 6379, db 0 redis cache 2. redis cache.set pickle.dumps value , pickle.dumps hidden, cell 3. cache return redis cache.get pickle.dumps value cache return none logger.info cache hit took ms .format value, time.time start return pickle.loads cache return 4. cache return redis cache.get pickle.dumps value cache return none logger.info cache hit took ms .format value, time.time start return pickle.loads cache return cache miss logging would happen .set called example log 2019 04 25 14 04 00,344 id 1 info cache encoder cache hit tensor 2, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 3 took 0.00019598007202148438 ms 2019 04 25 14 04 00,347 id 1 info cache encoder cache miss tensor 2, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 3 took 0.001859426498413086 ms weird thing hit happened miss, looks like something unreliable serialisation 2019 04 25 14 04 00,303 id 1 info cache encoder cache hit tensor 2, 12, 3 took 0.0001957416534423828 ms 2019 04 25 14 04 00,312 id 1 info cache encoder cache miss tensor 2, 12, 3 took 0.0018868446350097656 ms expected behavior consistent cache hits tensor set pickle.dumps cache return environment pytorch version e.g., 1.0 1.01 os e.g., linux ubuntu 16.04 installed pytorch , , source pip install build command used compiling source python version 3.6 cuda cudnn version 8 gpu models configuration geforce gtx 1050 relevant information additional context using str value instead pickle.dumps value , cache hits consistent however, large tensors get injected str output, ultimately fails . makes think problem pickle.dumps serializes tensors. saw similar issue https github.com pytorch pytorch issues 9168 changes improving speed along lines moving byteio . related issue thought would post issue here.",0,"serialization tensors pickle.dumps seems inconsistent, leading inconsistent redis cache hit miss","serialization tensors pickle.dumps seems inconsistent, leading inconsistent redis cache hit miss bug scenario redis cache set call set pickle.dumps tensor value call get pickle.dumps tensor call get pickle.dumps tensor different call hit call 1, potential miss call 2 inconsistent hits misses occur 1000 calls tensors objects repeated 9 times reproduce steps reproduce behavior skeleton 0. import redis 0. import time 0. value torch.tensor 0. start time.time 1. redis cache redis.strictredis host localhost , port 6379, db 0 redis cache 2. redis cache.set pickle.dumps value , pickle.dumps hidden, cell 3. cache return redis cache.get pickle.dumps value cache return none logger.info cache hit took ms .format value, time.time start return pickle.loads cache return 4. cache return redis cache.get pickle.dumps value cache return none logger.info cache hit took ms .format value, time.time start return pickle.loads cache return cache miss logging would happen .set called example log 2019 04 25 14 04 00,344 id 1 info cache encoder cache hit tensor 2, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 3 took 0.00019598007202148438 ms 2019 04 25 14 04 00,347 id 1 info cache encoder cache miss tensor 2, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 3 took 0.001859426498413086 ms weird thing hit happened miss, looks like something unreliable serialisation 2019 04 25 14 04 00,303 id 1 info cache encoder cache hit tensor 2, 12, 3 took 0.0001957416534423828 ms 2019 04 25 14 04 00,312 id 1 info cache encoder cache miss tensor 2, 12, 3 took 0.0018868446350097656 ms expected behavior consistent cache hits tensor set pickle.dumps cache return environment pytorch version e.g., 1.0 1.01 os e.g., linux ubuntu 16.04 installed pytorch , , source pip install build command used compiling source python version 3.6 cuda cudnn version 8 gpu models configuration geforce gtx 1050 relevant information additional context using str value instead pickle.dumps value , cache hits consistent however, large tensors get injected str output, ultimately fails . makes think problem pickle.dumps serializes tensors. saw similar issue https github.com pytorch pytorch issues 9168 changes improving speed along lines moving byteio . related issue thought would post issue here."
pytorch,4573,"soumith looks like tutorial http pytorch.org tutorials intermediate char rnn classification tutorial.html bug code looks like output forward pass done input processed data computed rnn cell ie self.i2h combined so, become guys agree, happy make minor frustrating know what's going change docs.",0,documentation char rnn classification tutorial wrong output cell calculation,"documentation char rnn classification tutorial wrong output cell calculation soumith looks like tutorial http pytorch.org tutorials intermediate char rnn classification tutorial.html bug code looks like output forward pass done input processed data computed rnn cell ie self.i2h combined so, become guys agree, happy make minor frustrating know what's going change docs."
pytorch,5198,easy implement. need figure good name api. can't reuse ambiguous input 2d. sounds weird me..,0,feature request support batch diag,feature request support batch diag easy implement. need figure good name api. can't reuse ambiguous input 2d. sounds weird me..
pytorch,7544,"dropout always scale 1 1 p training, want get original outputs. get it?",0,make dropout 'not scaling' training?,"make dropout 'not scaling' training? dropout always scale 1 1 p training, want get original outputs. get it?"
pytorch,992,jzbontar reports serialization storage remapping trying load somewhere codepath. get error loading cuda tensor cpu machine. code use https gist.github.com jzbontar b50f8c9dd22e49ff68c7c91dad63166a. error assertionerror ' nfound nvidia driver system... machines use pytorch version 0.1.10.,0,loading gpu checkpoints cpu remapping trying load torch.cuda,loading gpu checkpoints cpu remapping trying load torch.cuda jzbontar reports serialization storage remapping trying load somewhere codepath. get error loading cuda tensor cpu machine. code use https gist.github.com jzbontar b50f8c9dd22e49ff68c7c91dad63166a. error assertionerror ' nfound nvidia driver system... machines use pytorch version 0.1.10.
pytorch,9990,"getting following compile error trying build caffe2 git master cmake command output https bpaste.net show f14fd917039e https bpaste.net show f14fd917039e caffe2 pytorch stable release v0.4.1 builds fine cmake options opencl . caffe2 git master builds fine using cmake options switching opencl . error seems related gcc version used. error occurs gcc8, gcc7 gcc5.4. system information os arch linux x86 64 compiler gcc 8.1.1 mentioned, error also occurs gcc7 gcc5.4 caffe2 git master opencl headers 2.2.20170516",0,build fails caffe2 git master opencl,"build fails caffe2 git master opencl getting following compile error trying build caffe2 git master cmake command output https bpaste.net show f14fd917039e https bpaste.net show f14fd917039e caffe2 pytorch stable release v0.4.1 builds fine cmake options opencl . caffe2 git master builds fine using cmake options switching opencl . error seems related gcc version used. error occurs gcc8, gcc7 gcc5.4. system information os arch linux x86 64 compiler gcc 8.1.1 mentioned, error also occurs gcc7 gcc5.4 caffe2 git master opencl headers 2.2.20170516"
pytorch,2240,"hello, error. running macos 10.12.6 16g29 , python 2.7 anaconda 3, cuda 8.0.90, apple llvm version 8.0.0 clang 800.0.42.1 . installed llvm 8.0.0 specifically since one listed cuda docs. successfully built pytorch source python 3.6 computer before, clueless happening. use python 3 now. hope information helpful you.",0,error building torch source python 2.7 macos,"error building torch source python 2.7 macos hello, error. running macos 10.12.6 16g29 , python 2.7 anaconda 3, cuda 8.0.90, apple llvm version 8.0.0 clang 800.0.42.1 . installed llvm 8.0.0 specifically since one listed cuda docs. successfully built pytorch source python 3.6 computer before, clueless happening. use python 3 now. hope information helpful you."
pytorch,12127,"try use checkpoint function,when train,gpu keeps memory work train progress work either without error print . make checkpoint function work.",0,use checkpoint function,"use checkpoint function try use checkpoint function,when train,gpu keeps memory work train progress work either without error print . make checkpoint function work."
pytorch,20717,"bug reproduce training , validation test dataset. created dataloaders three training shuffle true valdiation test shuffle false . one code epoch loop contains iterator train validation another one , epoch loop contains iterator train , validation test. check output code 1 2 1. https www.kaggle.com suchith0312 pytorch dataloader testing?scriptversionid 14425205 2. https www.kaggle.com suchith0312 pytorch dataloader testing?scriptversionid 14425182 expected behavior train id's output files must different. environment pytorch version 1.0.1.post2 python version 3.6.6 cuda version 10.0 cudnn version 7.4.2",0,problem dataloader,"problem dataloader bug reproduce training , validation test dataset. created dataloaders three training shuffle true valdiation test shuffle false . one code epoch loop contains iterator train validation another one , epoch loop contains iterator train , validation test. check output code 1 2 1. https www.kaggle.com suchith0312 pytorch dataloader testing?scriptversionid 14425205 2. https www.kaggle.com suchith0312 pytorch dataloader testing?scriptversionid 14425182 expected behavior train id's output files must different. environment pytorch version 1.0.1.post2 python version 3.6.6 cuda version 10.0 cudnn version 7.4.2"
pytorch,6472,"minimum example torch.zeros 10 torch.longtensor 11 returns bogus result 0.4, error 0.3.1 fail torch.zeros 10 10 torch.zeros 10 11 torch.zeros 10 torch.longtensor 10 works correctly 0.3.1.post2. work 0.4.0a0 df039e2. compiled",0,advanced indexing validate negative indexes regression,"advanced indexing validate negative indexes regression minimum example torch.zeros 10 torch.longtensor 11 returns bogus result 0.4, error 0.3.1 fail torch.zeros 10 10 torch.zeros 10 11 torch.zeros 10 torch.longtensor 10 works correctly 0.3.1.post2. work 0.4.0a0 df039e2. compiled"
pytorch,23002,"pass zero tensor categorical, expect torch throw exception. instead, python quits unexpectedly",0,wrong input categorical makes python quit unexpectedly,"wrong input categorical makes python quit unexpectedly pass zero tensor categorical, expect torch throw exception. instead, python quits unexpectedly"
pytorch,19305,"bug reproduce code auto facedetect forward tensor image .totuple c10 cuda cudacachingallocator emptycache ...some tensor ops gpu boundingboxesofone torch masked select boundingboxesofone, mask facedetect sfd net,after forward ,if call emptycache ,it cost 170ms time emptycache call emptycache ,masked select cost 170ms know optimize situationthks environment libtorch version 1.0 os windows10 cuda cudnn version 9.0 gpu models configuration gtx 1060 additional context",0,libtorch gpu efficiency,"libtorch gpu efficiency bug reproduce code auto facedetect forward tensor image .totuple c10 cuda cudacachingallocator emptycache ...some tensor ops gpu boundingboxesofone torch masked select boundingboxesofone, mask facedetect sfd net,after forward ,if call emptycache ,it cost 170ms time emptycache call emptycache ,masked select cost 170ms know optimize situationthks environment libtorch version 1.0 os windows10 cuda cudnn version 9.0 gpu models configuration gtx 1060 additional context"
pytorch,16045,"issue description computing loss shapes , error thrown. indeed, colleagues tell case version 0.4, lost version 1.0. code example leave y.reshape commented out, correct estimates straightforward linear regression problem found loss fails decrease region near global optimum convex problem. yet, error raised. uncomment y.reshape, correct estimates found loss decreases anticipated way. system info collecting environment information... pytorch version 1.0.0 debug build cuda used build pytorch none os ubuntu 16.04.5 lts gcc version ubuntu 5.4.0 6ubuntu1 16.04.11 5.4.0 20160609 cmake version could collect python version 3.5 cuda available cuda runtime version cuda gpu models configuration cuda nvidia driver version cuda cudnn version cuda versions relevant libraries pip could collect conda blas 1.0 mkl conda mkl 2017.0.3 0 conda torch 1.0.0 conda torchvision 0.2.1",0,"bug fail throw error computing loss tensors shapes n, 1 n","bug fail throw error computing loss tensors shapes n, 1 n issue description computing loss shapes , error thrown. indeed, colleagues tell case version 0.4, lost version 1.0. code example leave y.reshape commented out, correct estimates straightforward linear regression problem found loss fails decrease region near global optimum convex problem. yet, error raised. uncomment y.reshape, correct estimates found loss decreases anticipated way. system info collecting environment information... pytorch version 1.0.0 debug build cuda used build pytorch none os ubuntu 16.04.5 lts gcc version ubuntu 5.4.0 6ubuntu1 16.04.11 5.4.0 20160609 cmake version could collect python version 3.5 cuda available cuda runtime version cuda gpu models configuration cuda nvidia driver version cuda cudnn version cuda versions relevant libraries pip could collect conda blas 1.0 mkl conda mkl 2017.0.3 0 conda torch 1.0.0 conda torchvision 0.2.1"
pytorch,10303,"code dataset before, got following error. however could work well half mouth ago. pytorch's version 0.3.1, find version 0.4.0 also meeting error raceback recent call last file main.py , line 341, main file main.py , line 141, main train train loader, model, criterion, optimizer, epoch file main.py , line 192, train prec1, prec5 accuracy output.data, target, topk 1,5 file main.py , line 329, accuracy , pred output.topk maxk, 1, true, true runtimeerror invalid argument 5 k range dimension pytorch torch lib thc generic thctensortopk.cu 21 pytorch torch lib thcunn classnllcriterion.cu 101 void cunn classnllcriterion updateoutput kernel dtype , dtype , dtype , long , dtype , int, int, int, int, long dtype float, acctype float block 0,0,0 , thread 0,0,0 assertion 0 n classes failed. pytorch torch lib thcunn classnllcriterion.cu 101 void cunn classnllcriterion updateoutput kernel dtype , dtype , dtype , long , dtype , int, int, int, int, long dtype float, acctype float block 0,0,0 , thread 1,0,0 assertion 0 n classes failed. pytorch torch lib thcunn classnllcriterion.cu 101 void cunn classnllcriterion updateoutput kernel dtype , dtype , dtype , long , dtype , int, int, int, int, long dtype float, acctype float block 0,0,0 , thread 2,0,0 assertion 0 n classes failed. pytorch torch lib thcunn classnllcriterion.cu 101 void cunn classnllcriterion updateoutput kernel dtype , dtype , dtype , long , dtype , int, int, int, int, long dtype float, acctype float block 0,0,0 , thread 3,0,0 assertion 0 n classes failed. thcudacheck fail file pytorch torch lib thc generic thcstorage.c line 184 error 59 device side assert triggered terminate called throwing instance 'std runtime error' cuda runtime error 59 device side assert triggered pytorch torch lib thc generic thcstorage.c 184 aborted core dumped",0,cuda runtime error 59 device side assert running torch.topk,"cuda runtime error 59 device side assert running torch.topk code dataset before, got following error. however could work well half mouth ago. pytorch's version 0.3.1, find version 0.4.0 also meeting error raceback recent call last file main.py , line 341, main file main.py , line 141, main train train loader, model, criterion, optimizer, epoch file main.py , line 192, train prec1, prec5 accuracy output.data, target, topk 1,5 file main.py , line 329, accuracy , pred output.topk maxk, 1, true, true runtimeerror invalid argument 5 k range dimension pytorch torch lib thc generic thctensortopk.cu 21 pytorch torch lib thcunn classnllcriterion.cu 101 void cunn classnllcriterion updateoutput kernel dtype , dtype , dtype , long , dtype , int, int, int, int, long dtype float, acctype float block 0,0,0 , thread 0,0,0 assertion 0 n classes failed. pytorch torch lib thcunn classnllcriterion.cu 101 void cunn classnllcriterion updateoutput kernel dtype , dtype , dtype , long , dtype , int, int, int, int, long dtype float, acctype float block 0,0,0 , thread 1,0,0 assertion 0 n classes failed. pytorch torch lib thcunn classnllcriterion.cu 101 void cunn classnllcriterion updateoutput kernel dtype , dtype , dtype , long , dtype , int, int, int, int, long dtype float, acctype float block 0,0,0 , thread 2,0,0 assertion 0 n classes failed. pytorch torch lib thcunn classnllcriterion.cu 101 void cunn classnllcriterion updateoutput kernel dtype , dtype , dtype , long , dtype , int, int, int, int, long dtype float, acctype float block 0,0,0 , thread 3,0,0 assertion 0 n classes failed. thcudacheck fail file pytorch torch lib thc generic thcstorage.c line 184 error 59 device side assert triggered terminate called throwing instance 'std runtime error' cuda runtime error 59 device side assert triggered pytorch torch lib thc generic thcstorage.c 184 aborted core dumped"
pytorch,24093,"bug installing pytorch conda new environment, always installs compiled cuda version 10 cudatoolkit 10.0. whether conda install pytorch 1.2, 1.1, 1.0.1, 1.0 cuda90 always tries install cuda 10 version package well cudatoolkit 10.0. via pytorch conda channel. machine older driver version can't easily upgraded. also tried pip installing https download.pytorch.org whl cu90 stable still insists using cuda 10. reproduce conda create name test env python 3.6 conda activate test env conda install pytorch 1.1 cuda90 c pytorch conda list python run torch.version.cuda also lists cuda 10. following new packages installed blas 1.0 mkl cffi 1.12.3 py36h2e261b9 0 cuda90 1.0 h6433d27 0 pytorch cudatoolkit 10.0.130 0 intel openmp 2019.4 243 libgfortran ng 7.3.0 hdf63c60 0 mkl 2019.4 243 mkl fft 1.0.12 py36ha843d7b 0 mkl random 1.0.2 py36hd81dba3 0 ninja 1.9.0 py36hfd86e86 0 numpy 1.16.4 py36h7e9f1db 0 numpy base 1.16.4 py36hde5b4d6 0 pycparser 2.19 py36 0 pytorch 1.1.0 py3.6 cuda10.0.130 cudnn7.5.1 0 pytorch proceed n ? n expected behavior able select cuda version. environment collecting environment information... pytorch version n debug build n cuda used build pytorch n os ubuntu 16.04.3 lts gcc version ubuntu 5.4.0 6ubuntu1 16.04.10 5.4.0 20160609 cmake version version 3.5.1 python version 3.6 cuda available n cuda runtime version could collect gpu models configuration gpu 0 titan xp gpu 1 titan xp gpu 2 titan xp gpu 3 titan xp nvidia driver version 390.67 cudnn version usr lib x86 64 linux gnu libcudnn.so.7.1.3 versions relevant libraries pip could collect conda could collect",0,pytorch forced use cuda 10 new update,"pytorch forced use cuda 10 new update bug installing pytorch conda new environment, always installs compiled cuda version 10 cudatoolkit 10.0. whether conda install pytorch 1.2, 1.1, 1.0.1, 1.0 cuda90 always tries install cuda 10 version package well cudatoolkit 10.0. via pytorch conda channel. machine older driver version can't easily upgraded. also tried pip installing https download.pytorch.org whl cu90 stable still insists using cuda 10. reproduce conda create name test env python 3.6 conda activate test env conda install pytorch 1.1 cuda90 c pytorch conda list python run torch.version.cuda also lists cuda 10. following new packages installed blas 1.0 mkl cffi 1.12.3 py36h2e261b9 0 cuda90 1.0 h6433d27 0 pytorch cudatoolkit 10.0.130 0 intel openmp 2019.4 243 libgfortran ng 7.3.0 hdf63c60 0 mkl 2019.4 243 mkl fft 1.0.12 py36ha843d7b 0 mkl random 1.0.2 py36hd81dba3 0 ninja 1.9.0 py36hfd86e86 0 numpy 1.16.4 py36h7e9f1db 0 numpy base 1.16.4 py36hde5b4d6 0 pycparser 2.19 py36 0 pytorch 1.1.0 py3.6 cuda10.0.130 cudnn7.5.1 0 pytorch proceed n ? n expected behavior able select cuda version. environment collecting environment information... pytorch version n debug build n cuda used build pytorch n os ubuntu 16.04.3 lts gcc version ubuntu 5.4.0 6ubuntu1 16.04.10 5.4.0 20160609 cmake version version 3.5.1 python version 3.6 cuda available n cuda runtime version could collect gpu models configuration gpu 0 titan xp gpu 1 titan xp gpu 2 titan xp gpu 3 titan xp nvidia driver version 390.67 cudnn version usr lib x86 64 linux gnu libcudnn.so.7.1.3 versions relevant libraries pip could collect conda could collect"
pytorch,15016,bug reproduce steps reproduce behavior expected behavior environment pytorch version 1.0.0a0 54b5dd9 debug build cuda used build pytorch 9.2.88 os centos linux 7 core gcc version gcc 4.8.5 20150623 red hat 4.8.5 28 cmake version version 3.12.2 python version 3.5 cuda available yes cuda runtime version could collect gpu models configuration gpu 0 tesla m40 gpu 1 tesla m40 nvidia driver version 396.26 cudnn version probably one following usr local cuda 8.0 targets x86 64 linux lib libcudnn.so.5.0.5 usr local cuda 8.0 targets x86 64 linux lib libcudnn.so.5.1.3 usr local cuda 8.0 targets x86 64 linux lib libcudnn.so.6.0.20 usr local cuda 8.0 targets x86 64 linux lib libcudnn.so.6.0.21 usr local cuda 8.0 targets x86 64 linux lib libcudnn static.a usr local cuda 9.2 targets x86 64 linux lib libcudnn.so.7.1.2 usr local cuda 9.2 targets x86 64 linux lib libcudnn.so.7.4.1 usr local cuda 9.2 targets x86 64 linux lib libcudnn static.a usr local fbcode gcc 5 glibc 2.23 lib libcudnn.so.6 usr local fbcode gcc 5 glibc 2.23 lib libcudnn.so.6.0.20 usr local fbcode gcc 5 glibc 2.23 lib libcudnn.so.6.0.21 usr local fbcode gcc 5 glibc 2.23 lib libcudnn.so.7.1.2 usr local fbcode gcc 5 glibc 2.23 lib libcudnn.so.7.1.4 usr local fbcode platform007 lib libcudnn.so.7.1.4 versions relevant libraries pip could collect conda magma cuda92 2.4.0 1 pytorch conda torch 1.0.0a0 54b5dd9,0,torch.tril support 0 sized dims,torch.tril support 0 sized dims bug reproduce steps reproduce behavior expected behavior environment pytorch version 1.0.0a0 54b5dd9 debug build cuda used build pytorch 9.2.88 os centos linux 7 core gcc version gcc 4.8.5 20150623 red hat 4.8.5 28 cmake version version 3.12.2 python version 3.5 cuda available yes cuda runtime version could collect gpu models configuration gpu 0 tesla m40 gpu 1 tesla m40 nvidia driver version 396.26 cudnn version probably one following usr local cuda 8.0 targets x86 64 linux lib libcudnn.so.5.0.5 usr local cuda 8.0 targets x86 64 linux lib libcudnn.so.5.1.3 usr local cuda 8.0 targets x86 64 linux lib libcudnn.so.6.0.20 usr local cuda 8.0 targets x86 64 linux lib libcudnn.so.6.0.21 usr local cuda 8.0 targets x86 64 linux lib libcudnn static.a usr local cuda 9.2 targets x86 64 linux lib libcudnn.so.7.1.2 usr local cuda 9.2 targets x86 64 linux lib libcudnn.so.7.4.1 usr local cuda 9.2 targets x86 64 linux lib libcudnn static.a usr local fbcode gcc 5 glibc 2.23 lib libcudnn.so.6 usr local fbcode gcc 5 glibc 2.23 lib libcudnn.so.6.0.20 usr local fbcode gcc 5 glibc 2.23 lib libcudnn.so.6.0.21 usr local fbcode gcc 5 glibc 2.23 lib libcudnn.so.7.1.2 usr local fbcode gcc 5 glibc 2.23 lib libcudnn.so.7.1.4 usr local fbcode platform007 lib libcudnn.so.7.1.4 versions relevant libraries pip could collect conda magma cuda92 2.4.0 1 pytorch conda torch 1.0.0a0 54b5dd9
pytorch,14653,repro output potential super confusing end users. probably preserve original source highlighting,0,jit error reporting imported modules highlights serialization code,jit error reporting imported modules highlights serialization code repro output potential super confusing end users. probably preserve original source highlighting
pytorch,29024,"model based e inception,after quantized , found forward slower, original model run cpu cost 200ms 20ms gpu , quantized model cost 290ms, caused operation layers concatenate?? cc jerryzh168 jianyuh dzhulgakov raghuramank100 jamesr66a",0,speed slows model quantization,"speed slows model quantization model based e inception,after quantized , found forward slower, original model run cpu cost 200ms 20ms gpu , quantized model cost 290ms, caused operation layers concatenate?? cc jerryzh168 jianyuh dzhulgakov raghuramank100 jamesr66a"
pytorch,31483,"although installed pytorch text python python3 using want text classification tutorial, get error know missing here. way fix that? cc ezyang",0,undefined symbol py zerostruct,"undefined symbol py zerostruct although installed pytorch text python python3 using want text classification tutorial, get error know missing here. way fix that? cc ezyang"
pytorch,20048,"building pytorch source past couple months. since around 1 maybe 2 month ago unable forward passing models using package pytorch. mistaken, last version pytorch built still allows use ing https github.com pytorch pytorch tree bad4442 one precisely, version pytorch using allows ing . able ing pretty annoying cause written work flow way depends using ing. one part work flow start tens processes forward passing model cpu memory. pretty useful super efficient since weights model copied every time start new process. however, processes weights model needs get copied gonna efficient anymore. another part framework, start process model still cpu's memory within process model copied gpu memory. bit inefficient still allows start 4 5 processes depending gpu memory forward pass inputs. here's pseudocode current versions pytorch since 1 2 months ago cannot anymore. afraid happening due indirect affects commits intend disable ing. wonder guys either revert changes caused enable feature again. also, using pytorch versions master branch since 1 2 months ago might get following within started process might somewhat relevant lazy initialization cuda https github.com pytorch pytorch pull 2811 .",0,forking possible anymore using pytorch version 2 months ago,"forking possible anymore using pytorch version 2 months ago building pytorch source past couple months. since around 1 maybe 2 month ago unable forward passing models using package pytorch. mistaken, last version pytorch built still allows use ing https github.com pytorch pytorch tree bad4442 one precisely, version pytorch using allows ing . able ing pretty annoying cause written work flow way depends using ing. one part work flow start tens processes forward passing model cpu memory. pretty useful super efficient since weights model copied every time start new process. however, processes weights model needs get copied gonna efficient anymore. another part framework, start process model still cpu's memory within process model copied gpu memory. bit inefficient still allows start 4 5 processes depending gpu memory forward pass inputs. here's pseudocode current versions pytorch since 1 2 months ago cannot anymore. afraid happening due indirect affects commits intend disable ing. wonder guys either revert changes caused enable feature again. also, using pytorch versions master branch since 1 2 months ago might get following within started process might somewhat relevant lazy initialization cuda https github.com pytorch pytorch pull 2811 ."
pytorch,8652,"trying use dataloader sampler strings index dataset. default batchsampler https github.com pytorch pytorch blob master torch utils data sampler.py l104 threw error essentially str cannot casted int. cast question happens line 139 currently copied source batch loader master, removed cast, everything worked expected. torch.utils.data https pytorch.org docs stable data.html dataloader suggest samplers return integers, various samplers say list indices. index set integers. dataset say getitem support integer indexing, why? really, choice sampler including choosing default default sampler drives whether dataset must indexable integers. hand, sometimes sensible key dataset. example, sampler could request filename loaded. or, actual data could organized dict like examples . along way, noticed batchloader asserting sampler sampler. case 0.4.0, master . really understand why. duck type sampler iterable? try except around loop catches typeerror returns whatever type error. breaks example mistaken, excludes useful way unittest. mwe",0,batch sampler dataset indexing restricted,"batch sampler dataset indexing restricted trying use dataloader sampler strings index dataset. default batchsampler https github.com pytorch pytorch blob master torch utils data sampler.py l104 threw error essentially str cannot casted int. cast question happens line 139 currently copied source batch loader master, removed cast, everything worked expected. torch.utils.data https pytorch.org docs stable data.html dataloader suggest samplers return integers, various samplers say list indices. index set integers. dataset say getitem support integer indexing, why? really, choice sampler including choosing default default sampler drives whether dataset must indexable integers. hand, sometimes sensible key dataset. example, sampler could request filename loaded. or, actual data could organized dict like examples . along way, noticed batchloader asserting sampler sampler. case 0.4.0, master . really understand why. duck type sampler iterable? try except around loop catches typeerror returns whatever type error. breaks example mistaken, excludes useful way unittest. mwe"
pytorch,9383,"second dimension must complex imaginary part, clear docs. currently https pytorch.org docs master torch.html?highlight eig torch.eig without specified shapes",0,docs make clear format torch.eig eigenvalues,"docs make clear format torch.eig eigenvalues second dimension must complex imaginary part, clear docs. currently https pytorch.org docs master torch.html?highlight eig torch.eig without specified shapes"
pytorch,2011,running import torch gives following error latest master git log commit ebdec9a837074a303fd5ffb6f319cd593955becc author lynic date fri jul 7 23 06 56 2017 0800 skip distributed tests supported 2004,0,"build passing, runtime failing commit ebdec9a837074a303fd5ffb6f319cd593955becc","build passing, runtime failing commit ebdec9a837074a303fd5ffb6f319cd593955becc running import torch gives following error latest master git log commit ebdec9a837074a303fd5ffb6f319cd593955becc author lynic date fri jul 7 23 06 56 2017 0800 skip distributed tests supported 2004"
pytorch,30717,excerpt circleci build https circleci.com gh pytorch pytorch 3768299 branch cc ezyang gchanan zou3519 ailzhang,0,testtorchdevicetypexla could start grpc server,testtorchdevicetypexla could start grpc server excerpt circleci build https circleci.com gh pytorch pytorch 3768299 branch cc ezyang gchanan zou3519 ailzhang
pytorch,15187,"bug trying build pytorch complex https github.com roger luo pytorch complex work 3 month following mac os x mojave. got following error. bunch others tried build one test , build script pytorch complex. still got this. reproduce steps reproduce behavior 1. git clone pytorch 1.0.0 later 2. git clone , run expected behavior get error. environment additional context search, guess might relate https github.com martinmoene gsl lite issues 63 however, found includes strange... cc ezyang",0,complex extension work,"complex extension work bug trying build pytorch complex https github.com roger luo pytorch complex work 3 month following mac os x mojave. got following error. bunch others tried build one test , build script pytorch complex. still got this. reproduce steps reproduce behavior 1. git clone pytorch 1.0.0 later 2. git clone , run expected behavior get error. environment additional context search, guess might relate https github.com martinmoene gsl lite issues 63 however, found includes strange... cc ezyang"
pytorch,7428,"steps reproduce 1. build commit https github.com pytorch pytorch pull 7275 commits 8d10a9245e364920e7fdba3af03b3a14ff41f126 cuda pytorch linux xenial cuda9 cudnn7 py3 test alternately, ezyang test jit hangtest jitcpp build libtorch bin test jit expected result terminates actual result hang ctrl c finally comes back cc lantiga goldsborough",0,libtorch test jit hangs error occurs,"libtorch test jit hangs error occurs steps reproduce 1. build commit https github.com pytorch pytorch pull 7275 commits 8d10a9245e364920e7fdba3af03b3a14ff41f126 cuda pytorch linux xenial cuda9 cudnn7 py3 test alternately, ezyang test jit hangtest jitcpp build libtorch bin test jit expected result terminates actual result hang ctrl c finally comes back cc lantiga goldsborough"
pytorch,923,"building form source mac os x fails. turns missing dependency thpp thcs, adding dependency cmakelists.txt sufficient successful build.",0,build broken mac os x,"build broken mac os x building form source mac os x fails. turns missing dependency thpp thcs, adding dependency cmakelists.txt sufficient successful build."
pytorch,20465,"bug convnet training gpu penalizing gradient growth backpropagating gradient gradient following error happens indexvalue 0 indexvalue src.sizes dim environment pytorch version 1.1.0 debug build cuda used build pytorch 10.0.130 os ubuntu 18.04.1 lts gcc version ubuntu 7.3.0 27ubuntu1 18.04 7.3.0 cmake version version 3.6.2 python version 3.6 cuda available yes cuda runtime version 9.1.85 gpu models configuration gpu 0 titan x pascal gpu 1 titan x pascal gpu 2 titan x pascal gpu 3 titan x pascal nvidia driver version 418.56 cudnn version probably one following usr lib x86 64 linux gnu libcudnn.so.7.1.3 usr local cuda 9.0 lib64 libcudnn.so.7.2.1 versions relevant libraries pip3 numpy 1.15.0 pip3 torch 1.1.0 pip3 torchfile 0.1.0 pip3 torchvision 0.2.2 conda blas 1.0 mkl conda cuda92 1.0 0 pytorch conda mkl 2018.0.3 1 conda mkl fft 1.0.4 py36h4414c95 1 conda mkl random 1.0.1 py36h4414c95 1 conda pytorch 1.1.0 py3.6 cuda10.0.130 cudnn7.5.1 0 pytorch conda torchfile 0.1.0 py 0 conda forge conda torchvision 0.2.2 py 3 pytorch additional context change coefficient 0.1 10 error happen least 40 epochs , reduce learning rate 1 0.001.",0,second order gradient cuda error,"second order gradient cuda error bug convnet training gpu penalizing gradient growth backpropagating gradient gradient following error happens indexvalue 0 indexvalue src.sizes dim environment pytorch version 1.1.0 debug build cuda used build pytorch 10.0.130 os ubuntu 18.04.1 lts gcc version ubuntu 7.3.0 27ubuntu1 18.04 7.3.0 cmake version version 3.6.2 python version 3.6 cuda available yes cuda runtime version 9.1.85 gpu models configuration gpu 0 titan x pascal gpu 1 titan x pascal gpu 2 titan x pascal gpu 3 titan x pascal nvidia driver version 418.56 cudnn version probably one following usr lib x86 64 linux gnu libcudnn.so.7.1.3 usr local cuda 9.0 lib64 libcudnn.so.7.2.1 versions relevant libraries pip3 numpy 1.15.0 pip3 torch 1.1.0 pip3 torchfile 0.1.0 pip3 torchvision 0.2.2 conda blas 1.0 mkl conda cuda92 1.0 0 pytorch conda mkl 2018.0.3 1 conda mkl fft 1.0.4 py36h4414c95 1 conda mkl random 1.0.1 py36h4414c95 1 conda pytorch 1.1.0 py3.6 cuda10.0.130 cudnn7.5.1 0 pytorch conda torchfile 0.1.0 py 0 conda forge conda torchvision 0.2.2 py 3 pytorch additional context change coefficient 0.1 10 error happen least 40 epochs , reduce learning rate 1 0.001."
pytorch,12981,"sorry trouble you, get error start pytorch set pytorch build stable os windows package conda language python 3.6 cuda 8.0 shows run command pytorchpip3 first command alright, shows requested packages already installed. import torch. second command shows tried use , shows message. find error message,but",0,torchvision install error,"torchvision install error sorry trouble you, get error start pytorch set pytorch build stable os windows package conda language python 3.6 cuda 8.0 shows run command pytorchpip3 first command alright, shows requested packages already installed. import torch. second command shows tried use , shows message. find error message,but"
pytorch,16277,bug hi going torchvision code implement c api noticed inconsistency. line 298 inception.py stddev class type basicconv2d set 0.01 line 60 checks conv2d linear basicconv2d none put code inside previous statement printed convs inside conv1 basicconv2d inceptionaux get initialized stddev 0.1 error intentional?,0,inconsistency inception model,inconsistency inception model bug hi going torchvision code implement c api noticed inconsistency. line 298 inception.py stddev class type basicconv2d set 0.01 line 60 checks conv2d linear basicconv2d none put code inside previous statement printed convs inside conv1 basicconv2d inceptionaux get initialized stddev 0.1 error intentional?
pytorch,18354,large,0,port spatialreflectionpadding temporalreflectionpadding aten,port spatialreflectionpadding temporalreflectionpadding aten large
pytorch,16042,"bug hi, sorry bug tried post forums getting response reproduce import torch x torch.randn 1,1,1,1 .requires grad true torch.nn.conv2d 1,1,1 x .mean 2 conv grad, torch.autograd.grad y, x, create graph true, retain graph true print conv grad ,conv grad torch.nn.linear 1,1 x .mean 2 linear grad, torch.autograd.grad y, x, create graph true, retain graph true print linear grad ,linear grad expected behavior think gradient variables associated grad fn, however linear version does. math here, 1x1 convolution vs 1x1 linear layer? maybe missing something",0,grad fn missing?,"grad fn missing? bug hi, sorry bug tried post forums getting response reproduce import torch x torch.randn 1,1,1,1 .requires grad true torch.nn.conv2d 1,1,1 x .mean 2 conv grad, torch.autograd.grad y, x, create graph true, retain graph true print conv grad ,conv grad torch.nn.linear 1,1 x .mean 2 linear grad, torch.autograd.grad y, x, create graph true, retain graph true print linear grad ,linear grad expected behavior think gradient variables associated grad fn, however linear version does. math here, 1x1 convolution vs 1x1 linear layer? maybe missing something"
pytorch,23823,"issue description pytorch 1.1.0 cannot installed windows 10 python 3.7 installed microsoft store. reason combination long python installation path used microsoft store installer b long filename path used within pytorch collect distribute fpn rpn proposals op test.test collect dist.zip c default, windows 10 still refuses paths longer 256 characters code example fails error message error caused 256 character limit windows paths. file pathname long, matter short path 257 characters longer. system info pytorch version 1.1.0 os microsoft windows 10 pro, version 1903 python version 3.7 versions relevant libraries pip3 numpy 1.17.0 pip3 torch 1.1.0 pip3 torchvision 0.3.0",0,pytorch cannot installed windows 10 python 3.7 installed microsoft store,"pytorch cannot installed windows 10 python 3.7 installed microsoft store issue description pytorch 1.1.0 cannot installed windows 10 python 3.7 installed microsoft store. reason combination long python installation path used microsoft store installer b long filename path used within pytorch collect distribute fpn rpn proposals op test.test collect dist.zip c default, windows 10 still refuses paths longer 256 characters code example fails error message error caused 256 character limit windows paths. file pathname long, matter short path 257 characters longer. system info pytorch version 1.1.0 os microsoft windows 10 pro, version 1903 python version 3.7 versions relevant libraries pip3 numpy 1.17.0 pip3 torch 1.1.0 pip3 torchvision 0.3.0"
pytorch,24537,porting th operators essential code simplicity performance reasons. porting guides q available umbrella issue 24507 feel free add vitalyfedyunin reviewer get prioritized review.,0,migrate asin asin th aten cuda,migrate asin asin th aten cuda porting th operators essential code simplicity performance reasons. porting guides q available umbrella issue 24507 feel free add vitalyfedyunin reviewer get prioritized review.
pytorch,2870,"size reported end 32, 1 , 32, 1, 1 . using torch 0.2.0.post3 cpu gpu.",0,torch.max extra dimensions slice size 1,"torch.max extra dimensions slice size 1 size reported end 32, 1 , 32, 1, 1 . using torch 0.2.0.post3 cpu gpu."
pytorch,22269,"add user facing function, expose torch.jit. c.function details https github.com pytorch pytorch pull 22090 discussion r296443544",0,"add user facing function, expose torch.jit. c.function","add user facing function, expose torch.jit. c.function add user facing function, expose torch.jit. c.function details https github.com pytorch pytorch pull 22090 discussion r296443544"
pytorch,7525,"sample failing ci build https ci.pytorch.org jenkins job pytorch builds job pytorch linux trusty pynightly test 6167 console investigation, error happens use directly e.g., use , goes away use newer python 3.7 nightly e.g., deadsnakes . might bug python 3.7 slash setuptools subsequently fixed. unfortunately, preinstall tqdm sufficient avoid problem. torchvision might enough. giving try.",0,tqdm install failing valueerror bad marshal data unknown type code,"tqdm install failing valueerror bad marshal data unknown type code sample failing ci build https ci.pytorch.org jenkins job pytorch builds job pytorch linux trusty pynightly test 6167 console investigation, error happens use directly e.g., use , goes away use newer python 3.7 nightly e.g., deadsnakes . might bug python 3.7 slash setuptools subsequently fixed. unfortunately, preinstall tqdm sufficient avoid problem. torchvision might enough. giving try."
pytorch,3326,"example gives runtimeerror expected variable argument, got torch.longtensor",0,nn.embeddingbag seem support none offset 2d tensors,"nn.embeddingbag seem support none offset 2d tensors example gives runtimeerror expected variable argument, got torch.longtensor"
pytorch,8867,"currently mix actual types script trace function module causes issues, example, since inline scriptmodule instance graphexecutor https github.com pytorch pytorch blob 2b926aafb09575dd83ebf4fe2a76cbe239596f0b torch csrc jit script init.cpp l82 . causes problems case calling script traced modules script functions show s. let's make script,traced function,module produce module, work inlining logic merge modules cases. greatly simplify logic make things easier understand maintain",0,jit normalize representation traced scripted functions modules,"jit normalize representation traced scripted functions modules currently mix actual types script trace function module causes issues, example, since inline scriptmodule instance graphexecutor https github.com pytorch pytorch blob 2b926aafb09575dd83ebf4fe2a76cbe239596f0b torch csrc jit script init.cpp l82 . causes problems case calling script traced modules script functions show s. let's make script,traced function,module produce module, work inlining logic merge modules cases. greatly simplify logic make things easier understand maintain"
pytorch,17335,nan,0,.circleci regenerate.sh work run root directory,.circleci regenerate.sh work run root directory nan
pytorch,19162,"feature function denormalize image based mean standard deviation. motivation working images nn's trained specific dataset example imagenet , image first normalized mean standard deviation dataset. want save image later process use function torchvision.utils.save image . however image still normalized different mean standard deviation compared original image. option denormalize image initial normalization undone saved image mean std. pitch extra parameter torchvision.utils.save image function denormalize image based mean standardization array. alternatives one way tackle problem currently use transforms.normalize function. current implementation shown below. one flaw implementation image clipped keep values 0 1. thus information lost. sure operation lossless.",0,denormalize option torchvision.utils.save image,"denormalize option torchvision.utils.save image feature function denormalize image based mean standard deviation. motivation working images nn's trained specific dataset example imagenet , image first normalized mean standard deviation dataset. want save image later process use function torchvision.utils.save image . however image still normalized different mean standard deviation compared original image. option denormalize image initial normalization undone saved image mean std. pitch extra parameter torchvision.utils.save image function denormalize image based mean standardization array. alternatives one way tackle problem currently use transforms.normalize function. current implementation shown below. one flaw implementation image clipped keep values 0 1. thus information lost. sure operation lossless."
pytorch,5066,"tried update pytorch 0.2 0.3. update fine, run codes, raises exception cxxabi 1.3.8' found required home sliu426 anaconda2 lib python2.7 site packages rdkit chem .. .. .. .. . . libicui18n.so.58 import torch rdkit import chem pip install pytorch , guess installation changes paths. simply switch two lines codes works well solution little tricky, wondering there's better one?",0,import error updating 0.3,"import error updating 0.3 tried update pytorch 0.2 0.3. update fine, run codes, raises exception cxxabi 1.3.8' found required home sliu426 anaconda2 lib python2.7 site packages rdkit chem .. .. .. .. . . libicui18n.so.58 import torch rdkit import chem pip install pytorch , guess installation changes paths. simply switch two lines codes works well solution little tricky, wondering there's better one?"
pytorch,30796,"questions help want build pytorch os built protobuf lib rather third part protobuf, prefix change, anyone help me? please note issue tracker help form issue closed. set listed resources available website https pytorch.org resources . primary means support discussion forum discussion forum https discuss.pytorch.org",0,build pytorch local protobuf rather third party protobuf?,"build pytorch local protobuf rather third party protobuf? questions help want build pytorch os built protobuf lib rather third part protobuf, prefix change, anyone help me? please note issue tracker help form issue closed. set listed resources available website https pytorch.org resources . primary means support discussion forum discussion forum https discuss.pytorch.org"
pytorch,123,"torch.inttensor 10 .random 4 returning integers 0 3 instead 1 4, make consistent numpy.random.randint?",0,0 indexed random,"0 indexed random torch.inttensor 10 .random 4 returning integers 0 3 instead 1 4, make consistent numpy.random.randint?"
pytorch,9987,"could fix submodule found problem caffe2? remote repository found. fatal repository 'https github.com rlovelett eigen.git ' found fatal clone 'https github.com rlovelett eigen.git' submodule path ' home ly workspace git pose caffe2 third party eigen' failed failed clone 'third party eigen' second time, aborting",0,eigen submodule found,"eigen submodule found could fix submodule found problem caffe2? remote repository found. fatal repository 'https github.com rlovelett eigen.git ' found fatal clone 'https github.com rlovelett eigen.git' submodule path ' home ly workspace git pose caffe2 third party eigen' failed failed clone 'third party eigen' second time, aborting"
pytorch,24918,"several test suites, example test indexing, throw deprecation warnings like",0,lot deprecation warnings tests,"lot deprecation warnings tests several test suites, example test indexing, throw deprecation warnings like"
pytorch,27690,"bug multi gpu gathering much slower scattering reproduce run following script multi gpu machine replicate issue. creates large tensor cpu scatters multiple gpus, also creates large tested pytorch 1.1.0 pytorch 1.2.0. output note batch size args.sz 128, tensor scattered gathered 1gib. ngpus batch size scatter time gather time 2 64 0.07 0.35 2 128 0.14 0.7 2 256 0.27 1.4 4 64 0.12 0.35 4 128 0.16 0.7 4 256 0.29 1.6 expected behavior would expect scatter gather timings lot closer this, factor 5 out. environment",0,multi gpu gather much slower scatter,"multi gpu gather much slower scatter bug multi gpu gathering much slower scattering reproduce run following script multi gpu machine replicate issue. creates large tensor cpu scatters multiple gpus, also creates large tested pytorch 1.1.0 pytorch 1.2.0. output note batch size args.sz 128, tensor scattered gathered 1gib. ngpus batch size scatter time gather time 2 64 0.07 0.35 2 128 0.14 0.7 2 256 0.27 1.4 4 64 0.12 0.35 4 128 0.16 0.7 4 256 0.29 1.6 expected behavior would expect scatter gather timings lot closer this, factor 5 out. environment"
pytorch,11232,nan,0,sparse autograd create get indices values allow backward via ctor,sparse autograd create get indices values allow backward via ctor nan
pytorch,25320,"reproduce configuration log expand detail first error expand detail second error expand detail third error third one second one, happens target cc peterjc123",0,compilation error windows,"compilation error windows reproduce configuration log expand detail first error expand detail second error expand detail third error third one second one, happens target cc peterjc123"
pytorch,140,"snippet reproduce. problem https github.com pytorch pytorch blob master torch autograd functions tensor.py l27 , https github.com pytorch pytorch blob master torch autograd functions tensor.py l40 https github.com pytorch pytorch blob master torch autograd functions tensor.py l45 , due fact return numbers indexing 1d tensor think desired behaviour broadcasting yet? . maybe check input 1d index range 1 use instead case? autograd return 1d tensor anyway index 1d tensor forward, solution could avoid sync point cuda tensors return number . or, another solution would something like 0d tensors behave almost like numbers? seems numpy does.",0,index setvalue work 1d variables,"index setvalue work 1d variables snippet reproduce. problem https github.com pytorch pytorch blob master torch autograd functions tensor.py l27 , https github.com pytorch pytorch blob master torch autograd functions tensor.py l40 https github.com pytorch pytorch blob master torch autograd functions tensor.py l45 , due fact return numbers indexing 1d tensor think desired behaviour broadcasting yet? . maybe check input 1d index range 1 use instead case? autograd return 1d tensor anyway index 1d tensor forward, solution could avoid sync point cuda tensors return number . or, another solution would something like 0d tensors behave almost like numbers? seems numpy does."
pytorch,23738,reproducer python 3.5 conda environment,0,python 3.5 conda nightlies work,python 3.5 conda nightlies work reproducer python 3.5 conda environment
pytorch,14967,"bug pytorch 1.0 cuda 10.0 onnx 1.2.1 1.3.0. export onnx model core dumps ubuntu gpu instance reproduce steps reproduce behavior spin ubuntu gpu instance like ec2 p2 g3 1. install cuda 10, cudnn 7.4.1, nccl 2.3.7 2. anaconda 2. conda create n pytorch p27 python 2.7 3. conda install pytorch 1.0.0 py2.7 cuda10.0.130 cudnn7.4.1 1 torchvision 0.2.1 cuda100 1.0 c pytorch 4. pip install u onnx 1.2.1 1.3.0 example used replicate https github.com onnx tutorials blob master tutorials pytorchonnxexport.ipynb error illegal instruction core dumped expected behavior exported .onnx file model environment please copy paste output environment collection script https raw.githubusercontent.com pytorch pytorch master torch utils collect env.py pytorch version 1.0.0 debug build cuda used build pytorch 10.0.130 os ubuntu 16.04.5 lts gcc version ubuntu 5.4.0 6ubuntu1 16.04.10 5.4.0 20160609 cmake version version 3.5.1 python version 2.7 cuda available yes cuda runtime version 10.0.130 gpu models configuration gpu 0 tesla m60 gpu 1 tesla m60 gpu 2 tesla m60 gpu 3 tesla m60 nvidia driver version 410.79 cudnn version probably one following usr local cuda 10.0 lib64 libcudnn.so.7.4.1 usr local cuda 10.0 lib64 libcudnn static.a usr local cuda 8.0 lib64 libcudnn.so.6.0.21 usr local cuda 8.0 lib64 libcudnn static.a usr local cuda 9.0 lib64 libcudnn.so.7.3.1 usr local cuda 9.0 lib64 libcudnn static.a usr local cuda 9.2 lib64 libcudnn.so.7.3.1 usr local cuda 9.2 lib64 libcudnn static.a versions relevant libraries pip could collect conda blas 1.0 mkl conda cuda100 1.0 0 pytorch conda mkl 2018.0.3 1 conda mkl fft 1.0.6 py27h7dd41cf 0 conda mkl random 1.0.1 py27h4414c95 1 conda pytorch 1.0.0 py2.7 cuda10.0.130 cudnn7.4.1 1 cuda100 pytorch conda torchvision 0.2.1 py 2 pytorch additional context complete list packages packages environment name version build channel blas 1.0 mkl ca certificates 2018.03.07 0 certifi 2018.10.15 py27 0 cffi 1.11.5 py27he75722e 1 cuda100 1.0 0 pytorch freetype 2.9.1 h8a8886c 1 intel openmp 2019.1 144 jpeg 9b h024ee3a 2 libedit 3.1.20170329 h6b74fdf 2 libffi 3.2.1 hd88cf55 4 libgcc ng 8.2.0 hdf63c60 1 libgfortran ng 7.3.0 hdf63c60 0 libpng 1.6.35 hbc83047 0 libstdcxx ng 8.2.0 hdf63c60 1 libtiff 4.0.9 he85c1e1 2 mkl 2018.0.3 1 mkl fft 1.0.6 py27h7dd41cf 0 mkl random 1.0.1 py27h4414c95 1 ncurses 6.1 he6710b0 1 ninja 1.8.2 py27h6bb024c 1 numpy 1.15.4 py27h1d66e8a 0 numpy base 1.15.4 py27h81de0dd 0 olefile 0.46 py27 0 onnx 1.3.0 openssl 1.1.1a h7b6447c 0 pillow 5.3.0 py27h34e0f95 0 pip 18.1 py27 0 protobuf 3.6.1 pycparser 2.19 py27 0 python 2.7.15 h9bab390 4 pytorch 1.0.0 py2.7 cuda10.0.130 cudnn7.4.1 1 cuda100 pytorch readline 7.0 h7b6447c 5 setuptools 40.6.2 py27 0 six 1.11.0 py27 1 sqlite 3.25.3 h7b6447c 0 tk 8.6.8 hbc83047 0 torchvision 0.2.1 py 2 pytorch typing 3.6.6 typing extensions 3.6.6 wheel 0.32.3 py27 0 xz 5.2.4 h14c3975 4 zlib 1.2.11 h7b6447c 3 opening problem behalf team member surajkota",0,onnx pytorch 1.0 cuda 10.0. onnx export core dumps gpu machine,"onnx pytorch 1.0 cuda 10.0. onnx export core dumps gpu machine bug pytorch 1.0 cuda 10.0 onnx 1.2.1 1.3.0. export onnx model core dumps ubuntu gpu instance reproduce steps reproduce behavior spin ubuntu gpu instance like ec2 p2 g3 1. install cuda 10, cudnn 7.4.1, nccl 2.3.7 2. anaconda 2. conda create n pytorch p27 python 2.7 3. conda install pytorch 1.0.0 py2.7 cuda10.0.130 cudnn7.4.1 1 torchvision 0.2.1 cuda100 1.0 c pytorch 4. pip install u onnx 1.2.1 1.3.0 example used replicate https github.com onnx tutorials blob master tutorials pytorchonnxexport.ipynb error illegal instruction core dumped expected behavior exported .onnx file model environment please copy paste output environment collection script https raw.githubusercontent.com pytorch pytorch master torch utils collect env.py pytorch version 1.0.0 debug build cuda used build pytorch 10.0.130 os ubuntu 16.04.5 lts gcc version ubuntu 5.4.0 6ubuntu1 16.04.10 5.4.0 20160609 cmake version version 3.5.1 python version 2.7 cuda available yes cuda runtime version 10.0.130 gpu models configuration gpu 0 tesla m60 gpu 1 tesla m60 gpu 2 tesla m60 gpu 3 tesla m60 nvidia driver version 410.79 cudnn version probably one following usr local cuda 10.0 lib64 libcudnn.so.7.4.1 usr local cuda 10.0 lib64 libcudnn static.a usr local cuda 8.0 lib64 libcudnn.so.6.0.21 usr local cuda 8.0 lib64 libcudnn static.a usr local cuda 9.0 lib64 libcudnn.so.7.3.1 usr local cuda 9.0 lib64 libcudnn static.a usr local cuda 9.2 lib64 libcudnn.so.7.3.1 usr local cuda 9.2 lib64 libcudnn static.a versions relevant libraries pip could collect conda blas 1.0 mkl conda cuda100 1.0 0 pytorch conda mkl 2018.0.3 1 conda mkl fft 1.0.6 py27h7dd41cf 0 conda mkl random 1.0.1 py27h4414c95 1 conda pytorch 1.0.0 py2.7 cuda10.0.130 cudnn7.4.1 1 cuda100 pytorch conda torchvision 0.2.1 py 2 pytorch additional context complete list packages packages environment name version build channel blas 1.0 mkl ca certificates 2018.03.07 0 certifi 2018.10.15 py27 0 cffi 1.11.5 py27he75722e 1 cuda100 1.0 0 pytorch freetype 2.9.1 h8a8886c 1 intel openmp 2019.1 144 jpeg 9b h024ee3a 2 libedit 3.1.20170329 h6b74fdf 2 libffi 3.2.1 hd88cf55 4 libgcc ng 8.2.0 hdf63c60 1 libgfortran ng 7.3.0 hdf63c60 0 libpng 1.6.35 hbc83047 0 libstdcxx ng 8.2.0 hdf63c60 1 libtiff 4.0.9 he85c1e1 2 mkl 2018.0.3 1 mkl fft 1.0.6 py27h7dd41cf 0 mkl random 1.0.1 py27h4414c95 1 ncurses 6.1 he6710b0 1 ninja 1.8.2 py27h6bb024c 1 numpy 1.15.4 py27h1d66e8a 0 numpy base 1.15.4 py27h81de0dd 0 olefile 0.46 py27 0 onnx 1.3.0 openssl 1.1.1a h7b6447c 0 pillow 5.3.0 py27h34e0f95 0 pip 18.1 py27 0 protobuf 3.6.1 pycparser 2.19 py27 0 python 2.7.15 h9bab390 4 pytorch 1.0.0 py2.7 cuda10.0.130 cudnn7.4.1 1 cuda100 pytorch readline 7.0 h7b6447c 5 setuptools 40.6.2 py27 0 six 1.11.0 py27 1 sqlite 3.25.3 h7b6447c 0 tk 8.6.8 hbc83047 0 torchvision 0.2.1 py 2 pytorch typing 3.6.6 typing extensions 3.6.6 wheel 0.32.3 py27 0 xz 5.2.4 h14c3975 4 zlib 1.2.11 h7b6447c 3 opening problem behalf team member surajkota"
pytorch,8388,"observed running simultaneous dataparallels might result least one models unable progress all. system configuration pytorch 0.4.0 stable release, installed using ec2 p2.8xlarge 8 x k80s cuda 8 python 3.6 separate machine 8 x titan x, following happened tried running multiple parallel models there's ram that's somehow unaccounted for.",0,running simultaneous dataparallels potentially result locked models,"running simultaneous dataparallels potentially result locked models observed running simultaneous dataparallels might result least one models unable progress all. system configuration pytorch 0.4.0 stable release, installed using ec2 p2.8xlarge 8 x k80s cuda 8 python 3.6 separate machine 8 x titan x, following happened tried running multiple parallel models there's ram that's somehow unaccounted for."
pytorch,17315,"bug playing around intermediate gradients . discovered strange behaviour . looks like default disabled inside hooks using extract intermediate gradients . why? avoid kind backward inside backward ? reproduce plain hook works fine. output autograd inside hook fails. output ok, fails . looks like need turn . autograd inside hook seems work fine. output woks fine. addition, outside hooks gradients works usual. autograd outside hooks works expected. output expected behaviour expected work usual inside hooks . sure bug smth. worrying, behave way? environment",0,autograd inside hooks disabled default?,"autograd inside hooks disabled default? bug playing around intermediate gradients . discovered strange behaviour . looks like default disabled inside hooks using extract intermediate gradients . why? avoid kind backward inside backward ? reproduce plain hook works fine. output autograd inside hook fails. output ok, fails . looks like need turn . autograd inside hook seems work fine. output woks fine. addition, outside hooks gradients works usual. autograd outside hooks works expected. output expected behaviour expected work usual inside hooks . sure bug smth. worrying, behave way? environment"
pytorch,371,looks like nn.embedding backwards work batch size 1024. think switches sorting algorithms something runs registers. here's repro running k40 cc wickedfoo,0,nn.embedding cuda many resources requested launch,nn.embedding cuda many resources requested launch looks like nn.embedding backwards work batch size 1024. think switches sorting algorithms something runs registers. here's repro running k40 cc wickedfoo
pytorch,8522,support scatter dispatch. example failure affected tests,0,jit unsupported op descriptor scatter,jit unsupported op descriptor scatter support scatter dispatch. example failure affected tests
pytorch,28653,try post train network dataset accuracy good enough. try qat slow. way quickly run qat nvidia gpu? cc jerryzh168 jianyuh dzhulgakov raghuramank100 jamesr66a,0,qat cuda,qat cuda try post train network dataset accuracy good enough. try qat slow. way quickly run qat nvidia gpu? cc jerryzh168 jianyuh dzhulgakov raghuramank100 jamesr66a
pytorch,23222,"bug applied random.shuffle 1 tensor, num tensor changed. reproduce steps reproduce behavior 1.do follows torch.arange 10 random.shuffle expected behavior expected tensor 1, 8, 5, 3, 9, 0, 2, 6, 4, 7 got tensor 0, 1, 2, 1, 4, 5, 1, 5, 7, 4 sure whether bug. environment python 3.7.0 torch 1.1.0",0,maybe problem using random.shuffle,"maybe problem using random.shuffle bug applied random.shuffle 1 tensor, num tensor changed. reproduce steps reproduce behavior 1.do follows torch.arange 10 random.shuffle expected behavior expected tensor 1, 8, 5, 3, 9, 0, 2, 6, 4, 7 got tensor 0, 1, 2, 1, 4, 5, 1, 5, 7, 4 sure whether bug. environment python 3.7.0 torch 1.1.0"
pytorch,10807,"build caffe2 origin caffe2 ubuntu16.04 anaconda3, cmake summary summary general cmake version 3.6.3 cmake command home hengshan .conda envs caffe2 py2 bin cmake git version v0.8.1 1502 g3c9081d dirty system linux c compiler usr bin c c compiler version 5.4.0 blas eigen cxx flags fvisibility inlines hidden donnx namespace onnx c2 o2 fpic wno narrowing wno invalid partial specialization build type release compile definitions build binary build custom protobuf protobuf compiler protobuf includes protobuf libraries build docs build python python version 2.7.13 python includes home hengshan .conda envs caffe2 py2 include python2.7 build shared libs build test use aten use asan use cuda cuda version 8.0 cudnn version 6.0.21 cuda root directory usr local cuda 8.0 cuda library usr lib x86 64 linux gnu libcuda.so cuda nvrtc library usr local lib libnvrtc.so cuda runtime library usr local cuda 8.0 lib64 libcudart.so cuda include path usr local cuda 8.0 include nvcc executable usr local cuda 8.0 bin nvcc cuda host compiler usr bin cc use eigen blas 1 use ffmpeg use gflags use glog use gloo use leveldb leveldb version 1.18 snappy version 1.1.3 use lite proto use lmdb lmdb version 0.9.17 use metal use mkl use mobile opengl use mpi use nccl use nervana gpu use nnpack use observers use opencv opencv version 3.3.0 use openmp use prof use redis use rocksdb use zmq configuring done generating done error .. lib libcaffe2.so onnx c2 getemptystringalreadyinited abi cxx11 collect2 error ld returned 1 exit status caffe2 cmakefiles logging test.dir build.make 117 recipe target 'bin logging test' failed make 2 bin logging test error 1 cmakefiles makefile2 1346 recipe target 'caffe2 cmakefiles logging test.dir all' failed make 1 caffe2 cmakefiles logging test.dir error 2 .. lib libcaffe2.so onnx c2 getemptystringalreadyinited abi cxx11 collect2 error ld returned 1 exit status",0,caffe2 libcaffe2.soonnx c2 getemptystringalreadyinited abi cxx11 undefined reference,"caffe2 libcaffe2.soonnx c2 getemptystringalreadyinited abi cxx11 undefined reference build caffe2 origin caffe2 ubuntu16.04 anaconda3, cmake summary summary general cmake version 3.6.3 cmake command home hengshan .conda envs caffe2 py2 bin cmake git version v0.8.1 1502 g3c9081d dirty system linux c compiler usr bin c c compiler version 5.4.0 blas eigen cxx flags fvisibility inlines hidden donnx namespace onnx c2 o2 fpic wno narrowing wno invalid partial specialization build type release compile definitions build binary build custom protobuf protobuf compiler protobuf includes protobuf libraries build docs build python python version 2.7.13 python includes home hengshan .conda envs caffe2 py2 include python2.7 build shared libs build test use aten use asan use cuda cuda version 8.0 cudnn version 6.0.21 cuda root directory usr local cuda 8.0 cuda library usr lib x86 64 linux gnu libcuda.so cuda nvrtc library usr local lib libnvrtc.so cuda runtime library usr local cuda 8.0 lib64 libcudart.so cuda include path usr local cuda 8.0 include nvcc executable usr local cuda 8.0 bin nvcc cuda host compiler usr bin cc use eigen blas 1 use ffmpeg use gflags use glog use gloo use leveldb leveldb version 1.18 snappy version 1.1.3 use lite proto use lmdb lmdb version 0.9.17 use metal use mkl use mobile opengl use mpi use nccl use nervana gpu use nnpack use observers use opencv opencv version 3.3.0 use openmp use prof use redis use rocksdb use zmq configuring done generating done error .. lib libcaffe2.so onnx c2 getemptystringalreadyinited abi cxx11 collect2 error ld returned 1 exit status caffe2 cmakefiles logging test.dir build.make 117 recipe target 'bin logging test' failed make 2 bin logging test error 1 cmakefiles makefile2 1346 recipe target 'caffe2 cmakefiles logging test.dir all' failed make 1 caffe2 cmakefiles logging test.dir error 2 .. lib libcaffe2.so onnx c2 getemptystringalreadyinited abi cxx11 collect2 error ld returned 1 exit status"
pytorch,19697,"bug concerns method torch.transpose input, dim0, dim1 tensor according documentation, https pytorch.org docs stable torch.html resulting tensor shares underlying storage input tensor, changing content one would change content other. however, transposed versions original tensor changed original tensor's data changed via set method specifically . reproduce tensor 1., 2. ttr tensor 1. , 2. tensor 5., 2. ttr tensor 5. , 2. transposed versions updated, expected tensor 10., 20. ttr tensor 10. , 20. transposed versions updated, expected tensor 12., 22. ttr tensor 12. , 22. transposed versions updated, expected tensor 1., 2. ttr tensor 12. , 22. expected behavior expect changing content one via set would change content other. , per documentation. environment pytorch version 1.0.1.post2 debug build cuda used build pytorch none os mac osx 10.14.3 gcc version could collect cmake version version 3.13.4 python version 3.7 cuda available cuda runtime version cuda gpu models configuration cuda nvidia driver version cuda cudnn version cuda versions relevant libraries pip3 numpy 1.14.5 pip3 torch 1.0.1.post2 pip3 torchvision 0.2.2.post3 conda could collect additional context",0,torch.transpose shares underlying storage original tensor,"torch.transpose shares underlying storage original tensor bug concerns method torch.transpose input, dim0, dim1 tensor according documentation, https pytorch.org docs stable torch.html resulting tensor shares underlying storage input tensor, changing content one would change content other. however, transposed versions original tensor changed original tensor's data changed via set method specifically . reproduce tensor 1., 2. ttr tensor 1. , 2. tensor 5., 2. ttr tensor 5. , 2. transposed versions updated, expected tensor 10., 20. ttr tensor 10. , 20. transposed versions updated, expected tensor 12., 22. ttr tensor 12. , 22. transposed versions updated, expected tensor 1., 2. ttr tensor 12. , 22. expected behavior expect changing content one via set would change content other. , per documentation. environment pytorch version 1.0.1.post2 debug build cuda used build pytorch none os mac osx 10.14.3 gcc version could collect cmake version version 3.13.4 python version 3.7 cuda available cuda runtime version cuda gpu models configuration cuda nvidia driver version cuda cudnn version cuda versions relevant libraries pip3 numpy 1.14.5 pip3 torch 1.0.1.post2 pip3 torchvision 0.2.2.post3 conda could collect additional context"
pytorch,15557,"questions help made main.exe ,which made main.py pyinstaller. main.exe,then faced main.exe error unrecognized arguments multiprocessing fork parent pid 7056 pipe handle 1188 erroe stopped. change main.py code operate main.exe without error? really appreciate anyone give advice soon possile cuda9, python3.6 ,pytorch windows cuda9, oswindows10 pro 64bit nvidia gtx980",0,"main.exe ,which made main.py pyinstaller , faced error","main.exe ,which made main.py pyinstaller , faced error questions help made main.exe ,which made main.py pyinstaller. main.exe,then faced main.exe error unrecognized arguments multiprocessing fork parent pid 7056 pipe handle 1188 erroe stopped. change main.py code operate main.exe without error? really appreciate anyone give advice soon possile cuda9, python3.6 ,pytorch windows cuda9, oswindows10 pro 64bit nvidia gtx980"
pytorch,28639,"bug calling cudart cudaapisetdevice appears expensive operation. however, system single gpu, unnecessary call all. tested windows. ! image https user images.githubusercontent.com 61218 67546817 99188780 f740 11e9 9566 792c5eab40e4.png reproduce steps reproduce behavior 1. run models benchmarking turned 2. profile application expected behavior set device even called single gpu device device 0 always used anyway. environment pytorch version e.g., 1.0 1.3.0 os e.g., linux windows x64 msvc 2019 installed pytorch , , source prebuilt website build command used compiling source n python version n cuda cudnn version 10.1 7.6 gpu models configuration gtx 2060 relevant information cc vitalyfedyunin ngimel mruberry",0,set cuda device expensive operation,"set cuda device expensive operation bug calling cudart cudaapisetdevice appears expensive operation. however, system single gpu, unnecessary call all. tested windows. ! image https user images.githubusercontent.com 61218 67546817 99188780 f740 11e9 9566 792c5eab40e4.png reproduce steps reproduce behavior 1. run models benchmarking turned 2. profile application expected behavior set device even called single gpu device device 0 always used anyway. environment pytorch version e.g., 1.0 1.3.0 os e.g., linux windows x64 msvc 2019 installed pytorch , , source prebuilt website build command used compiling source n python version n cuda cudnn version 10.1 7.6 gpu models configuration gtx 2060 relevant information cc vitalyfedyunin ngimel mruberry"
pytorch,8594,"issue matches https github.com pytorch pytorch issues 7019 url suggested solution work me. code never returns value import torch import torch.nn nn torch.autograd import variable class net nn.module def init self super net, self . init self.dense nn.linear 256, 512 def forward self, input return self.dense input name ' main ' model net model nn.dataparallel model .cuda x variable torch.rand 128, 256 model x gets stuck forever solution delete nccl shm disable 1 nccl p2p disable 1 etc nccl.conf etc nccl.conf ubuntu 18.04 unset environment variables get nccl version 2.1.15 cuda9.0 rig 2637 2637 0 info net using interface enp0s31f6 192.168.85.32 rig 2637 2637 0 info net socket 1 interfaces found rig 2637 2637 3 info using 256 threads rig 2637 2637 3 info min comp cap 6 rig 2637 2637 3 info nccl single ring threshold 131072 rig 2637 2637 3 info ring 00 0 1 2 3 rig 2637 2637 0 info ring 00 0 0 1 1 via p2p direct pointer rig 2637 2637 1 info ring 00 1 1 2 2 via p2p direct pointer rig 2637 2637 2 info ring 00 2 2 3 3 via p2p direct pointer rig 2637 2637 3 info ring 00 3 3 0 0 via p2p direct pointer rig 2637 2637 0 info launch mode group cgmd c gives file home minimumnz anaconda3 envs tacotron lib python3.6 threading.py , line 1072, wait tstate lock elif lock.acquire block, timeout seems stuck thread. collecting environment information... pytorch version 0.4.0 debug build cuda used build pytorch 9.0.176 os ubuntu 18.04 lts gcc version ubuntu 7.3.0 16ubuntu3 7.3.0 cmake version could collect python version 3.6 cuda available yes cuda runtime version could collect gpu models configuration gpu 0 geforce gtx 1080 ti gpu 1 geforce gtx 1080 ti gpu 2 geforce gtx 1080 ti gpu 3 geforce gtx 1080 ti nvidia driver version 390.67 cudnn version probably one following usr lib x86 64 linux gnu libcudnn.so.7.1.3 usr lib x86 64 linux gnu libcudnn static v7.a versions relevant libraries pip numpy 1.14.5 pip torch 0.4.0 pip torchvision 0.2.1 conda cuda90 1.0 h6433d27 0 pytorch conda pytorch 0.4.0 py36 cuda9.0.176 cudnn7.1.2 1 cuda90 pytorch conda torch 0.4.0 conda torchvision 0.2.1 py36 1 pytorch",0,pytorch 0.4 hangs nn.dataparallel,"pytorch 0.4 hangs nn.dataparallel issue matches https github.com pytorch pytorch issues 7019 url suggested solution work me. code never returns value import torch import torch.nn nn torch.autograd import variable class net nn.module def init self super net, self . init self.dense nn.linear 256, 512 def forward self, input return self.dense input name ' main ' model net model nn.dataparallel model .cuda x variable torch.rand 128, 256 model x gets stuck forever solution delete nccl shm disable 1 nccl p2p disable 1 etc nccl.conf etc nccl.conf ubuntu 18.04 unset environment variables get nccl version 2.1.15 cuda9.0 rig 2637 2637 0 info net using interface enp0s31f6 192.168.85.32 rig 2637 2637 0 info net socket 1 interfaces found rig 2637 2637 3 info using 256 threads rig 2637 2637 3 info min comp cap 6 rig 2637 2637 3 info nccl single ring threshold 131072 rig 2637 2637 3 info ring 00 0 1 2 3 rig 2637 2637 0 info ring 00 0 0 1 1 via p2p direct pointer rig 2637 2637 1 info ring 00 1 1 2 2 via p2p direct pointer rig 2637 2637 2 info ring 00 2 2 3 3 via p2p direct pointer rig 2637 2637 3 info ring 00 3 3 0 0 via p2p direct pointer rig 2637 2637 0 info launch mode group cgmd c gives file home minimumnz anaconda3 envs tacotron lib python3.6 threading.py , line 1072, wait tstate lock elif lock.acquire block, timeout seems stuck thread. collecting environment information... pytorch version 0.4.0 debug build cuda used build pytorch 9.0.176 os ubuntu 18.04 lts gcc version ubuntu 7.3.0 16ubuntu3 7.3.0 cmake version could collect python version 3.6 cuda available yes cuda runtime version could collect gpu models configuration gpu 0 geforce gtx 1080 ti gpu 1 geforce gtx 1080 ti gpu 2 geforce gtx 1080 ti gpu 3 geforce gtx 1080 ti nvidia driver version 390.67 cudnn version probably one following usr lib x86 64 linux gnu libcudnn.so.7.1.3 usr lib x86 64 linux gnu libcudnn static v7.a versions relevant libraries pip numpy 1.14.5 pip torch 0.4.0 pip torchvision 0.2.1 conda cuda90 1.0 h6433d27 0 pytorch conda pytorch 0.4.0 py36 cuda9.0.176 cudnn7.1.2 1 cuda90 pytorch conda torch 0.4.0 conda torchvision 0.2.1 py36 1 pytorch"
pytorch,31419,"bug trying load tensor blob leads error java.lang.unsatisfiedlinkerror implementation found com.facebook.jni.hybriddata org.pytorch.tensor.inithybrid tried java org pytorch tensor inithybrid java org pytorch tensor inithybrid org.pytorch.tensor.inithybrid native method org.pytorch.tensor.inithybrid tensor.java 337 org.pytorch.tensor.fromblob tensor.java 289 recently introduced bug, nightly snapshot last week worked fine. reproduce steps reproduce behavior 1. create android project add pytorch 1.4.0 nighly snapshot support e.g. shown https github.com pytorch pytorch issues 29806 2. add following lines val inputheight 299 val inputwidth 299 val inputtensorbuffer tensor.allocatefloatbuffer 3 inputwidth inputheight val inputtensor tensor.fromblob inputtensorbuffer, longarrayof 1, 3, inputheight.tolong , inputwidth.tolong 3. build run app 4. app crashes error expected behavior loading tensor memory without error worked nightly build last week environment pytorch version e.g., 1.0 1.4.0 nightly snapshot 18 dec 2019 os e.g., linux windows 10 android studio 3.5.3, openjdk jre 1.8.0 202 cuda cudnn version none cpu gpu models configuration none additional context using stable version option, 1.3.0 breaking bug loading inception v3 model https github.com pytorch pytorch issues 29806",0,android inithybrid missing broken pytorch 1.4.0 nightly,"android inithybrid missing broken pytorch 1.4.0 nightly bug trying load tensor blob leads error java.lang.unsatisfiedlinkerror implementation found com.facebook.jni.hybriddata org.pytorch.tensor.inithybrid tried java org pytorch tensor inithybrid java org pytorch tensor inithybrid org.pytorch.tensor.inithybrid native method org.pytorch.tensor.inithybrid tensor.java 337 org.pytorch.tensor.fromblob tensor.java 289 recently introduced bug, nightly snapshot last week worked fine. reproduce steps reproduce behavior 1. create android project add pytorch 1.4.0 nighly snapshot support e.g. shown https github.com pytorch pytorch issues 29806 2. add following lines val inputheight 299 val inputwidth 299 val inputtensorbuffer tensor.allocatefloatbuffer 3 inputwidth inputheight val inputtensor tensor.fromblob inputtensorbuffer, longarrayof 1, 3, inputheight.tolong , inputwidth.tolong 3. build run app 4. app crashes error expected behavior loading tensor memory without error worked nightly build last week environment pytorch version e.g., 1.0 1.4.0 nightly snapshot 18 dec 2019 os e.g., linux windows 10 android studio 3.5.3, openjdk jre 1.8.0 202 cuda cudnn version none cpu gpu models configuration none additional context using stable version option, 1.3.0 breaking bug loading inception v3 model https github.com pytorch pytorch issues 29806"
pytorch,19104,"tried use dataparallel, came across problem arguments located different gpus , found use 0.5.0 version solve problem forum. however ,i trying find correct version long time result.can anyone help ? ps, tried run code pytorch1.0, error came dataloader part randomsampler object attribute 'replacement .and found solution. thank !!",0,0.5.0version pytorch?,"0.5.0version pytorch? tried use dataparallel, came across problem arguments located different gpus , found use 0.5.0 version solve problem forum. however ,i trying find correct version long time result.can anyone help ? ps, tried run code pytorch1.0, error came dataloader part randomsampler object attribute 'replacement .and found solution. thank !!"
pytorch,17531,"bug onnx loop produced conform onnx loop spec second input onnx loop conditional variable , got . reproduce run following script environment details produces following onnx program tensor type see program print . however, second input , required onnx spec https github.com onnx onnx blob master docs operators.md loop . environment pytorch version 1.0.1.post2 debug build cuda used build pytorch none os mac osx 10.14.1 gcc version could collect cmake version version 3.13.4 python version 3.7 cuda available cuda runtime version cuda gpu models configuration cuda nvidia driver version cuda cudnn version cuda versions relevant libraries pip3 numpy 1.16.1 pip3 torch 1.0.1.post2 conda could collect",0,jit graph onnx loop cond variable tensor bool,"jit graph onnx loop cond variable tensor bool bug onnx loop produced conform onnx loop spec second input onnx loop conditional variable , got . reproduce run following script environment details produces following onnx program tensor type see program print . however, second input , required onnx spec https github.com onnx onnx blob master docs operators.md loop . environment pytorch version 1.0.1.post2 debug build cuda used build pytorch none os mac osx 10.14.1 gcc version could collect cmake version version 3.13.4 python version 3.7 cuda available cuda runtime version cuda gpu models configuration cuda nvidia driver version cuda cudnn version cuda versions relevant libraries pip3 numpy 1.16.1 pip3 torch 1.0.1.post2 conda could collect"
pytorch,27680,"bug reproduce steps reproduce behavior 1. 1. 1. expected behavior environment please copy paste output environment collection script https raw.githubusercontent.com pytorch pytorch master torch utils collect env.py fill checklist manually . get script run pytorch version e.g., 1.0 os e.g., linux installed pytorch , , source build command used compiling source python version cuda cudnn version gpu models configuration relevant information additional context",0,1 e software libtorch include torch csrc utils variadic.h 195 error c2951,"1 e software libtorch include torch csrc utils variadic.h 195 error c2951 bug reproduce steps reproduce behavior 1. 1. 1. expected behavior environment please copy paste output environment collection script https raw.githubusercontent.com pytorch pytorch master torch utils collect env.py fill checklist manually . get script run pytorch version e.g., 1.0 os e.g., linux installed pytorch , , source build command used compiling source python version cuda cudnn version gpu models configuration relevant information additional context"
pytorch,9634,"issue description built simple model single bias layer adding set constants inputs follows however, try training adam optimizer, weights bias layer change all. checking shows indeed optimizer, checking running returns vector zeros. soon change forward function, model trains perfectly. issue also occurs operators . model works seems difference gradients calculated means gradients zero used training model work. intended behaviour? system info",0,model gradients x x bias x bias,"model gradients x x bias x bias issue description built simple model single bias layer adding set constants inputs follows however, try training adam optimizer, weights bias layer change all. checking shows indeed optimizer, checking running returns vector zeros. soon change forward function, model trains perfectly. issue also occurs operators . model works seems difference gradients calculated means gradients zero used training model work. intended behaviour? system info"
pytorch,1984,pytorch version '0.1.12 2' run snippet required serialised tensor included attachment. output issue.zip https github.com pytorch pytorch files 1124350 issue.zip,0,different results batch size 1 variables,different results batch size 1 variables pytorch version '0.1.12 2' run snippet required serialised tensor included attachment. output issue.zip https github.com pytorch pytorch files 1124350 issue.zip
pytorch,12562,"based notes ezyang, zdevito gchanan. like declarations.yaml single, externally visible api consumers, internal external, go through. now, designed assumption internal consumers make use it, refactor consumers wish change declarations.yaml. however, quickly gotten state even internal consumers unmaintainable. issue tracks list changes like apply declarations.yaml. meta principle metadata type primarily consists type string e.g., . however, may metadata. x stop hard coding list arguments expands into. instead, list kwargs tensoroptions expands specified used everywhere. de c ify type syntax. means putting input type basically, drop use . type system c python agnostic. determine something reference not, determined parameter question input output parameter, part type string. x add optional type syntax. wanchaol working replace simple list . x eliminate entirely optional supported, arguments complex default values simply specify argument optional, let kernel compute default argument x unify . think two th native different paths, accidentally. x augment type strings alias sets, perhaps exclamation marks. could used remove annotations. handled jit team. x eliminate . gchanan working eliminate . would replaced outputs inner native functions, actual differentiable functions. blocked adding named outputs. x add named outputs, ala . helps clarity defining derivatives multi return functions. eliminate , since eliminated broadcasting type eliminate eliminate , instead inferring name function. put inference function one place make everyone use it. eliminate , one actually needs anymore",0,declarations.yaml cleanup,"declarations.yaml cleanup based notes ezyang, zdevito gchanan. like declarations.yaml single, externally visible api consumers, internal external, go through. now, designed assumption internal consumers make use it, refactor consumers wish change declarations.yaml. however, quickly gotten state even internal consumers unmaintainable. issue tracks list changes like apply declarations.yaml. meta principle metadata type primarily consists type string e.g., . however, may metadata. x stop hard coding list arguments expands into. instead, list kwargs tensoroptions expands specified used everywhere. de c ify type syntax. means putting input type basically, drop use . type system c python agnostic. determine something reference not, determined parameter question input output parameter, part type string. x add optional type syntax. wanchaol working replace simple list . x eliminate entirely optional supported, arguments complex default values simply specify argument optional, let kernel compute default argument x unify . think two th native different paths, accidentally. x augment type strings alias sets, perhaps exclamation marks. could used remove annotations. handled jit team. x eliminate . gchanan working eliminate . would replaced outputs inner native functions, actual differentiable functions. blocked adding named outputs. x add named outputs, ala . helps clarity defining derivatives multi return functions. eliminate , since eliminated broadcasting type eliminate eliminate , instead inferring name function. put inference function one place make everyone use it. eliminate , one actually needs anymore"
pytorch,30400,bug miniz missing .gitmodule miniz third party steps reproduce behavior 1.sync lastest pytorch code 1.run build 1.report 'caffe2 cmakelist.txt missing miniz' seems .gitmodule match third party completely?,0,miniz missing .gitmodule,miniz missing .gitmodule bug miniz missing .gitmodule miniz third party steps reproduce behavior 1.sync lastest pytorch code 1.run build 1.report 'caffe2 cmakelist.txt missing miniz' seems .gitmodule match third party completely?
pytorch,1301,nan,0,test failure running pytorch without cudnn,test failure running pytorch without cudnn nan
pytorch,675,"failed build cuda version pytorch without cudnn latest source. os debian unstable experimental compiler gcc 5, g 5 cuda 8.0.44 package provided debian buildlog http debomatic amd64.debian.net distribution experimental pytorch contrib 0.1.7 1 buildlog idea",0,build nccl failed build libnccl debian unstable,"build nccl failed build libnccl debian unstable failed build cuda version pytorch without cudnn latest source. os debian unstable experimental compiler gcc 5, g 5 cuda 8.0.44 package provided debian buildlog http debomatic amd64.debian.net distribution experimental pytorch contrib 0.1.7 1 buildlog idea"
pytorch,9167,seems like used all. could someone tell directly ? https github.com pytorch pytorch blob 4b2b6907929093634c1e452aa2a5a85e4dd9a793 torch csrc distributed module.cpp l786,0,redundant code distributed,redundant code distributed seems like used all. could someone tell directly ? https github.com pytorch pytorch blob 4b2b6907929093634c1e452aa2a5a85e4dd9a793 torch csrc distributed module.cpp l786
pytorch,7072,"issue description run test case code system 2 gpus get stack trace traceback recent call last file testnet.py , line 51, r net v runtimeerror arguments located different gpus home tester pytorch aten src thc generic thctensormathpairwise.cu 250 code example",0,jit fails gpu ! 0,"jit fails gpu ! 0 issue description run test case code system 2 gpus get stack trace traceback recent call last file testnet.py , line 51, r net v runtimeerror arguments located different gpus home tester pytorch aten src thc generic thctensormathpairwise.cu 250 code example"
pytorch,30331,"branch v1.4.0 cut. need particular patches onto branch, please comment send pr v1.4.0 branch instead master . current prs open v1.4.0 branch https github.com pytorch pytorch pulls?utf8 e2 9c 93 q 3apr 3aopen base 3av1.4.0",0,v1.4.0 release tracker,"v1.4.0 release tracker branch v1.4.0 cut. need particular patches onto branch, please comment send pr v1.4.0 branch instead master . current prs open v1.4.0 branch https github.com pytorch pytorch pulls?utf8 e2 9c 93 q 3apr 3aopen base 3av1.4.0"
pytorch,24583,porting th operators essential code simplicity performance reasons. porting guides q available umbrella issue 24507 feel free add vitalyfedyunin reviewer get prioritized review.,0,migrate leaky relu leaky relu th aten cuda,migrate leaky relu leaky relu th aten cuda porting th operators essential code simplicity performance reasons. porting guides q available umbrella issue 24507 feel free add vitalyfedyunin reviewer get prioritized review.
pytorch,30321,os x builds failing due https app.circleci.com jobs github pytorch pytorch 3698524 circleci ticket https support.circleci.com hc en us requests 63093,0,dyld library loaded usr local opt openssl lib libssl.1.0.0.dylib referenced usr local bin sccache,dyld library loaded usr local opt openssl lib libssl.1.0.0.dylib referenced usr local bin sccache os x builds failing due https app.circleci.com jobs github pytorch pytorch 3698524 circleci ticket https support.circleci.com hc en us requests 63093
pytorch,18682,"questions help trying implement paper https arxiv.org pdf 1803.09050.pdf noisy label task. found training time extremely time consuming, troubled us long time. algorithm tryinng implement following ! image https user images.githubusercontent.com 30048368 55319105 c94bc080 54a6 11e9 923f 81b819c96154.png code following find time backward meta val stage takes 99 meta val training time. moreover, meta val training time takes 93 training time. could anyone give suggestions resolve issue? thank much!",0,time consuming gradient computation backward realizing meta network,"time consuming gradient computation backward realizing meta network questions help trying implement paper https arxiv.org pdf 1803.09050.pdf noisy label task. found training time extremely time consuming, troubled us long time. algorithm tryinng implement following ! image https user images.githubusercontent.com 30048368 55319105 c94bc080 54a6 11e9 923f 81b819c96154.png code following find time backward meta val stage takes 99 meta val training time. moreover, meta val training time takes 93 training time. could anyone give suggestions resolve issue? thank much!"
pytorch,3999,"spent lot time save file torch.save . file size 3.5g, costs 2hr 49m totally. make sense?",0,save big file slow,"save big file slow spent lot time save file torch.save . file size 3.5g, costs 2hr 49m totally. make sense?"
pytorch,3610,"running cyclegan https github.com junyanz pytorch cyclegan pix2pix master takes huge amount memory 11g default options . takes 3.5g 0.3 0.2 branches. memory usage increases run iteration. seems tensors freed properly. remember running pre aten merge version without issue. repro command way, happen model repo, pix2pix",0,high gpu memory usage master,"high gpu memory usage master running cyclegan https github.com junyanz pytorch cyclegan pix2pix master takes huge amount memory 11g default options . takes 3.5g 0.3 0.2 branches. memory usage increases run iteration. seems tensors freed properly. remember running pre aten merge version without issue. repro command way, happen model repo, pix2pix"
pytorch,27123,"hello, implemented dnc codes pytorch version 1.2. got runtime errors trained dnc model. attach file contains error messages, long write here. solve problem.",0,runtimeerror cuda error device side assert triggered,"runtimeerror cuda error device side assert triggered hello, implemented dnc codes pytorch version 1.2. got runtime errors trained dnc model. attach file contains error messages, long write here. solve problem."
pytorch,22171,"similar thing compile submodule none 14533 , could skip compiling non taken branch checks since result constant support use cases like cc suo",0,jit compile relevant branch isinstance,"jit compile relevant branch isinstance similar thing compile submodule none 14533 , could skip compiling non taken branch checks since result constant support use cases like cc suo"
pytorch,15287,"feature see https pytorch.org docs stable torch.html?highlight lerp torch.lerp function available exist pytorch 1.0. motivation feature request motivated current research requirements way generative models developed gans specifically i.e. manipulation output samples made latent input space. paper dcgan https arxiv.org abs 1511.06434 performed latent interpolation linearly https pytorch.org docs stable torch.html?highlight lerp torch.lerp would suffice sampling generative networks https arxiv.org abs 1609.04468 paper suggests use slerp function https en.wikipedia.org wiki slerp interpolation. moreover following recent works go slerp too, instance recently introduced gan 2.0 https arxiv.org abs 1812.04948 , etc. pitch please make slerp https en.wikipedia.org wiki slerp function available sample generative networks effectively. alternatives alternative please provide pytorch based code snippet compute slerp function https en.wikipedia.org wiki slerp effectively easily. additional context wikipedia's article showing example c python numpy code snippet https en.wikipedia.org wiki slerp source code slerp https en.wikipedia.org wiki slerp .",0,please make slerp function available generative model sampling,"please make slerp function available generative model sampling feature see https pytorch.org docs stable torch.html?highlight lerp torch.lerp function available exist pytorch 1.0. motivation feature request motivated current research requirements way generative models developed gans specifically i.e. manipulation output samples made latent input space. paper dcgan https arxiv.org abs 1511.06434 performed latent interpolation linearly https pytorch.org docs stable torch.html?highlight lerp torch.lerp would suffice sampling generative networks https arxiv.org abs 1609.04468 paper suggests use slerp function https en.wikipedia.org wiki slerp interpolation. moreover following recent works go slerp too, instance recently introduced gan 2.0 https arxiv.org abs 1812.04948 , etc. pitch please make slerp https en.wikipedia.org wiki slerp function available sample generative networks effectively. alternatives alternative please provide pytorch based code snippet compute slerp function https en.wikipedia.org wiki slerp effectively easily. additional context wikipedia's article showing example c python numpy code snippet https en.wikipedia.org wiki slerp source code slerp https en.wikipedia.org wiki slerp ."
pytorch,6194,"couple packages import project. obviously, one . noticed import one packages things fine. however, import first get segmentation fault. picky importing module first reason, need import torch first work made things bit complicated me. wonder, python issue could issue maybe package bpy issue? anyone know figure causing segmentation fault?",0,import order matters segmentation fault torch imported first,"import order matters segmentation fault torch imported first couple packages import project. obviously, one . noticed import one packages things fine. however, import first get segmentation fault. picky importing module first reason, need import torch first work made things bit complicated me. wonder, python issue could issue maybe package bpy issue? anyone know figure causing segmentation fault?"
pytorch,3914,"pytorch master 0.4 709fcfd, variables cannot indexed. used work pytorch 0.2 f964105",0,broken bytetensor indexing variables still works tensors,"broken bytetensor indexing variables still works tensors pytorch master 0.4 709fcfd, variables cannot indexed. used work pytorch 0.2 f964105"
pytorch,17971,"bug ! 2019 03 13 20 11 29 https user images.githubusercontent.com 12708080 54278734 2d791400 45ce 11e9 8926 64ffb9c5c2ed.png ! 2019 03 13 20 12 06 https user images.githubusercontent.com 12708080 54278743 34078b80 45ce 11e9 90ef f2291ef9b95e.png reproduce steps reproduce behavior implementing network like change https github.com drsleep light weight refinenet https github.com drsleep multi task refinenet ! 2019 03 13 20 39 56 https user images.githubusercontent.com 12708080 54279505 310d9a80 45d0 11e9 803a 0c7b5eeffb01.png ! 2019 03 13 20 39 35 https user images.githubusercontent.com 12708080 54279506 310d9a80 45d0 11e9 8429 bb7dfc595c70.png ! 2019 03 13 20 38 45 https user images.githubusercontent.com 12708080 54279508 31a63100 45d0 11e9 9463 71bd9e3ea21b.png ! 2019 03 13 20 38 34 https user images.githubusercontent.com 12708080 54279509 31a63100 45d0 11e9 87ee bdd67041d22d.png replace datasets.py class pad object pad image mask desired size args size int minimum length width img val array image padding value msk val int mask padding value def init self, size, img val, msk val, dpt val none self.size size self.img val img val self.msk val msk val dpt val none self.dpt val msk val else self.dpt val dpt val def call self, sample image, mask, depth sample 'image' , sample 'mask' , sample 'depth' h, w image.shape 2 h pad int np.clip self.size h 1 2, 0, 1e6 w pad int np.clip self.size w 1 2, 0, 1e6 pad h pad, h pad , w pad, w pad image np.stack np.pad image , ,c , pad, mode 'constant', constant values self.img val c c range 3 , axis 2 mask np.pad mask, pad, mode 'constant', constant values self.msk val depth np.pad depth, pad, mode 'constant', constant values self.dpt val return 'image' image, 'mask' mask, 'depth' depth class randomcrop object crop randomly image sample. args output size tuple int desired output size. int, square crop made. def init self, crop size assert isinstance crop size, int self.crop size crop size self.crop size 2 ! 0 self.crop size 1 def call self, sample image, mask, depth sample 'image' , sample 'mask' , sample 'depth' h, w image.shape 2 new h min h, self.crop size new w min w, self.crop size top np.random.randint 0, h new h 1 left np.random.randint 0, w new w 1 image image top top new h, left left new w mask mask top top new h, left left new w depth depth top top new h, left left new w return 'image' image, 'mask' mask, 'depth' depth class resizeshorterscale object resize shorter side given value randomly scale. def init self, shorter side, low scale, high scale assert isinstance shorter side, int self.shorter side shorter side self.low scale low scale self.high scale high scale def call self, sample image, mask, depth sample 'image' , sample 'mask' , sample 'depth' min side min image.shape 2 scale np.random.uniform self.low scale, self.high scale min side scale self.shorter side scale self.shorter side 1. min side image cv2.resize image, none, fx scale, fy scale, interpolation cv2.inter cubic mask cv2.resize mask, none, fx scale, fy scale, interpolation cv2.inter nearest depth cv2.resize depth, none, fx scale, fy scale, interpolation cv2.inter nearest return 'image' image, 'mask' mask, 'depth' depth class randommirror object randomly flip image mask def init self pass def call self, sample image, mask, depth sample 'image' , sample 'mask' , sample 'depth' mirror np.random.randint 2 mirror image cv2.flip image, 1 mask cv2.flip mask, 1 depth cv2.flip depth, 1 return 'image' image, 'mask' mask, 'depth' depth class normalise object normalise tensor image mean standard deviation. given mean r, g, b std r, g, b , normalise channel torch. tensor, i.e. channel channel mean std args mean sequence sequence means r, g, b channels respecitvely. std sequence sequence standard deviations r, g, b channels respecitvely. def init self, scale, mean, std self.scale scale self.mean mean self.std std def call self, sample image sample 'image' return 'image' self.scale image self.mean self.std, 'mask' sample 'mask' , 'depth' sample 'depth' class totensor object convert ndarrays sample tensors. def call self, sample image, mask, depth sample 'image' , sample 'mask' , sample 'depth' swap color axis numpy image h x w x c torch image c x h x w image image.transpose 2, 0, 1 return 'image' torch.from numpy image , 'mask' torch.from numpy mask , 'depth' torch.from numpy depth class nyudataset dataset nyuv2 40 def init self, data file, data dir, transform trn none, transform val none args data file string path data file annotations. data dir string directory images. transform trn, val callable, optional optional transform applied sample. open data file, 'rb' f datalist f.readlines self.datalist i,l,m i,l,m map lambda x x.decode 'utf 8' .strip ' n' .split ' t' , datalist self.root dir data dir self.transform trn transform trn self.transform val transform val self.stage 'train' def set stage self, stage self.stage stage def len self return len self.datalist def getitem self, idx img name os.path.join self.root dir, self.datalist idx 0 msk name os.path.join self.root dir, self.datalist idx 1 img name self.datalist idx 0 msk name self.datalist idx 1 dpt name self.datalist idx 2 def read image x img arr np.array image.open x len img arr.shape 2 grayscale img arr np.tile img arr, 3, 1, 1 .transpose 1, 2, 0 return img arr image read image img name mask np.array image.open msk name depth np.array image.open dpt name img name ! msk name assert len mask.shape 2, 'masks must encoded without colourmap' sample 'image' image, 'mask' mask, 'depth' depth self.stage 'train' self.transform trn sample self.transform trn sample elif self.stage 'val' self.transform val sample self.transform val sample return sample expected behavior environment conda install yes file requirements.txt file may used create environment using conda create name file platform linux 64 atomicwrites 1.2.1 py36 0 attrs 18.2.0 py36h28b3542 0 backcall 0.1.0 py36 0 blas 1.0 mkl ca certificates 2018.03.07 0 certifi 2018.10.15 py36 0 cffi 1.11.5 py36he75722e 1 cloudpickle 0.6.1 py36 0 cudatoolkit 9.0 h13b8566 0 cudnn 7.1.2 cuda9.0 0 cycler 0.10.0 py36h93f1223 0 cython 0.29 py36he6710b0 0 cytoolz 0.9.0.1 py36h14c3975 1 dask core 0.20.0 py36 0 dbus 1.13.2 h714fa37 1 decorator 4.3.0 py36 0 expat 2.2.6 he6710b0 0 fontconfig 2.13.0 h9420a91 0 freetype 2.9.1 h8a8886c 1 glib 2.56.2 hd408876 0 gst plugins base 1.14.0 hbbd80ab 1 gstreamer 1.14.0 hb453b48 1 icu 58.2 h9c2bf20 1 imageio 2.4.1 py36 0 intel openmp 2019.0 118 ipython 7.1.1 py36h39e3cac 0 ipython genutils 0.2.0 py36hb52b0d5 0 jedi 0.13.1 py36 0 jpeg 9b h024ee3a 2 kiwisolver 1.0.1 py36hf484d3e 0 libedit 3.1.20170329 h6b74fdf 2 libffi 3.2.1 hd88cf55 4 libgcc ng 8.2.0 hdf63c60 1 libgfortran ng 7.3.0 hdf63c60 0 libpng 1.6.35 hbc83047 0 libstdcxx ng 8.2.0 hdf63c60 1 libtiff 4.0.9 he85c1e1 2 libuuid 1.0.3 h1bed415 2 libxcb 1.13 h1bed415 1 libxml2 2.9.8 h26e45fe 1 markdown 2.6.8 py36 0 matplotlib 3.0.1 py36h5429711 0 mkl 2018.0.3 1 mkl fft 1.0.6 py36h7dd41cf 0 mkl random 1.0.1 py36h4414c95 1 itertools 4.3.0 py36 0 nccl 1.3.5 cuda9.0 0 ncurses 6.1 hf484d3e 0 networkx 2.2 py36 1 ninja 1.8.2 py36h6bb024c 1 numpy 1.15.3 py36h1d66e8a 0 numpy base 1.15.3 py36h81de0dd 0 olefile 0.46 py36 0 openssl 1.0.2p h14c3975 0 parso 0.3.1 py36 0 pcre 8.42 h439df22 0 pexpect 4.6.0 py36 0 pickleshare 0.7.5 py36 0 pillow 5.3.0 py36h34e0f95 0 pip 10.0.1 py36 0 pluggy 0.8.0 py36 0 prompt toolkit 2.0.7 py36 0 ptyprocess 0.6.0 py36 0 py 1.7.0 py36 0 pycparser 2.19 py36 0 pydot 1.2.4 py36 0 pygments 2.2.0 py36h0d3125c 0 pyparsing 2.2.2 py36 0 pyqt 5.9.2 py36h05f1152 2 pytest 3.9.3 py36 0 pytest runner 4.2 py36 0 python 3.6.6 h6e4f718 2 python dateutil 2.7.5 py36 0 pytorch 0.4.1 py36 py35 py27 9.0.176 7.1.2 2 pytz 2018.7 py36 0 pywavelets 1.0.1 py36hdd07704 0 qt 5.9.6 h8703b6f 2 readline 7.0 h7b6447c 5 scikit image 0.14.1 py36he6710b0 0 scipy 1.1.0 py36hfa4b5c9 1 setuptools 40.4.3 py36 0 sip 4.19.8 py36hf484d3e 0 six 1.11.0 py36 1 sqlite 3.25.2 h7b6447c 0 tk 8.6.8 hbc83047 0 toolz 0.9.0 py36 0 torchvision 0.2.1 py 2 tornado 5.1.1 py36h7b6447c 0 traitlets 4.3.2 py36 0 wcwidth 0.1.7 py36 0 wheel 0.32.2 py36 0 xz 5.2.4 h14c3975 4 zlib 1.2.11 ha838bed 2 condapip , source conda build command used compiling source conda install yes file requirements.txt python version 3.6 cuda cudnn version 9.0 7.1.2 gpu models configuration gtx1070ti relevant information",0,cudnn error cudnn status execution failed,"cudnn error cudnn status execution failed bug ! 2019 03 13 20 11 29 https user images.githubusercontent.com 12708080 54278734 2d791400 45ce 11e9 8926 64ffb9c5c2ed.png ! 2019 03 13 20 12 06 https user images.githubusercontent.com 12708080 54278743 34078b80 45ce 11e9 90ef f2291ef9b95e.png reproduce steps reproduce behavior implementing network like change https github.com drsleep light weight refinenet https github.com drsleep multi task refinenet ! 2019 03 13 20 39 56 https user images.githubusercontent.com 12708080 54279505 310d9a80 45d0 11e9 803a 0c7b5eeffb01.png ! 2019 03 13 20 39 35 https user images.githubusercontent.com 12708080 54279506 310d9a80 45d0 11e9 8429 bb7dfc595c70.png ! 2019 03 13 20 38 45 https user images.githubusercontent.com 12708080 54279508 31a63100 45d0 11e9 9463 71bd9e3ea21b.png ! 2019 03 13 20 38 34 https user images.githubusercontent.com 12708080 54279509 31a63100 45d0 11e9 87ee bdd67041d22d.png replace datasets.py class pad object pad image mask desired size args size int minimum length width img val array image padding value msk val int mask padding value def init self, size, img val, msk val, dpt val none self.size size self.img val img val self.msk val msk val dpt val none self.dpt val msk val else self.dpt val dpt val def call self, sample image, mask, depth sample 'image' , sample 'mask' , sample 'depth' h, w image.shape 2 h pad int np.clip self.size h 1 2, 0, 1e6 w pad int np.clip self.size w 1 2, 0, 1e6 pad h pad, h pad , w pad, w pad image np.stack np.pad image , ,c , pad, mode 'constant', constant values self.img val c c range 3 , axis 2 mask np.pad mask, pad, mode 'constant', constant values self.msk val depth np.pad depth, pad, mode 'constant', constant values self.dpt val return 'image' image, 'mask' mask, 'depth' depth class randomcrop object crop randomly image sample. args output size tuple int desired output size. int, square crop made. def init self, crop size assert isinstance crop size, int self.crop size crop size self.crop size 2 ! 0 self.crop size 1 def call self, sample image, mask, depth sample 'image' , sample 'mask' , sample 'depth' h, w image.shape 2 new h min h, self.crop size new w min w, self.crop size top np.random.randint 0, h new h 1 left np.random.randint 0, w new w 1 image image top top new h, left left new w mask mask top top new h, left left new w depth depth top top new h, left left new w return 'image' image, 'mask' mask, 'depth' depth class resizeshorterscale object resize shorter side given value randomly scale. def init self, shorter side, low scale, high scale assert isinstance shorter side, int self.shorter side shorter side self.low scale low scale self.high scale high scale def call self, sample image, mask, depth sample 'image' , sample 'mask' , sample 'depth' min side min image.shape 2 scale np.random.uniform self.low scale, self.high scale min side scale self.shorter side scale self.shorter side 1. min side image cv2.resize image, none, fx scale, fy scale, interpolation cv2.inter cubic mask cv2.resize mask, none, fx scale, fy scale, interpolation cv2.inter nearest depth cv2.resize depth, none, fx scale, fy scale, interpolation cv2.inter nearest return 'image' image, 'mask' mask, 'depth' depth class randommirror object randomly flip image mask def init self pass def call self, sample image, mask, depth sample 'image' , sample 'mask' , sample 'depth' mirror np.random.randint 2 mirror image cv2.flip image, 1 mask cv2.flip mask, 1 depth cv2.flip depth, 1 return 'image' image, 'mask' mask, 'depth' depth class normalise object normalise tensor image mean standard deviation. given mean r, g, b std r, g, b , normalise channel torch. tensor, i.e. channel channel mean std args mean sequence sequence means r, g, b channels respecitvely. std sequence sequence standard deviations r, g, b channels respecitvely. def init self, scale, mean, std self.scale scale self.mean mean self.std std def call self, sample image sample 'image' return 'image' self.scale image self.mean self.std, 'mask' sample 'mask' , 'depth' sample 'depth' class totensor object convert ndarrays sample tensors. def call self, sample image, mask, depth sample 'image' , sample 'mask' , sample 'depth' swap color axis numpy image h x w x c torch image c x h x w image image.transpose 2, 0, 1 return 'image' torch.from numpy image , 'mask' torch.from numpy mask , 'depth' torch.from numpy depth class nyudataset dataset nyuv2 40 def init self, data file, data dir, transform trn none, transform val none args data file string path data file annotations. data dir string directory images. transform trn, val callable, optional optional transform applied sample. open data file, 'rb' f datalist f.readlines self.datalist i,l,m i,l,m map lambda x x.decode 'utf 8' .strip ' n' .split ' t' , datalist self.root dir data dir self.transform trn transform trn self.transform val transform val self.stage 'train' def set stage self, stage self.stage stage def len self return len self.datalist def getitem self, idx img name os.path.join self.root dir, self.datalist idx 0 msk name os.path.join self.root dir, self.datalist idx 1 img name self.datalist idx 0 msk name self.datalist idx 1 dpt name self.datalist idx 2 def read image x img arr np.array image.open x len img arr.shape 2 grayscale img arr np.tile img arr, 3, 1, 1 .transpose 1, 2, 0 return img arr image read image img name mask np.array image.open msk name depth np.array image.open dpt name img name ! msk name assert len mask.shape 2, 'masks must encoded without colourmap' sample 'image' image, 'mask' mask, 'depth' depth self.stage 'train' self.transform trn sample self.transform trn sample elif self.stage 'val' self.transform val sample self.transform val sample return sample expected behavior environment conda install yes file requirements.txt file may used create environment using conda create name file platform linux 64 atomicwrites 1.2.1 py36 0 attrs 18.2.0 py36h28b3542 0 backcall 0.1.0 py36 0 blas 1.0 mkl ca certificates 2018.03.07 0 certifi 2018.10.15 py36 0 cffi 1.11.5 py36he75722e 1 cloudpickle 0.6.1 py36 0 cudatoolkit 9.0 h13b8566 0 cudnn 7.1.2 cuda9.0 0 cycler 0.10.0 py36h93f1223 0 cython 0.29 py36he6710b0 0 cytoolz 0.9.0.1 py36h14c3975 1 dask core 0.20.0 py36 0 dbus 1.13.2 h714fa37 1 decorator 4.3.0 py36 0 expat 2.2.6 he6710b0 0 fontconfig 2.13.0 h9420a91 0 freetype 2.9.1 h8a8886c 1 glib 2.56.2 hd408876 0 gst plugins base 1.14.0 hbbd80ab 1 gstreamer 1.14.0 hb453b48 1 icu 58.2 h9c2bf20 1 imageio 2.4.1 py36 0 intel openmp 2019.0 118 ipython 7.1.1 py36h39e3cac 0 ipython genutils 0.2.0 py36hb52b0d5 0 jedi 0.13.1 py36 0 jpeg 9b h024ee3a 2 kiwisolver 1.0.1 py36hf484d3e 0 libedit 3.1.20170329 h6b74fdf 2 libffi 3.2.1 hd88cf55 4 libgcc ng 8.2.0 hdf63c60 1 libgfortran ng 7.3.0 hdf63c60 0 libpng 1.6.35 hbc83047 0 libstdcxx ng 8.2.0 hdf63c60 1 libtiff 4.0.9 he85c1e1 2 libuuid 1.0.3 h1bed415 2 libxcb 1.13 h1bed415 1 libxml2 2.9.8 h26e45fe 1 markdown 2.6.8 py36 0 matplotlib 3.0.1 py36h5429711 0 mkl 2018.0.3 1 mkl fft 1.0.6 py36h7dd41cf 0 mkl random 1.0.1 py36h4414c95 1 itertools 4.3.0 py36 0 nccl 1.3.5 cuda9.0 0 ncurses 6.1 hf484d3e 0 networkx 2.2 py36 1 ninja 1.8.2 py36h6bb024c 1 numpy 1.15.3 py36h1d66e8a 0 numpy base 1.15.3 py36h81de0dd 0 olefile 0.46 py36 0 openssl 1.0.2p h14c3975 0 parso 0.3.1 py36 0 pcre 8.42 h439df22 0 pexpect 4.6.0 py36 0 pickleshare 0.7.5 py36 0 pillow 5.3.0 py36h34e0f95 0 pip 10.0.1 py36 0 pluggy 0.8.0 py36 0 prompt toolkit 2.0.7 py36 0 ptyprocess 0.6.0 py36 0 py 1.7.0 py36 0 pycparser 2.19 py36 0 pydot 1.2.4 py36 0 pygments 2.2.0 py36h0d3125c 0 pyparsing 2.2.2 py36 0 pyqt 5.9.2 py36h05f1152 2 pytest 3.9.3 py36 0 pytest runner 4.2 py36 0 python 3.6.6 h6e4f718 2 python dateutil 2.7.5 py36 0 pytorch 0.4.1 py36 py35 py27 9.0.176 7.1.2 2 pytz 2018.7 py36 0 pywavelets 1.0.1 py36hdd07704 0 qt 5.9.6 h8703b6f 2 readline 7.0 h7b6447c 5 scikit image 0.14.1 py36he6710b0 0 scipy 1.1.0 py36hfa4b5c9 1 setuptools 40.4.3 py36 0 sip 4.19.8 py36hf484d3e 0 six 1.11.0 py36 1 sqlite 3.25.2 h7b6447c 0 tk 8.6.8 hbc83047 0 toolz 0.9.0 py36 0 torchvision 0.2.1 py 2 tornado 5.1.1 py36h7b6447c 0 traitlets 4.3.2 py36 0 wcwidth 0.1.7 py36 0 wheel 0.32.2 py36 0 xz 5.2.4 h14c3975 4 zlib 1.2.11 ha838bed 2 condapip , source conda build command used compiling source conda install yes file requirements.txt python version 3.6 cuda cudnn version 9.0 7.1.2 gpu models configuration gtx1070ti relevant information"
pytorch,15202,tutorial https pytorch.org tutorials beginner blitz autograd tutorial.html gradients ! image https user images.githubusercontent.com 1032377 49982017 a95cb500 ff28 11e8 9465 7b0da9ce39c4.png,0,font size equations tutorial small,font size equations tutorial small tutorial https pytorch.org tutorials beginner blitz autograd tutorial.html gradients ! image https user images.githubusercontent.com 1032377 49982017 a95cb500 ff28 11e8 9465 7b0da9ce39c4.png
pytorch,15066,there's still page https pytorch.org docs master ffi.html?highlight ffi,0,docs delete ffi documentation,docs delete ffi documentation there's still page https pytorch.org docs master ffi.html?highlight ffi
pytorch,8987,"hi, met following issue build pytorch source. build caffe2 successfully using , build pytorch using . anyone know solve it? thanks! issue description failed run 'bash tools build pytorch libs.sh use nnpack caffe2 nanopb libshm gloo thd' code example system info pytorch caffe2 pytorch installed pytorch conda, pip, source source build command used compiling source os ubuntu 16.04 pytorch version source code cloned master branch python version 3.5.2 cuda cudnn version n gpu models configuration n gcc version compiling source 5.4 cmake version 3.11.0 versions relevant libraries wall wno unused wno attributes wno unused result wno psabi ffp contract fno math errno fno trapping mathonnx getemptystringalreadyinited abi cxx11 ' home xxx project svn store pytorch pytorch git build lib libcaffe2.so undefined reference onnx getemptystringalreadyinited abi cxx11 ' home xxx project svn store pytorch pytorch git build lib libcaffe2.so undefined reference",0,undefined reference onnx getemptystringalreadyinited abi cxx11 ',"undefined reference onnx getemptystringalreadyinited abi cxx11 ' hi, met following issue build pytorch source. build caffe2 successfully using , build pytorch using . anyone know solve it? thanks! issue description failed run 'bash tools build pytorch libs.sh use nnpack caffe2 nanopb libshm gloo thd' code example system info pytorch caffe2 pytorch installed pytorch conda, pip, source source build command used compiling source os ubuntu 16.04 pytorch version source code cloned master branch python version 3.5.2 cuda cudnn version n gpu models configuration n gcc version compiling source 5.4 cmake version 3.11.0 versions relevant libraries wall wno unused wno attributes wno unused result wno psabi ffp contract fno math errno fno trapping mathonnx getemptystringalreadyinited abi cxx11 ' home xxx project svn store pytorch pytorch git build lib libcaffe2.so undefined reference onnx getemptystringalreadyinited abi cxx11 ' home xxx project svn store pytorch pytorch git build lib libcaffe2.so undefined reference"
pytorch,828,think index copy statement work intended. guess statement something like,0,backward autograd index function broken indexing longtensor,backward autograd index function broken indexing longtensor think index copy statement work intended. guess statement something like
pytorch,4858,"running ubuntu 16.04.3 aws p3.2xlarge, nvidia libcuda1 384 package, version 384.111 0ubuntu0.16.04.1, pytorch version 0.3.0.post4 cuda90, installed via conda. circumstances, cuda multinomial sampler sample event zero probability. script reproduces this. tried reduce distribution minimal case. output script. attached rng state https github.com pytorch pytorch files 1665704 failed state.pt.gz prior failure. reproduction state demonstrated script. bests regards, alex save path failed state.pt",0,cuda multinomial replacement select zero probability events,"cuda multinomial replacement select zero probability events running ubuntu 16.04.3 aws p3.2xlarge, nvidia libcuda1 384 package, version 384.111 0ubuntu0.16.04.1, pytorch version 0.3.0.post4 cuda90, installed via conda. circumstances, cuda multinomial sampler sample event zero probability. script reproduces this. tried reduce distribution minimal case. output script. attached rng state https github.com pytorch pytorch files 1665704 failed state.pt.gz prior failure. reproduction state demonstrated script. bests regards, alex save path failed state.pt"
pytorch,27820,"bug trying import creates error pycharm. look following picture ! https user images.githubusercontent.com 44257865 66712058 9c6b5500 edc9 11e9 9492 64c416939e7b.png says , , see ! https user images.githubusercontent.com 44257865 66712088 f9670b00 edc9 11e9 9ad4 417bc791c207.png obviously super class exists. believe added. reproduce steps reproduce behavior 1. open pycharm 2. write 3. error expected behavior unresolved reference hinted. environment additional context cc ssnl ezyang",0,iterabledataset added dataset.pyi,"iterabledataset added dataset.pyi bug trying import creates error pycharm. look following picture ! https user images.githubusercontent.com 44257865 66712058 9c6b5500 edc9 11e9 9492 64c416939e7b.png says , , see ! https user images.githubusercontent.com 44257865 66712088 f9670b00 edc9 11e9 9ad4 417bc791c207.png obviously super class exists. believe added. reproduce steps reproduce behavior 1. open pycharm 2. write 3. error expected behavior unresolved reference hinted. environment additional context cc ssnl ezyang"
pytorch,2733,"hi, would like use distributed module train convolution net cpu cluster. investigating code, function torch.cuda.device count called several places, used populate device ids list. since gpu devices cluster, method device count always return 0 subsequent attempt access device ids 0 result index exception. taking naive path changing device count always returns number nodes intend use get different error input.is cuda input inputs raise typeerror 'broadcast function implemented cpu tensors' would like ask whether plans implement distributed module train networks multi cpu cluster. many thanks",0,pytorch multi cpu cluster,"pytorch multi cpu cluster hi, would like use distributed module train convolution net cpu cluster. investigating code, function torch.cuda.device count called several places, used populate device ids list. since gpu devices cluster, method device count always return 0 subsequent attempt access device ids 0 result index exception. taking naive path changing device count always returns number nodes intend use get different error input.is cuda input inputs raise typeerror 'broadcast function implemented cpu tensors' would like ask whether plans implement distributed module train networks multi cpu cluster. many thanks"
pytorch,16428,documentation instead ? https pytorch.org docs stable nn.html?highlight tripletmarginloss torch.nn.tripletmarginloss,0,'swap' type torch.nn.tripletmarginloss,'swap' type torch.nn.tripletmarginloss documentation instead ? https pytorch.org docs stable nn.html?highlight tripletmarginloss torch.nn.tripletmarginloss
pytorch,6512,code prints using tensor overload. change dispatch code never use scalar overloads really tensors.,0,scalar operations traced incorrectly,scalar operations traced incorrectly code prints using tensor overload. change dispatch code never use scalar overloads really tensors.
pytorch,24639,porting th operators essential code simplicity performance reasons. porting guides q available umbrella issue 24507 feel free add vitalyfedyunin reviewer get prioritized review.,0,migrate std th aten cuda,migrate std th aten cuda porting th operators essential code simplicity performance reasons. porting guides q available umbrella issue 24507 feel free add vitalyfedyunin reviewer get prioritized review.
pytorch,566,"install instructions please go http pytorch.org needed work across many different linux distros, new old. manylinux build wheel centos5 yes! docker machine.",0,rebuild pip wheels manylinux,"rebuild pip wheels manylinux install instructions please go http pytorch.org needed work across many different linux distros, new old. manylinux build wheel centos5 yes! docker machine."
pytorch,6140,following compile expected behavior dim wraps around match torch semantics . cc zdevito,0,jit script support dim wrapping,jit script support dim wrapping following compile expected behavior dim wraps around match torch semantics . cc zdevito
pytorch,956,"try load openface model https storage.cmusatyalab.org openface models nn4.small2.v1.t7 model load lua 'nn4.small2.v1.t7',unknown classes true , then, convert unknown classes. but, got errors line torch utils serialization read lua file.py , line 269, batchnorm reader obj.running var obj.running var.pow 2 .add obj.eps thank you.",0,load lua yields object attribute 'running var' batchnormalization,"load lua yields object attribute 'running var' batchnormalization try load openface model https storage.cmusatyalab.org openface models nn4.small2.v1.t7 model load lua 'nn4.small2.v1.t7',unknown classes true , then, convert unknown classes. but, got errors line torch utils serialization read lua file.py , line 269, batchnorm reader obj.running var obj.running var.pow 2 .add obj.eps thank you."
pytorch,4119,"turning debug 1 turns inlining. however, declare function , get object code generated it, means undefined symbol errors. might notice release build, since everything gets inlined away. catch errors like ci debug 1. probably pick one platform run only.",0,ci debug 1,"ci debug 1 turning debug 1 turns inlining. however, declare function , get object code generated it, means undefined symbol errors. might notice release build, since everything gets inlined away. catch errors like ci debug 1. probably pick one platform run only."
pytorch,23826,run before. progress https github.com pytorch pytorch pull 23718,0,quantization enable quantization oss tests,quantization enable quantization oss tests run before. progress https github.com pytorch pytorch pull 23718
pytorch,25971,bug reproduce call got cc pietern mrshenli pritamdamania87 zhaojuanmao satgera,0,"outdated exception message, call init rpc name first.","outdated exception message, call init rpc name first. bug reproduce call got cc pietern mrshenli pritamdamania87 zhaojuanmao satgera"
pytorch,24958,"feature sparse adam optimizer, https github.com pytorch pytorch blob master torch optim sparse adam.py 'exp avg' 'exp avg sq' initialized first step taken. step, check state initialized. compare adagrad https github.com pytorch pytorch blob master torch optim adagrad.py , 'sum' initialized optimizer initialized. reason this? motivation manipulations 'exp avg' 'exp avg sq', code would clearer run first step coding manipulators. also, unless missing something, code would seem cleaner state initialized optimizer init, instead checking initiated already step, though there's probably performance reduction way currently coded. pitch move init method. additional context here's full code sample agreement this, already created pull request, https github.com pytorch pytorch pull 24960 cc vincentqb",0,"sparse adam, initialize state 'exp avg' state 'exp avg sq' optimizer initialization, step","sparse adam, initialize state 'exp avg' state 'exp avg sq' optimizer initialization, step feature sparse adam optimizer, https github.com pytorch pytorch blob master torch optim sparse adam.py 'exp avg' 'exp avg sq' initialized first step taken. step, check state initialized. compare adagrad https github.com pytorch pytorch blob master torch optim adagrad.py , 'sum' initialized optimizer initialized. reason this? motivation manipulations 'exp avg' 'exp avg sq', code would clearer run first step coding manipulators. also, unless missing something, code would seem cleaner state initialized optimizer init, instead checking initiated already step, though there's probably performance reduction way currently coded. pitch move init method. additional context here's full code sample agreement this, already created pull request, https github.com pytorch pytorch pull 24960 cc vincentqb"
pytorch,16574,"generated type stub module , good enough get autocomplete working pycharm. however, type stub well tested actually, know, accurately reflecting types functions light testing, enough unlikely actually good enough actually use mypy typecheck pytorch using code. seems like something people might interested in, creating issue track. please emoji something would use. cc ezyang gchanan zou3519",0,make possible use mypy typecheck code uses pytorch,"make possible use mypy typecheck code uses pytorch generated type stub module , good enough get autocomplete working pycharm. however, type stub well tested actually, know, accurately reflecting types functions light testing, enough unlikely actually good enough actually use mypy typecheck pytorch using code. seems like something people might interested in, creating issue track. please emoji something would use. cc ezyang gchanan zou3519"
pytorch,17569,"jax case reader jax https github.com google jax full numpy acceleration autodiff functional neural networks, essentially autograd 2.0. italian association machine learning https iaml.it blog jax intro english feature request possible incorporate jax implementation pytorch? looks promising speed machine learning. would like know plan jax. thanks.",0,possible integrate jax pytorch ?,"possible integrate jax pytorch ? jax case reader jax https github.com google jax full numpy acceleration autodiff functional neural networks, essentially autograd 2.0. italian association machine learning https iaml.it blog jax intro english feature request possible incorporate jax implementation pytorch? looks promising speed machine learning. would like know plan jax. thanks."
pytorch,2369,would like calculate derivative node respect image input implement visualization methods using pytorch. suggestions ?,0,calculate derivative node respect input image,calculate derivative node respect input image would like calculate derivative node respect image input implement visualization methods using pytorch. suggestions ?
pytorch,15138,"bug reproduce steps reproduce behavior 1.build pytorch source. 2.replace libs folder libtorch https download.pytorch.org libtorch nightly cu90 libtorch shared deps latest.zip pytorch build lib lib folder created arm building pytorch . 3.build example cpp error message nvidia tegra ubuntu uvaidya libtorch example app build make 50 linking cxx executable example app cmakefiles example app.dir example app.cpp.o function c10 error error c10 sourcelocation, std string const ' example app.cpp .text. zn3c106devicec2ens 10devicetypees zn3c106devicec5ens 10devicetypees 0x1bc undefined reference tensoroptions device const' example app.cpp .text. znk2at13tensoroptions6deviceev znk2at13tensoroptions6deviceev 0x30 undefined reference tensoroptions requires grad const' example app.cpp .text. znk2at13tensoroptions13requires gradev znk2at13tensoroptions13requires gradev 0x38 undefined reference c10 impl getdeviceguardimpl c10 devicetype ' example app.cpp .text. zn3c104impl18getdeviceguardimplens 10devicetypee zn3c104impl18getdeviceguardimplens 10devicetypee 0x11c undefined reference torch autograd make variable tensor, bool ' example app.cpp .text. zn5torch8autograd13make variableen2at6tensoreb zn5torch8autograd13make variableen2at6tensoreb 0xbc undefined reference torch jit sourcerange highlight std ostream const' example app.cpp .text. znk5torch3jit11sourcerange9highlighterso znk5torch3jit11sourcerange9highlighterso 0x1c0 undefined reference c10 error error c10 sourcelocation, std string const ' example app.cpp .text. znk5torch3jit11sourcerange9highlighterso znk5torch3jit11sourcerange9highlighterso 0x41c undefined reference c10 error error c10 sourcelocation, std string const ' follow cmakefiles example app.dir example app.cpp.o function c10 symbol fromqualstring std string const ' cmakefiles example app.dir example app.cpp.o function c10 error error c10 sourcelocation, std string const ' collect2 error ld returned 1 exit status cmakefiles example app.dir build.make 99 recipe target 'example app' failed make 2 example app error 1 cmakefiles makefile2 72 recipe target 'cmakefiles example app.dir all' failed make 1 cmakefiles example app.dir error 2 makefile 83 recipe target 'all' failed make error 2 environment collecting environment information... pytorch version 1.0.0a0 db5d313 debug build cuda used build pytorch 9.2.78 os ubuntu 16.04 lts gcc version ubuntu linaro 5.4.0 6ubuntu1 16.04.10 5.4.0 20160609 cmake version version 3.12.3 python version 3.5 cuda available yes cuda runtime version could collect gpu models configuration could collect nvidia driver version could collect cudnn version probably one following usr lib aarch64 linux gnu libcudnn.so.7.1.2 usr lib aarch64 linux gnu libcudnn static v7.a versions relevant libraries pip could collect conda could collect",0,build simple c example cpp using libtorch fails arm undefined reference c10 error error,"build simple c example cpp using libtorch fails arm undefined reference c10 error error bug reproduce steps reproduce behavior 1.build pytorch source. 2.replace libs folder libtorch https download.pytorch.org libtorch nightly cu90 libtorch shared deps latest.zip pytorch build lib lib folder created arm building pytorch . 3.build example cpp error message nvidia tegra ubuntu uvaidya libtorch example app build make 50 linking cxx executable example app cmakefiles example app.dir example app.cpp.o function c10 error error c10 sourcelocation, std string const ' example app.cpp .text. zn3c106devicec2ens 10devicetypees zn3c106devicec5ens 10devicetypees 0x1bc undefined reference tensoroptions device const' example app.cpp .text. znk2at13tensoroptions6deviceev znk2at13tensoroptions6deviceev 0x30 undefined reference tensoroptions requires grad const' example app.cpp .text. znk2at13tensoroptions13requires gradev znk2at13tensoroptions13requires gradev 0x38 undefined reference c10 impl getdeviceguardimpl c10 devicetype ' example app.cpp .text. zn3c104impl18getdeviceguardimplens 10devicetypee zn3c104impl18getdeviceguardimplens 10devicetypee 0x11c undefined reference torch autograd make variable tensor, bool ' example app.cpp .text. zn5torch8autograd13make variableen2at6tensoreb zn5torch8autograd13make variableen2at6tensoreb 0xbc undefined reference torch jit sourcerange highlight std ostream const' example app.cpp .text. znk5torch3jit11sourcerange9highlighterso znk5torch3jit11sourcerange9highlighterso 0x1c0 undefined reference c10 error error c10 sourcelocation, std string const ' example app.cpp .text. znk5torch3jit11sourcerange9highlighterso znk5torch3jit11sourcerange9highlighterso 0x41c undefined reference c10 error error c10 sourcelocation, std string const ' follow cmakefiles example app.dir example app.cpp.o function c10 symbol fromqualstring std string const ' cmakefiles example app.dir example app.cpp.o function c10 error error c10 sourcelocation, std string const ' collect2 error ld returned 1 exit status cmakefiles example app.dir build.make 99 recipe target 'example app' failed make 2 example app error 1 cmakefiles makefile2 72 recipe target 'cmakefiles example app.dir all' failed make 1 cmakefiles example app.dir error 2 makefile 83 recipe target 'all' failed make error 2 environment collecting environment information... pytorch version 1.0.0a0 db5d313 debug build cuda used build pytorch 9.2.78 os ubuntu 16.04 lts gcc version ubuntu linaro 5.4.0 6ubuntu1 16.04.10 5.4.0 20160609 cmake version version 3.12.3 python version 3.5 cuda available yes cuda runtime version could collect gpu models configuration could collect nvidia driver version could collect cudnn version probably one following usr lib aarch64 linux gnu libcudnn.so.7.1.2 usr lib aarch64 linux gnu libcudnn static v7.a versions relevant libraries pip could collect conda could collect"
pytorch,4408,"using model contains linear layer bias, receive error constructed minimal working example reproduces environment info",0,"torch.onnx.export fails linear bias false, torch.onnx.symbolic.matmul exist","torch.onnx.export fails linear bias false, torch.onnx.symbolic.matmul exist using model contains linear layer bias, receive error constructed minimal working example reproduces environment info"
pytorch,19057,building blocked executing python setup.py build environment info os ubuntu 16.04 gcc 5.3.0 cuda 9.0 gpu geforce gtx 1080 2 pytorch installed 1.0.0 try build pytorch 1.0.0 source seems blocked . waited long time go on. happen?,0,building blocked executing python setup.py build compiler supports long double,building blocked executing python setup.py build compiler supports long double building blocked executing python setup.py build environment info os ubuntu 16.04 gcc 5.3.0 cuda 9.0 gpu geforce gtx 1080 2 pytorch installed 1.0.0 try build pytorch 1.0.0 source seems blocked . waited long time go on. happen?
pytorch,6462,"can't find document batch size setting dataparallel mode. set batch size using dataparallel mode pytorch? similiar keras? thanks keras, parallel model like dataparallel mode pytorch , document clear think keras document beginging e.g. batch size 64 use gpus 2, divide input 2 sub batches 32 samples, process sub batch one gpu, return full batch 64 processed samples call distributed 8 gpus. since batch size 256, gpu process 32 samples. parallel model.fit x, y, epochs 20, batch size 256 keras document ended",0,set batch size using dataparallel mode pytorch?,"set batch size using dataparallel mode pytorch? can't find document batch size setting dataparallel mode. set batch size using dataparallel mode pytorch? similiar keras? thanks keras, parallel model like dataparallel mode pytorch , document clear think keras document beginging e.g. batch size 64 use gpus 2, divide input 2 sub batches 32 samples, process sub batch one gpu, return full batch 64 processed samples call distributed 8 gpus. since batch size 256, gpu process 32 samples. parallel model.fit x, y, epochs 20, batch size 256 keras document ended"
pytorch,22586,"bug reproduce got results gpu nan 0.004 probability happen, tot 38 cpu nan 0.000 probability happen, tot 0 expected behavior nan happen environment pytorch version 1.1.0 os linux installed pytorch python version 2.7 cuda cudnn version 9.0",0,got nan gumbel softmax calculated gpu,"got nan gumbel softmax calculated gpu bug reproduce got results gpu nan 0.004 probability happen, tot 38 cpu nan 0.000 probability happen, tot 0 expected behavior nan happen environment pytorch version 1.1.0 os linux installed pytorch python version 2.7 cuda cudnn version 9.0"
pytorch,4053,"soumith environment unbuntu 16.04 torch 0.3.0.post4 cp35 cp35m linux x86 64.whl pytorch.org windows 10 pytorch master compiled msvc according internal logicit failure crash. tested code pytorch 0.2.0 , crash.",0,grad fn crashed pytorch 0.3.0,"grad fn crashed pytorch 0.3.0 soumith environment unbuntu 16.04 torch 0.3.0.post4 cp35 cp35m linux x86 64.whl pytorch.org windows 10 pytorch master compiled msvc according internal logicit failure crash. tested code pytorch 0.2.0 , crash."
pytorch,1623,"hi, two linux servers. code works one machine other. one work, error message occured batch normalization convolution fine . also, code works cpu gpu. guess problem mainly related gpu libraries like cuda cudnn. one work following configurations nvidia driver 375.26 cuda 8.0.61 cudnn 5.1.10 one work , following configurations nvidia driver 367.48 cuda 8.0.44 cudnn 5.1.5 tensorflow run environment flawless though. wondering caused incompatibility pytorch 0.1.12 cuda 8.0.61 cudnn 5.1.10 ?",0,incompatibility pytorch 0.1.12 cuda 8.0.61 cudnn 5.1.10 ?,"incompatibility pytorch 0.1.12 cuda 8.0.61 cudnn 5.1.10 ? hi, two linux servers. code works one machine other. one work, error message occured batch normalization convolution fine . also, code works cpu gpu. guess problem mainly related gpu libraries like cuda cudnn. one work following configurations nvidia driver 375.26 cuda 8.0.61 cudnn 5.1.10 one work , following configurations nvidia driver 367.48 cuda 8.0.44 cudnn 5.1.5 tensorflow run environment flawless though. wondering caused incompatibility pytorch 0.1.12 cuda 8.0.61 cudnn 5.1.10 ?"
pytorch,17048,"bug trying subclass ends makes sense, possible subclass python's well . type, rather context method main problem expose module impossible subclass might related 16954 think dupe, main problem exposing module. reproduce steps reproduce behavior 1. expected behavior possible subclass torch's multiprocessing pool. environment pytorch version 1.0.0 debug build cuda used build pytorch 9.0.176 os ubuntu 16.04 lts, gcc version ubuntu 5.4.0 6ubuntu1 16.04.11 5.4.0 20160609 cmake version could collect python version 3.6 cuda available yes cuda runtime version could collect gpu models configuration gpu 0 tesla p100 pcie 12gb nvidia driver version 390.30 cudnn version could collect versions relevant libraries pip3 numpy 1.14.3 pip3 torch 1.0.0 pip3 torchvision 0.2.1 conda could collect",0,subclass subclass torch.multiprocessing.pool,"subclass subclass torch.multiprocessing.pool bug trying subclass ends makes sense, possible subclass python's well . type, rather context method main problem expose module impossible subclass might related 16954 think dupe, main problem exposing module. reproduce steps reproduce behavior 1. expected behavior possible subclass torch's multiprocessing pool. environment pytorch version 1.0.0 debug build cuda used build pytorch 9.0.176 os ubuntu 16.04 lts, gcc version ubuntu 5.4.0 6ubuntu1 16.04.11 5.4.0 20160609 cmake version could collect python version 3.6 cuda available yes cuda runtime version could collect gpu models configuration gpu 0 tesla p100 pcie 12gb nvidia driver version 390.30 cudnn version could collect versions relevant libraries pip3 numpy 1.14.3 pip3 torch 1.0.0 pip3 torchvision 0.2.1 conda could collect"
pytorch,18873,nan,0,run shellcheck oss builds,run shellcheck oss builds nan
pytorch,9226,nan,0,warn batchsize gpu data parallel,warn batchsize gpu data parallel nan
pytorch,8393,see also https github.com pytorch pytorch issues 8410 example output operator level tests fail,0,jit script can't allocate zero gradients value without type,jit script can't allocate zero gradients value without type see also https github.com pytorch pytorch issues 8410 example output operator level tests fail
pytorch,947,"right now, thpp exposes four templatized classes propose consolidate single class, like following on. fashion, nothing current api would change current code uses thpp would work , users thpp would able choose level templatization want. current methods subset classes, either template class specialization, simply make methods part classes. interested using thpp separate project, would cleaner tensor class templatized least device addition type , frameworks e.g mxnet . happy submit pr developers interested, otherwise make changes locally.",0,"thpp tensor templatized device maybe density, e.g sparse vs dense","thpp tensor templatized device maybe density, e.g sparse vs dense right now, thpp exposes four templatized classes propose consolidate single class, like following on. fashion, nothing current api would change current code uses thpp would work , users thpp would able choose level templatization want. current methods subset classes, either template class specialization, simply make methods part classes. interested using thpp separate project, would cleaner tensor class templatized least device addition type , frameworks e.g mxnet . happy submit pr developers interested, otherwise make changes locally."
pytorch,3567,"installed source, tried various branches master, v0.3.0, soumith patch 1 , still getting error. full uninstall reinstall help either. calling form torchvision module works fine, creating variables device seem provoke error either. also ran got log https gist.github.com vladislavzavadskyy 8b1a5abffe6298fe2908fe0e3bd6cbf9 , suggests searches cudnn 6, idea fix it.",0,undefined symbol cudnnsetconvolutiongroupcount running cudnn 7.0.3 cuda 9,"undefined symbol cudnnsetconvolutiongroupcount running cudnn 7.0.3 cuda 9 installed source, tried various branches master, v0.3.0, soumith patch 1 , still getting error. full uninstall reinstall help either. calling form torchvision module works fine, creating variables device seem provoke error either. also ran got log https gist.github.com vladislavzavadskyy 8b1a5abffe6298fe2908fe0e3bd6cbf9 , suggests searches cudnn 6, idea fix it."
pytorch,17465,"hi, wanna work conv2d, release make source code con2d available. approach conv2d? questions help please note issue tracker help form issue closed. set listed resources available website https pytorch.org resources . primary means support discussion forum discussion forum https discuss.pytorch.org",0,source code conv2d,"source code conv2d hi, wanna work conv2d, release make source code con2d available. approach conv2d? questions help please note issue tracker help form issue closed. set listed resources available website https pytorch.org resources . primary means support discussion forum discussion forum https discuss.pytorch.org"
pytorch,27296,"add quantized tensor creation functions torch.tensor documentation, add torch.quantize per tensor torch.quantize per channel . raghuramank100 visibility",0,document torch.quantize per tensor torch.quantize per channel,"document torch.quantize per tensor torch.quantize per channel add quantized tensor creation functions torch.tensor documentation, add torch.quantize per tensor torch.quantize per channel . raghuramank100 visibility"
pytorch,15589,"version stable 1.0.0 get url https download.pytorch.org libtorch cu90 libtorch win shared deps latest.zip test code include include include include using namespace std int main int argc, const char argv tensor tensor torch rand 2, 3 .to kcuda std cout tensor std endl string model name model.pt try std ifstream model name, std ios base binary in.fail cout failed open model endl else cout successed open model endl check !in.fail , load could open file , model name cout parsed checking endl std shared ptr module torch jit load model name module kcuda assert module ! nullptr std cout ok n std vector inputs inputs.push back torch ones 1, 3, 224, 224 .to kcuda execute model turn output tensor. tensor output module forward inputs .totensor std cout output.slice dim 1, start 0, end 5 ' n' catch exception err cout err.what endl cout finished endl output learn cv lab example app build6 debug example app.exe model.pt 0.6020 0.1421 0.9155 0.6821 0.7416 0.7934 variable cudafloattype 2,3 load could open file load .. torch csrc jit import.cpp 250 backtrace available finished model python trained,with following code import torch import torchvision instance model. model torchvision.models.resnet18 example input would normally provide model's forward method. example torch.rand 1, 3, 224, 224 use torch.jit.trace generate torch.jit.scriptmodule via tracing. traced script module torch.jit.trace model, example save traced script module.save model.pt right position, check code shows.",0,unable load python trained model libtorch c api windows,"unable load python trained model libtorch c api windows version stable 1.0.0 get url https download.pytorch.org libtorch cu90 libtorch win shared deps latest.zip test code include include include include using namespace std int main int argc, const char argv tensor tensor torch rand 2, 3 .to kcuda std cout tensor std endl string model name model.pt try std ifstream model name, std ios base binary in.fail cout failed open model endl else cout successed open model endl check !in.fail , load could open file , model name cout parsed checking endl std shared ptr module torch jit load model name module kcuda assert module ! nullptr std cout ok n std vector inputs inputs.push back torch ones 1, 3, 224, 224 .to kcuda execute model turn output tensor. tensor output module forward inputs .totensor std cout output.slice dim 1, start 0, end 5 ' n' catch exception err cout err.what endl cout finished endl output learn cv lab example app build6 debug example app.exe model.pt 0.6020 0.1421 0.9155 0.6821 0.7416 0.7934 variable cudafloattype 2,3 load could open file load .. torch csrc jit import.cpp 250 backtrace available finished model python trained,with following code import torch import torchvision instance model. model torchvision.models.resnet18 example input would normally provide model's forward method. example torch.rand 1, 3, 224, 224 use torch.jit.trace generate torch.jit.scriptmodule via tracing. traced script module torch.jit.trace model, example save traced script module.save model.pt right position, check code shows."
pytorch,27507,feature export meshgrid onnx motivation instead error use pitch use export meshgrid alternatives use code,0,onnx export torch.meshgrid,onnx export torch.meshgrid feature export meshgrid onnx motivation instead error use pitch use export meshgrid alternatives use code
pytorch,2062,"installing pytorch source using python 2.7. getting following error file scratch0 softwares pytorch new torch lib aten gen.py , line 6, import preprocess declarations file scratch0 softwares pytorch new torch lib aten preprocess declarations.py , line 3, function wrapper import type formal generic file scratch0 softwares pytorch new torch lib aten function wrapper.py , line 2, code template import codetemplate file scratch0 softwares pytorch new torch lib aten code template.py , line 13, class codetemplate object file scratch0 softwares pytorch new torch lib aten code template.py , line 15, codetemplate ' n ? w w ,? w w ,? ', re.multiline file scratch0 softwares virtualpython env new lib64 python2.7 re.py , line 190, compile return compile pattern, flags file scratch0 softwares virtualpython env new lib64 python2.7 re.py , line 242, compile raise error, v invalid expression sre constants.error nothing repeat cmake error cmakelists.txt 126 message failed get generated cpp list configuring incomplete, errors occurred!",0,installation error,"installation error installing pytorch source using python 2.7. getting following error file scratch0 softwares pytorch new torch lib aten gen.py , line 6, import preprocess declarations file scratch0 softwares pytorch new torch lib aten preprocess declarations.py , line 3, function wrapper import type formal generic file scratch0 softwares pytorch new torch lib aten function wrapper.py , line 2, code template import codetemplate file scratch0 softwares pytorch new torch lib aten code template.py , line 13, class codetemplate object file scratch0 softwares pytorch new torch lib aten code template.py , line 15, codetemplate ' n ? w w ,? w w ,? ', re.multiline file scratch0 softwares virtualpython env new lib64 python2.7 re.py , line 190, compile return compile pattern, flags file scratch0 softwares virtualpython env new lib64 python2.7 re.py , line 242, compile raise error, v invalid expression sre constants.error nothing repeat cmake error cmakelists.txt 126 message failed get generated cpp list configuring incomplete, errors occurred!"
pytorch,10617,trying install pytorch pyenv python 2.7 installed libraries like numpy pandas worked fine. pip list gives macos 10.13.5.,0,command python setup.py egg info failed error code 1,command python setup.py egg info failed error code 1 trying install pytorch pyenv python 2.7 installed libraries like numpy pandas worked fine. pip list gives macos 10.13.5.
pytorch,11757,"issue description far, builds ci numpy support, meaning issue exposed entirety yet. tried build pytorch without numpy support, possible first. editing made sense accomplish this, seems caffe2 requires numpy support. error got almost building pytorch. code example file system info pytorch caffe2 pytorch installed pytorch conda, pip, source source build command used compiling source os ubuntu 18.04.1 python version 3.7 gcc version compiling source 7.3 cmake version 3.11.3 versions relevant libraries none",0,unable build pytorch source without numpy support,"unable build pytorch source without numpy support issue description far, builds ci numpy support, meaning issue exposed entirety yet. tried build pytorch without numpy support, possible first. editing made sense accomplish this, seems caffe2 requires numpy support. error got almost building pytorch. code example file system info pytorch caffe2 pytorch installed pytorch conda, pip, source source build command used compiling source os ubuntu 18.04.1 python version 3.7 gcc version compiling source 7.3 cmake version 3.11.3 versions relevant libraries none"
pytorch,2754,"using ppc64 ubuntu 16.04. compiled pytorch version 0.2.0 running problems test cuda.py. isolate failures commented types float types like types torch.floattensor, torch.doubletensor, torch.longtensor, torch.inttensor, torch.shorttensor, torch.chartensor, torch.bytetensor, float types torch.floattensor, torch.doubletensor todo add half... run python test cuda.py get bunch errors opt pytorch torch lib thc thctensortopk.cuh 431 void gathertopk tensorinfo, indextype, indextype, indextype, indextype, tensorinfo, indextype, indextype, tensorinfo, indextype char, indextype unsigned int, dim 3, order true block 64,0,0 , thread 9,0,0 assertion failed. difference line block thread data also get bunch different tests error test chartensor tril main .testcuda traceback recent call last file mytest cuda.py , line 358, tmp gpu tensor gpu cpu tensor file opt pytorch test common.py , line 89, gpu return obj.clone .type file opt conda envs pytorch py35 lib python3.5 site packages torch utils.py , line 35, type return new type self.size .copy self, async runtimeerror cuda runtime error 59 device side assert triggered opt pytorch torch lib thc generic thctensorcopy.c 18 failing thctensorcopy.c 18 failures happen type torch.chartensor output nvidia smi nvidia smi fri sep 15 21 54 54 2017 nvidia smi 384.66 driver version 384.66 gpu name persistence bus id disp.a volatile uncorr. ecc fan temp perf pwr usage cap memory usage gpu util compute m. 0 tesla k80 00000000 03 00.0 0 n 45c p0 59w 149w 426mib 11439mib 0 default 1 tesla k80 00000000 04 00.0 0 n 30c p8 30w 149w 1mib 11439mib 0 default 2 tesla k80 00000002 03 00.0 0 n 32c p8 26w 149w 1mib 11439mib 0 default 3 tesla k80 00000002 04 00.0 0 n 25c p8 29w 149w 1mib 11439mib 0 default processes gpu memory gpu pid type process name usage",0,ppc64le test cuda.py failures,"ppc64le test cuda.py failures using ppc64 ubuntu 16.04. compiled pytorch version 0.2.0 running problems test cuda.py. isolate failures commented types float types like types torch.floattensor, torch.doubletensor, torch.longtensor, torch.inttensor, torch.shorttensor, torch.chartensor, torch.bytetensor, float types torch.floattensor, torch.doubletensor todo add half... run python test cuda.py get bunch errors opt pytorch torch lib thc thctensortopk.cuh 431 void gathertopk tensorinfo, indextype, indextype, indextype, indextype, tensorinfo, indextype, indextype, tensorinfo, indextype char, indextype unsigned int, dim 3, order true block 64,0,0 , thread 9,0,0 assertion failed. difference line block thread data also get bunch different tests error test chartensor tril main .testcuda traceback recent call last file mytest cuda.py , line 358, tmp gpu tensor gpu cpu tensor file opt pytorch test common.py , line 89, gpu return obj.clone .type file opt conda envs pytorch py35 lib python3.5 site packages torch utils.py , line 35, type return new type self.size .copy self, async runtimeerror cuda runtime error 59 device side assert triggered opt pytorch torch lib thc generic thctensorcopy.c 18 failing thctensorcopy.c 18 failures happen type torch.chartensor output nvidia smi nvidia smi fri sep 15 21 54 54 2017 nvidia smi 384.66 driver version 384.66 gpu name persistence bus id disp.a volatile uncorr. ecc fan temp perf pwr usage cap memory usage gpu util compute m. 0 tesla k80 00000000 03 00.0 0 n 45c p0 59w 149w 426mib 11439mib 0 default 1 tesla k80 00000000 04 00.0 0 n 30c p8 30w 149w 1mib 11439mib 0 default 2 tesla k80 00000002 03 00.0 0 n 32c p8 26w 149w 1mib 11439mib 0 default 3 tesla k80 00000002 04 00.0 0 n 25c p8 29w 149w 1mib 11439mib 0 default processes gpu memory gpu pid type process name usage"
pytorch,25801,cc suo,0,jit missing source highlight error,jit missing source highlight error cc suo
pytorch,3946,"keras,there timedistributed function https github.com fchollet keras blob master keras layers wrappers.py apply layer temporal slice, hope author develop function like that.",0,pytorch function work keras's timedistributed ?,"pytorch function work keras's timedistributed ? keras,there timedistributed function https github.com fchollet keras blob master keras layers wrappers.py apply layer temporal slice, hope author develop function like that."
pytorch,16693,error building latest pytorch installing steps presented official website. macos. ui. interesting string.h anaconda directory,0,build error build caffe error,build error build caffe error error building latest pytorch installing steps presented official website. macos. ui. interesting string.h anaconda directory
pytorch,17537,bug reproduce end getting giberrish tensor returned function. function returns python,0,cannot initial blob std vector.data,cannot initial blob std vector.data bug reproduce end getting giberrish tensor returned function. function returns python
pytorch,27626,"feature bug using cross entropy k target class sequence application, expected squeeze calling loss suppose past vector minibatch size, sequence length, k tag ? because, line code step giving prob vector greater 1 almost equal 1. exponantial application sure. example situation,",0,cross entropy sequence log softmax almost equal 1,"cross entropy sequence log softmax almost equal 1 feature bug using cross entropy k target class sequence application, expected squeeze calling loss suppose past vector minibatch size, sequence length, k tag ? because, line code step giving prob vector greater 1 almost equal 1. exponantial application sure. example situation,"
pytorch,768,"seems support bitwise operations numpy https docs.scipy.org doc numpy reference generated numpy.bitwise xor.html bitwise operations also supported cuda https docs.nvidia.com cuda cuda c programming guide warp shuffle functions yet, implementation binary operation takes two bytetensor. undertsanding inefficient, one bit within every byte tensor actually encoding something. please correct wrong!! record neither tensorflow bitwise operations",0,feature request bitwise operations,"feature request bitwise operations seems support bitwise operations numpy https docs.scipy.org doc numpy reference generated numpy.bitwise xor.html bitwise operations also supported cuda https docs.nvidia.com cuda cuda c programming guide warp shuffle functions yet, implementation binary operation takes two bytetensor. undertsanding inefficient, one bit within every byte tensor actually encoding something. please correct wrong!! record neither tensorflow bitwise operations"
pytorch,12946,bug non argument tensors requires grad true traced constants reproduce expected behavior output requires grad true one backprop environment pt master reported ssnl,0,jit trace non argument tensors requires grad true traced constants,jit trace non argument tensors requires grad true traced constants bug non argument tensors requires grad true traced constants reproduce expected behavior output requires grad true one backprop environment pt master reported ssnl
pytorch,12901,"bug torch.nn.l1loss 's parameter reduction work b net . reproduce steps reproduce behavior outputs environment pytorch version 0.4.1 debug build cuda used build pytorch 9.2.148 os debian gnu linux 9.5 stretch gcc version debian 6.3.0 18 deb9u1 6.3.0 20170516 cmake version could collect python version 3.5 cuda available yes cuda runtime version could collect gpu models configuration gpu 0 tesla k80 nvidia driver version 396.44 cudnn version probably one following usr local cuda 9.2 lib64 libcudnn.so.7.2.1 usr local cuda 9.2 lib64 libcudnn static.a versions relevant libraries pip intel numpy 1.15.1 pip numpy 1.15.1 pip torch 0.4.1 pip torchvision 0.2.1 conda could collect additional context checked source. f.l1 loss , use convert reduction integer. however, calls pointwise loss , assumes reduction still str. that's reason bug.",0,torch.nn.l1loss works incorrectly certain situations,"torch.nn.l1loss works incorrectly certain situations bug torch.nn.l1loss 's parameter reduction work b net . reproduce steps reproduce behavior outputs environment pytorch version 0.4.1 debug build cuda used build pytorch 9.2.148 os debian gnu linux 9.5 stretch gcc version debian 6.3.0 18 deb9u1 6.3.0 20170516 cmake version could collect python version 3.5 cuda available yes cuda runtime version could collect gpu models configuration gpu 0 tesla k80 nvidia driver version 396.44 cudnn version probably one following usr local cuda 9.2 lib64 libcudnn.so.7.2.1 usr local cuda 9.2 lib64 libcudnn static.a versions relevant libraries pip intel numpy 1.15.1 pip numpy 1.15.1 pip torch 0.4.1 pip torchvision 0.2.1 conda could collect additional context checked source. f.l1 loss , use convert reduction integer. however, calls pointwise loss , assumes reduction still str. that's reason bug."
pytorch,1151,"hi, want load model trained torch, contains layer nn.spatialupsamplingbilinear , convert code simple blow generate exception find pytorch, function like layer ?",0,pytorch implement spatialupsamplingbilinear layer?,"pytorch implement spatialupsamplingbilinear layer? hi, want load model trained torch, contains layer nn.spatialupsamplingbilinear , convert code simple blow generate exception find pytorch, function like layer ?"
pytorch,16558,"installed pytorch .3.1 using pip3 python3.5, installation successful. however, unable load package python import torch traceback recent call last file , line 1, file user .local lib python3.5 site packages torch init .py , line 56, torch. c import importerror libgcc s.so.1 failed map segment shared object anybody help, resolve this? best",0,pytorch import failed map segment error,"pytorch import failed map segment error installed pytorch .3.1 using pip3 python3.5, installation successful. however, unable load package python import torch traceback recent call last file , line 1, file user .local lib python3.5 site packages torch init .py , line 56, torch. c import importerror libgcc s.so.1 failed map segment shared object anybody help, resolve this? best"
pytorch,23589,"tried script issue valueerror one element tensors converted python scalars 22674 newest pytorch version, seems work. idea configuration using pytorch version 1.1.0 python version 3.7.3 torchvision 0.3.0",0,valueerror one element tensors converted python scalars,"valueerror one element tensors converted python scalars tried script issue valueerror one element tensors converted python scalars 22674 newest pytorch version, seems work. idea configuration using pytorch version 1.1.0 python version 3.7.3 torchvision 0.3.0"
pytorch,2281,ubuntu 16.04 titan x. trying install v0.2.0 branch source. created new conda env followed instructions readme.md. python setup.py install fails error look like attaching complete log file log file.txt https github.com pytorch pytorch files 1196589 log file.txt sasank.,0,error installing branch v0.2.0 source,error installing branch v0.2.0 source ubuntu 16.04 titan x. trying install v0.2.0 branch source. created new conda env followed instructions readme.md. python setup.py install fails error look like attaching complete log file log file.txt https github.com pytorch pytorch files 1196589 log file.txt sasank.
pytorch,28795,building pytorch getting following error. would great know fix it. thank,0,getting build error,getting build error building pytorch getting following error. would great know fix it. thank
pytorch,5486,,0,tensors gracefully compare nonetype,tensors gracefully compare nonetype 
pytorch,1228,"might relevant 691. installed pytorch source mac os successfully, causes error importerror traceback recent call last 1 import torch users dqwang study tools anaconda2 lib python2.7 site packages torch init .pyc 51 sys.setdlopenflags dl flags.rtld global dl flags.rtld 52 53 torch. c import 54 55 name name dir c importerror dlopen users dqwang study tools anaconda2 lib python2.7 site packages torch c.so, 10 symbol found znss4 rep20 empty rep storagee referenced users dqwang study tools anaconda2 lib python2.7 site packages torch c.so expected flat namespace users dqwang study tools anaconda2 lib python2.7 site packages torch c.so author thread got away issue installing conda package rather source, solved case well. install source need cuda.",0,import torch causes error,"import torch causes error might relevant 691. installed pytorch source mac os successfully, causes error importerror traceback recent call last 1 import torch users dqwang study tools anaconda2 lib python2.7 site packages torch init .pyc 51 sys.setdlopenflags dl flags.rtld global dl flags.rtld 52 53 torch. c import 54 55 name name dir c importerror dlopen users dqwang study tools anaconda2 lib python2.7 site packages torch c.so, 10 symbol found znss4 rep20 empty rep storagee referenced users dqwang study tools anaconda2 lib python2.7 site packages torch c.so expected flat namespace users dqwang study tools anaconda2 lib python2.7 site packages torch c.so author thread got away issue installing conda package rather source, solved case well. install source need cuda."
pytorch,1752,"using dockerfile, error directory '.' installable. file 'setup.py' found. believe located line pip install v . anyone know fix it? thanks!",0,dockerfile directory '.' installable. file 'setup.py' found,"dockerfile directory '.' installable. file 'setup.py' found using dockerfile, error directory '.' installable. file 'setup.py' found. believe located line pip install v . anyone know fix it? thanks!"
pytorch,24587,porting th operators essential code simplicity performance reasons. porting guides q available umbrella issue 24507 feel free add vitalyfedyunin reviewer get prioritized review.,0,migrate log10 log10 th aten cuda,migrate log10 log10 th aten cuda porting th operators essential code simplicity performance reasons. porting guides q available umbrella issue 24507 feel free add vitalyfedyunin reviewer get prioritized review.
pytorch,7660,"currently many operators aten ir multiple forms. operator takes non tensor argument e.g. intlist int64 two forms one non tensor arguments encoded tensor inputs one arguments attributes graph. makes every pass write ir harder needs handle cases. proposed solution 1. canonicalize tensor input format. graphs optimized format. format general inputs required constants. 2. expose function compiler general usage. optimizations check const ness inputs work. 3. interpretation, lower back attribute format thing constants. already includes function, written pass.",0,canonicalize aten operators ir,"canonicalize aten operators ir currently many operators aten ir multiple forms. operator takes non tensor argument e.g. intlist int64 two forms one non tensor arguments encoded tensor inputs one arguments attributes graph. makes every pass write ir harder needs handle cases. proposed solution 1. canonicalize tensor input format. graphs optimized format. format general inputs required constants. 2. expose function compiler general usage. optimizations check const ness inputs work. 3. interpretation, lower back attribute format thing constants. already includes function, written pass."
pytorch,1930,nan,0,different behavior sum place cpu vs gpu,different behavior sum place cpu vs gpu nan
pytorch,10660,"hi, tired use torch.mm get matrix multiplication, found returned slightly different results called two equivalent way. assume two matrix matrix matrix b want get matrix multiplication matrix matrix b 1. first, use matrix matrix b input directly, torch.mm matrix a, matrix b 2. hand, call torch.mm row matrix together matrix b inputs, concatenate results. two results expected exactly same, still slightly diferent. somebody explain reason? thanks, test code. output. 2018 08 18 21 32 11,294 info 87997 test.py 81 difference two values tensor 1.00000e 06 1.3113 2018 08 18 21 32 11,295 info 87997 test.py 83 batch id 0 2018 08 18 21 32 11,295 info 87997 test.py 84 0.89934576 0.11556479 0.48109883 0.3961889 2018 08 18 21 32 11,295 info 87997 test.py 85 0.89934576 0.11556476 0.4810989 0.39618894 2018 08 18 21 32 11,296 info 87997 test.py 83 batch id 1 2018 08 18 21 32 11,296 info 87997 test.py 84 2.6500921 0.5928108 2.213797 0.39320117 2018 08 18 21 32 11,296 info 87997 test.py 85 2.6500921 0.59281075 2.2137969 0.3932012 2018 08 18 21 32 11,296 info 87997 test.py 83 batch id 2 2018 08 18 21 32 11,296 info 87997 test.py 84 1.3148142 3.5158207 2.2286756 2.1263554 2018 08 18 21 32 11,297 info 87997 test.py 85 1.3148141 3.515821 2.2286754 2.1263554 2018 08 18 21 32 11,297 info 87997 test.py 83 batch id 3 2018 08 18 21 32 11,297 info 87997 test.py 84 0.25188264 1.0855039 1.4054772 0.47265998 2018 08 18 21 32 11,297 info 87997 test.py 85 0.25188267 1.085504 1.4054773 0.47265998 2018 08 18 21 32 11,298 info 87997 test.py 83 batch id 4 2018 08 18 21 32 11,298 info 87997 test.py 84 1.250929 3.790292 1.7040166 2.443583 2018 08 18 21 32 11,298 info 87997 test.py 85 1.250929 3.790292 1.7040166 2.443583",0,inconsistent output torch.mm called two mathematically equivalent way,"inconsistent output torch.mm called two mathematically equivalent way hi, tired use torch.mm get matrix multiplication, found returned slightly different results called two equivalent way. assume two matrix matrix matrix b want get matrix multiplication matrix matrix b 1. first, use matrix matrix b input directly, torch.mm matrix a, matrix b 2. hand, call torch.mm row matrix together matrix b inputs, concatenate results. two results expected exactly same, still slightly diferent. somebody explain reason? thanks, test code. output. 2018 08 18 21 32 11,294 info 87997 test.py 81 difference two values tensor 1.00000e 06 1.3113 2018 08 18 21 32 11,295 info 87997 test.py 83 batch id 0 2018 08 18 21 32 11,295 info 87997 test.py 84 0.89934576 0.11556479 0.48109883 0.3961889 2018 08 18 21 32 11,295 info 87997 test.py 85 0.89934576 0.11556476 0.4810989 0.39618894 2018 08 18 21 32 11,296 info 87997 test.py 83 batch id 1 2018 08 18 21 32 11,296 info 87997 test.py 84 2.6500921 0.5928108 2.213797 0.39320117 2018 08 18 21 32 11,296 info 87997 test.py 85 2.6500921 0.59281075 2.2137969 0.3932012 2018 08 18 21 32 11,296 info 87997 test.py 83 batch id 2 2018 08 18 21 32 11,296 info 87997 test.py 84 1.3148142 3.5158207 2.2286756 2.1263554 2018 08 18 21 32 11,297 info 87997 test.py 85 1.3148141 3.515821 2.2286754 2.1263554 2018 08 18 21 32 11,297 info 87997 test.py 83 batch id 3 2018 08 18 21 32 11,297 info 87997 test.py 84 0.25188264 1.0855039 1.4054772 0.47265998 2018 08 18 21 32 11,297 info 87997 test.py 85 0.25188267 1.085504 1.4054773 0.47265998 2018 08 18 21 32 11,298 info 87997 test.py 83 batch id 4 2018 08 18 21 32 11,298 info 87997 test.py 84 1.250929 3.790292 1.7040166 2.443583 2018 08 18 21 32 11,298 info 87997 test.py 85 1.250929 3.790292 1.7040166 2.443583"
pytorch,1528,"seems like use longtensor indexing duplicate entries variable, breaks autograd. specifically, entry vector duplicated, backward maintains gradients last copy element. simple example however version version 0.1.11 fc48d2c, tested 0.1.11 9f3119a well. happens cuda cpu tensors.",0,longtensor indexing duplication propagating gradients,"longtensor indexing duplication propagating gradients seems like use longtensor indexing duplicate entries variable, breaks autograd. specifically, entry vector duplicated, backward maintains gradients last copy element. simple example however version version 0.1.11 fc48d2c, tested 0.1.11 9f3119a well. happens cuda cpu tensors."
pytorch,7276,"think converting caffe2 model pytorch model continue changing model, e.g fine tuning, transfer learning? according new feature pytorch 1.0, seems possible, it?",0,feature request caffe2 model pytorch model,"feature request caffe2 model pytorch model think converting caffe2 model pytorch model continue changing model, e.g fine tuning, transfer learning? according new feature pytorch 1.0, seems possible, it?"
pytorch,871,"gpu produce indices bounds. consider following code here's output example run here, sampled indices 0 5, one values . iteration happens random. following code uses probabilities closer actual probabilities problem problem occurs much later average though . example output snippet",0,gpu torch.multinomial produces bounds index,"gpu torch.multinomial produces bounds index gpu produce indices bounds. consider following code here's output example run here, sampled indices 0 5, one values . iteration happens random. following code uses probabilities closer actual probabilities problem problem occurs much later average though . example output snippet"
pytorch,13787,"bug using distributeddataparallel mpi backend assigning gpu single process host, program crashes end epoch. failure always consistent. reproduce https github.com pytorch examples blob master imagenet main.py https github.com pytorch examples blob master imagenet main.py run modifications environment additional context crash less likely occur decreased, increased. core dump generated pytorchsegfault https github.com pytorch pytorch files 2572658 pytorchsegfault.txt",0,dataloader segmentation fault using mpi backend single process per gpu,"dataloader segmentation fault using mpi backend single process per gpu bug using distributeddataparallel mpi backend assigning gpu single process host, program crashes end epoch. failure always consistent. reproduce https github.com pytorch examples blob master imagenet main.py https github.com pytorch examples blob master imagenet main.py run modifications environment additional context crash less likely occur decreased, increased. core dump generated pytorchsegfault https github.com pytorch pytorch files 2572658 pytorchsegfault.txt"
pytorch,125,"default tests deterministic, individually. randomized tests run nightly something, default.",0,make tests deterministic,"make tests deterministic default tests deterministic, individually. randomized tests run nightly something, default."
pytorch,30986,bug gives fixed https github.com pytorch pytorch pull 30898,0,"wrong result cpu implementation m, .addmv m, 0 , 0, blas used","wrong result cpu implementation m, .addmv m, 0 , 0, blas used bug gives fixed https github.com pytorch pytorch pull 30898"
pytorch,11901,"cases, convenient arrange parameters 2d 3d arrays rather 1d list. one could solve computing nd 1d offset array, could avoided objects could nested example",0,feature request ability nest parameterlist,"feature request ability nest parameterlist cases, convenient arrange parameters 2d 3d arrays rather 1d list. one could solve computing nd 1d offset array, could avoided objects could nested example"
pytorch,4031,interrupt pytorch script using ctrl c occasionally gpu memory deallocated. also threads related script may may still running. running kill using kill 9 pid . however deallocate memory gpu. tue dec 5 14 12 11 2017 nvidia smi 384.98 driver version 384.98 gpu name persistence bus id disp.a volatile uncorr. ecc fan temp perf pwr usage cap memory usage gpu util compute m. 0 titan x pascal 00000000 02 00.0 n 31 54c p2 58w 250w 7829mib 12189mib 3 default processes gpu memory gpu pid type process name usage 0 1213 g usr lib xorg xorg 166mib 0 2355 g compiz 249mib 0 32412 g token 00df6ccab2487bd6f9ae70914f3a9358 8mib,0,nvidia memory deallocated interupt,nvidia memory deallocated interupt interrupt pytorch script using ctrl c occasionally gpu memory deallocated. also threads related script may may still running. running kill using kill 9 pid . however deallocate memory gpu. tue dec 5 14 12 11 2017 nvidia smi 384.98 driver version 384.98 gpu name persistence bus id disp.a volatile uncorr. ecc fan temp perf pwr usage cap memory usage gpu util compute m. 0 titan x pascal 00000000 02 00.0 n 31 54c p2 58w 250w 7829mib 12189mib 3 default processes gpu memory gpu pid type process name usage 0 1213 g usr lib xorg xorg 166mib 0 2355 g compiz 249mib 0 32412 g token 00df6ccab2487bd6f9ae70914f3a9358 8mib
pytorch,23236,"bug detection module present torchvision.models reproduce steps reproduce behavior 1. import detection torchvision,models python script expected behavior environment please copy paste output environment collection script https raw.githubusercontent.com pytorch pytorch master torch utils collect env.py fill checklist manually . get script run pytorch version e.g., 1.0 os e.g., linux installed pytorch , , source build command used compiling source python version cuda cudnn version gpu models configuration relevant information additional context",0,attributeerror module 'torchvision.models' attribute 'detection',"attributeerror module 'torchvision.models' attribute 'detection' bug detection module present torchvision.models reproduce steps reproduce behavior 1. import detection torchvision,models python script expected behavior environment please copy paste output environment collection script https raw.githubusercontent.com pytorch pytorch master torch utils collect env.py fill checklist manually . get script run pytorch version e.g., 1.0 os e.g., linux installed pytorch , , source build command used compiling source python version cuda cudnn version gpu models configuration relevant information additional context"
pytorch,7528,fails looks like indicates trying run proto compiler proto file created. probably means dep declared didn't. intermittent sometimes lucky build gets ordered right way. maybe get reported onnx repo. cc houseroad bddppq anderspapitto cases failing https github.com pytorch pytorch pull 7465 https ci.pytorch.org jenkins job caffe2 builds job py2 mkl ubuntu16.04 build 4286 console,0,caffe2 c2 onnx build flaky due lack ordering dep,caffe2 c2 onnx build flaky due lack ordering dep fails looks like indicates trying run proto compiler proto file created. probably means dep declared didn't. intermittent sometimes lucky build gets ordered right way. maybe get reported onnx repo. cc houseroad bddppq anderspapitto cases failing https github.com pytorch pytorch pull 7465 https ci.pytorch.org jenkins job caffe2 builds job py2 mkl ubuntu16.04 build 4286 console
pytorch,24856,minimal repro cc suo,0,jit schema matching incorrectly types call append argument type scalar,jit schema matching incorrectly types call append argument type scalar minimal repro cc suo
pytorch,25176,"bug numpy arrays dtype interpreted masks slicing torch arrays, like tensors dtype are. insead, arrays interpreted indices. reproduce steps reproduce behavior output expected behavior expected behaviour result slicing torch tensors dtype, input get expected output environment pytorch version 1.2.0a0 0885dd2 os ubuntu 16.04.6 lts installed pytorch nvidia docker image python version 3.6 cuda cudnn version nvidia driver version 384.183, gpu models configuration tesla v100",0,tensor slicing boolean numpy mask wrong,"tensor slicing boolean numpy mask wrong bug numpy arrays dtype interpreted masks slicing torch arrays, like tensors dtype are. insead, arrays interpreted indices. reproduce steps reproduce behavior output expected behavior expected behaviour result slicing torch tensors dtype, input get expected output environment pytorch version 1.2.0a0 0885dd2 os ubuntu 16.04.6 lts installed pytorch nvidia docker image python version 3.6 cuda cudnn version nvidia driver version 384.183, gpu models configuration tesla v100"
pytorch,987,"create module class add layer it, can't see new layer try use ipython's autocomplete. example, following class class example nn.module def init self super . init self.fully connected nn.linear 20, 1 ex example type , options 'forward' 'float'. type , get completion option attribute. limited understanding, think strings returned method attr attr ex. dir attr.startswith 'f' 'forward', 'float' first issue, modify class method def dir self return super . dir list self. modules get possible completion. sure intentional design decision, would nice relevant possible attributes available auto complete, still becoming familiar library.",0,implement nn.module. dir,"implement nn.module. dir create module class add layer it, can't see new layer try use ipython's autocomplete. example, following class class example nn.module def init self super . init self.fully connected nn.linear 20, 1 ex example type , options 'forward' 'float'. type , get completion option attribute. limited understanding, think strings returned method attr attr ex. dir attr.startswith 'f' 'forward', 'float' first issue, modify class method def dir self return super . dir list self. modules get possible completion. sure intentional design decision, would nice relevant possible attributes available auto complete, still becoming familiar library."
pytorch,2656,"import os import numpy np import cpickle import torch import torchvision import gzip class yt8m torch.utils.data.dataset def init self, data dir self.data dir data dir self.file list os.listdir data dir attributeerror 'module' object attribute 'data' attribute 'data'?",0,attributeerror 'module' object attribute 'data',"attributeerror 'module' object attribute 'data' import os import numpy np import cpickle import torch import torchvision import gzip class yt8m torch.utils.data.dataset def init self, data dir self.data dir data dir self.file list os.listdir data dir attributeerror 'module' object attribute 'data' attribute 'data'?"
pytorch,5917,"os ubuntu 14.04 lts pytorch version 0.4.0 installed pytorch conda, pip, source pip python version 2.7.6 cuda cudnn version 8 gpu models configuration tesla k20 gcc version compiling source 4.8.4",0,type object 'embedding' attribute 'from pretrained',"type object 'embedding' attribute 'from pretrained' os ubuntu 14.04 lts pytorch version 0.4.0 installed pytorch conda, pip, source pip python version 2.7.6 cuda cudnn version 8 gpu models configuration tesla k20 gcc version compiling source 4.8.4"
pytorch,20697,bug sometimes indexing 1 dimension tensor single element returns tensor reduced rank. reproduce steps reproduce behavior bug gets even better environment collecting environment information... pytorch version 1.1.0 debug build cuda used build pytorch 10.0.130 os ubuntu 18.04.1 lts gcc version ubuntu 7.4.0 1ubuntu1 18.04 7.4.0 cmake version version 3.10.2 python version 3.7 cuda available yes cuda runtime version 10.0.130 gpu models configuration gpu 0 quadro gp100 gpu 1 quadro gp100 nvidia driver version 410.79 cudnn version could collect versions relevant libraries pip numpy 1.16.2 pip torch 1.1.0 pip torchvision 0.2.2 conda mkl 2019.3 199 conda pytorch 1.1.0 py3.7 cuda10.0.130 cudnn7.5.1 0 pytorch conda pytorch nightly 1.1.0.dev20190411 py3.7 cuda10.0.130 cudnn7.4.2 0 pytorch conda torchvision 0.2.2 py 3 pytorch,0,indexing 1 dimension int64 tensor returns incorrect shape,indexing 1 dimension int64 tensor returns incorrect shape bug sometimes indexing 1 dimension tensor single element returns tensor reduced rank. reproduce steps reproduce behavior bug gets even better environment collecting environment information... pytorch version 1.1.0 debug build cuda used build pytorch 10.0.130 os ubuntu 18.04.1 lts gcc version ubuntu 7.4.0 1ubuntu1 18.04 7.4.0 cmake version version 3.10.2 python version 3.7 cuda available yes cuda runtime version 10.0.130 gpu models configuration gpu 0 quadro gp100 gpu 1 quadro gp100 nvidia driver version 410.79 cudnn version could collect versions relevant libraries pip numpy 1.16.2 pip torch 1.1.0 pip torchvision 0.2.2 conda mkl 2019.3 199 conda pytorch 1.1.0 py3.7 cuda10.0.130 cudnn7.5.1 0 pytorch conda pytorch nightly 1.1.0.dev20190411 py3.7 cuda10.0.130 cudnn7.4.2 0 pytorch conda torchvision 0.2.2 py 3 pytorch
pytorch,14563,"initialize global process group using function. function takes backend argument initialize either gloo, nccl, mpi backend. loss want use gloo nccl frontend. way today manually creating processgroup instances using directly. case dealing cpu cuda tensors want use gloo cpu ones nccl2 cuda ones, consider supporting mode initialize single backend, multiple time. use process group dispatcher class forwards calls appropriate process group depending device type tensor arguments. class subclass processgroup base class c use python side well c side. figure right way expose function. cc teng li",0,address multiple process groups torch.distributed,"address multiple process groups torch.distributed initialize global process group using function. function takes backend argument initialize either gloo, nccl, mpi backend. loss want use gloo nccl frontend. way today manually creating processgroup instances using directly. case dealing cpu cuda tensors want use gloo cpu ones nccl2 cuda ones, consider supporting mode initialize single backend, multiple time. use process group dispatcher class forwards calls appropriate process group depending device type tensor arguments. class subclass processgroup base class c use python side well c side. figure right way expose function. cc teng li"
pytorch,18083,"hi, class inherit that? want create general , , later fill type layer want, e.g., capability current api? done easily python, though need declaration hinders python's easiness. thanks, afshin",0,"common class linear, conv, lstm,","common class linear, conv, lstm, hi, class inherit that? want create general , , later fill type layer want, e.g., capability current api? done easily python, though need declaration hinders python's easiness. thanks, afshin"
pytorch,2627,"running code results segfault. running code gpu works flawlessly. output gdb https gist.github.com dchansen 6c616b73e65a68c027efa29082e777a1 issue originally raised https discuss.pytorch.org segmentation fault sequential conv3d cpu 6353 , encountered bug independently.",0,conv3d cpu segfaults,"conv3d cpu segfaults running code results segfault. running code gpu works flawlessly. output gdb https gist.github.com dchansen 6c616b73e65a68c027efa29082e777a1 issue originally raised https discuss.pytorch.org segmentation fault sequential conv3d cpu 6353 , encountered bug independently."
pytorch,17108,"questions help please note issue tracker help form issue closed. set listed resources available website https pytorch.org resources . primary means support discussion forum discussion forum https discuss.pytorch.org trying run test.py file anaconda prompt got messages cuda available please assign gpu core int, 1 0 thcudacheck fail file .. aten src thc thcgeneral.cpp line 87 error 30 unknown error traceback recent call last file vslcore.py , line 202, dqnagent file vslcore.py , line 87, dqnagent torch.set default tensor type 'torch.cuda.floattensor' file softwares anaconda3 lib site packages torch init .py , line 158, set default tensor type c. set default tensor type file softwares anaconda3 lib site packages torch cuda init .py , line 162, lazy init torch. c. cuda init runtimeerror cuda runtime error 30 unknown error .. aten src thc thcgeneral.cpp 87 do?",0,running windows 10 cuda runtime error 30 unknown error .. aten src thc thcgeneral.cpp 87,"running windows 10 cuda runtime error 30 unknown error .. aten src thc thcgeneral.cpp 87 questions help please note issue tracker help form issue closed. set listed resources available website https pytorch.org resources . primary means support discussion forum discussion forum https discuss.pytorch.org trying run test.py file anaconda prompt got messages cuda available please assign gpu core int, 1 0 thcudacheck fail file .. aten src thc thcgeneral.cpp line 87 error 30 unknown error traceback recent call last file vslcore.py , line 202, dqnagent file vslcore.py , line 87, dqnagent torch.set default tensor type 'torch.cuda.floattensor' file softwares anaconda3 lib site packages torch init .py , line 158, set default tensor type c. set default tensor type file softwares anaconda3 lib site packages torch cuda init .py , line 162, lazy init torch. c. cuda init runtimeerror cuda runtime error 30 unknown error .. aten src thc thcgeneral.cpp 87 do?"
pytorch,30076,"trying create nn libtorch 1.3 c using cuda 10.1 windows 10. build using visual studio 2019. far tried basic examples mnist example 1 cpu working. however cannot run cuda. tried move model gpu described 2 , working. move model gpu memory, write model.to kcuda . make sure inputs model also living cuda memory calling tensor.to kcuda , return new tensor cuda memory. tried simple int main auto net std make shared net torch kcuda crashes tried move simple tensors gpu memory crashes well. include int main torch tensor torch ones 2, 2 , torch requires grad torch tensor b torch randn 2, 2 a.to torch kcuda crashes b.to torch kcuda auto c b got exception thrown 0x00007ffb8263a839 resnet50.exe microsoft c exception c10 error memory location 0x000000e574979f30. unhandled exception 0x00007ffb8263a839 resnet50.exe microsoft c exception c10 error memory location 0x000000e574979f30. debug mode pointing auto operator parameters... args decltype std declval std forward args return kernel func std forward args using shows find cuda device. therefore decided build source working either. related https github.com pytorch pytorch issues 30075 issue 525005329 post. 1 https pytorch.org cppdocs frontend.html end end example 2 https pytorch.org tutorials advanced cpp export.html step 4 executing script module c",0,libtorch c cuda raising exception,"libtorch c cuda raising exception trying create nn libtorch 1.3 c using cuda 10.1 windows 10. build using visual studio 2019. far tried basic examples mnist example 1 cpu working. however cannot run cuda. tried move model gpu described 2 , working. move model gpu memory, write model.to kcuda . make sure inputs model also living cuda memory calling tensor.to kcuda , return new tensor cuda memory. tried simple int main auto net std make shared net torch kcuda crashes tried move simple tensors gpu memory crashes well. include int main torch tensor torch ones 2, 2 , torch requires grad torch tensor b torch randn 2, 2 a.to torch kcuda crashes b.to torch kcuda auto c b got exception thrown 0x00007ffb8263a839 resnet50.exe microsoft c exception c10 error memory location 0x000000e574979f30. unhandled exception 0x00007ffb8263a839 resnet50.exe microsoft c exception c10 error memory location 0x000000e574979f30. debug mode pointing auto operator parameters... args decltype std declval std forward args return kernel func std forward args using shows find cuda device. therefore decided build source working either. related https github.com pytorch pytorch issues 30075 issue 525005329 post. 1 https pytorch.org cppdocs frontend.html end end example 2 https pytorch.org tutorials advanced cpp export.html step 4 executing script module c"
pytorch,15839,"documentation feature request? type annotated stub files pep561 https www.python.org dev peps pep 0561 type hinted source pep484 https www.python.org dev peps pep 0484 would beneficial maintainability, new users, making contributions easier.",0,mypy type hinting stub files?,"mypy type hinting stub files? documentation feature request? type annotated stub files pep561 https www.python.org dev peps pep 0561 type hinted source pep484 https www.python.org dev peps pep 0484 would beneficial maintainability, new users, making contributions easier."
pytorch,16501,"bug summing dimension 0 tensor 2 dimensions getting scalar, whereas summing dimension 2 gives correct answer. expected? reproduce expected behavior environment",0,sparse sum sparse sum dimmension gives unexpected results.,"sparse sum sparse sum dimmension gives unexpected results. bug summing dimension 0 tensor 2 dimensions getting scalar, whereas summing dimension 2 gives correct answer. expected? reproduce expected behavior environment"
pytorch,28346,bug getting error loading quantized resnet18 iphone ios demo app pytorchdemo https github.com pytorch ios demo app tree master pytorchdemo . reproduce convert model run ios demo app pytorchdemo https github.com pytorch ios demo app tree master pytorchdemo . get stacktrace above. forked demo coverted models mirth ios demo app https github.com mirth ios demo app expected behavior default image demo getting 50ms per frame iphone 6s. iphone getting 250ms per frame. getting error stacktrace above. expecting error. environment pytorch version 1.3.0 debug build cuda used build pytorch none os mac osx 10.15 gcc version could collect cmake version version 3.11.1 python version 3.6 cuda available cuda runtime version cuda gpu models configuration cuda nvidia driver version cuda cudnn version cuda versions relevant libraries pip could collect conda mkl 2018.0.0 hc285769 4 conda mkl service 1.1.2 py36h7ea6df4 4 conda torch 1.3.0 pypi 0 pypi conda torchfile 0.1.0 pypi 0 pypi conda torchnet 0.0.4 pypi 0 pypi conda torchsummary 1.5.1 pypi 0 pypi conda torchvision 0.4.0 pypi 0 pypi cc suo jerryzh168 jianyuh dzhulgakov raghuramank100,0,error quantize dynamic resnet18 torch.jit.trace ios demo app pytorchdemo,error quantize dynamic resnet18 torch.jit.trace ios demo app pytorchdemo bug getting error loading quantized resnet18 iphone ios demo app pytorchdemo https github.com pytorch ios demo app tree master pytorchdemo . reproduce convert model run ios demo app pytorchdemo https github.com pytorch ios demo app tree master pytorchdemo . get stacktrace above. forked demo coverted models mirth ios demo app https github.com mirth ios demo app expected behavior default image demo getting 50ms per frame iphone 6s. iphone getting 250ms per frame. getting error stacktrace above. expecting error. environment pytorch version 1.3.0 debug build cuda used build pytorch none os mac osx 10.15 gcc version could collect cmake version version 3.11.1 python version 3.6 cuda available cuda runtime version cuda gpu models configuration cuda nvidia driver version cuda cudnn version cuda versions relevant libraries pip could collect conda mkl 2018.0.0 hc285769 4 conda mkl service 1.1.2 py36h7ea6df4 4 conda torch 1.3.0 pypi 0 pypi conda torchfile 0.1.0 pypi 0 pypi conda torchnet 0.0.4 pypi 0 pypi conda torchsummary 1.5.1 pypi 0 pypi conda torchvision 0.4.0 pypi 0 pypi cc suo jerryzh168 jianyuh dzhulgakov raghuramank100
pytorch,24452,"questions help 4 10 models execute seriallyevery model 3d convolutional modeli tried tensorrt,but tensorrt support 2 conv, else try? needs stable fast",0,model deployment multiple serial modelsusing 3dconv,"model deployment multiple serial modelsusing 3dconv questions help 4 10 models execute seriallyevery model 3d convolutional modeli tried tensorrt,but tensorrt support 2 conv, else try? needs stable fast"
pytorch,671,"right now, volumetricconvolution thnn mark handle bias null cases",0,conv3d needs bias option,"conv3d needs bias option right now, volumetricconvolution thnn mark handle bias null cases"
pytorch,17161,"feature currently, indexing torch.halftensor like half tensor indices gives runtime error. runtimeerror index implemented 'torch.halftensor'. so, requesting indexing method. motivation would great able index half tensors. current use case large scale similarity metric computations embeddings gpu figured could use float16 reduce memory usage. pitch indexing method added halftensor objects. alternatives use case alternative chunk tensors appropriately similarity computations. works well, imagine use cases indexing halftensor objects needed.",0,index operation supported torch.halftensor,"index operation supported torch.halftensor feature currently, indexing torch.halftensor like half tensor indices gives runtime error. runtimeerror index implemented 'torch.halftensor'. so, requesting indexing method. motivation would great able index half tensors. current use case large scale similarity metric computations embeddings gpu figured could use float16 reduce memory usage. pitch indexing method added halftensor objects. alternatives use case alternative chunk tensors appropriately similarity computations. works well, imagine use cases indexing halftensor objects needed."
pytorch,2404,"seem issues corner cases fails 2 1 dimensional resp. words, unnecessary trailing causes failures, numpy. mwe https gist.github.com arunmallya 23457765e37faf8932da9276e2b449a8",0,advanced indexing issues,"advanced indexing issues seem issues corner cases fails 2 1 dimensional resp. words, unnecessary trailing causes failures, numpy. mwe https gist.github.com arunmallya 23457765e37faf8932da9276e2b449a8"
pytorch,27844,bug treats tag branch names file paths tag branch path separator e.g. linux e.g. branch torch.hub.list moabitcoin ig65m pytorch issues 4 torch hub integration fails e.g. see reproduce steps reproduce behavior 1. add simple hubconf.py repository 1. create tag branch path separator name 1. use torch hub functionality tag branch expected behavior works arbitrary git tags branches environment env additional context workaround create tag branch alias without path separator it. cc ailzhang,0,torch.hub handle tags branches path separator e.g.,torch.hub handle tags branches path separator e.g. bug treats tag branch names file paths tag branch path separator e.g. linux e.g. branch torch.hub.list moabitcoin ig65m pytorch issues 4 torch hub integration fails e.g. see reproduce steps reproduce behavior 1. add simple hubconf.py repository 1. create tag branch path separator name 1. use torch hub functionality tag branch expected behavior works arbitrary git tags branches environment env additional context workaround create tag branch alias without path separator it. cc ailzhang
pytorch,8109,"issue description indexing part tensor, entire original tensor saved. code example expect divided tensors stored. 100kb however, actually stored original matrix size. approximately 12mb system info pytorch conda windows 10 0.4.0 python 3.6 cuda 9.0 nvidia geforce 1070",0,sometimes torch.save method work properly.,"sometimes torch.save method work properly. issue description indexing part tensor, entire original tensor saved. code example expect divided tensors stored. 100kb however, actually stored original matrix size. approximately 12mb system info pytorch conda windows 10 0.4.0 python 3.6 cuda 9.0 nvidia geforce 1070"
pytorch,5553,"ran error parameter broadcast step . sounds like this, precise . 2332 possibly related.",0,nccl error 1 using torch.nn.dataparallel,"nccl error 1 using torch.nn.dataparallel ran error parameter broadcast step . sounds like this, precise . 2332 possibly related."
pytorch,10857,"question would like help support, please ask forums https discuss.pytorch.org . submitting feature request, please preface title feature request . submitting bug report, please fill following details. issue description code example system info please copy paste output environment collection script https raw.githubusercontent.com pytorch pytorch master torch utils collect env.py fill checklist manually . get script run pytorch caffe2 pytorch installed pytorch conda, pip, source source build command used compiling source python setup.py build os centos7 pytorch version master branch ca567862 python version 3.7.0 cuda cudnn version 9.2 7.2.1 gpu models configuration nvidia gtx1070 gcc version compiling source 4.8.5 cmake version 3.12.0 versions relevant libraries",0,got error ninja error loading 'build build.global.ninja' file directory,"got error ninja error loading 'build build.global.ninja' file directory question would like help support, please ask forums https discuss.pytorch.org . submitting feature request, please preface title feature request . submitting bug report, please fill following details. issue description code example system info please copy paste output environment collection script https raw.githubusercontent.com pytorch pytorch master torch utils collect env.py fill checklist manually . get script run pytorch caffe2 pytorch installed pytorch conda, pip, source source build command used compiling source python setup.py build os centos7 pytorch version master branch ca567862 python version 3.7.0 cuda cudnn version 9.2 7.2.1 gpu models configuration nvidia gtx1070 gcc version compiling source 4.8.5 cmake version 3.12.0 versions relevant libraries"
pytorch,11482,issue description got problem tried download mnist dataset. tested twice pytorch installed using pip pytorch compiled source. code example system info pytorch version 0.3.1 debug build cuda used build pytorch none os mac osx 10.13.6 gcc version could collect cmake version version 3.12.1 python version 3.6 cuda available cuda runtime version cuda gpu models configuration cuda nvidia driver version cuda cudnn version cuda versions relevant libraries pip could collect conda could collect,0,error 3 decompressing data invalid block type,error 3 decompressing data invalid block type issue description got problem tried download mnist dataset. tested twice pytorch installed using pip pytorch compiled source. code example system info pytorch version 0.3.1 debug build cuda used build pytorch none os mac osx 10.13.6 gcc version could collect cmake version version 3.12.1 python version 3.6 cuda available cuda runtime version cuda gpu models configuration cuda nvidia driver version cuda cudnn version cuda versions relevant libraries pip could collect conda could collect
pytorch,9264,encountered segfault running dilation pytorch 0.4.0 cpu specific tensor sizes. code reproduce behavior stack trace gdb environment information thanks help!,0,dilated conv3d segfaults cpu,dilated conv3d segfaults cpu encountered segfault running dilation pytorch 0.4.0 cpu specific tensor sizes. code reproduce behavior stack trace gdb environment information thanks help!
pytorch,367,nan,0,support dilation conv1d conv3d,support dilation conv1d conv3d nan
pytorch,10435,"issue description compile error using https github.com pytorch pytorch blob ab6afc2b238deb9e3b731399a367385518b788e5 modules rocksdb rocksdb.cc l70 system info pytorch caffe2 caffe2 installed pytorch conda, pip, source source build command used compiling source cmake ninja os centos pytorch version master gcc version compiling source 6.3.1 cmake version 3.12",0,caffe2 bug error iso c forbids declaration disable copy assign type,"caffe2 bug error iso c forbids declaration disable copy assign type issue description compile error using https github.com pytorch pytorch blob ab6afc2b238deb9e3b731399a367385518b788e5 modules rocksdb rocksdb.cc l70 system info pytorch caffe2 caffe2 installed pytorch conda, pip, source source build command used compiling source cmake ninja os centos pytorch version master gcc version compiling source 6.3.1 cmake version 3.12"
pytorch,7989,"current documentation https pytorch.org docs stable tensors.html?highlight clone torch.tensor.clone returns copy tensor. copy size data type . documentation fails elucidate fact gradient propagating cloned tensor propagate original tensor. critical functionality clone method called copy , begging newcomers make hard find mistakes.",0,documentation request 'clone' needs better documentation,"documentation request 'clone' needs better documentation current documentation https pytorch.org docs stable tensors.html?highlight clone torch.tensor.clone returns copy tensor. copy size data type . documentation fails elucidate fact gradient propagating cloned tensor propagate original tensor. critical functionality clone method called copy , begging newcomers make hard find mistakes."
pytorch,17029,bug seeing floating point exception nightly containers v1.0.1 container. believe mkl dnn patch needs master? soumith,0,floating point exception pytorch nightlies,floating point exception pytorch nightlies bug seeing floating point exception nightly containers v1.0.1 container. believe mkl dnn patch needs master? soumith
pytorch,28201,"feature improve performance groupnorm operator. motivation similar https github.com pytorch pytorch issues 27633, current groupnorm implementation reshaping input batchnorm get moments input, using addcmul affine. implementation inefficient cpu cuda. performance benchmark input shape 128, 256, 28, 28 , num groups 32 shown below. performance benchmark input shape 256, 512, 56, 56 , num groups 32 shown below. see cpu gpu version groupnorm, using batchnorm addcmul make things slow especially backward pass. actually cpu side, since batchnorm inference affine function fused conv, makes groupnorm slow using batchnorm cpu inference. pitch implement optimized version groupnorm fused everything together. alternatives additional context cc dzhulgakov ngimel ppwwyyxx",0,optimize groupnorm pytorch,"optimize groupnorm pytorch feature improve performance groupnorm operator. motivation similar https github.com pytorch pytorch issues 27633, current groupnorm implementation reshaping input batchnorm get moments input, using addcmul affine. implementation inefficient cpu cuda. performance benchmark input shape 128, 256, 28, 28 , num groups 32 shown below. performance benchmark input shape 256, 512, 56, 56 , num groups 32 shown below. see cpu gpu version groupnorm, using batchnorm addcmul make things slow especially backward pass. actually cpu side, since batchnorm inference affine function fused conv, makes groupnorm slow using batchnorm cpu inference. pitch implement optimized version groupnorm fused everything together. alternatives additional context cc dzhulgakov ngimel ppwwyyxx"
pytorch,4336,"hi, build cpu version source, met following issue. using python 3.5, ubuntu 16.04. anyone know fix it? thanks lot! nnp convolution kernel gradient' .. libaten.so.1 undefined reference pthreadpool create' .. libaten.so.1 undefined reference nnp convolution inference' .. libaten.so.1 undefined reference nnp convolution kernel gradient' .. libaten.so.1 undefined reference pthreadpool create' .. libaten.so.1 undefined reference nnp convolution inference' .. libaten.so.1 undefined reference nnp convolution kernel gradient' .. libaten.so.1 undefined reference pthreadpool create' .. libaten.so.1 undefined reference nnp convolution inference' .. libaten.so.1 undefined reference nnp convolution kernel gradient' .. libaten.so.1 undefined reference pthreadpool create' .. libaten.so.1 undefined reference nnp convolution inference' .. libaten.so.1 undefined reference",0,linking error .. libaten.so.1 undefined reference functions nnpack libs...,"linking error .. libaten.so.1 undefined reference functions nnpack libs... hi, build cpu version source, met following issue. using python 3.5, ubuntu 16.04. anyone know fix it? thanks lot! nnp convolution kernel gradient' .. libaten.so.1 undefined reference pthreadpool create' .. libaten.so.1 undefined reference nnp convolution inference' .. libaten.so.1 undefined reference nnp convolution kernel gradient' .. libaten.so.1 undefined reference pthreadpool create' .. libaten.so.1 undefined reference nnp convolution inference' .. libaten.so.1 undefined reference nnp convolution kernel gradient' .. libaten.so.1 undefined reference pthreadpool create' .. libaten.so.1 undefined reference nnp convolution inference' .. libaten.so.1 undefined reference nnp convolution kernel gradient' .. libaten.so.1 undefined reference pthreadpool create' .. libaten.so.1 undefined reference nnp convolution inference' .. libaten.so.1 undefined reference"
pytorch,16990,"https github.com pytorch pytorch blob 4292d13240e23a4a343b4ccb153214ab11c8d255 aten src aten native quantizedlinear.cpp l39 l40 here, e.g., contiguous, temporary tensor freed making pointers invalid. cc jamesr66a",0,unsafe .data fbgemm integration,"unsafe .data fbgemm integration https github.com pytorch pytorch blob 4292d13240e23a4a343b4ccb153214ab11c8d255 aten src aten native quantizedlinear.cpp l39 l40 here, e.g., contiguous, temporary tensor freed making pointers invalid. cc jamesr66a"
pytorch,6287,"title says, might nice able put aten tensors gpus easily c .",0,.todevice aten tensors,".todevice aten tensors title says, might nice able put aten tensors gpus easily c ."
pytorch,3214,"install pytorch using pip, found lr scheduler.py , line missing https github.com pytorch pytorch blob d89d9d74bdd0b64c119980003aa29195a61f93a9 torch optim init .py l18 thus user manually add use torch.optim.lr scheduler. hope updated next version.",0,cannot directly use lr scheduler current pip install package,"cannot directly use lr scheduler current pip install package install pytorch using pip, found lr scheduler.py , line missing https github.com pytorch pytorch blob d89d9d74bdd0b64c119980003aa29195a61f93a9 torch optim init .py l18 thus user manually add use torch.optim.lr scheduler. hope updated next version."
pytorch,8330,really reason work internally pieces implemented already.,0,x.copy work sparse x,x.copy work sparse x really reason work internally pieces implemented already.
pytorch,17489,"bug hi, can't build windows debug cpu version, build fails linking. using latest source code. end post exact commands execute. really libtorch build, care python, understand whole thing, feel obliged build all. thank you, slawek 1. first observation, cmakelists.txt contains set cmake cxx flags debug cmake cxx flags debug fno omit frame pointer o0 set cmake linker flags debug cmake static linker flags debug fno omit frame pointer o0 causes endless warnings unknown options. compilation appears successful 2000 files failures happen linking 2. 3. then, another failure now, downloaded copied python37 d.lib c local anaconda3 envs pytorchdebug libs python37 d.dll c local anaconda3 envs pytorchdebug, help. environment pytorch version 1, latest code os windows 10 build commands call c local anaconda3 scripts activate pytorchdebug rem done conda install numpy pyyaml mkl mkl include setuptools cmake cffi typing rem done before, f progs2 pytorchdebug git clone recursive https github.com pytorch pytorch f cd progs2 pytorchdebug pytorch set vs150comntools c program files x86 microsoft visual studio 2017 community vc auxiliary build set cmake generator visual studio 15 2017 win64 set distutils use sdk 1 set cuda 1 set debug 1 call vs150comntools vcvarsall.bat x64 vcvars ver 14.11 python setup.py install python version 3.7",0,windows cpu debug build fails linking stage,"windows cpu debug build fails linking stage bug hi, can't build windows debug cpu version, build fails linking. using latest source code. end post exact commands execute. really libtorch build, care python, understand whole thing, feel obliged build all. thank you, slawek 1. first observation, cmakelists.txt contains set cmake cxx flags debug cmake cxx flags debug fno omit frame pointer o0 set cmake linker flags debug cmake static linker flags debug fno omit frame pointer o0 causes endless warnings unknown options. compilation appears successful 2000 files failures happen linking 2. 3. then, another failure now, downloaded copied python37 d.lib c local anaconda3 envs pytorchdebug libs python37 d.dll c local anaconda3 envs pytorchdebug, help. environment pytorch version 1, latest code os windows 10 build commands call c local anaconda3 scripts activate pytorchdebug rem done conda install numpy pyyaml mkl mkl include setuptools cmake cffi typing rem done before, f progs2 pytorchdebug git clone recursive https github.com pytorch pytorch f cd progs2 pytorchdebug pytorch set vs150comntools c program files x86 microsoft visual studio 2017 community vc auxiliary build set cmake generator visual studio 15 2017 win64 set distutils use sdk 1 set cuda 1 set debug 1 call vs150comntools vcvarsall.bat x64 vcvars ver 14.11 python setup.py install python version 3.7"
pytorch,28347,"bug using target tensor object, sometimes change dimensionality target tensor. creating 1,n tensor, sometimes calling changes n 1 vector. cuda cpu. reproduce output expected behavior change shape target tensor. environment pytorch 1.3, ubuntu 18.04lts, py 3.6.9 anaconda. detail additional context started way tensor wanted make sure whole thing happen gpu, first thing figured out. see another way works. seems like bug. cc ezyang gchanan zou3519 jerryzh168",0,arange sometimes changes dimensionality output tensor,"arange sometimes changes dimensionality output tensor bug using target tensor object, sometimes change dimensionality target tensor. creating 1,n tensor, sometimes calling changes n 1 vector. cuda cpu. reproduce output expected behavior change shape target tensor. environment pytorch 1.3, ubuntu 18.04lts, py 3.6.9 anaconda. detail additional context started way tensor wanted make sure whole thing happen gpu, first thing figured out. see another way works. seems like bug. cc ezyang gchanan zou3519 jerryzh168"
pytorch,2344,noticed tried use cosineembeddingloss model copied gpu. default assumption using cuda wrong way. thoughts?,0,cosineembeddingloss support cuda tensors?,cosineembeddingloss support cuda tensors? noticed tried use cosineembeddingloss model copied gpu. default assumption using cuda wrong way. thoughts?
pytorch,3491,"run codes using pytorch 0.2.0, python 3.5 error using cpu woks fine",0,runtimeerror cuda error 3 initialization error,"runtimeerror cuda error 3 initialization error run codes using pytorch 0.2.0, python 3.5 error using cpu woks fine"
pytorch,24612,porting th operators essential code simplicity performance reasons. porting guides q available umbrella issue 24507 feel free add vitalyfedyunin reviewer get prioritized review.,0,migrate polygamma polygamma th aten cuda,migrate polygamma polygamma th aten cuda porting th operators essential code simplicity performance reasons. porting guides q available umbrella issue 24507 feel free add vitalyfedyunin reviewer get prioritized review.
pytorch,13,,0,infinite recursion indexing variable,infinite recursion indexing variable 
pytorch,26834,parameters fakequantize function showing right.,0,fakequantize params rendering correctly,fakequantize params rendering correctly parameters fakequantize function showing right.
pytorch,6131,"aosokin reports https github.com pytorch pytorch issues 6068 issuecomment 377226963 might another related issue dependencies. import torch anaconda env pytorch uses mkl libmkl gf lp64.so installed anaconda root. consequently, root env different versions case root 2018.0.1 env 2018.0.2 1 getting error updating mkl anaconda root 2018.0.2 1 solved problem.",0,build picks wrong copy mkl multiple available,"build picks wrong copy mkl multiple available aosokin reports https github.com pytorch pytorch issues 6068 issuecomment 377226963 might another related issue dependencies. import torch anaconda env pytorch uses mkl libmkl gf lp64.so installed anaconda root. consequently, root env different versions case root 2018.0.1 env 2018.0.2 1 getting error updating mkl anaconda root 2018.0.2 1 solved problem."
pytorch,22549,"documentation minor minor detail reproducibility section docs faq, would add simple subsection snippet code show programmatically check running version pytorch. also encourage users take account heterogeneity pytorch versions code. way, simple regex enough assuming version numbering change .",0,programmatically check pytorch version,"programmatically check pytorch version documentation minor minor detail reproducibility section docs faq, would add simple subsection snippet code show programmatically check running version pytorch. also encourage users take account heterogeneity pytorch versions code. way, simple regex enough assuming version numbering change ."
pytorch,19366,"bug export module onnx, attribute written lower case. several backends assume strings case sensitive, blocking issue runtime. open issue https github.com onnx onnx issues 1934 onnx repo make explicit spec. pytorch's onnx exporter also consider writing strings correct case attribute. reproduce steps reproduce behavior 1. create simple model module. 1. export model onnx using 1. check attribute case string. example, string 'tanh', ideally 'tanh'. expected behavior attribute value match case specified onnx spec. environment seen environments.",0,onnx rnn activation function exported onnx wrong string case,"onnx rnn activation function exported onnx wrong string case bug export module onnx, attribute written lower case. several backends assume strings case sensitive, blocking issue runtime. open issue https github.com onnx onnx issues 1934 onnx repo make explicit spec. pytorch's onnx exporter also consider writing strings correct case attribute. reproduce steps reproduce behavior 1. create simple model module. 1. export model onnx using 1. check attribute case string. example, string 'tanh', ideally 'tanh'. expected behavior attribute value match case specified onnx spec. environment seen environments."
pytorch,685,"able reproduce simple example now, try make one later. basically facing issue code variable , solve issue. however, solves it, seems weird me..",0,detach working properly stochastic variables,"detach working properly stochastic variables able reproduce simple example now, try make one later. basically facing issue code variable , solve issue. however, solves it, seems weird me.."
pytorch,3388,nan,0,recipe target 'cmakefiles thd.dir base tensordescriptor.cpp.o' failed,recipe target 'cmakefiles thd.dir base tensordescriptor.cpp.o' failed nan
pytorch,23649,run need root access install final package. install user home mean user packages pip ?,0,installation non root access,installation non root access run need root access install final package. install user home mean user packages pip ?
pytorch,12241,add add add autograd support,0,torch.sum sparse tensor,torch.sum sparse tensor add add add autograd support
pytorch,26922,"context https github.com pytorch pytorch pull 25527 files r328763171 distautogradcontainer's api currently exposes mutable references distautogradcontext. could lead 'use free' scenarios distautogradcontainer cleans context, another thread continues use reference. modify api ensure returns shared ptrs user avoid sort ownership issues. cc ezyang gchanan zou3519 pietern mrshenli pritamdamania87 zhaojuanmao satgera rohan varma gqchen aazzolini xush6528",0,distautogradcontainer expose references distautogradcontext,"distautogradcontainer expose references distautogradcontext context https github.com pytorch pytorch pull 25527 files r328763171 distautogradcontainer's api currently exposes mutable references distautogradcontext. could lead 'use free' scenarios distautogradcontainer cleans context, another thread continues use reference. modify api ensure returns shared ptrs user avoid sort ownership issues. cc ezyang gchanan zou3519 pietern mrshenli pritamdamania87 zhaojuanmao satgera rohan varma gqchen aazzolini xush6528"
pytorch,24711,porting th operators essential code simplicity performance reasons. porting guides q available umbrella issue 24507 feel free add vitalyfedyunin reviewer get prioritized review.,0,migrate hardtanh backward th aten cpu,migrate hardtanh backward th aten cpu porting th operators essential code simplicity performance reasons. porting guides q available umbrella issue 24507 feel free add vitalyfedyunin reviewer get prioritized review.
pytorch,29293,"would like know build pytorch sm 70 sm 75, automatically use tensor cores explicitly specify something compilation run. comment?",0,using tensor cores,"using tensor cores would like know build pytorch sm 70 sm 75, automatically use tensor cores explicitly specify something compilation run. comment?"
pytorch,13806,"compile pytorch source 1.0.0a0 54e8623 . attempt load data get crash occurs using cifar10 dataset here, problem occur mnist. system macos 10.12.6. could problem here?",0,omp error doubly linked openmp loading cifar10 dataset,"omp error doubly linked openmp loading cifar10 dataset compile pytorch source 1.0.0a0 54e8623 . attempt load data get crash occurs using cifar10 dataset here, problem occur mnist. system macos 10.12.6. could problem here?"
pytorch,14057,"bug pickling object, python 3.5 reports obscure error can't pickle int objects . reproduce steps reproduce behavior expected behavior python 3.6 one pickle torch dtypes successfully. environment",0,unable pickle torch dtype objects python 3.5,"unable pickle torch dtype objects python 3.5 bug pickling object, python 3.5 reports obscure error can't pickle int objects . reproduce steps reproduce behavior expected behavior python 3.6 one pickle torch dtypes successfully. environment"
pytorch,22616,"bug pytorch , parameter sparse tensor, fails. mini snipped throws following error pytorch error. reproduce steps reproduce behavior expected behavior loads sparse tensor state dict. environment virtual environment torch snipped throws error",0,error loading sparse tensor parameter state dict pytorch 1.1.0,"error loading sparse tensor parameter state dict pytorch 1.1.0 bug pytorch , parameter sparse tensor, fails. mini snipped throws following error pytorch error. reproduce steps reproduce behavior expected behavior loads sparse tensor state dict. environment virtual environment torch snipped throws error"
pytorch,14078,"bug pytorch fails finish building, possible bug see . reproduce environment pytorch version e.g., 1.0 master branch 1.0 os e.g., linux debian stretch installed pytorch , , source source build command used compiling source see python version 3.6 cuda cudnn version n gpu models configuration n relevant information n additional context",0,build failing torch lib c10d processgroupmpi.cpp,"build failing torch lib c10d processgroupmpi.cpp bug pytorch fails finish building, possible bug see . reproduce environment pytorch version e.g., 1.0 master branch 1.0 os e.g., linux debian stretch installed pytorch , , source source build command used compiling source see python version 3.6 cuda cudnn version n gpu models configuration n relevant information n additional context"
pytorch,30989,"following tutorial, combine model parallel ddp together last layer parameter really large model parallel work like code source https github.com bindog pytorch model parallel blob 8d1c0b91d4eb75c861059f4940159b06574189df model.py l39 . however, stuck gpu list first ddp 0, 1, 2, 3 gpu list second ddp 4, 5, 6, 7 modify source code make sure chunk follow parameter gpu list support output multiple device, reorganize last layer? cc pietern mrshenli pritamdamania87 zhaojuanmao satgera rohan varma gqchen aazzolini xush6528",0,distributed data parallel support model parallel output model different devices?,"distributed data parallel support model parallel output model different devices? following tutorial, combine model parallel ddp together last layer parameter really large model parallel work like code source https github.com bindog pytorch model parallel blob 8d1c0b91d4eb75c861059f4940159b06574189df model.py l39 . however, stuck gpu list first ddp 0, 1, 2, 3 gpu list second ddp 4, 5, 6, 7 modify source code make sure chunk follow parameter gpu list support output multiple device, reorganize last layer? cc pietern mrshenli pritamdamania87 zhaojuanmao satgera rohan varma gqchen aazzolini xush6528"
pytorch,24304,"bug right returns tensor scale, zero point , think maybe return values directly, two tensors? https github.com pytorch pytorch blob master torch quantization observer.py l65",0,minmax observer return tensor scale tensor zero point?,"minmax observer return tensor scale tensor zero point? bug right returns tensor scale, zero point , think maybe return values directly, two tensors? https github.com pytorch pytorch blob master torch quantization observer.py l65"
pytorch,15166,"pytorch1.0 bug torch.nn.functional.multi margin loss example import torch import torch.nn.functional f 1.,2.,3.,4. b 2 input torch.tensor a,requires grad true target torch.tensor b k torch.tensor c output f.multi margin loss input,target,p 1.,margin 1.,reduce false,size average false print output tensor 0.5000 , grad fn",0,bug torch.nn.functional.multi margin loss,"bug torch.nn.functional.multi margin loss pytorch1.0 bug torch.nn.functional.multi margin loss example import torch import torch.nn.functional f 1.,2.,3.,4. b 2 input torch.tensor a,requires grad true target torch.tensor b k torch.tensor c output f.multi margin loss input,target,p 1.,margin 1.,reduce false,size average false print output tensor 0.5000 , grad fn"
pytorch,27450,cc jerryzh168 jianyuh dzhulgakov raghuramank100,0,formatting issue dynamic quantization function,formatting issue dynamic quantization function cc jerryzh168 jianyuh dzhulgakov raghuramank100
pytorch,5463,"bad build reproducibility, since file paths may change systems. file paths come sourcelocation started exporting c7cb6a795e550ef2373d8127e120744db16b61cf cc jamesr66a",0,onnx export protobufs leak file paths file system,"onnx export protobufs leak file paths file system bad build reproducibility, since file paths may change systems. file paths come sourcelocation started exporting c7cb6a795e550ef2373d8127e120744db16b61cf cc jamesr66a"
pytorch,827,"hi guys, currently trying call backward network trained fp16 linear output results error easily fixed replacing calls non stateless versions, done https github.com ajbrock pytorch blob master torch nn functions linear.py . sure replacement line pytorch style plans implement stateless methods halftensors would render irrelevant , submit pr needbe. best, andy",0,halftensor training needs non stateless method f.linear,"halftensor training needs non stateless method f.linear hi guys, currently trying call backward network trained fp16 linear output results error easily fixed replacing calls non stateless versions, done https github.com ajbrock pytorch blob master torch nn functions linear.py . sure replacement line pytorch style plans implement stateless methods halftensors would render irrelevant , submit pr needbe. best, andy"
pytorch,5285,"os ubuntu 16.04 pytorch version 0.3.1b0 2b47480 installed pytorch conda, pip, source source python version 3.6.4 cuda cudnn version cuda 9.0 cudnn 7 gpu models configuration gtx 1080ti well gtx 1070 gcc version compiling source gcc ubuntu 5.4.0 6ubuntu1 16.04.6 5.4.0 20160609 hello, implementing srgan paper https arxiv.org abs 1609.04802 ran memory leak issue inference cpu. issue easily reproductible side implementation https github.com ekami torchlight tree showcase memory leak . clone repository , cd folder run script run inference cpu. get memory leak following message run script defaults cuda memory leak vanishes. exact line cause memory leak one https github.com ekami torchlight blob showcase memory leak torchlight nn models srgan.py l42 . remove line get execute script cpu poof memory leak vanishes. tried 2 different computer software installed hardware one amd fx 8350 gtx 1070 second intel i7 7700k gtx 1080ti get memory leak machines.",0,bug memory leak convnet cpu,"bug memory leak convnet cpu os ubuntu 16.04 pytorch version 0.3.1b0 2b47480 installed pytorch conda, pip, source source python version 3.6.4 cuda cudnn version cuda 9.0 cudnn 7 gpu models configuration gtx 1080ti well gtx 1070 gcc version compiling source gcc ubuntu 5.4.0 6ubuntu1 16.04.6 5.4.0 20160609 hello, implementing srgan paper https arxiv.org abs 1609.04802 ran memory leak issue inference cpu. issue easily reproductible side implementation https github.com ekami torchlight tree showcase memory leak . clone repository , cd folder run script run inference cpu. get memory leak following message run script defaults cuda memory leak vanishes. exact line cause memory leak one https github.com ekami torchlight blob showcase memory leak torchlight nn models srgan.py l42 . remove line get execute script cpu poof memory leak vanishes. tried 2 different computer software installed hardware one amd fx 8350 gtx 1070 second intel i7 7700k gtx 1080ti get memory leak machines."
pytorch,2670,"bug opennmt first created attribute tried add buffer. behavior cause exist, cast . think behavior counter intuitive, throw error. jianyuzhan",0,adding attribute buffer throw error,"adding attribute buffer throw error bug opennmt first created attribute tried add buffer. behavior cause exist, cast . think behavior counter intuitive, throw error. jianyuzhan"
pytorch,17615,bug assigning long tensor float tensor silently fails. reproduce expected behavior tensor correctly copied error message raised. environment,0,long tensor float tensor slicing assignment fails,long tensor float tensor slicing assignment fails bug assigning long tensor float tensor silently fails. reproduce expected behavior tensor correctly copied error message raised. environment
pytorch,18862,"bug pytorch . gpu times reported p100. case matmul uses 12 gb memory use 3 mb. i.e. using 4096x memory necessary note equivalent following memory efficient operation b also equivalent following memory efficient faster, may require copy output may batched column major without extra work c gpu, takes 125 ms uses 12 gb memory, b takes 22 ms, c takes 1 ms. originally https discuss.pytorch.org unexpected huge memory cost matmul 41642 4 see also https github.com pytorch pytorch issues 13222 may related believe problem unnecessary contiguous call https github.com pytorch pytorch blob 15b318de840de61e2e789c013e34d23819715090 aten src aten native linearalgebra.cpp l460 l461 instead using may possible use . might achieve performance b .",0,matmul uses much memory batched cases,"matmul uses much memory batched cases bug pytorch . gpu times reported p100. case matmul uses 12 gb memory use 3 mb. i.e. using 4096x memory necessary note equivalent following memory efficient operation b also equivalent following memory efficient faster, may require copy output may batched column major without extra work c gpu, takes 125 ms uses 12 gb memory, b takes 22 ms, c takes 1 ms. originally https discuss.pytorch.org unexpected huge memory cost matmul 41642 4 see also https github.com pytorch pytorch issues 13222 may related believe problem unnecessary contiguous call https github.com pytorch pytorch blob 15b318de840de61e2e789c013e34d23819715090 aten src aten native linearalgebra.cpp l460 l461 instead using may possible use . might achieve performance b ."
pytorch,17355,"bug missing many global operators , , . reproduce expected behavior global ops contain valid s. environment pytorch version 1.0.1.post2 debug build cuda used build pytorch 9.0.176 os ubuntu 16.04.5 lts gcc version ubuntu 5.4.0 6ubuntu1 16.04.11 5.4.0 20160609 cmake version version 3.9.4 python version 3.6 cuda available yes cuda runtime version could collect gpu models configuration gpu 0 geforce gtx titan black gpu 1 geforce gtx titan black gpu 2 geforce gtx titan black gpu 3 geforce gtx titan black nvidia driver version 390.30 cudnn version usr lib x86 64 linux gnu libcudnn.so.7.1.1",0,python doc global op carried source,"python doc global op carried source bug missing many global operators , , . reproduce expected behavior global ops contain valid s. environment pytorch version 1.0.1.post2 debug build cuda used build pytorch 9.0.176 os ubuntu 16.04.5 lts gcc version ubuntu 5.4.0 6ubuntu1 16.04.11 5.4.0 20160609 cmake version version 3.9.4 python version 3.6 cuda available yes cuda runtime version could collect gpu models configuration gpu 0 geforce gtx titan black gpu 1 geforce gtx titan black gpu 2 geforce gtx titan black gpu 3 geforce gtx titan black nvidia driver version 390.30 cudnn version usr lib x86 64 linux gnu libcudnn.so.7.1.1"
pytorch,23925,"bug , grid point falls exactly high boundary image , gradient based border padding scheme, give either gradient inside boundary, zero outside boundary either could valid, since non differentiable point . instead, gradient currently based zero padding image, gives wacky results. problem occurs 2d cpu. reflection modes cuda version 3d cpu version also problem, arguably worse, since incorrect gradient also negated. furthermore, inconsistency behavior cpu cuda kernels. example notice wacky last row last column. gradient currently calculated image zero padded. result ideally look like finds gradient using bounds neighbor. less ideal, still palatable result would finds gradient using bounds, border padded neighbor. reflection mode cpu instance, try using commands, gives exact problematic result. using reflection mode cuda, however, well 3d grid sample cpu problematic gradients negated! also problematic, course, even mismatch cpu cuda behaviors. mode, think makes sense set gradient cases zero, since sort apex symmetric hill. setting take gradient one side might also acceptable practical purposes. mode, contrast, think makes sense always take non zero gradient inner side, since outer side gradient zero effectively stop training see related discussion 7002 7049 . pytorch version tested commit https github.com pytorch pytorch commit 0539462ca2966aa29657b58aeb17a85c21524d31",0,'border' 'reflection modes grid sample incorrect gradients border,"'border' 'reflection modes grid sample incorrect gradients border bug , grid point falls exactly high boundary image , gradient based border padding scheme, give either gradient inside boundary, zero outside boundary either could valid, since non differentiable point . instead, gradient currently based zero padding image, gives wacky results. problem occurs 2d cpu. reflection modes cuda version 3d cpu version also problem, arguably worse, since incorrect gradient also negated. furthermore, inconsistency behavior cpu cuda kernels. example notice wacky last row last column. gradient currently calculated image zero padded. result ideally look like finds gradient using bounds neighbor. less ideal, still palatable result would finds gradient using bounds, border padded neighbor. reflection mode cpu instance, try using commands, gives exact problematic result. using reflection mode cuda, however, well 3d grid sample cpu problematic gradients negated! also problematic, course, even mismatch cpu cuda behaviors. mode, think makes sense set gradient cases zero, since sort apex symmetric hill. setting take gradient one side might also acceptable practical purposes. mode, contrast, think makes sense always take non zero gradient inner side, since outer side gradient zero effectively stop training see related discussion 7002 7049 . pytorch version tested commit https github.com pytorch pytorch commit 0539462ca2966aa29657b58aeb17a85c21524d31"
pytorch,4651,"launch distributed pytorch code cluster two nodes? suppose node one gpu. def init processes rank, size, fn, backend 'gloo' dist.init process group backend,init method 'tcp 172.16.1.186 2222', rank rank,world size size fn rank, size name main size 2 processes rank range size p process target init processes, args rank, size, run p.start processes.append p p processes p.join part code.i run it.however,the result run one node qsub run command two nodes. part pbs command nodes gpu06 ppn 24 gpu07 ppn 24. run command python train dist.py. something wrong?thanks help.",0,distributed pytorch cluster,"distributed pytorch cluster launch distributed pytorch code cluster two nodes? suppose node one gpu. def init processes rank, size, fn, backend 'gloo' dist.init process group backend,init method 'tcp 172.16.1.186 2222', rank rank,world size size fn rank, size name main size 2 processes rank range size p process target init processes, args rank, size, run p.start processes.append p p processes p.join part code.i run it.however,the result run one node qsub run command two nodes. part pbs command nodes gpu06 ppn 24 gpu07 ppn 24. run command python train dist.py. something wrong?thanks help."
pytorch,8554,"tried rebase rebuild torch source today, got error see full gist https gist.github.com elanmart 8fca075c596378bd785abd8079567684 . future reference, getting rid old install updating via seems solved issue. cc soumith cpuhrsch",0,pytorch build vmslog2 declared scope,"pytorch build vmslog2 declared scope tried rebase rebuild torch source today, got error see full gist https gist.github.com elanmart 8fca075c596378bd785abd8079567684 . future reference, getting rid old install updating via seems solved issue. cc soumith cpuhrsch"
pytorch,3243,"build pytorch source following readme directions 1 , get error machine details nvcc release 7.5, v7.5.17 gcc version 4.8.3 20140911 red hat 4.8.3 9 gcc gcc version 4.8.3 20140911 red hat 4.8.3 9 gcc python 3.6.1 anaconda os red hat sure cudnn. using amazon's ec2 deep learning ami, say cudnn drivers https aws.amazon.com amazon ai amis 1 https github.com pytorch pytorch source",0,compile error t0 declared scope,"compile error t0 declared scope build pytorch source following readme directions 1 , get error machine details nvcc release 7.5, v7.5.17 gcc version 4.8.3 20140911 red hat 4.8.3 9 gcc gcc version 4.8.3 20140911 red hat 4.8.3 9 gcc python 3.6.1 anaconda os red hat sure cudnn. using amazon's ec2 deep learning ami, say cudnn drivers https aws.amazon.com amazon ai amis 1 https github.com pytorch pytorch source"
pytorch,219,"right now, conv2d wrapped so.",0,conv2dtranspose needs cudnn integration,"conv2dtranspose needs cudnn integration right now, conv2d wrapped so."
pytorch,3238,"seems erroneous dimension calculation function uses private function. input 1d tensor, implicit dimension computed 1, problem since invalid 1d tensor. minimal reproducible example produces error running pytorch installed source, commit dc6510f7",0,dimension issue softmax related functions 1d tensors,"dimension issue softmax related functions 1d tensors seems erroneous dimension calculation function uses private function. input 1d tensor, implicit dimension computed 1, problem since invalid 1d tensor. minimal reproducible example produces error running pytorch installed source, commit dc6510f7"
pytorch,907,"two problems way manage cudnnhandles handles.cpp https github.com pytorch pytorch blob master torch csrc cudnn handles.cpp 1. sometimes see sigsegv exit handles destructed 2. work non default streams tried making handles table thread local, causes slow downs deadlocks handles destructed every forward pass data parallel module. figure better solution.",0,fix cudnnhandle handles.cpp,"fix cudnnhandle handles.cpp two problems way manage cudnnhandles handles.cpp https github.com pytorch pytorch blob master torch csrc cudnn handles.cpp 1. sometimes see sigsegv exit handles destructed 2. work non default streams tried making handles table thread local, causes slow downs deadlocks handles destructed every forward pass data parallel module. figure better solution."
pytorch,7373,"question would like help support, please ask forums https discuss.pytorch.org . submitting feature request, please preface title feature request . submitting bug report, please fill following details. issue description provide short description. code example please try provide minimal example repro bug. error messages stack traces also helpful. system info please copy paste output environment collection script https raw.githubusercontent.com pytorch pytorch master torch utils collect env.py fill checklist manually . get script run pytorch caffe2 installed pytorch conda, pip, source build command used compiling source os pytorch version python version cuda cudnn version gpu models configuration gcc version compiling source cmake version versions relevant libraries",0,help delete issue,"help delete issue question would like help support, please ask forums https discuss.pytorch.org . submitting feature request, please preface title feature request . submitting bug report, please fill following details. issue description provide short description. code example please try provide minimal example repro bug. error messages stack traces also helpful. system info please copy paste output environment collection script https raw.githubusercontent.com pytorch pytorch master torch utils collect env.py fill checklist manually . get script run pytorch caffe2 installed pytorch conda, pip, source build command used compiling source os pytorch version python version cuda cudnn version gpu models configuration gcc version compiling source cmake version versions relevant libraries"
pytorch,11751,"issue description found several errors autograd segfault using hook e.g. non reachable tensor, whose grad implicitely calculated output function gradient graph independent backprop root tensor. see code traceback below. hook called non reachable tensor output index operation e.g. root depends complete . see code below. issue related others, encountered run, want mention here, too. hook called e.g. , argument tensor values, actually used value required therefore modifiable. see code below. traceback segfault notice lines . think, actual source problem is, https github.com pytorch pytorch blob 6f6b03566ba3c4828f6ee87a772f9d161be0bae7 torch csrc autograd engine.cpp l447 line, inputs initialized variables values, fallbacks function overrides them. wrong? really would like fix this, honest, sure enough expertise right. open issue others. hope, helps. code example a0grad 0a1a0a.grada0grad none 0a1a0segmentation faultgradnone0.backward .grad system info pytorch caffe2 pytorch installed pytorch conda, pip, source tested two versions errors current master source, v0.4.1 via pip build command used compiling source os linux mint 18.3 sylvia pytorch version tested two versions errors current master, v0.4.1 python version 3.6.6 cuda cudnn version none gpu models configuration cuda gcc version compiling source ubuntu 5.4.0 6ubuntu1 16.04.10 5.4.0 20160609 cmake version version 3.12.0 versions relevant libraries pip 18.0",0,segfault autograd using hook,"segfault autograd using hook issue description found several errors autograd segfault using hook e.g. non reachable tensor, whose grad implicitely calculated output function gradient graph independent backprop root tensor. see code traceback below. hook called non reachable tensor output index operation e.g. root depends complete . see code below. issue related others, encountered run, want mention here, too. hook called e.g. , argument tensor values, actually used value required therefore modifiable. see code below. traceback segfault notice lines . think, actual source problem is, https github.com pytorch pytorch blob 6f6b03566ba3c4828f6ee87a772f9d161be0bae7 torch csrc autograd engine.cpp l447 line, inputs initialized variables values, fallbacks function overrides them. wrong? really would like fix this, honest, sure enough expertise right. open issue others. hope, helps. code example a0grad 0a1a0a.grada0grad none 0a1a0segmentation faultgradnone0.backward .grad system info pytorch caffe2 pytorch installed pytorch conda, pip, source tested two versions errors current master source, v0.4.1 via pip build command used compiling source os linux mint 18.3 sylvia pytorch version tested two versions errors current master, v0.4.1 python version 3.6.6 cuda cudnn version none gpu models configuration cuda gcc version compiling source ubuntu 5.4.0 6ubuntu1 16.04.10 5.4.0 20160609 cmake version version 3.12.0 versions relevant libraries pip 18.0"
pytorch,21761,,0,jit bad error message instantiating script class init,jit bad error message instantiating script class init 
pytorch,23616,"split whitespace, right splits outputs cc suo",0,jit string.split splits newlines,"jit string.split splits newlines split whitespace, right splits outputs cc suo"
pytorch,21680,"bug reproduce steps reproduce behavior 1.i updated pytorch version ctcuse pytorch nightly, train ,nn.ctcloss still zero,so,i would like ask version pytorch nightly solved problem 1. 1. expected behavior environment please copy paste output environment collection script https raw.githubusercontent.com pytorch pytorch master torch utils collect env.py fill checklist manually . get script run pytorch version e.g., 1.0 os e.g., linux installed pytorch , , source build command used compiling source python version cuda cudnn version gpu models configuration relevant information additional context",0,disable nondeterministic ctcloss cudnn,"disable nondeterministic ctcloss cudnn bug reproduce steps reproduce behavior 1.i updated pytorch version ctcuse pytorch nightly, train ,nn.ctcloss still zero,so,i would like ask version pytorch nightly solved problem 1. 1. expected behavior environment please copy paste output environment collection script https raw.githubusercontent.com pytorch pytorch master torch utils collect env.py fill checklist manually . get script run pytorch version e.g., 1.0 os e.g., linux installed pytorch , , source build command used compiling source python version cuda cudnn version gpu models configuration relevant information additional context"
pytorch,3093,"hi getting following error try build seems like flags added within past couple weeks, can't find anything googling errors. cudnnv5 cuda 8.0 ubuntu16.04 box. provide details needed. thanks",0,better error messages compiling cudnn v5 supported anymore,"better error messages compiling cudnn v5 supported anymore hi getting following error try build seems like flags added within past couple weeks, can't find anything googling errors. cudnnv5 cuda 8.0 ubuntu16.04 box. provide details needed. thanks"
pytorch,6285,"autogpu compiled cuda. instead, auto gpu.cpp file set macro compiling it... third party lib includes auto gpu.h without , try use it, things auto gpu.h wrapped compile work. solution see split .cpp cuda baked pytorch .so.",0,autogpu header relies cuda,"autogpu header relies cuda autogpu compiled cuda. instead, auto gpu.cpp file set macro compiling it... third party lib includes auto gpu.h without , try use it, things auto gpu.h wrapped compile work. solution see split .cpp cuda baked pytorch .so."
pytorch,4281,"looking make pull request bipolar activation functions https arxiv.org abs 1709.04054 . hope get feedback. trick make activation functions self centering. short relu keeps positive numbers, shifts post activation mean positive direction. however, every neuron instead keep negative numbers, cancel effect. half neurons , half . i.i.d. input vector, effect halve mean vector, post activation, . paper find empirically help learning rnns convnets. empirical results relus elus, would expect hold anything relu like. one way implement would put something like following nn functional.py also includes bipolar max pooling functions i.e. min pooling half inputs . 1 would interest pull request this? 2 comments implementation? course, add doc strings, also put something similar nn modules activation.py",0,implementation bipolar activation functions,"implementation bipolar activation functions looking make pull request bipolar activation functions https arxiv.org abs 1709.04054 . hope get feedback. trick make activation functions self centering. short relu keeps positive numbers, shifts post activation mean positive direction. however, every neuron instead keep negative numbers, cancel effect. half neurons , half . i.i.d. input vector, effect halve mean vector, post activation, . paper find empirically help learning rnns convnets. empirical results relus elus, would expect hold anything relu like. one way implement would put something like following nn functional.py also includes bipolar max pooling functions i.e. min pooling half inputs . 1 would interest pull request this? 2 comments implementation? course, add doc strings, also put something similar nn modules activation.py"
pytorch,6034,"pytorch 0.3 can't deal 0 dim can't find 0.4 u help me? pytorch github issues guidelines like limit issues bug reports feature requests. question would like help support, please visit forums https discuss.pytorch.org submitting feature request, please preface title feature request . submitting bug report, please include following information relevant os pytorch version installed pytorch conda, pip, source python version cuda cudnn version gpu models configuration gcc version compiling source addition, including following information also helpful us diagnose problem script reproduce bug. please try provide minimal test case possible. error messages stack traces bug context around trying",0,could get pytorch0.4 maskrcnn. want deal 0dim,"could get pytorch0.4 maskrcnn. want deal 0dim pytorch 0.3 can't deal 0 dim can't find 0.4 u help me? pytorch github issues guidelines like limit issues bug reports feature requests. question would like help support, please visit forums https discuss.pytorch.org submitting feature request, please preface title feature request . submitting bug report, please include following information relevant os pytorch version installed pytorch conda, pip, source python version cuda cudnn version gpu models configuration gcc version compiling source addition, including following information also helpful us diagnose problem script reproduce bug. please try provide minimal test case possible. error messages stack traces bug context around trying"
pytorch,20875,"bug per https pytorch.org docs stable nn.html torch.nn.batchnorm2d expect result batchnorm2d x mean scale sqrt var eps bias. however case. given already loaded batchnorm2d module bn0, following environment condapip , source pip python version 3.5 cuda cudnn version 9.2 7.1.3 gpu models configuration relevant information",0,batchnorm2d implementation returns different results expected,"batchnorm2d implementation returns different results expected bug per https pytorch.org docs stable nn.html torch.nn.batchnorm2d expect result batchnorm2d x mean scale sqrt var eps bias. however case. given already loaded batchnorm2d module bn0, following environment condapip , source pip python version 3.5 cuda cudnn version 9.2 7.1.3 gpu models configuration relevant information"
pytorch,957,"know bug might reported already find anything . could least added documentation operation tensor even aware expand copy data, believe find result quite counter intuitive. course, fixed atm either replacing , requires memory. would awesome arithmetic ops like special case tensor's 0 dimension edit torch version string",0,counter intuitive behavior expand arithmetic op,"counter intuitive behavior expand arithmetic op know bug might reported already find anything . could least added documentation operation tensor even aware expand copy data, believe find result quite counter intuitive. course, fixed atm either replacing , requires memory. would awesome arithmetic ops like special case tensor's 0 dimension edit torch version string"
pytorch,10041,"problem main blocker using torch.distributions jit function used every distribution's method. currently calls hood, results jit error expensive workaround replace non jittable invocation jittable, roughly proposed solution soumith neerajprad suggested implementing c version , would added benefit speeding non jitted code.",0,jit support torch.distributions.utils.broadcast,"jit support torch.distributions.utils.broadcast problem main blocker using torch.distributions jit function used every distribution's method. currently calls hood, results jit error expensive workaround replace non jittable invocation jittable, roughly proposed solution soumith neerajprad suggested implementing c version , would added benefit speeding non jitted code."
pytorch,7722,"issue description general situation pretrained language model, first phase want train new embedding layer added fine tuning whole thing. worked fine 0.3 sends error message back propagation. minimal reproduction create simple model linear layer lstm, freeze second layer applying require grads false parameters try compute back propagation. code example see https github.com sgugger deep learning blob master bug 20with 20frozen 20lstm 20layer.ipynb system info pytorch version 0.4.0 debug build cuda used build pytorch 9.0 os microsoft windows 10 home gcc version could collect cmake version could collect python version 3.6 cuda available yes cuda runtime version 9.1.85 gpu models configuration could collect nvidia driver version could collect cudnn version could collect versions relevant libraries pip could collect conda could collect pytorch installed conda, bug also appears linux instances. thanks help!",0,error backprop using frozen lstm layers new 0.4.0,"error backprop using frozen lstm layers new 0.4.0 issue description general situation pretrained language model, first phase want train new embedding layer added fine tuning whole thing. worked fine 0.3 sends error message back propagation. minimal reproduction create simple model linear layer lstm, freeze second layer applying require grads false parameters try compute back propagation. code example see https github.com sgugger deep learning blob master bug 20with 20frozen 20lstm 20layer.ipynb system info pytorch version 0.4.0 debug build cuda used build pytorch 9.0 os microsoft windows 10 home gcc version could collect cmake version could collect python version 3.6 cuda available yes cuda runtime version 9.1.85 gpu models configuration could collect nvidia driver version could collect cudnn version could collect versions relevant libraries pip could collect conda could collect pytorch installed conda, bug also appears linux instances. thanks help!"
pytorch,12540,"nervanagpu currently included third party module https github.com pytorch pytorch tree c2a57d082d36f30f32d5c4cc7147ab57ce2c3097 third party . however repository active development anymore modified since 3 years ago. checking commit added it, one https github.com pytorch pytorch commit 9201cdd029aa820ebc92c71bd4f6d4105163ca80 optimized gemm kernel developed nervana. 2 kernels interest nervana's work nervana gemm nervana gemm extremely tuned maxwell architecture, volta turing, gpus offering tensor cores much faster. expect nvidia tuned gemm implementation since then. alternatives cublas nervana maxas https github.com nervanasystems maxas isolated gemm kernel much smaller library full nervanagpu, probably possible clone maintain it. honorary mention nvidia cutlass https github.com nvidia cutlass , achieves 90 speed cublas supports tensor cores without assembly. nvidia's tensorrt inference. nervana convolution kernel used, think main appeal winograd kernel discussed extensively https github.com soumith convnet benchmarks issues 93 even nervanagpu repo neon repo. https github.com nervanasystems nervanagpu issues 25 issuecomment 203168418 . furthermore integrated cudnn torch nchw layout instead neon's chwn layout convolution probably one nvidia focus past 2 years. conclusion benchmarks show alternatives nervana achieves speed current gpu wide range input size tensor cores currently restrictive , think nervanagpu code dropped save code, maintenance, dependencies build options.",0,rfc removing nervana gpu,"rfc removing nervana gpu nervanagpu currently included third party module https github.com pytorch pytorch tree c2a57d082d36f30f32d5c4cc7147ab57ce2c3097 third party . however repository active development anymore modified since 3 years ago. checking commit added it, one https github.com pytorch pytorch commit 9201cdd029aa820ebc92c71bd4f6d4105163ca80 optimized gemm kernel developed nervana. 2 kernels interest nervana's work nervana gemm nervana gemm extremely tuned maxwell architecture, volta turing, gpus offering tensor cores much faster. expect nvidia tuned gemm implementation since then. alternatives cublas nervana maxas https github.com nervanasystems maxas isolated gemm kernel much smaller library full nervanagpu, probably possible clone maintain it. honorary mention nvidia cutlass https github.com nvidia cutlass , achieves 90 speed cublas supports tensor cores without assembly. nvidia's tensorrt inference. nervana convolution kernel used, think main appeal winograd kernel discussed extensively https github.com soumith convnet benchmarks issues 93 even nervanagpu repo neon repo. https github.com nervanasystems nervanagpu issues 25 issuecomment 203168418 . furthermore integrated cudnn torch nchw layout instead neon's chwn layout convolution probably one nvidia focus past 2 years. conclusion benchmarks show alternatives nervana achieves speed current gpu wide range input size tensor cores currently restrictive , think nervanagpu code dropped save code, maintenance, dependencies build options."
pytorch,30446,"feature official adafactor optimizer motivation efficient optimizer adafactor currently widely used big models, saves lot memory due sub linear running average gradient, sometimes result sigificant memory footprint reduce larger batch size. pitch alternatives additional context cc vincentqb",0,plan adafactor optimizer?,"plan adafactor optimizer? feature official adafactor optimizer motivation efficient optimizer adafactor currently widely used big models, saves lot memory due sub linear running average gradient, sometimes result sigificant memory footprint reduce larger batch size. pitch alternatives additional context cc vincentqb"
pytorch,4333,"hi, recently working summarization project. training, saved best model development set. however, loading best model testing dev set gives different rouge result 0.18218091939853281 0.18217045231619222 . although difference small, raises much concerns. colleagues told also encountered issue observed 2 points drop qa task . wrote small script https gist.github.com magic282 94bdbb3b9ddef891d7aa40f0b0069a0f , found parameters identical, result different saving loading. also found thread https discuss.pytorch.org saving loading model pytorch 2610 21 code github gist runs windows peterjc123 0.3.0 build . linux also 0.3.0 , built docker image installing pytorch conda raises error understand thread posted https discuss.pytorch.org parameters different loading model 11457",0,model behaves differently saving loading,"model behaves differently saving loading hi, recently working summarization project. training, saved best model development set. however, loading best model testing dev set gives different rouge result 0.18218091939853281 0.18217045231619222 . although difference small, raises much concerns. colleagues told also encountered issue observed 2 points drop qa task . wrote small script https gist.github.com magic282 94bdbb3b9ddef891d7aa40f0b0069a0f , found parameters identical, result different saving loading. also found thread https discuss.pytorch.org saving loading model pytorch 2610 21 code github gist runs windows peterjc123 0.3.0 build . linux also 0.3.0 , built docker image installing pytorch conda raises error understand thread posted https discuss.pytorch.org parameters different loading model 11457"
pytorch,24991,feature add support selu activation calculate gain function. https pytorch.org docs stable modules torch nn init.html calculate gain cc alband mruberry ssnl,0,feature request add support selu activation calculate gain function.,feature request add support selu activation calculate gain function. feature add support selu activation calculate gain function. https pytorch.org docs stable modules torch nn init.html calculate gain cc alband mruberry ssnl
pytorch,15129,"bug wanted obtain convolutional features resnet 101. already figured it, wanted run simple test check applying correct preprocessing steps, instead omitting last layer, transformed whole network nn.sequential , get error runtimeerror size mismatch, m1 2048 x 1 , m2 2048 x 1000 pytorch aten src th generic thtensormath.cpp 940 reproduce code run resnet eval valid image path, get following output course, transforming whole net sequential model non sense, replace first line docstring resnet eval get expected output another approach remove final layer, reshape output tensor add last layer again, like again, works expected, giving output running print tensor values, identical two last cases. come accross mismatch error first case equivalently, last case valid ? expected behavior first case run smoothly, giving output second one. environment tested python3.5 torch 1.0.0 torchvision 0.2.1",0,resnet 101 run converted sequential returns dimension mismatch,"resnet 101 run converted sequential returns dimension mismatch bug wanted obtain convolutional features resnet 101. already figured it, wanted run simple test check applying correct preprocessing steps, instead omitting last layer, transformed whole network nn.sequential , get error runtimeerror size mismatch, m1 2048 x 1 , m2 2048 x 1000 pytorch aten src th generic thtensormath.cpp 940 reproduce code run resnet eval valid image path, get following output course, transforming whole net sequential model non sense, replace first line docstring resnet eval get expected output another approach remove final layer, reshape output tensor add last layer again, like again, works expected, giving output running print tensor values, identical two last cases. come accross mismatch error first case equivalently, last case valid ? expected behavior first case run smoothly, giving output second one. environment tested python3.5 torch 1.0.0 torchvision 0.2.1"
pytorch,16198,"bug ppc64le ci build fails psutil found. running test dataloader 2019 01 20 15 16 53.873206 traceback recent call last file test dataloader.py , line 24, import psutil modulenotfounderror module named 'psutil' pytorch version pull master last 3 days. ci log https powerci.osuosl.org user avmgithub views view pytorch job pytorch linux cuda92 cudnn7 py3 mpi build test gpu 269 console cc ssnl",0,ppc64le ci build failure possibly due 16006,"ppc64le ci build failure possibly due 16006 bug ppc64le ci build fails psutil found. running test dataloader 2019 01 20 15 16 53.873206 traceback recent call last file test dataloader.py , line 24, import psutil modulenotfounderror module named 'psutil' pytorch version pull master last 3 days. ci log https powerci.osuosl.org user avmgithub views view pytorch job pytorch linux cuda92 cudnn7 py3 mpi build test gpu 269 console cc ssnl"
pytorch,22017,"compiled pytorch ubuntu18.04, gcc 7.4.0, git checkout v1.0.0 compiling. compiling process begins, terminated 34 error what's wrong solve problem? anyone help issue?",0,mkldnn version.h file directory,"mkldnn version.h file directory compiled pytorch ubuntu18.04, gcc 7.4.0, git checkout v1.0.0 compiling. compiling process begins, terminated 34 error what's wrong solve problem? anyone help issue?"
pytorch,6486,gh pages branch exist.,0,caffe2 website?,caffe2 website? gh pages branch exist.
pytorch,19807,"hi everyone, trying user ffi extensions order add cuda compiled functions. error encountered follows cffi.cdeferror cannot parse include 29 1 directives supported yet environmental configuration follows cuda compilation tool version 9.0, v9.0.176 pycparser version 2.18 pytorch version 0.4.0 could please provide assitance issue? thanks.",0,create extension error pytorch 0.4.0,"create extension error pytorch 0.4.0 hi everyone, trying user ffi extensions order add cuda compiled functions. error encountered follows cffi.cdeferror cannot parse include 29 1 directives supported yet environmental configuration follows cuda compilation tool version 9.0, v9.0.176 pycparser version 2.18 pytorch version 0.4.0 could please provide assitance issue? thanks."
pytorch,3064,reported https discuss.pytorch.org cudnn status supported error occurs apply autograd grad compute high order differentiation 8256,0,double backward conv2d failing cudnn status supported,double backward conv2d failing cudnn status supported reported https discuss.pytorch.org cudnn status supported error occurs apply autograd grad compute high order differentiation 8256
pytorch,16326,"bug c frontend unable support std vector reproduce python code cpp code error message preliminary debugging particular, offending function call site offending function. seem check generic types. like . instead, goes straight environment pytorch version 4edc8273eb223785929c0017caf15c101964d480",0,jit c frontend unable support std vector,"jit c frontend unable support std vector bug c frontend unable support std vector reproduce python code cpp code error message preliminary debugging particular, offending function call site offending function. seem check generic types. like . instead, goes straight environment pytorch version 4edc8273eb223785929c0017caf15c101964d480"
pytorch,21701,nan,0,add comments classes bailout graph.cpp,add comments classes bailout graph.cpp nan
pytorch,3798,"hi everyone, silly gotcha installing latest master source ubuntu 16 space path install from, fails compile compiler simple test , throws fairly obscure error messages cc unrecognised compiler. perhaps needs line instructions, took digging compiler logs find issue unless missed something! . apologies duplicated, find relevant issue quick search. happy contribute fix docs etc required.",0,can't install source path contains space,"can't install source path contains space hi everyone, silly gotcha installing latest master source ubuntu 16 space path install from, fails compile compiler simple test , throws fairly obscure error messages cc unrecognised compiler. perhaps needs line instructions, took digging compiler logs find issue unless missed something! . apologies duplicated, find relevant issue quick search. happy contribute fix docs etc required."
pytorch,11121,"issue description using nfs, sometimes find machines without cuda , although pytorch installed cuda enabled machine. pytorch fails load crashes instead. code example system info pytorch caffe2 pytorch installed pytorch conda, pip, source , cuda enabled machine os centos 7 pytorch version python version cuda cudnn version",0,running torch.cuda.is available import torch. c import,"running torch.cuda.is available import torch. c import issue description using nfs, sometimes find machines without cuda , although pytorch installed cuda enabled machine. pytorch fails load crashes instead. code example system info pytorch caffe2 pytorch installed pytorch conda, pip, source , cuda enabled machine os centos 7 pytorch version python version cuda cudnn version"
pytorch,29211,"appears even fixes, longer works dtypes reproduce environment additional context blocking us convert fp32 sampling",0,multinomial kernel cuda implemented 'half',"multinomial kernel cuda implemented 'half' appears even fixes, longer works dtypes reproduce environment additional context blocking us convert fp32 sampling"
pytorch,14891,"bug parameters buffers must checked recursively load procedure module.cpp l297 https github.com pytorch pytorch blob 8311bbee7f1dd33346f18c769cfeb8c5b5941874 torch csrc api src nn module.cpp l297 . example, like",0,wrong recursive module load,"wrong recursive module load bug parameters buffers must checked recursively load procedure module.cpp l297 https github.com pytorch pytorch blob 8311bbee7f1dd33346f18c769cfeb8c5b5941874 torch csrc api src nn module.cpp l297 . example, like"
pytorch,30818,nan,0,kthvalue median scalar dim 1 inconsistent cpu cuda,kthvalue median scalar dim 1 inconsistent cpu cuda nan
pytorch,16229,first surfaced 15817 filing new bug clarity. error looks like https ci.pytorch.org jenkins job caffe2 builds job py2 devtoolset7 rocmrpm centos7.5 test 5483 console seems specifically rocm failure. cc bddppq iotamudelta,0,test dag net forking flaky rocm,test dag net forking flaky rocm first surfaced 15817 filing new bug clarity. error looks like https ci.pytorch.org jenkins job caffe2 builds job py2 devtoolset7 rocmrpm centos7.5 test 5483 console seems specifically rocm failure. cc bddppq iotamudelta
pytorch,27569,"bug torchscript onnx conversion simple module fails one jit compile model, everything works. reproduce output expected behavior expected jit compiled module consisting two children export onnx without hassle. environment google colab, think moment pytorch 1.2.0 python 3.6 additional context played around torchscript tracing onnx export modules work namedtuples, got errors. trying get minimal example led code namedtuples. cc suo",0,export torchscript onnx torch.onnx.symbolic opset9.dim exist,"export torchscript onnx torch.onnx.symbolic opset9.dim exist bug torchscript onnx conversion simple module fails one jit compile model, everything works. reproduce output expected behavior expected jit compiled module consisting two children export onnx without hassle. environment google colab, think moment pytorch 1.2.0 python 3.6 additional context played around torchscript tracing onnx export modules work namedtuples, got errors. trying get minimal example led code namedtuples. cc suo"
pytorch,10690,nan,0,non blocking cuda2cpu copy require contiguity,non blocking cuda2cpu copy require contiguity nan
pytorch,30578,"bug diagonal elements torch.symeig's gradients factor 2. further, checking gradients torch.autograd.gradcheck gives runtimeerror. reproduce following code computes eigenvalue decomposition symmetric matrix corresponding derivatives two different approaches. one approach uses torch.symeig backward compute gradient. second approach uses torch.symeig numerical differentiation compute gradient. further, torch.autograd.gradcheck called test torch.symeig gives runtimeerror. prints observations made first, since numerical differentiation implemented take symmetry matrix account, upper triangular matrix computed. true version implemented error message torch.autograd.gradcheck. two numerical differentiation versions identical. second, diagonal elements computed via backward computed via numerical differentiation differ factor 2. expected behavior derivative computed via backward least upper triangular part identical numerical differentiation gradient. reason case structure inputs torch.symeig i.e., symmetry inputs taken account backward pass torch.symeig. derivatives functions symmetric matrices need extra care, equation 138 matrix cookbook https www.math.uwaterloo.ca hwolkowi matrixcookbook.pdf highlights. far tell, line code computes return value symeig's backward https github.com pytorch pytorch blob dd52f50fc85e6710f020936c1fc5f14673508350 tools autograd templates functions.cpp l1797 needs modified. demonstrate this, following implements custom gradient symeig backward pass return value changed slightly. prints agrees numerical differentiation. so, implementation symeig backward changed? environment output environment collection script cc vishwakftw ssnl jianyuh",0,gradients symeig correct gradcheck fails,"gradients symeig correct gradcheck fails bug diagonal elements torch.symeig's gradients factor 2. further, checking gradients torch.autograd.gradcheck gives runtimeerror. reproduce following code computes eigenvalue decomposition symmetric matrix corresponding derivatives two different approaches. one approach uses torch.symeig backward compute gradient. second approach uses torch.symeig numerical differentiation compute gradient. further, torch.autograd.gradcheck called test torch.symeig gives runtimeerror. prints observations made first, since numerical differentiation implemented take symmetry matrix account, upper triangular matrix computed. true version implemented error message torch.autograd.gradcheck. two numerical differentiation versions identical. second, diagonal elements computed via backward computed via numerical differentiation differ factor 2. expected behavior derivative computed via backward least upper triangular part identical numerical differentiation gradient. reason case structure inputs torch.symeig i.e., symmetry inputs taken account backward pass torch.symeig. derivatives functions symmetric matrices need extra care, equation 138 matrix cookbook https www.math.uwaterloo.ca hwolkowi matrixcookbook.pdf highlights. far tell, line code computes return value symeig's backward https github.com pytorch pytorch blob dd52f50fc85e6710f020936c1fc5f14673508350 tools autograd templates functions.cpp l1797 needs modified. demonstrate this, following implements custom gradient symeig backward pass return value changed slightly. prints agrees numerical differentiation. so, implementation symeig backward changed? environment output environment collection script cc vishwakftw ssnl jianyuh"
pytorch,16181,feature enable call layer nn.module like python dict. add bellow function . reference chainer.chain https github.com chainer chainer blob v5.1.0 chainer link.py l871 l873 motivation access layer network easily especially beginners . old new maybe different concept pytorch?,0,feature request add getitem nn.module,feature request add getitem nn.module feature enable call layer nn.module like python dict. add bellow function . reference chainer.chain https github.com chainer chainer blob v5.1.0 chainer link.py l871 l873 motivation access layer network easily especially beginners . old new maybe different concept pytorch?
pytorch,20794,"compile c extension windows pytorch1.1.0. get following error. resolve question? thanks! e program files python35 lib site packages torch include thc thcnumerics.cuh 190 one operator matches operands built operator arithmetic arithmetic function operator const half , const half operand types c10 half c10 half e program files python35 lib site packages torch include thc thcnumerics.cuh 191 one operator matches operands built operator arithmetic arithmetic function operator const half , const half operand types c10 half c10 half e program files python35 lib site packages torch include thc thcnumerics.cuh 192 one operator matches operands built operator arithmetic arithmetic function operator const half , const half operand types c10 half c10 half e program files python35 lib site packages torch include thc thcnumerics.cuh 193 one operator matches operands built operator arithmetic arithmetic function operator const half , const half operand types c10 half c10 half e program files python35 lib site packages torch include thc thcnumerics.cuh 194 one operator matches operands built operator arithmetic arithmetic function operator const half , const half operand types c10 half c10 half e program files python35 lib site packages torch include thc thcnumerics.cuh 196 one operator ! matches operands built operator arithmetic ! arithmetic function operator! const half , const half operand types c10 half ! c10 half",0,pytorch1.1.0 windows one operator matches operands,"pytorch1.1.0 windows one operator matches operands compile c extension windows pytorch1.1.0. get following error. resolve question? thanks! e program files python35 lib site packages torch include thc thcnumerics.cuh 190 one operator matches operands built operator arithmetic arithmetic function operator const half , const half operand types c10 half c10 half e program files python35 lib site packages torch include thc thcnumerics.cuh 191 one operator matches operands built operator arithmetic arithmetic function operator const half , const half operand types c10 half c10 half e program files python35 lib site packages torch include thc thcnumerics.cuh 192 one operator matches operands built operator arithmetic arithmetic function operator const half , const half operand types c10 half c10 half e program files python35 lib site packages torch include thc thcnumerics.cuh 193 one operator matches operands built operator arithmetic arithmetic function operator const half , const half operand types c10 half c10 half e program files python35 lib site packages torch include thc thcnumerics.cuh 194 one operator matches operands built operator arithmetic arithmetic function operator const half , const half operand types c10 half c10 half e program files python35 lib site packages torch include thc thcnumerics.cuh 196 one operator ! matches operands built operator arithmetic ! arithmetic function operator! const half , const half operand types c10 half ! c10 half"
pytorch,7496,"issue description tried train model multiple gpus. however, launch program, seems model allocated gpus, data fed model. using nvidia smi, find hundreds mb memory consumed gpu. guess memory usage model initialization gpu. sharing 8 gpus others server, limit program gpu 2 gpu 3 following command. code example ran official tutorial machine thing happens again. https pytorch.org tutorials beginner blitz data parallel tutorial.html system info pytorch version 0.4.0 debug build cuda used build pytorch 8.0.61 os ubuntu 16.04.3 lts gcc version ubuntu 5.4.0 6ubuntu1 16.04.9 5.4.0 20160609 cmake version version 3.5.1 python version 3.6 cuda available yes cuda runtime version 7.5.17 gpu models configuration gpu 0 geforce gtx titan x gpu 1 geforce gtx titan x gpu 2 geforce gtx titan x gpu 3 geforce gtx titan x gpu 4 geforce gtx titan x gpu 5 geforce gtx titan x gpu 6 geforce gtx titan x gpu 7 geforce gtx titan x nvidia driver version 375.88 cudnn version probably one following usr lib x86 64 linux gnu libcudnn.so.5.1.10 usr lib x86 64 linux gnu libcudnn.so.6.0.20 usr lib x86 64 linux gnu libcudnn static v5.a usr lib x86 64 linux gnu libcudnn static v6.a usr local lib python2.7 dist packages torch lib libcudnn.so.6 versions relevant libraries pip3 msgpack numpy 0.4.1 pip3 numpy 1.14.0 pip3 numpydoc 0.7.0 pip3 torch 0.4.0 pip3 torchtext 0.2.3 pip3 torchvision 0.2.1 conda torch 0.4.0 conda torchtext 0.2.3 conda torchvision 0.2.1",0,train model using multiple gpus,"train model using multiple gpus issue description tried train model multiple gpus. however, launch program, seems model allocated gpus, data fed model. using nvidia smi, find hundreds mb memory consumed gpu. guess memory usage model initialization gpu. sharing 8 gpus others server, limit program gpu 2 gpu 3 following command. code example ran official tutorial machine thing happens again. https pytorch.org tutorials beginner blitz data parallel tutorial.html system info pytorch version 0.4.0 debug build cuda used build pytorch 8.0.61 os ubuntu 16.04.3 lts gcc version ubuntu 5.4.0 6ubuntu1 16.04.9 5.4.0 20160609 cmake version version 3.5.1 python version 3.6 cuda available yes cuda runtime version 7.5.17 gpu models configuration gpu 0 geforce gtx titan x gpu 1 geforce gtx titan x gpu 2 geforce gtx titan x gpu 3 geforce gtx titan x gpu 4 geforce gtx titan x gpu 5 geforce gtx titan x gpu 6 geforce gtx titan x gpu 7 geforce gtx titan x nvidia driver version 375.88 cudnn version probably one following usr lib x86 64 linux gnu libcudnn.so.5.1.10 usr lib x86 64 linux gnu libcudnn.so.6.0.20 usr lib x86 64 linux gnu libcudnn static v5.a usr lib x86 64 linux gnu libcudnn static v6.a usr local lib python2.7 dist packages torch lib libcudnn.so.6 versions relevant libraries pip3 msgpack numpy 0.4.1 pip3 numpy 1.14.0 pip3 numpydoc 0.7.0 pip3 torch 0.4.0 pip3 torchtext 0.2.3 pip3 torchvision 0.2.1 conda torch 0.4.0 conda torchtext 0.2.3 conda torchvision 0.2.1"
pytorch,24869,feature implement dictionary ops c motivation dictionary ops replace current vocab class torchtext serve new class purpose . could also applied transforming audio script. think dictionary ops pytorch core library makes easier maintain. pitch baselines included findword addword count nwords count ntokens count nlables getword readfromsequence readfromfile,0,dictionary c,dictionary c feature implement dictionary ops c motivation dictionary ops replace current vocab class torchtext serve new class purpose . could also applied transforming audio script. think dictionary ops pytorch core library makes easier maintain. pitch baselines included findword addword count nwords count ntokens count nlables getword readfromsequence readfromfile
pytorch,10858,issue description mixing torch.no grad decorator torch.no grad context enables gradients per torch.is grad enabled. code example output false true expectation would expectation within grad context gradients would always disabled. expectation matches output using nested contexts below. output false false system info pytorch version 0.4.1 debug build cuda used build pytorch 9.2.148 os ubuntu 16.04.5 lts gcc version ubuntu 5.4.0 6ubuntu1 16.04.10 5.4.0 20160609 cmake version version 3.5.1 python version 3.6 cuda available yes cuda runtime version could collect gpu models configuration gpu 0 geforce gtx 1080 ti nvidia driver version 396.54 cudnn version could collect versions relevant libraries pip could collect conda cuda92 1.0 0 pytorch conda pytorch 0.4.1 py36 cuda9.2.148 cudnn7.1.4 1 cuda92 pytorch conda torchvision 0.2.1 py36 1 pytorch conda warpctc pytorch 0.1,0,unexpected behaviour mixing grad decorator grad context manager,unexpected behaviour mixing grad decorator grad context manager issue description mixing torch.no grad decorator torch.no grad context enables gradients per torch.is grad enabled. code example output false true expectation would expectation within grad context gradients would always disabled. expectation matches output using nested contexts below. output false false system info pytorch version 0.4.1 debug build cuda used build pytorch 9.2.148 os ubuntu 16.04.5 lts gcc version ubuntu 5.4.0 6ubuntu1 16.04.10 5.4.0 20160609 cmake version version 3.5.1 python version 3.6 cuda available yes cuda runtime version could collect gpu models configuration gpu 0 geforce gtx 1080 ti nvidia driver version 396.54 cudnn version could collect versions relevant libraries pip could collect conda cuda92 1.0 0 pytorch conda pytorch 0.4.1 py36 cuda9.2.148 cudnn7.1.4 1 cuda92 pytorch conda torchvision 0.2.1 py36 1 pytorch conda warpctc pytorch 0.1
pytorch,8502,operator level tests fail using script. example failure affected tests,0,jit support none script,jit support none script operator level tests fail using script. example failure affected tests
pytorch,30375,bug trying run distributed deep learning workload multiple nodes using gloo. unfortunately receive error two node environment two vms. vms 16 gb ram 8 cores. reproduce code similar link https pytorch.org tutorials intermediate dist tuto.html difference initiation phase expected behavior expected behavior seeing error code distributed training process kicking off. environment pytorch version 1.4.0.dev20191122 cpu debug build cuda used build pytorch none os ubuntu 18.04.3 lts gcc version ubuntu 7.4.0 1ubuntu1 18.04.1 7.4.0 cmake version could collect python version 3.6 cuda available cuda runtime version cuda gpu models configuration cuda nvidia driver version cuda cudnn version cuda versions relevant libraries pip3 numpy 1.17.4 pip3 torch 1.4.0.dev20191122 cpu pip3 torchvision 0.5.0.dev20191122 cpu conda could collect cc pietern mrshenli pritamdamania87 zhaojuanmao satgera rohan varma gqchen aazzolini xush6528,0,distributed using gloo multiple nodes work,distributed using gloo multiple nodes work bug trying run distributed deep learning workload multiple nodes using gloo. unfortunately receive error two node environment two vms. vms 16 gb ram 8 cores. reproduce code similar link https pytorch.org tutorials intermediate dist tuto.html difference initiation phase expected behavior expected behavior seeing error code distributed training process kicking off. environment pytorch version 1.4.0.dev20191122 cpu debug build cuda used build pytorch none os ubuntu 18.04.3 lts gcc version ubuntu 7.4.0 1ubuntu1 18.04.1 7.4.0 cmake version could collect python version 3.6 cuda available cuda runtime version cuda gpu models configuration cuda nvidia driver version cuda cudnn version cuda versions relevant libraries pip3 numpy 1.17.4 pip3 torch 1.4.0.dev20191122 cpu pip3 torchvision 0.5.0.dev20191122 cpu conda could collect cc pietern mrshenli pritamdamania87 zhaojuanmao satgera rohan varma gqchen aazzolini xush6528
pytorch,9527,"traceback recent call last file tools test net.py , line 151, main ind range args.range, multi gpu testing false file tools test net.py , line 113, main engine.test net dataset multi gpu multi gpu testing file home chandu workspace tracking dat lib core test engine.py , line 321, test net dataset boxes, segms, keyps test net file home chandu workspace tracking dat lib core test engine.py , line 135, test net model initialize model cfg file home chandu workspace tracking dat lib core test engine.py , line 59, initialize model cfg init params cfg.test.init random vars loading file home chandu workspace tracking dat lib modeling model builder.py , line 63, create return get func model name init model model name, train, init params file home chandu workspace tracking dat lib modeling model builder.py , line 156, keypoint rcnn add roi keypoint head func get func cfg.krcnn.roi keypoints head file home chandu workspace tracking dat lib modeling model builder.py , line 307, build generic fast rcnn model build data parallel model model, single gpu build func file home chandu workspace tracking dat lib modeling model builder.py , line 953, build data parallel model single gpu build func model file home chandu workspace tracking dat lib modeling model builder.py , line 199, single gpu build func blob conv, dim conv, spatial scale conv add conv body func model file home chandu workspace tracking dat lib modeling fpn3d.py , line 51, add fpn resnet101 conv5 body resnet.stage info resnet101 conv5 file home chandu workspace tracking dat lib modeling fpn3d.py , line 98, add fpn generic onto body conv body func model file home chandu workspace tracking dat lib modeling resnet3d.py , line 389, add resnet101 conv5 body return add resnet convx body model, 3, 4, 23, 3 , freeze 2 file home chandu workspace tracking dat lib modeling resnet3d.py , line 262, add resnet convx body p model.affinechannelnd p, 'res conv1 bn', dim feat dims 0 , inplace true file home chandu workspace tracking dat lib modeling detector.py , line 94, affinechannelnd cfg.model.use bn file home chandu workspace tracking anaconda2 envs gtracking lib python2.7 site packages caffe2 python core.py , line 2082, getattr , .join workspace.c.nearby opnames op type ' ' attributeerror method affinechannelnd registered operator. mean affinechannel getting error?",0,attributeerror method affinechannelnd registered operator. mean affinechannel,"attributeerror method affinechannelnd registered operator. mean affinechannel traceback recent call last file tools test net.py , line 151, main ind range args.range, multi gpu testing false file tools test net.py , line 113, main engine.test net dataset multi gpu multi gpu testing file home chandu workspace tracking dat lib core test engine.py , line 321, test net dataset boxes, segms, keyps test net file home chandu workspace tracking dat lib core test engine.py , line 135, test net model initialize model cfg file home chandu workspace tracking dat lib core test engine.py , line 59, initialize model cfg init params cfg.test.init random vars loading file home chandu workspace tracking dat lib modeling model builder.py , line 63, create return get func model name init model model name, train, init params file home chandu workspace tracking dat lib modeling model builder.py , line 156, keypoint rcnn add roi keypoint head func get func cfg.krcnn.roi keypoints head file home chandu workspace tracking dat lib modeling model builder.py , line 307, build generic fast rcnn model build data parallel model model, single gpu build func file home chandu workspace tracking dat lib modeling model builder.py , line 953, build data parallel model single gpu build func model file home chandu workspace tracking dat lib modeling model builder.py , line 199, single gpu build func blob conv, dim conv, spatial scale conv add conv body func model file home chandu workspace tracking dat lib modeling fpn3d.py , line 51, add fpn resnet101 conv5 body resnet.stage info resnet101 conv5 file home chandu workspace tracking dat lib modeling fpn3d.py , line 98, add fpn generic onto body conv body func model file home chandu workspace tracking dat lib modeling resnet3d.py , line 389, add resnet101 conv5 body return add resnet convx body model, 3, 4, 23, 3 , freeze 2 file home chandu workspace tracking dat lib modeling resnet3d.py , line 262, add resnet convx body p model.affinechannelnd p, 'res conv1 bn', dim feat dims 0 , inplace true file home chandu workspace tracking dat lib modeling detector.py , line 94, affinechannelnd cfg.model.use bn file home chandu workspace tracking anaconda2 envs gtracking lib python2.7 site packages caffe2 python core.py , line 2082, getattr , .join workspace.c.nearby opnames op type ' ' attributeerror method affinechannelnd registered operator. mean affinechannel getting error?"
pytorch,1113,float train data. send dataloader get enumerate. found type data changed torch.doubletensor. really need original type. found caused torch utils data dataloader.py' definition default collate. bug? could make keep original datatype?,0,possibly bug default collate dataloader change type data?,possibly bug default collate dataloader change type data? float train data. send dataloader get enumerate. found type data changed torch.doubletensor. really need original type. found caused torch utils data dataloader.py' definition default collate. bug? could make keep original datatype?
pytorch,422,"seems like something users'll forget do, might get lot subtle bug reports this. seems like good usability fix early on.",0,get rid requiring super. init nn.container subclasses,"get rid requiring super. init nn.container subclasses seems like something users'll forget do, might get lot subtle bug reports this. seems like good usability fix early on."
pytorch,6655,"cannot load forum, error code err timed get 300ms ping discuss.pytorch.org use chrome win10 64",0,cannot load forum,"cannot load forum cannot load forum, error code err timed get 300ms ping discuss.pytorch.org use chrome win10 64"
pytorch,6384,"hello, trying run transfering model pytorch caffe2 mobile using onnx https github.com onnx tutorials blob master tutorials pytorchcaffe2superresolution.ipynb tutorial. however, get segmentation fault error. think segmentation error triggered second part tutorial loading onnx model caffe2. searching might cause error noticed tutorial 2 different versions one onnx github tutorial pytorch http pytorch.org tutorials advanced super resolution caffe2.html website. onnx caffe2 import two tutorials different second part along implementation. one uses import caffe2.python.onnx.backend import onnx caffe2.backend. tried tutorial get segmentation fault both. also, comment onnx caffe2 tutorial related import lines, first part pytorch onnx seems work without error. below, describe create virtual env results gdb getting segmentation fault dependencies cuda 8.0 cudnn v6.0 opencv 3.4.1 nccl 2.0.5 caffe2 2018 03 02 run code used following command 6gb ram, 1 gpu 6gb also, upgraded numpy pyyaml versioons otherwise, get upgrading numpy version, run code get segmentation fault tried run code gdb well see exactly fault occurs tutorial, segmantation fault occurs place. know fix error. change anything code. sure getting error. really appreciate help fix it. note already opened issue https github.com onnx tutorials issues 29 onnx repo want cross reference issue since noticed get error pytorch tutorial http pytorch.org tutorials advanced super resolution caffe2.html well.",0,caffe2 pytorch pytorch caffe2 mobile using onnx tutorial segmentation fault,"caffe2 pytorch pytorch caffe2 mobile using onnx tutorial segmentation fault hello, trying run transfering model pytorch caffe2 mobile using onnx https github.com onnx tutorials blob master tutorials pytorchcaffe2superresolution.ipynb tutorial. however, get segmentation fault error. think segmentation error triggered second part tutorial loading onnx model caffe2. searching might cause error noticed tutorial 2 different versions one onnx github tutorial pytorch http pytorch.org tutorials advanced super resolution caffe2.html website. onnx caffe2 import two tutorials different second part along implementation. one uses import caffe2.python.onnx.backend import onnx caffe2.backend. tried tutorial get segmentation fault both. also, comment onnx caffe2 tutorial related import lines, first part pytorch onnx seems work without error. below, describe create virtual env results gdb getting segmentation fault dependencies cuda 8.0 cudnn v6.0 opencv 3.4.1 nccl 2.0.5 caffe2 2018 03 02 run code used following command 6gb ram, 1 gpu 6gb also, upgraded numpy pyyaml versioons otherwise, get upgrading numpy version, run code get segmentation fault tried run code gdb well see exactly fault occurs tutorial, segmantation fault occurs place. know fix error. change anything code. sure getting error. really appreciate help fix it. note already opened issue https github.com onnx tutorials issues 29 onnx repo want cross reference issue since noticed get error pytorch tutorial http pytorch.org tutorials advanced super resolution caffe2.html well."
pytorch,24690,porting th operators essential code simplicity performance reasons. porting guides q available umbrella issue 24507 feel free add vitalyfedyunin reviewer get prioritized review.,0,migrate digamma digamma th aten cpu,migrate digamma digamma th aten cpu porting th operators essential code simplicity performance reasons. porting guides q available umbrella issue 24507 feel free add vitalyfedyunin reviewer get prioritized review.
pytorch,29805,"questions help attribute 'use res connect' type 'bool' usable script method forget add constants ? pycode feathernetb models feathernet2.py 77 15 torch.jit.script method def forward self, x str self.use res connect true return x self.conv x else self.downsample none return self.downsample x self.conv x else return self.conv x please note issue tracker help form issue closed. set listed resources available website https pytorch.org resources . primary means support discussion forum discussion forum https discuss.pytorch.org",0,use torch script convert error,"use torch script convert error questions help attribute 'use res connect' type 'bool' usable script method forget add constants ? pycode feathernetb models feathernet2.py 77 15 torch.jit.script method def forward self, x str self.use res connect true return x self.conv x else self.downsample none return self.downsample x self.conv x else return self.conv x please note issue tracker help form issue closed. set listed resources available website https pytorch.org resources . primary means support discussion forum discussion forum https discuss.pytorch.org"
pytorch,6996,"issue description ignores cuda case crash pure tensor. code example system info pytorch caffe2 pytorch installed pytorch conda, pip, source conda build command used compiling source os ubuntu 16.04 pytorch version 0.4.0 python version 3.6.3 cuda cudnn version 8 5 gpu models configuration gtx 1050 ti gcc version compiling source cmake version versions relevant libraries",0,pytorch 0.4 bug torch.min torch.max ignores nan cuda case crash pure nan tensor,"pytorch 0.4 bug torch.min torch.max ignores nan cuda case crash pure nan tensor issue description ignores cuda case crash pure tensor. code example system info pytorch caffe2 pytorch installed pytorch conda, pip, source conda build command used compiling source os ubuntu 16.04 pytorch version 0.4.0 python version 3.6.3 cuda cudnn version 8 5 gpu models configuration gtx 1050 ti gcc version compiling source cmake version versions relevant libraries"
pytorch,8519,operator level tests fail. example failure affected tests,0,jit passed parameter 1d longtensor expected,jit passed parameter 1d longtensor expected operator level tests fail. example failure affected tests
pytorch,15958,"questions help please note issue tracker help form issue closed. set listed resources available website https pytorch.org resources . primary means support discussion forum discussion forum https discuss.pytorch.org installed pytorch stable 1.0, linux, pip, python 3.6, successfully. however, pip3 install torchvision, began downloading pytorch cpu. cancelled installing torchvision! waiting help!",0,"stable 1.0 cuda 10 successfully, cannot pip3 install torchvision!","stable 1.0 cuda 10 successfully, cannot pip3 install torchvision! questions help please note issue tracker help form issue closed. set listed resources available website https pytorch.org resources . primary means support discussion forum discussion forum https discuss.pytorch.org installed pytorch stable 1.0, linux, pip, python 3.6, successfully. however, pip3 install torchvision, began downloading pytorch cpu. cancelled installing torchvision! waiting help!"
pytorch,1738,"hi, current batchnorm implementation one pair running mean running var. however, modules neural networks may used twice one network forward. based experimental experience also stated paper https arxiv.org pdf 1603.09025.pdf , different running mean running var computed different iterations time steps . found experiments, testing data, net.eval performs much worse net.train backward turned . difference situation different running mean running var used. make batchnorm keep multiple running mean running var choose use first pair first iteration, second pair second iteration, on?",0,make batchnorm maintain multiple pairs running mean running var?,"make batchnorm maintain multiple pairs running mean running var? hi, current batchnorm implementation one pair running mean running var. however, modules neural networks may used twice one network forward. based experimental experience also stated paper https arxiv.org pdf 1603.09025.pdf , different running mean running var computed different iterations time steps . found experiments, testing data, net.eval performs much worse net.train backward turned . difference situation different running mean running var used. make batchnorm keep multiple running mean running var choose use first pair first iteration, second pair second iteration, on?"
pytorch,2527,"made small changes example code. changed lines https github.com pytorch examples blob 86bc3e516dde92fee0ce00668a3714c03155dd44 imagenet main.py l80 l88 and, run script, throws cuda runtime error follows cuda version follows much detailed post also available https stackoverflow.com q 45861767 4993513",0,pytorch giving cuda runtime error 30,"pytorch giving cuda runtime error 30 made small changes example code. changed lines https github.com pytorch examples blob 86bc3e516dde92fee0ce00668a3714c03155dd44 imagenet main.py l80 l88 and, run script, throws cuda runtime error follows cuda version follows much detailed post also available https stackoverflow.com q 45861767 4993513"
pytorch,15920,"questions help observation 1 use default bn layer use model.eval infence. test result significant discrepancy test test. 2 set momentum 0.0 0.1 different tests. results also different. discrepancy designed true? make sense. guess guess bn layer uses different mean var different tests. model senstive mean var changes? question 1 guesses correct? correct, always use global mean global var result training eval? 2 bn parameters designed changes time? different inference sequence causes diffferent result? tried tried set momentum 0 track running stats false load model. work.",0,let batchnorm use global mean training inferance?,"let batchnorm use global mean training inferance? questions help observation 1 use default bn layer use model.eval infence. test result significant discrepancy test test. 2 set momentum 0.0 0.1 different tests. results also different. discrepancy designed true? make sense. guess guess bn layer uses different mean var different tests. model senstive mean var changes? question 1 guesses correct? correct, always use global mean global var result training eval? 2 bn parameters designed changes time? different inference sequence causes diffferent result? tried tried set momentum 0 track running stats false load model. work."
pytorch,9499,"currently implemented python torch nn functions padding.py https github.com pytorch pytorch blob master torch nn functions padding.py . like implemented native function aten. note need update porting. reading material 1. add native function https github.com pytorch pytorch tree master aten src aten native 2. derivatives added tips context object native functions instead, something needs reused , input argument output , use directly . otherwise, make output",0,move constantpadnd aten,"move constantpadnd aten currently implemented python torch nn functions padding.py https github.com pytorch pytorch blob master torch nn functions padding.py . like implemented native function aten. note need update porting. reading material 1. add native function https github.com pytorch pytorch tree master aten src aten native 2. derivatives added tips context object native functions instead, something needs reused , input argument output , use directly . otherwise, make output"
pytorch,22697,documentation https pytorch.org docs master torch.html torch.lerp probably must broadcastable end.,0,docs torch.lerp typo,docs torch.lerp typo documentation https pytorch.org docs master torch.html torch.lerp probably must broadcastable end.
