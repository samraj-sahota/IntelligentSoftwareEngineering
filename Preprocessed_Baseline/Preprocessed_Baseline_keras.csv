Repository,Number,Body,class,Title,Combined_Text
keras,11776,improve performance gpu utilization small models see analysis https github.com rossumai keras multi gpu blob master blog docs measurements.md . note created fchollet requests contributions .,1,improve performance gpu utilization multi gpu model small models.,improve performance gpu utilization multi gpu model small models. improve performance gpu utilization small models see analysis https github.com rossumai keras multi gpu blob master blog docs measurements.md . note created fchollet requests contributions .
keras,8707,deleted,1,good accuracy test data bad prediction behaviour lstm,good accuracy test data bad prediction behaviour lstm deleted
keras,11267,"similar problem old issue https github.com keras team keras issues 7723 wrongly closed stale , epochs, validation calculation slower epoch training part. validation training sets number examples. two phases take time 18 cores xeon cpu, validation takes 4 times training time intel phi architeclure tensorflow mkl binary . think suspect model evaluation validation calculation take advantage parallelization multi threading. could core problem. please check. regards please make sure boxes checked submit issue. issue implementation question , please ask question stackoverflow http stackoverflow.com questions tagged keras keras slack channel https keras slack autojoin.herokuapp.com instead opening github issue. thank you! x check date master branch keras. update x check version tensorflow date. installation instructions found https www.tensorflow.org get started os setup . x linked old issue already it. provide link github gist python script reproduce issue copy script short .",1,"model.fit slow validation, evident intel phi","model.fit slow validation, evident intel phi similar problem old issue https github.com keras team keras issues 7723 wrongly closed stale , epochs, validation calculation slower epoch training part. validation training sets number examples. two phases take time 18 cores xeon cpu, validation takes 4 times training time intel phi architeclure tensorflow mkl binary . think suspect model evaluation validation calculation take advantage parallelization multi threading. could core problem. please check. regards please make sure boxes checked submit issue. issue implementation question , please ask question stackoverflow http stackoverflow.com questions tagged keras keras slack channel https keras slack autojoin.herokuapp.com instead opening github issue. thank you! x check date master branch keras. update x check version tensorflow date. installation instructions found https www.tensorflow.org get started os setup . x linked old issue already it. provide link github gist python script reproduce issue copy script short ."
keras,511,"trying mnist examples keras documentation well cifar10 work use gpu. accuracy always 0.1 nan. cpu works correctly gpu float64 also works correctly although slowly. tried restart laptop like another user suggests also cleaning purging theano cache, problem remains. system mac os x 10.10. gtx 750m. theano without keras works properly gpu float32 libraries examples.",1,low nan accuracy examples running gpu float32,"low nan accuracy examples running gpu float32 trying mnist examples keras documentation well cifar10 work use gpu. accuracy always 0.1 nan. cpu works correctly gpu float64 also works correctly although slowly. tried restart laptop like another user suggests also cleaning purging theano cache, problem remains. system mac os x 10.10. gtx 750m. theano without keras works properly gpu float32 libraries examples."
keras,12812,"training gru autoencoder eeg data, discover model predicts lot poorly train loss mse suggests evaluate , test batch , predict hand coded loss, fit agree loss x10 train values results below. exploring various predictions, often seems network performs well except bias offset . aside this, without clue cause underlying discrepancy disabling dropout batch norm help. clues? help appreciated. additional details cudnngru stateful implementation, tensorflow backend layers output layers input, encoder, latent encoder latent, latent decoder 25 separate , 10 min sequences fed 400 timesteps 1 sec time 10 60 600 'windows' parallel, non shuffled applied testing new x25 10 min sequences training, results differ keras 2.2.4, python 3.6, spyder 3.3.4 via anaconda results model.fit x,x,validation data x,x 25 25 1s 42ms step loss 0.1780 val loss 2.9175 25 25 1s 39ms step loss 0.1794 val loss 2.9380 ! enter image description 1 1 1 https i.stack.imgur.com 1nyu5.png",1,"validation loss train loss, data","validation loss train loss, data training gru autoencoder eeg data, discover model predicts lot poorly train loss mse suggests evaluate , test batch , predict hand coded loss, fit agree loss x10 train values results below. exploring various predictions, often seems network performs well except bias offset . aside this, without clue cause underlying discrepancy disabling dropout batch norm help. clues? help appreciated. additional details cudnngru stateful implementation, tensorflow backend layers output layers input, encoder, latent encoder latent, latent decoder 25 separate , 10 min sequences fed 400 timesteps 1 sec time 10 60 600 'windows' parallel, non shuffled applied testing new x25 10 min sequences training, results differ keras 2.2.4, python 3.6, spyder 3.3.4 via anaconda results model.fit x,x,validation data x,x 25 25 1s 42ms step loss 0.1780 val loss 2.9175 25 25 1s 39ms step loss 0.1794 val loss 2.9380 ! enter image description 1 1 1 https i.stack.imgur.com 1nyu5.png"
keras,4108,"hi, wrote custom loss function uses built binary crossentropy loss however, behavior different use function instead passing 'binary crossentropy' model's compile method. function yields worse results. suspect might numerical problem know fix it. gist, https gist.github.com ehsanei 90203acd0026d915699a7cbb9af584a9 get 99 validation accuracy mnist built function 96 validation accuracy loss first epoch. difference much bigger dataset.",1,different behavior custom loss function.,"different behavior custom loss function. hi, wrote custom loss function uses built binary crossentropy loss however, behavior different use function instead passing 'binary crossentropy' model's compile method. function yields worse results. suspect might numerical problem know fix it. gist, https gist.github.com ehsanei 90203acd0026d915699a7cbb9af584a9 get 99 validation accuracy mnist built function 96 validation accuracy loss first epoch. difference much bigger dataset."
keras,4633,"hello there, exact message middle training model tf issue two setups setup 1 32gb ram, gpu setup 2 p2.xlarge instance 60gb ram, 1 2 tesla k80 model goes like lstm 2, lstm16, lstm8, dense1 happens lstm gru network data 3gb csv 4 0.75gb memory usage theano like ! 15321436 1588947471135425 2108629537 png https cloud.githubusercontent.com assets 9210039 20986714 125561f4 bcca 11e6 9c38 47bfb33ad91b.jpeg happens roughly theano invokes g compilation blows oom happens tf, plateaus 35gb memory runs happily, even though setup 1 might question leave optimization , setup 2 works like charm. think worth investigating. theano 0.9.0.dev3 keras 1.1.0",1,theano memory issues.,"theano memory issues. hello there, exact message middle training model tf issue two setups setup 1 32gb ram, gpu setup 2 p2.xlarge instance 60gb ram, 1 2 tesla k80 model goes like lstm 2, lstm16, lstm8, dense1 happens lstm gru network data 3gb csv 4 0.75gb memory usage theano like ! 15321436 1588947471135425 2108629537 png https cloud.githubusercontent.com assets 9210039 20986714 125561f4 bcca 11e6 9c38 47bfb33ad91b.jpeg happens roughly theano invokes g compilation blows oom happens tf, plateaus 35gb memory runs happily, even though setup 1 might question leave optimization , setup 2 works like charm. think worth investigating. theano 0.9.0.dev3 keras 1.1.0"
keras,6688,"trying build binary classification algorithm output 0 1 dataset contains normal malicious network packets. dataset shape converting ip 's hexa decimal ! capture1 https cloud.githubusercontent.com assets 18170760 26241844 86366da2 3c86 11e7 9620 24e6dd6f74e0.png note final column output. keras model however, tried different optimizers, activation functions, number layers, accuracy reaching 0.5 ! capture https cloud.githubusercontent.com assets 18170760 26241854 8c530380 3c86 11e7 92c6 c9afd759eac0.png even tried grid search searching best parameters, maximum 0.5. anyone knows output always like that? enhance it. thanks advance!",1,keras accuracy increasing 50,"keras accuracy increasing 50 trying build binary classification algorithm output 0 1 dataset contains normal malicious network packets. dataset shape converting ip 's hexa decimal ! capture1 https cloud.githubusercontent.com assets 18170760 26241844 86366da2 3c86 11e7 9620 24e6dd6f74e0.png note final column output. keras model however, tried different optimizers, activation functions, number layers, accuracy reaching 0.5 ! capture https cloud.githubusercontent.com assets 18170760 26241854 8c530380 3c86 11e7 92c6 c9afd759eac0.png even tried grid search searching best parameters, maximum 0.5. anyone knows output always like that? enhance it. thanks advance!"
keras,9762,"turn shuffle training, speed become 1 3 . use fully connected network, 5m sample, 300 feature",1,shuffle slow using fcn,"shuffle slow using fcn turn shuffle training, speed become 1 3 . use fully connected network, 5m sample, 300 feature"
keras,9766,"hi, new keras two trials centered around mnist sequential model. first one used traditional approach second used functional one. performance wise, first 10 fold faster functional even that, first gave accuracy 97 second 50 using cpu based tensorflow backend. clues?",1,learning fiting performance,"learning fiting performance hi, new keras two trials centered around mnist sequential model. first one used traditional approach second used functional one. performance wise, first 10 fold faster functional even that, first gave accuracy 97 second 50 using cpu based tensorflow backend. clues?"
keras,5671,"put loss function within metric yet metric entirely different, example training output note output per sample two coordinate x . can't work different. different consistent even new epoch starts. train , like generator outputs multiple inputs shape shape , similar inputs. instance 128.",1,loss metric completely different,"loss metric completely different put loss function within metric yet metric entirely different, example training output note output per sample two coordinate x . can't work different. different consistent even new epoch starts. train , like generator outputs multiple inputs shape shape , similar inputs. instance 128."
keras,1063,"hello, training single lstm layer parameters number examples 27,000 size sample 2500, 1 , dtype float64 size target 250, 1 , dtype float64 batch size 64 activation linear use geforce gtx 970 gpu training really slow single epoch takes 678 seconds, seems really slow. idea could happening speed things up? thanks!",1,lstm training really slow,"lstm training really slow hello, training single lstm layer parameters number examples 27,000 size sample 2500, 1 , dtype float64 size target 250, 1 , dtype float64 batch size 64 activation linear use geforce gtx 970 gpu training really slow single epoch takes 678 seconds, seems really slow. idea could happening speed things up? thanks!"
keras,12843,"configuration library version python 3.6.8 gcc 7.3.0 tensorflow base tensorflow gpu 1.13.1 keras gpu keras base 2.2.4 theano 1.0.4 cudnn 7.3.1 cudatoolkit 10.0.130 machine configuration ubuntu 18.04.2 lts intel r xeon r cpu e5 2640 v4 2.40ghz 128gb ram cuda version 10.2 double geforce gtx 1080ti description used pre trained keras model lenet structure fashion mnist datasets tensorflow theano backends respectively, noticed big difference two backends. accuracy tensorflow 91.6 accuracy theano 40.6 . ! acc https user images.githubusercontent.com 17698785 58078171 abfebd00 7be0 11e9 8980 a3e2686cac16.png prediction results two backends shown below, respectively ! tensorflow result https user images.githubusercontent.com 17698785 58078197 bcaf3300 7be0 11e9 91a5 2ad46142c1ef.png ! theano result https user images.githubusercontent.com 17698785 58078200 bcaf3300 7be0 11e9 83f3 4ed88c93991d.png based results, problem seems occur backend theano. also tried localize problem, found inconsistency may happen first layer model, convolutional layer compared outputs layer two backends, found many values close . layer also used many models inconsistency. therefore, suspect problem happens specific context parameters . particular, constructed model layer using parameters shuffling weights , inconsistency still exists. attched code pretrained model .h5 file help reproduce inconsistency. inconsistency 1.zip https github.com keras team keras files 3201344 inconsistency 1.zip",1,inconsistency convolutional layer tensorflow theano backends,"inconsistency convolutional layer tensorflow theano backends configuration library version python 3.6.8 gcc 7.3.0 tensorflow base tensorflow gpu 1.13.1 keras gpu keras base 2.2.4 theano 1.0.4 cudnn 7.3.1 cudatoolkit 10.0.130 machine configuration ubuntu 18.04.2 lts intel r xeon r cpu e5 2640 v4 2.40ghz 128gb ram cuda version 10.2 double geforce gtx 1080ti description used pre trained keras model lenet structure fashion mnist datasets tensorflow theano backends respectively, noticed big difference two backends. accuracy tensorflow 91.6 accuracy theano 40.6 . ! acc https user images.githubusercontent.com 17698785 58078171 abfebd00 7be0 11e9 8980 a3e2686cac16.png prediction results two backends shown below, respectively ! tensorflow result https user images.githubusercontent.com 17698785 58078197 bcaf3300 7be0 11e9 91a5 2ad46142c1ef.png ! theano result https user images.githubusercontent.com 17698785 58078200 bcaf3300 7be0 11e9 83f3 4ed88c93991d.png based results, problem seems occur backend theano. also tried localize problem, found inconsistency may happen first layer model, convolutional layer compared outputs layer two backends, found many values close . layer also used many models inconsistency. therefore, suspect problem happens specific context parameters . particular, constructed model layer using parameters shuffling weights , inconsistency still exists. attched code pretrained model .h5 file help reproduce inconsistency. inconsistency 1.zip https github.com keras team keras files 3201344 inconsistency 1.zip"
keras,7724,input like means increase one metric means decrease means change metric. array 83 items 83 fields output labels array categorical array shows effect metrics single metric used following code loss 100 epochs decrease loss percentage?,1,decrease loss keras training usnig lstm,decrease loss keras training usnig lstm input like means increase one metric means decrease means change metric. array 83 items 83 fields output labels array categorical array shows effect metrics single metric used following code loss 100 epochs decrease loss percentage?
keras,12343,"hi! strange behavior, trying make transfer learning mobilenetv2 keras. use keras.utils.sequence class inheritant data generators. one train, one validation. train loss decreases, validation loss increases since training start. even use single generator training validation absolutely data . model https gist.github.com dzubape efeefa77fdb1901d8f99c201b54381df seems, validation loss could different training loss, like 2.5 vs 0.01 way thinking wrong?",1,"fit generator data, different loss","fit generator data, different loss hi! strange behavior, trying make transfer learning mobilenetv2 keras. use keras.utils.sequence class inheritant data generators. one train, one validation. train loss decreases, validation loss increases since training start. even use single generator training validation absolutely data . model https gist.github.com dzubape efeefa77fdb1901d8f99c201b54381df seems, validation loss could different training loss, like 2.5 vs 0.01 way thinking wrong?"
keras,6199,"application need 2 output trained network one final layer one intermediate layer . since model.predict gives output last layer, wrote theano function fetch output intermediate layer final layer. works fine, almost 20 times slower compared model.predict . anybody help understand theano function slower make faster.",1,teano.function slower compared model.predict,"teano.function slower compared model.predict application need 2 output trained network one final layer one intermediate layer . since model.predict gives output last layer, wrote theano function fetch output intermediate layer final layer. works fine, almost 20 times slower compared model.predict . anybody help understand theano function slower make faster."
keras,4171,"trying train simple mlp model numerical data binary classification, train see continues forever... view code question here, without data loading portion look output model, see predicts things tried tried several machines different hardware ubuntu versions, different combinations keras 1.0.7, 1.0.8, latest commit theano 0.8.2 latest commit versions, cpu gpu training result every case. tried removing sigmoid activation optimising mse instead, results loss 10 digits long. tried single dense neuron activation various simplified architecures , tried several different optimizers adam, sgd, rmsprop . matter change model, cannot seem get anything else. verified nans infs matrix, values binary. matrix passed keras numpy array. help would much appreciated, labouring problem hours without able fix it.",1,high loss 7 binary classification,"high loss 7 binary classification trying train simple mlp model numerical data binary classification, train see continues forever... view code question here, without data loading portion look output model, see predicts things tried tried several machines different hardware ubuntu versions, different combinations keras 1.0.7, 1.0.8, latest commit theano 0.8.2 latest commit versions, cpu gpu training result every case. tried removing sigmoid activation optimising mse instead, results loss 10 digits long. tried single dense neuron activation various simplified architecures , tried several different optimizers adam, sgd, rmsprop . matter change model, cannot seem get anything else. verified nans infs matrix, values binary. matrix passed keras numpy array. help would much appreciated, labouring problem hours without able fix it."
keras,13389,"system information written custom code opposed using example directory yes, custom keras sequence os platform distribution e.g., linux ubuntu 16.04 ubuntu 16.04 tensorflow backend yes yes tensorflow version 1.14.0 keras version 2.3.0 python version python 3.6.8 cuda cudnn version 10.1 gpu model memory asus dual geforce rtx 2080 ti 2 pieces obtain tensorflow version python c import tensorflow tf print tf.git version, tf.version obtain keras version python c 'import keras k print k. version ' describe current behavior model multiple outputs, hence loss output overall weighted loss. weighted sum validation loss different outputs add overall validation loss, val loss. describe expected behavior expecting val loss weighted sum validation loss model output. code reproduce issue info logs include logs source code would helpful diagnose problem. including tracebacks, please include full traceback. large logs files attached.",1,val loss making sense multiple outputs weighted loss,"val loss making sense multiple outputs weighted loss system information written custom code opposed using example directory yes, custom keras sequence os platform distribution e.g., linux ubuntu 16.04 ubuntu 16.04 tensorflow backend yes yes tensorflow version 1.14.0 keras version 2.3.0 python version python 3.6.8 cuda cudnn version 10.1 gpu model memory asus dual geforce rtx 2080 ti 2 pieces obtain tensorflow version python c import tensorflow tf print tf.git version, tf.version obtain keras version python c 'import keras k print k. version ' describe current behavior model multiple outputs, hence loss output overall weighted loss. weighted sum validation loss different outputs add overall validation loss, val loss. describe expected behavior expecting val loss weighted sum validation loss model output. code reproduce issue info logs include logs source code would helpful diagnose problem. including tracebacks, please include full traceback. large logs files attached."
keras,10832,"recently bought second gpu order speed neural network computations. use keras 2.2.2 tensorflow 1.9.0. run normal model sped one gpu might expected. makes model run 60 times faster compared cpu amd 1950x . use instruction combined power cards results slowdown factor 100 . happens models rnn, cnn dense. cards seem work, however, seem alternate work other. also cpu activity increase quite lot. using cards cpu activity one core 50 , using one card 8 10 . selecting gpu separately using usual speed occurs. cannot use multi gpu model case. select gpu multi gpu model slow occurs. printed topology see could find anything strange, far expert this. issue issue 9204. issue discusses models cause slowdown using multiple gpu's. far see issue related specific model. anyone idea exactly wrong setup? keras 2.2.2, tensorflow 1.9.0, python 3.5.5, ubuntu mate 18.04 nvidia drivers 390.48 problems ubuntu mate 16.04.4 nvidia drivers 390.130 . simple mnist code used benchmark setup.",1,slowdown factor 100 adding second gpu,"slowdown factor 100 adding second gpu recently bought second gpu order speed neural network computations. use keras 2.2.2 tensorflow 1.9.0. run normal model sped one gpu might expected. makes model run 60 times faster compared cpu amd 1950x . use instruction combined power cards results slowdown factor 100 . happens models rnn, cnn dense. cards seem work, however, seem alternate work other. also cpu activity increase quite lot. using cards cpu activity one core 50 , using one card 8 10 . selecting gpu separately using usual speed occurs. cannot use multi gpu model case. select gpu multi gpu model slow occurs. printed topology see could find anything strange, far expert this. issue issue 9204. issue discusses models cause slowdown using multiple gpu's. far see issue related specific model. anyone idea exactly wrong setup? keras 2.2.2, tensorflow 1.9.0, python 3.5.5, ubuntu mate 18.04 nvidia drivers 390.48 problems ubuntu mate 16.04.4 nvidia drivers 390.130 . simple mnist code used benchmark setup."
keras,8273,"idea training loss smooth validation loss noisy across epochs? ! image https user images.githubusercontent.com 4671752 32121045 4b16b5b8 bb31 11e7 86e0 8690ce9f867c.png implementing deep learning model diabetic retinopathy detection binary classification using data set fundus photographs provided kaggle competition 2 . using keras 2.0 tensorflow backend. data set big fit memory, using , randomly taking images training validation folders train model model.fit generator train generator, steps per epoch train generator.samples training batch size, epochs int config 'training' 'epochs' , validation data validation generator, validation steps validation generator.samples validation batch size, class weight none cnn architecture vgg16 dropout 0.5 last two fully connected layers, batch normalization first fully connected layer, data augmentation consisting flipping images horizontally vertically . training validation samples normalized using training set mean standard deviation. batch size 32. activation loss function . find implementation github 3 definitely nothing overfitting, tried highly regularized model behavior quite same. related sampling validation set? similar problem before? thanks!! 2 https www.kaggle.com c diabetic retinopathy detection 3 https github.com ignaciorlando cnn dr kaggle",1,noisy validation loss keras using fit generator,"noisy validation loss keras using fit generator idea training loss smooth validation loss noisy across epochs? ! image https user images.githubusercontent.com 4671752 32121045 4b16b5b8 bb31 11e7 86e0 8690ce9f867c.png implementing deep learning model diabetic retinopathy detection binary classification using data set fundus photographs provided kaggle competition 2 . using keras 2.0 tensorflow backend. data set big fit memory, using , randomly taking images training validation folders train model model.fit generator train generator, steps per epoch train generator.samples training batch size, epochs int config 'training' 'epochs' , validation data validation generator, validation steps validation generator.samples validation batch size, class weight none cnn architecture vgg16 dropout 0.5 last two fully connected layers, batch normalization first fully connected layer, data augmentation consisting flipping images horizontally vertically . training validation samples normalized using training set mean standard deviation. batch size 32. activation loss function . find implementation github 3 definitely nothing overfitting, tried highly regularized model behavior quite same. related sampling validation set? similar problem before? thanks!! 2 https www.kaggle.com c diabetic retinopathy detection 3 https github.com ignaciorlando cnn dr kaggle"
keras,11858,"hi, sure labelled bug, problematic. batchnormalization seems silently produce nan weights training training dataset size multiple . example, training dataset 1825, 401, 401, 3 , validation dataset 140, 401, 401, 3 , , , training apparently goes fine weights nans, e.g. think happens training dataset 1825 gets split sets . there's going set 1 training image, maybe work batchnormalization. inference trained model gives solution make sure number training images multiple .",1,batchnormalization produces nan weights without nan loss,"batchnormalization produces nan weights without nan loss hi, sure labelled bug, problematic. batchnormalization seems silently produce nan weights training training dataset size multiple . example, training dataset 1825, 401, 401, 3 , validation dataset 140, 401, 401, 3 , , , training apparently goes fine weights nans, e.g. think happens training dataset 1825 gets split sets . there's going set 1 training image, maybe work batchnormalization. inference trained model gives solution make sure number training images multiple ."
keras,8792,"please make sure boxes checked submit issue. issue implementation question, please ask question stackoverflow http stackoverflow.com questions tagged keras join keras slack channel https keras slack autojoin.herokuapp.com ask instead filing github issue. thank you! x check date master branch keras. update pip install git git github.com fchollet keras.git upgrade deps x running tensorflow, check date latest version. installation instructions found https www.tensorflow.org get started os setup . x running theano, check date master branch theano. update pip install git git github.com theano theano.git upgrade deps x provide link github gist python script reproduce issue copy script short . running keras 2.1.2 well keras github master validation accuracy achieved dogs vs. cats example chapter 5.3 dl w python book https github.com fchollet deep learning python notebooks blob master 5.3 using pretrained convnet.ipynb 90 . however, validation accuracy running keras 2.0.9 96 . code well cats vs. dogs images required reproduce following github repo script also inline https github.com jjallaire keras vgg16 accuracy one noteworthy thing keras 2.1.2 case training takes 22 seconds epoch whereas keras 2.0.9 50 seconds per epoch amazon p2.xlarge instance .",1,regression training accuracy vgg16 application,"regression training accuracy vgg16 application please make sure boxes checked submit issue. issue implementation question, please ask question stackoverflow http stackoverflow.com questions tagged keras join keras slack channel https keras slack autojoin.herokuapp.com ask instead filing github issue. thank you! x check date master branch keras. update pip install git git github.com fchollet keras.git upgrade deps x running tensorflow, check date latest version. installation instructions found https www.tensorflow.org get started os setup . x running theano, check date master branch theano. update pip install git git github.com theano theano.git upgrade deps x provide link github gist python script reproduce issue copy script short . running keras 2.1.2 well keras github master validation accuracy achieved dogs vs. cats example chapter 5.3 dl w python book https github.com fchollet deep learning python notebooks blob master 5.3 using pretrained convnet.ipynb 90 . however, validation accuracy running keras 2.0.9 96 . code well cats vs. dogs images required reproduce following github repo script also inline https github.com jjallaire keras vgg16 accuracy one noteworthy thing keras 2.1.2 case training takes 22 seconds epoch whereas keras 2.0.9 50 seconds per epoch amazon p2.xlarge instance ."
keras,5720,"trying train segmentation network. running issues trying use image augmentation. attached function sets generator, fits though think really need , zips label image generators. trying zip program hangs never moves on. playing around seems flow function. whenever call flow function runs indefinitely. made changes image.py file allow 3d manipulations, check doublechecked even gone back implemented changes clean copy image.py make sure changing anything effect process. image.py.zip https github.com fchollet keras files 836158 image.py.zip attached zip updated image.py. functions start left 2d counterparts, within random transform 3d manipulations located. anyone thoughts? thanks, anthony.",1,image augmentation flow hanging forever training,"image augmentation flow hanging forever training trying train segmentation network. running issues trying use image augmentation. attached function sets generator, fits though think really need , zips label image generators. trying zip program hangs never moves on. playing around seems flow function. whenever call flow function runs indefinitely. made changes image.py file allow 3d manipulations, check doublechecked even gone back implemented changes clean copy image.py make sure changing anything effect process. image.py.zip https github.com fchollet keras files 836158 image.py.zip attached zip updated image.py. functions start left 2d counterparts, within random transform 3d manipulations located. anyone thoughts? thanks, anthony."
keras,11866,"using 2d data classification problem using keras. defining keras model following returns compiled model following number parameters parameters reading documentation inspecting code https github.com keras team keras blob 88af7d0c97497b5c3a198ee9416b2accfbc72c36 keras layers core.py l880 input shape 1d going use last dimension input dimension. thus layer 1100 parameters 10 100 100 weights bias . understand dot product input weights performed. data 2d 5 10 data points, using 100 10 weights weights input values multiplied? change input data 5,10 500,10 something changes? always idea output neuron k something like x 1, x 2 previous outpus, however case 2d know happening here.",1,number parameters dense layer 2d input,"number parameters dense layer 2d input using 2d data classification problem using keras. defining keras model following returns compiled model following number parameters parameters reading documentation inspecting code https github.com keras team keras blob 88af7d0c97497b5c3a198ee9416b2accfbc72c36 keras layers core.py l880 input shape 1d going use last dimension input dimension. thus layer 1100 parameters 10 100 100 weights bias . understand dot product input weights performed. data 2d 5 10 data points, using 100 10 weights weights input values multiplied? change input data 5,10 500,10 something changes? always idea output neuron k something like x 1, x 2 previous outpus, however case 2d know happening here."
keras,3675,"hi, noticed use fit generator pickle safe true, number workers 1, max generator queue 2 python processes accumulate memory leading hanging execution due available memory 40gb ram . happening setting pickle safe false.",1,memory consumption using fit generator,"memory consumption using fit generator hi, noticed use fit generator pickle safe true, number workers 1, max generator queue 2 python processes accumulate memory leading hanging execution due available memory 40gb ram . happening setting pickle safe false."
keras,11356,"training lstm models, switching cudnnlstm improves training speed significantly gpu machines. training convlstm2d models extremely slow, plan even possible make sth like cudnnconvlstm2d?",1,convlstm2d slow training,"convlstm2d slow training training lstm models, switching cudnnlstm improves training speed significantly gpu machines. training convlstm2d models extremely slow, plan even possible make sth like cudnnconvlstm2d?"
keras,8289,"found use adam optimizer ,train loss beginning epoch drops soon slows like epoch 1 100 loss 70.0 60.0 55. ...... 40.0 39.95 39.90 39.89 ..... epoch 2 100 loss 30 25 20 ...... 18.0 17.9 ..... 17.75 17.73 epoch 3 100 loss 10 8 7 ...... 6 6.9 .....",1,"use adam optimizer ,train loss beginning epoch drops soon slows","use adam optimizer ,train loss beginning epoch drops soon slows found use adam optimizer ,train loss beginning epoch drops soon slows like epoch 1 100 loss 70.0 60.0 55. ...... 40.0 39.95 39.90 39.89 ..... epoch 2 100 loss 30 25 20 ...... 18.0 17.9 ..... 17.75 17.73 epoch 3 100 loss 10 8 7 ...... 6 6.9 ....."
keras,8802,"sharing layers siamese style, able synchronize dropped units. example, code below, effect. parameter effect either. shared layers synchronize dropped units default, otherwise shared meaningful way. output note 1. inference nothing dropped, loss 0, expected 2. dropout rate 0, loss 0 training, expected 3. note epochs 2,3,7,8,9 loss 4, understand all. maybe another bug, need investigate further.",1,"dropout layer shared siamese style, dropped units synchronized.","dropout layer shared siamese style, dropped units synchronized. sharing layers siamese style, able synchronize dropped units. example, code below, effect. parameter effect either. shared layers synchronize dropped units default, otherwise shared meaningful way. output note 1. inference nothing dropped, loss 0, expected 2. dropout rate 0, loss 0 training, expected 3. note epochs 2,3,7,8,9 loss 4, understand all. maybe another bug, need investigate further."
keras,1133,"use cnn new keras, eg keras 0.3.0, get quite slow cnn training using mnist cnn.py 2x faster cpu version. well, keras 0.2.0 got almost 60x faster cpu. understand. happend dec 01, 2015?",1,performance new keras reduce much?,"performance new keras reduce much? use cnn new keras, eg keras 0.3.0, get quite slow cnn training using mnist cnn.py 2x faster cpu version. well, keras 0.2.0 got almost 60x faster cpu. understand. happend dec 01, 2015?"
keras,10350,"hi, used basic character level seq2seq model example https github.com keras team keras blob master examples lstm seq2seq.py french english translation. accuracy measure seems comparing entire decoder output, i.e max decoder output length. would like accuracy represent characters till first occurrence stop character ' n' case compare predicted output padded target output. example consider example gives 63 character long decoder sequence i.e decoder target output one hot representation '' empty string represents zero padded input vector 'c', 'o', 'u', 'r', 'e', 'z', ' xe2', ' x80', ' xaf', '!', ' n', '', '', '', '' '', '', '', '', '', '', '' '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '' '', '', '', '', '' '', '', '', '', '', '', '', '', '', '', '', '' '', '', '', '', '', '', '', '' fitting, predicted decoder output gives output using argmax one hot representation 'c', 'o', 'u', 'r', 'e', 'z', ' xe2', ' x80', ' xaf', '!', ' n', 'i', 'r', ' ', '?', ' n', 'i', 'l', ' xc3', ' xa9', ' ', '?', ' n', 'i', ' ', 'd', 'e', 'u', 's', ' ', 'c', 'o', 'u', 'r', 'e', ' xe2', ' x80', ' xaf', '!', ' n', ' n', ' n', ' ', '!', ' n', ' n', 'i', ' ', 'c', 'h', 'i', 'e', ' xe2', ' x80', ' xaf', '!', ' n', ' n', ' n', ' ', 'd', 'i', 'r', 'e' keras accuracy 11 63 however, relevant accuracy 11 11 need model consider accuracy beyond stop character. way get correct accuracy ignoring padded output ?",1,incorrect accuracy measure padded target sequence seq2seq model,"incorrect accuracy measure padded target sequence seq2seq model hi, used basic character level seq2seq model example https github.com keras team keras blob master examples lstm seq2seq.py french english translation. accuracy measure seems comparing entire decoder output, i.e max decoder output length. would like accuracy represent characters till first occurrence stop character ' n' case compare predicted output padded target output. example consider example gives 63 character long decoder sequence i.e decoder target output one hot representation '' empty string represents zero padded input vector 'c', 'o', 'u', 'r', 'e', 'z', ' xe2', ' x80', ' xaf', '!', ' n', '', '', '', '' '', '', '', '', '', '', '' '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '' '', '', '', '', '' '', '', '', '', '', '', '', '', '', '', '', '' '', '', '', '', '', '', '', '' fitting, predicted decoder output gives output using argmax one hot representation 'c', 'o', 'u', 'r', 'e', 'z', ' xe2', ' x80', ' xaf', '!', ' n', 'i', 'r', ' ', '?', ' n', 'i', 'l', ' xc3', ' xa9', ' ', '?', ' n', 'i', ' ', 'd', 'e', 'u', 's', ' ', 'c', 'o', 'u', 'r', 'e', ' xe2', ' x80', ' xaf', '!', ' n', ' n', ' n', ' ', '!', ' n', ' n', 'i', ' ', 'c', 'h', 'i', 'e', ' xe2', ' x80', ' xaf', '!', ' n', ' n', ' n', ' ', 'd', 'i', 'r', 'e' keras accuracy 11 63 however, relevant accuracy 11 11 need model consider accuracy beyond stop character. way get correct accuracy ignoring padded output ?"
keras,4728,"new keras deep learning general. trying implementation visual attention based image caption generation based xu et. al https arxiv.org pdf 1502.03044.pdf created new class attentionlstm based existing lstm class. want retrieve value one states alpha weights features vectors , however whenever access end batch , always comes zero tensor. model follows sequence length 45 max sentence length sequence length 3 1 image, 1 start token, 1 end token output dim 512 annotation dim 512 word dim 512 annotation size 196 x inp input shape sequence length 1, vocab count z inp input shape annotation size, annotation dim, z mean input shape annotation dim, h dense dense output dim, input dim annotation dim, activation 'softmax' z mean c dense dense output dim, input dim annotation dim, activation 'softmax' z mean xt dense timedistributed dense word dim x inp alstm layer attentionlstm output dim word dim, z dim annotation dim, w regularizer l2 0.01 , u regularizer l2 0.01 , z regularizer l2 0.01 , dropout w 0.3, dropout u 0.3, dropout z 0.3, return sequences true alstm alstm layer xt dense, h dense, c dense, z inp tdense timedistributed dense vocab count alstm act activation 'softmax' tdense model model input x inp, z inp, z mean , output act model.compile optimizer 'rmsprop', loss 'categorical crossentropy', metrics 'accuracy' model.fit x train, train, batch size 1, nb epoch 1, verbose 1 attention following code function def step self, x, states prev h1 states 0 prev c1 states 1 proj z states 2 alphaz states 3 b u states 4 b w states 5 b z states 6 proj state k.dot prev h1, self.wd att proj z proj z proj state , none, proj list proj list.append proj z proj z k.tanh proj z alpha k.dot proj z, self.u att self.b2 att alpha shape alpha.shape alpha k.softmax alpha.reshape alpha shape 0 , alpha shape 1 alphaz alpha self.alphaz alpha z self.initial z alpha , , none .sum 1 remaing code lstm.step get alpha value, defined following function alphaz alstm layer.states 3 alpha func k.function x inp, z inp, z mean , alphaz al alpha func x train print al print statement always returns setting alpha zero . something wrong model way retrieve alpha ? better way get value ? know there's way make layer give multiple outputs x check date master branch keras. update pip install git git github.com fchollet keras.git upgrade deps x running theano, check date master branch theano. update pip install git git github.com theano theano.git upgrade deps x provide link github gist python script reproduce issue copy script short .",1,layer state value always zero,"layer state value always zero new keras deep learning general. trying implementation visual attention based image caption generation based xu et. al https arxiv.org pdf 1502.03044.pdf created new class attentionlstm based existing lstm class. want retrieve value one states alpha weights features vectors , however whenever access end batch , always comes zero tensor. model follows sequence length 45 max sentence length sequence length 3 1 image, 1 start token, 1 end token output dim 512 annotation dim 512 word dim 512 annotation size 196 x inp input shape sequence length 1, vocab count z inp input shape annotation size, annotation dim, z mean input shape annotation dim, h dense dense output dim, input dim annotation dim, activation 'softmax' z mean c dense dense output dim, input dim annotation dim, activation 'softmax' z mean xt dense timedistributed dense word dim x inp alstm layer attentionlstm output dim word dim, z dim annotation dim, w regularizer l2 0.01 , u regularizer l2 0.01 , z regularizer l2 0.01 , dropout w 0.3, dropout u 0.3, dropout z 0.3, return sequences true alstm alstm layer xt dense, h dense, c dense, z inp tdense timedistributed dense vocab count alstm act activation 'softmax' tdense model model input x inp, z inp, z mean , output act model.compile optimizer 'rmsprop', loss 'categorical crossentropy', metrics 'accuracy' model.fit x train, train, batch size 1, nb epoch 1, verbose 1 attention following code function def step self, x, states prev h1 states 0 prev c1 states 1 proj z states 2 alphaz states 3 b u states 4 b w states 5 b z states 6 proj state k.dot prev h1, self.wd att proj z proj z proj state , none, proj list proj list.append proj z proj z k.tanh proj z alpha k.dot proj z, self.u att self.b2 att alpha shape alpha.shape alpha k.softmax alpha.reshape alpha shape 0 , alpha shape 1 alphaz alpha self.alphaz alpha z self.initial z alpha , , none .sum 1 remaing code lstm.step get alpha value, defined following function alphaz alstm layer.states 3 alpha func k.function x inp, z inp, z mean , alphaz al alpha func x train print al print statement always returns setting alpha zero . something wrong model way retrieve alpha ? better way get value ? know there's way make layer give multiple outputs x check date master branch keras. update pip install git git github.com fchollet keras.git upgrade deps x running theano, check date master branch theano. update pip install git git github.com theano theano.git upgrade deps x provide link github gist python script reproduce issue copy script short ."
keras,11897,"reference https stackoverflow.com questions 53400472 keras model weights layers become nans , noticed nans present batch norm layers. must happening? problem training like vanishing gradient gradients exploding? nans placeholders biases? prior trying train siamese network triplet loss mentioned https stackoverflow.com questions 53400472 keras model weights layers become nans architecture loss sample weights listed https stackoverflow.com questions 53830547 batch norm layer weights parameters turning nans training siamese ne link gist code using training https gist.github.com sidgairo18 dca347edd4588484237a231d7dab9a63 please make sure boxes checked submit issue. issue implementation question , please ask question stackoverflow http stackoverflow.com questions tagged keras keras slack channel https keras slack autojoin.herokuapp.com instead opening github issue. thank you! yes check date master branch keras. update yes check version tensorflow date. installation instructions found https www.tensorflow.org get started os setup . yes provide link github gist python script reproduce issue copy script short .",1,batch norm layer weights parameters turning nans training siamese network triplet loss,"batch norm layer weights parameters turning nans training siamese network triplet loss reference https stackoverflow.com questions 53400472 keras model weights layers become nans , noticed nans present batch norm layers. must happening? problem training like vanishing gradient gradients exploding? nans placeholders biases? prior trying train siamese network triplet loss mentioned https stackoverflow.com questions 53400472 keras model weights layers become nans architecture loss sample weights listed https stackoverflow.com questions 53830547 batch norm layer weights parameters turning nans training siamese ne link gist code using training https gist.github.com sidgairo18 dca347edd4588484237a231d7dab9a63 please make sure boxes checked submit issue. issue implementation question , please ask question stackoverflow http stackoverflow.com questions tagged keras keras slack channel https keras slack autojoin.herokuapp.com instead opening github issue. thank you! yes check date master branch keras. update yes check version tensorflow date. installation instructions found https www.tensorflow.org get started os setup . yes provide link github gist python script reproduce issue copy script short ."
keras,2172,"hi, modelling simple text classification task 3 target labels. using word2vec embeddings input lstm layer. size sequence. word2vec dimension. 3d tensor shape simple vector labels like 0,2,1,0,1,0,... training model, always get accuracy scores 1 like could issue here? ps using keras 0.3.2",1,accuracy always 1 using lstm,"accuracy always 1 using lstm hi, modelling simple text classification task 3 target labels. using word2vec embeddings input lstm layer. size sequence. word2vec dimension. 3d tensor shape simple vector labels like 0,2,1,0,1,0,... training model, always get accuracy scores 1 like could issue here? ps using keras 0.3.2"
keras,3713,"hi, training deep recurrent model batches dataset time, manually slicing data calling model.fit it. noticed use larger dataset batch size, validation data , training slows quite bit, monitoring cpu usage see process seems io bound way one core constantly 100 , cores sporadically spike slightly, seemingly minibatch start end . reference, relevant part training code clarify model gets amount training data call, data view copy relevant chunk big matrix systems ram far fully used supply 2.5 whole dataset validation data, amount val data higher use larger dataset note though val testing seem slowing training, cpu usage poor epoch, testing larger dataset, time one chunk takes train. 200k samples, around 240 seconds per call, 2 million samples, goes around 1000 seconds. time epoch takes seems increasing call tried rmsprop sgd optimizers, sgd seems slow less, cpu usage still bad large dataset could slowing down? drop dataset size around 2 chunks, cpu usage basically perfect, ram seem problem. edit training time definitely appears slow epoch",1,training slows using larger dataset,"training slows using larger dataset hi, training deep recurrent model batches dataset time, manually slicing data calling model.fit it. noticed use larger dataset batch size, validation data , training slows quite bit, monitoring cpu usage see process seems io bound way one core constantly 100 , cores sporadically spike slightly, seemingly minibatch start end . reference, relevant part training code clarify model gets amount training data call, data view copy relevant chunk big matrix systems ram far fully used supply 2.5 whole dataset validation data, amount val data higher use larger dataset note though val testing seem slowing training, cpu usage poor epoch, testing larger dataset, time one chunk takes train. 200k samples, around 240 seconds per call, 2 million samples, goes around 1000 seconds. time epoch takes seems increasing call tried rmsprop sgd optimizers, sgd seems slow less, cpu usage still bad large dataset could slowing down? drop dataset size around 2 chunks, cpu usage basically perfect, ram seem problem. edit training time definitely appears slow epoch"
keras,9350,"running lstm keras default program using sentiment corpus jupyter notebook 5 hours getting results. show continue processing. gup error recovered. details system linux x86 64 operating system, nvidia driver version 384.111, gpus geforce gtx 960m gpu 0 run example code keras lstm first know process shall run code corpus datafile data pd.read csv ' home mazhar downloads sentiment.csv' keeping neccessary columns data data 'text','sentiment' model embed dim 128 lstm 196 model sequential model.add embedding max fatures, embed dim,input length x.shape 1 , dropout 0.2 model.add lstm lstm out, dropout u 0.2, dropout w 0.2 model.add dense 2,activation 'softmax' model.compile loss 'categorical crossentropy', optimizer 'adam',metrics 'accuracy' print model.summary please help guide resolve problem",1,lstm responding slow,"lstm responding slow running lstm keras default program using sentiment corpus jupyter notebook 5 hours getting results. show continue processing. gup error recovered. details system linux x86 64 operating system, nvidia driver version 384.111, gpus geforce gtx 960m gpu 0 run example code keras lstm first know process shall run code corpus datafile data pd.read csv ' home mazhar downloads sentiment.csv' keeping neccessary columns data data 'text','sentiment' model embed dim 128 lstm 196 model sequential model.add embedding max fatures, embed dim,input length x.shape 1 , dropout 0.2 model.add lstm lstm out, dropout u 0.2, dropout w 0.2 model.add dense 2,activation 'softmax' model.compile loss 'categorical crossentropy', optimizer 'adam',metrics 'accuracy' print model.summary please help guide resolve problem"
keras,8856,"found shift training loss, tried see shift, set model non trainable ! image https user images.githubusercontent.com 13257051 34256650 4dd550c6 e691 11e7 8942 5bd249f5c1ed.png choose ten images training samples also used validation data , set batch size 5 shuffle data.before training model, evaluate samples, result ! image https user images.githubusercontent.com 13257051 34256707 93529636 e691 11e7 8f7f fa3b04684517.png trained model actually could trained print fit history, result ! image https user images.githubusercontent.com 13257051 34256735 bace98cc e691 11e7 8b58 47f349909d8e.png found even model trained, validation loss never change, training loss different among three epochs. found set shuffle false, result right. wanna know bug shuffle make difference training.",1,training loss changed even trainable params zero,"training loss changed even trainable params zero found shift training loss, tried see shift, set model non trainable ! image https user images.githubusercontent.com 13257051 34256650 4dd550c6 e691 11e7 8942 5bd249f5c1ed.png choose ten images training samples also used validation data , set batch size 5 shuffle data.before training model, evaluate samples, result ! image https user images.githubusercontent.com 13257051 34256707 93529636 e691 11e7 8f7f fa3b04684517.png trained model actually could trained print fit history, result ! image https user images.githubusercontent.com 13257051 34256735 bace98cc e691 11e7 8b58 47f349909d8e.png found even model trained, validation loss never change, training loss different among three epochs. found set shuffle false, result right. wanna know bug shuffle make difference training."
keras,6808,"hi everyone, recently updated keras version 2.0.4 saw big drop performance compared keras 1.2.2. basically trying find mapping movie titles movie plot summaries called synopses using seq2seq lstms. model pretty fast keras 1.2.2, took lots time train last version. model sequential model.add embedding vocab size, embedding dim, input length max length, mask zero true model.add lstm 1024, return sequences true model.add lstm 1024, return sequences true model.add timedistributed dense vocab size, activation 'softmax' model.compile loss 'sparse categorical crossentropy', optimizer adam , metrics 'accuracy' change anything model code general keras 1.2.2 keras 2.0.4, trained nvidia quadro k6000 theano date cases simply ran update it. however, training much slower. using keras 1.2.2 using keras 2.0.4 example above, delta seconds training whole dataset, much much slower. anyone know wrong could point right direction ?",1,training much slower keras 2.0.4 keras 1.2.2,"training much slower keras 2.0.4 keras 1.2.2 hi everyone, recently updated keras version 2.0.4 saw big drop performance compared keras 1.2.2. basically trying find mapping movie titles movie plot summaries called synopses using seq2seq lstms. model pretty fast keras 1.2.2, took lots time train last version. model sequential model.add embedding vocab size, embedding dim, input length max length, mask zero true model.add lstm 1024, return sequences true model.add lstm 1024, return sequences true model.add timedistributed dense vocab size, activation 'softmax' model.compile loss 'sparse categorical crossentropy', optimizer adam , metrics 'accuracy' change anything model code general keras 1.2.2 keras 2.0.4, trained nvidia quadro k6000 theano date cases simply ran update it. however, training much slower. using keras 1.2.2 using keras 2.0.4 example above, delta seconds training whole dataset, much much slower. anyone know wrong could point right direction ?"
keras,10906,"came weird problem use keras lstm model. build single layer lstm try play it. found output model different single input multiple inputs, shown following code. def lstmtest training, latent dim 10 , time dim, input dim training.shape define input sequence process it. encoder inputs input shape time dim, input dim , name 'input' encoder lstm latent dim, return state false, name 'lstm' encoder outputs encoder encoder inputs model model encoder inputs, encoder outputs return model def trainingtest model, training, nb epoch 10, batch size 300 model.compile optimizer 'adam', loss 'mse', metrics 'acc' history model.fit training, training , 1, 10 , epochs nb epoch, batch size batch size, shuffle true, verbose 1, .history return history myvector 20 8 range 100 myvector np.array myvector lstmtest lstmtest myvector history trainingtest lstmtest, myvector vector myvector 2 res1 lstmtest.predict vector vector myvector 1 res2 lstmtest.predict vector res2 0 res1 0 array 0.0000000e 00, 0.0000000e 00, 0.0000000e 00, 5.8207661e 11, 0.0000000e 00, 2.3283064e 10, 0.0000000e 00, 0.0000000e 00, 0.0000000e 00, 0.0000000e 00 , dtype float32 change res2 res1 got expected result array 0., 0., 0., 0., 0., 0., 0., 0., 0., 0. , dtype float32 use tensorflow backend",1,keras lstm different output single input,"keras lstm different output single input came weird problem use keras lstm model. build single layer lstm try play it. found output model different single input multiple inputs, shown following code. def lstmtest training, latent dim 10 , time dim, input dim training.shape define input sequence process it. encoder inputs input shape time dim, input dim , name 'input' encoder lstm latent dim, return state false, name 'lstm' encoder outputs encoder encoder inputs model model encoder inputs, encoder outputs return model def trainingtest model, training, nb epoch 10, batch size 300 model.compile optimizer 'adam', loss 'mse', metrics 'acc' history model.fit training, training , 1, 10 , epochs nb epoch, batch size batch size, shuffle true, verbose 1, .history return history myvector 20 8 range 100 myvector np.array myvector lstmtest lstmtest myvector history trainingtest lstmtest, myvector vector myvector 2 res1 lstmtest.predict vector vector myvector 1 res2 lstmtest.predict vector res2 0 res1 0 array 0.0000000e 00, 0.0000000e 00, 0.0000000e 00, 5.8207661e 11, 0.0000000e 00, 2.3283064e 10, 0.0000000e 00, 0.0000000e 00, 0.0000000e 00, 0.0000000e 00 , dtype float32 change res2 res1 got expected result array 0., 0., 0., 0., 0., 0., 0., 0., 0., 0. , dtype float32 use tensorflow backend"
keras,667,"hi, wondering example lstm layer eventually feeds regression output layer. hard time putting one together. hoping feed one sample time, sample looks like input x,x,x,x , x,x,x,x , x,x,x,x , output way, would expect lstm layer takes 4 inputs 3 iterations produces output h feed regression layer. cheers",1,regression output lstm input layer,"regression output lstm input layer hi, wondering example lstm layer eventually feeds regression output layer. hard time putting one together. hoping feed one sample time, sample looks like input x,x,x,x , x,x,x,x , x,x,x,x , output way, would expect lstm layer takes 4 inputs 3 iterations produces output h feed regression layer. cheers"
keras,6300,"regression predicted values simple dense model keras v1.2.2 v2.0. code reproduce below. using keras v1.2.2 commit 4fa7e5d using keras april 17 v2.0 commit 73bf06fb023a8b37ddf2e2a168bbf920c7a6c766 using theano april 17 tot commit 388805f946685e86225cdf602eb8a4f0059f9667 running theano.cuda geforce gt 750m, cudnn v5105",1,regression initial dense model prediction keras v1.2.2 v2.0,"regression initial dense model prediction keras v1.2.2 v2.0 regression predicted values simple dense model keras v1.2.2 v2.0. code reproduce below. using keras v1.2.2 commit 4fa7e5d using keras april 17 v2.0 commit 73bf06fb023a8b37ddf2e2a168bbf920c7a6c766 using theano april 17 tot commit 388805f946685e86225cdf602eb8a4f0059f9667 running theano.cuda geforce gt 750m, cudnn v5105"
keras,2718,"anyone ideas validation accuracy curve would noisy? ! screen shot 2016 05 13 10 22 04 https cloud.githubusercontent.com assets 6086781 15250847 92ada084 18f4 11e6 9527 b0871e776065.png completeness, mention validation set systematically different training set transfer learning situation . provide details needed specific application model, interested kind generic explanation noisy accuracy curve.",1,noisy validation accuracy,"noisy validation accuracy anyone ideas validation accuracy curve would noisy? ! screen shot 2016 05 13 10 22 04 https cloud.githubusercontent.com assets 6086781 15250847 92ada084 18f4 11e6 9527 b0871e776065.png completeness, mention validation set systematically different training set transfer learning situation . provide details needed specific application model, interested kind generic explanation noisy accuracy curve."
keras,4256,"theano recently introduced https github.com theano theano pull 4915 cudnn5 support recurrent networks decided give spin keras lstm network. however, noticed marginal benefit seems likely noise anything else. reason seeing bigger performance improvements? cudnn 4, theano 0.8.2, keras 1.1.0 train 3000000 samples, validate 857222 samples epoch 1 100 3000000 3000000 781s loss 2.2539 acc 0.6299 val loss 1.6206 val acc 0.7274 cudnn 5, theano 0.9.0, keras 1.1.0 train 3000000 samples, validate 857222 samples epoch 1 100 3000000 3000000 765s loss 2.2588 acc 0.6289 val loss 1.6214 val acc 0.7274 cudnn 5, theano 0.9.0, keras 1.1.1 train 3000000 samples, validate 857222 samples epoch 1 100 3000000 3000000 761s loss 2.2578 acc 0.6292 val loss 1.6234 val acc 0.7271",1,cudnn 5 theano 0.9.0 improving lstm performance,"cudnn 5 theano 0.9.0 improving lstm performance theano recently introduced https github.com theano theano pull 4915 cudnn5 support recurrent networks decided give spin keras lstm network. however, noticed marginal benefit seems likely noise anything else. reason seeing bigger performance improvements? cudnn 4, theano 0.8.2, keras 1.1.0 train 3000000 samples, validate 857222 samples epoch 1 100 3000000 3000000 781s loss 2.2539 acc 0.6299 val loss 1.6206 val acc 0.7274 cudnn 5, theano 0.9.0, keras 1.1.0 train 3000000 samples, validate 857222 samples epoch 1 100 3000000 3000000 765s loss 2.2588 acc 0.6289 val loss 1.6214 val acc 0.7274 cudnn 5, theano 0.9.0, keras 1.1.1 train 3000000 samples, validate 857222 samples epoch 1 100 3000000 3000000 761s loss 2.2578 acc 0.6292 val loss 1.6234 val acc 0.7271"
keras,5285,"lot sample images three classes, labels wrong, trained classifier achieve good performance, correct wrong labels needs consume lot time. eliminate effect error labels, intend redefine loss function like calculating losses batch data, largest loss removed others batch. calculate mean remaining elements loss value batch. achieve goal, better way achieve purpose?",1,customize loss function way ignore samples large loss,"customize loss function way ignore samples large loss lot sample images three classes, labels wrong, trained classifier achieve good performance, correct wrong labels needs consume lot time. eliminate effect error labels, intend redefine loss function like calculating losses batch data, largest loss removed others batch. calculate mean remaining elements loss value batch. achieve goal, better way achieve purpose?"
keras,6825,"comment lines, change smaller number works fine mse loss 1.0 .",1,loss function ignore large masked values,"loss function ignore large masked values comment lines, change smaller number works fine mse loss 1.0 ."
keras,2730,"found fit generator quite slow. makes disk read speed major bottleneck network relatively shallow. using multiprocessing module instead threading one, able get significant improvements speed half time shown gist example https gist.github.com tdeboissiere 195dde7fddfcf622a82a895b90d2c800 . could look making pull request new implementation think speed gain worth it.",1,improving speed fit generator,"improving speed fit generator found fit generator quite slow. makes disk read speed major bottleneck network relatively shallow. using multiprocessing module instead threading one, able get significant improvements speed half time shown gist example https gist.github.com tdeboissiere 195dde7fddfcf622a82a895b90d2c800 . could look making pull request new implementation think speed gain worth it."
keras,3755,"training deep cnn 4 layers data. used categorical crossentropy loss function. training, training loss keeps decreasing training accuracy keeps increasing convergence. validation loss started increasing validation accuracy still improving. curves loss accuracy shown following figures ! acc cnn32p 32p 32p 32p adam1e 4 5000 https cloud.githubusercontent.com assets 8138843 18455543 3b15fc32 7910 11e6 93a5 72374837a78d.png ! loss cnn32p 32p 32p 32p adam1e 4 5000 https cloud.githubusercontent.com assets 8138843 18455542 3b14ebee 7910 11e6 873b ca2a4dc3dfe7.png also seems validation loss keep going train model epochs. anyone idea what's going here? thanks lot!",1,validation loss increases validation accuracy still improving,"validation loss increases validation accuracy still improving training deep cnn 4 layers data. used categorical crossentropy loss function. training, training loss keeps decreasing training accuracy keeps increasing convergence. validation loss started increasing validation accuracy still improving. curves loss accuracy shown following figures ! acc cnn32p 32p 32p 32p adam1e 4 5000 https cloud.githubusercontent.com assets 8138843 18455543 3b15fc32 7910 11e6 93a5 72374837a78d.png ! loss cnn32p 32p 32p 32p adam1e 4 5000 https cloud.githubusercontent.com assets 8138843 18455542 3b14ebee 7910 11e6 873b ca2a4dc3dfe7.png also seems validation loss keep going train model epochs. anyone idea what's going here? thanks lot!"
keras,12977,"model website 1 behaves exactly expected. defined . want convert defined make flexible future use. conversion, performance plummeted. original model find website following code conversion difference see count counts it, believe make model structure different. however, performance ! enter image description 2 2 performance converted ! enter image description 3 3 1 https www.tensorflow.org tutorials keras basic regression 2 https i.stack.imgur.com pdtra.png 3 https i.stack.imgur.com vfvxp.png anyone tell wrong? context read post https github.com keras team keras issues 8001 , code run cpu google colab code plot losses code train model exact models suggestion appreciated!",1,training performance different exact data architecture. difference using .sequential .model,"training performance different exact data architecture. difference using .sequential .model model website 1 behaves exactly expected. defined . want convert defined make flexible future use. conversion, performance plummeted. original model find website following code conversion difference see count counts it, believe make model structure different. however, performance ! enter image description 2 2 performance converted ! enter image description 3 3 1 https www.tensorflow.org tutorials keras basic regression 2 https i.stack.imgur.com pdtra.png 3 https i.stack.imgur.com vfvxp.png anyone tell wrong? context read post https github.com keras team keras issues 8001 , code run cpu google colab code plot losses code train model exact models suggestion appreciated!"
keras,3766,"run example code lstm networks uses imdb dataset keras. one find code following link. https github.com fchollet keras blob master examples imdb lstm.py url problem code progresses training loss decreases training accuracy increases expected validation accuracy fluctuates interval validation loss increases high value. attach part log training phase below. even observe training loss small 0.01 0.03 sometimes increases next epoch decreases again. mention seen epochs 75 77. general decreases. behaviour expected higher number epochs? mean fluctuations train loss train acc. expect training accuracy always increases 0.99 1 training loss always decreases. moreover, validation accuracy start maybe 0.4 raise example 0.8 end. validation accuracy improve epochs point waiting epochs? also test accuracy close 0.81 end. checked overfitting. could come explanation fluctuations val acc overfitting. may causing fluctuations? also tried data came situation. processed data similar way. mean training, validation test points processed logic ones example code. besides, think code takes last output lstm time steps sample feeding dense layer. want take mean max outputs time steps lstm layer dense layer? apply operations keras? far know return sequence true must used. combine outputs time steps sample obtain single representation sentence dense layer? help would appreciated. using theano backend. loading data... 25000 train sequences 25000 test sequences pad sequences samples x time x train shape 25000, 80 x test shape 25000, 80 build model... train... train 22500 samples, validate 2500 samples epoch 1 100 22500 22500 236s loss 0.5438 acc 0.7209 val loss 0.4305 val acc 0.8076 epoch 2 100 22500 22500 237s loss 0.3843 acc 0.8346 val loss 0.3791 val acc 0.8332 epoch 3 100 22500 22500 245s loss 0.3099 acc 0.8716 val loss 0.3736 val acc 0.8440 epoch 4 100 22500 22500 243s loss 0.2458 acc 0.9023 val loss 0.4206 val acc 0.8372 epoch 5 100 22500 22500 239s loss 0.2120 acc 0.9138 val loss 0.3844 val acc 0.8384 .... .... epoch 75 100 22500 22500 238s loss 0.0134 acc 0.9868 val loss 0.9045 val acc 0.8132 epoch 76 100 22500 22500 241s loss 0.0156 acc 0.9845 val loss 0.9078 val acc 0.8211 epoch 77 100 22500 22500 235s loss 0.0129 acc 0.9883 val loss 0.9105 val acc 0.8234",1,fluctuating validation accuracy lstm network different ways sentence representation,"fluctuating validation accuracy lstm network different ways sentence representation run example code lstm networks uses imdb dataset keras. one find code following link. https github.com fchollet keras blob master examples imdb lstm.py url problem code progresses training loss decreases training accuracy increases expected validation accuracy fluctuates interval validation loss increases high value. attach part log training phase below. even observe training loss small 0.01 0.03 sometimes increases next epoch decreases again. mention seen epochs 75 77. general decreases. behaviour expected higher number epochs? mean fluctuations train loss train acc. expect training accuracy always increases 0.99 1 training loss always decreases. moreover, validation accuracy start maybe 0.4 raise example 0.8 end. validation accuracy improve epochs point waiting epochs? also test accuracy close 0.81 end. checked overfitting. could come explanation fluctuations val acc overfitting. may causing fluctuations? also tried data came situation. processed data similar way. mean training, validation test points processed logic ones example code. besides, think code takes last output lstm time steps sample feeding dense layer. want take mean max outputs time steps lstm layer dense layer? apply operations keras? far know return sequence true must used. combine outputs time steps sample obtain single representation sentence dense layer? help would appreciated. using theano backend. loading data... 25000 train sequences 25000 test sequences pad sequences samples x time x train shape 25000, 80 x test shape 25000, 80 build model... train... train 22500 samples, validate 2500 samples epoch 1 100 22500 22500 236s loss 0.5438 acc 0.7209 val loss 0.4305 val acc 0.8076 epoch 2 100 22500 22500 237s loss 0.3843 acc 0.8346 val loss 0.3791 val acc 0.8332 epoch 3 100 22500 22500 245s loss 0.3099 acc 0.8716 val loss 0.3736 val acc 0.8440 epoch 4 100 22500 22500 243s loss 0.2458 acc 0.9023 val loss 0.4206 val acc 0.8372 epoch 5 100 22500 22500 239s loss 0.2120 acc 0.9138 val loss 0.3844 val acc 0.8384 .... .... epoch 75 100 22500 22500 238s loss 0.0134 acc 0.9868 val loss 0.9045 val acc 0.8132 epoch 76 100 22500 22500 241s loss 0.0156 acc 0.9845 val loss 0.9078 val acc 0.8211 epoch 77 100 22500 22500 235s loss 0.0129 acc 0.9883 val loss 0.9105 val acc 0.8234"
keras,10943,"hi, pre trained model try use. h5 weight file loaded keras. run using theano, output tensorflow cntk tensorflow cntk agree investigation, found attribute original backend file set plaidml understand one computational engines tensorflow. function need convert kernel engine saving.py, convert original backend lookup table. mean sue theano need convert weight keras lead inconsistent output. top1 accuracy tensorflow cntk around 91 44 theano.",1,inconsistent run using pre trained h5 weight file theano vs tensorflow cntk,"inconsistent run using pre trained h5 weight file theano vs tensorflow cntk hi, pre trained model try use. h5 weight file loaded keras. run using theano, output tensorflow cntk tensorflow cntk agree investigation, found attribute original backend file set plaidml understand one computational engines tensorflow. function need convert kernel engine saving.py, convert original backend lookup table. mean sue theano need convert weight keras lead inconsistent output. top1 accuracy tensorflow cntk around 91 44 theano."
keras,6339,"upgraded new verison keras noticing huge drop performance 200 seconds per epoch old keras 40,000 seconds per epoch new keras. attached two shots showing training old keras new. models architecture confirmed utilizing gpu training. coded functional api. problem change using different tensorflow backends 0.12 1.0 . side side tests, thinking may bug keras. training output keras 1.1.0 270 sec epoch ! kerasold https cloud.githubusercontent.com assets 25262161 25255979 b878359e 25fb 11e7 857a 2c37d7be4561.png training output keras 2.0.3 38000 sec epoch ! kerasnew https cloud.githubusercontent.com assets 25262161 25256006 d7bdf786 25fb 11e7 8df3 93f0bdbf1ea3.png",1,large differences performance keras 2.0.3 keras 1.1.0 potential bug?,"large differences performance keras 2.0.3 keras 1.1.0 potential bug? upgraded new verison keras noticing huge drop performance 200 seconds per epoch old keras 40,000 seconds per epoch new keras. attached two shots showing training old keras new. models architecture confirmed utilizing gpu training. coded functional api. problem change using different tensorflow backends 0.12 1.0 . side side tests, thinking may bug keras. training output keras 1.1.0 270 sec epoch ! kerasold https cloud.githubusercontent.com assets 25262161 25255979 b878359e 25fb 11e7 857a 2c37d7be4561.png training output keras 2.0.3 38000 sec epoch ! kerasnew https cloud.githubusercontent.com assets 25262161 25256006 d7bdf786 25fb 11e7 8df3 93f0bdbf1ea3.png"
keras,10947,"hi there, running macbook pro gpu . simple cnn similar https github.com keras team keras blob master examples mnist cnn.py example. finding faster total time running example individually. reason behavior? always faster run batches like this?",1,model.predict runs faster test examples passed batch vs one one.,"model.predict runs faster test examples passed batch vs one one. hi there, running macbook pro gpu . simple cnn similar https github.com keras team keras blob master examples mnist cnn.py example. finding faster total time running example individually. reason behavior? always faster run batches like this?"
keras,707,"far understand, seems accuracy take account possible timesteps masking . model, categorical accuracy computed correct approach take account however, see theano documentation http deeplearning.net software theano library tensor basic.html indexing work, indeed intention idea fix it?",1,wrong accuracy masking,"wrong accuracy masking far understand, seems accuracy take account possible timesteps masking . model, categorical accuracy computed correct approach take account however, see theano documentation http deeplearning.net software theano library tensor basic.html indexing work, indeed intention idea fix it?"
keras,6341,"trying understand behavior fit generator use dataset large fit memory, use following generator function imitate behavior fit keras version 1.2.0 1. load x data train data train memory 2. create simple sequential model 3. define generator function following way 4. use generator function fit generator fit generator works fine simple toy data, true data, result totally different using fit batch size dataset loss either get better generates crazy output. addition, fit generator faster even though expect slower. make things weird, fit generator faster fit even loading dataset hard disk similar generator function. see difference fit generator fit expect generator sampling replacement, think create big difference behaviour.",1,different behaviour fit fit generator,"different behaviour fit fit generator trying understand behavior fit generator use dataset large fit memory, use following generator function imitate behavior fit keras version 1.2.0 1. load x data train data train memory 2. create simple sequential model 3. define generator function following way 4. use generator function fit generator fit generator works fine simple toy data, true data, result totally different using fit batch size dataset loss either get better generates crazy output. addition, fit generator faster even though expect slower. make things weird, fit generator faster fit even loading dataset hard disk similar generator function. see difference fit generator fit expect generator sampling replacement, think create big difference behaviour."
keras,198,"training binary classification net loss function, found method managed correctly classify test cases, error become 0 thus loss function becomes nan. training steps cause future calls return nans well, certainly intended. ideally, net would stop training occurs. least, case loss function becoming nan handled gracefully.",1,nan accuracy reaches 1 logistic loss,"nan accuracy reaches 1 logistic loss training binary classification net loss function, found method managed correctly classify test cases, error become 0 thus loss function becomes nan. training steps cause future calls return nans well, certainly intended. ideally, net would stop training occurs. least, case loss function becoming nan handled gracefully."
keras,3782,"using theano backend keras running jupyter ipython notebook. everything working fine couples days ago, today notice running slow !, think running cpu much faster that. running mnist example https github.com fchollet keras blob master examples mnist cnn.py without making changes. batch 128 size takes long process, around 5 minutes something finish, next batch takes time almost longer, used run mnist example fast 170 seconds whole epoch . please help could figure problem note tried using anaconda tried using winpython problem occur. using windows 10, 16gb ram, nvidia gtx 660 2 gb, cudnn either enabled disabled problem occur theano issues thread https github.com theano theano issues 4974",1,theano running slow gpu,"theano running slow gpu using theano backend keras running jupyter ipython notebook. everything working fine couples days ago, today notice running slow !, think running cpu much faster that. running mnist example https github.com fchollet keras blob master examples mnist cnn.py without making changes. batch 128 size takes long process, around 5 minutes something finish, next batch takes time almost longer, used run mnist example fast 170 seconds whole epoch . please help could figure problem note tried using anaconda tried using winpython problem occur. using windows 10, 16gb ram, nvidia gtx 660 2 gb, cudnn either enabled disabled problem occur theano issues thread https github.com theano theano issues 4974"
keras,13521,"hello, noticed always got different results fit generator fit. specifically, noticed always got good result faster fit compared using fit generator. faster mean given epoch training model fit always gives better fit generator. also note make sure implemented generator properly including shuffling fit generator. also often noticed using fit gets better result compared using fit generator. curious observe behavior? so, fit fit generator have?",1,got different results using fit fit generator,"got different results using fit fit generator hello, noticed always got different results fit generator fit. specifically, noticed always got good result faster fit compared using fit generator. faster mean given epoch training model fit always gives better fit generator. also note make sure implemented generator properly including shuffling fit generator. also often noticed using fit gets better result compared using fit generator. curious observe behavior? so, fit fit generator have?"
keras,2259,"pretrained models often finetuned small datasets. use cases relevant add dropout prevent overfitting. however, adding recurrent dropout loaded model recompiling model effect all. modified demonstrate this. recurrent dropout 0.999, model able learn anything, actually starts overfitting dramatically gets accuracy 99 3 epochs . see code below.",1,bug recurrent dropout fails silenty set loaded model,"bug recurrent dropout fails silenty set loaded model pretrained models often finetuned small datasets. use cases relevant add dropout prevent overfitting. however, adding recurrent dropout loaded model recompiling model effect all. modified demonstrate this. recurrent dropout 0.999, model able learn anything, actually starts overfitting dramatically gets accuracy 99 3 epochs . see code below."
keras,724,"list sequential values. want feed rnn predict next value sequence. 0.43589744 0.44230769 0.49358974 ..., 0.71153846 0.70833333 0.69230769 keep getting accuracy 1.0. found similar issue classification methods used worked me. get decreasing loss accuracy always 1.0 fix this? model sequential model.add simplernn 1, 100 model.add dense 100, 1, activation sigmoid model.compile loss mean squared error , optimizer sgd epoch 0 1517 1517 0s loss 0.0726 acc 1.0000 val loss 0.0636 val acc 1.0000 epoch 1 1517 1517 0s loss 0.0720 acc 1.0000 val loss 0.0629 val acc 1.0000",1,predictions using rnns accuracy always 1.0,"predictions using rnns accuracy always 1.0 list sequential values. want feed rnn predict next value sequence. 0.43589744 0.44230769 0.49358974 ..., 0.71153846 0.70833333 0.69230769 keep getting accuracy 1.0. found similar issue classification methods used worked me. get decreasing loss accuracy always 1.0 fix this? model sequential model.add simplernn 1, 100 model.add dense 100, 1, activation sigmoid model.compile loss mean squared error , optimizer sgd epoch 0 1517 1517 0s loss 0.0726 acc 1.0000 val loss 0.0636 val acc 1.0000 epoch 1 1517 1517 0s loss 0.0720 acc 1.0000 val loss 0.0629 val acc 1.0000"
keras,1237,"hi, trying build bi directional gru gender recognition text. model follows model graph model.add input name 'input', input shape none,max features , dtype 'float' model.add node gru 100, activation 'sigmoid', inner activation 'hard sigmoid' , name 'forward', input 'input' backwards time direction model.add node gru 100, activation 'sigmoid', inner activation 'hard sigmoid', go backwards true , name 'backward', input 'input' model.add node dropout 0.5 , name 'first dropout', inputs 'forward','backward' model.add node dense 1, activation 'sigmoid' , name 'sigmoid', input 'first dropout' model.add output name 'output', input 'sigmoid' model.compile 'sgd', 'output' 'binary crossentropy' data 3d numpy array x,y,z x number samples, time dimension words document example z word2vec encoding word input dimension . x ints z floats. labels binary. unfortunately, train, cross entropy loss seems jump place, end test accuracy 0. idea is? thank much!",1,loss increases training progresses,"loss increases training progresses hi, trying build bi directional gru gender recognition text. model follows model graph model.add input name 'input', input shape none,max features , dtype 'float' model.add node gru 100, activation 'sigmoid', inner activation 'hard sigmoid' , name 'forward', input 'input' backwards time direction model.add node gru 100, activation 'sigmoid', inner activation 'hard sigmoid', go backwards true , name 'backward', input 'input' model.add node dropout 0.5 , name 'first dropout', inputs 'forward','backward' model.add node dense 1, activation 'sigmoid' , name 'sigmoid', input 'first dropout' model.add output name 'output', input 'sigmoid' model.compile 'sgd', 'output' 'binary crossentropy' data 3d numpy array x,y,z x number samples, time dimension words document example z word2vec encoding word input dimension . x ints z floats. labels binary. unfortunately, train, cross entropy loss seems jump place, end test accuracy 0. idea is? thank much!"
keras,8929,"posting issue since body stackoverflow seems know problem. assume bug post here. implementing variant cnn described paper 1 . problem loss decreasing understand why. said concerning accuracy stuck 0.5 less . problem 2 classes classification. using data website 2 suspected optimizer changed whithout improvements. pretty sure data using ok used lstm classifier fine. code keras.layers import embedding keras.layers import conv2d keras.models import sequential keras.layers import maxpooling2d keras.layers import reshape keras.layers import flatten keras.layers import dense keras.layers import dropout keras.layers import input keras import backend k keras.models import model import tensorflow tf sklearn import preprocessing import keras import numpy np import pdb using multiple filters config tf.configproto pre allocate memory allocate needed config.gpu options.allow growth true allow total half gpu memory allocated config.gpu options.per process gpu memory fraction 0.7 create session options specified. config.gpu options.per process gpu memory fraction 0.65 k.tensorflow backend.set session tf.session config config class classifier def init self,vocab,maxlen 75 self.model sequential self.embed dim 30 self.batch size 30 self.max sequence length maxlen self.nb labels 2 self.model sequential self.vocab vocab def fit self, x,y pdb.set trace mainin input shape self.max sequence length, , dtype 'int32', name 'main input' x embedding len self.vocab 2,self.embed dim,input length self.max sequence length mainin x reshape 1,self.max sequence length, self.embed dim x x1 conv2d 128, strides 2,kernel size 5 ,activation relu , padding 'same' x x1 maxpooling2d self.max sequence length 5 1,1 ,padding 'same' x1 x1 flatten x1 x2 conv2d 128, strides 2, kernel size 4, activation sigmoid , padding 'same' x x2 maxpooling2d self.max sequence length 4 1,1 ,padding 'same' x2 x2 flatten x2 x3 conv2d 128, strides 2, kernel size 3, activation tanh , padding 'same' x x3 maxpooling2d self.max sequence length 3 1,1 ,padding 'same' x3 x3 flatten x3 combinedx keras.layers.concatenate x1,x2,x3 ,axis 1 combinedx dense 64, activation relu combinedx combinedx dropout 0.2 combinedx output dense self.nb labels, activation sigmoid combinedx output dense 2, activation softmax combinedx output dense 1, activation sigmoid combinedx encoder preprocessing.labelencoder encoder.fit encoded encoder.transform labels keras.utils.to categorical encoded y, num classes 2 labels pdb.set trace inputs2 x self.model model inputs mainin, outputs output self.model.compile loss 'binary crossentropy', optimizer 'adam', metrics 'acc' self.model.compile loss 'binary crossentropy', optimizer 'rmsprop', metrics 'acc' self.model.fit inputs2,labels,epochs 7, batch size self.batch size def predict self, x return self.model.predict np.array x def predict proba self, x return self.model.predict np.array x , self.batch size code preprocess data loading file file2 pd.read csv sentiment analysis dataset.csv ,error bad lines false splitting train et test set sklearn.model selection import train test split text list file2.sentimenttext file2.groupby 'sentiment' .count train data,test data,train label,test label train test split text, file2.sentiment, test size 0.4, random state 42 buidling dictionary vocabdic dict document train data document document.split word document word vocabdic vocabdic word len vocabdic 1 vocabdic 'unk' len vocabdic 1 coding documents def codedocuments documents,dictionnary documentsarray list i,document enumerate documents templist list document document.split word document word vocabdic word vocabdic word else word vocabdic 'unk' templist.append word documentsarray.append templist return np.array documentsarray train docs codedocuments train data,vocabdic test docs codedocuments test data,vocabdic padding documents keras.preprocessing import sequence maxlen 75 train set sequence.pad sequences train docs, maxlen maxlen test set sequence.pad sequences test docs, maxlen maxlen calling model model classifier vocabdic,maxlen model.fit train set 50000 ,train label 50000 1 https arxiv.org abs 1408.5882 2 http thinknook.com twitter sentiment analysis training corpus dataset 2012 09 22",1,loss accuracy decreasing,"loss accuracy decreasing posting issue since body stackoverflow seems know problem. assume bug post here. implementing variant cnn described paper 1 . problem loss decreasing understand why. said concerning accuracy stuck 0.5 less . problem 2 classes classification. using data website 2 suspected optimizer changed whithout improvements. pretty sure data using ok used lstm classifier fine. code keras.layers import embedding keras.layers import conv2d keras.models import sequential keras.layers import maxpooling2d keras.layers import reshape keras.layers import flatten keras.layers import dense keras.layers import dropout keras.layers import input keras import backend k keras.models import model import tensorflow tf sklearn import preprocessing import keras import numpy np import pdb using multiple filters config tf.configproto pre allocate memory allocate needed config.gpu options.allow growth true allow total half gpu memory allocated config.gpu options.per process gpu memory fraction 0.7 create session options specified. config.gpu options.per process gpu memory fraction 0.65 k.tensorflow backend.set session tf.session config config class classifier def init self,vocab,maxlen 75 self.model sequential self.embed dim 30 self.batch size 30 self.max sequence length maxlen self.nb labels 2 self.model sequential self.vocab vocab def fit self, x,y pdb.set trace mainin input shape self.max sequence length, , dtype 'int32', name 'main input' x embedding len self.vocab 2,self.embed dim,input length self.max sequence length mainin x reshape 1,self.max sequence length, self.embed dim x x1 conv2d 128, strides 2,kernel size 5 ,activation relu , padding 'same' x x1 maxpooling2d self.max sequence length 5 1,1 ,padding 'same' x1 x1 flatten x1 x2 conv2d 128, strides 2, kernel size 4, activation sigmoid , padding 'same' x x2 maxpooling2d self.max sequence length 4 1,1 ,padding 'same' x2 x2 flatten x2 x3 conv2d 128, strides 2, kernel size 3, activation tanh , padding 'same' x x3 maxpooling2d self.max sequence length 3 1,1 ,padding 'same' x3 x3 flatten x3 combinedx keras.layers.concatenate x1,x2,x3 ,axis 1 combinedx dense 64, activation relu combinedx combinedx dropout 0.2 combinedx output dense self.nb labels, activation sigmoid combinedx output dense 2, activation softmax combinedx output dense 1, activation sigmoid combinedx encoder preprocessing.labelencoder encoder.fit encoded encoder.transform labels keras.utils.to categorical encoded y, num classes 2 labels pdb.set trace inputs2 x self.model model inputs mainin, outputs output self.model.compile loss 'binary crossentropy', optimizer 'adam', metrics 'acc' self.model.compile loss 'binary crossentropy', optimizer 'rmsprop', metrics 'acc' self.model.fit inputs2,labels,epochs 7, batch size self.batch size def predict self, x return self.model.predict np.array x def predict proba self, x return self.model.predict np.array x , self.batch size code preprocess data loading file file2 pd.read csv sentiment analysis dataset.csv ,error bad lines false splitting train et test set sklearn.model selection import train test split text list file2.sentimenttext file2.groupby 'sentiment' .count train data,test data,train label,test label train test split text, file2.sentiment, test size 0.4, random state 42 buidling dictionary vocabdic dict document train data document document.split word document word vocabdic vocabdic word len vocabdic 1 vocabdic 'unk' len vocabdic 1 coding documents def codedocuments documents,dictionnary documentsarray list i,document enumerate documents templist list document document.split word document word vocabdic word vocabdic word else word vocabdic 'unk' templist.append word documentsarray.append templist return np.array documentsarray train docs codedocuments train data,vocabdic test docs codedocuments test data,vocabdic padding documents keras.preprocessing import sequence maxlen 75 train set sequence.pad sequences train docs, maxlen maxlen test set sequence.pad sequences test docs, maxlen maxlen calling model model classifier vocabdic,maxlen model.fit train set 50000 ,train label 50000 1 https arxiv.org abs 1408.5882 2 http thinknook.com twitter sentiment analysis training corpus dataset 2012 09 22"
keras,7394,"https github.com fchollet keras blob master keras engine training.py l463 strange. code used weighting apply sample weighting weights none reduce score array ndim weight array ndim k.ndim score array weight ndim k.ndim weights score array k.mean score array, axis list range weight ndim, ndim score array weights score array k.mean k.cast k.not equal weights, 0 , k.floatx return k.mean score array think line score array k.mean k.cast k.not equal weights, 0 , k.floatx become score array k.mean weights would make weighted loss normalized. instance, n samples weights w k non zero weights, weighted loss sum loss w range n k. think instead sum loss w range n sum w .",1,normalization training weights,"normalization training weights https github.com fchollet keras blob master keras engine training.py l463 strange. code used weighting apply sample weighting weights none reduce score array ndim weight array ndim k.ndim score array weight ndim k.ndim weights score array k.mean score array, axis list range weight ndim, ndim score array weights score array k.mean k.cast k.not equal weights, 0 , k.floatx return k.mean score array think line score array k.mean k.cast k.not equal weights, 0 , k.floatx become score array k.mean weights would make weighted loss normalized. instance, n samples weights w k non zero weights, weighted loss sum loss w range n k. think instead sum loss w range n sum w ."
keras,7404,"z mean z log var look like same. ..... x input batch shape batch size, original dim h dense intermediate dim, activation 'relu' x z mean dense latent dim h z log var dense latent dim h ..... possible?",1,z mean z log var variational autoencoder.py,"z mean z log var variational autoencoder.py z mean z log var look like same. ..... x input batch shape batch size, original dim h dense intermediate dim, activation 'relu' x z mean dense latent dim h z log var dense latent dim h ..... possible?"
keras,7409,"platform i5 7500, 1080ti, newest keras backends, windows 10 model mnist, cnn https github.com minimaxir keras cntk benchmark blob master test files mnist cnn.py url model sequential model.add conv2d 20, 5, 5 , activation 'relu', input shape input shape model.add maxpooling2d pool size 2, 2 model.add conv2d 40, 5, 5 , activation 'relu' model.add maxpooling2d pool size 2, 2 model.add dropout 0.5 model.add flatten model.add dense 128, activation 'relu' model.add dropout 0.5 model.add dense num classes, activation 'softmax' model takes 2s per epoch tensorflow, 3s cntk 11s theano. simple code train mnist takes 1.3s per epoch theano, 2.0s tensorflow 3.1s cntk.",1,keras slow theano,"keras slow theano platform i5 7500, 1080ti, newest keras backends, windows 10 model mnist, cnn https github.com minimaxir keras cntk benchmark blob master test files mnist cnn.py url model sequential model.add conv2d 20, 5, 5 , activation 'relu', input shape input shape model.add maxpooling2d pool size 2, 2 model.add conv2d 40, 5, 5 , activation 'relu' model.add maxpooling2d pool size 2, 2 model.add dropout 0.5 model.add flatten model.add dense 128, activation 'relu' model.add dropout 0.5 model.add dense num classes, activation 'softmax' model takes 2s per epoch tensorflow, 3s cntk 11s theano. simple code train mnist takes 1.3s per epoch theano, 2.0s tensorflow 3.1s cntk."
keras,3826,"created feature vectors irregular multivariate time series additionally contains static metadata features. converted values one hot encoded representation continuous time series values via binning established thresholds one hot encoding . time order irregular intervals concatenate together like following simplified example pad length zeros fundamental question actually appropriate data representation? read approach creating stationary time series representations usually involve creating standardized distance metrics. run lit similar simple approach. tried lstm models appropriate time series slow low accuracy case , find treating traditional binary classification problem gives really exceptional accuracy task even dropout, l2 regularization applied . advice insight would appreciated!",1,keras binary classification time series static metadata,"keras binary classification time series static metadata created feature vectors irregular multivariate time series additionally contains static metadata features. converted values one hot encoded representation continuous time series values via binning established thresholds one hot encoding . time order irregular intervals concatenate together like following simplified example pad length zeros fundamental question actually appropriate data representation? read approach creating stationary time series representations usually involve creating standardized distance metrics. run lit similar simple approach. tried lstm models appropriate time series slow low accuracy case , find treating traditional binary classification problem gives really exceptional accuracy task even dropout, l2 regularization applied . advice insight would appreciated!"
keras,249,"mnist example seems running 2x slower gpu tesla k10.g1.8gb cpu. run profiling seems spending large amount time gpufromhost. would indicate data loaded onto gpu shared variables fact can't really tell keras code would taking place thus minibatch must transferred gpu iteration training. yet examples give instructions running gpu, thus keras presumably optimized gpu. missing something here?",1,mnist example 2x slower gpu,"mnist example 2x slower gpu mnist example seems running 2x slower gpu tesla k10.g1.8gb cpu. run profiling seems spending large amount time gpufromhost. would indicate data loaded onto gpu shared variables fact can't really tell keras code would taking place thus minibatch must transferred gpu iteration training. yet examples give instructions running gpu, thus keras presumably optimized gpu. missing something here?"
keras,6906,"application need 2 output trained network one final layer one intermediate layer . since model.predict gives output last layer, wrote theano function fetch output intermediate layer final layer. works fine, almost 20 times slower compared model.predict . anybody help understand theano function slower make faster.",1,teano.function slower compared model.predict,"teano.function slower compared model.predict application need 2 output trained network one final layer one intermediate layer . since model.predict gives output last layer, wrote theano function fetch output intermediate layer final layer. works fine, almost 20 times slower compared model.predict . anybody help understand theano function slower make faster."
keras,3325,"one many users issues passing pre trained embedding matrix keras embedding layer. code currently looks like following first row pre trained matrix contains random vector special token 'pad'. token assigned index 0 corpus assume index special meaning lookup. second row matrix contains trained vector 'unk' token returned whenever word found vocabulary 'pad' ! 'unk' . embedding layer input dim , number rows matrix. output dim , dimension word vectors. input length fixed number words padded sentences padding 0 . mask zero argument set false matrix already vector assigned first row 'pad' token index 0 . running small training set 200 labeled sentences 3 labels accuracy around 55 training validation, prediction completely fails even training set. almost sentences therein labeled same. comes questions 1. reasonable use small training set predict labels sentences given pre trained word embeddnig matrix? 2. tried passing pre trained embedding matrix produces results terms accuracy predicted labels. make sure matrix used lstm classifier? 3. necessary zero vector 'pad' token random like did? 4. observe accuracy highly correlated proportion labels training set. think information help debug issue? 5. could add complete example documentation multiclass classification sentences given word vectors? seems many people struggling get working. please let know need anything else order help issue.",1,misclassification small training set pre trained word vectors,"misclassification small training set pre trained word vectors one many users issues passing pre trained embedding matrix keras embedding layer. code currently looks like following first row pre trained matrix contains random vector special token 'pad'. token assigned index 0 corpus assume index special meaning lookup. second row matrix contains trained vector 'unk' token returned whenever word found vocabulary 'pad' ! 'unk' . embedding layer input dim , number rows matrix. output dim , dimension word vectors. input length fixed number words padded sentences padding 0 . mask zero argument set false matrix already vector assigned first row 'pad' token index 0 . running small training set 200 labeled sentences 3 labels accuracy around 55 training validation, prediction completely fails even training set. almost sentences therein labeled same. comes questions 1. reasonable use small training set predict labels sentences given pre trained word embeddnig matrix? 2. tried passing pre trained embedding matrix produces results terms accuracy predicted labels. make sure matrix used lstm classifier? 3. necessary zero vector 'pad' token random like did? 4. observe accuracy highly correlated proportion labels training set. think information help debug issue? 5. could add complete example documentation multiclass classification sentences given word vectors? seems many people struggling get working. please let know need anything else order help issue."
keras,13053,"trying build lstm architecture predict sickness rate. actually stuck 40 accuracy, new machine learning tried several tips like changing optimzer, layer node number dropout value without improving. could guys help advice. x array composed 10 columns array one column sickness rate model output . evaluate thank advance",1,improve lstm accuracy,"improve lstm accuracy trying build lstm architecture predict sickness rate. actually stuck 40 accuracy, new machine learning tried several tips like changing optimzer, layer node number dropout value without improving. could guys help advice. x array composed 10 columns array one column sickness rate model output . evaluate thank advance"
keras,11006,"hi, using keras lstm time series prediction let time series n . network input time shifted vector 0 , 1 , 2 etc.... network training, calculate one step prediction, add prediction input vector first place release last value. get prediction e.g. 50 future points. problem is, predict method slow case. noticed, prediction time change little changing network size small networks e.g. 1 50 lstm cells . know, prediction whole array input vectors significantly faster, want. tried change keras backend cntk theano, worse tensorflow. possibility speed up? adding fast predict method one input vector? would way use tensorflow cntk directly without running keras? model use example function multiple step prediction know, converting array lists backwards optimal keras version 2.2.2 tensorflow version 1.5.0 problems newer versions profiling shows, bottle neck ist pywrap tensorflow internal.tf run method.",1,predict point point slow,"predict point point slow hi, using keras lstm time series prediction let time series n . network input time shifted vector 0 , 1 , 2 etc.... network training, calculate one step prediction, add prediction input vector first place release last value. get prediction e.g. 50 future points. problem is, predict method slow case. noticed, prediction time change little changing network size small networks e.g. 1 50 lstm cells . know, prediction whole array input vectors significantly faster, want. tried change keras backend cntk theano, worse tensorflow. possibility speed up? adding fast predict method one input vector? would way use tensorflow cntk directly without running keras? model use example function multiple step prediction know, converting array lists backwards optimal keras version 2.2.2 tensorflow version 1.5.0 problems newer versions profiling shows, bottle neck ist pywrap tensorflow internal.tf run method."
keras,11014,"using model multiple outputs getting nan loss despite outputs loss seems valid. screenshot attached. behavior remains change loss functions tried standard 'mae', 'mse', 'categorical crossentropy' customized loss functions well . behavior remains change optimizers tries adam sgd momentum . advice would appreciated. multiple loss seem converge though screenshot attached . ! image https user images.githubusercontent.com 25052915 44705551 a2a85000 aaa7 11e8 9b94 be5a7b3f0311.png ! image https user images.githubusercontent.com 25052915 44705587 bb186a80 aaa7 11e8 818b 7f7a4478c5cc.png loss val loss nan ! image https user images.githubusercontent.com 25052915 44705694 06cb1400 aaa8 11e8 8a43 5ee17f353eed.png",1,nan loss although multiple outputs loss nan,"nan loss although multiple outputs loss nan using model multiple outputs getting nan loss despite outputs loss seems valid. screenshot attached. behavior remains change loss functions tried standard 'mae', 'mse', 'categorical crossentropy' customized loss functions well . behavior remains change optimizers tries adam sgd momentum . advice would appreciated. multiple loss seem converge though screenshot attached . ! image https user images.githubusercontent.com 25052915 44705551 a2a85000 aaa7 11e8 9b94 be5a7b3f0311.png ! image https user images.githubusercontent.com 25052915 44705587 bb186a80 aaa7 11e8 818b 7f7a4478c5cc.png loss val loss nan ! image https user images.githubusercontent.com 25052915 44705694 06cb1400 aaa8 11e8 8a43 5ee17f353eed.png"
keras,4365,"noticed embedding layer tensorflow backend converting sparse gradient updates dense ones killing performance, well gobbling lots memory. making unusable large scale problem large embedding layer. script makes model single large embedding layer using keras tensorflow directly. keras, takes 2.3 seconds batch uses 9 gb memory training. tensorflow takes 20 ms batch 100x faster uses 4 g memory. using tensorflow 0.11.0rc2 master branch keras. outputs tensorflow outputs",1,embedding tensorflow slow converts indices dense gradients,"embedding tensorflow slow converts indices dense gradients noticed embedding layer tensorflow backend converting sparse gradient updates dense ones killing performance, well gobbling lots memory. making unusable large scale problem large embedding layer. script makes model single large embedding layer using keras tensorflow directly. keras, takes 2.3 seconds batch uses 9 gb memory training. tensorflow takes 20 ms batch 100x faster uses 4 g memory. using tensorflow 0.11.0rc2 master branch keras. outputs tensorflow outputs"
keras,270,"see code below, behavior batch normalization seems dependent batch samples. given sample always classified exactly mode 0 least . note single item batch, output model.predict arr1 1 identical output model.predict arr1 1 . read source code thousand times cannot find reason happen. got working theory issue running mean running std mutable within get output . values also inspectable stored tensor value issue makes batch normalization fairly useless cannot guarantee sample always generate prediction.",1,inconsistent behavior batch normalization layer,"inconsistent behavior batch normalization layer see code below, behavior batch normalization seems dependent batch samples. given sample always classified exactly mode 0 least . note single item batch, output model.predict arr1 1 identical output model.predict arr1 1 . read source code thousand times cannot find reason happen. got working theory issue running mean running std mutable within get output . values also inspectable stored tensor value issue makes batch normalization fairly useless cannot guarantee sample always generate prediction."
keras,13582,"hi! training small keras model 32k params dataset composed 50000 samples, stored disk hdd . sample size 249.1 kb. validation dataset 8000 samples. use custom data generator training unpickles data disk feeds model. epoch last 390 iterations batch size 128 plus 64 iters validation . set fit generator. also use threadsafe iterations. using ubuntu 16.04 python 3, keras 2.2.4 tensorflow gpu 1.13.1. pc 32 gb ram 15 used 32 gb swapping memory 5 used . problem training, first 250 iterations first epoch last seconds later training slowed down. gpu usage 0 disk usage . first epoch lasts 550 sec. second one 1700 sec. problem related hdd disk usage? cannot find reason slowing down.",1,training slows fixed num iterations,"training slows fixed num iterations hi! training small keras model 32k params dataset composed 50000 samples, stored disk hdd . sample size 249.1 kb. validation dataset 8000 samples. use custom data generator training unpickles data disk feeds model. epoch last 390 iterations batch size 128 plus 64 iters validation . set fit generator. also use threadsafe iterations. using ubuntu 16.04 python 3, keras 2.2.4 tensorflow gpu 1.13.1. pc 32 gb ram 15 used 32 gb swapping memory 5 used . problem training, first 250 iterations first epoch last seconds later training slowed down. gpu usage 0 disk usage . first epoch lasts 550 sec. second one 1700 sec. problem related hdd disk usage? cannot find reason slowing down."
keras,13585,"system information written custom code opposed using example directory os platform distribution e.g., linux ubuntu 16.04 centos linux release 7.2.1511 core tensorflow backend yes yes tensorflow version v1.12.3 0 g41e0a4f56c 1.12.3 keras version 2.1.6 tf python version python 3.6.5 cuda cudnn version 9.0.176 7.6.3.30 gpu model memory tesla p100 pcie 16gb x4 model shown below, many bn layers. test set 128 n, 40, 40, 1 , batch size 128 normal run code single gpu however, use multiple gpusmulti gpu num, weight bn layer becomes nan. strangely, seems related gpu num gpu num 2, weights become nan 129th step training, gpu num 4, weights become nan 65th step training, whether change batch size 64, 128, 512, behaviors still exist code solve problem",1,batchnormalization produces nan weights without nan loss duplicate,"batchnormalization produces nan weights without nan loss duplicate system information written custom code opposed using example directory os platform distribution e.g., linux ubuntu 16.04 centos linux release 7.2.1511 core tensorflow backend yes yes tensorflow version v1.12.3 0 g41e0a4f56c 1.12.3 keras version 2.1.6 tf python version python 3.6.5 cuda cudnn version 9.0.176 7.6.3.30 gpu model memory tesla p100 pcie 16gb x4 model shown below, many bn layers. test set 128 n, 40, 40, 1 , batch size 128 normal run code single gpu however, use multiple gpusmulti gpu num, weight bn layer becomes nan. strangely, seems related gpu num gpu num 2, weights become nan 129th step training, gpu num 4, weights become nan 65th step training, whether change batch size 64, 128, 512, behaviors still exist code solve problem"
keras,7954,"conclusion observation keras cosine proximity stuck 1 3 noted numerous post, keras seriously currently issue cosine proximity https github.com fchollet keras issues 3031 https github.com fchollet keras issues 5046 code jupyter notebook simple test printed result true cosine proximity actually 0.9986, keras shows near 1 3. course keras would use negative cosine proximity minimization purpose, 0.9986.., case, trust outcome metric keras cosine proximity",1,wrong result cosine proximity keras 2.0.8,"wrong result cosine proximity keras 2.0.8 conclusion observation keras cosine proximity stuck 1 3 noted numerous post, keras seriously currently issue cosine proximity https github.com fchollet keras issues 3031 https github.com fchollet keras issues 5046 code jupyter notebook simple test printed result true cosine proximity actually 0.9986, keras shows near 1 3. course keras would use negative cosine proximity minimization purpose, 0.9986.., case, trust outcome metric keras cosine proximity"
keras,13601,"system information tensorflow backend yes yes tensorflow version 1.13.2 keras version 2.3.1 python version 3.6 cuda cudnn version 10.0 describe current behavior training network multiple output layers 3 case , observe wrong validation loss describe expected behavior validation loss look like one training. even training, loss sum outputs, dimension. problem keras 2.2.4. problem pop version 2.2.5 upwards. unfortunately, can't go back 2.2.4, need new test begin callback. code reproduce issue provide reproducible test case bare minimum necessary generate problem. info logs",1,wrong validation loss multi output architecture,"wrong validation loss multi output architecture system information tensorflow backend yes yes tensorflow version 1.13.2 keras version 2.3.1 python version 3.6 cuda cudnn version 10.0 describe current behavior training network multiple output layers 3 case , observe wrong validation loss describe expected behavior validation loss look like one training. even training, loss sum outputs, dimension. problem keras 2.2.4. problem pop version 2.2.5 upwards. unfortunately, can't go back 2.2.4, need new test begin callback. code reproduce issue provide reproducible test case bare minimum necessary generate problem. info logs"
keras,6447,"trying learn regression problem. data mostly one hot encoded categorical variables, one continuous. target output probability 0 1 . code sure seems learning something ! hist https cloud.githubusercontent.com assets 3537118 25560256 935f61a0 2d04 11e7 8406 14281273edb4.png print preds 0 10 gives even though print evals gives loss mse even call training data. tried regularization, regularization, different optimizers, different learning rates, mean std normalization, less depth, depth, result. ideas?",1,model.predict gives output inputs,"model.predict gives output inputs trying learn regression problem. data mostly one hot encoded categorical variables, one continuous. target output probability 0 1 . code sure seems learning something ! hist https cloud.githubusercontent.com assets 3537118 25560256 935f61a0 2d04 11e7 8406 14281273edb4.png print preds 0 10 gives even though print evals gives loss mse even call training data. tried regularization, regularization, different optimizers, different learning rates, mean std normalization, less depth, depth, result. ideas?"
keras,4404,"hello, run example mnist cnn cpu win7. result bad, fixed test accuracy 0.098 even 12 epochs. new keras, want know debug it. thx suggestion. packages updated latest version keras version 1.1.1 theano version 0.8.2 log train 60000 samples, validate 10000 samples epoch 1 12 60000 60000 5584s loss nan acc 0.0988 val loss nan val acc 0.0980 epoch 2 12 60000 60000 5578s loss nan acc 0.0987 val loss nan val acc 0.0980 epoch 3 12 60000 60000 5577s loss nan acc 0.0987 val loss nan val acc 0.0980 epoch 4 12 60000 60000 5556s loss nan acc 0.0987 val loss nan val acc 0.0980 epoch 5 12 60000 60000 5559s loss nan acc 0.0987 val loss nan val acc 0.0980 epoch 6 12 60000 60000 5549s loss nan acc 0.0987 val loss nan val acc 0.0980 epoch 7 12 60000 60000 5550s loss nan acc 0.0987 val loss nan val acc 0.0980 epoch 8 12 60000 60000 5546s loss nan acc 0.0987 val loss nan val acc 0.0980 epoch 9 12 60000 60000 5548s loss nan acc 0.0987 val loss nan val acc 0.0980 epoch 10 12 60000 60000 5550s loss nan acc 0.0987 val loss nan val acc 0.0980 epoch 11 12 60000 60000 5549s loss nan acc 0.0987 val loss nan val acc 0.0980 epoch 12 12 60000 60000 5565s loss nan acc 0.0987 val loss nan val acc 0.0980 test score nan test accuracy 0.098",1,low accuracy example mnist cnn,"low accuracy example mnist cnn hello, run example mnist cnn cpu win7. result bad, fixed test accuracy 0.098 even 12 epochs. new keras, want know debug it. thx suggestion. packages updated latest version keras version 1.1.1 theano version 0.8.2 log train 60000 samples, validate 10000 samples epoch 1 12 60000 60000 5584s loss nan acc 0.0988 val loss nan val acc 0.0980 epoch 2 12 60000 60000 5578s loss nan acc 0.0987 val loss nan val acc 0.0980 epoch 3 12 60000 60000 5577s loss nan acc 0.0987 val loss nan val acc 0.0980 epoch 4 12 60000 60000 5556s loss nan acc 0.0987 val loss nan val acc 0.0980 epoch 5 12 60000 60000 5559s loss nan acc 0.0987 val loss nan val acc 0.0980 epoch 6 12 60000 60000 5549s loss nan acc 0.0987 val loss nan val acc 0.0980 epoch 7 12 60000 60000 5550s loss nan acc 0.0987 val loss nan val acc 0.0980 epoch 8 12 60000 60000 5546s loss nan acc 0.0987 val loss nan val acc 0.0980 epoch 9 12 60000 60000 5548s loss nan acc 0.0987 val loss nan val acc 0.0980 epoch 10 12 60000 60000 5550s loss nan acc 0.0987 val loss nan val acc 0.0980 epoch 11 12 60000 60000 5549s loss nan acc 0.0987 val loss nan val acc 0.0980 epoch 12 12 60000 60000 5565s loss nan acc 0.0987 val loss nan val acc 0.0980 test score nan test accuracy 0.098"
keras,13621,"following last two lines model use predictions model output, model learn anything, remove predictions layer add activations parameter outputs dense layer, works fine.",1,using output separate acitvation layer causing poor accuracy.,"using output separate acitvation layer causing poor accuracy. following last two lines model use predictions model output, model learn anything, remove predictions layer add activations parameter outputs dense layer, works fine."
keras,3894,"got strange question. train two layers cnn using .flow directory , training accuracy high, validation accuracy low. following code ,very simple. keras.preprocessing.image import imagedatagenerator keras.models import sequential keras.layers import convolution2d, maxpooling2d keras.layers import activation, dropout, flatten, dense keras.callbacks import earlystopping model sequential model.add convolution2d 32, 5,5, input shape 3,28,28 model.add activation 'relu' model.add maxpooling2d pool size 2,2 model.add convolution2d 32, 3,3 model.add activation 'relu' model.add maxpooling2d pool size 2,2 model.add flatten model.add dense 128 model.add activation 'relu' model.add dropout 0.5 model.add dense 10 model.add activation 'softmax' train datagen imagedatagenerator rescale 1. 255, shear range 0.2, zoom range 0.2, horizontal flip true test datagen imagedatagenerator rescale 1. 255 train generator train datagen.flow directory r'c users zhx desktop mnist train', target size 28,28 , classes '0','1','2','3','4','5','6','7','8','9' , batch size 60, class mode 'categorical', shuffle true validation generator test datagen.flow directory r'c users zhx desktop mnist test', target size 28, 28 , classes '0','1','2','3','4','5','6','7','8','9' , batch size 100, class mode 'categorical', shuffle true model.compile loss 'categorical crossentropy', optimizer 'rmsprop', metrics 'accuracy' early stopping earlystopping monitor 'val loss', patience 2 model.fit generator train generator, samples per epoch 60000, nb epoch 10, validation data validation generator, callbacks early stopping ,nb val samples 10000 json string model.to json open r'c users zhx desktop mnistmodel mnistcnn arc.json','w' .write json string model.save weights r'c users zhx desktop mnistmodel mnistcnn weights.h5' score model.evaluate generator validation generator, 10000 print 'test score ', score 0 print 'test accuracy ', score 1 log last using theano backend. found 60000 images belonging 10 classes. found 10000 images belonging 10 classes. epoch 1 10 60 60000 .............................. eta 812s loss 2.2756 acc 0.1667 120 60000 .............................. eta 690s loss 2.2897 acc 0.1667 180 60000 .............................. eta 688s loss 2.2728 acc 0.1833 240 60000 .............................. eta 721s loss 2.2692 acc 0.1792 300 60000 .............................. eta 768s loss 2.2633 acc 0.1800 360 60000 .............................. eta 833s loss 2.2561 acc 0.1750 420 60000 .............................. eta 884s loss 2.2437 acc 0.1786 warning warnings module file c winpython 64bit 3.4.4.4qt5 python 3.4.4.amd64 lib site packages keras callbacks.py , line 67 delta median userwarning method batch end slow compared batch update 0.484438 . check callbacks. 480 60000 .............................. eta 1002s loss 2.2285 acc 0.2042 540 60000 .............................. eta 978s loss 2.2102 acc 0.2241 600 60000 .............................. eta 979s loss 2.1920 acc 0.2317 660 60000 .............................. eta 1013s loss 2.1655 acc 0.2439 warning warnings module file c winpython 64bit 3.4.4.4qt5 python 3.4.4.amd64 lib site packages keras callbacks.py , line 67 delta median userwarning method batch end slow compared batch update 0.578522 . check callbacks. 720 60000 .............................. eta 1057s loss 2.1447 acc 0.2514 780 60000 .............................. eta 1038s loss 2.1188 acc 0.2577 840 60000 .............................. eta 1029s loss 2.1058 acc 0.2631 ......... 59400 60000 . eta 11s loss 0.1535 acc 0.9568 59460 60000 . eta 10s loss 0.1537 acc 0.9567 59520 60000 . eta 9s loss 0.1537 acc 0.9568 warning warnings module file c winpython 64bit 3.4.4.4qt5 python 3.4.4.amd64 lib site packages keras callbacks.py , line 67 delta median userwarning method batch end slow compared batch update 0.705150 . check callbacks. 59580 60000 . eta 8s loss 0.1536 acc 0.9567 warning warnings module file c winpython 64bit 3.4.4.4qt5 python 3.4.4.amd64 lib site packages keras callbacks.py , line 67 delta median userwarning method batch end slow compared batch update 0.606548 . check callbacks. 59640 60000 . eta 6s loss 0.1536 acc 0.9567 59700 60000 . eta 5s loss 0.1535 acc 0.9568 59760 60000 . eta 4s loss 0.1535 acc 0.9568 59820 60000 . eta 3s loss 0.1535 acc 0.9568 warning warnings module file c winpython 64bit 3.4.4.4qt5 python 3.4.4.amd64 lib site packages keras callbacks.py , line 67 delta median userwarning method batch end slow compared batch update 0.461924 . check callbacks. 59880 60000 . eta 2s loss 0.1535 acc 0.9567 59940 60000 . eta 1s loss 0.1535 acc 0.9567 60000 60000 1189s loss 0.1535 acc 0.9567 val loss 8.0909 val acc 0.1783 test score 8.09085823536 test accuracy 0.178299999908 one give suggestion? thanks.",1,"training accuracy high, validation accuracy low?","training accuracy high, validation accuracy low? got strange question. train two layers cnn using .flow directory , training accuracy high, validation accuracy low. following code ,very simple. keras.preprocessing.image import imagedatagenerator keras.models import sequential keras.layers import convolution2d, maxpooling2d keras.layers import activation, dropout, flatten, dense keras.callbacks import earlystopping model sequential model.add convolution2d 32, 5,5, input shape 3,28,28 model.add activation 'relu' model.add maxpooling2d pool size 2,2 model.add convolution2d 32, 3,3 model.add activation 'relu' model.add maxpooling2d pool size 2,2 model.add flatten model.add dense 128 model.add activation 'relu' model.add dropout 0.5 model.add dense 10 model.add activation 'softmax' train datagen imagedatagenerator rescale 1. 255, shear range 0.2, zoom range 0.2, horizontal flip true test datagen imagedatagenerator rescale 1. 255 train generator train datagen.flow directory r'c users zhx desktop mnist train', target size 28,28 , classes '0','1','2','3','4','5','6','7','8','9' , batch size 60, class mode 'categorical', shuffle true validation generator test datagen.flow directory r'c users zhx desktop mnist test', target size 28, 28 , classes '0','1','2','3','4','5','6','7','8','9' , batch size 100, class mode 'categorical', shuffle true model.compile loss 'categorical crossentropy', optimizer 'rmsprop', metrics 'accuracy' early stopping earlystopping monitor 'val loss', patience 2 model.fit generator train generator, samples per epoch 60000, nb epoch 10, validation data validation generator, callbacks early stopping ,nb val samples 10000 json string model.to json open r'c users zhx desktop mnistmodel mnistcnn arc.json','w' .write json string model.save weights r'c users zhx desktop mnistmodel mnistcnn weights.h5' score model.evaluate generator validation generator, 10000 print 'test score ', score 0 print 'test accuracy ', score 1 log last using theano backend. found 60000 images belonging 10 classes. found 10000 images belonging 10 classes. epoch 1 10 60 60000 .............................. eta 812s loss 2.2756 acc 0.1667 120 60000 .............................. eta 690s loss 2.2897 acc 0.1667 180 60000 .............................. eta 688s loss 2.2728 acc 0.1833 240 60000 .............................. eta 721s loss 2.2692 acc 0.1792 300 60000 .............................. eta 768s loss 2.2633 acc 0.1800 360 60000 .............................. eta 833s loss 2.2561 acc 0.1750 420 60000 .............................. eta 884s loss 2.2437 acc 0.1786 warning warnings module file c winpython 64bit 3.4.4.4qt5 python 3.4.4.amd64 lib site packages keras callbacks.py , line 67 delta median userwarning method batch end slow compared batch update 0.484438 . check callbacks. 480 60000 .............................. eta 1002s loss 2.2285 acc 0.2042 540 60000 .............................. eta 978s loss 2.2102 acc 0.2241 600 60000 .............................. eta 979s loss 2.1920 acc 0.2317 660 60000 .............................. eta 1013s loss 2.1655 acc 0.2439 warning warnings module file c winpython 64bit 3.4.4.4qt5 python 3.4.4.amd64 lib site packages keras callbacks.py , line 67 delta median userwarning method batch end slow compared batch update 0.578522 . check callbacks. 720 60000 .............................. eta 1057s loss 2.1447 acc 0.2514 780 60000 .............................. eta 1038s loss 2.1188 acc 0.2577 840 60000 .............................. eta 1029s loss 2.1058 acc 0.2631 ......... 59400 60000 . eta 11s loss 0.1535 acc 0.9568 59460 60000 . eta 10s loss 0.1537 acc 0.9567 59520 60000 . eta 9s loss 0.1537 acc 0.9568 warning warnings module file c winpython 64bit 3.4.4.4qt5 python 3.4.4.amd64 lib site packages keras callbacks.py , line 67 delta median userwarning method batch end slow compared batch update 0.705150 . check callbacks. 59580 60000 . eta 8s loss 0.1536 acc 0.9567 warning warnings module file c winpython 64bit 3.4.4.4qt5 python 3.4.4.amd64 lib site packages keras callbacks.py , line 67 delta median userwarning method batch end slow compared batch update 0.606548 . check callbacks. 59640 60000 . eta 6s loss 0.1536 acc 0.9567 59700 60000 . eta 5s loss 0.1535 acc 0.9568 59760 60000 . eta 4s loss 0.1535 acc 0.9568 59820 60000 . eta 3s loss 0.1535 acc 0.9568 warning warnings module file c winpython 64bit 3.4.4.4qt5 python 3.4.4.amd64 lib site packages keras callbacks.py , line 67 delta median userwarning method batch end slow compared batch update 0.461924 . check callbacks. 59880 60000 . eta 2s loss 0.1535 acc 0.9567 59940 60000 . eta 1s loss 0.1535 acc 0.9567 60000 60000 1189s loss 0.1535 acc 0.9567 val loss 8.0909 val acc 0.1783 test score 8.09085823536 test accuracy 0.178299999908 one give suggestion? thanks."
keras,11070,"hi all, found problem occur tensorflow forced use cpu think implies tensorflow bug keras bug, maybe issue closed. first off, using keras that's distributed tensorflow 1.10.0 let know opened issue repo instead. using sequence sequence model based keras blogpost https blog.keras.io ten minute introduction sequence sequence learning keras.html wrapped fairly complicated object although issue also occurs simplified version linked . create new model gridsearch clearing tf session graph gets big slows training starts accuracy either 0 70 . pair screenshots show mean good https i.imgur.com 7mt5siv.png bad https i.imgur.com mz3ndcb.png see first screenshot, accuracy low trending upwards. second, accuracy two models starts 70 increase another model starts 3 also increase . happens whether create new, blank models load pretrained weights new models . check date master branch keras. update pip install git git github.com keras team keras.git upgrade deps x running tensorflow, check date latest version. installation instructions found https www.tensorflow.org get started os setup . running theano, check date master branch theano. update pip install git git github.com theano theano.git upgrade deps x provide link github gist python script reproduce issue copy script short . minimal gist reproduces problem https gist.github.com chrisswinchatt 97304761e9f875dfd34e3339891a5475",1,accuracy oscillates 0 70 creating new models,"accuracy oscillates 0 70 creating new models hi all, found problem occur tensorflow forced use cpu think implies tensorflow bug keras bug, maybe issue closed. first off, using keras that's distributed tensorflow 1.10.0 let know opened issue repo instead. using sequence sequence model based keras blogpost https blog.keras.io ten minute introduction sequence sequence learning keras.html wrapped fairly complicated object although issue also occurs simplified version linked . create new model gridsearch clearing tf session graph gets big slows training starts accuracy either 0 70 . pair screenshots show mean good https i.imgur.com 7mt5siv.png bad https i.imgur.com mz3ndcb.png see first screenshot, accuracy low trending upwards. second, accuracy two models starts 70 increase another model starts 3 also increase . happens whether create new, blank models load pretrained weights new models . check date master branch keras. update pip install git git github.com keras team keras.git upgrade deps x running tensorflow, check date latest version. installation instructions found https www.tensorflow.org get started os setup . running theano, check date master branch theano. update pip install git git github.com theano theano.git upgrade deps x provide link github gist python script reproduce issue copy script short . minimal gist reproduces problem https gist.github.com chrisswinchatt 97304761e9f875dfd34e3339891a5475"
keras,6470,"guys upgraded keras theano keras 2.0 theano 9.0. tested gpu works fine, test script ok. also installed cudnn 5110 works fine too.,but script runs slowly. even used configuration theano make performance better forced use dnn, use float32 warn float 64 , made optimizations, learning process goes long anyway. cnn 16 layers 2 fc layers . keras 1, 1 epoch took 1m, keras2.0 takes 3.24m .is idea make speed better? almost tests related theano gpu run without problem.",1,"keras 2.0 slow, even theano 0.9","keras 2.0 slow, even theano 0.9 guys upgraded keras theano keras 2.0 theano 9.0. tested gpu works fine, test script ok. also installed cudnn 5110 works fine too.,but script runs slowly. even used configuration theano make performance better forced use dnn, use float32 warn float 64 , made optimizations, learning process goes long anyway. cnn 16 layers 2 fc layers . keras 1, 1 epoch took 1m, keras2.0 takes 3.24m .is idea make speed better? almost tests related theano gpu run without problem."
keras,9543,"hi everyone, personal project, bi gpu slower mono gpu. therefore, wanted check problem project something else. using cifar10 resnet.py model, added model compilation annnnnnd 1 gpu, 1 epoch 27 sec 2 gpus, 1 epoch 32 sec ideas ? thanks !",1,cifar10 resnet.py multi gpu slower mono gpu,"cifar10 resnet.py multi gpu slower mono gpu hi everyone, personal project, bi gpu slower mono gpu. therefore, wanted check problem project something else. using cifar10 resnet.py model, added model compilation annnnnnd 1 gpu, 1 epoch 27 sec 2 gpus, 1 epoch 32 sec ideas ? thanks !"
keras,5960,"hi guys, executed lstm dataset 7 epochs. inputs input shape 40, , dtype 'int32' embs embedding 19449, embedding vecor length, input length 40, weights w1 ,mask zero true,trainable false embsyo embs inputs lstm1 lstm 300,activation 'tanh', recurrent activation 'sigmoid' embsyo dense 5, activation 'softmax' lstm1 model model input inputs, output model.compile loss 'categorical crossentropy', optimizer 'rmsprop', metrics 'accuracy' however, time run model, get different accuracy test dataset. like one time get 32 next time get 26 . keep epochs fixed 7 even removed dropouts avoid randomness. way get keras give atleast similar accuracy percentage time run code? thanks advance",1,lack consistency keras model,"lack consistency keras model hi guys, executed lstm dataset 7 epochs. inputs input shape 40, , dtype 'int32' embs embedding 19449, embedding vecor length, input length 40, weights w1 ,mask zero true,trainable false embsyo embs inputs lstm1 lstm 300,activation 'tanh', recurrent activation 'sigmoid' embsyo dense 5, activation 'softmax' lstm1 model model input inputs, output model.compile loss 'categorical crossentropy', optimizer 'rmsprop', metrics 'accuracy' however, time run model, get different accuracy test dataset. like one time get 32 next time get 26 . keep epochs fixed 7 even removed dropouts avoid randomness. way get keras give atleast similar accuracy percentage time run code? thanks advance"
keras,10063,"want use vgg code. accuracy loss changing. solve problem? dataset medical image predicting idc non idc code. result here, epoch 1 8 1299 1298 27s 21ms step loss 0.5580 acc 0.7216 val loss 0.5227 val acc 0.7386 epoch 2 8 1299 1298 27s 21ms step loss 0.5260 acc 0.7466 val loss 0.5321 val acc 0.7298 epoch 3 8 1299 1298 27s 21ms step loss 0.5175 acc 0.7512 val loss 0.5170 val acc 0.7412 epoch 4 8 1299 1298 27s 21ms step loss 0.5166 acc 0.7556 val loss 0.5086 val acc 0.7528 epoch 5 8 1299 1298 27s 21ms step loss 0.5141 acc 0.7562 val loss 0.5017 val acc 0.7572 epoch 6 8 1299 1298 27s 21ms step loss 0.5119 acc 0.7602 val loss 0.5061 val acc 0.7515 epoch 7 8 1299 1298 27s 21ms step loss 0.5090 acc 0.7591 val loss 0.4999 val acc 0.7611 epoch 8 8 1299 1298 27s 21ms step loss 0.5100 acc 0.7624 val loss 0.5043 val acc 0.7539 keras cnn 1c accuracy 0.7538994800234126 please make sure boxes checked submit issue. issue implementation question, please ask question stackoverflow http stackoverflow.com questions tagged keras join keras slack channel https keras slack autojoin.herokuapp.com ask instead filing github issue. thank you! x check date master branch keras. update pip install git git github.com keras team keras.git upgrade deps x running tensorflow, check date latest version. installation instructions found https www.tensorflow.org get started os setup . x running theano, check date master branch theano. update pip install git git github.com theano theano.git upgrade deps x provide link github gist python script reproduce issue copy script short .",1,accuracy loss changing,"accuracy loss changing want use vgg code. accuracy loss changing. solve problem? dataset medical image predicting idc non idc code. result here, epoch 1 8 1299 1298 27s 21ms step loss 0.5580 acc 0.7216 val loss 0.5227 val acc 0.7386 epoch 2 8 1299 1298 27s 21ms step loss 0.5260 acc 0.7466 val loss 0.5321 val acc 0.7298 epoch 3 8 1299 1298 27s 21ms step loss 0.5175 acc 0.7512 val loss 0.5170 val acc 0.7412 epoch 4 8 1299 1298 27s 21ms step loss 0.5166 acc 0.7556 val loss 0.5086 val acc 0.7528 epoch 5 8 1299 1298 27s 21ms step loss 0.5141 acc 0.7562 val loss 0.5017 val acc 0.7572 epoch 6 8 1299 1298 27s 21ms step loss 0.5119 acc 0.7602 val loss 0.5061 val acc 0.7515 epoch 7 8 1299 1298 27s 21ms step loss 0.5090 acc 0.7591 val loss 0.4999 val acc 0.7611 epoch 8 8 1299 1298 27s 21ms step loss 0.5100 acc 0.7624 val loss 0.5043 val acc 0.7539 keras cnn 1c accuracy 0.7538994800234126 please make sure boxes checked submit issue. issue implementation question, please ask question stackoverflow http stackoverflow.com questions tagged keras join keras slack channel https keras slack autojoin.herokuapp.com ask instead filing github issue. thank you! x check date master branch keras. update pip install git git github.com keras team keras.git upgrade deps x running tensorflow, check date latest version. installation instructions found https www.tensorflow.org get started os setup . x running theano, check date master branch theano. update pip install git git github.com theano theano.git upgrade deps x provide link github gist python script reproduce issue copy script short ."
keras,13136,"trying train cnn using frames portray shooting ball basket. aim network able classify result hit miss correctly. train network, training accuracy increases slowly reaches 100 , validation accuracy remains around 65 important mention 65 percentage shots miss label anyone experience similar problem? using pytorch resnet18 tried architectures well gave result . frames jpg images sie 224. optimizer, adam sgd gave result thank advance",1,"training accuracy increases, val accuracy stays","training accuracy increases, val accuracy stays trying train cnn using frames portray shooting ball basket. aim network able classify result hit miss correctly. train network, training accuracy increases slowly reaches 100 , validation accuracy remains around 65 important mention 65 percentage shots miss label anyone experience similar problem? using pytorch resnet18 tried architectures well gave result . frames jpg images sie 224. optimizer, adam sgd gave result thank advance"
keras,12625,"80 gpu memory get's full loading pre trained xception model. deleting model , memory get empty flush. also used codes like k.clear session , gc.collect , tf.reset default graph , del model none worked. gpu properties say's 85 memory full. nothing flush gpu memory except numba.cuda.close allow use gpu again. way clear restarting kernel rerun code. looking script code add code allow use code loop clear gpu every loop. part code image input input shape 224, 224, 3 base model xception input tensor image input, include top false,weights 'imagenet' base model.compile loss 'categorical crossentropy',optimizer 'adadelta',metrics 'accuracy' hist base model.fit x,y,epochs 2 system information written custom code windows 10 64 bit tensorflow installed conda install tensorflow gpu tensorflow version 1.3 python version 3.6 cuda cudnn version 9.2 gpu model memory asus gtx 1060 6gb",1,clearing gpu memory keras,"clearing gpu memory keras 80 gpu memory get's full loading pre trained xception model. deleting model , memory get empty flush. also used codes like k.clear session , gc.collect , tf.reset default graph , del model none worked. gpu properties say's 85 memory full. nothing flush gpu memory except numba.cuda.close allow use gpu again. way clear restarting kernel rerun code. looking script code add code allow use code loop clear gpu every loop. part code image input input shape 224, 224, 3 base model xception input tensor image input, include top false,weights 'imagenet' base model.compile loss 'categorical crossentropy',optimizer 'adadelta',metrics 'accuracy' hist base model.fit x,y,epochs 2 system information written custom code windows 10 64 bit tensorflow installed conda install tensorflow gpu tensorflow version 1.3 python version 3.6 cuda cudnn version 9.2 gpu model memory asus gtx 1060 6gb"
keras,1360,"theano keras fresher, want learn , think interesting helpful. following question confuses one week. can't work try ways mentioned before. want sentiment analysis texts three classes. train word2vec dim 600 gensim. train data 10475 sequences different length. label shape 10475,3 setting maxlen sequence 200, every sequence converted 200 600 2d array.if sequence's length less 200, remaining values filled 0 padding , resulting rows zeroes. feed lstm, lstm code following model.fit train,label,batch size 100,nb epoch 4,verbose 1,shuffle true,validation split 0.1,show accuracy true getting loss nan train 9430 samples, validate 1048 samples epoch 1 4 9430 9430 99s loss nan acc 0.2992 val loss nan val acc 0.1355 epoch 2 4 9430 9430 96s loss nan acc 0.2992 val loss nan val acc 0.1355 epoch 3 4 9430 9430 96s loss nan acc 0.2992 val loss nan val acc 0.1355 epoch 4 4 1600 9430 ......................... eta 75s loss nan acc 0.3038 test different optimizer,also improve epsilon value, set clipnorm optimizer different loss functions 'mean squared error', 'categorical crossentropy' on, failed. also cpu gpu mode, loss value also nan. even switch convolution2d loss values remain nan ways solve? wondering what's real reason nan loss value? solve debug it? word2vec data wrong , padding method wrong other? keras can't solve, choose another deep learning package, reason theano? then? please help.",1,lstm loss nan pre trained word2vec,"lstm loss nan pre trained word2vec theano keras fresher, want learn , think interesting helpful. following question confuses one week. can't work try ways mentioned before. want sentiment analysis texts three classes. train word2vec dim 600 gensim. train data 10475 sequences different length. label shape 10475,3 setting maxlen sequence 200, every sequence converted 200 600 2d array.if sequence's length less 200, remaining values filled 0 padding , resulting rows zeroes. feed lstm, lstm code following model.fit train,label,batch size 100,nb epoch 4,verbose 1,shuffle true,validation split 0.1,show accuracy true getting loss nan train 9430 samples, validate 1048 samples epoch 1 4 9430 9430 99s loss nan acc 0.2992 val loss nan val acc 0.1355 epoch 2 4 9430 9430 96s loss nan acc 0.2992 val loss nan val acc 0.1355 epoch 3 4 9430 9430 96s loss nan acc 0.2992 val loss nan val acc 0.1355 epoch 4 4 1600 9430 ......................... eta 75s loss nan acc 0.3038 test different optimizer,also improve epsilon value, set clipnorm optimizer different loss functions 'mean squared error', 'categorical crossentropy' on, failed. also cpu gpu mode, loss value also nan. even switch convolution2d loss values remain nan ways solve? wondering what's real reason nan loss value? solve debug it? word2vec data wrong , padding method wrong other? keras can't solve, choose another deep learning package, reason theano? then? please help."
keras,8020,"hello everyone. reading many discussions google groups regarding tf backend issues found wordy without bottom line question, least opinion short answer. using cnn mnist model proposed https github.com fchollet keras blob master examples mnist cnn.py url run performances tests. using p2.xlarge amazon instance tesla k80 single gpu. python version 3.6 keras 2.0.8 conda keras gpu version tensorflow 1.2.0 theano 0.9.0 keras.json file .keras keras.json theanorc file contains run code get theano tensorflow roughly equal theano 8s per epoch, tf 10s per epoch loads data faster ruuning model whose input shape 224x224x3x30000 images theano outperforms tenserflow factor 2.5! although takes 5 minutes theano initialize images tensorflow backend ! image https user images.githubusercontent.com 31940058 31015242 c0ba6c6c a527 11e7 9751 c072b40aafe7.png theano backend ! image https user images.githubusercontent.com 31940058 31018700 aa694f2e a535 11e7 8b9a 9c81a65eb6db.png one thing execute import keras tensorflow backend receive theano backend thanks help!",1,tensorflow backend slower theano cnn models,"tensorflow backend slower theano cnn models hello everyone. reading many discussions google groups regarding tf backend issues found wordy without bottom line question, least opinion short answer. using cnn mnist model proposed https github.com fchollet keras blob master examples mnist cnn.py url run performances tests. using p2.xlarge amazon instance tesla k80 single gpu. python version 3.6 keras 2.0.8 conda keras gpu version tensorflow 1.2.0 theano 0.9.0 keras.json file .keras keras.json theanorc file contains run code get theano tensorflow roughly equal theano 8s per epoch, tf 10s per epoch loads data faster ruuning model whose input shape 224x224x3x30000 images theano outperforms tenserflow factor 2.5! although takes 5 minutes theano initialize images tensorflow backend ! image https user images.githubusercontent.com 31940058 31015242 c0ba6c6c a527 11e7 9751 c072b40aafe7.png theano backend ! image https user images.githubusercontent.com 31940058 31018700 aa694f2e a535 11e7 8b9a 9c81a65eb6db.png one thing execute import keras tensorflow backend receive theano backend thanks help!"
keras,1369,"currently, using bidirectional rnn lstm brnn blstm sequence classification return sequence true, i.e., classification time step . according official provided example, brnn required implemented graph bidirectionalrnn api support mask input . besides, mini batch input, preprocessing pad 0 make input batch size, input length, input features , seems explicit mask input keras. listed implementation show accuracy true modellstm.fit fit graph support , accuracy take input mask consideration. besides, example, without explicit input mask , output result classification np.argmax 0 0 0 0 0 i.e., class 1 input np.array 0 0 0 , indeed wrong . actually input mask input np.array 0 0 0 0 , means 0 input output calculated accuracy. could modify code accomplish code. thanks lot nice assistance. fchollet farizrahman4u",1,accuracy using graph implement bidirectional rnn classification note correct,"accuracy using graph implement bidirectional rnn classification note correct currently, using bidirectional rnn lstm brnn blstm sequence classification return sequence true, i.e., classification time step . according official provided example, brnn required implemented graph bidirectionalrnn api support mask input . besides, mini batch input, preprocessing pad 0 make input batch size, input length, input features , seems explicit mask input keras. listed implementation show accuracy true modellstm.fit fit graph support , accuracy take input mask consideration. besides, example, without explicit input mask , output result classification np.argmax 0 0 0 0 0 i.e., class 1 input np.array 0 0 0 , indeed wrong . actually input mask input np.array 0 0 0 0 , means 0 input output calculated accuracy. could modify code accomplish code. thanks lot nice assistance. fchollet farizrahman4u"
keras,8027,"update finally, found problem. related activation function fault. sorry that. closed issue. addressing sentence level binary classification task. data consists 3 subarrays tokens left context, core, right context. used keras devise several alternatives convolutional neural networks validate one best fit problem. newbie python keras decided start simpler solutions order test changes improve metrics accuracy, precision, recall, f1 auc roc . first simplification regarding input data decided ignore contexts create sequential model keras see, use fixed size inputs applied padding preprocessing. also used embedding layer word2vec model. model returns following results wished implement select subarray input data inside cnn means lambda layers. use following definition lambda layer lambda lambda x x , 1 , output shape 500, input summary new cnn see almost prior results disgusting accuracy stops 60 obviously, precision, recall f1 low 0.10 regarding first model results. know what's happening know networks different thought. clue regarding issue? note asked question stackoverflow think could issue regarding keras implementation. link https stackoverflow.com questions 46491418 2 almost equal keras cnn returns 2 quite different results",1,2 almost equal cnn returns 2 quite different results,"2 almost equal cnn returns 2 quite different results update finally, found problem. related activation function fault. sorry that. closed issue. addressing sentence level binary classification task. data consists 3 subarrays tokens left context, core, right context. used keras devise several alternatives convolutional neural networks validate one best fit problem. newbie python keras decided start simpler solutions order test changes improve metrics accuracy, precision, recall, f1 auc roc . first simplification regarding input data decided ignore contexts create sequential model keras see, use fixed size inputs applied padding preprocessing. also used embedding layer word2vec model. model returns following results wished implement select subarray input data inside cnn means lambda layers. use following definition lambda layer lambda lambda x x , 1 , output shape 500, input summary new cnn see almost prior results disgusting accuracy stops 60 obviously, precision, recall f1 low 0.10 regarding first model results. know what's happening know networks different thought. clue regarding issue? note asked question stackoverflow think could issue regarding keras implementation. link https stackoverflow.com questions 46491418 2 almost equal keras cnn returns 2 quite different results"
keras,863,"noticed include weight regularization l2 weight regularization, significantly slows training. someone provide explanation is?",1,keras slow regularization?,"keras slow regularization? noticed include weight regularization l2 weight regularization, significantly slows training. someone provide explanation is?"
keras,866,"latest updates scan theano, anybody else experiencing dramatically different training times? particular, lstm faster, gru jzs1 3 much, much slower relative comparison test network, exchanging recurrent layer network, lstm pre theano scan changes 24s. anybody insight this?",1,"latest theano updates, lstm faster gru much slower","latest theano updates, lstm faster gru much slower latest updates scan theano, anybody else experiencing dramatically different training times? particular, lstm faster, gru jzs1 3 much, much slower relative comparison test network, exchanging recurrent layer network, lstm pre theano scan changes 24s. anybody insight this?"
keras,867,"hello, normal use mlp make regression time series like sunspot 'loss' 'val loss' decrease training acc val acc always 1.0000 ? information, put linear activation output layer one neural mse hiden layer 'relu' activation dropout thank anybody give good reason",1,regression mlp,"regression mlp hello, normal use mlp make regression time series like sunspot 'loss' 'val loss' decrease training acc val acc always 1.0000 ? information, put linear activation output layer one neural mse hiden layer 'relu' activation dropout thank anybody give good reason"
keras,11119,"noticed process saving loading saved models slows generate models. clarify, say load 10 models hdf5 files. 1st model load loaded quickly, every successive load progressively slower. models size, memory issue. attached python script reproduce issue. os ubuntu 16.04 backend tensorflow using cpu thank reproducing code example example output",1,progressively slower model save load times,"progressively slower model save load times noticed process saving loading saved models slows generate models. clarify, say load 10 models hdf5 files. 1st model load loaded quickly, every successive load progressively slower. models size, memory issue. attached python script reproduce issue. os ubuntu 16.04 backend tensorflow using cpu thank reproducing code example example output"
keras,10096,"observed massive slowdowns keras tensorflow models tensorflow versions newer 1.4.1. sure issue tensorflow way keras creates tensorflow models, cross posting issue repos. script reproducing issue setup fitting timings fit method tensorflow 1.4.1 2.91 452 ms per loop obtained using ipython's magic, 7 loops tensorflow 1.5.0 cpu times user 2min 19s , sys 5min 22s, total 7min 41s wall time 1min 2s tensorflow 1.6.0 cpu times user 5min 5s , sys 12min 31s, total 17min 36s wall time 2min 37s tensorflow 1.7.0 cpu times user 5min 5s , sys 12min 39s, total 17min 45s wall time 2min 39s so, seems massive slowdown version 1,5, one 1.6 similar speed 1.7 . tests run conda environment python 3.6.5 keras 2.1.5, corresponding tensorflow versions coming anaconda channel. gpu accelerated version keras tensorflow conda package present issue. thanks advance!",1,speed regression change tensorflow backend version,"speed regression change tensorflow backend version observed massive slowdowns keras tensorflow models tensorflow versions newer 1.4.1. sure issue tensorflow way keras creates tensorflow models, cross posting issue repos. script reproducing issue setup fitting timings fit method tensorflow 1.4.1 2.91 452 ms per loop obtained using ipython's magic, 7 loops tensorflow 1.5.0 cpu times user 2min 19s , sys 5min 22s, total 7min 41s wall time 1min 2s tensorflow 1.6.0 cpu times user 5min 5s , sys 12min 31s, total 17min 36s wall time 2min 37s tensorflow 1.7.0 cpu times user 5min 5s , sys 12min 39s, total 17min 45s wall time 2min 39s so, seems massive slowdown version 1,5, one 1.6 similar speed 1.7 . tests run conda environment python 3.6.5 keras 2.1.5, corresponding tensorflow versions coming anaconda channel. gpu accelerated version keras tensorflow conda package present issue. thanks advance!"
keras,13172,"hello everyone, might problem metrics training using generator. system information written custom code opposed using example directory yes os platform distribution e.g., linux ubuntu 16.04 windows 10 tensorflow backend yes yes tensorflow version 1.10.0 keras version 2.2.4 python version 3.6.8 cuda cudnn version none gpu model memory none current behavior training, shows low scores. expected behavior training, model performs well. actual performance trained model task largely outclasses suggested performance estimate training. therefore expect metrics training show better values. code reproduce issue info logs code generator. false metrics came introduced generator training. anyone similar experiences? anyone tell generator something obvious important missing? thank much.",1,low scores training good perormance real data fit generator,"low scores training good perormance real data fit generator hello everyone, might problem metrics training using generator. system information written custom code opposed using example directory yes os platform distribution e.g., linux ubuntu 16.04 windows 10 tensorflow backend yes yes tensorflow version 1.10.0 keras version 2.2.4 python version 3.6.8 cuda cudnn version none gpu model memory none current behavior training, shows low scores. expected behavior training, model performs well. actual performance trained model task largely outclasses suggested performance estimate training. therefore expect metrics training show better values. code reproduce issue info logs code generator. false metrics came introduced generator training. anyone similar experiences? anyone tell generator something obvious important missing? thank much."
keras,10624,"post keras blog https blog.keras.io using pre trained word embeddings keras model.html yields 95 validation accuracy 2 epochs. given example code, can't reproduce fact validation accuracy go 75 loss high well . following example code https blog.keras.io using pre trained word embeddings keras model.html ...with exception open glove vector file encoding 'utf 8' fails otherwise. link provided example blog post itself.",1,accuracy issue using 20 newsgroups example model,"accuracy issue using 20 newsgroups example model post keras blog https blog.keras.io using pre trained word embeddings keras model.html yields 95 validation accuracy 2 epochs. given example code, can't reproduce fact validation accuracy go 75 loss high well . following example code https blog.keras.io using pre trained word embeddings keras model.html ...with exception open glove vector file encoding 'utf 8' fails otherwise. link provided example blog post itself."
keras,7558,"cost2 categorical crossentropy causes poor results finally, results good using cost1. used mnist cnn algorithm, follows",1,issue categorical crossentropy keras softmax cross entropy logits tensorflow,"issue categorical crossentropy keras softmax cross entropy logits tensorflow cost2 categorical crossentropy causes poor results finally, results good using cost1. used mnist cnn algorithm, follows"
keras,10632,"train resnet v2 110 model cifar10, get 92.x accuracy test set. table, shows test accuracy 93.x . wonder get that?",1,low test accuracy resnet 110 cifar10,"low test accuracy resnet 110 cifar10 train resnet v2 110 model cifar10, get 92.x accuracy test set. table, shows test accuracy 93.x . wonder get that?"
keras,11665,"x check date master branch keras. update x check version tensorflow date. installation instructions found https www.tensorflow.org get started os setup . x provide link github gist python script reproduce issue copy script short . performing batch matrix multiplies two tensors size batch, n, batch, m, k get tensor size batch, n, k , matrix products. behavior done tf.matmul k.batch dot default axis arguments. however k.batch dot, elementwise multiplication line https github.com keras team keras blob 75a35032e194a2d065b0071a9e786adf6cee83ea keras backend tensorflow backend.py l1248 eats lot memory. elementwise multiplication followed summing axis course mathematically equivalent matrix multiply, two step implementation, tensorflow assigns memory intermediate large tensor. simple example, small gpu nvidia 970 able perform calculation using tf.matmul, using k.batch dot tensorflow fails oom error. fails tries assign tensor size 100, 10000, 500, 32 elementwise multiply batch dot dimension 10000 strictly necessary case since interested sum .",1,poor memory performance k.batch dot tensorflow backend relative batched tf.matmul,"poor memory performance k.batch dot tensorflow backend relative batched tf.matmul x check date master branch keras. update x check version tensorflow date. installation instructions found https www.tensorflow.org get started os setup . x provide link github gist python script reproduce issue copy script short . performing batch matrix multiplies two tensors size batch, n, batch, m, k get tensor size batch, n, k , matrix products. behavior done tf.matmul k.batch dot default axis arguments. however k.batch dot, elementwise multiplication line https github.com keras team keras blob 75a35032e194a2d065b0071a9e786adf6cee83ea keras backend tensorflow backend.py l1248 eats lot memory. elementwise multiplication followed summing axis course mathematically equivalent matrix multiply, two step implementation, tensorflow assigns memory intermediate large tensor. simple example, small gpu nvidia 970 able perform calculation using tf.matmul, using k.batch dot tensorflow fails oom error. fails tries assign tensor size 100, 10000, 500, 32 elementwise multiply batch dot dimension 10000 strictly necessary case since interested sum ."
keras,3996,"using tf following model, run model.fit takes 10 minutes actually starts anything. out.profile.zip https github.com fchollet keras files 517347 out.profile.zip ideas?",1,model.fit takes long time start,"model.fit takes long time start using tf following model, run model.fit takes 10 minutes actually starts anything. out.profile.zip https github.com fchollet keras files 517347 out.profile.zip ideas?"
keras,13212,"data loaded . watched issue https github.com keras team keras issues 3477, applied tweaks shuffle good order images fs remove transformations test dataset want print result prediction draw confusion matrix. use datas validation training prediction. training, model achieve 91.5 accuracy. gives accuracy. prediction, completely different accuracy, 86 . training python code evaluate python code prediction python code runs jupyter notebook python 3. issue missing something ?",1,accuracy validation data training prediction,"accuracy validation data training prediction data loaded . watched issue https github.com keras team keras issues 3477, applied tweaks shuffle good order images fs remove transformations test dataset want print result prediction draw confusion matrix. use datas validation training prediction. training, model achieve 91.5 accuracy. gives accuracy. prediction, completely different accuracy, 86 . training python code evaluate python code prediction python code runs jupyter notebook python 3. issue missing something ?"
keras,1438,discussion https groups.google.com tensorflow.org msg discuss v6aebw4nlae vuage nxewaj mark daoust suggested impl leaky relu better change current impl something like think two identical least alpha 1. preferable use tf.nn.relu faster? one place use tf.constant cast alpha correct type place use tf.cast cast max value. one better equivalent? thanks,1,"simplify relu x, alpha 0., max value none impl tensorflow backend.py","simplify relu x, alpha 0., max value none impl tensorflow backend.py discussion https groups.google.com tensorflow.org msg discuss v6aebw4nlae vuage nxewaj mark daoust suggested impl leaky relu better change current impl something like think two identical least alpha 1. preferable use tf.nn.relu faster? one place use tf.constant cast alpha correct type place use tf.cast cast max value. one better equivalent? thanks"
keras,415,"hi, started using keras. awesome work! tried use lstm following code compared efficiency char rnn https github.com karpathy char rnn , found implementation keras 4 times slower karpathy's batchsize . something wrong? attached theano profile result https gist.github.com ffmpbgrnn 842e1910f216b1e00e27 thanks you!",1,slow training lstm,"slow training lstm hi, started using keras. awesome work! tried use lstm following code compared efficiency char rnn https github.com karpathy char rnn , found implementation keras 4 times slower karpathy's batchsize . something wrong? attached theano profile result https gist.github.com ffmpbgrnn 842e1910f216b1e00e27 thanks you!"
keras,9122,"trying train simple cnn windows tensorflow backend windows. inputs quite large 256x256 train network gtx 1080. model trained saved, get oom errors restoring it. order able resume training, need divide batch size 4. reason increased memory usage reloading model way avoid it? 1st run output.txt https github.com keras team keras files 1645667 1st run output.txt 2nd run output.txt https github.com keras team keras files 1645668 2nd run output.txt",1,increased gpu memory usage restoring saved model,"increased gpu memory usage restoring saved model trying train simple cnn windows tensorflow backend windows. inputs quite large 256x256 train network gtx 1080. model trained saved, get oom errors restoring it. order able resume training, need divide batch size 4. reason increased memory usage reloading model way avoid it? 1st run output.txt https github.com keras team keras files 1645667 1st run output.txt 2nd run output.txt https github.com keras team keras files 1645668 2nd run output.txt"
keras,933,"hello, learning use graph seems powerful implemented one previous model uses sequential. model using sequential number dimension set random model works fine reimplementation using graph impression exactly model grateful somebody spotted something wrong . model based graph gives loss 3.6 loss one around 0.002. reason please ? thank help",1,model graph gives bad performance,"model graph gives bad performance hello, learning use graph seems powerful implemented one previous model uses sequential. model using sequential number dimension set random model works fine reimplementation using graph impression exactly model grateful somebody spotted something wrong . model based graph gives loss 3.6 loss one around 0.002. reason please ? thank help"
keras,11692,"cnn model learning slow epochs testing accuracy increasing. using learning rate 0.0003 dropout 0.55 . batch size 50 , training samples 8000 grayscale images testing samples 1600 grayscale images. model given below. ! image https user images.githubusercontent.com 17239812 48775610 61fb5400 ecef 11e8 9cc3 57ee1df8f657.png increase training testing accuracy? ! image https user images.githubusercontent.com 17239812 48775842 0aa9b380 ecf0 11e8 908a 3f6df616d0e4.png accuracy loss getting 30th epoch. training accuracy 0.77 last 5 epochs!!",1,slow learning overfitting cnn,"slow learning overfitting cnn cnn model learning slow epochs testing accuracy increasing. using learning rate 0.0003 dropout 0.55 . batch size 50 , training samples 8000 grayscale images testing samples 1600 grayscale images. model given below. ! image https user images.githubusercontent.com 17239812 48775610 61fb5400 ecef 11e8 9cc3 57ee1df8f657.png increase training testing accuracy? ! image https user images.githubusercontent.com 17239812 48775842 0aa9b380 ecf0 11e8 908a 3f6df616d0e4.png accuracy loss getting 30th epoch. training accuracy 0.77 last 5 epochs!!"
keras,8114,"hi there, trying utilise bottleneck features produced vgg16 new classification task limited data. right now, following please, excuse code quality returns following output modelmodel outputs softmax.0, inputs input 3 fairly accustom keras deep learning general, yet use bottleneck technique. believe problem likely related corresponding predicted output informative labels, struggling find solution. help would fantastic! thanks, keiron.",1,"prediction score extremely low, despite high 94 validation accuracy bottleneck features vgg16","prediction score extremely low, despite high 94 validation accuracy bottleneck features vgg16 hi there, trying utilise bottleneck features produced vgg16 new classification task limited data. right now, following please, excuse code quality returns following output modelmodel outputs softmax.0, inputs input 3 fairly accustom keras deep learning general, yet use bottleneck technique. believe problem likely related corresponding predicted output informative labels, struggling find solution. help would fantastic! thanks, keiron."
keras,3508,"hi everyone tried examples mnist use cpu work well tensorflow theano tried , use gpu tensorflow backend mnist cnn.py working, low accuracy , guess there's wrong cnn gpu. issue 511 https github.com fchollet keras issues 511 solve problem. configuration 1.keras keras.json image dim ordering tf , epsilon 1e 05, floatx float32 , backend tensorflow 2source code changed like line 35x train x train.reshape x train.shape 0 , img rows, img cols, 1 line 36x test x test.reshape x test.shape 0 , img rows, img cols, 1 line 51model.add convolution2d nb filters, nb conv, nb conv, line 52 border mode 'valid', line 53 input shape img rows, img cols, 1 3keras version1.0.7, gpu geforce gtx1070, cuda toolkit version7.5, cudnn version6.5tensorflow version0.9r detail information 1on gpu tensorflow dllearning mnist python mnist cnn.py tensorflow stream executor dso loader.cc 108 successfully opened cuda library libcublas.so locally tensorflow stream executor dso loader.cc 108 successfully opened cuda library libcudnn.so locally tensorflow stream executor dso loader.cc 108 successfully opened cuda library libcufft.so locally tensorflow stream executor dso loader.cc 108 successfully opened cuda library libcuda.so locally tensorflow stream executor dso loader.cc 108 successfully opened cuda library libcurand.so locally using tensorflow backend. x train shape 60000, 28, 28, 1 60000 train samples 10000 test samples tensorflow stream executor cuda cuda gpu executor.cc 924 successful numa node read sysfs negative value 1 , must least one numa node, returning numa node zero tensorflow core common runtime gpu gpu init.cc 102 found device 0 properties name geforce gtx 1070 major 6 minor 1 memoryclockrate ghz 1.7845 pcibusid 0000 01 00.0 total memory 7.92gib free memory 1.93gib tensorflow core common runtime gpu gpu init.cc 126 dma 0 tensorflow core common runtime gpu gpu init.cc 136 0 tensorflow core common runtime gpu gpu device.cc 806 creating tensorflow device gpu 0 device 0, name geforce gtx 1070, pci bus id 0000 01 00.0 train 60000 samples, validate 10000 samples epoch 1 12 60000 60000 12s loss 2.3065 acc 0.1103 val loss 2.3011 val acc 0.1137 epoch 2 12 60000 60000 6s loss 2.3022 acc 0.1122 val loss 2.3011 val acc 0.1135 epoch 3 12 60000 60000 6s loss 2.3014 acc 0.1123 val loss 2.3011 val acc 0.1135 epoch 4 12 60000 60000 6s loss 2.3025 acc 0.1121 val loss 2.3010 val acc 0.1135 epoch 5 12 60000 60000 5s loss 2.3013 acc 0.1124 val loss 2.3010 val acc 0.1135 epoch 6 12 60000 60000 5s loss 2.3013 acc 0.1124 val loss 2.3010 val acc 0.1135 epoch 7 12 60000 60000 6s loss 2.3013 acc 0.1124 val loss 2.3021 val acc 0.1135 epoch 8 12 60000 60000 6s loss 2.3013 acc 0.1124 val loss 2.3017 val acc 0.1135 epoch 9 12 60000 60000 6s loss 2.3013 acc 0.1123 val loss 2.3010 val acc 0.1135 epoch 10 12 60000 60000 5s loss 2.3012 acc 0.1123 val loss 2.3011 val acc 0.1135 epoch 11 12 60000 60000 6s loss 2.3013 acc 0.1124 val loss 2.3011 val acc 0.1135 epoch 12 12 60000 60000 6s loss 2.3013 acc 0.1124 val loss 2.3011 val acc 0.1135 test score 2.30111371231 test accuracy 0.1135 2on cpu train 60000 samples, validate 10000 samples epoch 1 12 60000 60000 71s loss 0.3994 acc 0.8764 val loss 0.1060 val acc 0.9674 epoch 2 12 60000 60000 72s loss 0.1495 acc 0.9557 val loss 0.0677 val acc 0.9785 epoch 3 12 60000 60000 72s loss 0.1119 acc 0.9675 val loss 0.0537 val acc 0.9823 epoch 4 12 60000 60000 100s loss 0.0914 acc 0.9724 val loss 0.0474 val acc 0.9839 epoch 5 12 60000 60000 114s loss 0.0805 acc 0.9758 val loss 0.0393 val acc 0.9874 epoch 6 12 60000 60000 117s loss 0.0721 acc 0.9783 val loss 0.0381 val acc 0.9873 epoch 7 12 60000 60000 118s loss 0.0664 acc 0.9806 val loss 0.0376 val acc 0.9874 epoch 8 12 60000 60000 121s loss 0.0621 acc 0.9815 val loss 0.0338 val acc 0.9883 epoch 9 12 60000 60000 121s loss 0.0565 acc 0.9834 val loss 0.0322 val acc 0.9887 epoch 10 12 60000 60000 124s loss 0.0542 acc 0.9840 val loss 0.0304 val acc 0.9906 epoch 11 12 60000 60000 121s loss 0.0491 acc 0.9850 val loss 0.0314 val acc 0.9896 epoch 12 12 60000 60000 116s loss 0.0485 acc 0.9857 val loss 0.0294 val acc 0.9898 test score 0.0294442383148 test accuracy 0.9898",1,low accuracy mnist cnn running gpu using tensorflow backend,"low accuracy mnist cnn running gpu using tensorflow backend hi everyone tried examples mnist use cpu work well tensorflow theano tried , use gpu tensorflow backend mnist cnn.py working, low accuracy , guess there's wrong cnn gpu. issue 511 https github.com fchollet keras issues 511 solve problem. configuration 1.keras keras.json image dim ordering tf , epsilon 1e 05, floatx float32 , backend tensorflow 2source code changed like line 35x train x train.reshape x train.shape 0 , img rows, img cols, 1 line 36x test x test.reshape x test.shape 0 , img rows, img cols, 1 line 51model.add convolution2d nb filters, nb conv, nb conv, line 52 border mode 'valid', line 53 input shape img rows, img cols, 1 3keras version1.0.7, gpu geforce gtx1070, cuda toolkit version7.5, cudnn version6.5tensorflow version0.9r detail information 1on gpu tensorflow dllearning mnist python mnist cnn.py tensorflow stream executor dso loader.cc 108 successfully opened cuda library libcublas.so locally tensorflow stream executor dso loader.cc 108 successfully opened cuda library libcudnn.so locally tensorflow stream executor dso loader.cc 108 successfully opened cuda library libcufft.so locally tensorflow stream executor dso loader.cc 108 successfully opened cuda library libcuda.so locally tensorflow stream executor dso loader.cc 108 successfully opened cuda library libcurand.so locally using tensorflow backend. x train shape 60000, 28, 28, 1 60000 train samples 10000 test samples tensorflow stream executor cuda cuda gpu executor.cc 924 successful numa node read sysfs negative value 1 , must least one numa node, returning numa node zero tensorflow core common runtime gpu gpu init.cc 102 found device 0 properties name geforce gtx 1070 major 6 minor 1 memoryclockrate ghz 1.7845 pcibusid 0000 01 00.0 total memory 7.92gib free memory 1.93gib tensorflow core common runtime gpu gpu init.cc 126 dma 0 tensorflow core common runtime gpu gpu init.cc 136 0 tensorflow core common runtime gpu gpu device.cc 806 creating tensorflow device gpu 0 device 0, name geforce gtx 1070, pci bus id 0000 01 00.0 train 60000 samples, validate 10000 samples epoch 1 12 60000 60000 12s loss 2.3065 acc 0.1103 val loss 2.3011 val acc 0.1137 epoch 2 12 60000 60000 6s loss 2.3022 acc 0.1122 val loss 2.3011 val acc 0.1135 epoch 3 12 60000 60000 6s loss 2.3014 acc 0.1123 val loss 2.3011 val acc 0.1135 epoch 4 12 60000 60000 6s loss 2.3025 acc 0.1121 val loss 2.3010 val acc 0.1135 epoch 5 12 60000 60000 5s loss 2.3013 acc 0.1124 val loss 2.3010 val acc 0.1135 epoch 6 12 60000 60000 5s loss 2.3013 acc 0.1124 val loss 2.3010 val acc 0.1135 epoch 7 12 60000 60000 6s loss 2.3013 acc 0.1124 val loss 2.3021 val acc 0.1135 epoch 8 12 60000 60000 6s loss 2.3013 acc 0.1124 val loss 2.3017 val acc 0.1135 epoch 9 12 60000 60000 6s loss 2.3013 acc 0.1123 val loss 2.3010 val acc 0.1135 epoch 10 12 60000 60000 5s loss 2.3012 acc 0.1123 val loss 2.3011 val acc 0.1135 epoch 11 12 60000 60000 6s loss 2.3013 acc 0.1124 val loss 2.3011 val acc 0.1135 epoch 12 12 60000 60000 6s loss 2.3013 acc 0.1124 val loss 2.3011 val acc 0.1135 test score 2.30111371231 test accuracy 0.1135 2on cpu train 60000 samples, validate 10000 samples epoch 1 12 60000 60000 71s loss 0.3994 acc 0.8764 val loss 0.1060 val acc 0.9674 epoch 2 12 60000 60000 72s loss 0.1495 acc 0.9557 val loss 0.0677 val acc 0.9785 epoch 3 12 60000 60000 72s loss 0.1119 acc 0.9675 val loss 0.0537 val acc 0.9823 epoch 4 12 60000 60000 100s loss 0.0914 acc 0.9724 val loss 0.0474 val acc 0.9839 epoch 5 12 60000 60000 114s loss 0.0805 acc 0.9758 val loss 0.0393 val acc 0.9874 epoch 6 12 60000 60000 117s loss 0.0721 acc 0.9783 val loss 0.0381 val acc 0.9873 epoch 7 12 60000 60000 118s loss 0.0664 acc 0.9806 val loss 0.0376 val acc 0.9874 epoch 8 12 60000 60000 121s loss 0.0621 acc 0.9815 val loss 0.0338 val acc 0.9883 epoch 9 12 60000 60000 121s loss 0.0565 acc 0.9834 val loss 0.0322 val acc 0.9887 epoch 10 12 60000 60000 124s loss 0.0542 acc 0.9840 val loss 0.0304 val acc 0.9906 epoch 11 12 60000 60000 121s loss 0.0491 acc 0.9850 val loss 0.0314 val acc 0.9896 epoch 12 12 60000 60000 116s loss 0.0485 acc 0.9857 val loss 0.0294 val acc 0.9898 test score 0.0294442383148 test accuracy 0.9898"
keras,8629,created siamese like network. training keras loss converge all. raw tensorflow loss function training op converges quickly. https gist.github.com aelphy c73aa56bf2f410b1423c63bd7087142e,1,"siamese network converge, raw tf network converges.","siamese network converge, raw tf network converges. created siamese like network. training keras loss converge all. raw tensorflow loss function training op converges quickly. https gist.github.com aelphy c73aa56bf2f410b1423c63bd7087142e"
keras,949,"hello, trying code something like http arxiv.org abs 1506.04214 start trying implement simple conv 1d time series forex . goal predict 10th value last value temporal window 60 values window want predict 70th data compatible conv 1d prepare data array 5019 matrix 60 raw 1 column code javascript sizetraining 0.9 sizetest 0.1 window 60 h 10 print 'loading data ...' forex forex x train, train , x test,y test forex.load data sizetraining, sizetest, window, h xtrain mean x train.mean xtrain ecart type x train.std x train x train xtrain mean xtrain ecart type xtest mean x test.mean xtest ecart type x test.std x test x test xtest mean xtest ecart type ytrain mean train.mean ytrain ecart type train.std train train ytrain mean ytrain ecart type ytest mean test.mean ytest ecart type test.std test test ytest mean ytest ecart type print 'x reshape' x train np.reshape x train, 1,window,1 x test np.reshape x test, 1,window,1 print x train.shape print x train print 'y reshapet' train np.reshape train, 1,1 test np.reshape train, 1,1 print train.shape model sequential premiere couche de convolution model.add convolution1d nb filter 60,filter length 3,border mode full ,activation relu ,subsample length 1,input dim 1,input length 60 conv1 activation 'relu' model.add conv1 premiere couche de subsampling model.add maxpooling1d pool length 2 model.add dropout 0.25 model.add convolution1d nb filter 60,filter length 3,border mode full ,activation relu ,subsample length 1 conv1 activation 'relu' model.add conv1 premiere couche de subsampling model.add maxpooling1d pool length 2 model.add dropout 0.25 applatit la sortie pour la presenter au mlp model.add flatten premiere couche du mlp avec activation de type maxoutdense model.add dense output dim 500 model.add dropout 0.5 model.add dense output dim 500 model.add dropout 0.5 model.add dense 1 model.add activation 'linear' model.compile loss 'mse', optimizer 'adadelta' history model.fit x train, train, batch size 128, nb epoch 2, show accuracy false, verbose 1, validation data x test, test score model.evaluate x test, test, show accuracy false, verbose 0 ''' problem",1,loss nan training second batch size convolution 1d,"loss nan training second batch size convolution 1d hello, trying code something like http arxiv.org abs 1506.04214 start trying implement simple conv 1d time series forex . goal predict 10th value last value temporal window 60 values window want predict 70th data compatible conv 1d prepare data array 5019 matrix 60 raw 1 column code javascript sizetraining 0.9 sizetest 0.1 window 60 h 10 print 'loading data ...' forex forex x train, train , x test,y test forex.load data sizetraining, sizetest, window, h xtrain mean x train.mean xtrain ecart type x train.std x train x train xtrain mean xtrain ecart type xtest mean x test.mean xtest ecart type x test.std x test x test xtest mean xtest ecart type ytrain mean train.mean ytrain ecart type train.std train train ytrain mean ytrain ecart type ytest mean test.mean ytest ecart type test.std test test ytest mean ytest ecart type print 'x reshape' x train np.reshape x train, 1,window,1 x test np.reshape x test, 1,window,1 print x train.shape print x train print 'y reshapet' train np.reshape train, 1,1 test np.reshape train, 1,1 print train.shape model sequential premiere couche de convolution model.add convolution1d nb filter 60,filter length 3,border mode full ,activation relu ,subsample length 1,input dim 1,input length 60 conv1 activation 'relu' model.add conv1 premiere couche de subsampling model.add maxpooling1d pool length 2 model.add dropout 0.25 model.add convolution1d nb filter 60,filter length 3,border mode full ,activation relu ,subsample length 1 conv1 activation 'relu' model.add conv1 premiere couche de subsampling model.add maxpooling1d pool length 2 model.add dropout 0.25 applatit la sortie pour la presenter au mlp model.add flatten premiere couche du mlp avec activation de type maxoutdense model.add dense output dim 500 model.add dropout 0.5 model.add dense output dim 500 model.add dropout 0.5 model.add dense 1 model.add activation 'linear' model.compile loss 'mse', optimizer 'adadelta' history model.fit x train, train, batch size 128, nb epoch 2, show accuracy false, verbose 1, validation data x test, test score model.evaluate x test, test, show accuracy false, verbose 0 ''' problem"
keras,1465,"use keras run forward pass weights pre trained another network, forward pass terribly slow. takes 2 3 seconds darknet https github.com pjreddie darknet , takes 27 seconds run forward pass. result expected, know done wrong here. model keras network similar googlenet 27 layers total. using 3gb quadro k4000",1,impletation feedforward slow,"impletation feedforward slow use keras run forward pass weights pre trained another network, forward pass terribly slow. takes 2 3 seconds darknet https github.com pjreddie darknet , takes 27 seconds run forward pass. result expected, know done wrong here. model keras network similar googlenet 27 layers total. using 3gb quadro k4000"
keras,4030,"regression cnn model object detection tasks. use one output one loss function, result good. add another output zero loss weight top performance much worse. note tried different loss weight values , gave bad results. thought loss weight zero, contribution back propagation updates. alter sub network , learning change either. idea wrong? thanks lot!",1,performance issue multiple loss functions,"performance issue multiple loss functions regression cnn model object detection tasks. use one output one loss function, result good. add another output zero loss weight top performance much worse. note tried different loss weight values , gave bad results. thought loss weight zero, contribution back propagation updates. alter sub network , learning change either. idea wrong? thanks lot!"
keras,8640,"trying train model gpu instead cpu aws p2.xlarge instance jupyter notebook. using tensorflow gpu backend installed mentioned . seeing speed improvements training models instances compared using cpu, infact getting training speeds per epoch almost getting 4 core laptop cpu p2.xlarge also 4 vcpus tesla k80 gpu . sure need changes code accommodate faster parallel processing gpu offer. pasting code model also interestingly gpu seems utilizing 50 60 processing power almost memory every time check gpu status using fall 0 1mib respectively training also you's like see logs using gpu jupyter notebook please suggest could problem. thanks ton looking anyways!",1,improvements training speed gpu partial gpu usage?!,"improvements training speed gpu partial gpu usage?! trying train model gpu instead cpu aws p2.xlarge instance jupyter notebook. using tensorflow gpu backend installed mentioned . seeing speed improvements training models instances compared using cpu, infact getting training speeds per epoch almost getting 4 core laptop cpu p2.xlarge also 4 vcpus tesla k80 gpu . sure need changes code accommodate faster parallel processing gpu offer. pasting code model also interestingly gpu seems utilizing 50 60 processing power almost memory every time check gpu status using fall 0 1mib respectively training also you's like see logs using gpu jupyter notebook please suggest could problem. thanks ton looking anyways!"
keras,9672,"similar problem kaggle tutorial https www.kaggle.com eliotbarr text mining sklearn keras mlp lstm cnn, refer it. look code block number 30 31 suppose accuracy scores same, fact, different. possible? one accuracy calculated model.evaluate one calculated accuracy score sklearn .",1,different accuracy score keras.model.evaluate sklearn.accuracy score,"different accuracy score keras.model.evaluate sklearn.accuracy score similar problem kaggle tutorial https www.kaggle.com eliotbarr text mining sklearn keras mlp lstm cnn, refer it. look code block number 30 31 suppose accuracy scores same, fact, different. possible? one accuracy calculated model.evaluate one calculated accuracy score sklearn ."
keras,9674,"hi! working transferring nasnet mobile weights keras pytorch. transferring weights making predictions image using frameworks getting absolutely values confidence middle layers network. however, evaluation imagenet using transferred weights pytorch gave acc 1 38.110 acc 5 60.844 also, tested script using evaluation imagenet works correct, got correct values architectures. wondering could double check weight provide nasnet mobile, probably issue them!",1,nasnet mobile gives low accuracy imagenet,"nasnet mobile gives low accuracy imagenet hi! working transferring nasnet mobile weights keras pytorch. transferring weights making predictions image using frameworks getting absolutely values confidence middle layers network. however, evaluation imagenet using transferred weights pytorch gave acc 1 38.110 acc 5 60.844 also, tested script using evaluation imagenet works correct, got correct values architectures. wondering could double check weight provide nasnet mobile, probably issue them!"
keras,13266,tested example keras examples lstm seq2seq.py accuracy metrics. result model achieve high accuracy. found one hot encoding tokenizing applied back padding sentences.,1,low accuracy incorrect one hot encoding lstm seq2seq.py example,low accuracy incorrect one hot encoding lstm seq2seq.py example tested example keras examples lstm seq2seq.py accuracy metrics. result model achieve high accuracy. found one hot encoding tokenizing applied back padding sentences.
keras,4563,"training lstm model multiple time series regression. but, losses always either high nan. tried several optimizers , . here's script shape n samples train, max timesteps, max features shape n samples add values trying predict high. idea might going wroing?",1,high nan loss performing multiple sequence time series regression,"high nan loss performing multiple sequence time series regression training lstm model multiple time series regression. but, losses always either high nan. tried several optimizers , . here's script shape n samples train, max timesteps, max features shape n samples add values trying predict high. idea might going wroing?"
keras,10706,reproducible example consider rmse mae same. happens placed together. bug? output ! image https user images.githubusercontent.com 22788747 42806109 39fc16be 89e0 11e8 80d8 5e73ba8b2541.png,1,custom metric mae rmse,custom metric mae rmse reproducible example consider rmse mae same. happens placed together. bug? output ! image https user images.githubusercontent.com 22788747 42806109 39fc16be 89e0 11e8 80d8 5e73ba8b2541.png
keras,6108,image ocr example kears 10 times faster keras2 ? changed ?,1,image ocr example performance,image ocr example performance image ocr example kears 10 times faster keras2 ? changed ?
keras,6625,"new keras. said, trying avoid using scikit's pipeline code since relatively slower using keras network. achieve result using keras's implementation? achieve using normalizationlayer? could figure tried following using keras 2.0.4 tensorflow 0.12.1.",1,replace scikitlearn's pipelines keras' layers,"replace scikitlearn's pipelines keras' layers new keras. said, trying avoid using scikit's pipeline code since relatively slower using keras network. achieve result using keras's implementation? achieve using normalizationlayer? could figure tried following using keras 2.0.4 tensorflow 0.12.1."
keras,12770,"please make sure boxes checked submit issue. issue implementation question, please ask question stackoverflow http stackoverflow.com questions tagged keras join keras slack channel https keras slack autojoin.herokuapp.com ask instead filing github issue. thank you! x check date master branch keras. update pip install git git github.com keras team keras.git upgrade deps x running tensorflow, check date latest version. installation instructions found https www.tensorflow.org get started os setup . running theano, check date master branch theano. update pip install git git github.com theano theano.git upgrade deps x provide link github gist python script reproduce issue copy script short . building siamese network using vgg19 base network. everything went fine throughout training testing. however, noticed training, accuracy quite high took 2 epochs reach 100 accuracy, amazing , testing, accuracy around 0 sometimes even lower, shown . code located repo https github.com ayaz amin ayaznet . run , output run , output expecting output showing ones 0 means similarity, 1 meaning full similarity . pretty sure nothing wrong code. anything wrong? bug? also, using tf.keras tensorflow 2.0.",1,siamese network performance issues,"siamese network performance issues please make sure boxes checked submit issue. issue implementation question, please ask question stackoverflow http stackoverflow.com questions tagged keras join keras slack channel https keras slack autojoin.herokuapp.com ask instead filing github issue. thank you! x check date master branch keras. update pip install git git github.com keras team keras.git upgrade deps x running tensorflow, check date latest version. installation instructions found https www.tensorflow.org get started os setup . running theano, check date master branch theano. update pip install git git github.com theano theano.git upgrade deps x provide link github gist python script reproduce issue copy script short . building siamese network using vgg19 base network. everything went fine throughout training testing. however, noticed training, accuracy quite high took 2 epochs reach 100 accuracy, amazing , testing, accuracy around 0 sometimes even lower, shown . code located repo https github.com ayaz amin ayaznet . run , output run , output expecting output showing ones 0 means similarity, 1 meaning full similarity . pretty sure nothing wrong code. anything wrong? bug? also, using tf.keras tensorflow 2.0."
keras,4580,"hi all, running model contains gru modules rnn keras. training, cpu spikes 400 950 , every spike ends several seconds returns 99 cpu usage baseline. ideally would like cpu stay steady 700 , spiking translates inefficient runtime. .theanorc file suggestions?",1,cpu usage spikes using keras,"cpu usage spikes using keras hi all, running model contains gru modules rnn keras. training, cpu spikes 400 950 , every spike ends several seconds returns 99 cpu usage baseline. ideally would like cpu stay steady 700 , spiking translates inefficient runtime. .theanorc file suggestions?"
keras,11236,"hi, facing increase validation loss using multi gpu model train cnn. kept parameters constant batchsize, learning rate . implemented created single gpu model num gpus 1 import tensorflow tf tf.device ' cpu 0' model create model converted parallel model num gpus 1 parallel model multi gpu model model, gpus num gpus compile run training parallel model.compile parallel model.fit generator ideas might source error? using latest keras current master latest tensorflow 1.10.1 . best, thorsten",1,higher validation loss training loss using multi gpu model,"higher validation loss training loss using multi gpu model hi, facing increase validation loss using multi gpu model train cnn. kept parameters constant batchsize, learning rate . implemented created single gpu model num gpus 1 import tensorflow tf tf.device ' cpu 0' model create model converted parallel model num gpus 1 parallel model multi gpu model model, gpus num gpus compile run training parallel model.compile parallel model.fit generator ideas might source error? using latest keras current master latest tensorflow 1.10.1 . best, thorsten"
keras,10214,"hello everyone! using keras 2.1.6 tf 1.8. want fine tune pre trained inceptionv3 new set classes shown https keras.io applications code literally copy paste documentation. that, noticed accuracy training dataset high one validation set low. immediately suspected fitting wasting several hours could find anything. tried fit evaluate dataset. surprise accuracy reported fit 87.2 one reported evaluate 62.9 dataset. possible? bug something? tried also keras 2.1.5 2.1.4 get problem. something wrong? following documentation.",1,extremely low accuracy finetuning inceptionv3 overfitting,"extremely low accuracy finetuning inceptionv3 overfitting hello everyone! using keras 2.1.6 tf 1.8. want fine tune pre trained inceptionv3 new set classes shown https keras.io applications code literally copy paste documentation. that, noticed accuracy training dataset high one validation set low. immediately suspected fitting wasting several hours could find anything. tried fit evaluate dataset. surprise accuracy reported fit 87.2 one reported evaluate 62.9 dataset. possible? bug something? tried also keras 2.1.5 2.1.4 get problem. something wrong? following documentation."
keras,3562,"suppose missing something obvious progress bar reports much higher training loss validation loss data same. that? pretty much same? split data 80 20, example everything looks fine, slightly worse validation loss training loss, expected.",1,"training data validation data same, come losses differ?","training data validation data same, come losses differ? suppose missing something obvious progress bar reports much higher training loss validation loss data same. that? pretty much same? split data 80 20, example everything looks fine, slightly worse validation loss training loss, expected."
keras,3563,"copied code mnist cnn.py exactly except modified print lines compatible python 2.7. np.random.seed 1337 kept well reproducibility. however, get 98.78 . clues reach 99.25 test accuracy?",1,mnist cnn.py reach 99.25 test accuracy,"mnist cnn.py reach 99.25 test accuracy copied code mnist cnn.py exactly except modified print lines compatible python 2.7. np.random.seed 1337 kept well reproducibility. however, get 98.78 . clues reach 99.25 test accuracy?"
keras,6636,"snippet keras mnist lenet example source https github.com fchollet keras blob master examples mnist cnn.py l57 implementation uses source https github.com fchollet keras blob master keras backend tensorflow backend.py l2750 tensorflow doc source https www.tensorflow.org api docs python tf nn softmax cross entropy logits states warning op expects unscaled logits, since performs softmax logits internally efficiency. call op output softmax, produce incorrect results. missing something violating warning due calling softmax activation?",1,mnist cnn example uses double softmax?,"mnist cnn example uses double softmax? snippet keras mnist lenet example source https github.com fchollet keras blob master examples mnist cnn.py l57 implementation uses source https github.com fchollet keras blob master keras backend tensorflow backend.py l2750 tensorflow doc source https www.tensorflow.org api docs python tf nn softmax cross entropy logits states warning op expects unscaled logits, since performs softmax logits internally efficiency. call op output softmax, produce incorrect results. missing something violating warning due calling softmax activation?"
keras,13293,"imported mobilenetv2 accuracy incredibly low close 0 correct code keras.applications.mobilenet v2 import mobilenetv2 model1 mobilenetv2 weights 'imagenet' import cv2 x cv2.imread 'image input.jpg' x cv2.resize x, 224,224 model1.predict x np.newaxis, , , wheras import vgg resnet keras.applications.vgg16 import vgg16 model2 mobilenetv2 weights 'imagenet' model2.predict x np.newaxis, , , vgg much much better accuracy keras trained mobilenetv2 properly.",1,mobilenetv2 poor accuracy,"mobilenetv2 poor accuracy imported mobilenetv2 accuracy incredibly low close 0 correct code keras.applications.mobilenet v2 import mobilenetv2 model1 mobilenetv2 weights 'imagenet' import cv2 x cv2.imread 'image input.jpg' x cv2.resize x, 224,224 model1.predict x np.newaxis, , , wheras import vgg resnet keras.applications.vgg16 import vgg16 model2 mobilenetv2 weights 'imagenet' model2.predict x np.newaxis, , , vgg much much better accuracy keras trained mobilenetv2 properly."
keras,1006,"using keras build convolution neural networks binary classification. using input data, tried vary model structure i.e. filter size, number filters, number hidden layer neurons better performance. noticed certain models, training accuracy remains unchanged low value 50 training epochs. one model structure follows used whole training set validation get training accuracy end epoch. output epoch looks like plotted filter weights convolution layer epoch stayed epochs minor changes rarely. beginning epoch 1 ! epoch1 batch1 layer0 weight img https cloud.githubusercontent.com assets 7233438 11153096 32e26b80 89fc 11e5 8baa 71a2c2b2ef53.png beginning epoch 3 ! epoch3 batch1 layer0 weight img https cloud.githubusercontent.com assets 7233438 11153097 32ee8bf4 89fc 11e5 8252 6bedd64d4888.png beginning epoch 5 ! epoch5 batch1 layer0 weight img https cloud.githubusercontent.com assets 7233438 11153095 32e1ff56 89fc 11e5 9f39 e5e471a69538.png also tried rmsprop optimizer, accuracy epoch exactly 0.4473 . however filter weights formed different pattern end epoch 1 remained unchanged that. may missed? tried model structure directly theano problem persisted, guess problem keras. identify cause problem, think would helpful track activation gradient values mini batch. keras theano? would right direction look into? thanks lot. cheng",1,accuracy stop improving simple convolution network,"accuracy stop improving simple convolution network using keras build convolution neural networks binary classification. using input data, tried vary model structure i.e. filter size, number filters, number hidden layer neurons better performance. noticed certain models, training accuracy remains unchanged low value 50 training epochs. one model structure follows used whole training set validation get training accuracy end epoch. output epoch looks like plotted filter weights convolution layer epoch stayed epochs minor changes rarely. beginning epoch 1 ! epoch1 batch1 layer0 weight img https cloud.githubusercontent.com assets 7233438 11153096 32e26b80 89fc 11e5 8baa 71a2c2b2ef53.png beginning epoch 3 ! epoch3 batch1 layer0 weight img https cloud.githubusercontent.com assets 7233438 11153097 32ee8bf4 89fc 11e5 8252 6bedd64d4888.png beginning epoch 5 ! epoch5 batch1 layer0 weight img https cloud.githubusercontent.com assets 7233438 11153095 32e1ff56 89fc 11e5 9f39 e5e471a69538.png also tried rmsprop optimizer, accuracy epoch exactly 0.4473 . however filter weights formed different pattern end epoch 1 remained unchanged that. may missed? tried model structure directly theano problem persisted, guess problem keras. identify cause problem, think would helpful track activation gradient values mini batch. keras theano? would right direction look into? thanks lot. cheng"
keras,3572,"trying test fit generator found speed rather slow around less 100 examples processing per second, wondering anything wrong code? train.txt file format 785 floats separate comma, last label.",1,fit generator slow,"fit generator slow trying test fit generator found speed rather slow around less 100 examples processing per second, wondering anything wrong code? train.txt file format 785 floats separate comma, last label."
keras,3576,"training simple neural network keras theano backend consisting 4 dense layers connected merge layer softmax classifier layer. using adam training, first epochs train 60s cpu but, that, training time per epoch starts increasing, taking 400s epoch 70, making unusable. anything wrong code? supposed happen bug? happens using adam, sgd, adadelta, rmsprop adagrad. use methods adam produces far better results. code",1,training adam gets slower epoch,"training adam gets slower epoch training simple neural network keras theano backend consisting 4 dense layers connected merge layer softmax classifier layer. using adam training, first epochs train 60s cpu but, that, training time per epoch starts increasing, taking 400s epoch 70, making unusable. anything wrong code? supposed happen bug? happens using adam, sgd, adadelta, rmsprop adagrad. use methods adam produces far better results. code"
keras,3578,"hi, training 2 hidden layer nn notice convergence faster using model.fit using model.fit generator. optimizer cases 'sgd' batch size 32 cases. val loss decreases much rapidly using fit. expected? val loss calculated differently fit generator? thanks advance",1,fit generator slow converge model.fit,"fit generator slow converge model.fit hi, training 2 hidden layer nn notice convergence faster using model.fit using model.fit generator. optimizer cases 'sgd' batch size 32 cases. val loss decreases much rapidly using fit. expected? val loss calculated differently fit generator? thanks advance"
keras,13311,"hi everyone, tried train dnn model keras, acc val acc got low, could someone give advice solve it? thank much advance! code. import keras import numpy np import pandas pd keras.models import sequential keras.models import model keras.layers import dense, dropout, activation, input keras import optimizers, losses keras.layers.normalization import batchnormalization keras.callbacks import earlystopping keras.utils import np utils, generic utils sklearn.preprocessing import labelencoder sklearn.preprocessing import standardscaler def load data train path, train true df pd.read csv path x df.values.copy train np.random.shuffle x x, labels x , 1 1 .astype np.float32 , x , 1 return x, labels else x, ids x , 1 .astype np.float32 , x , 0 .astype str return x, ids def load data test path df pd.read csv path x df.values.copy x, labels x , 1 1 .astype np.float32 , x , 1 return x, labels def preprocess data x, scaler none scaler scaler standardscaler scaler.fit x x scaler.transform x another type x standerscaler .fit transferom x return x, scaler def preprocess labels labels, encoder none, categorical true encoder encoder labelencoder encoder.fit labels encoder.transform labels .astype np.int32 categorical np utils.to categorical return y, encoder print loading data... x train, labels load data train 'train.csv', train true x train, scaler preprocess data x train train, encoder preprocess labels labels x test, labels load data test 'dev.csv' x test, scaler preprocess data x test test, encoder preprocess labels labels nb classes train.shape 1 print nb classes, 'classes' dims x train.shape 1 print dims, 'dims' print building model... model sequential model.add dense 128, activation 'relu', input dim dims model.add batchnormalization model.add dropout 0.2 model.add dense nb classes, activation 'softmax' early stopping earlystopping monitor 'val loss', patience 30, mode 'auto', verbose 2 sgd optimizers.sgd lr 0.01, decay 1e 6, momentum 0.9, nesterov true model.compile optimizer sgd, loss losses.mean squared logarithmic error, metrics 'accuracy' print training model... hist model.fit x train, train, epochs 200, batch size 200, validation split 0.1 score model.evaluate x test, test, verbose 2 classlabel model.predict classes x test print generating submission... print 'test score ', score 0 print 'test accuracy ', score 1 print model.summary",1,low accuracy,"low accuracy hi everyone, tried train dnn model keras, acc val acc got low, could someone give advice solve it? thank much advance! code. import keras import numpy np import pandas pd keras.models import sequential keras.models import model keras.layers import dense, dropout, activation, input keras import optimizers, losses keras.layers.normalization import batchnormalization keras.callbacks import earlystopping keras.utils import np utils, generic utils sklearn.preprocessing import labelencoder sklearn.preprocessing import standardscaler def load data train path, train true df pd.read csv path x df.values.copy train np.random.shuffle x x, labels x , 1 1 .astype np.float32 , x , 1 return x, labels else x, ids x , 1 .astype np.float32 , x , 0 .astype str return x, ids def load data test path df pd.read csv path x df.values.copy x, labels x , 1 1 .astype np.float32 , x , 1 return x, labels def preprocess data x, scaler none scaler scaler standardscaler scaler.fit x x scaler.transform x another type x standerscaler .fit transferom x return x, scaler def preprocess labels labels, encoder none, categorical true encoder encoder labelencoder encoder.fit labels encoder.transform labels .astype np.int32 categorical np utils.to categorical return y, encoder print loading data... x train, labels load data train 'train.csv', train true x train, scaler preprocess data x train train, encoder preprocess labels labels x test, labels load data test 'dev.csv' x test, scaler preprocess data x test test, encoder preprocess labels labels nb classes train.shape 1 print nb classes, 'classes' dims x train.shape 1 print dims, 'dims' print building model... model sequential model.add dense 128, activation 'relu', input dim dims model.add batchnormalization model.add dropout 0.2 model.add dense nb classes, activation 'softmax' early stopping earlystopping monitor 'val loss', patience 30, mode 'auto', verbose 2 sgd optimizers.sgd lr 0.01, decay 1e 6, momentum 0.9, nesterov true model.compile optimizer sgd, loss losses.mean squared logarithmic error, metrics 'accuracy' print training model... hist model.fit x train, train, epochs 200, batch size 200, validation split 0.1 score model.evaluate x test, test, verbose 2 classlabel model.predict classes x test print generating submission... print 'test score ', score 0 print 'test accuracy ', score 1 print model.summary"
keras,10232,nan,0,use multicore cpu prediction?,use multicore cpu prediction? nan
keras,12148,"! image https user images.githubusercontent.com 32533059 51807246 9ecf6280 22bf 11e9 817a d312001698d2.png hello,i question show picture want using https github.com yhenon keras frcnn 's vgg model , want change model,but search day ,i can't find question solustion discuusion https stats.stackexchange.com questions 282282 spatial dropout 2d implemented exactly need spatial dropout2d define pixel ,this mean need provide 0,0,0,0,0,0,0 0,1,0,1,0,1,0 0,0,0,0,0,0,0 0,1,0,1,0,1,0 0,0,0,0,0,0,0 0,1,0,1,0,1,0 0,0,0,0,0,0,0 like mask drop",0,keras sparse dense?,"keras sparse dense? ! image https user images.githubusercontent.com 32533059 51807246 9ecf6280 22bf 11e9 817a d312001698d2.png hello,i question show picture want using https github.com yhenon keras frcnn 's vgg model , want change model,but search day ,i can't find question solustion discuusion https stats.stackexchange.com questions 282282 spatial dropout 2d implemented exactly need spatial dropout2d define pixel ,this mean need provide 0,0,0,0,0,0,0 0,1,0,1,0,1,0 0,0,0,0,0,0,0 0,1,0,1,0,1,0 0,0,0,0,0,0,0 0,1,0,1,0,1,0 0,0,0,0,0,0,0 like mask drop"
keras,12715,"system information ubuntu 16.04 tensorflow backend yes yes tensorflow version v1.10.0 0 g656e7a2b34 1.10.0 keras version 2.2.4 python version 3.5 describe current behavior training model shared weights using concatenate function using pretrained vgg16 network non trainable layers seen save model without error, try following error appears set layers vgg trainable, error appear. describe expected behavior could load model? code reproduce issue info save load it, error appear. error shown trying load entire model.",0,error loading model shared layers pretrained network,"error loading model shared layers pretrained network system information ubuntu 16.04 tensorflow backend yes yes tensorflow version v1.10.0 0 g656e7a2b34 1.10.0 keras version 2.2.4 python version 3.5 describe current behavior training model shared weights using concatenate function using pretrained vgg16 network non trainable layers seen save model without error, try following error appears set layers vgg trainable, error appear. describe expected behavior could load model? code reproduce issue info save load it, error appear. error shown trying load entire model."
keras,9591,"hi all, created cnn successfully, tried implement model ios project got following error size output layer 'chairconfidence' neural network match number classes classifier. code keras predict properly, ios app got error",0,size output layer 'chairconfidence' neural network match number classes classifier.,"size output layer 'chairconfidence' neural network match number classes classifier. hi all, created cnn successfully, tried implement model ios project got following error size output layer 'chairconfidence' neural network match number classes classifier. code keras predict properly, ios app got error"
keras,11430,"implemented tf operations lambda layer, found layer weights model. train lambda layer load weights? want use lambda layer transform tf tensor keras tensor",0,lambda layer weights?,"lambda layer weights? implemented tf operations lambda layer, found layer weights model. train lambda layer load weights? want use lambda layer transform tf tensor keras tensor"
keras,11304,"source code, find constraints weights keras apply gradient descent updating. could apply weights directly? example, dense layer, kernel constraint unit norm, define x unit norm w rather x w?",0,apply constraints construct graph?,"apply constraints construct graph? source code, find constraints weights keras apply gradient descent updating. could apply weights directly? example, dense layer, kernel constraint unit norm, define x unit norm w rather x w?"
keras,11315,"hello,i question implement lstm encoding decoding text data sequential model api instead functional api.",0,keras sequential model,"keras sequential model hello,i question implement lstm encoding decoding text data sequential model api instead functional api."
keras,12598,supported kernel weights bias weights. propose add capability kernel weights consistant across api.,0,kernel initializer 'zeros' work dense layers,kernel initializer 'zeros' work dense layers supported kernel weights bias weights. propose add capability kernel weights consistant across api.
keras,11867,"dear, scenario description step 1 replaced backbone yolov3 darknet53 mobilenet. mobilenet weights loaded imagenet, using . step 2 also want load pre trained weights detector yolov3, download .cfg .weights files https pjreddie.com darknet yolo find file links shown original page. yolov3 416 coco trainval test dev 55.3 65.86 bn 35 cfg weights obviously, .weights .cfg files yolov3 416 include weights yolov3 detector, backbone, darknet53. step 3 develop new yolo body yolo body backbone yolo detector , using, example, weights path place save .weights file step 2 course, .cfg .weights converted .h5 file . comes issue new yolo body composed mobilenet yolov3, loaded pre trained .h5 weights darknet53 yolov3. guarantee load correct weights correct model, scenario, obviously, want load yolov3 detector part new yolo body, darknet53 weights .h5 file automatically dropped. guarantee this, formal way? simply setting skip mismatch true ?",0,"guarantee keras model loading pretrained weights correctly, setting skip mismatch true ?","guarantee keras model loading pretrained weights correctly, setting skip mismatch true ? dear, scenario description step 1 replaced backbone yolov3 darknet53 mobilenet. mobilenet weights loaded imagenet, using . step 2 also want load pre trained weights detector yolov3, download .cfg .weights files https pjreddie.com darknet yolo find file links shown original page. yolov3 416 coco trainval test dev 55.3 65.86 bn 35 cfg weights obviously, .weights .cfg files yolov3 416 include weights yolov3 detector, backbone, darknet53. step 3 develop new yolo body yolo body backbone yolo detector , using, example, weights path place save .weights file step 2 course, .cfg .weights converted .h5 file . comes issue new yolo body composed mobilenet yolov3, loaded pre trained .h5 weights darknet53 yolov3. guarantee load correct weights correct model, scenario, obviously, want load yolov3 detector part new yolo body, darknet53 weights .h5 file automatically dropped. guarantee this, formal way? simply setting skip mismatch true ?"
keras,9877,"feature request keras blog 1 explains nicely, small dataset augmented following code keras.preprocessing.image import imagedatagenerator datagen imagedatagenerator rotation range 40, width shift range 0.2, height shift range 0.2, rescale 1. 255, shear range 0.2, zoom range 0.2, horizontal flip true, fill mode 'nearest' sure vanilla example introduced blog works well, similarly simple scenarios. much complicated scenario, want use weights models pretrained famous coco dataset object detection 2 , transfer learn new classes, limited amount data 1000 . labeling granularity datasets per image, per objects inside images. i.e., image may contain one objects marked polygonical bounding boxes bounding boxes labeled according object names contain. complex labeling information encoded json format, like following example info year 2018, version null, description peaches , contributor ralph r4robotics.com.au , url labelbox.io , date created 2018 04 07t10 08 51.409340 00 00 , images id cjfp6vz7xfwz20198ixce9la4 , width 274, height 184, file name https firebasestorage.googleapis.com v0 b labelbox 193903.appspot.com cjfp6hjghfuvd01147d130984 2f5a7fdf5d 201a 40d0 bfef c36d6ed02212 2fpeach8.jpg?alt media token 11337eaa 4ffd 4dfb b3ec 9c4ee6bd2f17 , license null, flickr url https firebasestorage.googleapis.com v0 b labelbox 193903.appspot.com cjfp6hjghfuvd01147d130984 2f5a7fdf5d 201a 40d0 bfef c36d6ed02212 2fpeach8.jpg?alt media token 11337eaa 4ffd 4dfb b3ec 9c4ee6bd2f17 , coco url https firebasestorage.googleapis.com v0 b labelbox 193903.appspot.com cjfp6hjghfuvd01147d130984 2f5a7fdf5d 201a 40d0 bfef c36d6ed02212 2fpeach8.jpg?alt media token 11337eaa 4ffd 4dfb b3ec 9c4ee6bd2f17 , date captured null , id cjfp6wqfhfwyu0107il09db3p , width 275, height 183, file name https firebasestorage.googleapis.com v0 b labelbox 193903.appspot.com cjfp6hjghfuvd01147d130984 2f5a7fdf5d 201a 40d0 bfef c36d6ed02212 2fpeach9.jpg?alt media token 39dd5e97 c411 43e9 9ba3 9f51a334c7c7 , license null, flickr url https firebasestorage.googleapis.com v0 b labelbox 193903.appspot.com cjfp6hjghfuvd01147d130984 2f5a7fdf5d 201a 40d0 bfef c36d6ed02212 2fpeach9.jpg?alt media token 39dd5e97 c411 43e9 9ba3 9f51a334c7c7 , coco url https firebasestorage.googleapis.com v0 b labelbox 193903.appspot.com cjfp6hjghfuvd01147d130984 2f5a7fdf5d 201a 40d0 bfef c36d6ed02212 2fpeach9.jpg?alt media token 39dd5e97 c411 43e9 9ba3 9f51a334c7c7 , date captured null , annotations id 23, image id cjfp6vz7xfwz20198ixce9la4 , category id 1, segmentation 31.0, 72.0, 63.0, 84.0, 75.0, 105.0, 67.0, 134.0, 68.0, 158.0, 44.0, 174.0, 24.0, 178.0, 2.0, 172.0, 2.0, 82.0, 31.0, 72.0 , area 6301.0, bbox 2.0, 6.0, 73.0, 106.0 , iscrowd 0 , id 24, image id cjfp6vz7xfwz20198ixce9la4 , category id 1, segmentation 75.0, 103.0, 108.0, 76.0, 137.0, 74.0, 166.0, 89.0, 182.0, 104.0, 188.0, 145.0, 179.0, 171.0, 167.0, 183.0, 92.0, 183.0, 72.0, 158.0, 68.0, 134.0, 75.0, 103.0 , area 10652.5, bbox 68.0, 1.0, 120.0, 109.0 , iscrowd 0 , id 25, image id cjfp6vz7xfwz20198ixce9la4 , category id 1, segmentation 169.0, 92.0, 182.0, 66.0, 211.0, 53.0, 246.0, 66.0, 262.0, 80.0, 268.0, 95.0, 261.0, 129.0, 241.0, 145.0, 216.0, 153.0, 188.0, 143.0, 184.0, 105.0, 169.0, 92.0 , area 6838.5, bbox 169.0, 31.0, 99.0, 100.0 , iscrowd 0 , id 26, image id cjfp6wqfhfwyu0107il09db3p , category id 1, segmentation 86.0, 54.0, 109.0, 56.0, 119.0, 73.0, 113.0, 92.0, 93.0, 101.0, 76.0, 92.0, 70.0, 77.0, 71.0, 63.0, 86.0, 54.0 , area 1715.0, bbox 70.0, 82.0, 49.0, 47.0 , iscrowd 0 , id 27, image id cjfp6wqfhfwyu0107il09db3p , category id 1, segmentation 117.0, 95.0, 123.0, 110.0, 136.0, 118.0, 153.0, 113.0, 159.0, 99.0, 158.0, 87.0, 145.0, 79.0, 132.0, 76.0, 123.0, 84.0, 117.0, 95.0 , area 1260.0, bbox 117.0, 65.0, 42.0, 42.0 , iscrowd 0 , id 28, image id cjfp6wqfhfwyu0107il09db3p , category id 1, segmentation 109.0, 54.0, 115.0, 40.0, 133.0, 32.0, 146.0, 34.0, 157.0, 43.0, 161.0, 58.0, 152.0, 72.0, 133.0, 76.0, 119.0, 71.0, 109.0, 54.0 , area 1660.5, bbox 109.0, 107.0, 52.0, 44.0 , iscrowd 0 , licenses , categories supercategory peach , id 1, name peach obviously, augmentation scenario bit complicated, since images distorted rotated, also bounding boxes. would make keras much powerful save users lot painstaking labeling work, dataset could augmented within keras pipeline. increasing relevance object detection, might feature consider. 1 https blog.keras.io building powerful image classification models using little data.html 2 http cocodataset.org home",0,data augmentation object detection fully convolutional networks,"data augmentation object detection fully convolutional networks feature request keras blog 1 explains nicely, small dataset augmented following code keras.preprocessing.image import imagedatagenerator datagen imagedatagenerator rotation range 40, width shift range 0.2, height shift range 0.2, rescale 1. 255, shear range 0.2, zoom range 0.2, horizontal flip true, fill mode 'nearest' sure vanilla example introduced blog works well, similarly simple scenarios. much complicated scenario, want use weights models pretrained famous coco dataset object detection 2 , transfer learn new classes, limited amount data 1000 . labeling granularity datasets per image, per objects inside images. i.e., image may contain one objects marked polygonical bounding boxes bounding boxes labeled according object names contain. complex labeling information encoded json format, like following example info year 2018, version null, description peaches , contributor ralph r4robotics.com.au , url labelbox.io , date created 2018 04 07t10 08 51.409340 00 00 , images id cjfp6vz7xfwz20198ixce9la4 , width 274, height 184, file name https firebasestorage.googleapis.com v0 b labelbox 193903.appspot.com cjfp6hjghfuvd01147d130984 2f5a7fdf5d 201a 40d0 bfef c36d6ed02212 2fpeach8.jpg?alt media token 11337eaa 4ffd 4dfb b3ec 9c4ee6bd2f17 , license null, flickr url https firebasestorage.googleapis.com v0 b labelbox 193903.appspot.com cjfp6hjghfuvd01147d130984 2f5a7fdf5d 201a 40d0 bfef c36d6ed02212 2fpeach8.jpg?alt media token 11337eaa 4ffd 4dfb b3ec 9c4ee6bd2f17 , coco url https firebasestorage.googleapis.com v0 b labelbox 193903.appspot.com cjfp6hjghfuvd01147d130984 2f5a7fdf5d 201a 40d0 bfef c36d6ed02212 2fpeach8.jpg?alt media token 11337eaa 4ffd 4dfb b3ec 9c4ee6bd2f17 , date captured null , id cjfp6wqfhfwyu0107il09db3p , width 275, height 183, file name https firebasestorage.googleapis.com v0 b labelbox 193903.appspot.com cjfp6hjghfuvd01147d130984 2f5a7fdf5d 201a 40d0 bfef c36d6ed02212 2fpeach9.jpg?alt media token 39dd5e97 c411 43e9 9ba3 9f51a334c7c7 , license null, flickr url https firebasestorage.googleapis.com v0 b labelbox 193903.appspot.com cjfp6hjghfuvd01147d130984 2f5a7fdf5d 201a 40d0 bfef c36d6ed02212 2fpeach9.jpg?alt media token 39dd5e97 c411 43e9 9ba3 9f51a334c7c7 , coco url https firebasestorage.googleapis.com v0 b labelbox 193903.appspot.com cjfp6hjghfuvd01147d130984 2f5a7fdf5d 201a 40d0 bfef c36d6ed02212 2fpeach9.jpg?alt media token 39dd5e97 c411 43e9 9ba3 9f51a334c7c7 , date captured null , annotations id 23, image id cjfp6vz7xfwz20198ixce9la4 , category id 1, segmentation 31.0, 72.0, 63.0, 84.0, 75.0, 105.0, 67.0, 134.0, 68.0, 158.0, 44.0, 174.0, 24.0, 178.0, 2.0, 172.0, 2.0, 82.0, 31.0, 72.0 , area 6301.0, bbox 2.0, 6.0, 73.0, 106.0 , iscrowd 0 , id 24, image id cjfp6vz7xfwz20198ixce9la4 , category id 1, segmentation 75.0, 103.0, 108.0, 76.0, 137.0, 74.0, 166.0, 89.0, 182.0, 104.0, 188.0, 145.0, 179.0, 171.0, 167.0, 183.0, 92.0, 183.0, 72.0, 158.0, 68.0, 134.0, 75.0, 103.0 , area 10652.5, bbox 68.0, 1.0, 120.0, 109.0 , iscrowd 0 , id 25, image id cjfp6vz7xfwz20198ixce9la4 , category id 1, segmentation 169.0, 92.0, 182.0, 66.0, 211.0, 53.0, 246.0, 66.0, 262.0, 80.0, 268.0, 95.0, 261.0, 129.0, 241.0, 145.0, 216.0, 153.0, 188.0, 143.0, 184.0, 105.0, 169.0, 92.0 , area 6838.5, bbox 169.0, 31.0, 99.0, 100.0 , iscrowd 0 , id 26, image id cjfp6wqfhfwyu0107il09db3p , category id 1, segmentation 86.0, 54.0, 109.0, 56.0, 119.0, 73.0, 113.0, 92.0, 93.0, 101.0, 76.0, 92.0, 70.0, 77.0, 71.0, 63.0, 86.0, 54.0 , area 1715.0, bbox 70.0, 82.0, 49.0, 47.0 , iscrowd 0 , id 27, image id cjfp6wqfhfwyu0107il09db3p , category id 1, segmentation 117.0, 95.0, 123.0, 110.0, 136.0, 118.0, 153.0, 113.0, 159.0, 99.0, 158.0, 87.0, 145.0, 79.0, 132.0, 76.0, 123.0, 84.0, 117.0, 95.0 , area 1260.0, bbox 117.0, 65.0, 42.0, 42.0 , iscrowd 0 , id 28, image id cjfp6wqfhfwyu0107il09db3p , category id 1, segmentation 109.0, 54.0, 115.0, 40.0, 133.0, 32.0, 146.0, 34.0, 157.0, 43.0, 161.0, 58.0, 152.0, 72.0, 133.0, 76.0, 119.0, 71.0, 109.0, 54.0 , area 1660.5, bbox 109.0, 107.0, 52.0, 44.0 , iscrowd 0 , licenses , categories supercategory peach , id 1, name peach obviously, augmentation scenario bit complicated, since images distorted rotated, also bounding boxes. would make keras much powerful save users lot painstaking labeling work, dataset could augmented within keras pipeline. increasing relevance object detection, might feature consider. 1 https blog.keras.io building powerful image classification models using little data.html 2 http cocodataset.org home"
keras,11779,note created fchollet requests contributions .,0,add masking support convolution layers least determine whether feasible .,add masking support convolution layers least determine whether feasible . note created fchollet requests contributions .
keras,11055,! tim 20180901192728 https user images.githubusercontent.com 5326601 44947933 66fde500 ae1d 11e8 96fd 43ace664425f.png,0,"using pycharm remote run, progress bar refresh place.","using pycharm remote run, progress bar refresh place. ! tim 20180901192728 https user images.githubusercontent.com 5326601 44947933 66fde500 ae1d 11e8 96fd 43ace664425f.png"
keras,10612,"please make sure boxes checked submit issue. issue implementation question, please ask question stackoverflow http stackoverflow.com questions tagged keras join keras slack channel https keras slack autojoin.herokuapp.com ask instead filing github issue. thank you! x check date master branch keras. update pip install git git github.com keras team keras.git upgrade deps x running tensorflow, check date latest version. installation instructions found https www.tensorflow.org get started os setup . x provide link github gist python script reproduce issue copy script short . attempting use multiple sessions , kernel crashes without error. possibly oom problem, currently know. code crashes around 3 4 me, good. os windows 10 running cpu",0,keras crashes multiple sessions,"keras crashes multiple sessions please make sure boxes checked submit issue. issue implementation question, please ask question stackoverflow http stackoverflow.com questions tagged keras join keras slack channel https keras slack autojoin.herokuapp.com ask instead filing github issue. thank you! x check date master branch keras. update pip install git git github.com keras team keras.git upgrade deps x running tensorflow, check date latest version. installation instructions found https www.tensorflow.org get started os setup . x provide link github gist python script reproduce issue copy script short . attempting use multiple sessions , kernel crashes without error. possibly oom problem, currently know. code crashes around 3 4 me, good. os windows 10 running cpu"
keras,8657,nan,0,introduction global metrics precision recall,introduction global metrics precision recall nan
keras,9186,"hello, wanted use option using . object one hot encoded, know keys use dict. turns , mapping determined using using column index output make clearer documentation function? thank much, pierre",0,clearer doc class weights,"clearer doc class weights hello, wanted use option using . object one hot encoded, know keys use dict. turns , mapping determined using using column index output make clearer documentation function? thank much, pierre"
keras,11371,"looking keras documentation, noticed default keras.callbacks.earlystopping restore best weights false. documentation, described behaviour restore best weights whether restore model weights epoch best value monitored quantity. false, model weights obtained last step training used. find bit confusing think principle early stopping obtain best results validation set stopping optimisation validation loss increases, case without setting patience parameter. however, early stopping makes sense set patience parameter might get spurious overfittings due momentary local optima. case, would definitely want restore previous weights, might severely overfit end patience period. furthermore, could think reason would want restore best weights early stopping. case think want stop training significant improvement loss, order reduce training time, purpose early stopping. wondering use case make false default behaviour not, would make sense change this?",0,weight restoring default option earlystopping?,"weight restoring default option earlystopping? looking keras documentation, noticed default keras.callbacks.earlystopping restore best weights false. documentation, described behaviour restore best weights whether restore model weights epoch best value monitored quantity. false, model weights obtained last step training used. find bit confusing think principle early stopping obtain best results validation set stopping optimisation validation loss increases, case without setting patience parameter. however, early stopping makes sense set patience parameter might get spurious overfittings due momentary local optima. case, would definitely want restore previous weights, might severely overfit end patience period. furthermore, could think reason would want restore best weights early stopping. case think want stop training significant improvement loss, order reduce training time, purpose early stopping. wondering use case make false default behaviour not, would make sense change this?"
keras,9284,keras team official large model zoo like caffe2 https github.com caffe2 caffe2 wiki model zoo ? see following repositories github indeed insufficient https github.com albertomontesg keras model zoo https github.com yoctol yoctol keras layer zoo https github.com david vazquez keras zoo https github.com gkalliatakis keras application zoo best way get people start sharing pretrained models terms following 1. opening issue 2. discussing mailing list 3. send email major contributors keras even committers please let know questions concerns. thanks!,0,large model zoo keras along like caffe2 even better community contributors,large model zoo keras along like caffe2 even better community contributors keras team official large model zoo like caffe2 https github.com caffe2 caffe2 wiki model zoo ? see following repositories github indeed insufficient https github.com albertomontesg keras model zoo https github.com yoctol yoctol keras layer zoo https github.com david vazquez keras zoo https github.com gkalliatakis keras application zoo best way get people start sharing pretrained models terms following 1. opening issue 2. discussing mailing list 3. send email major contributors keras even committers please let know questions concerns. thanks!
keras,11194,"please give warning using default parameter . spent hour debugging magic number '32' model, occur anywhere code. successfully training, following failed input sample finally found culprit https keras.io models model predict batch size integer. unspecified, default 32. either use shape sample batch size, one model training use '1' default usually want predict one item anyways , fallback arbitrary default value without warning please.",0,batch size dangerous default,"batch size dangerous default please give warning using default parameter . spent hour debugging magic number '32' model, occur anywhere code. successfully training, following failed input sample finally found culprit https keras.io models model predict batch size integer. unspecified, default 32. either use shape sample batch size, one model training use '1' default usually want predict one item anyways , fallback arbitrary default value without warning please."
keras,10366,https keras.io preprocessing image imagedatagenerator class brightness range documented.,0,brightness range documented,brightness range documented https keras.io preprocessing image imagedatagenerator class brightness range documented.
keras,12879,"currently working regression problem need preprocess raw input data certain way what's begin done relevant feed neural network. aware preprocessing usually done outside model, also need gradients output network wrt. raw input data used loss function , need preprocessing included computational graph. gathered, need feed resulting tensor input neural network order two parts connected graph. important note preprocessing done entire training set, meaning needs done once. successfully implemented using keras backend functions, problem preprocessing part seems performed every forward pass model, really unnecessary, really slow. hope input tensor would treated static differentiable input network, forward backward propagation simply flowing network input output training, seem working way. code quite large intricate, leave semi pseudic code simply show gist trying do. preprocessing function added print statement tensor prints whenever function called, indeed print every epoch training. also happens even include gradient loss function, showing likely prints forward pass. model seem train predict intended however, implementation seems correct. might flaws logic understanding graph works, hoping simple fix problem. summarize 1. feed constant , differentiable tensor input neural network, without tensor calculated anew forward pass network? preprocessed tensor contains training samples, would also like split train smaller batches, 2. given exists solution 1., batch training tie this? create generator fit fit generator, possible let keras handle fit? hope problem clear enough, grateful help leads!",0,get gradient preprocessing without forward passing time,"get gradient preprocessing without forward passing time currently working regression problem need preprocess raw input data certain way what's begin done relevant feed neural network. aware preprocessing usually done outside model, also need gradients output network wrt. raw input data used loss function , need preprocessing included computational graph. gathered, need feed resulting tensor input neural network order two parts connected graph. important note preprocessing done entire training set, meaning needs done once. successfully implemented using keras backend functions, problem preprocessing part seems performed every forward pass model, really unnecessary, really slow. hope input tensor would treated static differentiable input network, forward backward propagation simply flowing network input output training, seem working way. code quite large intricate, leave semi pseudic code simply show gist trying do. preprocessing function added print statement tensor prints whenever function called, indeed print every epoch training. also happens even include gradient loss function, showing likely prints forward pass. model seem train predict intended however, implementation seems correct. might flaws logic understanding graph works, hoping simple fix problem. summarize 1. feed constant , differentiable tensor input neural network, without tensor calculated anew forward pass network? preprocessed tensor contains training samples, would also like split train smaller batches, 2. given exists solution 1., batch training tie this? create generator fit fit generator, possible let keras handle fit? hope problem clear enough, grateful help leads!"
keras,11579,"new keras world trying build feedfoward net output vector parameters used linear ensembler, want minimize mse ensembler, then, created following loss function, g data matrix single estimators predictions, multiplyed theta net output gives real predictions. next, create model follows, course run, cause train single output meanwhile want train vector outputs, well, would pass sample index batch order use proper rows g data loss function calculation. ideas working around this?",0,custom output layer loss function,"custom output layer loss function new keras world trying build feedfoward net output vector parameters used linear ensembler, want minimize mse ensembler, then, created following loss function, g data matrix single estimators predictions, multiplyed theta net output gives real predictions. next, create model follows, course run, cause train single output meanwhile want train vector outputs, well, would pass sample index batch order use proper rows g data loss function calculation. ideas working around this?"
keras,3510,"mnist siamese example, two mnist images mapped onto vector. two vectors merged using euclidian distance. distance low images belong number high two images represent different images. questions line 59, labels set positive negative examples. euclidian distance numbers low, target 1 0 negative examples? also, vectors created using relu normally max value. euclidian distance kept 0 1?",0,question mnist siamese example,"question mnist siamese example mnist siamese example, two mnist images mapped onto vector. two vectors merged using euclidian distance. distance low images belong number high two images represent different images. questions line 59, labels set positive negative examples. euclidian distance numbers low, target 1 0 negative examples? also, vectors created using relu normally max value. euclidian distance kept 0 1?"
keras,7488,"documentation embedding layer bit clear... embedding layer performs word2vec something different? performs word2vec, would suggest mention documentation https keras.io layers embeddings add relevant reference. opinion current reference dropout rnn https arxiv.org abs 1512.05287 directly connected word2vec. keras keras layers embeddings.py https github.com fchollet keras blob master keras layers embeddings.py l11",0,embedding layer documentation,"embedding layer documentation documentation embedding layer bit clear... embedding layer performs word2vec something different? performs word2vec, would suggest mention documentation https keras.io layers embeddings add relevant reference. opinion current reference dropout rnn https arxiv.org abs 1512.05287 directly connected word2vec. keras keras layers embeddings.py https github.com fchollet keras blob master keras layers embeddings.py l11"
keras,53,"hey guys, cool project. theano interface really horrific putting. maybe wrong way access activations different layers? similar predict computed half way. would really useful analysis likes.",0,accessing internal states,"accessing internal states hey guys, cool project. theano interface really horrific putting. maybe wrong way access activations different layers? similar predict computed half way. would really useful analysis likes."
keras,2398,"hi, implement fully convolutional net keras, want able work input arbitrary size. case 1d data arbitrary number timesteps. use none instruction input could size 1st dimension, e.g. last layer fully cn model use actual length sequence filter length order obtain sequence labels correctly distributed along original timespan, e.g. soon know actual sequence length runtime, following options 1. omit parameter mandatory work 2. use none value filter length order tell keras value available runtime see input shape cause error see output 3. use kind placeholder value network buildup hope magically replaced real sequence length runtime. use option looks like works, wonder correct developers deal issue? may worth implementing filter length none option arbitrary sequence? example code https gist.github.com lukovkin e57dc3d40c9148a65c2bf40ea6360e45 environment python 3.4, tf 0.8, keras 1.0.1 attributeerror traceback recent call last root miniconda2 envs tf lib python3.4 site packages numpy core fromnumeric.py prod a, axis, dtype, out, keepdims 2488 try 2489 prod a.prod 2490 except attributeerror attributeerror 'tuple' object attribute 'prod' handling exception, another exception occurred typeerror traceback recent call last 1 model ufcnn model deconv regression false, output dim 3, features 4, 2 loss categorical crossentropy , sequence length 500, optimizer rmsprop notebook ufcnn keras models ufcnn functional.py ufcnn model deconv sequence length, features, nb filter, filter length, output dim, optimizer, loss, regression, class mode, activation, init 285 286 else 287 conv9 convolution1d nb filter output dim, filter length none, border mode 'same', init init, name 'conv9' relu8 288 activation activation 'softmax', name 'activation' conv9 289 output activation root miniconda2 envs tf lib python3.4 site packages keras 1.0.1 py3.4.egg keras engine topology.py call self, x, mask 456 '' 457 len input shapes 1 458 self.build input shapes 0 459 else 460 self.build input shapes root miniconda2 envs tf lib python3.4 site packages keras 1.0.1 py3.4.egg keras layers convolutional.py build self, input shape 118 input dim input shape 2 119 self.w shape self.nb filter, input dim, self.filter length, 1 120 self.w self.init self.w shape, name ' w'.format self.name 121 self.b k.zeros self.nb filter, , name ' b'.format self.name 122 self.trainable weights self.w, self.b root miniconda2 envs tf lib python3.4 site packages keras 1.0.1 py3.4.egg keras initializations.py lecun uniform shape, name, dim ordering 41 http yann.lecun.com exdb publis pdf lecun 98b.pdf 42 ''' 43 fan in, fan get fans shape, dim ordering dim ordering 44 scale np.sqrt 3. fan 45 return uniform shape, scale, name name root miniconda2 envs tf lib python3.4 site packages keras 1.0.1 py3.4.egg keras initializations.py get fans shape, dim ordering 13 tf kernel shape ..., input depth, depth 14 dim ordering 'th' 15 fan np.prod shape 1 16 fan shape 0 17 elif dim ordering 'tf' root miniconda2 envs tf lib python3.4 site packages numpy core fromnumeric.py prod a, axis, dtype, out, keepdims 2490 except attributeerror 2491 return methods. prod a, axis axis, dtype dtype, 2492 out, keepdims keepdims 2493 return prod axis axis, dtype dtype, 2494 else root miniconda2 envs tf lib python3.4 site packages numpy core methods.py prod a, axis, dtype, out, keepdims 33 34 def prod a, axis none, dtype none, none, keepdims false 35 return umr prod a, axis, dtype, out, keepdims 36 37 def a, axis none, dtype none, none, keepdims false typeerror unsupported operand type 'int' 'nonetype' please make sure boxes checked submit issue. thank you! x check date master branch keras. update pip install git git github.com fchollet keras.git upgrade deps x running theano, check date master branch theano. update pip install git git github.com theano theano.git upgrade deps x provide link github gist python script reproduce issue copy script short .",0,correct value 'filter length' final layer fully convolutional net,"correct value 'filter length' final layer fully convolutional net hi, implement fully convolutional net keras, want able work input arbitrary size. case 1d data arbitrary number timesteps. use none instruction input could size 1st dimension, e.g. last layer fully cn model use actual length sequence filter length order obtain sequence labels correctly distributed along original timespan, e.g. soon know actual sequence length runtime, following options 1. omit parameter mandatory work 2. use none value filter length order tell keras value available runtime see input shape cause error see output 3. use kind placeholder value network buildup hope magically replaced real sequence length runtime. use option looks like works, wonder correct developers deal issue? may worth implementing filter length none option arbitrary sequence? example code https gist.github.com lukovkin e57dc3d40c9148a65c2bf40ea6360e45 environment python 3.4, tf 0.8, keras 1.0.1 attributeerror traceback recent call last root miniconda2 envs tf lib python3.4 site packages numpy core fromnumeric.py prod a, axis, dtype, out, keepdims 2488 try 2489 prod a.prod 2490 except attributeerror attributeerror 'tuple' object attribute 'prod' handling exception, another exception occurred typeerror traceback recent call last 1 model ufcnn model deconv regression false, output dim 3, features 4, 2 loss categorical crossentropy , sequence length 500, optimizer rmsprop notebook ufcnn keras models ufcnn functional.py ufcnn model deconv sequence length, features, nb filter, filter length, output dim, optimizer, loss, regression, class mode, activation, init 285 286 else 287 conv9 convolution1d nb filter output dim, filter length none, border mode 'same', init init, name 'conv9' relu8 288 activation activation 'softmax', name 'activation' conv9 289 output activation root miniconda2 envs tf lib python3.4 site packages keras 1.0.1 py3.4.egg keras engine topology.py call self, x, mask 456 '' 457 len input shapes 1 458 self.build input shapes 0 459 else 460 self.build input shapes root miniconda2 envs tf lib python3.4 site packages keras 1.0.1 py3.4.egg keras layers convolutional.py build self, input shape 118 input dim input shape 2 119 self.w shape self.nb filter, input dim, self.filter length, 1 120 self.w self.init self.w shape, name ' w'.format self.name 121 self.b k.zeros self.nb filter, , name ' b'.format self.name 122 self.trainable weights self.w, self.b root miniconda2 envs tf lib python3.4 site packages keras 1.0.1 py3.4.egg keras initializations.py lecun uniform shape, name, dim ordering 41 http yann.lecun.com exdb publis pdf lecun 98b.pdf 42 ''' 43 fan in, fan get fans shape, dim ordering dim ordering 44 scale np.sqrt 3. fan 45 return uniform shape, scale, name name root miniconda2 envs tf lib python3.4 site packages keras 1.0.1 py3.4.egg keras initializations.py get fans shape, dim ordering 13 tf kernel shape ..., input depth, depth 14 dim ordering 'th' 15 fan np.prod shape 1 16 fan shape 0 17 elif dim ordering 'tf' root miniconda2 envs tf lib python3.4 site packages numpy core fromnumeric.py prod a, axis, dtype, out, keepdims 2490 except attributeerror 2491 return methods. prod a, axis axis, dtype dtype, 2492 out, keepdims keepdims 2493 return prod axis axis, dtype dtype, 2494 else root miniconda2 envs tf lib python3.4 site packages numpy core methods.py prod a, axis, dtype, out, keepdims 33 34 def prod a, axis none, dtype none, none, keepdims false 35 return umr prod a, axis, dtype, out, keepdims 36 37 def a, axis none, dtype none, none, keepdims false typeerror unsupported operand type 'int' 'nonetype' please make sure boxes checked submit issue. thank you! x check date master branch keras. update pip install git git github.com fchollet keras.git upgrade deps x running theano, check date master branch theano. update pip install git git github.com theano theano.git upgrade deps x provide link github gist python script reproduce issue copy script short ."
keras,2608,"hello everybody, would like implement resnet network shortcut connections add zero entries features channels dimensions mismatch according original paper dimensions increase dotted line shortcuts fig. 3 , consider two options shortcut still performs identity mapping, extra zero entries padded increasing dimensions http arxiv.org pdf 1512.03385v1.pdf however able implement can't seem find answer web source code. implementations found use 1x1 convolution trick shortcut connections dimensions mismatch. layer would like implement would basically concatenate input tensor tensor zeros tensor compensate dimension mismatch. idea would something like this, could get working anyone idea implement layer ? thanks lot",0,zero padding resnet shortcut connections channel number increase,"zero padding resnet shortcut connections channel number increase hello everybody, would like implement resnet network shortcut connections add zero entries features channels dimensions mismatch according original paper dimensions increase dotted line shortcuts fig. 3 , consider two options shortcut still performs identity mapping, extra zero entries padded increasing dimensions http arxiv.org pdf 1512.03385v1.pdf however able implement can't seem find answer web source code. implementations found use 1x1 convolution trick shortcut connections dimensions mismatch. layer would like implement would basically concatenate input tensor tensor zeros tensor compensate dimension mismatch. idea would something like this, could get working anyone idea implement layer ? thanks lot"
keras,8446,"please make sure boxes checked submit issue. issue implementation question, please ask question stackoverflow http stackoverflow.com questions tagged keras join keras slack channel https keras slack autojoin.herokuapp.com ask instead filing github issue. thank you! x check date master branch keras. update pip install git git github.com fchollet keras.git upgrade deps x running tensorflow, check date latest version. installation instructions found https www.tensorflow.org get started os setup . short simple issue. following upgrading keras 2.0.9 using multi gpu model utility seem save models best weights using model.save. error get suspect problem gaining access model object. work around issue?",0,save model using model.save following multi gpu model,"save model using model.save following multi gpu model please make sure boxes checked submit issue. issue implementation question, please ask question stackoverflow http stackoverflow.com questions tagged keras join keras slack channel https keras slack autojoin.herokuapp.com ask instead filing github issue. thank you! x check date master branch keras. update pip install git git github.com fchollet keras.git upgrade deps x running tensorflow, check date latest version. installation instructions found https www.tensorflow.org get started os setup . short simple issue. following upgrading keras 2.0.9 using multi gpu model utility seem save models best weights using model.save. error get suspect problem gaining access model object. work around issue?"
keras,9496,"vae https github.com keras team keras blob master examples variational autoencoder.py l58 example, model compiled like latest master branch, compile method required arguments optimizer loss passed.",0,outdated vae example,"outdated vae example vae https github.com keras team keras blob master examples variational autoencoder.py l58 example, model compiled like latest master branch, compile method required arguments optimizer loss passed."
keras,5615,"error typeerror added layer must instance class layer. found tensor input 1 0 , shape ?, 784 , dtype float32",0,input layer requires instance class layer?,"input layer requires instance class layer? error typeerror added layer must instance class layer. found tensor input 1 0 , shape ?, 784 , dtype float32"
keras,13341,"system information written custom code opposed using example directory os platform distribution e.g., linux ubuntu 16.04 tensorflow backend yes tensorflow version 1.14.0 keras version 2.3.0 python version cuda cudnn version gpu model memory describe current behavior file usr local lib python3.6 dist packages keras engine training.py , line 222, compile masks masks file usr local lib python3.6 dist packages keras engine training.py , line 871, handle metrics self. per output metrics , target, output, output mask file usr local lib python3.6 dist packages keras engine training.py , line 842, handle per output metrics metric fn, true, pred, weights weights, mask mask file usr local lib python3.6 dist packages keras engine training utils.py , line 1022, call metric function mask math ops.cast mask, pred.dtype nameerror name 'math ops' defined describe expected behavior code reproduce issue info logs",0,nameerror name 'math ops' defined,"nameerror name 'math ops' defined system information written custom code opposed using example directory os platform distribution e.g., linux ubuntu 16.04 tensorflow backend yes tensorflow version 1.14.0 keras version 2.3.0 python version cuda cudnn version gpu model memory describe current behavior file usr local lib python3.6 dist packages keras engine training.py , line 222, compile masks masks file usr local lib python3.6 dist packages keras engine training.py , line 871, handle metrics self. per output metrics , target, output, output mask file usr local lib python3.6 dist packages keras engine training.py , line 842, handle per output metrics metric fn, true, pred, weights weights, mask mask file usr local lib python3.6 dist packages keras engine training utils.py , line 1022, call metric function mask math ops.cast mask, pred.dtype nameerror name 'math ops' defined describe expected behavior code reproduce issue info logs"
keras,2820,nan,0,self.pool size tuple pool size typeerror 'int' object iterable,self.pool size tuple pool size typeerror 'int' object iterable nan
keras,4015,imported use callbacks got know it? appreciate.,0,keras callbacks seem work import,keras callbacks seem work import imported use callbacks got know it? appreciate.
keras,5646,working brain tumor segmentation mri images using cnn. know implement segmentation cnn ? please help this. thanks kj,0,image segmentation,image segmentation working brain tumor segmentation mri images using cnn. know implement segmentation cnn ? please help this. thanks kj
keras,11460,"! img 20181023 154623 https user images.githubusercontent.com 25239334 47344598 75f56b00 d6db 11e8 86a5 a81732ee33d0.jpg hi, network like this. want stop backpropagation cnn inception. inception get gradients lstm. ? thank help.",0,stop backpropagation,"stop backpropagation ! img 20181023 154623 https user images.githubusercontent.com 25239334 47344598 75f56b00 d6db 11e8 86a5 a81732ee33d0.jpg hi, network like this. want stop backpropagation cnn inception. inception get gradients lstm. ? thank help."
keras,6651,"hi everyone, reasonably big dataset images, trying train inception model it. problem is, try use image generator apply random transformations , get memoryerror. weird, because, fit model without generator without transformations problem. possibly, generator memory leaks ? train model train batch method yield batch size generator solutions, way apply transformations images without generator ? images possibly ? thanks!",0,keras image generator memoryerror,"keras image generator memoryerror hi everyone, reasonably big dataset images, trying train inception model it. problem is, try use image generator apply random transformations , get memoryerror. weird, because, fit model without generator without transformations problem. possibly, generator memory leaks ? train model train batch method yield batch size generator solutions, way apply transformations images without generator ? images possibly ? thanks!"
keras,9198,"trying fine tune modified inceptionv3 model keras. follow example fine tune inceptionv3 new set classes page 1 . first trained top dense layers added inceptionv3 base model following code model model inputs base model.input, outputs predictions layer base model.layers layer.trainable false parallel model multi gpu model model, gpus 2 parallel model.compile optimizer 'rmsprop', loss 'categorical crossentropy' history parallel model.fit generator generate batches path , steps per epoch num images batch size, epochs num epochs that, try fine tune top 2 inception blocks inceptionv3. according example, layer model.layers 249 layer.trainable false layer model.layers 249 layer.trainable true model.compile optimizer sgd lr 0.0001, momentum 0.9 , loss 'categorical crossentropy' model.fit generator using , know freeze first 249 layers. mean, freeze layers gpu model like example , use freeze layers , weights top dense layers trained contained overwritten, right? hand, tried directly use , checked layers , showed i, layer enumerate parallel model.layers print i, layer.name 0, 'input 1' 1, 'lambda 1' 2, 'lambda 2' 3, 'model 1' 4, 'dense 3' 'lambda 1', 'lambda 2' 'model 1' layers shows 5 layers ? importantly, freeze layers ? 1 https keras.io applications",0,freeze layers multi gpu model keras,"freeze layers multi gpu model keras trying fine tune modified inceptionv3 model keras. follow example fine tune inceptionv3 new set classes page 1 . first trained top dense layers added inceptionv3 base model following code model model inputs base model.input, outputs predictions layer base model.layers layer.trainable false parallel model multi gpu model model, gpus 2 parallel model.compile optimizer 'rmsprop', loss 'categorical crossentropy' history parallel model.fit generator generate batches path , steps per epoch num images batch size, epochs num epochs that, try fine tune top 2 inception blocks inceptionv3. according example, layer model.layers 249 layer.trainable false layer model.layers 249 layer.trainable true model.compile optimizer sgd lr 0.0001, momentum 0.9 , loss 'categorical crossentropy' model.fit generator using , know freeze first 249 layers. mean, freeze layers gpu model like example , use freeze layers , weights top dense layers trained contained overwritten, right? hand, tried directly use , checked layers , showed i, layer enumerate parallel model.layers print i, layer.name 0, 'input 1' 1, 'lambda 1' 2, 'lambda 2' 3, 'model 1' 4, 'dense 3' 'lambda 1', 'lambda 2' 'model 1' layers shows 5 layers ? importantly, freeze layers ? 1 https keras.io applications"
keras,12782,"system information written custom code opposed using example directory yes os platform distribution e.g., linux ubuntu 16.04 windows 10 pro tensorflow backend yes yes tensorflow version 1.13.1 keras version 2.2.4 python version 3.7.3 cuda cudnn version na gpu model memory na describe current behavior creating lstm layer using functional moded error thrown describe expected behavior lstm layer created successfully code reproduce issue error log version code uses sequential model runs successfully success log",0,issue using lstm functional mode,"issue using lstm functional mode system information written custom code opposed using example directory yes os platform distribution e.g., linux ubuntu 16.04 windows 10 pro tensorflow backend yes yes tensorflow version 1.13.1 keras version 2.2.4 python version 3.7.3 cuda cudnn version na gpu model memory na describe current behavior creating lstm layer using functional moded error thrown describe expected behavior lstm layer created successfully code reproduce issue error log version code uses sequential model runs successfully success log"
keras,916,https github.com fchollet keras blob master examples mnist mlp.py l52 might want update example,0,test data used validation,test data used validation https github.com fchollet keras blob master examples mnist mlp.py l52 might want update example
keras,8897,"hi successfully installed theano, tensorflow keras always get error time try import keras. os windows 10 using python 3.5 3.63 reinstall anaconda python 3.5 troubleshooting kindly help thanks",0,'modulenotfounderror module named tensorflow',"'modulenotfounderror module named tensorflow' hi successfully installed theano, tensorflow keras always get error time try import keras. os windows 10 using python 3.5 3.63 reinstall anaconda python 3.5 troubleshooting kindly help thanks"
keras,9235,code works keras 2.1.2 keras 2.1.3,0,can't apply inception input tensor,can't apply inception input tensor code works keras 2.1.2 keras 2.1.3
keras,3120,"words, know keras install use theano tensorflow, still like control whether cpu gpu used. possible? could add flag purpose otherwise?",0,possible set cpu gpu usage without knowing backend?,"possible set cpu gpu usage without knowing backend? words, know keras install use theano tensorflow, still like control whether cpu gpu used. possible? could add flag purpose otherwise?"
keras,3457,load model saved hdf5 using custom loss function called get following exception even defined function current script still raises exeption. normal behaviour? install keras defining custom loss source code?,0,loading saved model custom loss,loading saved model custom loss load model saved hdf5 using custom loss function called get following exception even defined function current script still raises exeption. normal behaviour? install keras defining custom loss source code?
keras,2712,"hi, new keras neural network, wondering model.fit accumulative? basically, accidentally killed program yesterday, however save weights every epoch. wondering could load last weight model call model.fit ?",0,model.fit accumulative?,"model.fit accumulative? hi, new keras neural network, wondering model.fit accumulative? basically, accidentally killed program yesterday, however save weights every epoch. wondering could load last weight model call model.fit ?"
keras,3323,"see lot variables named like nb xx , nb classes , nb samples , etc. nb means here?",0,nb mean keras?,"nb mean keras? see lot variables named like nb xx , nb classes , nb samples , etc. nb means here?"
keras,11358,"please make sure boxes checked submit issue. issue implementation question, please ask question stackoverflow http stackoverflow.com questions tagged keras join keras slack channel https keras slack autojoin.herokuapp.com ask instead filing github issue. thank you! x check date master branch keras. update pip install git git github.com keras team keras.git upgrade deps running tensorflow, check date latest version. installation instructions found https www.tensorflow.org get started os setup . running theano, check date master branch theano. update pip install git git github.com theano theano.git upgrade deps provide link github gist python script reproduce issue copy script short .",0,error load model,"error load model please make sure boxes checked submit issue. issue implementation question, please ask question stackoverflow http stackoverflow.com questions tagged keras join keras slack channel https keras slack autojoin.herokuapp.com ask instead filing github issue. thank you! x check date master branch keras. update pip install git git github.com keras team keras.git upgrade deps running tensorflow, check date latest version. installation instructions found https www.tensorflow.org get started os setup . running theano, check date master branch theano. update pip install git git github.com theano theano.git upgrade deps provide link github gist python script reproduce issue copy script short ."
keras,8373,"ran code several times different pcs recent days, code able correctly generate samples. samples generated pure black, that, black image. ran code 50 epoches, keras 2 recent updated , theano tensorflow backend tried. noticed code, set discriminator.trainable false done line 160, seems never set true loop training. discriminator initialized never trained process? ! window https user images.githubusercontent.com 15433614 32375122 5d0e4396 c098 11e7 97f2 48bb5e69d1d8.png",0,meeting trouble running examples mnist acgan.py acgan model,"meeting trouble running examples mnist acgan.py acgan model ran code several times different pcs recent days, code able correctly generate samples. samples generated pure black, that, black image. ran code 50 epoches, keras 2 recent updated , theano tensorflow backend tried. noticed code, set discriminator.trainable false done line 160, seems never set true loop training. discriminator initialized never trained process? ! window https user images.githubusercontent.com 15433614 32375122 5d0e4396 c098 11e7 97f2 48bb5e69d1d8.png"
keras,5360,want implement lstm gru language model keras. seems one challenge. generate softmax vocabularies? i.e. cannot access weight embedding layer either implemented loss function well function lambda layer.,0,implement lstm gru language model keras?,implement lstm gru language model keras? want implement lstm gru language model keras. seems one challenge. generate softmax vocabularies? i.e. cannot access weight embedding layer either implemented loss function well function lambda layer.
keras,2145,"optimizer's class get config method returns dictionary single key name . however, optimizer also receive clipnorm clipvalue keyword arguments. would nice also add dictionary returned get config set. x check date master branch keras. update pip install git git github.com fchollet keras.git upgrade deps x running theano, check date master branch theano. update pip install git git github.com theano theano.git upgrade deps x provide link github gist python script reproduce issue copy script short . happy implement feature.",0,include clipnorm clipvalue optimizer.get config output,"include clipnorm clipvalue optimizer.get config output optimizer's class get config method returns dictionary single key name . however, optimizer also receive clipnorm clipvalue keyword arguments. would nice also add dictionary returned get config set. x check date master branch keras. update pip install git git github.com fchollet keras.git upgrade deps x running theano, check date master branch theano. update pip install git git github.com theano theano.git upgrade deps x provide link github gist python script reproduce issue copy script short . happy implement feature."
keras,6156,"please make sure boxes checked submit issue. issue implementation question, please ask question stackoverflow http stackoverflow.com questions tagged keras join keras slack channel https keras slack autojoin.herokuapp.com ask instead filing github issue. thank you! check date master branch keras. update pip install git git github.com fchollet keras.git upgrade deps running tensorflow, check date latest version. installation instructions found https www.tensorflow.org get started os setup . running theano, check date master branch theano. update pip install git git github.com theano theano.git upgrade deps provide link github gist python script reproduce issue copy script short .",0,"want use seq2seq model q a,but know output lstm decoder ,is array probability??","want use seq2seq model q a,but know output lstm decoder ,is array probability?? please make sure boxes checked submit issue. issue implementation question, please ask question stackoverflow http stackoverflow.com questions tagged keras join keras slack channel https keras slack autojoin.herokuapp.com ask instead filing github issue. thank you! check date master branch keras. update pip install git git github.com fchollet keras.git upgrade deps running tensorflow, check date latest version. installation instructions found https www.tensorflow.org get started os setup . running theano, check date master branch theano. update pip install git git github.com theano theano.git upgrade deps provide link github gist python script reproduce issue copy script short ."
keras,7089,"fchollet enable autocancelation travis? tested cancel redundant builds example, push twice pr . think enabled could wrong. https blog.travis ci.com 2017 03 22 introducing auto cancellation travis seems lot work recently, especially new backend. could help cut back little bit. cheers",0,autocancelation,"autocancelation fchollet enable autocancelation travis? tested cancel redundant builds example, push twice pr . think enabled could wrong. https blog.travis ci.com 2017 03 22 introducing auto cancellation travis seems lot work recently, especially new backend. could help cut back little bit. cheers"
keras,100,"key features keras time hits v1.0. visualization tools, possibly built top bokeh. able see everything that's going experiment, maybe even manage experiments gui. total visualization key good research. easy support spearmint hyperparameter search. complete unit tests. recent regularizers constraints debacle left kaggle users confused highlights reliable unit tests absolute priority. want able develop quality code safely confidence. better convnet features, including fft convolutions, maybe new padding options. medium term look incorporating support nervana systems' fast convolution kernels. let's stay state art support non sequential models, way done merge container takes list sequential models turns single output fork container replicates output sequential model list sequential models would trainable end end, course . see anything would like work on, please post thoughts would incorporate current architecture data structures. opportunity make big contribution deep learning ecosystem!",0,roadmap,"roadmap key features keras time hits v1.0. visualization tools, possibly built top bokeh. able see everything that's going experiment, maybe even manage experiments gui. total visualization key good research. easy support spearmint hyperparameter search. complete unit tests. recent regularizers constraints debacle left kaggle users confused highlights reliable unit tests absolute priority. want able develop quality code safely confidence. better convnet features, including fft convolutions, maybe new padding options. medium term look incorporating support nervana systems' fast convolution kernels. let's stay state art support non sequential models, way done merge container takes list sequential models turns single output fork container replicates output sequential model list sequential models would trainable end end, course . see anything would like work on, please post thoughts would incorporate current architecture data structures. opportunity make big contribution deep learning ecosystem!"
keras,8227,"paramater generatorenqueuer called 'random seed', everywhere else throughout keras, called 'seed'. rename it, provide backward compatibility not? https github.com fchollet keras blob master keras utils data utils.py l556",0,api inconsistency generatorenqueuer random seed,"api inconsistency generatorenqueuer random seed paramater generatorenqueuer called 'random seed', everywhere else throughout keras, called 'seed'. rename it, provide backward compatibility not? https github.com fchollet keras blob master keras utils data utils.py l556"
keras,2326,"hi, using following piece report training progress working before, upgrading latest , looks supplied anymore. gives keyerror. suspect related deprecated. looks documentation http keras.io callbacks updated keys well. thank much.",0,epoch end callback receive acc key logs argument anymore?,"epoch end callback receive acc key logs argument anymore? hi, using following piece report training progress working before, upgrading latest , looks supplied anymore. gives keyerror. suspect related deprecated. looks documentation http keras.io callbacks updated keys well. thank much."
keras,5291,"quite new keras. trying accomplish train word level text generation module. overcome problem varying length sentences training data, set batch size 1 trained model statefull true. predict next word given word. tried concatenate generated word input predict function case get output shape array predictions word input wished single prediction next word, considering previous words want design something like give start token generate say concatenate start token get start token ie add new row input vector next word generated consider start token word generate next word on.... current code batch function generates x arrays representing shape len corpus, 1, vocabulary one word ahead x like x you.... you..... thanks advance",0,text generation kearas lstm varying time step,"text generation kearas lstm varying time step quite new keras. trying accomplish train word level text generation module. overcome problem varying length sentences training data, set batch size 1 trained model statefull true. predict next word given word. tried concatenate generated word input predict function case get output shape array predictions word input wished single prediction next word, considering previous words want design something like give start token generate say concatenate start token get start token ie add new row input vector next word generated consider start token word generate next word on.... current code batch function generates x arrays representing shape len corpus, 1, vocabulary one word ahead x like x you.... you..... thanks advance"
keras,7548,returns parameter name accept. reproduce raises,0,kerasregressor kerasclassifier get params set params incompatible,kerasregressor kerasclassifier get params set params incompatible returns parameter name accept. reproduce raises
keras,6580,"hi, sure documentation categorical crossentropy seems compatible tensorflow one. mean look documentation loss functions i. e. https keras.io losses available loss functions end note categorical crossentropy. sentence note using categorical crossentropy loss, targets categorical format e.g. 10 classes, target sample 10 dimensional vector zeros expect 1 index corresponding class sample . understand possible target e.g. 0, 0, 1, 0, 0 ? tensorflow documentation softmax cross entropy logits https www.tensorflow.org api docs python tf nn softmax cross entropy logits classes mutually exclusive, probabilities need be. required row labels valid probability distribution. understand means target possible 0.0, 0.75, 0.25, 0.0, 0.0 ? categorical crossentropy defined https github.com tensorflow tensorflow blob r1.1 tensorflow contrib keras python keras backend.py indeed returns softmax cross entropy logits. missing something yes really sorry ? best wishes dawid",0,note categorical crossentropy?,"note categorical crossentropy? hi, sure documentation categorical crossentropy seems compatible tensorflow one. mean look documentation loss functions i. e. https keras.io losses available loss functions end note categorical crossentropy. sentence note using categorical crossentropy loss, targets categorical format e.g. 10 classes, target sample 10 dimensional vector zeros expect 1 index corresponding class sample . understand possible target e.g. 0, 0, 1, 0, 0 ? tensorflow documentation softmax cross entropy logits https www.tensorflow.org api docs python tf nn softmax cross entropy logits classes mutually exclusive, probabilities need be. required row labels valid probability distribution. understand means target possible 0.0, 0.75, 0.25, 0.0, 0.0 ? categorical crossentropy defined https github.com tensorflow tensorflow blob r1.1 tensorflow contrib keras python keras backend.py indeed returns softmax cross entropy logits. missing something yes really sorry ? best wishes dawid"
keras,3806,"hello , want ask opinion happening. explain yesterday, models simple working today. code follows code words 100 200 embedding size setup. produces following error. error produced use cnn lstm word2vec vectors. weird part models working yesterday value test them. thanks, nick",0,"lstm input shape error,","lstm input shape error, hello , want ask opinion happening. explain yesterday, models simple working today. code follows code words 100 200 embedding size setup. produces following error. error produced use cnn lstm word2vec vectors. weird part models working yesterday value test them. thanks, nick"
keras,3647,"error message full code attached using theano backend. using gpu device 0 geforce gtx 1060 3gb cnmem enabled initial size 81.0 memory, cudnn 5005 6l, 3l, 150l, 150l data predict traceback recent call last file c program files x86 jetbrains pycharm edu 2.0.4 helpers pydev pydevd.py , line 2411, globals debugger.run setup 'file' , none, none, module file c program files x86 jetbrains pycharm edu 2.0.4 helpers pydev pydevd.py , line 1802, run launch file, globals, locals execute script file c users byoru pycharmprojects maillession firstnn canvas.py , line 50, output model.predict testarrnp, batch size 2,verbose 1 file c program files x86 microsoft visual studio 12.0 vc theano keras keras models.py , line 664, predict return self.model.predict x, batch size batch size, verbose verbose file c program files x86 microsoft visual studio 12.0 vc theano keras keras engine training.py , line 1180, predict batch size batch size, verbose verbose file c program files x86 microsoft visual studio 12.0 vc theano keras keras engine training.py , line 879, predict loop batch outs f ins batch file c program files x86 microsoft visual studio 12.0 vc theano keras keras backend theano backend.py , line 655, call return self.function inputs file c program files x86 microsoft visual studio 12.0 vc theano theano compile function module.py , line 879, call storage map getattr self.fn, 'storage map', none file c program files x86 microsoft visual studio 12.0 vc theano theano gof link.py , line 325, raise op reraise exc type, exc value, exc trace file c program files x86 microsoft visual studio 12.0 vc theano theano compile function module.py , line 866, call self.fn output subset none else file c program files x86 microsoft visual studio 12.0 vc theano theano gof op.py , line 908, rval r p n, x 0 x , file c program files x86 microsoft visual studio 12.0 vc theano theano tensor nnet abstract conv.py , line 848, perform conv self.conv2d img, kern, mode valid , dilation self.filter dilation file c program files x86 microsoft visual studio 12.0 vc theano theano tensor nnet abstract conv.py , line 775, conv2d dilated kern n, im0, , indexerror index 1 bounds axis 1 size 1 apply node caused error abstractconv2d border mode 'valid', subsample 1, 1 , filter flip true, imshp none, none, none, none , kshp 32, 3, 3, 3 , filter dilation 1, 1 convolution2d input 1, hostfromgpu.0 toposort index 33 inputs types tensortype float32, 4d , tensortype float32, 4d inputs shapes 2l, 3l, 150l, 150l , 32l, 1l, 3l, 3l inputs strides 270000l, 90000l, 600l, 4l , 36l, 36l, 12l, 4l inputs values 'not shown', 'not shown' inputs type num 11, 11 outputs clients elemwise add,no inplace abstractconv2d border mode 'valid', subsample 1, 1 , filter flip true, imshp none, none, none, none , kshp 32, 3, 3, 3 , filter dilation 1, 1 .0, reshape 4 .0 backtrace node created use theano flag traceback.limit n make longer file c users byoru pycharmprojects maillession firstnn canvas.py , line 11, model.add convolution2d 32, 3, 3, input shape 3, 150, 150 file c program files x86 microsoft visual studio 12.0 vc theano keras keras models.py , line 275, add layer.create input layer batch input shape, input dtype file c program files x86 microsoft visual studio 12.0 vc theano keras keras engine topology.py , line 367, create input layer self x file c program files x86 microsoft visual studio 12.0 vc theano keras keras engine topology.py , line 511, call self.add inbound node inbound layers, node indices, tensor indices file c program files x86 microsoft visual studio 12.0 vc theano keras keras engine topology.py , line 569, add inbound node node.create node self, inbound layers, node indices, tensor indices file c program files x86 microsoft visual studio 12.0 vc theano keras keras engine topology.py , line 150, create node output tensors list outbound layer.call input tensors 0 , mask input masks 0 file c program files x86 microsoft visual studio 12.0 vc theano keras keras layers convolutional.py , line 353, call filter shape self.w shape file c program files x86 microsoft visual studio 12.0 vc theano keras keras backend theano backend.py , line 1073, conv2d filter shape filter shape debugprint apply node abstractconv2d border mode 'valid', subsample 1, 1 , filter flip true, imshp none, none, none, none , kshp 32, 3, 3, 3 , filter dilation 1, 1 id '' convolution2d input 1 id b hostfromgpu id c '' convolution2d 1 w id storage map footprint dense 1 w, shared input, shape 18496, 64 , elemsize 4 byte , totalsize 4734976 byte convolution2d input 1, input, shape 2l, 3l, 150l, 150l , elemsize 4 byte , totalsize 540000 byte , shared input, shape 92160, , elemsize 4 byte , totalsize 368640 byte , shared input, shape 92160, , elemsize 4 byte , totalsize 368640 byte , shared input, shape 92160, , elemsize 4 byte , totalsize 368640 byte convolution2d 3 w, shared input, shape 64, 32, 3, 3 , elemsize 4 byte , totalsize 73728 byte convolution2d 2 w, shared input, shape 32, 32, 3, 3 , elemsize 4 byte , totalsize 36864 byte hostfromgpu.0, shape 32l, 1l, 3l, 3l , elemsize 4 byte , totalsize 1152 byte convolution2d 1 w, shared input, shape 32, 1, 3, 3 , elemsize 4 byte , totalsize 1152 byte convolution2d 3 b, shared input, shape 64, , elemsize 4 byte , totalsize 256 byte dense 1 b, shared input, shape 64, , elemsize 4 byte , totalsize 256 byte dense 2 w, shared input, shape 64, 1 , elemsize 4 byte , totalsize 256 byte convolution2d 1 b, shared input, shape 32, , elemsize 4 byte , totalsize 128 byte convolution2d 2 b, shared input, shape 32, , elemsize 4 byte , totalsize 128 byte tensorconstant 1 32 1 1 , shape 4l, , elemsize 4 byte , totalsize 16 byte tensorconstant 1 64 1 1 , shape 4l, , elemsize 4 byte , totalsize 16 byte tensorconstant 1 32 1 1 , shape 4l, , elemsize 4 byte , totalsize 16 byte constant 0 , shape , elemsize 8 byte , totalsize 8.0 byte constant 0 , shape , elemsize 8 byte , totalsize 8.0 byte tensorconstant 0.5 , shape , elemsize 4 byte , totalsize 4.0 byte dense 2 b, shared input, shape 1, , elemsize 4 byte , totalsize 4 byte tensorconstant 0.5 , shape , elemsize 4 byte , totalsize 4.0 byte dimshuffle x,x,x,x .0, shape 1l, 1l, 1l, 1l , elemsize 4 byte , totalsize 4 byte tensorconstant 0.800000011921 , shape , elemsize 4 byte , totalsize 4.0 byte tensorconstant 0.800000011921 , shape , elemsize 4 byte , totalsize 4.0 byte tensorconstant 0.5 , shape , elemsize 4 byte , totalsize 4.0 byte dimshuffle x,x .0, shape 1l, 1l , elemsize 4 byte , totalsize 4 byte tensorconstant 0.5 , shape , elemsize 4 byte , totalsize 4.0 byte tensorconstant 0.5 , shape , elemsize 4 byte , totalsize 4.0 byte tensorconstant 1.0 , shape , elemsize 4 byte , totalsize 4.0 byte tensorconstant 0.5 , shape , elemsize 4 byte , totalsize 4.0 byte dimshuffle x,x,x,x .0, shape 1l, 1l, 1l, 1l , elemsize 4 byte , totalsize 4 byte dimshuffle x,x,x,x .0, shape 1l, 1l, 1l, 1l , elemsize 4 byte , totalsize 4 byte tensorconstant 0.800000011921 , shape , elemsize 4 byte , totalsize 4.0 byte tensorconstant 0.800000011921 , shape , elemsize 4 byte , totalsize 4.0 byte tensorconstant 0.0 , shape , elemsize 4 byte , totalsize 4.0 byte dimshuffle x,x,x,x .0, shape 1l, 1l, 1l, 1l , elemsize 1 byte , totalsize 1 byte dimshuffle x,x,x,x .0, shape 1l, 1l, 1l, 1l , elemsize 1 byte , totalsize 1 byte dimshuffle x,x .0, shape 1l, 1l , elemsize 1 byte , totalsize 1 byte keras learning phase, input, shape , elemsize 1 byte , totalsize 1.0 byte totalsize 6494952.0 byte 0.006 gb totalsize inputs 6493781.0 byte 0.006 gb full code import numpy np import os pil import image keras.models import sequential keras.layers import convolution2d, maxpooling2d keras.layers import activation, dropout, flatten, dense keras.preprocessing.image import imagedatagenerator model sequential model.add convolution2d 32, 3, 3, input shape 3, 150, 150 model.add activation 'relu' model.add maxpooling2d pool size 2, 2 model.add convolution2d 32, 3, 3 model.add activation 'relu' model.add maxpooling2d pool size 2, 2 model.add dropout 0.2 model.add convolution2d 64, 3, 3 model.add activation 'relu' model.add maxpooling2d pool size 2, 2 model.add dropout 0.2 model.add flatten converts 3d feature maps 1d feature vectors model.add dense 64 model.add activation 'relu' model.add dropout 0.5 model.add dense 1 model.add activation 'sigmoid' model.load weights 'weight1.h5' model.compile loss 'binary crossentropy', optimizer 'rmsprop', metrics 'accuracy' path 'test' testarr item os.listdir path imgpath path ' ' item img image.open imgpath resizedimg img.resize 150,150 ,image.antialias data np.array resizedimg data2 data.transpose 2,0,1 testarr.append data2 testarrnp np.asarray testarr print testarrnp.shape output model.predict testarrnp, batch size 2,verbose 1 print output",0,theanio flag error predict cnn,"theanio flag error predict cnn error message full code attached using theano backend. using gpu device 0 geforce gtx 1060 3gb cnmem enabled initial size 81.0 memory, cudnn 5005 6l, 3l, 150l, 150l data predict traceback recent call last file c program files x86 jetbrains pycharm edu 2.0.4 helpers pydev pydevd.py , line 2411, globals debugger.run setup 'file' , none, none, module file c program files x86 jetbrains pycharm edu 2.0.4 helpers pydev pydevd.py , line 1802, run launch file, globals, locals execute script file c users byoru pycharmprojects maillession firstnn canvas.py , line 50, output model.predict testarrnp, batch size 2,verbose 1 file c program files x86 microsoft visual studio 12.0 vc theano keras keras models.py , line 664, predict return self.model.predict x, batch size batch size, verbose verbose file c program files x86 microsoft visual studio 12.0 vc theano keras keras engine training.py , line 1180, predict batch size batch size, verbose verbose file c program files x86 microsoft visual studio 12.0 vc theano keras keras engine training.py , line 879, predict loop batch outs f ins batch file c program files x86 microsoft visual studio 12.0 vc theano keras keras backend theano backend.py , line 655, call return self.function inputs file c program files x86 microsoft visual studio 12.0 vc theano theano compile function module.py , line 879, call storage map getattr self.fn, 'storage map', none file c program files x86 microsoft visual studio 12.0 vc theano theano gof link.py , line 325, raise op reraise exc type, exc value, exc trace file c program files x86 microsoft visual studio 12.0 vc theano theano compile function module.py , line 866, call self.fn output subset none else file c program files x86 microsoft visual studio 12.0 vc theano theano gof op.py , line 908, rval r p n, x 0 x , file c program files x86 microsoft visual studio 12.0 vc theano theano tensor nnet abstract conv.py , line 848, perform conv self.conv2d img, kern, mode valid , dilation self.filter dilation file c program files x86 microsoft visual studio 12.0 vc theano theano tensor nnet abstract conv.py , line 775, conv2d dilated kern n, im0, , indexerror index 1 bounds axis 1 size 1 apply node caused error abstractconv2d border mode 'valid', subsample 1, 1 , filter flip true, imshp none, none, none, none , kshp 32, 3, 3, 3 , filter dilation 1, 1 convolution2d input 1, hostfromgpu.0 toposort index 33 inputs types tensortype float32, 4d , tensortype float32, 4d inputs shapes 2l, 3l, 150l, 150l , 32l, 1l, 3l, 3l inputs strides 270000l, 90000l, 600l, 4l , 36l, 36l, 12l, 4l inputs values 'not shown', 'not shown' inputs type num 11, 11 outputs clients elemwise add,no inplace abstractconv2d border mode 'valid', subsample 1, 1 , filter flip true, imshp none, none, none, none , kshp 32, 3, 3, 3 , filter dilation 1, 1 .0, reshape 4 .0 backtrace node created use theano flag traceback.limit n make longer file c users byoru pycharmprojects maillession firstnn canvas.py , line 11, model.add convolution2d 32, 3, 3, input shape 3, 150, 150 file c program files x86 microsoft visual studio 12.0 vc theano keras keras models.py , line 275, add layer.create input layer batch input shape, input dtype file c program files x86 microsoft visual studio 12.0 vc theano keras keras engine topology.py , line 367, create input layer self x file c program files x86 microsoft visual studio 12.0 vc theano keras keras engine topology.py , line 511, call self.add inbound node inbound layers, node indices, tensor indices file c program files x86 microsoft visual studio 12.0 vc theano keras keras engine topology.py , line 569, add inbound node node.create node self, inbound layers, node indices, tensor indices file c program files x86 microsoft visual studio 12.0 vc theano keras keras engine topology.py , line 150, create node output tensors list outbound layer.call input tensors 0 , mask input masks 0 file c program files x86 microsoft visual studio 12.0 vc theano keras keras layers convolutional.py , line 353, call filter shape self.w shape file c program files x86 microsoft visual studio 12.0 vc theano keras keras backend theano backend.py , line 1073, conv2d filter shape filter shape debugprint apply node abstractconv2d border mode 'valid', subsample 1, 1 , filter flip true, imshp none, none, none, none , kshp 32, 3, 3, 3 , filter dilation 1, 1 id '' convolution2d input 1 id b hostfromgpu id c '' convolution2d 1 w id storage map footprint dense 1 w, shared input, shape 18496, 64 , elemsize 4 byte , totalsize 4734976 byte convolution2d input 1, input, shape 2l, 3l, 150l, 150l , elemsize 4 byte , totalsize 540000 byte , shared input, shape 92160, , elemsize 4 byte , totalsize 368640 byte , shared input, shape 92160, , elemsize 4 byte , totalsize 368640 byte , shared input, shape 92160, , elemsize 4 byte , totalsize 368640 byte convolution2d 3 w, shared input, shape 64, 32, 3, 3 , elemsize 4 byte , totalsize 73728 byte convolution2d 2 w, shared input, shape 32, 32, 3, 3 , elemsize 4 byte , totalsize 36864 byte hostfromgpu.0, shape 32l, 1l, 3l, 3l , elemsize 4 byte , totalsize 1152 byte convolution2d 1 w, shared input, shape 32, 1, 3, 3 , elemsize 4 byte , totalsize 1152 byte convolution2d 3 b, shared input, shape 64, , elemsize 4 byte , totalsize 256 byte dense 1 b, shared input, shape 64, , elemsize 4 byte , totalsize 256 byte dense 2 w, shared input, shape 64, 1 , elemsize 4 byte , totalsize 256 byte convolution2d 1 b, shared input, shape 32, , elemsize 4 byte , totalsize 128 byte convolution2d 2 b, shared input, shape 32, , elemsize 4 byte , totalsize 128 byte tensorconstant 1 32 1 1 , shape 4l, , elemsize 4 byte , totalsize 16 byte tensorconstant 1 64 1 1 , shape 4l, , elemsize 4 byte , totalsize 16 byte tensorconstant 1 32 1 1 , shape 4l, , elemsize 4 byte , totalsize 16 byte constant 0 , shape , elemsize 8 byte , totalsize 8.0 byte constant 0 , shape , elemsize 8 byte , totalsize 8.0 byte tensorconstant 0.5 , shape , elemsize 4 byte , totalsize 4.0 byte dense 2 b, shared input, shape 1, , elemsize 4 byte , totalsize 4 byte tensorconstant 0.5 , shape , elemsize 4 byte , totalsize 4.0 byte dimshuffle x,x,x,x .0, shape 1l, 1l, 1l, 1l , elemsize 4 byte , totalsize 4 byte tensorconstant 0.800000011921 , shape , elemsize 4 byte , totalsize 4.0 byte tensorconstant 0.800000011921 , shape , elemsize 4 byte , totalsize 4.0 byte tensorconstant 0.5 , shape , elemsize 4 byte , totalsize 4.0 byte dimshuffle x,x .0, shape 1l, 1l , elemsize 4 byte , totalsize 4 byte tensorconstant 0.5 , shape , elemsize 4 byte , totalsize 4.0 byte tensorconstant 0.5 , shape , elemsize 4 byte , totalsize 4.0 byte tensorconstant 1.0 , shape , elemsize 4 byte , totalsize 4.0 byte tensorconstant 0.5 , shape , elemsize 4 byte , totalsize 4.0 byte dimshuffle x,x,x,x .0, shape 1l, 1l, 1l, 1l , elemsize 4 byte , totalsize 4 byte dimshuffle x,x,x,x .0, shape 1l, 1l, 1l, 1l , elemsize 4 byte , totalsize 4 byte tensorconstant 0.800000011921 , shape , elemsize 4 byte , totalsize 4.0 byte tensorconstant 0.800000011921 , shape , elemsize 4 byte , totalsize 4.0 byte tensorconstant 0.0 , shape , elemsize 4 byte , totalsize 4.0 byte dimshuffle x,x,x,x .0, shape 1l, 1l, 1l, 1l , elemsize 1 byte , totalsize 1 byte dimshuffle x,x,x,x .0, shape 1l, 1l, 1l, 1l , elemsize 1 byte , totalsize 1 byte dimshuffle x,x .0, shape 1l, 1l , elemsize 1 byte , totalsize 1 byte keras learning phase, input, shape , elemsize 1 byte , totalsize 1.0 byte totalsize 6494952.0 byte 0.006 gb totalsize inputs 6493781.0 byte 0.006 gb full code import numpy np import os pil import image keras.models import sequential keras.layers import convolution2d, maxpooling2d keras.layers import activation, dropout, flatten, dense keras.preprocessing.image import imagedatagenerator model sequential model.add convolution2d 32, 3, 3, input shape 3, 150, 150 model.add activation 'relu' model.add maxpooling2d pool size 2, 2 model.add convolution2d 32, 3, 3 model.add activation 'relu' model.add maxpooling2d pool size 2, 2 model.add dropout 0.2 model.add convolution2d 64, 3, 3 model.add activation 'relu' model.add maxpooling2d pool size 2, 2 model.add dropout 0.2 model.add flatten converts 3d feature maps 1d feature vectors model.add dense 64 model.add activation 'relu' model.add dropout 0.5 model.add dense 1 model.add activation 'sigmoid' model.load weights 'weight1.h5' model.compile loss 'binary crossentropy', optimizer 'rmsprop', metrics 'accuracy' path 'test' testarr item os.listdir path imgpath path ' ' item img image.open imgpath resizedimg img.resize 150,150 ,image.antialias data np.array resizedimg data2 data.transpose 2,0,1 testarr.append data2 testarrnp np.asarray testarr print testarrnp.shape output model.predict testarrnp, batch size 2,verbose 1 print output"
keras,6129,"keras throws error first layer layer pretty unusual use case, work . seems definition layer simply def linear x return x topology properly created. here's simple script reproduce error keras.models import sequential keras.layers import dense, activation import numpy np def build net bug activation model sequential model.add activation activation,input shape 12, model.add dense 2 model.compile loss 'categorical crossentropy', optimizer 'sgd' return model model build net bug 'linear' model.train batch np.zeros 32,12 ,np.zeros 32,2 print 'ok' producted following error path experimental keras keras engine topology.py 1516 userwarning model inputs must come keras input layer, cannot output previous non input layer. here, tensor specified input sequential 1 model input tensor, generated layer activation 1. note input tensors instantiated via . tensor caused issue activation 1 input 0 str x.name traceback recent call last file test keras bug 2.py , line 12, model build net bug 'linear' file test keras bug 2.py , line 9, build net bug model.compile loss 'categorical crossentropy', optimizer 'sgd' file path experimental keras keras models.py , line 761, compile self.build file path experimental keras keras models.py , line 520, build name self.name ' model' file path experimental keras keras legacy interfaces.py , line 88, wrapper return func args, kwargs file path experimental keras keras engine topology.py , line 1569, init layer.is placeholder attributeerror 'activation' object attribute 'is placeholder' replacting activation fixes issue. suggestions best way fix preferably without impacting performance ?",0,bug error using activation 'linear' first layer,"bug error using activation 'linear' first layer keras throws error first layer layer pretty unusual use case, work . seems definition layer simply def linear x return x topology properly created. here's simple script reproduce error keras.models import sequential keras.layers import dense, activation import numpy np def build net bug activation model sequential model.add activation activation,input shape 12, model.add dense 2 model.compile loss 'categorical crossentropy', optimizer 'sgd' return model model build net bug 'linear' model.train batch np.zeros 32,12 ,np.zeros 32,2 print 'ok' producted following error path experimental keras keras engine topology.py 1516 userwarning model inputs must come keras input layer, cannot output previous non input layer. here, tensor specified input sequential 1 model input tensor, generated layer activation 1. note input tensors instantiated via . tensor caused issue activation 1 input 0 str x.name traceback recent call last file test keras bug 2.py , line 12, model build net bug 'linear' file test keras bug 2.py , line 9, build net bug model.compile loss 'categorical crossentropy', optimizer 'sgd' file path experimental keras keras models.py , line 761, compile self.build file path experimental keras keras models.py , line 520, build name self.name ' model' file path experimental keras keras legacy interfaces.py , line 88, wrapper return func args, kwargs file path experimental keras keras engine topology.py , line 1569, init layer.is placeholder attributeerror 'activation' object attribute 'is placeholder' replacting activation fixes issue. suggestions best way fix preferably without impacting performance ?"
keras,2554,useful feature. removed? could readded?,0,multiple workers gone fit generator,multiple workers gone fit generator useful feature. removed? could readded?
keras,2028,"hi,everybody. convolution network, want use autoencode convolution network out, order learn condensed features. convolution part below, next? covolution part convolutionpart sequential convolutionpart.add convolution1d 100,3,input shape trian 1 .shape,300 convolutionpart.add maxpooling1d pool length 4 autoencoder",0,build autoencode layers?,"build autoencode layers? hi,everybody. convolution network, want use autoencode convolution network out, order learn condensed features. convolution part below, next? covolution part convolutionpart sequential convolutionpart.add convolution1d 100,3,input shape trian 1 .shape,300 convolutionpart.add maxpooling1d pool length 4 autoencoder"
keras,1614,"dear all, would like use imagedatagenerator regression problem detect facial key points https www.kaggle.com c facial keypoints detection . looking code got feeling value modified images transformed. true ? planning support kind regression mode parallel classification one",0,imagedatagenerator regression problem,"imagedatagenerator regression problem dear all, would like use imagedatagenerator regression problem detect facial key points https www.kaggle.com c facial keypoints detection . looking code got feeling value modified images transformed. true ? planning support kind regression mode parallel classification one"
keras,6213,"keras 2.0.2 tensorflow 1.0.0 python3 mac osx error using tensorflow backend network convolutioninput input shape 1, maxrow, cols , name 'convolutional input' x conv2d 32, 3, cols , input shape 1, maxrow, cols , data format 'channels first' convolutioninput x maxpooling2d pool size 2, 1 x x dropout .5 x convolutionoutput flatten x additionalinput input shape 1, , name 'additional input' x concatenate convolutionoutput, additionalinput , axis 1 x dense 64, activation 'relu' x x dense 64, activation 'relu' x finaloutput dense 2, activation 'softmax' x convonet model inputs convolutioninput, additionalinput , outputs finaloutput convonet.compile optimizer 'rmsprop', loss 'binary crossentropy', metrics 'accuracy' convonet.fit x 'convolutional input' trainingset 0 , 'additional input' trainingset 1 , trainlabels, epochs 20, batch size 10 get error x concatenate convolutionoutput, additionalinput , axis 1 typeerror init got multiple values argument 'axis' also tried including axis keyword argument got error traceback recent call last file users bl755p documents wrt nlp.py , line 683, x dense 64, activation 'relu' x file users bl755p anaconda envs att nlp keras2 lib python3.5 sitepackages keras engine topology.py , line 511, call self.assert input compatibility inputs file users bl755p anaconda envs att nlp keras2 lib python3.5 site packages keras engine topology.py , line 423, assert input compatibility ndim k.ndim x file users bl755p anaconda envs att nlp keras2 lib python3.5 site packages keras backend tensorflow backend.py , line 437, ndim dims x.get shape . dims attributeerror 'concatenate' object attribute 'get shape'",0,concatenate layer errors,"concatenate layer errors keras 2.0.2 tensorflow 1.0.0 python3 mac osx error using tensorflow backend network convolutioninput input shape 1, maxrow, cols , name 'convolutional input' x conv2d 32, 3, cols , input shape 1, maxrow, cols , data format 'channels first' convolutioninput x maxpooling2d pool size 2, 1 x x dropout .5 x convolutionoutput flatten x additionalinput input shape 1, , name 'additional input' x concatenate convolutionoutput, additionalinput , axis 1 x dense 64, activation 'relu' x x dense 64, activation 'relu' x finaloutput dense 2, activation 'softmax' x convonet model inputs convolutioninput, additionalinput , outputs finaloutput convonet.compile optimizer 'rmsprop', loss 'binary crossentropy', metrics 'accuracy' convonet.fit x 'convolutional input' trainingset 0 , 'additional input' trainingset 1 , trainlabels, epochs 20, batch size 10 get error x concatenate convolutionoutput, additionalinput , axis 1 typeerror init got multiple values argument 'axis' also tried including axis keyword argument got error traceback recent call last file users bl755p documents wrt nlp.py , line 683, x dense 64, activation 'relu' x file users bl755p anaconda envs att nlp keras2 lib python3.5 sitepackages keras engine topology.py , line 511, call self.assert input compatibility inputs file users bl755p anaconda envs att nlp keras2 lib python3.5 site packages keras engine topology.py , line 423, assert input compatibility ndim k.ndim x file users bl755p anaconda envs att nlp keras2 lib python3.5 site packages keras backend tensorflow backend.py , line 437, ndim dims x.get shape . dims attributeerror 'concatenate' object attribute 'get shape'"
keras,7197,"new paper lee et al. http www.kentonl.com pub llz.2017.pdf details new, simplified recurrent neural network dubbed recurrent additive network. seems well language modeling tasks compared lstm, despite using simpler computation less parameters. model would easy implement keras subclass recurrent, happy it. however, question something would pulled main repo? standard policy adding new sorts rnns?",0,feature wanted? recurrent additive networks. happy implement pr but...,"feature wanted? recurrent additive networks. happy implement pr but... new paper lee et al. http www.kentonl.com pub llz.2017.pdf details new, simplified recurrent neural network dubbed recurrent additive network. seems well language modeling tasks compared lstm, despite using simpler computation less parameters. model would easy implement keras subclass recurrent, happy it. however, question something would pulled main repo? standard policy adding new sorts rnns?"
keras,3311,"modified version file. downsampling input upsampling output compiles converges expected estimated output shape wrong says instead . may error deconv layer output shape estimation look yaringal , already solution let know. copy paste code reproduce results lambda sampling z mean, z log var",0,error deconv output shape estimation,"error deconv output shape estimation modified version file. downsampling input upsampling output compiles converges expected estimated output shape wrong says instead . may error deconv layer output shape estimation look yaringal , already solution let know. copy paste code reproduce results lambda sampling z mean, z log var"
keras,2931,"running deep net model keras code. training model running perfectly fne image size 76 76, working image dimension 128 128. got error file usr local lib python2.7 dist packages theano tensor opt.py , line 4262, get num denum pairs self.get num denum input2 input2 parent.inputs runtimeerror maximum recursion depth exceeded isinstance c, graph.constant len c.clients 1 runtimeerror 'maximum recursion depth exceeded calling python object', 'please, report theano dev mailing list. temporary work around, raise python stack limit import sys sys.setrecursionlimit 10000 ' using gpu machine nvidia gf110gl tesla m2075 fairly good configuration. want proceed temporary work around case, code getting stuck training. kindly suggest better way handle issue.",0,maximum recursion depth error model.fit,"maximum recursion depth error model.fit running deep net model keras code. training model running perfectly fne image size 76 76, working image dimension 128 128. got error file usr local lib python2.7 dist packages theano tensor opt.py , line 4262, get num denum pairs self.get num denum input2 input2 parent.inputs runtimeerror maximum recursion depth exceeded isinstance c, graph.constant len c.clients 1 runtimeerror 'maximum recursion depth exceeded calling python object', 'please, report theano dev mailing list. temporary work around, raise python stack limit import sys sys.setrecursionlimit 10000 ' using gpu machine nvidia gf110gl tesla m2075 fairly good configuration. want proceed temporary work around case, code getting stuck training. kindly suggest better way handle issue."
keras,3809,quite understand fit generator works. particular mean parameter samples per epoch? described https github.com fchollet keras issues 1627 samples per epoch batch size numbers batches fit generator read exactly samples per epoch samples generator. also said control batch size generator yieldings. begins really missing. let's assume generator return batch 5 single samples want train model 10 batches per epoch training 50 single samples per epoch. set samples per epochs given formula get 50 samples taken fit generator sample batch yielded generator get 5 50 250 single samples. see get samples number batch dividing input length length input shape's zero axis. therefore calculate much times need ask batch. seems intricately me. ok. let's suppose formula wrong case set samples per epoch batch size. looks confident me. generator take exactly sample per epoch batches generator. batch size units single samples computed dividing. question actually right?,0,fit generator works,fit generator works quite understand fit generator works. particular mean parameter samples per epoch? described https github.com fchollet keras issues 1627 samples per epoch batch size numbers batches fit generator read exactly samples per epoch samples generator. also said control batch size generator yieldings. begins really missing. let's assume generator return batch 5 single samples want train model 10 batches per epoch training 50 single samples per epoch. set samples per epochs given formula get 50 samples taken fit generator sample batch yielded generator get 5 50 250 single samples. see get samples number batch dividing input length length input shape's zero axis. therefore calculate much times need ask batch. seems intricately me. ok. let's suppose formula wrong case set samples per epoch batch size. looks confident me. generator take exactly sample per epoch batches generator. batch size units single samples computed dividing. question actually right?
keras,6205,"want use keras develop face recognition project, think maybe need transfer learning way use transfer learning keras 2.0.2 thank",0,use transfer learning keras 2.0.2,"use transfer learning keras 2.0.2 want use keras develop face recognition project, think maybe need transfer learning way use transfer learning keras 2.0.2 thank"
keras,571,"running keras eample imdb. fresh man, leaning dl long. like keras framework. looks easy. besides question title, also want know save output layer. thanks!",0,save loss acc batch file? output results layer?,"save loss acc batch file? output results layer? running keras eample imdb. fresh man, leaning dl long. like keras framework. looks easy. besides question title, also want know save output layer. thanks!"
keras,1787,"page returns 404 https github.com fchollet keras blob master examples skipgram word embeddings.py code taken keras, moved somewhere else? thanks, zach please make sure boxes checked submit issue. thank you! x check date master branch keras. update pip install git git github.com fchollet keras.git upgrade deps x running theano, check date master branch theano. update pip install git git github.com theano theano.git upgrade deps x provide link github gist python script reproduce issue copy script short .",0,happened wordcontextproduct?,"happened wordcontextproduct? page returns 404 https github.com fchollet keras blob master examples skipgram word embeddings.py code taken keras, moved somewhere else? thanks, zach please make sure boxes checked submit issue. thank you! x check date master branch keras. update pip install git git github.com fchollet keras.git upgrade deps x running theano, check date master branch theano. update pip install git git github.com theano theano.git upgrade deps x provide link github gist python script reproduce issue copy script short ."
keras,3310,"currently trying use multiple vgg16 models keras model zoo classify videos. stage taking frames video classify whole video. take vgg model, popout last two layers merge models togethers. method working right now, try train network. get error theano disconnected inputs, inputs apparently inputs popped vgg model. think might problem. also get error load weights vgg model. gist reproduces error https gist.github.com m1sk b8ed6d43a5ea86ae51f193d5fc2c01b3 also advice better way build network appreciated. specs latest keras theano git updated today running windows 8.1 anaconda full text error disconnectedinputerror traceback recent call last 79 train np.random.randint 2, size examples 80 81 merged model.fit x train,y train n programs anaconda lib site packages keras 1.0.6 py2.7.egg keras models.pyc fit self, x, y, batch size, nb epoch, verbose, callbacks, validation split, validation data, shuffle, class weight, sample weight, kwargs 430 shuffle shuffle, 431 class weight class weight, 432 sample weight sample weight 433 434 def evaluate self, x, y, batch size 32, verbose 1, n programs anaconda lib site packages keras 1.0.6 py2.7.egg keras engine training.pyc fit self, x, y, batch size, nb epoch, verbose, callbacks, validation split, validation data, shuffle, class weight, sample weight 1077 else 1078 ins x sample weights 1079 self. make train function 1080 f self.train function 1081 n programs anaconda lib site packages keras 1.0.6 py2.7.egg keras engine training.pyc make train function self 694 get trainable weights 695 trainable weights collect trainable weights self 696 training updates self.optimizer.get updates trainable weights, self.constraints, self.total loss 697 updates self.updates training updates 698 n programs anaconda lib site packages keras 1.0.6 py2.7.egg keras optimizers.pyc get updates self, params, constraints, loss 119 120 def get updates self, params, constraints, loss 121 grads self.get gradients loss, params 122 lr self.lr 1. 1. self.decay self.iterations 123 self.updates k.update add self.iterations, 1 n programs anaconda lib site packages keras 1.0.6 py2.7.egg keras optimizers.pyc get gradients self, loss, params 51 52 def get gradients self, loss, params 53 grads k.gradients loss, params 54 hasattr self, 'clipnorm' self.clipnorm 0 55 norm k.sqrt sum k.sum k.square g g grads n programs anaconda lib site packages keras 1.0.6 py2.7.egg keras backend theano backend.pyc gradients loss, variables 655 656 def gradients loss, variables 657 return t.grad loss, variables 658 659 n programs anaconda lib site packages theano 0.9.0.dev2 py2.7.egg theano gradient.pyc grad cost, wrt, consider constant, disconnected inputs, add names, known grads, return disconnected, null gradients 531 elem var app idx elem cost 532 elem grad dict 533 handle disconnected elem 534 grad dict elem disconnected type 535 n programs anaconda lib site packages theano 0.9.0.dev2 py2.7.egg theano gradient.pyc handle disconnected var 518 elif disconnected inputs 'raise' 519 message utils.get variable trace string var 520 raise disconnectedinputerror message 521 else 522 raise valueerror invalid value keyword disconnectedinputerror backtrace variable created file kerasmodelzoo vgg16.py , line 53, model vgg16 model.add dense 1000, activation 'softmax' file n programs anaconda lib site packages keras 1.0.6 py2.7.egg keras models.py , line 146, add output tensor layer self.outputs 0 file n programs anaconda lib site packages keras 1.0.6 py2.7.egg keras engine topology.py , line 458, call self.build input shapes 0 file n programs anaconda lib site packages keras 1.0.6 py2.7.egg keras layers core.py , line 604, build name ' w'.format self.name file n programs anaconda lib site packages keras 1.0.6 py2.7.egg keras initializations.py , line 59, glorot uniform return uniform shape, s, name name file n programs anaconda lib site packages keras 1.0.6 py2.7.egg keras initializations.py , line 32, uniform return k.random uniform variable shape, scale, scale, name name file n programs anaconda lib site packages keras 1.0.6 py2.7.egg keras backend theano backend.py , line 111, random uniform variable dtype dtype, name name file n programs anaconda lib site packages keras 1.0.6 py2.7.egg keras backend theano backend.py , line 40, variable return theano.shared value value, name name, strict false",0,disconnectedinputerror popping layers adding top,"disconnectedinputerror popping layers adding top currently trying use multiple vgg16 models keras model zoo classify videos. stage taking frames video classify whole video. take vgg model, popout last two layers merge models togethers. method working right now, try train network. get error theano disconnected inputs, inputs apparently inputs popped vgg model. think might problem. also get error load weights vgg model. gist reproduces error https gist.github.com m1sk b8ed6d43a5ea86ae51f193d5fc2c01b3 also advice better way build network appreciated. specs latest keras theano git updated today running windows 8.1 anaconda full text error disconnectedinputerror traceback recent call last 79 train np.random.randint 2, size examples 80 81 merged model.fit x train,y train n programs anaconda lib site packages keras 1.0.6 py2.7.egg keras models.pyc fit self, x, y, batch size, nb epoch, verbose, callbacks, validation split, validation data, shuffle, class weight, sample weight, kwargs 430 shuffle shuffle, 431 class weight class weight, 432 sample weight sample weight 433 434 def evaluate self, x, y, batch size 32, verbose 1, n programs anaconda lib site packages keras 1.0.6 py2.7.egg keras engine training.pyc fit self, x, y, batch size, nb epoch, verbose, callbacks, validation split, validation data, shuffle, class weight, sample weight 1077 else 1078 ins x sample weights 1079 self. make train function 1080 f self.train function 1081 n programs anaconda lib site packages keras 1.0.6 py2.7.egg keras engine training.pyc make train function self 694 get trainable weights 695 trainable weights collect trainable weights self 696 training updates self.optimizer.get updates trainable weights, self.constraints, self.total loss 697 updates self.updates training updates 698 n programs anaconda lib site packages keras 1.0.6 py2.7.egg keras optimizers.pyc get updates self, params, constraints, loss 119 120 def get updates self, params, constraints, loss 121 grads self.get gradients loss, params 122 lr self.lr 1. 1. self.decay self.iterations 123 self.updates k.update add self.iterations, 1 n programs anaconda lib site packages keras 1.0.6 py2.7.egg keras optimizers.pyc get gradients self, loss, params 51 52 def get gradients self, loss, params 53 grads k.gradients loss, params 54 hasattr self, 'clipnorm' self.clipnorm 0 55 norm k.sqrt sum k.sum k.square g g grads n programs anaconda lib site packages keras 1.0.6 py2.7.egg keras backend theano backend.pyc gradients loss, variables 655 656 def gradients loss, variables 657 return t.grad loss, variables 658 659 n programs anaconda lib site packages theano 0.9.0.dev2 py2.7.egg theano gradient.pyc grad cost, wrt, consider constant, disconnected inputs, add names, known grads, return disconnected, null gradients 531 elem var app idx elem cost 532 elem grad dict 533 handle disconnected elem 534 grad dict elem disconnected type 535 n programs anaconda lib site packages theano 0.9.0.dev2 py2.7.egg theano gradient.pyc handle disconnected var 518 elif disconnected inputs 'raise' 519 message utils.get variable trace string var 520 raise disconnectedinputerror message 521 else 522 raise valueerror invalid value keyword disconnectedinputerror backtrace variable created file kerasmodelzoo vgg16.py , line 53, model vgg16 model.add dense 1000, activation 'softmax' file n programs anaconda lib site packages keras 1.0.6 py2.7.egg keras models.py , line 146, add output tensor layer self.outputs 0 file n programs anaconda lib site packages keras 1.0.6 py2.7.egg keras engine topology.py , line 458, call self.build input shapes 0 file n programs anaconda lib site packages keras 1.0.6 py2.7.egg keras layers core.py , line 604, build name ' w'.format self.name file n programs anaconda lib site packages keras 1.0.6 py2.7.egg keras initializations.py , line 59, glorot uniform return uniform shape, s, name name file n programs anaconda lib site packages keras 1.0.6 py2.7.egg keras initializations.py , line 32, uniform return k.random uniform variable shape, scale, scale, name name file n programs anaconda lib site packages keras 1.0.6 py2.7.egg keras backend theano backend.py , line 111, random uniform variable dtype dtype, name name file n programs anaconda lib site packages keras 1.0.6 py2.7.egg keras backend theano backend.py , line 40, variable return theano.shared value value, name name, strict false"
keras,4239,"following snippet keras.layers import input, lstm, dense keras.layers.wrappers import timedistributed keras.models import model batch size 32 time steps 10 input dim 5 num classes 3 hidden size 16 inputs input batch shape batch size, time steps, input dim lstm hidden size, return sequences true inputs timedistributed dense num classes, activation 'softmax' exception raised model model input inputs, output model.compile optimizer 'rmsprop' raises following exception indicated line using tensorflow backend valueerror shape 1 merge 2 0 invariant loop. enters loop shape 10, 16 , shape 32, 3 one iteration. provide shape invariants using either argument tf.while loop set shape loop variables. seems compatibility issue tensorflow 0.11 tensorflow 0.10rc0 keras 1.1.0 works tensorflow 0.11rc0 keras 1.1.0 fails tensorflow 0.11rc1 keras 1.1.0 fails tested python 3 debian jessie ubuntu 14.04.5 lts.",0,shape invariance error using timedistributed layers tensorflow,"shape invariance error using timedistributed layers tensorflow following snippet keras.layers import input, lstm, dense keras.layers.wrappers import timedistributed keras.models import model batch size 32 time steps 10 input dim 5 num classes 3 hidden size 16 inputs input batch shape batch size, time steps, input dim lstm hidden size, return sequences true inputs timedistributed dense num classes, activation 'softmax' exception raised model model input inputs, output model.compile optimizer 'rmsprop' raises following exception indicated line using tensorflow backend valueerror shape 1 merge 2 0 invariant loop. enters loop shape 10, 16 , shape 32, 3 one iteration. provide shape invariants using either argument tf.while loop set shape loop variables. seems compatibility issue tensorflow 0.11 tensorflow 0.10rc0 keras 1.1.0 works tensorflow 0.11rc0 keras 1.1.0 fails tensorflow 0.11rc1 keras 1.1.0 fails tested python 3 debian jessie ubuntu 14.04.5 lts."
keras,1606,"looking way something seems conceptually simple, found way keras yet. cnn dropout applied various places network. training, like compute forward pass data dropout mask applied time compute forward pass different dropout mask applied. understand things correctly, done training. however, use standard model.predict x test training, dropout applied outputs multiplied dropout probability, yielding non stochastic set predictions. anyone know achieve this?",0,forward pass dropout,"forward pass dropout looking way something seems conceptually simple, found way keras yet. cnn dropout applied various places network. training, like compute forward pass data dropout mask applied time compute forward pass different dropout mask applied. understand things correctly, done training. however, use standard model.predict x test training, dropout applied outputs multiplied dropout probability, yielding non stochastic set predictions. anyone know achieve this?"
keras,1434,try run code output pile c code 1009 lines first lines says support code besides get another long error g gcc error log using windows run code powershell log.txt https github.com fchollet keras files 83558 log.txt thank you.,0,lstm text generation.py work,lstm text generation.py work try run code output pile c code 1009 lines first lines says support code besides get another long error g gcc error log using windows run code powershell log.txt https github.com fchollet keras files 83558 log.txt thank you.
keras,1907,"hi, wondering find explanation need apply pad sentences text embedding layer 2d transformation ? thanks!",0,pad sentences,"pad sentences hi, wondering find explanation need apply pad sentences text embedding layer 2d transformation ? thanks!"
keras,4919,"issue raised 953, believe fix c4f3155d192935c7cb659cac6d38c76b15ec971e solve issue. code reproduce",0,saving loading models frozen layers still working 1.2.0,"saving loading models frozen layers still working 1.2.0 issue raised 953, believe fix c4f3155d192935c7cb659cac6d38c76b15ec971e solve issue. code reproduce"
keras,6631,"hi, attempting implement entropysgd paper https arxiv.org pdf 1611.01838.pdf . however inner loop optimizer requires l gradient updates estimate exponential decayed mus used update parameters one batch data. looks like 'get updates' called training.py. question 1. way update network params, compute new loss get new gradients batch data optimizer class run l iterations langevin dynamics? 2. not, implemented ways added future feature? thanks",0,entropysgd gradient updates langevin dynamics,"entropysgd gradient updates langevin dynamics hi, attempting implement entropysgd paper https arxiv.org pdf 1611.01838.pdf . however inner loop optimizer requires l gradient updates estimate exponential decayed mus used update parameters one batch data. looks like 'get updates' called training.py. question 1. way update network params, compute new loss get new gradients batch data optimizer class run l iterations langevin dynamics? 2. not, implemented ways added future feature? thanks"
keras,4857,"currently, callbacks perform general function carrying action number epochs improvement monitored metric. classes could rewritten subclasses general class monitors quantity interest calls function quantity ceases improve. would make easy implement callbacks dependent training progress future.",0,implement general callback perform action epochs improvement monitored metric,"implement general callback perform action epochs improvement monitored metric currently, callbacks perform general function carrying action number epochs improvement monitored metric. classes could rewritten subclasses general class monitors quantity interest calls function quantity ceases improve. would make easy implement callbacks dependent training progress future."
keras,6860,"hi, csv files, file contains time series forecasting problem. problem thet whenever fit csv file file, precedent fit be,obviously, forget. method forget it? insert image problem. ! problema https cloud.githubusercontent.com assets 18617527 26780123 205af932 49e0 11e7 8e9b e634c86f77c3.jpg idea using lstm, testing want insert one single input time, predict value",0,training lstm .csv files,"training lstm .csv files hi, csv files, file contains time series forecasting problem. problem thet whenever fit csv file file, precedent fit be,obviously, forget. method forget it? insert image problem. ! problema https cloud.githubusercontent.com assets 18617527 26780123 205af932 49e0 11e7 8e9b e634c86f77c3.jpg idea using lstm, testing want insert one single input time, predict value"
keras,379,"building network using new graph model takes two sequences inputs, feeds rnn, concatenates final output vectors, passes dense layer. expected input size dense layer twice output dimensionality rnns seem working. anyone know may wrong? error get seems like two rnn outputs actually getting merged 10 dimensional vector, expected? edit error goes away model trains successfully using theano cpu backend. edit 2.0 error goes away even gpu switch jzs1 rnn lstm. edit 3.0 actually, error persists lstms using floatx 32. current hypothesis merging input variables get handled correctly calling external matrix multiply routine.",0,merge mode concat behaving unexpectedly? gpu backend,"merge mode concat behaving unexpectedly? gpu backend building network using new graph model takes two sequences inputs, feeds rnn, concatenates final output vectors, passes dense layer. expected input size dense layer twice output dimensionality rnns seem working. anyone know may wrong? error get seems like two rnn outputs actually getting merged 10 dimensional vector, expected? edit error goes away model trains successfully using theano cpu backend. edit 2.0 error goes away even gpu switch jzs1 rnn lstm. edit 3.0 actually, error persists lstms using floatx 32. current hypothesis merging input variables get handled correctly calling external matrix multiply routine."
keras,9116,"trying simple model, consists two separate inputs two different embedded inputs . problem toy scenario forms framework complex system need create multiple inputs multple embeddings, however need get basic system working. problem using multiple embeddings one input case, try using multiple inputs get follow error main 1 userwarning function deprecated removed 08 2017. use instead layers , e.g. , , etc. services tools anaconda3 4.0.0 lib python3.5 site packages keras legacy layers.py 460 userwarning layer deprecated removed 08 2017. use instead layers , e.g. , , etc. name name traceback recent call last file , line 3, model model inputs q1 ,q2 , q3 , q4 , outputs mc mergeout file services tools anaconda3 4.0.0 lib python3.5 site packages keras legacy interfaces.py , line 88, wrapper return func args, kwargs file services tools anaconda3 4.0.0 lib python3.5 site packages keras engine topology.py , line 1485, init inputs set set self.inputs typeerror unhashable type 'list' anyone tried multi input multiple embeddings input? help would greatly appreciated time deadline. thanks advance",0,multiple inputs multiple embeddings working,"multiple inputs multiple embeddings working trying simple model, consists two separate inputs two different embedded inputs . problem toy scenario forms framework complex system need create multiple inputs multple embeddings, however need get basic system working. problem using multiple embeddings one input case, try using multiple inputs get follow error main 1 userwarning function deprecated removed 08 2017. use instead layers , e.g. , , etc. services tools anaconda3 4.0.0 lib python3.5 site packages keras legacy layers.py 460 userwarning layer deprecated removed 08 2017. use instead layers , e.g. , , etc. name name traceback recent call last file , line 3, model model inputs q1 ,q2 , q3 , q4 , outputs mc mergeout file services tools anaconda3 4.0.0 lib python3.5 site packages keras legacy interfaces.py , line 88, wrapper return func args, kwargs file services tools anaconda3 4.0.0 lib python3.5 site packages keras engine topology.py , line 1485, init inputs set set self.inputs typeerror unhashable type 'list' anyone tried multi input multiple embeddings input? help would greatly appreciated time deadline. thanks advance"
keras,1348,"hi everyone, dataset 24 inputs, 1 output, categorical value. x train.shape 20000,24 x test.shape 5000,24 output one 1 six values a, b, c, d, e, f. train.shape 20000,6 . adapting https github.com fchollet keras issues 579 met problem line could help point wrong? thanks,",0,input shape error convnet 1d,"input shape error convnet 1d hi everyone, dataset 24 inputs, 1 output, categorical value. x train.shape 20000,24 x test.shape 5000,24 output one 1 six values a, b, c, d, e, f. train.shape 20000,6 . adapting https github.com fchollet keras issues 579 met problem line could help point wrong? thanks,"
keras,1130,"fix issue 1125, trying get error traceback theano date. missing anything?",0,type error stateful,"type error stateful fix issue 1125, trying get error traceback theano date. missing anything?"
keras,1587,"training model theano cuda, attempt specify large batch size 1024 case , reports memory error, understandable. however, change back size previously worked notebook , still memory, attempt free whatever allocated previous attempt, forced restart python process reload data recompile models . provide model code needed, laptop internet access currently.",0,memory issues gpu,"memory issues gpu training model theano cuda, attempt specify large batch size 1024 case , reports memory error, understandable. however, change back size previously worked notebook , still memory, attempt free whatever allocated previous attempt, forced restart python process reload data recompile models . provide model code needed, laptop internet access currently."
keras,1723,"hi all, first thanks wonderful framework great work guys done. trying use keras build simple lstm language model vocabulary size 50,000. considering one hot representation words, translates 50,000 classes. avoid padding, dividing training data 80 million sentences chunks chunk includes sentences length. model compiles well training loss decreasing two main problems 1 using function results huge memory computation? waste. tried avoid got rank mismatch error theano. work around this? 2 guess number classes huge running time becomes high. solution this? e.g., language model experiment, one lstm layer softmax layer top, 512 cells lstm, maximum sentence length 15, vocabulary size 50,000 mini batch size 1024 sentences, takes 4 sec mini batch gpu, long. also consumes 7 gb gpu memory believe function. thanks! hamid",0,"keras many number classes, e.g., language modelling vocabulary size 50,000.","keras many number classes, e.g., language modelling vocabulary size 50,000. hi all, first thanks wonderful framework great work guys done. trying use keras build simple lstm language model vocabulary size 50,000. considering one hot representation words, translates 50,000 classes. avoid padding, dividing training data 80 million sentences chunks chunk includes sentences length. model compiles well training loss decreasing two main problems 1 using function results huge memory computation? waste. tried avoid got rank mismatch error theano. work around this? 2 guess number classes huge running time becomes high. solution this? e.g., language model experiment, one lstm layer softmax layer top, 512 cells lstm, maximum sentence length 15, vocabulary size 50,000 mini batch size 1024 sentences, takes 4 sec mini batch gpu, long. also consumes 7 gb gpu memory believe function. thanks! hamid"
keras,7927,"hello, trouble able manage correctly cnn maybe lstm rnn. trying solve following problem. 20000 chunks data composed sequence 20 images. size image 40x40. images grayscale mode. chunk data, two possible labels, 0 1. classify chunk data two possible sets 0 1. numpy arrays, shape one data shape 20000, 20, 40, 40 labels shape 20000, 1 well, want train cnn predict label chunk data 0 1. remember chunk data composed 20 images 40x40 grayscale. important thing order 20 images matter. mean, managed 'little' video file 20 images. change order 20 images chunk data label different. know mandatory use rnn. read papers researchers used cnn without lstm lstm trying solve similar problems. think mandatory think use combination cnn timedistributed. right? thank much advance. kind regards, rub n",0,cnn manage sequences grayscale images like video files,"cnn manage sequences grayscale images like video files hello, trouble able manage correctly cnn maybe lstm rnn. trying solve following problem. 20000 chunks data composed sequence 20 images. size image 40x40. images grayscale mode. chunk data, two possible labels, 0 1. classify chunk data two possible sets 0 1. numpy arrays, shape one data shape 20000, 20, 40, 40 labels shape 20000, 1 well, want train cnn predict label chunk data 0 1. remember chunk data composed 20 images 40x40 grayscale. important thing order 20 images matter. mean, managed 'little' video file 20 images. change order 20 images chunk data label different. know mandatory use rnn. read papers researchers used cnn without lstm lstm trying solve similar problems. think mandatory think use combination cnn timedistributed. right? thank much advance. kind regards, rub n"
keras,11989,"try use pre trained models float16 mode, load. think fixing useful, especially object detection use cases.",0,pre trained models work float16 mode,"pre trained models work float16 mode try use pre trained models float16 mode, load. think fixing useful, especially object detection use cases."
keras,1573,"hello, trying add weights classes training right final layer, output shape layer one hot label size, using loss here. added class balancing layer seem help... suggestions greatly appreciated! update tried use send class weight dictionary , got error",0,regarding class balancing,"regarding class balancing hello, trying add weights classes training right final layer, output shape layer one hot label size, using loss here. added class balancing layer seem help... suggestions greatly appreciated! update tried use send class weight dictionary , got error"
keras,4585,"title states, run mnist acgan example discriminator loss quickly goes zero generator loss increases time. generated images end epoch black. digging myself, gans area expect progress slow. suggestions look? anyone able run example without problems? link example https github.com fchollet keras blob master examples mnist acgan.py x check date master branch keras. update pip install git git github.com fchollet keras.git upgrade deps x running theano, check date master branch theano. update pip install git git github.com theano theano.git upgrade deps x provide link github gist python script reproduce issue copy script short .",0,generator mnist acgan example collapses black images,"generator mnist acgan example collapses black images title states, run mnist acgan example discriminator loss quickly goes zero generator loss increases time. generated images end epoch black. digging myself, gans area expect progress slow. suggestions look? anyone able run example without problems? link example https github.com fchollet keras blob master examples mnist acgan.py x check date master branch keras. update pip install git git github.com fchollet keras.git upgrade deps x running theano, check date master branch theano. update pip install git git github.com theano theano.git upgrade deps x provide link github gist python script reproduce issue copy script short ."
keras,2366,"using modelcheckpoint described keras.io sequential model. save model using code described faq try reload based weights saved modelcheckpoint, tells model.load weights os.path.join 'checkpoints' ' weights.hdf5' traceback recent call last file , line 1, model.load weights os.path.join 'checkpoints' ' weights.hdf5' file c winpython winpython 64bit 3.4.4.1 python 3.4.4.amd64 lib site packages keras engine topology.py , line 2286, load weights str len flattened layers ' layers.' exception trying load weight file containing 31 layers model 30 layers. error way modelcheckpoint saves weights, instance including input layer weights 1?",0,modelcheckpoint saves one extra layer weights,"modelcheckpoint saves one extra layer weights using modelcheckpoint described keras.io sequential model. save model using code described faq try reload based weights saved modelcheckpoint, tells model.load weights os.path.join 'checkpoints' ' weights.hdf5' traceback recent call last file , line 1, model.load weights os.path.join 'checkpoints' ' weights.hdf5' file c winpython winpython 64bit 3.4.4.1 python 3.4.4.amd64 lib site packages keras engine topology.py , line 2286, load weights str len flattened layers ' layers.' exception trying load weight file containing 31 layers model 30 layers. error way modelcheckpoint saves weights, instance including input layer weights 1?"
keras,9262,"respect default float precision keras configuration file. case, default actual . related stackoverflow thread https stackoverflow.com q 48486775 1348273 . x check date master branch keras. update pip install git git github.com keras team keras.git upgrade deps x running tensorflow, check date latest version. installation instructions found https www.tensorflow.org get started os setup . running theano, check date master branch theano. update pip install git git github.com theano theano.git upgrade deps x provide link github gist python script reproduce issue copy script short . python import numpy np keras.utils import categorical print categorical np.ones 2 , 2 .dtype",0,categorical respect default float precision keras configuration file,"categorical respect default float precision keras configuration file respect default float precision keras configuration file. case, default actual . related stackoverflow thread https stackoverflow.com q 48486775 1348273 . x check date master branch keras. update pip install git git github.com keras team keras.git upgrade deps x running tensorflow, check date latest version. installation instructions found https www.tensorflow.org get started os setup . running theano, check date master branch theano. update pip install git git github.com theano theano.git upgrade deps x provide link github gist python script reproduce issue copy script short . python import numpy np keras.utils import categorical print categorical np.ones 2 , 2 .dtype"
keras,7355,go error mnist sklearn wrapper.py,0,mnist sklearn wrapper.py,mnist sklearn wrapper.py go error mnist sklearn wrapper.py
keras,4962,"text classification. also using pre trained word embeddings layer top end. pretty simple. want add attention model, know it. understanding set attention layer weigh timestep accordingly. way lstm return 3d tensor, right? do? way easily implement model attention using keras layers write custom layer? done available keras layers, would really appreciate example.",0,add attention top recurrent layer text classification,"add attention top recurrent layer text classification text classification. also using pre trained word embeddings layer top end. pretty simple. want add attention model, know it. understanding set attention layer weigh timestep accordingly. way lstm return 3d tensor, right? do? way easily implement model attention using keras layers write custom layer? done available keras layers, would really appreciate example."
keras,3657,"please make sure boxes checked submit issue. thank you! x check date master branch keras. update pip install git git github.com fchollet keras.git upgrade deps x running theano, check date master branch theano. update pip install git git github.com theano theano.git upgrade deps x provide link github gist python script reproduce issue copy script short . performing batch learning batches get error line code idea get error, seems happen randomly, anyone point right direction? module code running",0,attributeerror 'progbarlogger' object attribute 'log values',"attributeerror 'progbarlogger' object attribute 'log values' please make sure boxes checked submit issue. thank you! x check date master branch keras. update pip install git git github.com fchollet keras.git upgrade deps x running theano, check date master branch theano. update pip install git git github.com theano theano.git upgrade deps x provide link github gist python script reproduce issue copy script short . performing batch learning batches get error line code idea get error, seems happen randomly, anyone point right direction? module code running"
keras,6655,"edit new example proposal details https github.com fchollet keras pull 6891 issuecomment 307659460. discussion dense prediction api image segmentation https github.com fchollet keras issues 6538 brought possibility functional preprocessing api, could move preprocessing steps keras backend apis generalize preprocessing network designs. dropout https github.com fchollet keras blob master keras layers core.py l72 provides precedent augmentation layers. layers could designed much like dropout, would expect configured augmentation operations could applied identically one image inputs well one image label data, useful dense prediction tasks. could advantages easy use, easily applied consistently arbitrary data inputs, make possible use tf backend image augmentation apis https www.tensorflow.org api guides python image thus improving performance. would pros cons barriers functional preprocessing api? example usage, label augmentation optional dense prediction tasks",0,functional preprocessing augmentation api,"functional preprocessing augmentation api edit new example proposal details https github.com fchollet keras pull 6891 issuecomment 307659460. discussion dense prediction api image segmentation https github.com fchollet keras issues 6538 brought possibility functional preprocessing api, could move preprocessing steps keras backend apis generalize preprocessing network designs. dropout https github.com fchollet keras blob master keras layers core.py l72 provides precedent augmentation layers. layers could designed much like dropout, would expect configured augmentation operations could applied identically one image inputs well one image label data, useful dense prediction tasks. could advantages easy use, easily applied consistently arbitrary data inputs, make possible use tf backend image augmentation apis https www.tensorflow.org api guides python image thus improving performance. would pros cons barriers functional preprocessing api? example usage, label augmentation optional dense prediction tasks"
keras,1315,"usr bin python2.7 home dell dltest cifar test residul cifar residul net.py using theano backend. using gpu device 1 geforce gtx titan x cnmem disabled 'x train shape ', 50000, 3, 32, 32 50000, 'train samples' 10000, 'test samples' traceback recent call last file home dell dltest cifar test residul cifar residul net.py , line 87, model.compile loss 'categorical crossentropy', optimizer sgd file home dell .local lib python2.7 site packages keras models.py , line 372, compile self.y train self.get output train true file home dell .local lib python2.7 site packages keras layers containers.py , line 73, get output return self.layers 1 .get output train file home dell .local lib python2.7 site packages keras layers core.py , line 681, get output x self.get input train file home dell .local lib python2.7 site packages keras layers core.py , line 102, get input return self.previous.get output train train file home dell .local lib python2.7 site packages keras layers core.py , line 591, get output x self.get input train file home dell .local lib python2.7 site packages keras layers core.py , line 102, get input return self.previous.get output train train file home dell .local lib python2.7 site packages keras layers containers.py , line 216, get output return self.outputs self.output order 0 .get output train file home dell .local lib python2.7 site packages keras layers core.py , line 389, get output self.layers 0 .get output train file home dell .local lib python2.7 site packages keras layers convolutional.py , line 212, get output x self.get input train file home dell .local lib python2.7 site packages keras layers core.py , line 102, get input return self.previous.get output train train file home dell .local lib python2.7 site packages keras layers core.py , line 98, get output return self.get input train file home dell .local lib python2.7 site packages keras layers core.py , line 102, get input return self.previous.get output train train file home dell .local lib python2.7 site packages keras layers containers.py , line 216, get output return self.outputs self.output order 0 .get output train file home dell .local lib python2.7 site packages keras layers core.py , line 389, get output self.layers 0 .get output train file home dell .local lib python2.7 site packages keras layers convolutional.py , line 215, get output dim ordering self.dim ordering file home dell .local lib python2.7 site packages keras backend theano backend.py , line 543, conv2d border mode pad x, pad file usr local lib python2.7 dist packages theano sandbox cuda dnn.py , line 1191, dnn conv conv mode conv mode img.shape, kerns.shape file usr local lib python2.7 dist packages theano sandbox cuda dnn.py , line 251, init border mode tuple map int, border mode typeerror int argument must string number, 'tensorvariable' got bug delete usr local cuda lib64 libcudnn usr local cuda include cudnn.h add files cudnn7 5v3 path",0,bug theano sandbox cuda dnn.py,"bug theano sandbox cuda dnn.py usr bin python2.7 home dell dltest cifar test residul cifar residul net.py using theano backend. using gpu device 1 geforce gtx titan x cnmem disabled 'x train shape ', 50000, 3, 32, 32 50000, 'train samples' 10000, 'test samples' traceback recent call last file home dell dltest cifar test residul cifar residul net.py , line 87, model.compile loss 'categorical crossentropy', optimizer sgd file home dell .local lib python2.7 site packages keras models.py , line 372, compile self.y train self.get output train true file home dell .local lib python2.7 site packages keras layers containers.py , line 73, get output return self.layers 1 .get output train file home dell .local lib python2.7 site packages keras layers core.py , line 681, get output x self.get input train file home dell .local lib python2.7 site packages keras layers core.py , line 102, get input return self.previous.get output train train file home dell .local lib python2.7 site packages keras layers core.py , line 591, get output x self.get input train file home dell .local lib python2.7 site packages keras layers core.py , line 102, get input return self.previous.get output train train file home dell .local lib python2.7 site packages keras layers containers.py , line 216, get output return self.outputs self.output order 0 .get output train file home dell .local lib python2.7 site packages keras layers core.py , line 389, get output self.layers 0 .get output train file home dell .local lib python2.7 site packages keras layers convolutional.py , line 212, get output x self.get input train file home dell .local lib python2.7 site packages keras layers core.py , line 102, get input return self.previous.get output train train file home dell .local lib python2.7 site packages keras layers core.py , line 98, get output return self.get input train file home dell .local lib python2.7 site packages keras layers core.py , line 102, get input return self.previous.get output train train file home dell .local lib python2.7 site packages keras layers containers.py , line 216, get output return self.outputs self.output order 0 .get output train file home dell .local lib python2.7 site packages keras layers core.py , line 389, get output self.layers 0 .get output train file home dell .local lib python2.7 site packages keras layers convolutional.py , line 215, get output dim ordering self.dim ordering file home dell .local lib python2.7 site packages keras backend theano backend.py , line 543, conv2d border mode pad x, pad file usr local lib python2.7 dist packages theano sandbox cuda dnn.py , line 1191, dnn conv conv mode conv mode img.shape, kerns.shape file usr local lib python2.7 dist packages theano sandbox cuda dnn.py , line 251, init border mode tuple map int, border mode typeerror int argument must string number, 'tensorvariable' got bug delete usr local cuda lib64 libcudnn usr local cuda include cudnn.h add files cudnn7 5v3 path"
keras,741,"need train multi label softmax classifier, lot one hot code labels examples, change code it?",0,train multi label classifier,"train multi label classifier need train multi label softmax classifier, lot one hot code labels examples, change code it?"
keras,13105,"system information written custom code opposed using example directory os platform distribution e.g., linux ubuntu 16.04 arch linux tensorflow backend yes yes tensorflow version 1.13.1 keras version 2.1.6 python version 3.7 cuda cudnn version 10 gpu model memory gtx 1070 hello, using faster rcnn resnet50 base network object detection. uses custom batchnormalization layer named 'fixedbatchnormalization' resnet. want know what's difference custom layer official batchnormalization layer keras. know ongoing issue fine tuning resnet freezing layers. 'fixedbatchnormalization' repo solve issue anyway affect training evaluating? layer works keras version 2.1.6. link faster rcnn repo https github.com kbardool keras frcnn url fixedbatchnormalization code",0,custom batchnormalization layer resnet,"custom batchnormalization layer resnet system information written custom code opposed using example directory os platform distribution e.g., linux ubuntu 16.04 arch linux tensorflow backend yes yes tensorflow version 1.13.1 keras version 2.1.6 python version 3.7 cuda cudnn version 10 gpu model memory gtx 1070 hello, using faster rcnn resnet50 base network object detection. uses custom batchnormalization layer named 'fixedbatchnormalization' resnet. want know what's difference custom layer official batchnormalization layer keras. know ongoing issue fine tuning resnet freezing layers. 'fixedbatchnormalization' repo solve issue anyway affect training evaluating? layer works keras version 2.1.6. link faster rcnn repo https github.com kbardool keras frcnn url fixedbatchnormalization code"
keras,2466,"got error code see printed trainx shape got 28709l, 48l, 48l data dim correct",0,"error wrong number dimensions expected 4, got 3 shape","error wrong number dimensions expected 4, got 3 shape got error code see printed trainx shape got 28709l, 48l, 48l data dim correct"
keras,4232,"recurrence formula rnn , classification output softmax output rnn time . access output next time step? guess write custom rnn, bit confused recurrent functions recurrent.py keras sure parts modified.",0,accessing softmax output previous rnn state,"accessing softmax output previous rnn state recurrence formula rnn , classification output softmax output rnn time . access output next time step? guess write custom rnn, bit confused recurrent functions recurrent.py keras sure parts modified."
keras,6168,"please consider simple example nb samples 100000 x np.random.randn nb samples x 1 x x 1 x x.reshape len , 1, 1 y.reshape len , 1 basically x 1 model simply lag operator. try learn model stateful lstm, giving pairs values one one model sequential model.add lstm batch input shape 1, 1, 1 , output dim 10, activation 'tanh', stateful true model.add dense output dim 1, activation 'linear' model.compile loss 'mse', optimizer 'adam' epoch range 10000 model.reset states train loss 0 range train.shape 0 train loss model.train batch x train 1 , train 1 , print ' epoch', epoch, ' loss ', train loss float train.shape 0 seeing mean loss around 1, standard deviation randomly generated data, model seem learn. something wrong?",0,simple stateful lstm example,"simple stateful lstm example please consider simple example nb samples 100000 x np.random.randn nb samples x 1 x x 1 x x.reshape len , 1, 1 y.reshape len , 1 basically x 1 model simply lag operator. try learn model stateful lstm, giving pairs values one one model sequential model.add lstm batch input shape 1, 1, 1 , output dim 10, activation 'tanh', stateful true model.add dense output dim 1, activation 'linear' model.compile loss 'mse', optimizer 'adam' epoch range 10000 model.reset states train loss 0 range train.shape 0 train loss model.train batch x train 1 , train 1 , print ' epoch', epoch, ' loss ', train loss float train.shape 0 seeing mean loss around 1, standard deviation randomly generated data, model seem learn. something wrong?"
keras,3421,"getting weird error, keras modules load without problem. running variational autoencoder deconv.py ideas?",0,importerror cannot import name 'deconvolution2d' install source,"importerror cannot import name 'deconvolution2d' install source getting weird error, keras modules load without problem. running variational autoencoder deconv.py ideas?"
keras,11379,"trying implement dynamic zero padding keep second dimension constant tensor going convolutional layers stride 1, input tensor following shape batch size, time step, 50 , need time step dimension changed convolutional layers. tried use 'same' padding, however stride 1, work, created custom layer zeropadding, working tensors shape none, 100,50 , none, 120,50 , none, 60,50 , work dynamic shapes type none, none, 50 , get following error added custom class imdb example, make easier reproduce errors. change model.add embedding max features, embedding dims, input length none model.add embedding max features, embedding dims, input length 400 dynamic padding work, however needs work dimension type none. code left pad, right pad batch, axis pad, features batch, padded axis, features searched saw use k.shape inputs , get correct shape runtime, instead none, could make work keras, could anyone help me? another solution solve problem zero dynamic padding, welcome. question also stackoverflow zeropadding dynamic step 1 access actual shape tensor dimension none keras https stackoverflow.com questions 52788133 zeropadding dynamic step 1 access actual shape tensor dimen thank advance attention. thank you! v check date master branch keras. update v check version tensorflow date. installation instructions found https www.tensorflow.org get started os setup . v provide link github gist python script reproduce issue copy script short .",0,zeropadding dynamic step 1 access actual shape tensor dimension none keras,"zeropadding dynamic step 1 access actual shape tensor dimension none keras trying implement dynamic zero padding keep second dimension constant tensor going convolutional layers stride 1, input tensor following shape batch size, time step, 50 , need time step dimension changed convolutional layers. tried use 'same' padding, however stride 1, work, created custom layer zeropadding, working tensors shape none, 100,50 , none, 120,50 , none, 60,50 , work dynamic shapes type none, none, 50 , get following error added custom class imdb example, make easier reproduce errors. change model.add embedding max features, embedding dims, input length none model.add embedding max features, embedding dims, input length 400 dynamic padding work, however needs work dimension type none. code left pad, right pad batch, axis pad, features batch, padded axis, features searched saw use k.shape inputs , get correct shape runtime, instead none, could make work keras, could anyone help me? another solution solve problem zero dynamic padding, welcome. question also stackoverflow zeropadding dynamic step 1 access actual shape tensor dimension none keras https stackoverflow.com questions 52788133 zeropadding dynamic step 1 access actual shape tensor dimen thank advance attention. thank you! v check date master branch keras. update v check version tensorflow date. installation instructions found https www.tensorflow.org get started os setup . v provide link github gist python script reproduce issue copy script short ."
keras,5326,"provide toy example throws memory exception training vgg19 vgg16 models gpu. batch size dataset used tiny graphics card handle them. use resnet50 architecture instead, get error able use really big datasets batch sizes. training models cpu works fine network architectures. problem first appeared complex pipeline runs p2.16xlarge aws instances. reproduce problem using ubuntu 14.04, keras 1.2.1 tensorflow 0.12.1. anyone reproduce problem? thoughts? shake completeness also uploaded dataset https ufile.io c3fbf . error message using vgg19 successful execution resnet50",0,gpu runs memory vgg19 vgg16 resnet50,"gpu runs memory vgg19 vgg16 resnet50 provide toy example throws memory exception training vgg19 vgg16 models gpu. batch size dataset used tiny graphics card handle them. use resnet50 architecture instead, get error able use really big datasets batch sizes. training models cpu works fine network architectures. problem first appeared complex pipeline runs p2.16xlarge aws instances. reproduce problem using ubuntu 14.04, keras 1.2.1 tensorflow 0.12.1. anyone reproduce problem? thoughts? shake completeness also uploaded dataset https ufile.io c3fbf . error message using vgg19 successful execution resnet50"
keras,11311,"keras 2.2.4 tensorflow 1.11.0 python 3.6.6. trying set test harness lstm rnn. part this, want test effectiveness statefulness. since lstm first layer using statefulness requires using batch input shape attribute rather input shape attribute, attempting define layer first, set correct attributes. run function below, get attributeerror 'lstm' object attribute 'dtype'. traced specifically adding batch input shape attribute.",0,attributeerror attempting set batch input shape using setattr,"attributeerror attempting set batch input shape using setattr keras 2.2.4 tensorflow 1.11.0 python 3.6.6. trying set test harness lstm rnn. part this, want test effectiveness statefulness. since lstm first layer using statefulness requires using batch input shape attribute rather input shape attribute, attempting define layer first, set correct attributes. run function below, get attributeerror 'lstm' object attribute 'dtype'. traced specifically adding batch input shape attribute."
keras,459,"2d array consisting time nsecs , latitude, longitude velocity features input simplernn. shape 33336,4 nb samples, input dim . there's 3d array nb samples, timesteps, input dim needed. course, could simply use numpy.expand dims would something like 33336,1,4 . time features representing timesteps. confused actually create decent 3d array latitude, longitude velocity time feature required timesteps. tried different things like first creating 3d array adding data needed. get memory errors size. guys ideas could create reasonable input?",0,problem sequence input simplernn,"problem sequence input simplernn 2d array consisting time nsecs , latitude, longitude velocity features input simplernn. shape 33336,4 nb samples, input dim . there's 3d array nb samples, timesteps, input dim needed. course, could simply use numpy.expand dims would something like 33336,1,4 . time features representing timesteps. confused actually create decent 3d array latitude, longitude velocity time feature required timesteps. tried different things like first creating 3d array adding data needed. get memory errors size. guys ideas could create reasonable input?"
keras,5425,"hello, thanks advance help developers keras! working lstm networks, actually trying create cnn lstm network takes inputs images 3 channels. reading lot, still several doubts lstm layers really work, results obtaining experiments horrible, networks told give great results. read 4149 2403 clear mind enough know learn lot. first told task enumerate doubts recurrent layers. inputs images 3 channels, reshape data set following code order sequences time gives structures following shapes right, means nb samples 2126 number samples , sample sequence length 2 element sequence image 3 channels dimensions 10x8, right? outputs matrix dimensions , input image associated 3 numbers outputs. want feed net input sequences image sequence image 1 t, want net give output 3 numbers associated image time t. read lot problems sequences 1,t output 1, want output correspond last element input sequence. mind, know net long knew, make sure convolutional layer applayed element sequence separated. added first lstm layer connect second lstm layer. finally dense layer obtain 3 outputs need. comes doubts generals problem 1. network, calculating output last element sequences output future instant 1? 2. data reshape right? 3. really understand parameters lstm recurrent layers read keras documentation clear . moreover, understand difference cases image ! e4cdf91c 063f 11e6 8844 c89a9e134339 https cloud.githubusercontent.com assets 25105487 23065901 1db65978 f518 11e6 8915 2c57668e714e.png understand difference know programm layer obtain different cases. 4. read recommended use instead connect cnn layer lstm layer, me, reshape working. 5. using well layer? read https github.com fchollet keras blob master examples imdb cnn lstm.py url know 1d instead 2d, example use timedistributed layer. think that's moment. sorry big post, hope could help me.",0,"lstm different case sequences, doubts general cnn lstm network regression problem","lstm different case sequences, doubts general cnn lstm network regression problem hello, thanks advance help developers keras! working lstm networks, actually trying create cnn lstm network takes inputs images 3 channels. reading lot, still several doubts lstm layers really work, results obtaining experiments horrible, networks told give great results. read 4149 2403 clear mind enough know learn lot. first told task enumerate doubts recurrent layers. inputs images 3 channels, reshape data set following code order sequences time gives structures following shapes right, means nb samples 2126 number samples , sample sequence length 2 element sequence image 3 channels dimensions 10x8, right? outputs matrix dimensions , input image associated 3 numbers outputs. want feed net input sequences image sequence image 1 t, want net give output 3 numbers associated image time t. read lot problems sequences 1,t output 1, want output correspond last element input sequence. mind, know net long knew, make sure convolutional layer applayed element sequence separated. added first lstm layer connect second lstm layer. finally dense layer obtain 3 outputs need. comes doubts generals problem 1. network, calculating output last element sequences output future instant 1? 2. data reshape right? 3. really understand parameters lstm recurrent layers read keras documentation clear . moreover, understand difference cases image ! e4cdf91c 063f 11e6 8844 c89a9e134339 https cloud.githubusercontent.com assets 25105487 23065901 1db65978 f518 11e6 8915 2c57668e714e.png understand difference know programm layer obtain different cases. 4. read recommended use instead connect cnn layer lstm layer, me, reshape working. 5. using well layer? read https github.com fchollet keras blob master examples imdb cnn lstm.py url know 1d instead 2d, example use timedistributed layer. think that's moment. sorry big post, hope could help me."
keras,3089,"given data x n n data size, input dim , reconstruction autoencoder corresponding representation output encoder h . hope enforce constraints h , loss function x 2 lambda h ch 2 , c pre defined matrix. code follows x input shape 784, , name 'x' h dense 10, activation 'tank' x dense 784, activation 'tanh' h model model input x, output model.compile optimizer 'sgd', loss loss c, h, 0.2 c given matrix, whose shape n n, n size x. model.fit x, x, nb epoch 100, batch size 32 custom loss function def loss c, h, lmd c given, whose shape n n, n size x. global loss k.mean k.square encoded k.dot c, encoded , axis 1 def loss true, pred local loss k.mean k.square true pred , axis 1 return local loss lmd global loss return loss got following errors assertionerror theano assert failed! apply node caused error assert msg 'theano assert failed!' elemwise composite i0 abs i1 i2 i3 0, 2 .0, elemwise eq,no inplace .0 toposort index 46 inputs types tensortype float32, matrix , tensortype int8, scalar inputs shapes 32, 10 , inputs strides 40, 4 , inputs values 'not shown', array 0, dtype int8 outputs clients elemwise sub,no inplace assert msg 'theano assert failed!' .0, inplacedimshuffle x,x .0 hint running theano optimization disabled could give back trace node created. done setting theano flag 'optimizer fast compile'. work, theano optimizations disabled 'optimizer none'. hint use theano flag 'exception verbosity high' debugprint storage map footprint apply node. guess obtained error may result passing fix sized matrix c loss. anybody kindly help solve issue? thank much.",0,passing fixed matrix custom loss,"passing fixed matrix custom loss given data x n n data size, input dim , reconstruction autoencoder corresponding representation output encoder h . hope enforce constraints h , loss function x 2 lambda h ch 2 , c pre defined matrix. code follows x input shape 784, , name 'x' h dense 10, activation 'tank' x dense 784, activation 'tanh' h model model input x, output model.compile optimizer 'sgd', loss loss c, h, 0.2 c given matrix, whose shape n n, n size x. model.fit x, x, nb epoch 100, batch size 32 custom loss function def loss c, h, lmd c given, whose shape n n, n size x. global loss k.mean k.square encoded k.dot c, encoded , axis 1 def loss true, pred local loss k.mean k.square true pred , axis 1 return local loss lmd global loss return loss got following errors assertionerror theano assert failed! apply node caused error assert msg 'theano assert failed!' elemwise composite i0 abs i1 i2 i3 0, 2 .0, elemwise eq,no inplace .0 toposort index 46 inputs types tensortype float32, matrix , tensortype int8, scalar inputs shapes 32, 10 , inputs strides 40, 4 , inputs values 'not shown', array 0, dtype int8 outputs clients elemwise sub,no inplace assert msg 'theano assert failed!' .0, inplacedimshuffle x,x .0 hint running theano optimization disabled could give back trace node created. done setting theano flag 'optimizer fast compile'. work, theano optimizations disabled 'optimizer none'. hint use theano flag 'exception verbosity high' debugprint storage map footprint apply node. guess obtained error may result passing fix sized matrix c loss. anybody kindly help solve issue? thank much."
keras,7614,"developing classification based model predict 12 probability pixel image , built architecture , sure whether right , newbie deep learning. following function baseline architecture explain architecture , input 64 64 1 graysale image , followed many convolution layers flattened two different hidden layer u v , u represents u channel cieluv color space v represents v channel color space, u v channels reshaped 64 64 12 , lemme convert 4096 12 , 4096 pixel 12 corresponding probabilities sum 12 probability 1 happen sum overall matrix 4096 12 1 , make every 12 node sum 1 probability ? thanks advance",0,predicting n probabailities pixel keras,"predicting n probabailities pixel keras developing classification based model predict 12 probability pixel image , built architecture , sure whether right , newbie deep learning. following function baseline architecture explain architecture , input 64 64 1 graysale image , followed many convolution layers flattened two different hidden layer u v , u represents u channel cieluv color space v represents v channel color space, u v channels reshaped 64 64 12 , lemme convert 4096 12 , 4096 pixel 12 corresponding probabilities sum 12 probability 1 happen sum overall matrix 4096 12 1 , make every 12 node sum 1 probability ? thanks advance"
keras,11108,"hi, auto encoder know, two parts, encoder decoder. output encoder part 28x28 image want add another 28x28 image send 28x28x2 filter decoder part learning. want know, possible not? yes, how? please guide completely due beginner. attached code too. know using merge encoded w cv2.merge encoded,w adding w encoder output true not?thanks channels firstchannels firstchannels firstchannels first",0,adding one filter existing filter auto encoder learning,"adding one filter existing filter auto encoder learning hi, auto encoder know, two parts, encoder decoder. output encoder part 28x28 image want add another 28x28 image send 28x28x2 filter decoder part learning. want know, possible not? yes, how? please guide completely due beginner. attached code too. know using merge encoded w cv2.merge encoded,w adding w encoder output true not?thanks channels firstchannels firstchannels firstchannels first"
keras,8657,nan,0,introduction global metrics precision recall,introduction global metrics precision recall nan
keras,7393,"hi, currently experiments dataset classifying text document using embedding, conv1d dense layers. everything ok, running python script obtain following error related native code c c . first time got error, problem due incompatibility among different shapes, compile phase. someone give hint solve problem? thanks",0,error keras running python script,"error keras running python script hi, currently experiments dataset classifying text document using embedding, conv1d dense layers. everything ok, running python script obtain following error related native code c c . first time got error, problem due incompatibility among different shapes, compile phase. someone give hint solve problem? thanks"
keras,4746,"working guided backprop activation maximization. instead implementing rmsprop, adam etc., want reuse optimizers defined keras.",0,use keras optimizer backprop ing loss functions,"use keras optimizer backprop ing loss functions working guided backprop activation maximization. instead implementing rmsprop, adam etc., want reuse optimizers defined keras."
keras,13016,"please make sure bug feature request provide applicable information asked template. issue implementation question , please ask question stackoverflow http stackoverflow.com questions tagged keras keras slack channel https keras slack autojoin.herokuapp.com instead opening github issue. system information written custom code opposed using example directory os platform distribution e.g., linux ubuntu 16.04 tensorflow backend yes tensorflow version keras version python version cuda cudnn version gpu model memory obtain tensorflow version python c import tensorflow tf print tf.git version, tf.version obtain keras version python c 'import keras k print k. version ' describe current behavior describe expected behavior code reproduce issue provide reproducible test case bare minimum necessary generate problem. info logs include logs source code would helpful diagnose problem. including tracebacks, please include full traceback. large logs files attached.",0,something wrong u net,"something wrong u net please make sure bug feature request provide applicable information asked template. issue implementation question , please ask question stackoverflow http stackoverflow.com questions tagged keras keras slack channel https keras slack autojoin.herokuapp.com instead opening github issue. system information written custom code opposed using example directory os platform distribution e.g., linux ubuntu 16.04 tensorflow backend yes tensorflow version keras version python version cuda cudnn version gpu model memory obtain tensorflow version python c import tensorflow tf print tf.git version, tf.version obtain keras version python c 'import keras k print k. version ' describe current behavior describe expected behavior code reproduce issue provide reproducible test case bare minimum necessary generate problem. info logs include logs source code would helpful diagnose problem. including tracebacks, please include full traceback. large logs files attached."
keras,1732,"defined custom objective used optimize auc directly, roc auc score function sklearn need feed numpy array args. true pred tensor variable function gives error theano.gof.fg.missinginputerror 'undeclared input',",0,optimize auc directly,"optimize auc directly defined custom objective used optimize auc directly, roc auc score function sklearn need feed numpy array args. true pred tensor variable function gives error theano.gof.fg.missinginputerror 'undeclared input',"
keras,12897,"please make sure bug feature request provide applicable information asked template. issue implementation question , please ask question stackoverflow http stackoverflow.com questions tagged keras keras slack channel https keras slack autojoin.herokuapp.com instead opening github issue. system information written custom code opposed using example directory used default code exactly written. os platform distribution e.g., linux ubuntu 16.04 microsoft windows 10 enterprise tensorflow backend yes yes tensorflow version 1.13.1 keras version 2.2.4 python version python 3.7.3 cuda cudnn version none. using cpu vanilla keras installed anaconda. gpu model memory nvidia quadra p1000 describe current behavior crashes hit f5 spyder. talks pickle error. describe expected behavior would expect get line 29 print something training sequence count. line 28, choking, get print train sequences attempting load data, getting past line 29. select line 28, hit f9 run highlighted gives error again. code reproduce issue already it. literally copy paste run crash info logs traceback included did.",0,run default code imdb cnn.py gives pickle errors,"run default code imdb cnn.py gives pickle errors please make sure bug feature request provide applicable information asked template. issue implementation question , please ask question stackoverflow http stackoverflow.com questions tagged keras keras slack channel https keras slack autojoin.herokuapp.com instead opening github issue. system information written custom code opposed using example directory used default code exactly written. os platform distribution e.g., linux ubuntu 16.04 microsoft windows 10 enterprise tensorflow backend yes yes tensorflow version 1.13.1 keras version 2.2.4 python version python 3.7.3 cuda cudnn version none. using cpu vanilla keras installed anaconda. gpu model memory nvidia quadra p1000 describe current behavior crashes hit f5 spyder. talks pickle error. describe expected behavior would expect get line 29 print something training sequence count. line 28, choking, get print train sequences attempting load data, getting past line 29. select line 28, hit f9 run highlighted gives error again. code reproduce issue already it. literally copy paste run crash info logs traceback included did."
keras,1990,"anyone else ever machine throw cpu error shut fitting model gpu? happened 10 times, macbook pro geforce gt 750m dell server tesla m2070 q .",0,cpu error causing machine shut,"cpu error causing machine shut anyone else ever machine throw cpu error shut fitting model gpu? happened 10 times, macbook pro geforce gt 750m dell server tesla m2070 q ."
keras,8757,"hello, would like ask support hdf5matrix select data multiple tables time. mainly due constrains width tables hdf. storing waveforms case roughly 1k data points split multiple tables example hdf file data table1 data table2 model hdf5matrix file.hdf5 , data table1 b hdf5matrix file.hdf5 , data table2 a.fit b",0,support multitable input hdf,"support multitable input hdf hello, would like ask support hdf5matrix select data multiple tables time. mainly due constrains width tables hdf. storing waveforms case roughly 1k data points split multiple tables example hdf file data table1 data table2 model hdf5matrix file.hdf5 , data table1 b hdf5matrix file.hdf5 , data table2 a.fit b"
keras,8816,"tried merging input layer stacked cnns worked achive combining input layer lstm, gives dimension mismatch error merge step, match shapes? thanks!",0,merge lstm input layer?,"merge lstm input layer? tried merging input layer stacked cnns worked achive combining input layer lstm, gives dimension mismatch error merge step, match shapes? thanks!"
keras,10875,"hello. task create cetrain amount copies network run separate thread, waiting data given batch time received data make training step. using keras tensorflow gpu. first problem ran simply creating model one thread trying train another handled setting graph session, otherwise getting exception trying work nodes one graph summing nodes another. created separate sessions graphs nodes specifying session.as default graph.as default working, processes running, tf sessions created completely slowing calculations. returned idea one session only, getting via keras.backend.get session using everywhere together graph session. behavior becomes completely random. might run without errors 1 2 workers , might give error initialization depending fast starts threads. network creation update step looks like correct solution using sessions graphs situation?",0,multi thread online training multiple copies model,"multi thread online training multiple copies model hello. task create cetrain amount copies network run separate thread, waiting data given batch time received data make training step. using keras tensorflow gpu. first problem ran simply creating model one thread trying train another handled setting graph session, otherwise getting exception trying work nodes one graph summing nodes another. created separate sessions graphs nodes specifying session.as default graph.as default working, processes running, tf sessions created completely slowing calculations. returned idea one session only, getting via keras.backend.get session using everywhere together graph session. behavior becomes completely random. might run without errors 1 2 workers , might give error initialization depending fast starts threads. network creation update step looks like correct solution using sessions graphs situation?"
keras,7695,"hi folks, using keras tf. passing padded sequences input pad value 1 masking input layer mask value set 1.0. however, collect output bilstm layer, see forward states non zero masked positions. output please make sure boxes checked submit issue. issue implementation question, please ask question stackoverflow http stackoverflow.com questions tagged keras join keras slack channel https keras slack autojoin.herokuapp.com ask instead filing github issue. thank you! x check date master branch keras. update pip install git git github.com fchollet keras.git upgrade deps x running tensorflow, check date latest version. installation instructions found https www.tensorflow.org get started os setup . running theano, check date master branch theano. update pip install git git github.com theano theano.git upgrade deps provide link github gist python script reproduce issue copy script short .",0,forward states bidirectional lstm masked,"forward states bidirectional lstm masked hi folks, using keras tf. passing padded sequences input pad value 1 masking input layer mask value set 1.0. however, collect output bilstm layer, see forward states non zero masked positions. output please make sure boxes checked submit issue. issue implementation question, please ask question stackoverflow http stackoverflow.com questions tagged keras join keras slack channel https keras slack autojoin.herokuapp.com ask instead filing github issue. thank you! x check date master branch keras. update pip install git git github.com fchollet keras.git upgrade deps x running tensorflow, check date latest version. installation instructions found https www.tensorflow.org get started os setup . running theano, check date master branch theano. update pip install git git github.com theano theano.git upgrade deps provide link github gist python script reproduce issue copy script short ."
keras,7987,"image ocr code available example section, possible use code multiple lines? mean possible use code single printed document?",0,possible use image ocr example multiple line image?,"possible use image ocr example multiple line image? image ocr code available example section, possible use code multiple lines? mean possible use code single printed document?"
keras,11539,"confused behavior feed output of, say, embedding layer lstm layer, also specify input shape lstm layer. example code output embedding layer shape none, none, 10 . specify input shape lstm none, 32, 1 , match embedding layer's output. model still compiles without problem. behavior here? override previous input shape, opposite?",0,lstm input shape override input previous layer?,"lstm input shape override input previous layer? confused behavior feed output of, say, embedding layer lstm layer, also specify input shape lstm layer. example code output embedding layer shape none, none, 10 . specify input shape lstm none, 32, 1 , match embedding layer's output. model still compiles without problem. behavior here? override previous input shape, opposite?"
keras,11090,"way quickly change version keras dockerfile ? specifically, using keras docker, add command say specify version cuda. change version keras this? also possible make sure dependencies correct version too? example use keras version 1.2 would nice automatically pick right theano, thus pygpu, etc. version. thanks",0,specify different version keras docker install,"specify different version keras docker install way quickly change version keras dockerfile ? specifically, using keras docker, add command say specify version cuda. change version keras this? also possible make sure dependencies correct version too? example use keras version 1.2 would nice automatically pick right theano, thus pygpu, etc. version. thanks"
keras,8367,"please make sure boxes checked submit issue. issue implementation question, please ask question stackoverflow http stackoverflow.com questions tagged keras join keras slack channel https keras slack autojoin.herokuapp.com ask instead filing github issue. thank you! x check date master branch keras. update pip install git git github.com fchollet keras.git upgrade deps x running tensorflow, check date latest version. installation instructions found https www.tensorflow.org get started os setup . x running theano, check date master branch theano. update pip install git git github.com theano theano.git upgrade deps x provide link github gist python script reproduce issue copy script short .",0,binary cross entropy loss function working python 3.6,"binary cross entropy loss function working python 3.6 please make sure boxes checked submit issue. issue implementation question, please ask question stackoverflow http stackoverflow.com questions tagged keras join keras slack channel https keras slack autojoin.herokuapp.com ask instead filing github issue. thank you! x check date master branch keras. update pip install git git github.com fchollet keras.git upgrade deps x running tensorflow, check date latest version. installation instructions found https www.tensorflow.org get started os setup . x running theano, check date master branch theano. update pip install git git github.com theano theano.git upgrade deps x provide link github gist python script reproduce issue copy script short ."
keras,12679,"came across article https www.fast.ai 2018 04 30 dawnbench fastai imagenet wanted try implementing dynamic image sizes using imagedatagenerator keras applications models global pooling layer . allow model accept input shape progressively increasing input shape training, well training variety image sizes without resize input. however, problems encountered. first, keras.preprocessing.imagedatagenerator accept none none, none target size. target size none, none target size none also, keras applications models seem accept none, none, 3 input shape, seem work nasnet resnext. error get resnext example dynamic input shape may also break models due implementation detail depending input shape. example, keras contrib's implementation resnet, stride value conv2d requires calculation break input shape none, none, 3 would really great imagedatagenerator support none type target size fully convolutional networks image models global pooling require fixed input size. edit realised problem much difficult initially thought since numpy array representing batch images needs fixed size",0,feature request accept none target size imagedatagenerator,"feature request accept none target size imagedatagenerator came across article https www.fast.ai 2018 04 30 dawnbench fastai imagenet wanted try implementing dynamic image sizes using imagedatagenerator keras applications models global pooling layer . allow model accept input shape progressively increasing input shape training, well training variety image sizes without resize input. however, problems encountered. first, keras.preprocessing.imagedatagenerator accept none none, none target size. target size none, none target size none also, keras applications models seem accept none, none, 3 input shape, seem work nasnet resnext. error get resnext example dynamic input shape may also break models due implementation detail depending input shape. example, keras contrib's implementation resnet, stride value conv2d requires calculation break input shape none, none, 3 would really great imagedatagenerator support none type target size fully convolutional networks image models global pooling require fixed input size. edit realised problem much difficult initially thought since numpy array representing batch images needs fixed size"
keras,12240,beam search warns deprecated tf function tf.sparse.sparsetensortf.sparse.to dense solution change . work earlier versions tensorflow assume supported . x check date master branch keras. update x check version tensorflow date. installation instructions found https www.tensorflow.org get started os setup . 1.12.0 x provide link github gist python script reproduce issue copy script short .,0,k.ctc decode beam search tf.sparse dense deprecated,k.ctc decode beam search tf.sparse dense deprecated beam search warns deprecated tf function tf.sparse.sparsetensortf.sparse.to dense solution change . work earlier versions tensorflow assume supported . x check date master branch keras. update x check version tensorflow date. installation instructions found https www.tensorflow.org get started os setup . 1.12.0 x provide link github gist python script reproduce issue copy script short .
keras,11024,"hi, running seq2seq model using gru's 8 gpus, using fit generator error invalidargumenterror incompatible shapes 128,100 vs. 1024,100 node replica 0 model 1 gru 1 add add dt float, class loc train... reshape 1 , device job localhost replica 0 task 0 device gpu 0 replica 0 model 1 gru 1 biasadd, replica 0 model 1 gru 1 matmul 3 node replica 0 model 1 gru 2 identity 967 recv client terminated false, recv device job localhost replica 0 task 0 device cpu 0 , send device job localhost replica 0 task 0 device gpu 0 , send device incarnation 1, tensor name edge 7766 replica 0 model 1 gru 2 identity , tensor type dt int32, device job localhost replica 0 task 0 device cpu 0 cloopreplica 0 model 1 gru 2 tensorarrayreadv3 1 133 keras installed master directly updated latest tensorflow gpu pip. generator designed yield batch 1024 gpu would work batch 128. reduce example number gpus 4, would following error invalidargumenterror incompatible shapes 256,100 vs. 1024,100 going similar issues found following issue 9449 merge request 10845 saying issue fixed. installed keras source still issue. possible issue fixed grus? model definition word input input shape word dim, decoder inputs input shape none, decoder embed embedding input dim num tokens, output dim word dim decoder gru gru word dim, return sequences true, return state true decoder dense dense num tokens, activation 'softmax' embedded decoder embed decoder inputs gru output, state h decoder gru embedded, initial state word input decoder outputs decoder dense gru output model model word input, decoder inputs , decoder outputs rmsprop optimizers.rmsprop lr 0.001 parallel model multi gpu model model, gpus 8 parallel model.compile loss 'categorical crossentropy',optimizer rmsprop,metrics 'acc' filename 'model.h5' checkpoint modelcheckpoint filename, monitor 'loss', verbose 1, save best true, mode 'min' parallel model.fit generator data generator.generate , training size batch size num steps, num epochs, callbacks checkpoint , verbose 1 thanks! please make sure boxes checked submit issue. issue implementation question, please ask question stackoverflow http stackoverflow.com questions tagged keras join keras slack channel https keras slack autojoin.herokuapp.com ask instead filing github issue. thank you! x check date master branch keras. update pip install git git github.com keras team keras.git upgrade deps x running tensorflow, check date latest version. installation instructions found https www.tensorflow.org get started os setup . x running theano, check date master branch theano. update pip install git git github.com theano theano.git upgrade deps x provide link github gist python script reproduce issue copy script short .",0,multi gpu model invalidargumenterror seq2seq model,"multi gpu model invalidargumenterror seq2seq model hi, running seq2seq model using gru's 8 gpus, using fit generator error invalidargumenterror incompatible shapes 128,100 vs. 1024,100 node replica 0 model 1 gru 1 add add dt float, class loc train... reshape 1 , device job localhost replica 0 task 0 device gpu 0 replica 0 model 1 gru 1 biasadd, replica 0 model 1 gru 1 matmul 3 node replica 0 model 1 gru 2 identity 967 recv client terminated false, recv device job localhost replica 0 task 0 device cpu 0 , send device job localhost replica 0 task 0 device gpu 0 , send device incarnation 1, tensor name edge 7766 replica 0 model 1 gru 2 identity , tensor type dt int32, device job localhost replica 0 task 0 device cpu 0 cloopreplica 0 model 1 gru 2 tensorarrayreadv3 1 133 keras installed master directly updated latest tensorflow gpu pip. generator designed yield batch 1024 gpu would work batch 128. reduce example number gpus 4, would following error invalidargumenterror incompatible shapes 256,100 vs. 1024,100 going similar issues found following issue 9449 merge request 10845 saying issue fixed. installed keras source still issue. possible issue fixed grus? model definition word input input shape word dim, decoder inputs input shape none, decoder embed embedding input dim num tokens, output dim word dim decoder gru gru word dim, return sequences true, return state true decoder dense dense num tokens, activation 'softmax' embedded decoder embed decoder inputs gru output, state h decoder gru embedded, initial state word input decoder outputs decoder dense gru output model model word input, decoder inputs , decoder outputs rmsprop optimizers.rmsprop lr 0.001 parallel model multi gpu model model, gpus 8 parallel model.compile loss 'categorical crossentropy',optimizer rmsprop,metrics 'acc' filename 'model.h5' checkpoint modelcheckpoint filename, monitor 'loss', verbose 1, save best true, mode 'min' parallel model.fit generator data generator.generate , training size batch size num steps, num epochs, callbacks checkpoint , verbose 1 thanks! please make sure boxes checked submit issue. issue implementation question, please ask question stackoverflow http stackoverflow.com questions tagged keras join keras slack channel https keras slack autojoin.herokuapp.com ask instead filing github issue. thank you! x check date master branch keras. update pip install git git github.com keras team keras.git upgrade deps x running tensorflow, check date latest version. installation instructions found https www.tensorflow.org get started os setup . x running theano, check date master branch theano. update pip install git git github.com theano theano.git upgrade deps x provide link github gist python script reproduce issue copy script short ."
keras,10786,"hi! way use pre trained weights model takes input shape batch size, 50, 50, 4 initialized another model takes input shape batch size, 50, 50, 1 ? output shape size 50, 50, 1 . way train model takes input shape batch size, 50, 50, 4 test data input shape batch size, 50, 50, 1 ? output shape size 50, 50, 1 . thank you!",0,train test data different channel size,"train test data different channel size hi! way use pre trained weights model takes input shape batch size, 50, 50, 4 initialized another model takes input shape batch size, 50, 50, 1 ? output shape size 50, 50, 1 . way train model takes input shape batch size, 50, 50, 4 test data input shape batch size, 50, 50, 1 ? output shape size 50, 50, 1 . thank you!"
keras,8072,"confuse lstm keras. try map keras code lstm equation. found weights parameter keras code. ! image https user images.githubusercontent.com 10016227 31258172 999520a6 aa67 11e7 9fc4 ffaa6d646994.png think weight current weight cell recurrent weight cell. call function lstm keras code, found statement calculate input ,forget ,cell output term. ! image https user images.githubusercontent.com 10016227 31258223 e7627d42 aa67 11e7 8810 723bd7a05391.png input term lstm equation w ci c 1 can't find keras code. ! image https user images.githubusercontent.com 10016227 31258220 e31a4d32 aa67 11e7 8139 ae91968239ec.png could please anyone explains lstm equation keras code.",0,lstm equation keras,"lstm equation keras confuse lstm keras. try map keras code lstm equation. found weights parameter keras code. ! image https user images.githubusercontent.com 10016227 31258172 999520a6 aa67 11e7 9fc4 ffaa6d646994.png think weight current weight cell recurrent weight cell. call function lstm keras code, found statement calculate input ,forget ,cell output term. ! image https user images.githubusercontent.com 10016227 31258223 e7627d42 aa67 11e7 8810 723bd7a05391.png input term lstm equation w ci c 1 can't find keras code. ! image https user images.githubusercontent.com 10016227 31258220 e31a4d32 aa67 11e7 8139 ae91968239ec.png could please anyone explains lstm equation keras code."
keras,11471,"using custom generator get data paths seems working fine. however, one issue facing using custom generator that, unlike default generators keras, cannot really use attributes. trying example getting report confusion matrix testing set, however, cannot find way input classes generator. script works well default generator, case testgenerator defined custom generator inside custom generator using 2 default generators read 2 streams data. obviously get error question is, way define attributes custom generator another way feed labels confusion matrix without using attribute? new feature added deal kind situation ? tried instead cm confusion matrix np.argmax testgenerator 1 , axis 1 , pred gives supposed yield tuple 1,9,1024 , 1,9,4",0,access define classes attribute custom generator,"access define classes attribute custom generator using custom generator get data paths seems working fine. however, one issue facing using custom generator that, unlike default generators keras, cannot really use attributes. trying example getting report confusion matrix testing set, however, cannot find way input classes generator. script works well default generator, case testgenerator defined custom generator inside custom generator using 2 default generators read 2 streams data. obviously get error question is, way define attributes custom generator another way feed labels confusion matrix without using attribute? new feature added deal kind situation ? tried instead cm confusion matrix np.argmax testgenerator 1 , axis 1 , pred gives supposed yield tuple 1,9,1024 , 1,9,4"
keras,11618,"x check date master branch keras. update x check version tensorflow date. installation instructions found https www.tensorflow.org get started os setup . x provide link github gist python script reproduce issue copy script short . https gist.github.com dsamuylov 1f3d42478be4e277f776783f215816cf code actually long modelmodel 2 inputsmodel finalmodel final also added print statement inside method print , output modelmodel 2 inputsmodel final wrong somewhere bug?",0,failure saving weights model sub model shares weights another model,"failure saving weights model sub model shares weights another model x check date master branch keras. update x check version tensorflow date. installation instructions found https www.tensorflow.org get started os setup . x provide link github gist python script reproduce issue copy script short . https gist.github.com dsamuylov 1f3d42478be4e277f776783f215816cf code actually long modelmodel 2 inputsmodel finalmodel final also added print statement inside method print , output modelmodel 2 inputsmodel final wrong somewhere bug?"
keras,8331,recently started getting errors running training bit confusing happening sure code introduced issue . believe thread related issue using model training inside training data generator . wondering there's way use model training inside generator avoiding issue. full stack trace related https github.com fchollet keras issues 6462 https github.com fchollet keras issues 5511 https github.com fchollet keras issues 2397,0,stopiteration tensor element graph.,stopiteration tensor element graph. recently started getting errors running training bit confusing happening sure code introduced issue . believe thread related issue using model training inside training data generator . wondering there's way use model training inside generator avoiding issue. full stack trace related https github.com fchollet keras issues 6462 https github.com fchollet keras issues 5511 https github.com fchollet keras issues 2397
keras,10284,"boxes checked, latest version keras etc. want run keras production system files stored distributed filesystem, quite limiting pass filename string modelcheckpoint model.save. add option use file object instead passing filename? would uesful cases want save checkpoints cloud storage, want handle memory pass stringio object .",0,support file objects modelcheckpoint model.save,"support file objects modelcheckpoint model.save boxes checked, latest version keras etc. want run keras production system files stored distributed filesystem, quite limiting pass filename string modelcheckpoint model.save. add option use file object instead passing filename? would uesful cases want save checkpoints cloud storage, want handle memory pass stringio object ."
keras,13335,"system information written custom code find os platform distribution e.g., linux ubuntu 16.04 windows 10 tensorflow backend yes yes tensorflow version 1.8.0 keras version 2.1.5 python version 3.6 use sess.run obtain output model first layer pretrained vgg19 , obtain different output one obtained using predict function model. looking inference output. training all. weights freeze trainable. moreover time, run tf.global variables initializer, obtain different result whereas stochastic element code. find minimal code above.",0,side effect tf.global variables initializer evaluation output model,"side effect tf.global variables initializer evaluation output model system information written custom code find os platform distribution e.g., linux ubuntu 16.04 windows 10 tensorflow backend yes yes tensorflow version 1.8.0 keras version 2.1.5 python version 3.6 use sess.run obtain output model first layer pretrained vgg19 , obtain different output one obtained using predict function model. looking inference output. training all. weights freeze trainable. moreover time, run tf.global variables initializer, obtain different result whereas stochastic element code. find minimal code above."
keras,13562,"want limit cpu usage keras training program. found kaggle document https www.kaggle.com c porto seguro safe driver prediction discussion 43383 tensorflow 2.0, configproto session deprecated.",0,set cpu core number tensorflow 2.0,"set cpu core number tensorflow 2.0 want limit cpu usage keras training program. found kaggle document https www.kaggle.com c porto seguro safe driver prediction discussion 43383 tensorflow 2.0, configproto session deprecated."
keras,9407,"building objective function def loss true, pred true tf.reshape true, 1,256 256,14 shape num images, pixels, channels sum k.sum true,axis 1 indices k.tf.where k.equal sum, 0 get indices sum true 0 image channel true ind pred ind return keras.losses.binary crossentropy true, pred able figure assign true values pred. basically cannot help would much appreciated.",0,replacing tensor values specific indices,"replacing tensor values specific indices building objective function def loss true, pred true tf.reshape true, 1,256 256,14 shape num images, pixels, channels sum k.sum true,axis 1 indices k.tf.where k.equal sum, 0 get indices sum true 0 image channel true ind pred ind return keras.losses.binary crossentropy true, pred able figure assign true values pred. basically cannot help would much appreciated."
keras,10538,"hi guys, using autoencoder, batch 128 samples. could possible send data gpu execute batch faster? think bottleneck copy, see dedicated gpu memory changing 8gb 0gb windows, many many times, gpu smi stay 30 memory copy 5 idea something like multi gpu model.fit generator dae generator , steps per epoch ceil len df batch size 2 , cache batchs 100, example send 100x data epochs 1000, verbose 2 using tensorflow backend",0,fit generator gpu cache,"fit generator gpu cache hi guys, using autoencoder, batch 128 samples. could possible send data gpu execute batch faster? think bottleneck copy, see dedicated gpu memory changing 8gb 0gb windows, many many times, gpu smi stay 30 memory copy 5 idea something like multi gpu model.fit generator dae generator , steps per epoch ceil len df batch size 2 , cache batchs 100, example send 100x data epochs 1000, verbose 2 using tensorflow backend"
keras,9566,"hi, dear all, know 2.0.8 little bit old, however, observe issue 2.0.8 compared 2.0.4. fixed latest version, not, hope give information next updates. run code 2.0.8, pop error message like traceback recent call last file home fxt120230 study msp research visvad samsung vad fusion part ete vad vadete run.py , line 316, run ete exp para dict file home fxt120230 study msp research visvad samsung vad fusion part ete vad vadete run.py , line 112, run ete exp validation data aud valid,vid valid ,valid truth mem,valid weight ,sample weight train weight,verbose verbose file build bdist.linux x86 64 egg keras engine training.py , line 1601, fit file build bdist.linux x86 64 egg keras engine training.py , line 1183, fit loop file build bdist.linux x86 64 egg keras backend theano backend.py , line 1223, call file usr local lib python2.7 dist packages theano compile function module.py , line 917, call storage map getattr self.fn, 'storage map', none file usr local lib python2.7 dist packages theano gof link.py , line 325, raise op reraise exc type, exc value, exc trace file usr local lib python2.7 dist packages theano compile function module.py , line 903, call self.fn output subset none else file pygpu gpuarray.pyx , line 693, pygpu.gpuarray.pygpu empty pygpu gpuarray.c 9893 file pygpu gpuarray.pyx , line 301, pygpu.gpuarray.array empty pygpu gpuarray.c 5694 pygpu.gpuarray.gpuarrayexception cumemalloc cuda error memory memory apply node caused error gpuallocempty dtype 'int32', context name none elemwise composite switch lt maximum i0, i1 , i2 , maximum i0, i1 i3 , maximum i0, i1 i4 i5 .0, shape 0 .0, shape 1 .0 toposort index 1656 inputs types tensortype int64, scalar , tensortype int64, scalar , tensortype int64, scalar inputs shapes , , inputs strides , , inputs values array 2722 , array 15360 , array 6 outputs clients gpuincsubtensor inplaceset int64 gpuallocempty dtype 'int32', context name none .0, rebroadcast 0 .0, constant 1 hint running theano optimization disabled could give back trace node created. done setting theano flag 'optimizer fast compile'. work, theano optimizations disabled 'optimizer none'. hint use theano flag 'exception verbosity high' debugprint storage map footprint apply node. nework parameters listed following table layer type output shape param connected input 1 inputlayer none, 2721, 5 0 input 2 inputlayer none, 2721, 23 0 masking 1 masking none, 2721, 5 0 input 1 0 0 masking 2 masking none, 2721, 23 0 input 2 0 0 time distributed 1 timedistrib none, 2721, 16 288 masking 1 0 0 time distributed 3 timedistrib none, 2721, 64 4608 masking 2 0 0 dropout 1 dropout none, 2721, 16 0 time distributed 1 0 0 dropout 4 dropout none, 2721, 64 0 time distributed 3 0 0 time distributed 2 timedistrib none, 2721, 16 816 dropout 1 0 0 time distributed 4 timedistrib none, 2721, 64 12480 dropout 4 0 0 dropout 2 dropout none, 2721, 16 0 time distributed 2 0 0 dropout 5 dropout none, 2721, 64 0 time distributed 4 0 0 lstm 1 lstm none, 2721, 16 2112 dropout 2 0 0 lstm 3 lstm none, 2721, 64 33024 dropout 5 0 0 dropout 3 dropout none, 2721, 16 0 lstm 1 0 0 dropout 6 dropout none, 2721, 64 0 lstm 3 0 0 bidirectional 1 bidirectional none, 2721, 32 4224 dropout 3 0 0 bidirectional 2 bidirectional none, 2721, 128 66048 dropout 6 0 0 concatenate 1 concatenate none, 2721, 160 0 bidirectional 1 0 0 bidirectional 2 0 0 dropout 7 dropout none, 2721, 160 0 concatenate 1 0 0 bidirectional 3 bidirectional none, 2721, 256 295936 dropout 7 0 0 dropout 8 dropout none, 2721, 256 0 bidirectional 3 0 0 bidirectional 4 bidirectional none, 2721, 256 394240 dropout 8 0 0 dropout 9 dropout none, 2721, 256 0 bidirectional 4 0 0 time distributed 5 timedistrib none, 2721, 128 98688 dropout 9 0 0 dropout 10 dropout none, 2721, 128 0 time distributed 5 0 0 time distributed 6 timedistrib none, 2721, 2 258 dropout 10 0 0 activation 1 activation none, 2721, 2 0 time distributed 6 0 0 total params 912,722 trainable params 912,722 non trainable params 0 using theano 1.0.1. gpu setting printed using theano backend. using cudnn version 5110 context none mapped name none device cuda geforce gtx 1070 0000 04 00.0 switch back 2.0.4 without change, program run successfully. could anyone tell reason fix it? thank much!",0,keras 2.0.8 model.fit memory,"keras 2.0.8 model.fit memory hi, dear all, know 2.0.8 little bit old, however, observe issue 2.0.8 compared 2.0.4. fixed latest version, not, hope give information next updates. run code 2.0.8, pop error message like traceback recent call last file home fxt120230 study msp research visvad samsung vad fusion part ete vad vadete run.py , line 316, run ete exp para dict file home fxt120230 study msp research visvad samsung vad fusion part ete vad vadete run.py , line 112, run ete exp validation data aud valid,vid valid ,valid truth mem,valid weight ,sample weight train weight,verbose verbose file build bdist.linux x86 64 egg keras engine training.py , line 1601, fit file build bdist.linux x86 64 egg keras engine training.py , line 1183, fit loop file build bdist.linux x86 64 egg keras backend theano backend.py , line 1223, call file usr local lib python2.7 dist packages theano compile function module.py , line 917, call storage map getattr self.fn, 'storage map', none file usr local lib python2.7 dist packages theano gof link.py , line 325, raise op reraise exc type, exc value, exc trace file usr local lib python2.7 dist packages theano compile function module.py , line 903, call self.fn output subset none else file pygpu gpuarray.pyx , line 693, pygpu.gpuarray.pygpu empty pygpu gpuarray.c 9893 file pygpu gpuarray.pyx , line 301, pygpu.gpuarray.array empty pygpu gpuarray.c 5694 pygpu.gpuarray.gpuarrayexception cumemalloc cuda error memory memory apply node caused error gpuallocempty dtype 'int32', context name none elemwise composite switch lt maximum i0, i1 , i2 , maximum i0, i1 i3 , maximum i0, i1 i4 i5 .0, shape 0 .0, shape 1 .0 toposort index 1656 inputs types tensortype int64, scalar , tensortype int64, scalar , tensortype int64, scalar inputs shapes , , inputs strides , , inputs values array 2722 , array 15360 , array 6 outputs clients gpuincsubtensor inplaceset int64 gpuallocempty dtype 'int32', context name none .0, rebroadcast 0 .0, constant 1 hint running theano optimization disabled could give back trace node created. done setting theano flag 'optimizer fast compile'. work, theano optimizations disabled 'optimizer none'. hint use theano flag 'exception verbosity high' debugprint storage map footprint apply node. nework parameters listed following table layer type output shape param connected input 1 inputlayer none, 2721, 5 0 input 2 inputlayer none, 2721, 23 0 masking 1 masking none, 2721, 5 0 input 1 0 0 masking 2 masking none, 2721, 23 0 input 2 0 0 time distributed 1 timedistrib none, 2721, 16 288 masking 1 0 0 time distributed 3 timedistrib none, 2721, 64 4608 masking 2 0 0 dropout 1 dropout none, 2721, 16 0 time distributed 1 0 0 dropout 4 dropout none, 2721, 64 0 time distributed 3 0 0 time distributed 2 timedistrib none, 2721, 16 816 dropout 1 0 0 time distributed 4 timedistrib none, 2721, 64 12480 dropout 4 0 0 dropout 2 dropout none, 2721, 16 0 time distributed 2 0 0 dropout 5 dropout none, 2721, 64 0 time distributed 4 0 0 lstm 1 lstm none, 2721, 16 2112 dropout 2 0 0 lstm 3 lstm none, 2721, 64 33024 dropout 5 0 0 dropout 3 dropout none, 2721, 16 0 lstm 1 0 0 dropout 6 dropout none, 2721, 64 0 lstm 3 0 0 bidirectional 1 bidirectional none, 2721, 32 4224 dropout 3 0 0 bidirectional 2 bidirectional none, 2721, 128 66048 dropout 6 0 0 concatenate 1 concatenate none, 2721, 160 0 bidirectional 1 0 0 bidirectional 2 0 0 dropout 7 dropout none, 2721, 160 0 concatenate 1 0 0 bidirectional 3 bidirectional none, 2721, 256 295936 dropout 7 0 0 dropout 8 dropout none, 2721, 256 0 bidirectional 3 0 0 bidirectional 4 bidirectional none, 2721, 256 394240 dropout 8 0 0 dropout 9 dropout none, 2721, 256 0 bidirectional 4 0 0 time distributed 5 timedistrib none, 2721, 128 98688 dropout 9 0 0 dropout 10 dropout none, 2721, 128 0 time distributed 5 0 0 time distributed 6 timedistrib none, 2721, 2 258 dropout 10 0 0 activation 1 activation none, 2721, 2 0 time distributed 6 0 0 total params 912,722 trainable params 912,722 non trainable params 0 using theano 1.0.1. gpu setting printed using theano backend. using cudnn version 5110 context none mapped name none device cuda geforce gtx 1070 0000 04 00.0 switch back 2.0.4 without change, program run successfully. could anyone tell reason fix it? thank much!"
keras,8546,"sure fix error. please guide? found code flyyufelix https github.com flyyufelix cnn finetune blob master resnet 50.py get error conv2dconv2d 512, 3, 3 , name res5b branch2b , padding conv2dconv2d 2048, 1, 1 , name res5b branch2c conv2dconv2d 512, 1, 1 , name res5c branch2a conv2dconv2d 512, 3, 3 , name res5c branch2b , padding conv2dconv2d 2048, 1, 1 , name res5c branch2c conv2dconv2d 64, 7, 7 , name conv1 , strides 2, 2 conv2dconv2d 64, 1, 1 , name res2a branch2a , strides 1, 1 conv2dconv2d 64, 3, 3 , name res2a branch2b , padding conv2dconv2d 256, 1, 1 , name res2a branch2c conv2dconv2d 256, 1, 1 , name res2a branch1 , strides 1, 1 mergekeras.layers.mergeaddconcatenatemergekeras.layers.mergeaddconcatenateconv2dconv2d 64, 1, 1 , name res2b branch2a conv2dconv2d 64, 3, 3 , name res2b branch2b , padding conv2dconv2d 256, 1, 1 , name res2b branch2c mergekeras.layers.mergeaddconcatenateconv2dconv2d 64, 1, 1 , name res2c branch2a conv2dconv2d 64, 3, 3 , name res2c branch2b , padding conv2dconv2d 256, 1, 1 , name res2c branch2c conv2dconv2d 128, 1, 1 , name res3a branch2a , strides 2, 2 conv2dconv2d 128, 3, 3 , name res3a branch2b , padding conv2dconv2d 512, 1, 1 , name res3a branch2c conv2dconv2d 512, 1, 1 , name res3a branch1 , strides 2, 2 conv2dconv2d 128, 1, 1 , name res3b branch2a conv2dconv2d 128, 3, 3 , name res3b branch2b , padding conv2dconv2d 512, 1, 1 , name res3b branch2c conv2dconv2d 128, 1, 1 , name res3c branch2a conv2dconv2d 128, 3, 3 , name res3c branch2b , padding conv2dconv2d 512, 1, 1 , name res3c branch2c conv2dconv2d 128, 1, 1 , name res3d branch2a conv2dconv2d 128, 3, 3 , name res3d branch2b , padding conv2dconv2d 512, 1, 1 , name res3d branch2c conv2dconv2d 256, 1, 1 , name res4a branch2a , strides 2, 2 conv2dconv2d 256, 3, 3 , name res4a branch2b , padding conv2dconv2d 1024, 1, 1 , name res4a branch2c conv2dconv2d 1024, 1, 1 , name res4a branch1 , strides 2, 2 conv2dconv2d 256, 1, 1 , name res4b branch2a conv2dconv2d 256, 3, 3 , name res4b branch2b , padding conv2dconv2d 1024, 1, 1 , name res4b branch2c conv2dconv2d 256, 1, 1 , name res4c branch2a conv2dconv2d 256, 3, 3 , name res4c branch2b , padding conv2dconv2d 1024, 1, 1 , name res4c branch2c conv2dconv2d 256, 1, 1 , name res4d branch2a conv2dconv2d 256, 3, 3 , name res4d branch2b , padding conv2dconv2d 1024, 1, 1 , name res4d branch2c conv2dconv2d 256, 1, 1 , name res4e branch2a conv2dconv2d 256, 3, 3 , name res4e branch2b , padding conv2dconv2d 1024, 1, 1 , name res4e branch2c conv2dconv2d 256, 1, 1 , name res4f branch2a conv2dconv2d 256, 3, 3 , name res4f branch2b , padding conv2dconv2d 1024, 1, 1 , name res4f branch2c conv2dconv2d 512, 1, 1 , name res5a branch2a , strides 2, 2 conv2dconv2d 512, 3, 3 , name res5a branch2b , padding conv2dconv2d 2048, 1, 1 , name res5a branch2c conv2dconv2d 2048, 1, 1 , name res5a branch1 , strides 2, 2 conv2dconv2d 512, 1, 1 , name res5b branch2a conv2dconv2d 512, 3, 3 , name res5b branch2b , padding conv2dconv2d 2048, 1, 1 , name res5b branch2c conv2dconv2d 512, 1, 1 , name res5c branch2a conv2dconv2d 512, 3, 3 , name res5c branch2b , padding conv2dconv2d 2048, 1, 1 , name res5c branch2c",0,"oserror unable open file truncated file eof 98304, sblock base addr 0, stored eoa 102853048 removing model keras model solve","oserror unable open file truncated file eof 98304, sblock base addr 0, stored eoa 102853048 removing model keras model solve sure fix error. please guide? found code flyyufelix https github.com flyyufelix cnn finetune blob master resnet 50.py get error conv2dconv2d 512, 3, 3 , name res5b branch2b , padding conv2dconv2d 2048, 1, 1 , name res5b branch2c conv2dconv2d 512, 1, 1 , name res5c branch2a conv2dconv2d 512, 3, 3 , name res5c branch2b , padding conv2dconv2d 2048, 1, 1 , name res5c branch2c conv2dconv2d 64, 7, 7 , name conv1 , strides 2, 2 conv2dconv2d 64, 1, 1 , name res2a branch2a , strides 1, 1 conv2dconv2d 64, 3, 3 , name res2a branch2b , padding conv2dconv2d 256, 1, 1 , name res2a branch2c conv2dconv2d 256, 1, 1 , name res2a branch1 , strides 1, 1 mergekeras.layers.mergeaddconcatenatemergekeras.layers.mergeaddconcatenateconv2dconv2d 64, 1, 1 , name res2b branch2a conv2dconv2d 64, 3, 3 , name res2b branch2b , padding conv2dconv2d 256, 1, 1 , name res2b branch2c mergekeras.layers.mergeaddconcatenateconv2dconv2d 64, 1, 1 , name res2c branch2a conv2dconv2d 64, 3, 3 , name res2c branch2b , padding conv2dconv2d 256, 1, 1 , name res2c branch2c conv2dconv2d 128, 1, 1 , name res3a branch2a , strides 2, 2 conv2dconv2d 128, 3, 3 , name res3a branch2b , padding conv2dconv2d 512, 1, 1 , name res3a branch2c conv2dconv2d 512, 1, 1 , name res3a branch1 , strides 2, 2 conv2dconv2d 128, 1, 1 , name res3b branch2a conv2dconv2d 128, 3, 3 , name res3b branch2b , padding conv2dconv2d 512, 1, 1 , name res3b branch2c conv2dconv2d 128, 1, 1 , name res3c branch2a conv2dconv2d 128, 3, 3 , name res3c branch2b , padding conv2dconv2d 512, 1, 1 , name res3c branch2c conv2dconv2d 128, 1, 1 , name res3d branch2a conv2dconv2d 128, 3, 3 , name res3d branch2b , padding conv2dconv2d 512, 1, 1 , name res3d branch2c conv2dconv2d 256, 1, 1 , name res4a branch2a , strides 2, 2 conv2dconv2d 256, 3, 3 , name res4a branch2b , padding conv2dconv2d 1024, 1, 1 , name res4a branch2c conv2dconv2d 1024, 1, 1 , name res4a branch1 , strides 2, 2 conv2dconv2d 256, 1, 1 , name res4b branch2a conv2dconv2d 256, 3, 3 , name res4b branch2b , padding conv2dconv2d 1024, 1, 1 , name res4b branch2c conv2dconv2d 256, 1, 1 , name res4c branch2a conv2dconv2d 256, 3, 3 , name res4c branch2b , padding conv2dconv2d 1024, 1, 1 , name res4c branch2c conv2dconv2d 256, 1, 1 , name res4d branch2a conv2dconv2d 256, 3, 3 , name res4d branch2b , padding conv2dconv2d 1024, 1, 1 , name res4d branch2c conv2dconv2d 256, 1, 1 , name res4e branch2a conv2dconv2d 256, 3, 3 , name res4e branch2b , padding conv2dconv2d 1024, 1, 1 , name res4e branch2c conv2dconv2d 256, 1, 1 , name res4f branch2a conv2dconv2d 256, 3, 3 , name res4f branch2b , padding conv2dconv2d 1024, 1, 1 , name res4f branch2c conv2dconv2d 512, 1, 1 , name res5a branch2a , strides 2, 2 conv2dconv2d 512, 3, 3 , name res5a branch2b , padding conv2dconv2d 2048, 1, 1 , name res5a branch2c conv2dconv2d 2048, 1, 1 , name res5a branch1 , strides 2, 2 conv2dconv2d 512, 1, 1 , name res5b branch2a conv2dconv2d 512, 3, 3 , name res5b branch2b , padding conv2dconv2d 2048, 1, 1 , name res5b branch2c conv2dconv2d 512, 1, 1 , name res5c branch2a conv2dconv2d 512, 3, 3 , name res5c branch2b , padding conv2dconv2d 2048, 1, 1 , name res5c branch2c"
keras,8450,"built project using merge, merge layer once, use share layer another. problem can't find documentation explains merge share layer works like convolution max pooling layers. grateful anyone direct suggest paper helps understand layers work ???",0,merge layer documentation architecture,"merge layer documentation architecture built project using merge, merge layer once, use share layer another. problem can't find documentation explains merge share layer works like convolution max pooling layers. grateful anyone direct suggest paper helps understand layers work ???"
keras,10821,"thinking extra robustness one gets training randomizing data training set goes mini batches, leaving percentage fit. believe may possible present setting manually entering less actual number used. however, think useful enough feature could deserve option. perhaps could call automatically sets , checks sure mini batches used, either correctly sets , ignores thae last mini batches reshuffle adding couple lines extra code fitting routine.",0,could add option model.fit keep n mini batches per epoch robust training?,"could add option model.fit keep n mini batches per epoch robust training? thinking extra robustness one gets training randomizing data training set goes mini batches, leaving percentage fit. believe may possible present setting manually entering less actual number used. however, think useful enough feature could deserve option. perhaps could call automatically sets , checks sure mini batches used, either correctly sets , ignores thae last mini batches reshuffle adding couple lines extra code fitting routine."
keras,6368,"trying adopt drop based learning rate decay strategy categorical data classification task. everything goes fine pass callback kerasclassifier wrapper. drop based learning rate decay import numpy pandas import read csv keras.models import sequential keras.layers import dense keras.wrappers.scikit learn import kerasclassifier sklearn.model selection import cross val score sklearn.preprocessing import labelencoder sklearn.model selection import stratifiedkfold sklearn.preprocessing import minmaxscaler sklearn.preprocessing import standardscaler sklearn.pipeline import pipeline keras.utils import np utils pandas import read csv import numpy import math sklearn.preprocessing import labelencoder keras.callbacks import learningratescheduler learning rate schedule def step decay epoch initial lrate 0.2 drop 0.5 epochs drop 10.0 lrate initial lrate math.pow drop, math.floor 1 epoch epochs drop return lrate fix random seed reproducibility seed 7 numpy.random.seed seed dataframe read csv book1.csv , header none dataset dataframe.values x dataset ,0 15 .astype float dataset ,15 one hot encoding creating dummy variables categorical variable class encode class values integers encoder labelencoder encoder.fit encoded encoder.transform convert integers dummy variables i.e. one hot encoded dummy np utils.to categorical encoded create model def baseline model create model model sequential model.add dense 50, input dim 15, kernel initializer 'normal', activation 'relu' model.add dense 3, kernel initializer 'normal', activation 'sigmoid' sgd sgd lr 0.0, momentum 0.9, decay 0, nesterov false model.compile loss 'categorical crossentropy', optimizer sgd, metrics 'accuracy' return model learning schedule callback lrate learningratescheduler step decay callbacks list lrate estimators estimators.append 'minmaxscale', minmaxscaler estimators.append 'standardize', standardscaler estimators.append 'mlp', kerasclassifier build fn baseline model, epochs 100, batch size 5, callbacks lrate , verbose 1 pipeline pipeline estimators kfold stratifiedkfold n splits 2, shuffle true, random state seed results cross val score pipeline, x, encoded y, cv kfold print standardized .2f .2f results.mean 100, results.std 100 run code, exactly passing callback kerasclassifier wrapper estimators.append 'mlp', kerasclassifier build fn baseline model, epochs 100, batch size 5, callbacks lrate , verbose 1 , get error cannot clone object , constructor seem set parameter callbacks would like get code corrected right way passing callbacks wrapper. thanks advance.",0,issue passing callback keras classifier wrapper,"issue passing callback keras classifier wrapper trying adopt drop based learning rate decay strategy categorical data classification task. everything goes fine pass callback kerasclassifier wrapper. drop based learning rate decay import numpy pandas import read csv keras.models import sequential keras.layers import dense keras.wrappers.scikit learn import kerasclassifier sklearn.model selection import cross val score sklearn.preprocessing import labelencoder sklearn.model selection import stratifiedkfold sklearn.preprocessing import minmaxscaler sklearn.preprocessing import standardscaler sklearn.pipeline import pipeline keras.utils import np utils pandas import read csv import numpy import math sklearn.preprocessing import labelencoder keras.callbacks import learningratescheduler learning rate schedule def step decay epoch initial lrate 0.2 drop 0.5 epochs drop 10.0 lrate initial lrate math.pow drop, math.floor 1 epoch epochs drop return lrate fix random seed reproducibility seed 7 numpy.random.seed seed dataframe read csv book1.csv , header none dataset dataframe.values x dataset ,0 15 .astype float dataset ,15 one hot encoding creating dummy variables categorical variable class encode class values integers encoder labelencoder encoder.fit encoded encoder.transform convert integers dummy variables i.e. one hot encoded dummy np utils.to categorical encoded create model def baseline model create model model sequential model.add dense 50, input dim 15, kernel initializer 'normal', activation 'relu' model.add dense 3, kernel initializer 'normal', activation 'sigmoid' sgd sgd lr 0.0, momentum 0.9, decay 0, nesterov false model.compile loss 'categorical crossentropy', optimizer sgd, metrics 'accuracy' return model learning schedule callback lrate learningratescheduler step decay callbacks list lrate estimators estimators.append 'minmaxscale', minmaxscaler estimators.append 'standardize', standardscaler estimators.append 'mlp', kerasclassifier build fn baseline model, epochs 100, batch size 5, callbacks lrate , verbose 1 pipeline pipeline estimators kfold stratifiedkfold n splits 2, shuffle true, random state seed results cross val score pipeline, x, encoded y, cv kfold print standardized .2f .2f results.mean 100, results.std 100 run code, exactly passing callback kerasclassifier wrapper estimators.append 'mlp', kerasclassifier build fn baseline model, epochs 100, batch size 5, callbacks lrate , verbose 1 , get error cannot clone object , constructor seem set parameter callbacks would like get code corrected right way passing callbacks wrapper. thanks advance."
keras,12585,"please make sure boxes checked submit issue. issue implementation question , please ask question stackoverflow http stackoverflow.com questions tagged keras keras slack channel https keras slack autojoin.herokuapp.com instead opening github issue. thank you! x check date master branch keras. update x check version tensorflow date. installation instructions found https www.tensorflow.org get started os setup . provide link github gist python script reproduce issue copy script short .",0,tensorboard attributeerror 'model' object attribute ' eval function',"tensorboard attributeerror 'model' object attribute ' eval function' please make sure boxes checked submit issue. issue implementation question , please ask question stackoverflow http stackoverflow.com questions tagged keras keras slack channel https keras slack autojoin.herokuapp.com instead opening github issue. thank you! x check date master branch keras. update x check version tensorflow date. installation instructions found https www.tensorflow.org get started os setup . provide link github gist python script reproduce issue copy script short ."
keras,13522,"want perform hyperparameter optimization keras model. problem dataset quite big, normally training use fit generator load data batch disk, common package like sklearn gridsearch, etc. support fit method. use gridsearchcv implement fit generator method? possible use keras's scikit learn api together fit generator method? thank you.",0,use kerasclassifier fit generator method?,"use kerasclassifier fit generator method? want perform hyperparameter optimization keras model. problem dataset quite big, normally training use fit generator load data batch disk, common package like sklearn gridsearch, etc. support fit method. use gridsearchcv implement fit generator method? possible use keras's scikit learn api together fit generator method? thank you."
keras,11416,"hi, working heteroscedactic neural network. want get two neural networks loss function nn depends another nn. short example way so? either define correct way loss functions define another way nn? thank's help charles",0,two models cross loss function,"two models cross loss function hi, working heteroscedactic neural network. want get two neural networks loss function nn depends another nn. short example way so? either define correct way loss functions define another way nn? thank's help charles"
keras,9758,"noticed imagedatagenerator, setting parameter zoom range, uses different random transformation horizontal vertical axis, produces images random aspect ratio. checked source code https github.com keras team keras blob master keras preprocessing image.py l853 zoom transformation indeed random horizontal vertical axis. think counterintuitive expecting zoom transformation simply magnify unmagnify input image. instead, output zoom range images randomly stretched horizontally vertically. suggest introducing additional parameter like boolean zoom keep ar aspect ratio true, keeps aspect ratio constant simple magnification, zx zy random , false also randomly change aspect ratio zx rand, zy rand .",0,zoom range imagedatagenerator uses different scales x,"zoom range imagedatagenerator uses different scales x noticed imagedatagenerator, setting parameter zoom range, uses different random transformation horizontal vertical axis, produces images random aspect ratio. checked source code https github.com keras team keras blob master keras preprocessing image.py l853 zoom transformation indeed random horizontal vertical axis. think counterintuitive expecting zoom transformation simply magnify unmagnify input image. instead, output zoom range images randomly stretched horizontally vertically. suggest introducing additional parameter like boolean zoom keep ar aspect ratio true, keeps aspect ratio constant simple magnification, zx zy random , false also randomly change aspect ratio zx rand, zy rand ."
keras,10890,"propose dice score loss also known f1 score sorensen score added metric loss function commonly used image segmentation bounding box problems. within medical community, incredibly important function, although seen areas like astronomy https github.com jakeret tf unet . additionally, intersection union iou also known jaccard index another important metric loss classes problem. dice iou similar functions, dice score weights true positives intersection heavily false positives false negatives iou gives even weighting tp, fp, fn . cases false positives false negatives detrimental, iou produce better result dice. andrew ng stanford prof, google brain co founder, coursera founder even devotes entire video https www.coursera.org lecture convolutional neural networks intersection union p9gxz iou convolutional neural networks cnn's course metric use determine bounding box predictions working. according pull 7032 sought add dice, end result request closed added community continues bring express interest adding dice see comment https github.com keras team keras pull 7032 issuecomment 311459497 fchollet . time passed interest cnn's skyrocketed https trends.google.com trends explore?cat 174 date geo us q convolutional 20neural 20network , suggest reconsider adding dice iou becoming common place. dice proposed mentioned issues pull requests multiple times 292, 369, 2115, 2994, 3442, 3457, 3611, 3653, 3977, 5916, 6933, 7032, 8961, 9154, 9275, 9395, 9444, 9671, 10783 . additionally, iou also mentioned number times repository 2016, 2185, 6467, 6538, 8225, 8643, 8669, 9367, 10104, 10602, 10783 . keep mind references repository... plenty github repositories use dice iou loss function. furthermore, research community used dice iou numerous papers make use cnn's. 100 citations according google scholar, though many exist... 1 https ieeexplore.ieee.org abstract document 7785132 , 2 https ieeexplore.ieee.org abstract document 7444155 , 3 http papers.nips.cc paper 5207 deep neural networks object detection , 4 https www.cv foundation.org openaccess content cvpr 2014 papers erhan scalable object detection 2014 cvpr paper.pdf , 5 https www.cv foundation.org openaccess content cvpr workshops 2015 w01 html brebisson deep neural networks 2015 cvpr paper.html , 6 https www.sciencedirect.com science article pii s1053811914010660 , 7 https link.springer.com chapter 10.1007 978 3 319 46723 8 48 , 8 https www.computer.org csdl proceedings icip 1994 6952 02 00413580.pdf , 9 https link.springer.com article 10.1007 s11548 016 1501 5 , 10 https ieeexplore.ieee.org abstract document 7482843 sorry ahead time behind paywall . personally working medical research u net image segmentation found training model binary cross entropy loss function first switching dice loss additional training significantly improved performance using binary cross entropy. using custom loss use dice loss, however, would great see official version supported keras. given year past since pr 7032, would keras team reconsider implementing official version dice iou loss functions?",0,add dice loss intersection union,"add dice loss intersection union propose dice score loss also known f1 score sorensen score added metric loss function commonly used image segmentation bounding box problems. within medical community, incredibly important function, although seen areas like astronomy https github.com jakeret tf unet . additionally, intersection union iou also known jaccard index another important metric loss classes problem. dice iou similar functions, dice score weights true positives intersection heavily false positives false negatives iou gives even weighting tp, fp, fn . cases false positives false negatives detrimental, iou produce better result dice. andrew ng stanford prof, google brain co founder, coursera founder even devotes entire video https www.coursera.org lecture convolutional neural networks intersection union p9gxz iou convolutional neural networks cnn's course metric use determine bounding box predictions working. according pull 7032 sought add dice, end result request closed added community continues bring express interest adding dice see comment https github.com keras team keras pull 7032 issuecomment 311459497 fchollet . time passed interest cnn's skyrocketed https trends.google.com trends explore?cat 174 date geo us q convolutional 20neural 20network , suggest reconsider adding dice iou becoming common place. dice proposed mentioned issues pull requests multiple times 292, 369, 2115, 2994, 3442, 3457, 3611, 3653, 3977, 5916, 6933, 7032, 8961, 9154, 9275, 9395, 9444, 9671, 10783 . additionally, iou also mentioned number times repository 2016, 2185, 6467, 6538, 8225, 8643, 8669, 9367, 10104, 10602, 10783 . keep mind references repository... plenty github repositories use dice iou loss function. furthermore, research community used dice iou numerous papers make use cnn's. 100 citations according google scholar, though many exist... 1 https ieeexplore.ieee.org abstract document 7785132 , 2 https ieeexplore.ieee.org abstract document 7444155 , 3 http papers.nips.cc paper 5207 deep neural networks object detection , 4 https www.cv foundation.org openaccess content cvpr 2014 papers erhan scalable object detection 2014 cvpr paper.pdf , 5 https www.cv foundation.org openaccess content cvpr workshops 2015 w01 html brebisson deep neural networks 2015 cvpr paper.html , 6 https www.sciencedirect.com science article pii s1053811914010660 , 7 https link.springer.com chapter 10.1007 978 3 319 46723 8 48 , 8 https www.computer.org csdl proceedings icip 1994 6952 02 00413580.pdf , 9 https link.springer.com article 10.1007 s11548 016 1501 5 , 10 https ieeexplore.ieee.org abstract document 7482843 sorry ahead time behind paywall . personally working medical research u net image segmentation found training model binary cross entropy loss function first switching dice loss additional training significantly improved performance using binary cross entropy. using custom loss use dice loss, however, would great see official version supported keras. given year past since pr 7032, would keras team reconsider implementing official version dice iou loss functions?"
keras,6501,"hi, want reuse stateful lstm layers separate model like code simplified version get following error calling othermodel.predict whereas set stateful false, everything works. ubuntu 16.04, python 3.5.2, tensorflow 1.1.0",0,share stateful lstm layers,"share stateful lstm layers hi, want reuse stateful lstm layers separate model like code simplified version get following error calling othermodel.predict whereas set stateful false, everything works. ubuntu 16.04, python 3.5.2, tensorflow 1.1.0"
keras,11750,"hi guys, excuse question without much details, bound time order post results challenge platform, anyway try sum much can. multivariate time series, trained using rnn, periods repeating time indexes, 2013 01 2016 09, steps months, repeating, mean various subsets ordered january december, many times year, hundreds times, predicting next year knowing features. trained using lstm, 3 years, trying predict also repeating time series year 2017. used fixed batch size, one last layer binary target value used basic neural network batch size 12 chose 12 months, target train unbalanced problem predicting test like frustrating posting results values probability 0.5 near 1 means probability entry zero. could wrong !!? many thanks !! hope hear soon possible order tweak model get coherent outputs !",0,predicting real test set gives high probability 1 unbalanced data,"predicting real test set gives high probability 1 unbalanced data hi guys, excuse question without much details, bound time order post results challenge platform, anyway try sum much can. multivariate time series, trained using rnn, periods repeating time indexes, 2013 01 2016 09, steps months, repeating, mean various subsets ordered january december, many times year, hundreds times, predicting next year knowing features. trained using lstm, 3 years, trying predict also repeating time series year 2017. used fixed batch size, one last layer binary target value used basic neural network batch size 12 chose 12 months, target train unbalanced problem predicting test like frustrating posting results values probability 0.5 near 1 means probability entry zero. could wrong !!? many thanks !! hope hear soon possible order tweak model get coherent outputs !"
keras,12871,"trying similar things, getting stuck input model. first trained 3 different models model 1 3 labels 8,9,10 model 2 2 labels 30,31 model 3 4 labels 80,81,82,83 would like combine 3 models final model 1 output containing 9 labels 8,9,10,30,31,80,81,82,83 . final input needs get 1 input image instead 3 images. still get stuck. building kind hierarchy improve accuracy.",0,keras issue concatenate single input instead multiple,"keras issue concatenate single input instead multiple trying similar things, getting stuck input model. first trained 3 different models model 1 3 labels 8,9,10 model 2 2 labels 30,31 model 3 4 labels 80,81,82,83 would like combine 3 models final model 1 output containing 9 labels 8,9,10,30,31,80,81,82,83 . final input needs get 1 input image instead 3 images. still get stuck. building kind hierarchy improve accuracy."
keras,13218,"system information written custom code opposed using example directory yes os platform distribution e.g., linux ubuntu 16.04 linux centos 7 tensorflow backend yes yes tensorflow version v1.12.1 8795 ga675686 1.15.0 dev20190814 tf nightly dist keras version 2.2.4 python version 3.6 cuda cudnn version 10.2 gpu model memory 1080ti 11gb describe current behavior save model returns jason serialisable error similar, exactly same, issues posted here, supposedly resolved tf nightly install. cannot provide one hot encoded input data. describe expected behavior save best model's weights hdf5 file. code reproduce issue info logs typeerror traceback recent call last 1 get ipython .run line magic 'time', '' 2 cnn 1d model keras 1d cnn 3 cnn history dict cnn 1d model 1 .history 4 cnn history dict.keys 5 print n .2f cnn 1d model 0 .metrics names 1 , cnn 1d model 2 1 100 keras 1d cnn 100 epochs 1000, 101 batch size 2048, 102 validation split 0.2, shuffle true, callbacks tensorboard, earlystopper, checkpoint , verbose 0 103 cnn history model.fit partial seq train, 104 partial label train, harpy1 python v3 5.1.0 lib python3.6 site packages keras engine training.py fit self, x, y, batch size, epochs, verbose, callbacks, validation split, validation data, shuffle, class weight, sample weight, initial epoch, steps per epoch, validation steps, kwargs 1037 initial epoch initial epoch, 1038 steps per epoch steps per epoch, 1039 validation steps validation steps 1040 1041 def evaluate self, x none, none, harpy1 python v3 5.1.0 lib python3.6 site packages keras engine training arrays.py fit loop model, f, ins, labels, batch size, epochs, verbose, callbacks, val f, val ins, shuffle, callback metrics, initial epoch, steps per epoch, validation steps 215 l, zip labels, val outs 216 epoch logs 'val ' l 217 callbacks.on epoch end epoch, epoch logs 218 callback model.stop training 219 break harpy1 python v3 5.1.0 lib python3.6 site packages keras callbacks.py epoch end self, epoch, logs 77 logs logs 78 callback self.callbacks 79 callback.on epoch end epoch, logs 80 81 def batch begin self, batch, logs none harpy1 python v3 5.1.0 lib python3.6 site packages keras callbacks.py epoch end self, epoch, logs 444 self.model.save weights filepath, overwrite true 445 else 446 self.model.save filepath, overwrite true 447 else 448 self.verbose 0 harpy1 python v3 5.1.0 lib python3.6 site packages keras engine network.py save self, filepath, overwrite, include optimizer 1088 raise notimplementederror 1089 ..models import save model 1090 save model self, filepath, overwrite, include optimizer 1091 1092 def save weights self, filepath, overwrite true harpy1 python v3 5.1.0 lib python3.6 site packages keras engine saving.py save model model, filepath, overwrite, include optimizer 380 381 try 382 serialize model model, f, include optimizer 383 finally 384 opened new file harpy1 python v3 5.1.0 lib python3.6 site packages keras engine saving.py serialize model model, f, include optimizer 82 model config 'class name' model. class . name 83 model config 'config' model.get config 84 model config json.dumps model config, default get json type 85 model config model config.encode 'utf 8' 86 f 'model config' model config harpy1 python v3 5.1.0 lib python3.6 json init .py dumps obj, skipkeys, ensure ascii, check circular, allow nan, cls, indent, separators, default, sort keys, kw 236 check circular check circular, allow nan allow nan, indent indent, 237 separators separators, default default, sort keys sort keys, 238 kw .encode obj 239 240 harpy1 python v3 5.1.0 lib python3.6 json encoder.py encode self, 197 exceptions detailed. list call roughly 198 equivalent pysequence fast ''.join would do. 199 chunks self.iterencode o, one shot true 200 isinstance chunks, list, tuple 201 chunks list chunks harpy1 python v3 5.1.0 lib python3.6 json encoder.py iterencode self, o, one shot 255 self.key separator, self.item separator, self.sort keys, 256 self.skipkeys, one shot 257 return iterencode o, 0 258 259 def make iterencode markers, default, encoder, indent, floatstr, harpy1 python v3 5.1.0 lib python3.6 site packages keras engine saving.py get json type obj 72 return obj. name 73 74 raise typeerror 'not json serializable s' obj, 75 76 .. import version keras version typeerror json serializable",0,typeerror json serializable,"typeerror json serializable system information written custom code opposed using example directory yes os platform distribution e.g., linux ubuntu 16.04 linux centos 7 tensorflow backend yes yes tensorflow version v1.12.1 8795 ga675686 1.15.0 dev20190814 tf nightly dist keras version 2.2.4 python version 3.6 cuda cudnn version 10.2 gpu model memory 1080ti 11gb describe current behavior save model returns jason serialisable error similar, exactly same, issues posted here, supposedly resolved tf nightly install. cannot provide one hot encoded input data. describe expected behavior save best model's weights hdf5 file. code reproduce issue info logs typeerror traceback recent call last 1 get ipython .run line magic 'time', '' 2 cnn 1d model keras 1d cnn 3 cnn history dict cnn 1d model 1 .history 4 cnn history dict.keys 5 print n .2f cnn 1d model 0 .metrics names 1 , cnn 1d model 2 1 100 keras 1d cnn 100 epochs 1000, 101 batch size 2048, 102 validation split 0.2, shuffle true, callbacks tensorboard, earlystopper, checkpoint , verbose 0 103 cnn history model.fit partial seq train, 104 partial label train, harpy1 python v3 5.1.0 lib python3.6 site packages keras engine training.py fit self, x, y, batch size, epochs, verbose, callbacks, validation split, validation data, shuffle, class weight, sample weight, initial epoch, steps per epoch, validation steps, kwargs 1037 initial epoch initial epoch, 1038 steps per epoch steps per epoch, 1039 validation steps validation steps 1040 1041 def evaluate self, x none, none, harpy1 python v3 5.1.0 lib python3.6 site packages keras engine training arrays.py fit loop model, f, ins, labels, batch size, epochs, verbose, callbacks, val f, val ins, shuffle, callback metrics, initial epoch, steps per epoch, validation steps 215 l, zip labels, val outs 216 epoch logs 'val ' l 217 callbacks.on epoch end epoch, epoch logs 218 callback model.stop training 219 break harpy1 python v3 5.1.0 lib python3.6 site packages keras callbacks.py epoch end self, epoch, logs 77 logs logs 78 callback self.callbacks 79 callback.on epoch end epoch, logs 80 81 def batch begin self, batch, logs none harpy1 python v3 5.1.0 lib python3.6 site packages keras callbacks.py epoch end self, epoch, logs 444 self.model.save weights filepath, overwrite true 445 else 446 self.model.save filepath, overwrite true 447 else 448 self.verbose 0 harpy1 python v3 5.1.0 lib python3.6 site packages keras engine network.py save self, filepath, overwrite, include optimizer 1088 raise notimplementederror 1089 ..models import save model 1090 save model self, filepath, overwrite, include optimizer 1091 1092 def save weights self, filepath, overwrite true harpy1 python v3 5.1.0 lib python3.6 site packages keras engine saving.py save model model, filepath, overwrite, include optimizer 380 381 try 382 serialize model model, f, include optimizer 383 finally 384 opened new file harpy1 python v3 5.1.0 lib python3.6 site packages keras engine saving.py serialize model model, f, include optimizer 82 model config 'class name' model. class . name 83 model config 'config' model.get config 84 model config json.dumps model config, default get json type 85 model config model config.encode 'utf 8' 86 f 'model config' model config harpy1 python v3 5.1.0 lib python3.6 json init .py dumps obj, skipkeys, ensure ascii, check circular, allow nan, cls, indent, separators, default, sort keys, kw 236 check circular check circular, allow nan allow nan, indent indent, 237 separators separators, default default, sort keys sort keys, 238 kw .encode obj 239 240 harpy1 python v3 5.1.0 lib python3.6 json encoder.py encode self, 197 exceptions detailed. list call roughly 198 equivalent pysequence fast ''.join would do. 199 chunks self.iterencode o, one shot true 200 isinstance chunks, list, tuple 201 chunks list chunks harpy1 python v3 5.1.0 lib python3.6 json encoder.py iterencode self, o, one shot 255 self.key separator, self.item separator, self.sort keys, 256 self.skipkeys, one shot 257 return iterencode o, 0 258 259 def make iterencode markers, default, encoder, indent, floatstr, harpy1 python v3 5.1.0 lib python3.6 site packages keras engine saving.py get json type obj 72 return obj. name 73 74 raise typeerror 'not json serializable s' obj, 75 76 .. import version keras version typeerror json serializable"
keras,7783,"please make sure boxes checked submit issue. issue implementation question, please ask question stackoverflow http stackoverflow.com questions tagged keras join keras slack channel https keras slack autojoin.herokuapp.com ask instead filing github issue. thank you! x check date master branch keras. update pip install git git github.com fchollet keras.git upgrade deps x running tensorflow, check date latest version. installation instructions found https www.tensorflow.org get started os setup . running theano, check date master branch theano. update pip install git git github.com theano theano.git upgrade deps provide link github gist python script reproduce issue copy script short . running resnet resnet.get layer 'activation 49' .output flatten flatten resnet seed input shape 7, merged concatenate flatten, seed , axis 1 results zero dimensional arrays cannot concatenated however, running resnet resnet.get layer 'activation 49' .output flatten flatten resnet seed input shape 7, merged merge flatten, seed , mode 'concat', concat axis 1 compiles well without errors! bug?",0,bug concatenation layer,"bug concatenation layer please make sure boxes checked submit issue. issue implementation question, please ask question stackoverflow http stackoverflow.com questions tagged keras join keras slack channel https keras slack autojoin.herokuapp.com ask instead filing github issue. thank you! x check date master branch keras. update pip install git git github.com fchollet keras.git upgrade deps x running tensorflow, check date latest version. installation instructions found https www.tensorflow.org get started os setup . running theano, check date master branch theano. update pip install git git github.com theano theano.git upgrade deps provide link github gist python script reproduce issue copy script short . running resnet resnet.get layer 'activation 49' .output flatten flatten resnet seed input shape 7, merged concatenate flatten, seed , axis 1 results zero dimensional arrays cannot concatenated however, running resnet resnet.get layer 'activation 49' .output flatten flatten resnet seed input shape 7, merged merge flatten, seed , mode 'concat', concat axis 1 compiles well without errors! bug?"
keras,10661,running following script using tensorflow backend using cntk backend cntk keras outputs incorrectl tensor state.,0,incorrect state dimensions gru using cntk backend,incorrect state dimensions gru using cntk backend running following script using tensorflow backend using cntk backend cntk keras outputs incorrectl tensor state.
keras,7462,"generator used parameter validation data calling fit generator, calculate custom metrics end epoch? wanted try understand others going problem. here's code use model.fit generator generate data file 'data 0.1 percent training data.tsv', binarizer, batch size , validation data generate data file 'data 0.1 percent validation data.tsv', binarizer, batch size , steps per epoch math.ceil 1.0 109527 batch size , validation steps math.ceil 1.0 13692 batch size , epochs 10, verbose 1, class weight class weights, max queue size 2",0,metrics calculation callbacks using fit generator generators training validation,"metrics calculation callbacks using fit generator generators training validation generator used parameter validation data calling fit generator, calculate custom metrics end epoch? wanted try understand others going problem. here's code use model.fit generator generate data file 'data 0.1 percent training data.tsv', binarizer, batch size , validation data generate data file 'data 0.1 percent validation data.tsv', binarizer, batch size , steps per epoch math.ceil 1.0 109527 batch size , validation steps math.ceil 1.0 13692 batch size , epochs 10, verbose 1, class weight class weights, max queue size 2"
keras,12798,"please make sure boxes checked submit issue. issue implementation question, please ask question stackoverflow http stackoverflow.com questions tagged keras join keras slack channel https keras slack autojoin.herokuapp.com ask instead filing github issue. thank you! x check date master branch keras. update pip install git git github.com keras team keras.git upgrade deps x running tensorflow, check date latest version. installation instructions found https www.tensorflow.org get started os setup . x running theano, check date master branch theano. update pip install git git github.com theano theano.git upgrade deps x provide link github gist python script reproduce issue copy script short . want slack github integration notify team activity , received error reproduce 1. add github app slack workspace 2. channel, run 3. see error",0,enable slack github integration notifications,"enable slack github integration notifications please make sure boxes checked submit issue. issue implementation question, please ask question stackoverflow http stackoverflow.com questions tagged keras join keras slack channel https keras slack autojoin.herokuapp.com ask instead filing github issue. thank you! x check date master branch keras. update pip install git git github.com keras team keras.git upgrade deps x running tensorflow, check date latest version. installation instructions found https www.tensorflow.org get started os setup . x running theano, check date master branch theano. update pip install git git github.com theano theano.git upgrade deps x provide link github gist python script reproduce issue copy script short . want slack github integration notify team activity , received error reproduce 1. add github app slack workspace 2. channel, run 3. see error"
keras,13031,"hi used keras wrappers scikit implement adaboost single input keras models. solution works fine dense layers, 3d layers. moment try use 3d layers lstm simplernn, first model training runs ok immediately thereafter get following error message. valueerror input 0 layer simple rnn 1 incompatible layer expected ndim 3, found ndim 2. full shape received none, 3 recreate error, please install scikit nightly build 0.22.dev0. stable 0.21 release allow adaboost class fit method take 3d arrays required lstm. system information written custom code opposed using example directory custom code os platform distribution e.g., linux ubuntu 16.04 ubuntu 18.04 tensorflow backend yes yes tensorflow version 1.14.0 keras version 2.2.4 tf python version 3.7 cuda cudnn version none gpu model memory na tried use 2d input layer scikit stable version 0.21 using keras reshape layer convert 3d input. unfortunately, initial round training, following error message shown. 2 error messages occur point training. first epochs finish successfully, expect second round training start, error message pops up. file , line 3, raise tensorflow.python.framework.errors impl.invalidargumenterror input reshape tensor 150 values, requested shape 2200 node reshape 1 reshape op inference keras scratch graph 4029 logs attached errors. tensorflow log reshape.txt https github.com keras team keras files 3342360 tensorflow log reshape.txt tensorflow log 3d input.txt https github.com keras team keras files 3342361 tensorflow log 3d input.txt help would highly appreciated. thanks best regards, adeel",0,error using 3d inputs keras scikit learn wrapper adaboost,"error using 3d inputs keras scikit learn wrapper adaboost hi used keras wrappers scikit implement adaboost single input keras models. solution works fine dense layers, 3d layers. moment try use 3d layers lstm simplernn, first model training runs ok immediately thereafter get following error message. valueerror input 0 layer simple rnn 1 incompatible layer expected ndim 3, found ndim 2. full shape received none, 3 recreate error, please install scikit nightly build 0.22.dev0. stable 0.21 release allow adaboost class fit method take 3d arrays required lstm. system information written custom code opposed using example directory custom code os platform distribution e.g., linux ubuntu 16.04 ubuntu 18.04 tensorflow backend yes yes tensorflow version 1.14.0 keras version 2.2.4 tf python version 3.7 cuda cudnn version none gpu model memory na tried use 2d input layer scikit stable version 0.21 using keras reshape layer convert 3d input. unfortunately, initial round training, following error message shown. 2 error messages occur point training. first epochs finish successfully, expect second round training start, error message pops up. file , line 3, raise tensorflow.python.framework.errors impl.invalidargumenterror input reshape tensor 150 values, requested shape 2200 node reshape 1 reshape op inference keras scratch graph 4029 logs attached errors. tensorflow log reshape.txt https github.com keras team keras files 3342360 tensorflow log reshape.txt tensorflow log 3d input.txt https github.com keras team keras files 3342361 tensorflow log 3d input.txt help would highly appreciated. thanks best regards, adeel"
keras,11833,"hello , using keras tensorflow implement cnn nets edge detection , tried run code , error occurred . would like help solve it. thanks advance. code keras.models import sequential keras.layers import activation, dropout, flatten, dense, conv2d, maxpooling2d keras.utils import np utils keras import backend k k.set image dim ordering 'th' import json, pylab import cv2 import numpy np np.set printoptions threshold np.nan model data processing constants batch size 128 nb classes 2 nb epoch 7 input image dimensions img rows, img cols 48, 72 number convolutional filters use nb filters 32 size pooling area max pooling nb pool 5 convolution kernel size nb conv 3 architecture model sequential model.add conv2d nb filters, nb conv, nb conv , padding 'valid', input shape 1, img rows, img cols model.add activation 'relu' model.add conv2d nb filters, nb conv, nb conv model.add activation 'relu' model.add maxpooling2d pool size nb pool, nb pool model.add dropout 0.25 model.add flatten model.add dense 128 model.add activation 'relu' model.add dropout 0.5 model.add dense nb classes model.add activation 'softmax' model.compile loss 'sparse categorical crossentropy', optimizer 'adadelta', metrics 'accuracy' train imgs cv2.imread '1.jpg',0 ground truth train cv2.imread '1edges.jpg',0 test imgs cv2.imread '2.jpg' ,0 ground truth test cv2.imread '2edges.jpg',0 print 'preparing images...' x train np.array train imgs x test np.array ground truth train train np.array test imgs test np.array ground truth test prepare data x train x train.reshape x train.shape 0 , 1, img rows, img cols x test x test.reshape x test.shape 0 , 1, img rows, img cols x train x train.astype 'float32' x test x test.astype 'float32' x train 255 x test 255 print 'x train shape ', x train.shape print x train.shape, 'train samples' print x test.shape, 'test samples' convert class vectors binary class matrices convert class vectors binary class matrices keras.utils import categorical train np.divide train,255 test np.divide test,255 train categorical train,nb classes test categorical test,nb classes train plz print 'training model...' model.fit x train, train , batch size batch size, nb epoch nb epoch, verbose 1, validation data x test,y test let's dump model print 'saving model...' saved model model.to json open 'cnn architecture.json', 'w' outfile json.dump saved model, outfile model.save weights 'cnn weights.h5' url",0,valueerror setting array element sequence.,"valueerror setting array element sequence. hello , using keras tensorflow implement cnn nets edge detection , tried run code , error occurred . would like help solve it. thanks advance. code keras.models import sequential keras.layers import activation, dropout, flatten, dense, conv2d, maxpooling2d keras.utils import np utils keras import backend k k.set image dim ordering 'th' import json, pylab import cv2 import numpy np np.set printoptions threshold np.nan model data processing constants batch size 128 nb classes 2 nb epoch 7 input image dimensions img rows, img cols 48, 72 number convolutional filters use nb filters 32 size pooling area max pooling nb pool 5 convolution kernel size nb conv 3 architecture model sequential model.add conv2d nb filters, nb conv, nb conv , padding 'valid', input shape 1, img rows, img cols model.add activation 'relu' model.add conv2d nb filters, nb conv, nb conv model.add activation 'relu' model.add maxpooling2d pool size nb pool, nb pool model.add dropout 0.25 model.add flatten model.add dense 128 model.add activation 'relu' model.add dropout 0.5 model.add dense nb classes model.add activation 'softmax' model.compile loss 'sparse categorical crossentropy', optimizer 'adadelta', metrics 'accuracy' train imgs cv2.imread '1.jpg',0 ground truth train cv2.imread '1edges.jpg',0 test imgs cv2.imread '2.jpg' ,0 ground truth test cv2.imread '2edges.jpg',0 print 'preparing images...' x train np.array train imgs x test np.array ground truth train train np.array test imgs test np.array ground truth test prepare data x train x train.reshape x train.shape 0 , 1, img rows, img cols x test x test.reshape x test.shape 0 , 1, img rows, img cols x train x train.astype 'float32' x test x test.astype 'float32' x train 255 x test 255 print 'x train shape ', x train.shape print x train.shape, 'train samples' print x test.shape, 'test samples' convert class vectors binary class matrices convert class vectors binary class matrices keras.utils import categorical train np.divide train,255 test np.divide test,255 train categorical train,nb classes test categorical test,nb classes train plz print 'training model...' model.fit x train, train , batch size batch size, nb epoch nb epoch, verbose 1, validation data x test,y test let's dump model print 'saving model...' saved model model.to json open 'cnn architecture.json', 'w' outfile json.dump saved model, outfile model.save weights 'cnn weights.h5' url"
keras,11172,issue opened host discussion recurrent attention api keras. related issues 11142. 8296. 7633.,0,recurrent attention api keras,recurrent attention api keras issue opened host discussion recurrent attention api keras. related issues 11142. 8296. 7633.
keras,12838,"please make sure bug feature request provide applicable information asked template. issue implementation question , please ask question stackoverflow http stackoverflow.com questions tagged keras keras slack channel https keras slack autojoin.herokuapp.com instead opening github issue. system information written custom code opposed using example directory os platform distribution e.g., linux ubuntu 16.04 tensorflow backend yes yes tensorflow version latest update keras version date python version date cuda cudnn version gpu model memory keras.io documentation generative adversial network examples too, guess section",0,documentation required gan,"documentation required gan please make sure bug feature request provide applicable information asked template. issue implementation question , please ask question stackoverflow http stackoverflow.com questions tagged keras keras slack channel https keras slack autojoin.herokuapp.com instead opening github issue. system information written custom code opposed using example directory os platform distribution e.g., linux ubuntu 16.04 tensorflow backend yes yes tensorflow version latest update keras version date python version date cuda cudnn version gpu model memory keras.io documentation generative adversial network examples too, guess section"
keras,12944,"relating 12249 training autoencoder. feature vectors extracted hidden layers middle stored uint8 well utilised infer decoder later. furthermore, output autoencoder, is, reconstructed input, compare original input uint8 precision only. values weights, outputs layers etc. float32 float16. keras supported mixed precision training? yes, may know doc is? not, added? is, allowing programmers specify precision layer? thanks",0,request mixed precision training,"request mixed precision training relating 12249 training autoencoder. feature vectors extracted hidden layers middle stored uint8 well utilised infer decoder later. furthermore, output autoencoder, is, reconstructed input, compare original input uint8 precision only. values weights, outputs layers etc. float32 float16. keras supported mixed precision training? yes, may know doc is? not, added? is, allowing programmers specify precision layer? thanks"
keras,10424,"trying train model uses cosine distance negative samples. is, loss function based multiple values instead single true pred comparison. noticed two things. one, cosine distance negative samples function loss, found cntk cosine distance negative samples https docs.microsoft.com en us python api cntk.losses?view cntk py 2.5.1 . two, loss function, including custom functions, must conform signature loss true, pred . see reason this, libraries theano, cntk tensorflow able use loss function multiple arguments. wondering holding implementation features back. priority, may go ahead try implement myself.",0,feature cosine distance negative sampling,"feature cosine distance negative sampling trying train model uses cosine distance negative samples. is, loss function based multiple values instead single true pred comparison. noticed two things. one, cosine distance negative samples function loss, found cntk cosine distance negative samples https docs.microsoft.com en us python api cntk.losses?view cntk py 2.5.1 . two, loss function, including custom functions, must conform signature loss true, pred . see reason this, libraries theano, cntk tensorflow able use loss function multiple arguments. wondering holding implementation features back. priority, may go ahead try implement myself."
keras,7857,"model runs 2d convolutions image, turns image sequence, runs gru layers left right vertical slices image. train model, try load c , using cntk, fails, user defined function reshape batch defined keras python code. using ocr, model",0,reshapebatch aka reshape batch makes cntk model unable load c,"reshapebatch aka reshape batch makes cntk model unable load c model runs 2d convolutions image, turns image sequence, runs gru layers left right vertical slices image. train model, try load c , using cntk, fails, user defined function reshape batch defined keras python code. using ocr, model"
keras,10330,"hi, problems following error occurs call modelbut training. def exponent neg manhattan distance left, right helper function similarity estimate lstms outputs return k.exp k.sum k.abs left right , axis 1, keepdims true calculates distance defined malstm model distancefunc 'exponent neg manhattan distance' malstm distance merge mode lambda x exponent neg manhattan distance x 0 , x 1 , output shape lambda x x 0 0 , 1 left output, right output error users bai anaconda3 lib python3.6 site packages keras engine topology.py 1269 userwarning layer deprecated removed 08 2017. use instead layers , e.g. , , etc. return cls config traceback recent call last file users bai pyproject cikmanalyticup cikm train model.py , line 264, test file users bai pyproject cikmanalyticup cikm train model.py , line 249, test model model json json string file users bai anaconda3 lib python3.6 site packages keras models.py , line 349, model json return layer module.deserialize config, custom objects custom objects file users bai anaconda3 lib python3.6 site packages keras layers init .py , line 55, deserialize printable module name 'layer' file users bai anaconda3 lib python3.6 site packages keras utils generic utils.py , line 143, deserialize keras object list custom objects.items file users bai anaconda3 lib python3.6 site packages keras engine topology.py , line 2517, config process node layer, node data file users bai anaconda3 lib python3.6 site packages keras engine topology.py , line 2476, process node layer input tensors, kwargs file users bai anaconda3 lib python3.6 site packages keras engine topology.py , line 617, call output self.call inputs, kwargs file users bai anaconda3 lib python3.6 site packages keras legacy layers.py , line 208, call return self.mode inputs, arguments file users bai pyproject cikmanalyticup cikm train model.py , line 156, malstm distance merge mode lambda x exponent neg manhattan distance x 0 , x 1 , nameerror name 'exponent neg manhattan distance' defined hope get help",0,call model error,"call model error hi, problems following error occurs call modelbut training. def exponent neg manhattan distance left, right helper function similarity estimate lstms outputs return k.exp k.sum k.abs left right , axis 1, keepdims true calculates distance defined malstm model distancefunc 'exponent neg manhattan distance' malstm distance merge mode lambda x exponent neg manhattan distance x 0 , x 1 , output shape lambda x x 0 0 , 1 left output, right output error users bai anaconda3 lib python3.6 site packages keras engine topology.py 1269 userwarning layer deprecated removed 08 2017. use instead layers , e.g. , , etc. return cls config traceback recent call last file users bai pyproject cikmanalyticup cikm train model.py , line 264, test file users bai pyproject cikmanalyticup cikm train model.py , line 249, test model model json json string file users bai anaconda3 lib python3.6 site packages keras models.py , line 349, model json return layer module.deserialize config, custom objects custom objects file users bai anaconda3 lib python3.6 site packages keras layers init .py , line 55, deserialize printable module name 'layer' file users bai anaconda3 lib python3.6 site packages keras utils generic utils.py , line 143, deserialize keras object list custom objects.items file users bai anaconda3 lib python3.6 site packages keras engine topology.py , line 2517, config process node layer, node data file users bai anaconda3 lib python3.6 site packages keras engine topology.py , line 2476, process node layer input tensors, kwargs file users bai anaconda3 lib python3.6 site packages keras engine topology.py , line 617, call output self.call inputs, kwargs file users bai anaconda3 lib python3.6 site packages keras legacy layers.py , line 208, call return self.mode inputs, arguments file users bai pyproject cikmanalyticup cikm train model.py , line 156, malstm distance merge mode lambda x exponent neg manhattan distance x 0 , x 1 , nameerror name 'exponent neg manhattan distance' defined hope get help"
keras,12945,"evaluating data different parameters, came different results. guess related warning use multiprocessing truekeras.utils.sequence class. test data flowflow directory believe people would use. reasonable raise error give warning protect users getting wrong results?",0,model.evaluate generator may give wrong results using multiprocessing.,"model.evaluate generator may give wrong results using multiprocessing. evaluating data different parameters, came different results. guess related warning use multiprocessing truekeras.utils.sequence class. test data flowflow directory believe people would use. reasonable raise error give warning protect users getting wrong results?"
keras,13088,"want initialize initial state lstm layer final hidden state another lstm layer. basically, want implement conditional encoding explained paper https www.aclweb.org anthology d16 1084 ! image https user images.githubusercontent.com 32245327 60956724 fec33e00 a320 11e9 8ccd 6c8564c626ac.png access final hidden state one lstm layer use initial state another lstm layer? thanks",0,initialize lstm initial state manually,"initialize lstm initial state manually want initialize initial state lstm layer final hidden state another lstm layer. basically, want implement conditional encoding explained paper https www.aclweb.org anthology d16 1084 ! image https user images.githubusercontent.com 32245327 60956724 fec33e00 a320 11e9 8ccd 6c8564c626ac.png access final hidden state one lstm layer use initial state another lstm layer? thanks"
keras,12981,"system information written custom code opposed using example directory os platform distribution e.g., linux ubuntu 16.04 linux ubuntu 16.04 tensorflow backend yes yes tensorflow version 1.9.0 keras version 2.2.4 python version 3.5.2 cuda cudnn version 9.0 gpu model memory nvidia p6000 describe current behavior trying use dilated 3d convolutions. however, declare one, like run memory. says cannot allocate tensor . describe expected behavior given batch size set 64, 512 come from?",0,conv3d dilation increases batch size?,"conv3d dilation increases batch size? system information written custom code opposed using example directory os platform distribution e.g., linux ubuntu 16.04 linux ubuntu 16.04 tensorflow backend yes yes tensorflow version 1.9.0 keras version 2.2.4 python version 3.5.2 cuda cudnn version 9.0 gpu model memory nvidia p6000 describe current behavior trying use dilated 3d convolutions. however, declare one, like run memory. says cannot allocate tensor . describe expected behavior given batch size set 64, 512 come from?"
keras,9552,"hi earlier submission similar topic disappeared! twice. looked code section 6.3 advanced usage recurrent neural networks book, input arguments like input shape layers.gru missing keras documentation https keras.io layers recurrent gru https keras.io layers recurrent lstm also looked recurrent,py local variable input shape classes recurrant gru. missed? missing documentation makes learning difficult. locate input arguments particular class?",0,"arguments missing keras documentation, e.g. gru lstm ?","arguments missing keras documentation, e.g. gru lstm ? hi earlier submission similar topic disappeared! twice. looked code section 6.3 advanced usage recurrent neural networks book, input arguments like input shape layers.gru missing keras documentation https keras.io layers recurrent gru https keras.io layers recurrent lstm also looked recurrent,py local variable input shape classes recurrant gru. missed? missing documentation makes learning difficult. locate input arguments particular class?"
keras,12690,"short version custom regularizer object seems reconstructed batch. full version trying create custom keras regularizer uses distance layer's weights original weights, used work get zero difference times. regularizer code using tensorflow backend playing class bit, noted something strange regularizer object created training batch, explain getting zeros. got conclusion changing class seeing loss fact suffer penalty follows ugly check 1 throughout training. would expect regularizer object remain one throughout training. bug understanding usage custom regularizers correctly?",0,custom regularizer remain object,"custom regularizer remain object short version custom regularizer object seems reconstructed batch. full version trying create custom keras regularizer uses distance layer's weights original weights, used work get zero difference times. regularizer code using tensorflow backend playing class bit, noted something strange regularizer object created training batch, explain getting zeros. got conclusion changing class seeing loss fact suffer penalty follows ugly check 1 throughout training. would expect regularizer object remain one throughout training. bug understanding usage custom regularizers correctly?"
keras,12830,"quick question regarding use advanced activation functions. currently working rnn's regression problem. let's say sake question model looks something like following model.add lstm model.add elu question refers use elu activation function, default lstm 'tanh' set activation function, adding elu top makes data flow lstm tanh elu? regards,",0,using advanced activation functions,"using advanced activation functions quick question regarding use advanced activation functions. currently working rnn's regression problem. let's say sake question model looks something like following model.add lstm model.add elu question refers use elu activation function, default lstm 'tanh' set activation function, adding elu top makes data flow lstm tanh elu? regards,"
keras,9076,"greetings all, problem working binary classification. around 150 sequences, sequence 130000 timesteps, timestep 2 features, shape 1,130000,2 . sequences labelled 1 0 . want train lstm network, end, giving sequence 130000 timesteps 2 features each, predict 1 0 . hence problem mind many one right? 1 keep training 120 sequences rest 30 validation, number parameters model, smth like 120x130000x2 31,200,000 parameters? example lstm layer 4000 units? 2 network stateful? model thinking smth like tried one epoch fit it, memory gets quite high! recommendations whole problem approach?",0,rough estimate number parameters lstm architecture..,"rough estimate number parameters lstm architecture.. greetings all, problem working binary classification. around 150 sequences, sequence 130000 timesteps, timestep 2 features, shape 1,130000,2 . sequences labelled 1 0 . want train lstm network, end, giving sequence 130000 timesteps 2 features each, predict 1 0 . hence problem mind many one right? 1 keep training 120 sequences rest 30 validation, number parameters model, smth like 120x130000x2 31,200,000 parameters? example lstm layer 4000 units? 2 network stateful? model thinking smth like tried one epoch fit it, memory gets quite high! recommendations whole problem approach?"
keras,9526,"running variational autoencoder.py file keras examples , following error encountered typeerror compile missing 1 required positional argument 'loss' adding vae loss loss parameter compile i.e., vae.compile optimizer 'rmsprop' , loss vae loss instead vae.add loss vae loss , following error encountered typeerror using python allowed. use instead test tensor defined, use tensorflow ops tf.cond execute subgraphs conditioned value tensor. think problem need define custom loss function keras terms python function acts pred true, done here.",0,variational autoencoder custom loss function,"variational autoencoder custom loss function running variational autoencoder.py file keras examples , following error encountered typeerror compile missing 1 required positional argument 'loss' adding vae loss loss parameter compile i.e., vae.compile optimizer 'rmsprop' , loss vae loss instead vae.add loss vae loss , following error encountered typeerror using python allowed. use instead test tensor defined, use tensorflow ops tf.cond execute subgraphs conditioned value tensor. think problem need define custom loss function keras terms python function acts pred true, done here."
keras,10591,"interesting situation built network architecture using sequential functional api, however train one transfer weights like model2.set weight model1.get weight get outputs. used following code construct models using sequential model1 sequential model1.add gru 15, activation relu , input shape 1052,12 , return sequences true model1.add dense 1 using functional api inlayer input shape 1052,12 hidden gru 15, activation relu , return sequences true inlayer outlayer dense 1 hidden model2 model inputs inlayer, outputs outlayer easy get around sticking functional api, would situation occur, intended?",0,differences weight data structures sequential functional api?,"differences weight data structures sequential functional api? interesting situation built network architecture using sequential functional api, however train one transfer weights like model2.set weight model1.get weight get outputs. used following code construct models using sequential model1 sequential model1.add gru 15, activation relu , input shape 1052,12 , return sequences true model1.add dense 1 using functional api inlayer input shape 1052,12 hidden gru 15, activation relu , return sequences true inlayer outlayer dense 1 hidden model2 model inputs inlayer, outputs outlayer easy get around sticking functional api, would situation occur, intended?"
keras,7244,"hi, trying implement nonlinear timeseries prediction similar publication https www.ncbi.nlm.nih.gov pmc articles pmc5336098 though interested using cnn first layer. dataset shape , padded 0s shape . works fine normal lstms. bidirectional lstm implemented model like unfortunately getting weird results model look like model partly ignoring masking. problem bidirectional lstms masking seems reading data backwards, masking front back . therefore getting weird results. found old blog post http dirko.github.io bidirectional lstms keras handles similar problem though using old api. need provide distinct datasets padding correct place layer bi lstm? would feed data model? leave padding completely create batches size? even use ? x check date master branch keras. update pip install git git github.com fchollet keras.git upgrade deps x running tensorflow, check date latest version. installation instructions found https www.tensorflow.org get started os setup . cheers, hfjn",0,masking bidirectional lstms working?,"masking bidirectional lstms working? hi, trying implement nonlinear timeseries prediction similar publication https www.ncbi.nlm.nih.gov pmc articles pmc5336098 though interested using cnn first layer. dataset shape , padded 0s shape . works fine normal lstms. bidirectional lstm implemented model like unfortunately getting weird results model look like model partly ignoring masking. problem bidirectional lstms masking seems reading data backwards, masking front back . therefore getting weird results. found old blog post http dirko.github.io bidirectional lstms keras handles similar problem though using old api. need provide distinct datasets padding correct place layer bi lstm? would feed data model? leave padding completely create batches size? even use ? x check date master branch keras. update pip install git git github.com fchollet keras.git upgrade deps x running tensorflow, check date latest version. installation instructions found https www.tensorflow.org get started os setup . cheers, hfjn"
keras,8365,"would great someone suggest fix stop iteration problem. based property generator whenever call function, exhausted yields within index generator running next results stopiteration error case reaches last batch directory iterator, stop working. issue allowing compile model. please make sure boxes checked submit issue. issue implementation question, please ask question stackoverflow http stackoverflow.com questions tagged keras join keras slack channel https keras slack autojoin.herokuapp.com ask instead filing github issue. thank you! check date master branch keras. update pip install git git github.com fchollet keras.git upgrade deps running tensorflow, check date latest version. installation instructions found https www.tensorflow.org get started os setup . running theano, check date master branch theano. update pip install git git github.com theano theano.git upgrade deps provide link github gist python script reproduce issue copy script short .",0,issue generator trying call index generator next function getting error traceback recent call last generator ouput next output generator stopiteration.,"issue generator trying call index generator next function getting error traceback recent call last generator ouput next output generator stopiteration. would great someone suggest fix stop iteration problem. based property generator whenever call function, exhausted yields within index generator running next results stopiteration error case reaches last batch directory iterator, stop working. issue allowing compile model. please make sure boxes checked submit issue. issue implementation question, please ask question stackoverflow http stackoverflow.com questions tagged keras join keras slack channel https keras slack autojoin.herokuapp.com ask instead filing github issue. thank you! check date master branch keras. update pip install git git github.com fchollet keras.git upgrade deps running tensorflow, check date latest version. installation instructions found https www.tensorflow.org get started os setup . running theano, check date master branch theano. update pip install git git github.com theano theano.git upgrade deps provide link github gist python script reproduce issue copy script short ."
keras,8407,code error shown removing dropout initial state argument seems solve problem can't use help issue ?,0,error using dropout initial state gru layer,error using dropout initial state gru layer code error shown removing dropout initial state argument seems solve problem can't use help issue ?
keras,7217,"good job pre load data batches, avoid problem memory insufficiency, fit continuously available data. wondering, passing generator finite number outputs, possible add option behavior like function declares data epoch instead using ? idea whole data seen total epochs something like 5.6 epochs use , use train batch, need thread pre load batches preprocessing costs time. thanks.",0,fit generator declare epoch whole data generator finite number data instead steps per epoch?,"fit generator declare epoch whole data generator finite number data instead steps per epoch? good job pre load data batches, avoid problem memory insufficiency, fit continuously available data. wondering, passing generator finite number outputs, possible add option behavior like function declares data epoch instead using ? idea whole data seen total epochs something like 5.6 epochs use , use train batch, need thread pre load batches preprocessing costs time. thanks."
keras,11335,"seen several discussion topic various forums. however, none seem address central issue have. goes setup working lstm model. sentiment analysis application. save load model per faq https keras.io getting started faq save keras model works context . program .py file train save delete model load predict works. problem following work. program 1 train model. save model. program 2 load model. prediction. predictions completely compared single context run. similar issue discussion marked stale. https github.com keras team keras issues 4904 another work around seems https github.com keras team keras issues 7632 question store internal states pickle format lstm cases? another recommended way trying attempt? output",0,keras lstm model saving loading produces inconsistent results,"keras lstm model saving loading produces inconsistent results seen several discussion topic various forums. however, none seem address central issue have. goes setup working lstm model. sentiment analysis application. save load model per faq https keras.io getting started faq save keras model works context . program .py file train save delete model load predict works. problem following work. program 1 train model. save model. program 2 load model. prediction. predictions completely compared single context run. similar issue discussion marked stale. https github.com keras team keras issues 4904 another work around seems https github.com keras team keras issues 7632 question store internal states pickle format lstm cases? another recommended way trying attempt? output"
keras,9224,"hello, use pandas hdfstore keras, python program collapsed. code ok comment line . system win10 x64 pandas 0.20.3 keras 2.1.3",0,pandas hdfstore conflicts keras?!,"pandas hdfstore conflicts keras?! hello, use pandas hdfstore keras, python program collapsed. code ok comment line . system win10 x64 pandas 0.20.3 keras 2.1.3"
keras,13658,earlystopping restore best weights true makes typeerror 'nonetype' object subscriptable. restore best weights false exception. system information keras version 2.3.1 python version 3.6 stack trace,0,earlystopping restore best weights true makes typeerror 'nonetype' object subscriptable,earlystopping restore best weights true makes typeerror 'nonetype' object subscriptable earlystopping restore best weights true makes typeerror 'nonetype' object subscriptable. restore best weights false exception. system information keras version 2.3.1 python version 3.6 stack trace
keras,9468,"first off, examples wonderful. said, question improvement char level text generation example https github.com keras team keras blob master examples lstm text generation.py . trying example corpus kept running memoryerror allocating training set labels. noticed use 1 hot encoding indicate activated character. question bake embedding model identity matrix first lstm basically character embedding layer returns 1 hot using sparse categorical loss evaluation? reduces total memory usage factor number unique characters. seemed work tried out. note trying much longer input sequences 100 using lot memory. open thoughts, issues, etc.",0,reducing memory usage char level text generation lstms,"reducing memory usage char level text generation lstms first off, examples wonderful. said, question improvement char level text generation example https github.com keras team keras blob master examples lstm text generation.py . trying example corpus kept running memoryerror allocating training set labels. noticed use 1 hot encoding indicate activated character. question bake embedding model identity matrix first lstm basically character embedding layer returns 1 hot using sparse categorical loss evaluation? reduces total memory usage factor number unique characters. seemed work tried out. note trying much longer input sequences 100 using lot memory. open thoughts, issues, etc."
keras,13344,"import numpy np sklearn.model selection import train test split sklearn.utils import shuffle keras.layers import dense,flatten, conv2d keras.layers import maxpooling2d, dropout keras.utils import np utils, print summary import tensorflow tf keras.models import sequential keras.callbacks import modelcheckpoint import pickle keras.callbacks import tensorboard def keras model image x, image num classes 15 model sequential model.add conv2d 32, 5, 5 , input shape image x,image y,1 , activation 'relu' model.add maxpooling2d pool size 2, 2 , strides 2, 2 , padding 'same' model.add conv2d 64, 5, 5 , activation 'relu' model.add maxpooling2d pool size 2, 2 , strides 2, 2 , padding 'same' model.add flatten model.add dense 512, activation 'relu' model.add dropout 0.6 model.add dense 128, activation 'relu' model.add dropout 0.6 model.add dense num classes, activation 'softmax' model.compile loss 'categorical crossentropy', optimizer 'adam', metrics 'accuracy' filepath quickdraw.h5 checkpoint modelcheckpoint filepath, monitor 'val acc', verbose 1, save best true, mode 'max' callbacks list checkpoint return model, callbacks list def loadfrompickle open features , rb f features np.array pickle.load f open labels , rb f labels np.array pickle.load f return features, labels def augmentdata features, labels features np.append features, features , , 1 , axis 0 labels np.append labels, labels, axis 0 return features, labels def prepress labels labels labels np utils.to categorical labels return labels def main features, labels loadfrompickle features, labels augmentdata features, labels features, labels shuffle features, labels labels prepress labels labels train x, test x, train y, test train test split features, labels, random state 0, test size 0.1 train x train x.reshape train x.shape 0 , 28, 28, 1 test x test x.reshape test x.shape 0 , 28, 28, 1 model, callbacks list keras model 28,28 print summary model model.fit train x, train y, validation data test x, test , epochs 3, batch size 64, callbacks tensorboard log dir quickdraw model.save 'quickdraw.h5' main",0,"valueerror error checking target expected dense 3 shape 15, got array shape 1,","valueerror error checking target expected dense 3 shape 15, got array shape 1, import numpy np sklearn.model selection import train test split sklearn.utils import shuffle keras.layers import dense,flatten, conv2d keras.layers import maxpooling2d, dropout keras.utils import np utils, print summary import tensorflow tf keras.models import sequential keras.callbacks import modelcheckpoint import pickle keras.callbacks import tensorboard def keras model image x, image num classes 15 model sequential model.add conv2d 32, 5, 5 , input shape image x,image y,1 , activation 'relu' model.add maxpooling2d pool size 2, 2 , strides 2, 2 , padding 'same' model.add conv2d 64, 5, 5 , activation 'relu' model.add maxpooling2d pool size 2, 2 , strides 2, 2 , padding 'same' model.add flatten model.add dense 512, activation 'relu' model.add dropout 0.6 model.add dense 128, activation 'relu' model.add dropout 0.6 model.add dense num classes, activation 'softmax' model.compile loss 'categorical crossentropy', optimizer 'adam', metrics 'accuracy' filepath quickdraw.h5 checkpoint modelcheckpoint filepath, monitor 'val acc', verbose 1, save best true, mode 'max' callbacks list checkpoint return model, callbacks list def loadfrompickle open features , rb f features np.array pickle.load f open labels , rb f labels np.array pickle.load f return features, labels def augmentdata features, labels features np.append features, features , , 1 , axis 0 labels np.append labels, labels, axis 0 return features, labels def prepress labels labels labels np utils.to categorical labels return labels def main features, labels loadfrompickle features, labels augmentdata features, labels features, labels shuffle features, labels labels prepress labels labels train x, test x, train y, test train test split features, labels, random state 0, test size 0.1 train x train x.reshape train x.shape 0 , 28, 28, 1 test x test x.reshape test x.shape 0 , 28, 28, 1 model, callbacks list keras model 28,28 print summary model model.fit train x, train y, validation data test x, test , epochs 3, batch size 64, callbacks tensorboard log dir quickdraw model.save 'quickdraw.h5' main"
keras,11195,"code bellow keras doucumentation example keras.layers import conv2d, maxpooling2d, input input img input shape 256, 256, 3 tower 1 conv2d 64, 1, 1 , padding 'same', activation 'relu' input img tower 1 conv2d 64, 3, 3 , padding 'same', activation 'relu' tower 1 tower 2 conv2d 64, 1, 1 , padding 'same', activation 'relu' input img tower 2 conv2d 64, 5, 5 , padding 'same', activation 'relu' tower 2 tower 3 maxpooling2d 3, 3 , strides 1, 1 , padding 'same' input img tower 3 conv2d 64, 1, 1 , padding 'same', activation 'relu' tower 3 output keras.layers.concatenate tower 1, tower 2, tower 3 , axis 1 axis 1, output shape ?, 768, 256, 64 ? think axis 1, output shape none, 256 ,256, 192 .",0,inception module example kears doucumentation understand,"inception module example kears doucumentation understand code bellow keras doucumentation example keras.layers import conv2d, maxpooling2d, input input img input shape 256, 256, 3 tower 1 conv2d 64, 1, 1 , padding 'same', activation 'relu' input img tower 1 conv2d 64, 3, 3 , padding 'same', activation 'relu' tower 1 tower 2 conv2d 64, 1, 1 , padding 'same', activation 'relu' input img tower 2 conv2d 64, 5, 5 , padding 'same', activation 'relu' tower 2 tower 3 maxpooling2d 3, 3 , strides 1, 1 , padding 'same' input img tower 3 conv2d 64, 1, 1 , padding 'same', activation 'relu' tower 3 output keras.layers.concatenate tower 1, tower 2, tower 3 , axis 1 axis 1, output shape ?, 768, 256, 64 ? think axis 1, output shape none, 256 ,256, 192 ."
keras,11288,"hi, implemented generator sequence object. using without multi processing, works fine. times also job using multiprocessing true fit generator . however, sometimes unfortunately reproducible using multiprocessing true, program exit training finished. reaches last line print time needed training exit. kill ctrl c. times, processes still running tensorflow still occupies gpu memory. kill processes manually kill 9 id . think might problem generator https gist.github.com thorstenwagner 8033f43b99d1d3a1a6a31b054d91e7fc however, cannot nail specific line. best make multi threading save. observed problem keras 2.2.0, 2.2.2 2.2.3. tested tensorflow gpu 1.8.0 tensorflow gpu 1.10.1 backend. problem python 2 python 3.6. ideas might problem? best, thorsten",0,python sometimes exit using multiprocessing true fit generator,"python sometimes exit using multiprocessing true fit generator hi, implemented generator sequence object. using without multi processing, works fine. times also job using multiprocessing true fit generator . however, sometimes unfortunately reproducible using multiprocessing true, program exit training finished. reaches last line print time needed training exit. kill ctrl c. times, processes still running tensorflow still occupies gpu memory. kill processes manually kill 9 id . think might problem generator https gist.github.com thorstenwagner 8033f43b99d1d3a1a6a31b054d91e7fc however, cannot nail specific line. best make multi threading save. observed problem keras 2.2.0, 2.2.2 2.2.3. tested tensorflow gpu 1.8.0 tensorflow gpu 1.10.1 backend. problem python 2 python 3.6. ideas might problem? best, thorsten"
keras,12483,"first all, huge thanks effort. 2 submodels , form full using stacking logically series . mean accepts output plus extra input tensor output output full . full created successfully also able use . however, want parallelize training running 2 gpus, thus use fails error assertionerror could compute output tensor model 2 dense decoder truediv 0 , shape ?, 33, 22 , dtype float32 tried parallelizing two submodels individually using , yet succeed . problem appears full model. using tensorflow 1.12.0 keras 2.2.4 . snippet demonstrates problem least machine believe problem might similar 9599 mistaken.",0,assertionerror could compute output tensor using multi gpu model,"assertionerror could compute output tensor using multi gpu model first all, huge thanks effort. 2 submodels , form full using stacking logically series . mean accepts output plus extra input tensor output output full . full created successfully also able use . however, want parallelize training running 2 gpus, thus use fails error assertionerror could compute output tensor model 2 dense decoder truediv 0 , shape ?, 33, 22 , dtype float32 tried parallelizing two submodels individually using , yet succeed . problem appears full model. using tensorflow 1.12.0 keras 2.2.4 . snippet demonstrates problem least machine believe problem might similar 9599 mistaken."
keras,13115,"requesting built keras layer resizing tensors. today implement layer either lambda custom layer, simply wrap . causes huge headaches exporting coreml tf lite. existing layers like meet needs, since downsampling fixed output size, scaling factor work. coreml built layer, difficult connect custom keras layer coreml built in. ideal solution built keras layer, ask coreml team use it, implement new layer conversion mapping, convert model without digging guts keras coreml.",0,feature request built resizebilinear layer,"feature request built resizebilinear layer requesting built keras layer resizing tensors. today implement layer either lambda custom layer, simply wrap . causes huge headaches exporting coreml tf lite. existing layers like meet needs, since downsampling fixed output size, scaling factor work. coreml built layer, difficult connect custom keras layer coreml built in. ideal solution built keras layer, ask coreml team use it, implement new layer conversion mapping, convert model without digging guts keras coreml."
keras,12596,"keras.models import load model model load model 'atrs' notsanta, santa model.predict r 0 got shape ry 1, 1, 224, 224, 3 inspite adding one dimension gave two dimensions case wanted know shape r 400, 650, 3",0,"error checking input expected conv2d 21 input 4 dimensions, got array shape 1, 1, 224, 224, 3","error checking input expected conv2d 21 input 4 dimensions, got array shape 1, 1, 224, 224, 3 keras.models import load model model load model 'atrs' notsanta, santa model.predict r 0 got shape ry 1, 1, 224, 224, 3 inspite adding one dimension gave two dimensions case wanted know shape r 400, 650, 3"
keras,13375,"trying classify video 3 different classes. video different length frame. training data shape 104, none, 528 104 number videos none number frames video different 528 number features frame sequence frames video long using stateful lstm manage length sequences. defined model def lstm model model sequential model.add lstm units 256, input shape none, 528 , return sequences false, stateful true, batch size 1 model.add dropout 0.4 model.add dense 3, activation 'softmax' opt keras.optimizers.sgd lr 0.00005, decay 1e 6, momentum 0.9, nesterov true model.compile loss 'categorical crossentropy', optimizer opt, metrics 'accuracy' model.summary return model trained tested model def train model x, y, x test, test, model np.random.seed 200 epochs 100 maxlen 500 epoch range epochs mean tr loss, mean tr acc , print 'epoch ', epoch 1 sbj range x.shape 0 video x sbj sbj sbj, new sbj nb frame video.shape 0 count range nb frame maxlen 1 count nb frame maxlen seq video count maxlen count , else seq video count maxlen count count 1 maxlen count, seq np.expand dims seq, axis 0 ''' using train batch ''' tr loss, tr acc model.train batch seq, np.array new mean tr loss.append tr loss mean tr acc.append tr acc print 'training subject', sbj 1, 'done' model.reset states print 'accuracy training '.format np.mean mean tr acc print 'loss training '.format np.mean mean tr loss print ' ' print 'testing....' mean te loss, mean te acc , sbj test range x test.shape 0 video test x test sbj test new test test sbj test nb frame test video test.shape 0 range nb frame test maxlen 1 nb frame test maxlen seq test video test maxlen , else seq test video test maxlen 1 maxlen i, seq test np.expand dims seq test, axis 0 te loss, te acc model.test batch seq test, np.array new test mean te loss.append te loss mean te acc.append te acc print 'testing subject', sbj test 1, 'done' model.reset states print 'accuracy testing '.format np.mean mean te acc print 'loss testing '.format np.mean mean te loss code considered video separately video divided different frame sequences length 500 except last sequence frame video length frames divisible 500 . training accuracy test accuracy below. epoch1 accuracy training 0.3694 accuracy testing 0.3927 loss training 1.146 loss testing 1.109 epoch2 accuracy training 0.4423 accuracy testing 0.4048 loss training 1.053 loss testing 1.109 epoch3 accuracy training 0.5017 accuracy testing 0.4236 loss training 0.994 loss testing 1.115 epoch4 accuracy training 0.5491 accuracy testing 0.4099 loss training 0.94 loss testing 1.124 epoch5 accuracy training 0.5612 accuracy testing 0.4013 loss training 0.924 loss testing 1.128 epoch6 accuracy training 0.6142 accuracy testing 0.4113 loss training 0.859 loss testing 1.137 epoch7 accuracy training 0.6263 accuracy testing 0.4116 loss training 0.824 loss testing 1.142 epoch8 accuracy training 0.6659 accuracy testing 0.415 loss training 0.775 loss testing 1.152 100 epochs training accuracy increases testing accuracy improve. case overfitting adding dropout layer help didn't. so, confused cause. idea suggestion would appreciated.",0,"video classification using stateful lstm , validation accuracy improve","video classification using stateful lstm , validation accuracy improve trying classify video 3 different classes. video different length frame. training data shape 104, none, 528 104 number videos none number frames video different 528 number features frame sequence frames video long using stateful lstm manage length sequences. defined model def lstm model model sequential model.add lstm units 256, input shape none, 528 , return sequences false, stateful true, batch size 1 model.add dropout 0.4 model.add dense 3, activation 'softmax' opt keras.optimizers.sgd lr 0.00005, decay 1e 6, momentum 0.9, nesterov true model.compile loss 'categorical crossentropy', optimizer opt, metrics 'accuracy' model.summary return model trained tested model def train model x, y, x test, test, model np.random.seed 200 epochs 100 maxlen 500 epoch range epochs mean tr loss, mean tr acc , print 'epoch ', epoch 1 sbj range x.shape 0 video x sbj sbj sbj, new sbj nb frame video.shape 0 count range nb frame maxlen 1 count nb frame maxlen seq video count maxlen count , else seq video count maxlen count count 1 maxlen count, seq np.expand dims seq, axis 0 ''' using train batch ''' tr loss, tr acc model.train batch seq, np.array new mean tr loss.append tr loss mean tr acc.append tr acc print 'training subject', sbj 1, 'done' model.reset states print 'accuracy training '.format np.mean mean tr acc print 'loss training '.format np.mean mean tr loss print ' ' print 'testing....' mean te loss, mean te acc , sbj test range x test.shape 0 video test x test sbj test new test test sbj test nb frame test video test.shape 0 range nb frame test maxlen 1 nb frame test maxlen seq test video test maxlen , else seq test video test maxlen 1 maxlen i, seq test np.expand dims seq test, axis 0 te loss, te acc model.test batch seq test, np.array new test mean te loss.append te loss mean te acc.append te acc print 'testing subject', sbj test 1, 'done' model.reset states print 'accuracy testing '.format np.mean mean te acc print 'loss testing '.format np.mean mean te loss code considered video separately video divided different frame sequences length 500 except last sequence frame video length frames divisible 500 . training accuracy test accuracy below. epoch1 accuracy training 0.3694 accuracy testing 0.3927 loss training 1.146 loss testing 1.109 epoch2 accuracy training 0.4423 accuracy testing 0.4048 loss training 1.053 loss testing 1.109 epoch3 accuracy training 0.5017 accuracy testing 0.4236 loss training 0.994 loss testing 1.115 epoch4 accuracy training 0.5491 accuracy testing 0.4099 loss training 0.94 loss testing 1.124 epoch5 accuracy training 0.5612 accuracy testing 0.4013 loss training 0.924 loss testing 1.128 epoch6 accuracy training 0.6142 accuracy testing 0.4113 loss training 0.859 loss testing 1.137 epoch7 accuracy training 0.6263 accuracy testing 0.4116 loss training 0.824 loss testing 1.142 epoch8 accuracy training 0.6659 accuracy testing 0.415 loss training 0.775 loss testing 1.152 100 epochs training accuracy increases testing accuracy improve. case overfitting adding dropout layer help didn't. so, confused cause. idea suggestion would appreciated."
keras,10903,"hi, trying pass rgb image simulator custom neural network. source rgb generation simulator , dimension rgb image 3,144,256 . construct neural network now, rbg shape 1, 3, 144, 256 . error get rgb model.add conv2d 96, 11, 11 , strides 3, 3 , padding 'valid', activation 'relu', input shape rgb kshape, data format channels first file usr local lib python2.7 dist packages keras engine sequential.py , line 166, add layer x file usr local lib python2.7 dist packages keras engine base layer.py , line 414, call self.assert input compatibility inputs file usr local lib python2.7 dist packages keras engine base layer.py , line 311, assert input compatibility str k.ndim x valueerror input 0 incompatible layer conv2d 1 expected ndim 4, found ndim 5 keras complaining expected dimension 5 actual dimension 4? thanks",0,"valueerror input 0 incompatible layer conv2d 1 expeced ndim 4, found ndim 5","valueerror input 0 incompatible layer conv2d 1 expeced ndim 4, found ndim 5 hi, trying pass rgb image simulator custom neural network. source rgb generation simulator , dimension rgb image 3,144,256 . construct neural network now, rbg shape 1, 3, 144, 256 . error get rgb model.add conv2d 96, 11, 11 , strides 3, 3 , padding 'valid', activation 'relu', input shape rgb kshape, data format channels first file usr local lib python2.7 dist packages keras engine sequential.py , line 166, add layer x file usr local lib python2.7 dist packages keras engine base layer.py , line 414, call self.assert input compatibility inputs file usr local lib python2.7 dist packages keras engine base layer.py , line 311, assert input compatibility str k.ndim x valueerror input 0 incompatible layer conv2d 1 expected ndim 4, found ndim 5 keras complaining expected dimension 5 actual dimension 4? thanks"
keras,9845,"hello, using trained model form keras. model fit gpu memory want divide cpu gpu maybe gpu different machines. want get tf.variable cpu calculation gpu docs show place tf.variable cpu calculation gpu. taking pre trained keras model . want separate layers pre trained model different gpu different machines. correct use add layers sequence model like thank much.",0,keras trained model using different tf device,"keras trained model using different tf device hello, using trained model form keras. model fit gpu memory want divide cpu gpu maybe gpu different machines. want get tf.variable cpu calculation gpu docs show place tf.variable cpu calculation gpu. taking pre trained keras model . want separate layers pre trained model different gpu different machines. correct use add layers sequence model like thank much."
keras,10577,"traceback recent call last file c users xion anaconda2 envs tensorflow gpu lib site packages tensorfl ow python client session.py , line 1322, call return fn args file c users xion anaconda2 envs tensorflow gpu lib site packages tensorfl ow python client session.py , line 1305, run fn self. extend graph file c users xion anaconda2 envs tensorflow gpu lib site packages tensorfl ow python client session.py , line 1340, extend graph tf session.extendsession self. session tensorflow.python.framework.errors impl.invalidargumenterror opkernel gistered support op 'cudnnrnn' attrs. registered devices cpu , registered kernels device 'gpu' dt half device 'gpu' dt float device 'gpu' dt double node bidirectional 1 cudnnrnn 1 cudnnrnn dt float, direction un idirectional , dropout 0, input mode linear input , training true, rnn mode gru , seed 87654321, seed2 0 bidirectional 1 transpose 2, bidirectional 1 expa nddims 3, bidirectional 1 const 1, bidirectional 1 concat 1 handling exception, another exception occurred traceback recent call last file z trader connect.py , line 157, tick file z trader connect.py , line 74, tick model1 keras.models.load model 'z productionmodel.h5' file c users xion anaconda2 envs tensorflow gpu lib site packages keras en gine saving.py , line 264, load model load weights hdf5 group f 'model weights' , model.layers file c users xion anaconda2 envs tensorflow gpu lib site packages keras en gine saving.py , line 929, load weights hdf5 group k.batch set value weight value tuples file c users xion anaconda2 envs tensorflow gpu lib site packages keras ba ckend tensorflow backend.py , line 2435, batch set value get session .run assign ops, feed dict feed dict file c users xion anaconda2 envs tensorflow gpu lib site packages keras ba ckend tensorflow backend.py , line 196, get session tf.is variable initialized v v candidate vars file c users xion anaconda2 envs tensorflow gpu lib site packages tensorfl ow python client session.py , line 900, run run metadata ptr file c users xion anaconda2 envs tensorflow gpu lib site packages tensorfl ow python client session.py , line 1135, run feed dict tensor, options, run metadata file c users xion anaconda2 envs tensorflow gpu lib site packages tensorfl ow python client session.py , line 1316, run run metadata file c users xion anaconda2 envs tensorflow gpu lib site packages tensorfl ow python client session.py , line 1335, call raise type e node def, op, message tensorflow.python.framework.errors impl.invalidargumenterror opkernel gistered support op 'cudnnrnn' attrs. registered devices cpu , registered kernels device 'gpu' dt half device 'gpu' dt float device 'gpu' dt double node bidirectional 1 cudnnrnn 1 cudnnrnn dt float, direction un idirectional , dropout 0, input mode linear input , training true, rnn mode gru , seed 87654321, seed2 0 bidirectional 1 transpose 2, bidirectional 1 expa nddims 3, bidirectional 1 const 1, bidirectional 1 concat 1 caused op 'bidirectional 1 cudnnrnn 1', defined file z trader connect.py , line 157, tick file z trader connect.py , line 74, tick model1 keras.models.load model 'z productionmodel.h5' file c users xion anaconda2 envs tensorflow gpu lib site packages keras en gine saving.py , line 261, load model model model config model config, custom objects custom objects file c users xion anaconda2 envs tensorflow gpu lib site packages keras en gine saving.py , line 335, model config return deserialize config, custom objects custom objects file c users xion anaconda2 envs tensorflow gpu lib site packages keras la yers init .py , line 55, deserialize printable module name 'layer' file c users xion anaconda2 envs tensorflow gpu lib site packages keras ut ils generic utils.py , line 145, deserialize keras object list custom objects.items file c users xion anaconda2 envs tensorflow gpu lib site packages keras en gine sequential.py , line 293, config model.add layer file c users xion anaconda2 envs tensorflow gpu lib site packages keras en gine sequential.py , line 166, add layer x file c users xion anaconda2 envs tensorflow gpu lib site packages keras la yers wrappers.py , line 426, call return super bidirectional, self . call inputs, kwargs file c users xion anaconda2 envs tensorflow gpu lib site packages keras en gine base layer.py , line 460, call output self.call inputs, kwargs file c users xion anaconda2 envs tensorflow gpu lib site packages keras la yers wrappers.py , line 505, call rev self.backward layer.call inputs, kwargs file c users xion anaconda2 envs tensorflow gpu lib site packages keras la yers cudnn recurrent.py , line 90, call output, states self. process batch inputs, initial state file c users xion anaconda2 envs tensorflow gpu lib site packages keras la yers cudnn recurrent.py , line 297, process batch training true file c users xion anaconda2 envs tensorflow gpu lib site packages tensorfl ow contrib cudnn rnn python ops cudnn rnn ops.py , line 1623, call seed self. seed file c users xion anaconda2 envs tensorflow gpu lib site packages tensorfl ow contrib cudnn rnn python ops cudnn rnn ops.py , line 1012, cudnn rnn nput c direction, dropout, seed, name file c users xion anaconda2 envs tensorflow gpu lib site packages tensorfl ow contrib cudnn rnn python ops cudnn rnn ops.py , line 926, cudnn rnn name name file c users xion anaconda2 envs tensorflow gpu lib site packages tensorfl ow python ops gen cudnn rnn ops.py , line 143, cudnn rnn training training, name name file c users xion anaconda2 envs tensorflow gpu lib site packages tensorfl ow python framework op def library.py , line 787, apply op helper op def op def file c users xion anaconda2 envs tensorflow gpu lib site packages tensorfl ow python framework ops.py , line 3392, create op op def op def file c users xion anaconda2 envs tensorflow gpu lib site packages tensorfl ow python framework ops.py , line 1718, init self. traceback self. graph. extract stack pylint disable protected access invalidargumenterror see traceback opkernel registered su pport op 'cudnnrnn' attrs. registered devices cpu , registered ker nels device 'gpu' dt half device 'gpu' dt float device 'gpu' dt double node bidirectional 1 cudnnrnn 1 cudnnrnn dt float, direction un idirectional , dropout 0, input mode linear input , training true, rnn mode gru , seed 87654321, seed2 0 bidirectional 1 transpose 2, bidirectional 1 expa nddims 3, bidirectional 1 const 1, bidirectional 1 concat 1 tensorflow gpu c users xion python z trader connect.py csv eurusd,5.c sv using tensorflow backend. 2018 07 01 20 58 02.203507 src github tensorflow tensorflow core common ru ntime gpu gpu device.cc 1356 found device 0 properties name geforce gt 530 major 2 minor 1 memoryclockrate ghz 1.399 pcibusid 0000 01 00.0 totalmemory 2.00gib freememory 1.87gib 2018 07 01 20 58 02.204507 src github tensorflow tensorflow core common ru ntime gpu gpu device.cc 1406 ignoring visible gpu device device 0, name gefo rce gt 530, pci bus id 0000 01 00.0, compute capability 2.1 cuda compute capability 2.1. minimum required cuda capability 3.0. 2018 07 01 20 58 02.204507 src github tensorflow tensorflow core common ru ntime gpu gpu device.cc 923 device interconnect streamexecutor strength 1 edge matrix 2018 07 01 20 58 02.204507 src github tensorflow tensorflow core common ru ntime gpu gpu device.cc 929 0 2018 07 01 20 58 02.204507 src github tensorflow tensorflow core common ru ntime gpu gpu device.cc 942 0 n traceback recent call last file c users xion anaconda2 envs tensorflow gpu lib site packages tensorfl ow python client session.py , line 1322, call return fn args file c users xion anaconda2 envs tensorflow gpu lib site packages tensorfl ow python client session.py , line 1305, run fn self. extend graph file c users xion anaconda2 envs tensorflow gpu lib site packages tensorfl ow python client session.py , line 1340, extend graph tf session.extendsession self. session tensorflow.python.framework.errors impl.invalidargumenterror opkernel gistered support op 'cudnnrnn' attrs. registered devices cpu , registered kernels device 'gpu' dt half device 'gpu' dt float device 'gpu' dt double node bidirectional 1 cudnnrnn 1 cudnnrnn dt double, direction u nidirectional , dropout 0, input mode linear input , training true, rnn mode gru , seed 87654321, seed2 0 bidirectional 1 transpose 2, bidirectional 1 exp anddims 3, bidirectional 1 const 1, bidirectional 1 concat 1 handling exception, another exception occurred traceback recent call last file z trader connect.py , line 157, tick file z trader connect.py , line 74, tick model1 keras.models.load model 'z productionmodel.h5' file c users xion anaconda2 envs tensorflow gpu lib site packages keras en gine saving.py , line 264, load model load weights hdf5 group f 'model weights' , model.layers file c users xion anaconda2 envs tensorflow gpu lib site packages keras en gine saving.py , line 929, load weights hdf5 group k.batch set value weight value tuples file c users xion anaconda2 envs tensorflow gpu lib site packages keras ba ckend tensorflow backend.py , line 2435, batch set value get session .run assign ops, feed dict feed dict file c users xion anaconda2 envs tensorflow gpu lib site packages keras ba ckend tensorflow backend.py , line 196, get session tf.is variable initialized v v candidate vars file c users xion anaconda2 envs tensorflow gpu lib site packages tensorfl ow python client session.py , line 900, run run metadata ptr file c users xion anaconda2 envs tensorflow gpu lib site packages tensorfl ow python client session.py , line 1135, run feed dict tensor, options, run metadata file c users xion anaconda2 envs tensorflow gpu lib site packages tensorfl ow python client session.py , line 1316, run run metadata file c users xion anaconda2 envs tensorflow gpu lib site packages tensorfl ow python client session.py , line 1335, call raise type e node def, op, message tensorflow.python.framework.errors impl.invalidargumenterror opkernel gistered support op 'cudnnrnn' attrs. registered devices cpu , registered kernels device 'gpu' dt half device 'gpu' dt float device 'gpu' dt double node bidirectional 1 cudnnrnn 1 cudnnrnn dt double, direction u nidirectional , dropout 0, input mode linear input , training true, rnn mode gru , seed 87654321, seed2 0 bidirectional 1 transpose 2, bidirectional 1 exp anddims 3, bidirectional 1 const 1, bidirectional 1 concat 1 caused op 'bidirectional 1 cudnnrnn 1', defined file z trader connect.py , line 157, tick file z trader connect.py , line 74, tick model1 keras.models.load model 'z productionmodel.h5' file c users xion anaconda2 envs tensorflow gpu lib site packages keras en gine saving.py , line 261, load model model model config model config, custom objects custom objects file c users xion anaconda2 envs tensorflow gpu lib site packages keras en gine saving.py , line 335, model config return deserialize config, custom objects custom objects file c users xion anaconda2 envs tensorflow gpu lib site packages keras la yers init .py , line 55, deserialize printable module name 'layer' file c users xion anaconda2 envs tensorflow gpu lib site packages keras ut ils generic utils.py , line 145, deserialize keras object list custom objects.items file c users xion anaconda2 envs tensorflow gpu lib site packages keras en gine sequential.py , line 293, config model.add layer file c users xion anaconda2 envs tensorflow gpu lib site packages keras en gine sequential.py , line 166, add layer x file c users xion anaconda2 envs tensorflow gpu lib site packages keras la yers wrappers.py , line 426, call return super bidirectional, self . call inputs, kwargs file c users xion anaconda2 envs tensorflow gpu lib site packages keras en gine base layer.py , line 460, call output self.call inputs, kwargs file c users xion anaconda2 envs tensorflow gpu lib site packages keras la yers wrappers.py , line 505, call rev self.backward layer.call inputs, kwargs file c users xion anaconda2 envs tensorflow gpu lib site packages keras la yers cudnn recurrent.py , line 90, call output, states self. process batch inputs, initial state file c users xion anaconda2 envs tensorflow gpu lib site packages keras la yers cudnn recurrent.py , line 297, process batch training true file c users xion anaconda2 envs tensorflow gpu lib site packages tensorfl ow contrib cudnn rnn python ops cudnn rnn ops.py , line 1623, call seed self. seed file c users xion anaconda2 envs tensorflow gpu lib site packages tensorfl ow contrib cudnn rnn python ops cudnn rnn ops.py , line 1012, cudnn rnn nput c direction, dropout, seed, name file c users xion anaconda2 envs tensorflow gpu lib site packages tensorfl ow contrib cudnn rnn python ops cudnn rnn ops.py , line 926, cudnn rnn name name file c users xion anaconda2 envs tensorflow gpu lib site packages tensorfl ow python ops gen cudnn rnn ops.py , line 143, cudnn rnn training training, name name file c users xion anaconda2 envs tensorflow gpu lib site packages tensorfl ow python framework op def library.py , line 787, apply op helper op def op def file c users xion anaconda2 envs tensorflow gpu lib site packages tensorfl ow python framework ops.py , line 3392, create op op def op def file c users xion anaconda2 envs tensorflow gpu lib site packages tensorfl ow python framework ops.py , line 1718, init self. traceback self. graph. extract stack pylint disable protected access invalidargumenterror see traceback opkernel registered su pport op 'cudnnrnn' attrs. registered devices cpu , registered ker nels device 'gpu' dt half device 'gpu' dt float device 'gpu' dt double node bidirectional 1 cudnnrnn 1 cudnnrnn dt double, direction u nidirectional , dropout 0, input mode linear input , training true, rnn mode gru , seed 87654321, seed2 0 bidirectional 1 transpose 2, bidirectional 1 exp anddims 3, bidirectional 1 const 1, bidirectional 1 concat 1 tried fresh reinstall, change float fixes? idea getting this. p.s trained model titan v trying open computer geforce gt 530 gpu.",0,failing load h5 model using tf gpu backend keras?,"failing load h5 model using tf gpu backend keras? traceback recent call last file c users xion anaconda2 envs tensorflow gpu lib site packages tensorfl ow python client session.py , line 1322, call return fn args file c users xion anaconda2 envs tensorflow gpu lib site packages tensorfl ow python client session.py , line 1305, run fn self. extend graph file c users xion anaconda2 envs tensorflow gpu lib site packages tensorfl ow python client session.py , line 1340, extend graph tf session.extendsession self. session tensorflow.python.framework.errors impl.invalidargumenterror opkernel gistered support op 'cudnnrnn' attrs. registered devices cpu , registered kernels device 'gpu' dt half device 'gpu' dt float device 'gpu' dt double node bidirectional 1 cudnnrnn 1 cudnnrnn dt float, direction un idirectional , dropout 0, input mode linear input , training true, rnn mode gru , seed 87654321, seed2 0 bidirectional 1 transpose 2, bidirectional 1 expa nddims 3, bidirectional 1 const 1, bidirectional 1 concat 1 handling exception, another exception occurred traceback recent call last file z trader connect.py , line 157, tick file z trader connect.py , line 74, tick model1 keras.models.load model 'z productionmodel.h5' file c users xion anaconda2 envs tensorflow gpu lib site packages keras en gine saving.py , line 264, load model load weights hdf5 group f 'model weights' , model.layers file c users xion anaconda2 envs tensorflow gpu lib site packages keras en gine saving.py , line 929, load weights hdf5 group k.batch set value weight value tuples file c users xion anaconda2 envs tensorflow gpu lib site packages keras ba ckend tensorflow backend.py , line 2435, batch set value get session .run assign ops, feed dict feed dict file c users xion anaconda2 envs tensorflow gpu lib site packages keras ba ckend tensorflow backend.py , line 196, get session tf.is variable initialized v v candidate vars file c users xion anaconda2 envs tensorflow gpu lib site packages tensorfl ow python client session.py , line 900, run run metadata ptr file c users xion anaconda2 envs tensorflow gpu lib site packages tensorfl ow python client session.py , line 1135, run feed dict tensor, options, run metadata file c users xion anaconda2 envs tensorflow gpu lib site packages tensorfl ow python client session.py , line 1316, run run metadata file c users xion anaconda2 envs tensorflow gpu lib site packages tensorfl ow python client session.py , line 1335, call raise type e node def, op, message tensorflow.python.framework.errors impl.invalidargumenterror opkernel gistered support op 'cudnnrnn' attrs. registered devices cpu , registered kernels device 'gpu' dt half device 'gpu' dt float device 'gpu' dt double node bidirectional 1 cudnnrnn 1 cudnnrnn dt float, direction un idirectional , dropout 0, input mode linear input , training true, rnn mode gru , seed 87654321, seed2 0 bidirectional 1 transpose 2, bidirectional 1 expa nddims 3, bidirectional 1 const 1, bidirectional 1 concat 1 caused op 'bidirectional 1 cudnnrnn 1', defined file z trader connect.py , line 157, tick file z trader connect.py , line 74, tick model1 keras.models.load model 'z productionmodel.h5' file c users xion anaconda2 envs tensorflow gpu lib site packages keras en gine saving.py , line 261, load model model model config model config, custom objects custom objects file c users xion anaconda2 envs tensorflow gpu lib site packages keras en gine saving.py , line 335, model config return deserialize config, custom objects custom objects file c users xion anaconda2 envs tensorflow gpu lib site packages keras la yers init .py , line 55, deserialize printable module name 'layer' file c users xion anaconda2 envs tensorflow gpu lib site packages keras ut ils generic utils.py , line 145, deserialize keras object list custom objects.items file c users xion anaconda2 envs tensorflow gpu lib site packages keras en gine sequential.py , line 293, config model.add layer file c users xion anaconda2 envs tensorflow gpu lib site packages keras en gine sequential.py , line 166, add layer x file c users xion anaconda2 envs tensorflow gpu lib site packages keras la yers wrappers.py , line 426, call return super bidirectional, self . call inputs, kwargs file c users xion anaconda2 envs tensorflow gpu lib site packages keras en gine base layer.py , line 460, call output self.call inputs, kwargs file c users xion anaconda2 envs tensorflow gpu lib site packages keras la yers wrappers.py , line 505, call rev self.backward layer.call inputs, kwargs file c users xion anaconda2 envs tensorflow gpu lib site packages keras la yers cudnn recurrent.py , line 90, call output, states self. process batch inputs, initial state file c users xion anaconda2 envs tensorflow gpu lib site packages keras la yers cudnn recurrent.py , line 297, process batch training true file c users xion anaconda2 envs tensorflow gpu lib site packages tensorfl ow contrib cudnn rnn python ops cudnn rnn ops.py , line 1623, call seed self. seed file c users xion anaconda2 envs tensorflow gpu lib site packages tensorfl ow contrib cudnn rnn python ops cudnn rnn ops.py , line 1012, cudnn rnn nput c direction, dropout, seed, name file c users xion anaconda2 envs tensorflow gpu lib site packages tensorfl ow contrib cudnn rnn python ops cudnn rnn ops.py , line 926, cudnn rnn name name file c users xion anaconda2 envs tensorflow gpu lib site packages tensorfl ow python ops gen cudnn rnn ops.py , line 143, cudnn rnn training training, name name file c users xion anaconda2 envs tensorflow gpu lib site packages tensorfl ow python framework op def library.py , line 787, apply op helper op def op def file c users xion anaconda2 envs tensorflow gpu lib site packages tensorfl ow python framework ops.py , line 3392, create op op def op def file c users xion anaconda2 envs tensorflow gpu lib site packages tensorfl ow python framework ops.py , line 1718, init self. traceback self. graph. extract stack pylint disable protected access invalidargumenterror see traceback opkernel registered su pport op 'cudnnrnn' attrs. registered devices cpu , registered ker nels device 'gpu' dt half device 'gpu' dt float device 'gpu' dt double node bidirectional 1 cudnnrnn 1 cudnnrnn dt float, direction un idirectional , dropout 0, input mode linear input , training true, rnn mode gru , seed 87654321, seed2 0 bidirectional 1 transpose 2, bidirectional 1 expa nddims 3, bidirectional 1 const 1, bidirectional 1 concat 1 tensorflow gpu c users xion python z trader connect.py csv eurusd,5.c sv using tensorflow backend. 2018 07 01 20 58 02.203507 src github tensorflow tensorflow core common ru ntime gpu gpu device.cc 1356 found device 0 properties name geforce gt 530 major 2 minor 1 memoryclockrate ghz 1.399 pcibusid 0000 01 00.0 totalmemory 2.00gib freememory 1.87gib 2018 07 01 20 58 02.204507 src github tensorflow tensorflow core common ru ntime gpu gpu device.cc 1406 ignoring visible gpu device device 0, name gefo rce gt 530, pci bus id 0000 01 00.0, compute capability 2.1 cuda compute capability 2.1. minimum required cuda capability 3.0. 2018 07 01 20 58 02.204507 src github tensorflow tensorflow core common ru ntime gpu gpu device.cc 923 device interconnect streamexecutor strength 1 edge matrix 2018 07 01 20 58 02.204507 src github tensorflow tensorflow core common ru ntime gpu gpu device.cc 929 0 2018 07 01 20 58 02.204507 src github tensorflow tensorflow core common ru ntime gpu gpu device.cc 942 0 n traceback recent call last file c users xion anaconda2 envs tensorflow gpu lib site packages tensorfl ow python client session.py , line 1322, call return fn args file c users xion anaconda2 envs tensorflow gpu lib site packages tensorfl ow python client session.py , line 1305, run fn self. extend graph file c users xion anaconda2 envs tensorflow gpu lib site packages tensorfl ow python client session.py , line 1340, extend graph tf session.extendsession self. session tensorflow.python.framework.errors impl.invalidargumenterror opkernel gistered support op 'cudnnrnn' attrs. registered devices cpu , registered kernels device 'gpu' dt half device 'gpu' dt float device 'gpu' dt double node bidirectional 1 cudnnrnn 1 cudnnrnn dt double, direction u nidirectional , dropout 0, input mode linear input , training true, rnn mode gru , seed 87654321, seed2 0 bidirectional 1 transpose 2, bidirectional 1 exp anddims 3, bidirectional 1 const 1, bidirectional 1 concat 1 handling exception, another exception occurred traceback recent call last file z trader connect.py , line 157, tick file z trader connect.py , line 74, tick model1 keras.models.load model 'z productionmodel.h5' file c users xion anaconda2 envs tensorflow gpu lib site packages keras en gine saving.py , line 264, load model load weights hdf5 group f 'model weights' , model.layers file c users xion anaconda2 envs tensorflow gpu lib site packages keras en gine saving.py , line 929, load weights hdf5 group k.batch set value weight value tuples file c users xion anaconda2 envs tensorflow gpu lib site packages keras ba ckend tensorflow backend.py , line 2435, batch set value get session .run assign ops, feed dict feed dict file c users xion anaconda2 envs tensorflow gpu lib site packages keras ba ckend tensorflow backend.py , line 196, get session tf.is variable initialized v v candidate vars file c users xion anaconda2 envs tensorflow gpu lib site packages tensorfl ow python client session.py , line 900, run run metadata ptr file c users xion anaconda2 envs tensorflow gpu lib site packages tensorfl ow python client session.py , line 1135, run feed dict tensor, options, run metadata file c users xion anaconda2 envs tensorflow gpu lib site packages tensorfl ow python client session.py , line 1316, run run metadata file c users xion anaconda2 envs tensorflow gpu lib site packages tensorfl ow python client session.py , line 1335, call raise type e node def, op, message tensorflow.python.framework.errors impl.invalidargumenterror opkernel gistered support op 'cudnnrnn' attrs. registered devices cpu , registered kernels device 'gpu' dt half device 'gpu' dt float device 'gpu' dt double node bidirectional 1 cudnnrnn 1 cudnnrnn dt double, direction u nidirectional , dropout 0, input mode linear input , training true, rnn mode gru , seed 87654321, seed2 0 bidirectional 1 transpose 2, bidirectional 1 exp anddims 3, bidirectional 1 const 1, bidirectional 1 concat 1 caused op 'bidirectional 1 cudnnrnn 1', defined file z trader connect.py , line 157, tick file z trader connect.py , line 74, tick model1 keras.models.load model 'z productionmodel.h5' file c users xion anaconda2 envs tensorflow gpu lib site packages keras en gine saving.py , line 261, load model model model config model config, custom objects custom objects file c users xion anaconda2 envs tensorflow gpu lib site packages keras en gine saving.py , line 335, model config return deserialize config, custom objects custom objects file c users xion anaconda2 envs tensorflow gpu lib site packages keras la yers init .py , line 55, deserialize printable module name 'layer' file c users xion anaconda2 envs tensorflow gpu lib site packages keras ut ils generic utils.py , line 145, deserialize keras object list custom objects.items file c users xion anaconda2 envs tensorflow gpu lib site packages keras en gine sequential.py , line 293, config model.add layer file c users xion anaconda2 envs tensorflow gpu lib site packages keras en gine sequential.py , line 166, add layer x file c users xion anaconda2 envs tensorflow gpu lib site packages keras la yers wrappers.py , line 426, call return super bidirectional, self . call inputs, kwargs file c users xion anaconda2 envs tensorflow gpu lib site packages keras en gine base layer.py , line 460, call output self.call inputs, kwargs file c users xion anaconda2 envs tensorflow gpu lib site packages keras la yers wrappers.py , line 505, call rev self.backward layer.call inputs, kwargs file c users xion anaconda2 envs tensorflow gpu lib site packages keras la yers cudnn recurrent.py , line 90, call output, states self. process batch inputs, initial state file c users xion anaconda2 envs tensorflow gpu lib site packages keras la yers cudnn recurrent.py , line 297, process batch training true file c users xion anaconda2 envs tensorflow gpu lib site packages tensorfl ow contrib cudnn rnn python ops cudnn rnn ops.py , line 1623, call seed self. seed file c users xion anaconda2 envs tensorflow gpu lib site packages tensorfl ow contrib cudnn rnn python ops cudnn rnn ops.py , line 1012, cudnn rnn nput c direction, dropout, seed, name file c users xion anaconda2 envs tensorflow gpu lib site packages tensorfl ow contrib cudnn rnn python ops cudnn rnn ops.py , line 926, cudnn rnn name name file c users xion anaconda2 envs tensorflow gpu lib site packages tensorfl ow python ops gen cudnn rnn ops.py , line 143, cudnn rnn training training, name name file c users xion anaconda2 envs tensorflow gpu lib site packages tensorfl ow python framework op def library.py , line 787, apply op helper op def op def file c users xion anaconda2 envs tensorflow gpu lib site packages tensorfl ow python framework ops.py , line 3392, create op op def op def file c users xion anaconda2 envs tensorflow gpu lib site packages tensorfl ow python framework ops.py , line 1718, init self. traceback self. graph. extract stack pylint disable protected access invalidargumenterror see traceback opkernel registered su pport op 'cudnnrnn' attrs. registered devices cpu , registered ker nels device 'gpu' dt half device 'gpu' dt float device 'gpu' dt double node bidirectional 1 cudnnrnn 1 cudnnrnn dt double, direction u nidirectional , dropout 0, input mode linear input , training true, rnn mode gru , seed 87654321, seed2 0 bidirectional 1 transpose 2, bidirectional 1 exp anddims 3, bidirectional 1 const 1, bidirectional 1 concat 1 tried fresh reinstall, change float fixes? idea getting this. p.s trained model titan v trying open computer geforce gt 530 gpu."
keras,12504,"various blog posts, stackoverflow answers tweets say like easy convert keras model tensorflow keras model used tensorflow tpu wrapper get model support tpu. that's easy.. custom layers? convert custom layers get trained tpus? used keras backend functions? surely things messing getting nothing errors. plans releasing keras tpu support? yes, give estimated date till users wait?",0,way keras officially supporting colab tpus?,"way keras officially supporting colab tpus? various blog posts, stackoverflow answers tweets say like easy convert keras model tensorflow keras model used tensorflow tpu wrapper get model support tpu. that's easy.. custom layers? convert custom layers get trained tpus? used keras backend functions? surely things messing getting nothing errors. plans releasing keras tpu support? yes, give estimated date till users wait?"
keras,12370,"hi! glad see keras supported stateful metrics. using multi class average recall research projects past. used calculate callbacks, finally add metric. posted implementation gist https gist.github.com hawkinszhao 766305acfb0141b70370e5dcd9415eb6 . also, found keras going update basic metrics apis 12149. think work? submit pr time? multi class average recall widely used metric affective computing many research problems. besides, lot discussion 9393. think could certainly benefit lots people.",0,feature request multi class average recall metric,"feature request multi class average recall metric hi! glad see keras supported stateful metrics. using multi class average recall research projects past. used calculate callbacks, finally add metric. posted implementation gist https gist.github.com hawkinszhao 766305acfb0141b70370e5dcd9415eb6 . also, found keras going update basic metrics apis 12149. think work? submit pr time? multi class average recall widely used metric affective computing many research problems. besides, lot discussion 9393. think could certainly benefit lots people."
keras,10074,"please make sure boxes checked submit issue. issue implementation question, please ask question stackoverflow http stackoverflow.com questions tagged keras join keras slack channel https keras slack autojoin.herokuapp.com ask instead filing github issue. thank you! check date master branch keras. update pip install git git github.com keras team keras.git upgrade deps x running tensorflow, check date latest version. installation instructions found https www.tensorflow.org get started os setup . running theano, check date master branch theano. update pip install git git github.com theano theano.git upgrade deps provide link github gist python script reproduce issue copy script short . trying train model composed submodels, using tesnorboard callback. histogram freq set, there's error missing input submodel using tensorflow 1.8 error also older versions sample code reproduces error error reason using submodels need also run separately",0,'you must feed value placeholder tensor' using tensorboard callback submodels,"'you must feed value placeholder tensor' using tensorboard callback submodels please make sure boxes checked submit issue. issue implementation question, please ask question stackoverflow http stackoverflow.com questions tagged keras join keras slack channel https keras slack autojoin.herokuapp.com ask instead filing github issue. thank you! check date master branch keras. update pip install git git github.com keras team keras.git upgrade deps x running tensorflow, check date latest version. installation instructions found https www.tensorflow.org get started os setup . running theano, check date master branch theano. update pip install git git github.com theano theano.git upgrade deps provide link github gist python script reproduce issue copy script short . trying train model composed submodels, using tesnorboard callback. histogram freq set, there's error missing input submodel using tensorflow 1.8 error also older versions sample code reproduces error error reason using submodels need also run separately"
keras,9455,"completely confident issue title, find plausible would love someone keras community helped verify fix issue. here's steps reproduce 1. build model one batch norm layers. case functional api model, although doubt necessary reproduce. 2. run model.predict wrap timer sort 3. normalization.py 174 master add 4. run model.predict 5. observe model runs significantly faster. model get 3x speed increase.",0,batchnormalization calculates batch mean variance even inference mode,"batchnormalization calculates batch mean variance even inference mode completely confident issue title, find plausible would love someone keras community helped verify fix issue. here's steps reproduce 1. build model one batch norm layers. case functional api model, although doubt necessary reproduce. 2. run model.predict wrap timer sort 3. normalization.py 174 master add 4. run model.predict 5. observe model runs significantly faster. model get 3x speed increase."
keras,11703,"seems issue model.fit relation loading previous weights. problems basically seen logging epoch 00001 val acc improved inf 0.95089, saving model chicken weights.best.hdf5 every fit starts inf , negative infinitive thus ignoring last loaded weights results previous runs. overwrite best trainings previous runs. epoch 1 worse would still overwrite past saved results. due inf , fit cannot improve past runs.",0,train.fit epoch1 uses inf,"train.fit epoch1 uses inf seems issue model.fit relation loading previous weights. problems basically seen logging epoch 00001 val acc improved inf 0.95089, saving model chicken weights.best.hdf5 every fit starts inf , negative infinitive thus ignoring last loaded weights results previous runs. overwrite best trainings previous runs. epoch 1 worse would still overwrite past saved results. due inf , fit cannot improve past runs."
keras,12956,"configuration library version pyhton 3.6.8 gcc 7.3.0 tensorflow base tensorflow gpu 1.13.1 keras gpu keras base 2.2.4 theano 1.0.4 cudnn 7.6.0 cudatoolkit 10.0.130 machine configuration ubuntu 18.04.2 lts intel r xeon r cpu e5 2640 v4 2.40ghz 128gb ram cuda version 10.2 double geforce gtx 1080ti description use clone resnet50 model use set weights layer. find time consumption iteration becomes much larger time goes by. first iteration, clone stage takes 4 seconds setting weights stage takes 3 seconds turn 34 seconds 4 minutes 50th iteration. problem usage? would memory leak keras? hope hear soon. thanks advance. code shown bellow partial result ! image https user images.githubusercontent.com 17698785 59478187 019a5280 8e8b 11e9 8901 3e01a0b9daa4.png ! image https user images.githubusercontent.com 17698785 59478193 08c16080 8e8b 11e9 9e83 a595c0fe2e35.png",0,memory leak keras functions clone model set weights become slower slower used loop,"memory leak keras functions clone model set weights become slower slower used loop configuration library version pyhton 3.6.8 gcc 7.3.0 tensorflow base tensorflow gpu 1.13.1 keras gpu keras base 2.2.4 theano 1.0.4 cudnn 7.6.0 cudatoolkit 10.0.130 machine configuration ubuntu 18.04.2 lts intel r xeon r cpu e5 2640 v4 2.40ghz 128gb ram cuda version 10.2 double geforce gtx 1080ti description use clone resnet50 model use set weights layer. find time consumption iteration becomes much larger time goes by. first iteration, clone stage takes 4 seconds setting weights stage takes 3 seconds turn 34 seconds 4 minutes 50th iteration. problem usage? would memory leak keras? hope hear soon. thanks advance. code shown bellow partial result ! image https user images.githubusercontent.com 17698785 59478187 019a5280 8e8b 11e9 8901 3e01a0b9daa4.png ! image https user images.githubusercontent.com 17698785 59478193 08c16080 8e8b 11e9 9e83 a595c0fe2e35.png"
keras,10882,code package versions keras 2.2.0 keras applications 1.0.2 keras preprocessing 1.0.1 tensorflow gpu 1.9.0,0,unboundlocalerror local variable 'x' referenced assignment using load model,unboundlocalerror local variable 'x' referenced assignment using load model code package versions keras 2.2.0 keras applications 1.0.2 keras preprocessing 1.0.1 tensorflow gpu 1.9.0
keras,5014,"model fitted model.however load .h5 file,i got error. guess may caused complicacy model?",0,error loading saved model?,"error loading saved model? model fitted model.however load .h5 file,i got error. guess may caused complicacy model?"
keras,8423,"summary model imdb cnn sample, anyone tell purpose dense 6 layer? embedding 6 embedding none, 400, 50 250000 dropout 5 dropout none, 400, 50 0 conv1d 4 conv1d none, 398, 250 37750 global max pooling1d 5 glob none, 250 0 dense 6 dense none, 250 62750 dropout 6 dropout none, 250 0 activation 5 activation none, 250 0 dense 7 dense none, 1 251 activation 6 activation none, 1 0 total params 350,751 trainable params 350,751 non trainable params 0",0,use dense layer imdb cnn sample,"use dense layer imdb cnn sample summary model imdb cnn sample, anyone tell purpose dense 6 layer? embedding 6 embedding none, 400, 50 250000 dropout 5 dropout none, 400, 50 0 conv1d 4 conv1d none, 398, 250 37750 global max pooling1d 5 glob none, 250 0 dense 6 dense none, 250 62750 dropout 6 dropout none, 250 0 activation 5 activation none, 250 0 dense 7 dense none, 1 251 activation 6 activation none, 1 0 total params 350,751 trainable params 350,751 non trainable params 0"
keras,11493,"hi, feature request think could interesting several users wish share it, able implement it. field scientific computing especially computational fluid dynamic , attempts use neural network speed expensive iterative computations. done two ways improving quality initialisation computation neural network computation based jacobian evaluation newton raphson , estimating jacobian neural network. cases, quite common access derivatives function one want learn, therefore another kind training used sobolev training https arxiv.org abs 1706.04859 training, cost function dependant output values neural network ground truth, term depending derivatives outputs regards inputs added. requires provide also ground truth derivatives learning process. would great possibility provide derivatives values keras call like matrices derivatives outputs regards inputs. quality training improved, addition knowledge output value, one use also available knowledge derivative outputs regards inputs. would require compute also learning process derivative output network regards inputs, order able gradient based update. learning process achieved, outputs values derivatives values would learn single neural networks accessing values derivatives would adding value information gathered training, means able call best regards,",0,introduction sobolev training capability keras,"introduction sobolev training capability keras hi, feature request think could interesting several users wish share it, able implement it. field scientific computing especially computational fluid dynamic , attempts use neural network speed expensive iterative computations. done two ways improving quality initialisation computation neural network computation based jacobian evaluation newton raphson , estimating jacobian neural network. cases, quite common access derivatives function one want learn, therefore another kind training used sobolev training https arxiv.org abs 1706.04859 training, cost function dependant output values neural network ground truth, term depending derivatives outputs regards inputs added. requires provide also ground truth derivatives learning process. would great possibility provide derivatives values keras call like matrices derivatives outputs regards inputs. quality training improved, addition knowledge output value, one use also available knowledge derivative outputs regards inputs. would require compute also learning process derivative output network regards inputs, order able gradient based update. learning process achieved, outputs values derivatives values would learn single neural networks accessing values derivatives would adding value information gathered training, means able call best regards,"
keras,13098,"1. problem description design input layers via assigning tensors including constants, variables etc. avoid direct input, reloading model, training predicting valid. however, saving model reloading it, input layers via assigning tensors loaded direct input layers without constant variable tensors. want load originally designed model designing loss. 2. code example input1 input shape 10, 1 input2 input tensor k.random normal variable 10, 1 , 0, 1 input3 input tensor k.random normal 10, 1 x lambda lambda x x 0 x 1 x 2 input1, input2, input3 model model inputs input1, input2, input3 , outputs x model.inputs , , model.save 'example model.h5' r model load model 'example model.h5' r model.inputs , ,",0,reloading problem input layers assigned tensor,"reloading problem input layers assigned tensor 1. problem description design input layers via assigning tensors including constants, variables etc. avoid direct input, reloading model, training predicting valid. however, saving model reloading it, input layers via assigning tensors loaded direct input layers without constant variable tensors. want load originally designed model designing loss. 2. code example input1 input shape 10, 1 input2 input tensor k.random normal variable 10, 1 , 0, 1 input3 input tensor k.random normal 10, 1 x lambda lambda x x 0 x 1 x 2 input1, input2, input3 model model inputs input1, input2, input3 , outputs x model.inputs , , model.save 'example model.h5' r model load model 'example model.h5' r model.inputs , ,"
keras,10387,"please make sure boxes checked submit issue. issue implementation question, please ask question stackoverflow http stackoverflow.com questions tagged keras join keras slack channel https keras slack autojoin.herokuapp.com ask instead filing github issue. thank you! x check date master branch keras. update pip install git git github.com keras team keras.git upgrade deps x running tensorflow, check date latest version. installation instructions found https www.tensorflow.org get started os setup . running theano, check date master branch theano. update pip install git git github.com theano theano.git upgrade deps x provide link github gist python script reproduce issue copy script short . hello. problem memory error. wrote next script that, set value steps per epoch validation steps, get messages next type 2018 06 09 23 59 16.234583 w tensorflow core framework allocator.cc 101 allocation 1507688448 exceeds 10 system memory. found information problem. please, help me.",0,validation st,"validation st please make sure boxes checked submit issue. issue implementation question, please ask question stackoverflow http stackoverflow.com questions tagged keras join keras slack channel https keras slack autojoin.herokuapp.com ask instead filing github issue. thank you! x check date master branch keras. update pip install git git github.com keras team keras.git upgrade deps x running tensorflow, check date latest version. installation instructions found https www.tensorflow.org get started os setup . running theano, check date master branch theano. update pip install git git github.com theano theano.git upgrade deps x provide link github gist python script reproduce issue copy script short . hello. problem memory error. wrote next script that, set value steps per epoch validation steps, get messages next type 2018 06 09 23 59 16.234583 w tensorflow core framework allocator.cc 101 allocation 1507688448 exceeds 10 system memory. found information problem. please, help me."
keras,12269,"tensorflow version 1.5 keras version 2.2.4 everything ok train model gpu machine, something happens training predicting model cpu machine. seems embedding layser cannot recognize masking value 1. codes preprocess list word intergers padding filling 1. error again, everything fine gpu machine. what's wrong code running cpu machine ?",0,masking layer conflict embedding layer,"masking layer conflict embedding layer tensorflow version 1.5 keras version 2.2.4 everything ok train model gpu machine, something happens training predicting model cpu machine. seems embedding layser cannot recognize masking value 1. codes preprocess list word intergers padding filling 1. error again, everything fine gpu machine. what's wrong code running cpu machine ?"
keras,12524,"hello, would like create either deep dense neural net n outputs activation function example, 3 numbers output b rnn internally performs based observed effects. so, want say, feed stationary values x 1 1 reconsider loss value three inputs, order train optimise function issue need transform output loss given, eg loss true loss network function accepts three outputs network step a, b c , rounded integers function returns float 0.0 100.0 0.0 worst 100.0 best possible value possible get network learn function true loss? trying days implement it. return value function code, input code inside custom loss function executes start tf graphs . way replace loss function using lambda layer, custom loss function even callback? sure this, even possible ? question seems aiming somewhat similar goal https github.com keras team keras issues 2691 unfortunately quite good enough keras yet! apply problem help would much appreciated, thanks ps know genetic pso would probably better project show effects method results multiple problems",0,neural networks input dummy optimise transformed output?,"neural networks input dummy optimise transformed output? hello, would like create either deep dense neural net n outputs activation function example, 3 numbers output b rnn internally performs based observed effects. so, want say, feed stationary values x 1 1 reconsider loss value three inputs, order train optimise function issue need transform output loss given, eg loss true loss network function accepts three outputs network step a, b c , rounded integers function returns float 0.0 100.0 0.0 worst 100.0 best possible value possible get network learn function true loss? trying days implement it. return value function code, input code inside custom loss function executes start tf graphs . way replace loss function using lambda layer, custom loss function even callback? sure this, even possible ? question seems aiming somewhat similar goal https github.com keras team keras issues 2691 unfortunately quite good enough keras yet! apply problem help would much appreciated, thanks ps know genetic pso would probably better project show effects method results multiple problems"
keras,13637,parameter works ? testing code result keras 2.3.1 keras preprocessing 1.1.0 ! image https user images.githubusercontent.com 16003088 70897626 f8eb2a80 202d 11ea 9898 1320dcad52a1.png,0,classes working flow dataframe?,classes working flow dataframe? parameter works ? testing code result keras 2.3.1 keras preprocessing 1.1.0 ! image https user images.githubusercontent.com 16003088 70897626 f8eb2a80 202d 11ea 9898 1320dcad52a1.png
keras,8315,"hey there, converted mlp network lstm network check get better accuracy taking time consideration. sound signals split multiple frames 87 frames sound signal 39 features generated frames mel frequency cepstrum coefficients . mlp network input dimension 39 feautres lstm network reshape vector 3d tensor decided 87 frames timesteps. might make sense put value higher . reshaped vector . output vector also reshaped add wrapper around output layer many many categories nature classification problem. split reshape part code creation model depending timesteps dimensionalty 3d tensor changes. keras function categorical changes 3d tensor back 2d tensor, reshape again. another way tho accomplish this? greetings",0,categorical time sequence lstm,"categorical time sequence lstm hey there, converted mlp network lstm network check get better accuracy taking time consideration. sound signals split multiple frames 87 frames sound signal 39 features generated frames mel frequency cepstrum coefficients . mlp network input dimension 39 feautres lstm network reshape vector 3d tensor decided 87 frames timesteps. might make sense put value higher . reshaped vector . output vector also reshaped add wrapper around output layer many many categories nature classification problem. split reshape part code creation model depending timesteps dimensionalty 3d tensor changes. keras function categorical changes 3d tensor back 2d tensor, reshape again. another way tho accomplish this? greetings"
keras,9622,"official document resnet50 describes needs size picture must 197 197 least. however, every row data talks sample feature 737. then, use model resnet50 object? thank",0,questions resnet50,"questions resnet50 official document resnet50 describes needs size picture must 197 197 least. however, every row data talks sample feature 737. then, use model resnet50 object? thank"
keras,11097,"hi all, last weeks successful setting convolutional lstm using layers provided keras https github.com keras team keras blob master keras layers convolutional recurrent.py . another task past experiments phased lstm https arxiv.org abs 1610.09513 , also implemented keras tensorflow. now, things mind 1. phased lstm model seems really useful many purposes. yet, get really popular last years, wondering potential reasons. miss strong counter arguments, successful alternative approaches tackle issues like input different sources irregular sampling intervals ? 2. practice, two methods compatible, right? time gate could also included lstm cnn setting, analogous application classical lstm. yet, since find implementation, wonder might hurdles aware of. 3. first points disqualify idea already, feel free get contact would like cooperate implementation. thanks!",0,usefulness phased lstm cnn,"usefulness phased lstm cnn hi all, last weeks successful setting convolutional lstm using layers provided keras https github.com keras team keras blob master keras layers convolutional recurrent.py . another task past experiments phased lstm https arxiv.org abs 1610.09513 , also implemented keras tensorflow. now, things mind 1. phased lstm model seems really useful many purposes. yet, get really popular last years, wondering potential reasons. miss strong counter arguments, successful alternative approaches tackle issues like input different sources irregular sampling intervals ? 2. practice, two methods compatible, right? time gate could also included lstm cnn setting, analogous application classical lstm. yet, since find implementation, wonder might hurdles aware of. 3. first points disqualify idea already, feel free get contact would like cooperate implementation. thanks!"
keras,10496,"anaconda 2 created environment. python2.7 works , works . share samejason file. everytime use either backend, needs modify jason file accordingly. wondering simpler ways set file e.g., create two different jason files deal situation?",0,set keras jason file theano tensorflow backends respectively,"set keras jason file theano tensorflow backends respectively anaconda 2 created environment. python2.7 works , works . share samejason file. everytime use either backend, needs modify jason file accordingly. wondering simpler ways set file e.g., create two different jason files deal situation?"
keras,8224,"following https github.com fchollet keras issues 6142, defined model initial state parameter. model trained successfully. try evaluate model load model definition form json file, following error appeared. code snippet defining model code snippet loading model definiation json file thanks advance kind help.",0,bug loading model using gru initial state?,"bug loading model using gru initial state? following https github.com fchollet keras issues 6142, defined model initial state parameter. model trained successfully. try evaluate model load model definition form json file, following error appeared. code snippet defining model code snippet loading model definiation json file thanks advance kind help."
keras,12785,"currently working lstm based research problem. however, using rnns keras, demonstrate below, running mentioned error. use tf version 1.12.0 keras 2.2.4. seems work cells like lstmcell fails work ugrnncell.. idea fix issue. model run error obtained results error works seamlessly without error ugrnncell replaced lstmcell. help would appreciated.",0,"valueerror variable kernel already exists, disallowed. mean set reuse true reuse tf.auto reuse varscope?","valueerror variable kernel already exists, disallowed. mean set reuse true reuse tf.auto reuse varscope? currently working lstm based research problem. however, using rnns keras, demonstrate below, running mentioned error. use tf version 1.12.0 keras 2.2.4. seems work cells like lstmcell fails work ugrnncell.. idea fix issue. model run error obtained results error works seamlessly without error ugrnncell replaced lstmcell. help would appreciated."
keras,8562,"set problem trying solve. around 200k 64x64x3 rgb images patches terrain robot drove over. patch corresponding label roughness image patch is. roughness values range 0 160. data collected robot driving varying speeds, hence range roughness values. aim able predict roughness patch. using vgg 16 network, last layer modified regression. batch size 1024, loss mean sqaured error, optimize rmsprop. network shown below. problem training, network predicts exact value test image. another point note training loss always higher validation loss odd. lastly tried optimizers sgd adam, well varying batch sizes. right trying train network scratch seem promising. sure going wrong here, would really appreciate help get. thanks input tensor none img input input shape input shape else k.is keras tensor input tensor img input input tensor input tensor, shape input shape else img input input tensor block 1 x conv2d 64, 3, 3 , activation 'relu', padding 'same', name 'block1 conv1' img input x conv2d 64, 3, 3 , activation 'relu', padding 'same', name 'block1 conv2' x x maxpooling2d 2, 2 , strides 2, 2 , name 'block1 pool' x block 2 x conv2d 128, 3, 3 , activation 'relu', padding 'same', name 'block2 conv1' x x conv2d 128, 3, 3 , activation 'relu', padding 'same', name 'block2 conv2' x x maxpooling2d 2, 2 , strides 2, 2 , name 'block2 pool' x block 3 x conv2d 256, 3, 3 , activation 'relu', padding 'same', name 'block3 conv1' x x conv2d 256, 3, 3 , activation 'relu', padding 'same', name 'block3 conv2' x x conv2d 256, 3, 3 , activation 'relu', padding 'same', name 'block3 conv3' x x maxpooling2d 2, 2 , strides 2, 2 , name 'block3 pool' x block 4 x conv2d 512, 3, 3 , activation 'relu', padding 'same', name 'block4 conv1' x x conv2d 512, 3, 3 , activation 'relu', padding 'same', name 'block4 conv2' x x conv2d 512, 3, 3 , activation 'relu', padding 'same', name 'block4 conv3' x x maxpooling2d 2, 2 , strides 2, 2 , name 'block4 pool' x block 5 x conv2d 512, 3, 3 , activation 'relu', padding 'same', name 'block5 conv1' x x conv2d 512, 3, 3 , activation 'relu', padding 'same', name 'block5 conv2' x x conv2d 512, 3, 3 , activation 'relu', padding 'same', name 'block5 conv3' x x maxpooling2d 2, 2 , strides 2, 2 , name 'block5 pool' x x flatten name 'flatten' x x dense 4096, activation 'relu', name 'fc1' x x dense 4096, activation 'relu', name 'fc2' x x dense 1,name 'regression dense' x",0,cnn regression vgg 16 predicts value regardless input image.,"cnn regression vgg 16 predicts value regardless input image. set problem trying solve. around 200k 64x64x3 rgb images patches terrain robot drove over. patch corresponding label roughness image patch is. roughness values range 0 160. data collected robot driving varying speeds, hence range roughness values. aim able predict roughness patch. using vgg 16 network, last layer modified regression. batch size 1024, loss mean sqaured error, optimize rmsprop. network shown below. problem training, network predicts exact value test image. another point note training loss always higher validation loss odd. lastly tried optimizers sgd adam, well varying batch sizes. right trying train network scratch seem promising. sure going wrong here, would really appreciate help get. thanks input tensor none img input input shape input shape else k.is keras tensor input tensor img input input tensor input tensor, shape input shape else img input input tensor block 1 x conv2d 64, 3, 3 , activation 'relu', padding 'same', name 'block1 conv1' img input x conv2d 64, 3, 3 , activation 'relu', padding 'same', name 'block1 conv2' x x maxpooling2d 2, 2 , strides 2, 2 , name 'block1 pool' x block 2 x conv2d 128, 3, 3 , activation 'relu', padding 'same', name 'block2 conv1' x x conv2d 128, 3, 3 , activation 'relu', padding 'same', name 'block2 conv2' x x maxpooling2d 2, 2 , strides 2, 2 , name 'block2 pool' x block 3 x conv2d 256, 3, 3 , activation 'relu', padding 'same', name 'block3 conv1' x x conv2d 256, 3, 3 , activation 'relu', padding 'same', name 'block3 conv2' x x conv2d 256, 3, 3 , activation 'relu', padding 'same', name 'block3 conv3' x x maxpooling2d 2, 2 , strides 2, 2 , name 'block3 pool' x block 4 x conv2d 512, 3, 3 , activation 'relu', padding 'same', name 'block4 conv1' x x conv2d 512, 3, 3 , activation 'relu', padding 'same', name 'block4 conv2' x x conv2d 512, 3, 3 , activation 'relu', padding 'same', name 'block4 conv3' x x maxpooling2d 2, 2 , strides 2, 2 , name 'block4 pool' x block 5 x conv2d 512, 3, 3 , activation 'relu', padding 'same', name 'block5 conv1' x x conv2d 512, 3, 3 , activation 'relu', padding 'same', name 'block5 conv2' x x conv2d 512, 3, 3 , activation 'relu', padding 'same', name 'block5 conv3' x x maxpooling2d 2, 2 , strides 2, 2 , name 'block5 pool' x x flatten name 'flatten' x x dense 4096, activation 'relu', name 'fc1' x x dense 4096, activation 'relu', name 'fc2' x x dense 1,name 'regression dense' x"
keras,13492,"system information written custom code opposed using example directory os platform distribution e.g., linux ubuntu 16.04 windows 10 tensorflow backend yes yes tensorflow version 2.0.0 keras version 2.2.4 tf python version 3.6 cuda cudnn version 10.1 gpu model memory titan gtx 12gb describe current behavior layer type case batchnorm elu conv2d def dens block layer num, layer type, kwargs conc layer def block x conc layer.append x range layer num x layer type 32, 2, strides 1, 1 , padding 'same', kwargs x conc layer.append x x tf.keras.layers.concatenate axis axis list conc layer return x return block describe expected behavior without list tf.keras.layers.concatenate code work code reproduce issue import tensorflow tf import netron def bn elu conv filters, kernel size, strides 1, 1 , padding 'valid', kwargs def layer input tensor tf.keras.backend.name scope 'bn elu conv' x tf.keras.layers.batchnormalization input tensor x tf.keras.layers.elu x x tf.keras.layers.conv2d filters, kernel size, strides strides, padding padding, kwargs x return x return layer def conv bn elu filters, kernel size, strides 1, 1 , padding 'valid', kwargs def layer input tensor tf.keras.backend.name scope 'conv bn elu' x tf.keras.layers.conv2d filters, kernel size, strides strides, padding padding, kwargs input tensor x tf.keras.layers.batchnormalization x x tf.keras.layers.elu x return x return layer def dens block layer num, filters, kernels sizes, strides 1, 1 , padding 'same', layer type 'bn elu conv', axis 1, kwargs layer type get layertyp layer type filters check list filters, layer num kernels sizes check list kernels sizes, layer num conc layer def block x conc layer.append x range layer num x layer type filters , kernels sizes , strides strides, padding padding, kwargs x conc layer.append x x tf.keras.layers.concatenate axis axis conc layer x tf.keras.layers.concatenate axis axis list conc layer working return x return block def res block layer num, filters, kernels sizes, strides 1, 1 , padding 'same', layer type 'bn elu conv', conv block false, kwargs layer type get layertyp layer type filters check list filters, layer num kernels sizes check list kernels sizes, layer num def block x input tensor x range layer num x layer type filters , kernels sizes , strides strides, padding padding, kwargs x conv block input tensor layer type filters 1 , kernels sizes 1 , strides strides, padding padding, input tensor x tf.keras.layers.add x, input tensor return x return block def get layertyp layer typ layer typ 'bn elu conv' return bn elu conv layer typ 'conv bn elu' return conv bn elu else raise valueerror 'layer typ 'bn elu conv ' 'conv bn elu '' def check list x, list len isinstance x, list len x list len return x else raise typeerror 'x length list len' else return list x, list len def list x, list len new x range list len new x.append x return new x name ' main ' input tensor tf.keras.layers.input shape 256, 256, 3 x bn elu conv filters 3, kernel size 2 input tensor range 4 x dens block layer num 4, filters 32, kernels sizes 2 x model tf.keras.models.model input tensor, x model.compile loss 'binary crossentropy', metrics 'accuracy' model.summary model.save 'test.h5' netron.start 'test.h5' info logs valueerror graph disconnected cannot obtain value tensor tensor conv2d 2 identity 0 , shape none, 255, 255, 32 , dtype float32 layer concatenate . following previous layers accessed without issue 'input 1', 'batch normalization', 'elu', 'conv2d', 'batch normalization 1', 'elu 1', 'conv2d 1'",0,tf.keras.layers.concatenate axis axis conc layer working,"tf.keras.layers.concatenate axis axis conc layer working system information written custom code opposed using example directory os platform distribution e.g., linux ubuntu 16.04 windows 10 tensorflow backend yes yes tensorflow version 2.0.0 keras version 2.2.4 tf python version 3.6 cuda cudnn version 10.1 gpu model memory titan gtx 12gb describe current behavior layer type case batchnorm elu conv2d def dens block layer num, layer type, kwargs conc layer def block x conc layer.append x range layer num x layer type 32, 2, strides 1, 1 , padding 'same', kwargs x conc layer.append x x tf.keras.layers.concatenate axis axis list conc layer return x return block describe expected behavior without list tf.keras.layers.concatenate code work code reproduce issue import tensorflow tf import netron def bn elu conv filters, kernel size, strides 1, 1 , padding 'valid', kwargs def layer input tensor tf.keras.backend.name scope 'bn elu conv' x tf.keras.layers.batchnormalization input tensor x tf.keras.layers.elu x x tf.keras.layers.conv2d filters, kernel size, strides strides, padding padding, kwargs x return x return layer def conv bn elu filters, kernel size, strides 1, 1 , padding 'valid', kwargs def layer input tensor tf.keras.backend.name scope 'conv bn elu' x tf.keras.layers.conv2d filters, kernel size, strides strides, padding padding, kwargs input tensor x tf.keras.layers.batchnormalization x x tf.keras.layers.elu x return x return layer def dens block layer num, filters, kernels sizes, strides 1, 1 , padding 'same', layer type 'bn elu conv', axis 1, kwargs layer type get layertyp layer type filters check list filters, layer num kernels sizes check list kernels sizes, layer num conc layer def block x conc layer.append x range layer num x layer type filters , kernels sizes , strides strides, padding padding, kwargs x conc layer.append x x tf.keras.layers.concatenate axis axis conc layer x tf.keras.layers.concatenate axis axis list conc layer working return x return block def res block layer num, filters, kernels sizes, strides 1, 1 , padding 'same', layer type 'bn elu conv', conv block false, kwargs layer type get layertyp layer type filters check list filters, layer num kernels sizes check list kernels sizes, layer num def block x input tensor x range layer num x layer type filters , kernels sizes , strides strides, padding padding, kwargs x conv block input tensor layer type filters 1 , kernels sizes 1 , strides strides, padding padding, input tensor x tf.keras.layers.add x, input tensor return x return block def get layertyp layer typ layer typ 'bn elu conv' return bn elu conv layer typ 'conv bn elu' return conv bn elu else raise valueerror 'layer typ 'bn elu conv ' 'conv bn elu '' def check list x, list len isinstance x, list len x list len return x else raise typeerror 'x length list len' else return list x, list len def list x, list len new x range list len new x.append x return new x name ' main ' input tensor tf.keras.layers.input shape 256, 256, 3 x bn elu conv filters 3, kernel size 2 input tensor range 4 x dens block layer num 4, filters 32, kernels sizes 2 x model tf.keras.models.model input tensor, x model.compile loss 'binary crossentropy', metrics 'accuracy' model.summary model.save 'test.h5' netron.start 'test.h5' info logs valueerror graph disconnected cannot obtain value tensor tensor conv2d 2 identity 0 , shape none, 255, 255, 32 , dtype float32 layer concatenate . following previous layers accessed without issue 'input 1', 'batch normalization', 'elu', 'conv2d', 'batch normalization 1', 'elu 1', 'conv2d 1'"
keras,8661,"trying save best weights convoluted nn using keras, want evaluate model based best weights. file saving best weight create. given absolute path, changed name file nothing worked. although file created working directory, error occurs line model.load weights apnamodel.best.hdf5 code model sequential model.add conv2d 32, 2, strides 1, activation 'relu', input shape input shape model.add maxpooling2d pool size 2 model.add flatten model.add dense 50, activation 'relu' model.add dense 10, activation softmax nadam optimizers.nadam lr 0.002, beta 1 0.9, beta 2 0.999, epsilon 1e 08, schedule decay 0.004 model.compile loss 'categorical crossentropy', optimizer nadam, metrics 'accuracy' checkpoint filepath apnamodel.best.hdf5 checkpoint modelcheckpoint filepath, monitor 'val acc', verbose 0, save best true, mode 'max' callbacks list checkpoint earlystop earlystopping monitor 'val loss', min delta 0.0001, patience 3, verbose 0, mode 'auto' callbacks list earlystop history model.fit train data, label train tranformed, epochs 5, batch size 1000, validation data val data, label val tranformed , callbacks callbacks list score model.evaluate test data, label test tranformed, batch size 100 print score1 str score 0 print score2 str score 0 model sequential model.add conv2d 32, 2, strides 1, activation 'relu', input shape input shape model.add maxpooling2d pool size 2 model.add flatten model.add dense 50, activation 'relu' model.add dense 10, activation softmax model.load weights apnamodel.best.hdf5 nadam optimizers.nadam lr 0.002, beta 1 0.9, beta 2 0.999, epsilon 1e 08, schedule decay 0.004 model.compile loss 'categorical crossentropy', optimizer nadam, metrics 'accuracy' scores model.evaluate val data, label val tranformed print scores str scores",0,"unable open file unable open file name 'apnamodel.best.hdf5', errno 2, error message 'no file directory', flags 0, flags 0","unable open file unable open file name 'apnamodel.best.hdf5', errno 2, error message 'no file directory', flags 0, flags 0 trying save best weights convoluted nn using keras, want evaluate model based best weights. file saving best weight create. given absolute path, changed name file nothing worked. although file created working directory, error occurs line model.load weights apnamodel.best.hdf5 code model sequential model.add conv2d 32, 2, strides 1, activation 'relu', input shape input shape model.add maxpooling2d pool size 2 model.add flatten model.add dense 50, activation 'relu' model.add dense 10, activation softmax nadam optimizers.nadam lr 0.002, beta 1 0.9, beta 2 0.999, epsilon 1e 08, schedule decay 0.004 model.compile loss 'categorical crossentropy', optimizer nadam, metrics 'accuracy' checkpoint filepath apnamodel.best.hdf5 checkpoint modelcheckpoint filepath, monitor 'val acc', verbose 0, save best true, mode 'max' callbacks list checkpoint earlystop earlystopping monitor 'val loss', min delta 0.0001, patience 3, verbose 0, mode 'auto' callbacks list earlystop history model.fit train data, label train tranformed, epochs 5, batch size 1000, validation data val data, label val tranformed , callbacks callbacks list score model.evaluate test data, label test tranformed, batch size 100 print score1 str score 0 print score2 str score 0 model sequential model.add conv2d 32, 2, strides 1, activation 'relu', input shape input shape model.add maxpooling2d pool size 2 model.add flatten model.add dense 50, activation 'relu' model.add dense 10, activation softmax model.load weights apnamodel.best.hdf5 nadam optimizers.nadam lr 0.002, beta 1 0.9, beta 2 0.999, epsilon 1e 08, schedule decay 0.004 model.compile loss 'categorical crossentropy', optimizer nadam, metrics 'accuracy' scores model.evaluate val data, label val tranformed print scores str scores"
keras,8555,"running script checks output result prefix end. appropriate simple check validity paths beginning otherwise, error would come processing done.",0,file path check examples deep dream.py,"file path check examples deep dream.py running script checks output result prefix end. appropriate simple check validity paths beginning otherwise, error would come processing done."
keras,13573,"working lstm project. let's 100 input data. input layer uses slinding windows equal 10. here's lstm model inputs tf.keras.input shape 10,100 lstm 1 layers.lstm 100, stateful false, return sequences true inputs fc 1 layers.dense 100 lstm 1 lstm 2 layers.lstm 100, stateful false, return sequences false fc 1 label layers.dense 5, activation 'softmax' lstm 2 here's model summary. layer type output shape param input inputlayer none, 10, 100 0 lstm 1 lstm none, 10, 100 80400 dense dense none, 10, 100 10100 lstm 2 lstm none, 100 80400 label dense none, 5 1111 total params 172,011 trainable params 172,011 non trainable params 0 none code load data. h5 x 'x.h5' h5 'y.h5' x data hdf5matrix h5 x, start 0, end data length x data np.reshape x data, data length, 5 label hdf5matrix h5 y, start 0, end data length label np.reshape label, data length, 5 let's say function splits data sequences 10 feed model. question set return sequences true , code works fine feeding model sequences 10 slices output layer shape none, 10, 5 want way. set false , output layer shape none, 5 , get error message error checking target expected 2 dimensions, got array shape x, x, x . know shape problem solve ? need reshape ?",0,"lstm error checking target expected 2 dimensions, got array 3d shape","lstm error checking target expected 2 dimensions, got array 3d shape working lstm project. let's 100 input data. input layer uses slinding windows equal 10. here's lstm model inputs tf.keras.input shape 10,100 lstm 1 layers.lstm 100, stateful false, return sequences true inputs fc 1 layers.dense 100 lstm 1 lstm 2 layers.lstm 100, stateful false, return sequences false fc 1 label layers.dense 5, activation 'softmax' lstm 2 here's model summary. layer type output shape param input inputlayer none, 10, 100 0 lstm 1 lstm none, 10, 100 80400 dense dense none, 10, 100 10100 lstm 2 lstm none, 100 80400 label dense none, 5 1111 total params 172,011 trainable params 172,011 non trainable params 0 none code load data. h5 x 'x.h5' h5 'y.h5' x data hdf5matrix h5 x, start 0, end data length x data np.reshape x data, data length, 5 label hdf5matrix h5 y, start 0, end data length label np.reshape label, data length, 5 let's say function splits data sequences 10 feed model. question set return sequences true , code works fine feeding model sequences 10 slices output layer shape none, 10, 5 want way. set false , output layer shape none, 5 , get error message error checking target expected 2 dimensions, got array shape x, x, x . know shape problem solve ? need reshape ?"
keras,4278,"want use callbacks scikit learn wrapper. normally, using scikit learn wrappers, pass callbacks fit function outlined documentation https keras.io callbacks usage callbacks . however, using scikit learn wrappers, function method . documentation mentions contain arguments fit method https keras.io scikit learn api wrappers scikit learn api among others unable figure use pass callbacks fit function inside class. code looks like excluding code loading data brevity anyone know use setup?",0,pass callbacks scikit learn wrappers e.g. kerasclassifier,"pass callbacks scikit learn wrappers e.g. kerasclassifier want use callbacks scikit learn wrapper. normally, using scikit learn wrappers, pass callbacks fit function outlined documentation https keras.io callbacks usage callbacks . however, using scikit learn wrappers, function method . documentation mentions contain arguments fit method https keras.io scikit learn api wrappers scikit learn api among others unable figure use pass callbacks fit function inside class. code looks like excluding code loading data brevity anyone know use setup?"
keras,12777,"different set roi boxes input images. batch size 1 defined input images none, none, 3 input boxes none,4 handle variations, roi pooling layer need know many roi boxes get extracted. want know first dimension none,4 run time pass roi pooling layer . k.shape k.int shape working one get tensor none. ! model1 https user images.githubusercontent.com 30056321 57056497 f1c70600 6c70 11e9 92ac 84e971a45063.png",0,get none dimension tensor run time keras model?,"get none dimension tensor run time keras model? different set roi boxes input images. batch size 1 defined input images none, none, 3 input boxes none,4 handle variations, roi pooling layer need know many roi boxes get extracted. want know first dimension none,4 run time pass roi pooling layer . k.shape k.int shape working one get tensor none. ! model1 https user images.githubusercontent.com 30056321 57056497 f1c70600 6c70 11e9 92ac 84e971a45063.png"
keras,9786,"easy interface use dense, conv2d etc custom layer classes along trainable variables. something easily achieved pytorch.",0,use existing layers custom keras layers,"use existing layers custom keras layers easy interface use dense, conv2d etc custom layer classes along trainable variables. something easily achieved pytorch."
keras,8130,"hello, trying use model paired input images similar directory trees , augmented imagedatagenerator using also flow directory method infers labels folder structure . getting error keras can't handle way. combine generators using flow directory accepted fit generator? sample code model definition error get following typeerror error checking model input data numpy array, list dict numpy arrays. found",0,use fit generator multiple image inputs,"use fit generator multiple image inputs hello, trying use model paired input images similar directory trees , augmented imagedatagenerator using also flow directory method infers labels folder structure . getting error keras can't handle way. combine generators using flow directory accepted fit generator? sample code model definition error get following typeerror error checking model input data numpy array, list dict numpy arrays. found"
keras,7702,"created two imagedatagenerator objects process two images time one image mask train autoencoder u net. randomize dataset set . take images two folders one image mask , use two seed. however, practice work mask extracted correspond extracted image. images image mask file name. problem filenames data directories read different order. solve added next line directoryiterator end init function. problem seems solved. x check date master branch keras. update pip install git git github.com fchollet keras.git upgrade deps x running tensorflow, check date latest version. installation instructions found https www.tensorflow.org get started os setup . x running theano, check date master branch theano. update pip install git git github.com theano theano.git upgrade deps provide link github gist python script reproduce issue copy script short .",0,keras imagedatagenerator work properly shuffle transforming image mask together,"keras imagedatagenerator work properly shuffle transforming image mask together created two imagedatagenerator objects process two images time one image mask train autoencoder u net. randomize dataset set . take images two folders one image mask , use two seed. however, practice work mask extracted correspond extracted image. images image mask file name. problem filenames data directories read different order. solve added next line directoryiterator end init function. problem seems solved. x check date master branch keras. update pip install git git github.com fchollet keras.git upgrade deps x running tensorflow, check date latest version. installation instructions found https www.tensorflow.org get started os setup . x running theano, check date master branch theano. update pip install git git github.com theano theano.git upgrade deps provide link github gist python script reproduce issue copy script short ."
keras,9966,"model compiled , train returning tuples , evaluating full devset passed . model 2 inputs list 2 numpy arrays. now, custom callback https stackoverflow.com 49832560 932818 , check see 5.. ok, first extra element flattened sublist 2 elements new validation data although idea happens . second extra element actually scalar appended ! even come from???",0,unknown elements appear validation data,"unknown elements appear validation data model compiled , train returning tuples , evaluating full devset passed . model 2 inputs list 2 numpy arrays. now, custom callback https stackoverflow.com 49832560 932818 , check see 5.. ok, first extra element flattened sublist 2 elements new validation data although idea happens . second extra element actually scalar appended ! even come from???"
keras,7341,attention layer api like bidirectional layer would great. difficult create custom layers keras.,0,attention layer,attention layer attention layer api like bidirectional layer would great. difficult create custom layers keras.
keras,13571,"environment settings nvidia driver version 430.50 cuda version 10.1 geforce rtx 2080 ti ubuntu 16.04.6 lts library version libgpuarray 0.7.6 h14c3975 0 pygpu 0.7.6 py36h035aef0 0 tensorflow gpu 1.14.0 tensorflow estimator 1.14.0 theano 1.0.4 cntk gpu 2.7 numpy 1.14.6 py36h3b04361 4 numpy base 1.14.6 py36h81de0dd 4 keras applications 1.0.8 py 0 keras preprocessing 1.1.0 py 1 description mobilenet.1.00.224 imagenet origin0 nai1 ls1 gf1 gf2.h5 model image classification. find something strange run model inputs. load model keras, model works well backend cntk, configure tensorflow theano backend keras, output model nan, seems quite strange. attach .h5 file, input image code below. phenomenon described reproduced using command 20191114 nan.zip https github.com keras team keras files 3877869 20191114 nan.zip",0,nan output tensorflow theano backend cntk output normally,"nan output tensorflow theano backend cntk output normally environment settings nvidia driver version 430.50 cuda version 10.1 geforce rtx 2080 ti ubuntu 16.04.6 lts library version libgpuarray 0.7.6 h14c3975 0 pygpu 0.7.6 py36h035aef0 0 tensorflow gpu 1.14.0 tensorflow estimator 1.14.0 theano 1.0.4 cntk gpu 2.7 numpy 1.14.6 py36h3b04361 4 numpy base 1.14.6 py36h81de0dd 4 keras applications 1.0.8 py 0 keras preprocessing 1.1.0 py 1 description mobilenet.1.00.224 imagenet origin0 nai1 ls1 gf1 gf2.h5 model image classification. find something strange run model inputs. load model keras, model works well backend cntk, configure tensorflow theano backend keras, output model nan, seems quite strange. attach .h5 file, input image code below. phenomenon described reproduced using command 20191114 nan.zip https github.com keras team keras files 3877869 20191114 nan.zip"
keras,10089,"tensorflow version 1.3 , keras 2.0.8 code ! image https user images.githubusercontent.com 11004307 39503488 d1151ff6 4df8 11e8 97f3 ce8b9e3fb719.png",0,keras using tensorflow backend invalidargumenterror,"keras using tensorflow backend invalidargumenterror tensorflow version 1.3 , keras 2.0.8 code ! image https user images.githubusercontent.com 11004307 39503488 d1151ff6 4df8 11e8 97f3 ce8b9e3fb719.png"
keras,12637,"! , 2019 04 08 23 21 12 https user images.githubusercontent.com 33189954 55731397 1c98b280 5a55 11e9 993d 9af62380c639.png want represent 28x1x1 output layer keras see one single number dense layer",0,represent 28x1x1 output dense layer?,"represent 28x1x1 output dense layer? ! , 2019 04 08 23 21 12 https user images.githubusercontent.com 33189954 55731397 1c98b280 5a55 11e9 993d 9af62380c639.png want represent 28x1x1 output layer keras see one single number dense layer"
keras,8500,"training persisting model found link https github.com hironsan anago blob master anago models.py . trains loads machine without problem. however, persisted model loaded different machine one trained on, shows error message bellow. files needed reproduce error found https drive.google.com open?id 1xxezv3ylcp45aqfevf8pll3kgn9quypk .",0,saved model loading inconsistent behavior,"saved model loading inconsistent behavior training persisting model found link https github.com hironsan anago blob master anago models.py . trains loads machine without problem. however, persisted model loaded different machine one trained on, shows error message bellow. files needed reproduce error found https drive.google.com open?id 1xxezv3ylcp45aqfevf8pll3kgn9quypk ."
keras,6499,"hello, run slightly modified version keras fine tuning examples https blog.keras.io building powerful image classification models using little data.html fine tunes top layers keras 2.0.3 tensorflow ubuntu gpu . looks like following this, get unreliable validation accuracy results. example, predict generator predicts 640 800 80 classes correctly whereas evaluate generator produces accuracy score 95 . someone 3477 suggests remove parameter validation generator, get results 365 800 45 89 evaluate generator. something wrong evaluation due bug? many similar issues e.g. 3849, 6245 stated accuracy training afterwards match actual predictions. could someone experienced maybe shine light onto problem? thanks",0,evaluate generator produces wrong accuracy scores?,"evaluate generator produces wrong accuracy scores? hello, run slightly modified version keras fine tuning examples https blog.keras.io building powerful image classification models using little data.html fine tunes top layers keras 2.0.3 tensorflow ubuntu gpu . looks like following this, get unreliable validation accuracy results. example, predict generator predicts 640 800 80 classes correctly whereas evaluate generator produces accuracy score 95 . someone 3477 suggests remove parameter validation generator, get results 365 800 45 89 evaluate generator. something wrong evaluation due bug? many similar issues e.g. 3849, 6245 stated accuracy training afterwards match actual predictions. could someone experienced maybe shine light onto problem? thanks"
keras,12335,reproducible script throws x check date master branch keras. update x check version cntk date. x provide link github gist python script reproduce issue copy script short .,0,k.in top k cntk backend broken,k.in top k cntk backend broken reproducible script throws x check date master branch keras. update x check version cntk date. x provide link github gist python script reproduce issue copy script short .
keras,9495,"hi all, trying turn current vae example beta vae https openreview.net pdf?id sy2fzu9gl . hard time, asking community suggestion ideally code snippet. tf implementation https github.com miyosuda disentangled vae , would prefer keras one. thanks advance!",0,turn variational autoencoder example beta vae?,"turn variational autoencoder example beta vae? hi all, trying turn current vae example beta vae https openreview.net pdf?id sy2fzu9gl . hard time, asking community suggestion ideally code snippet. tf implementation https github.com miyosuda disentangled vae , would prefer keras one. thanks advance!"
keras,9727,"default, model training use default argument. however, whatever true false, unless workers 0, engine always create thread pool handle data reading, seems high frequent dead lock application exit quit training huge model like inception v3 resnet50, press many time kill hanging thread created thread pool. suggest set default argument create extra threads.",0,dead lock huge model using workers 0,"dead lock huge model using workers 0 default, model training use default argument. however, whatever true false, unless workers 0, engine always create thread pool handle data reading, seems high frequent dead lock application exit quit training huge model like inception v3 resnet50, press many time kill hanging thread created thread pool. suggest set default argument create extra threads."
keras,11099,"tensorboard keras crashes deep callbacks end epoch. here's simplified version code code import pandas pd import keras keras.models import sequential keras.layers import dense class toynet def init self, run 1, layer 1 nodes 50, layer 2 nodes 100, layer 3 nodes 50 self.run seq run self.layer 1 nodes layer 1 nodes self.layer 2 nodes layer 2 nodes self.layer 3 nodes layer 3 nodes define model self.model sequential self.model.add dense self.layer 1 nodes, input dim 9, activation 'relu', name 'layer 1' self.model.add dense self.layer 2 nodes, activation 'relu', name 'layer 2' self.model.add dense self.layer 3 nodes, activation 'relu', name 'layer 3' self.model.add dense 1, activation 'linear', name 'output layer' self.model.compile loss 'mean squared error', optimizer 'adam' create tensorboard logger log dir logs .format self.run seq self.logger keras.callbacks.tensorboard log dir log dir, histogram freq 5 def train self, x, y, epochs 50 train model self.model.fit x, y, epochs epochs, shuffle true, verbose 2, validation split 0.05, callbacks self.logger def test self, xt, yt test error rate self.model.evaluate xt, yt, verbose 0 print mean squared error mse test data set .format test error rate name ' main ' training data df pd.read csv sales data training scaled.csv x training data df.drop 'total earnings', axis 1 .values training data df 'total earnings' .values load test data set test data df pd.read csv sales data test scaled.csv x test test data df.drop 'total earnings', axis 1 .values test test data df 'total earnings' .values print run 1 toy1 toynet 1, 50, 100, 50 toy1.train x, toy1.test x test, test print run 2 toy2 toynet 2, 5, 100, 50 toy2.train x,y crashes end first epoch callbacks toy2.test x test, test output anaconda3 lib python3.6 site packages h5py init .py 36 futurewarning conversion second argument issubdtype deprecated. future, treated . . conv import register converters register converters using tensorflow backend. run 1 train 950 samples, validate 50 samples epoch 1 50 1s loss 0.0314 val loss 0.0043 epoch 2 50 0s loss 0.0048 val loss 0.0011 output snipped brevity epoch 50 50 0s loss 2.4237e 05 val loss 5.0361e 05 mean squared error mse test data set 7.575784547952935e 05 run 2 train 950 samples, validate 50 samples epoch 1 50 1s loss 0.0333 val loss 0.0172 invalidargumenterror traceback recent call last anaconda3 lib python3.6 site packages tensorflow python client session.py call self, fn, args 1321 try 1322 return fn args 1323 except errors.operror e anaconda3 lib python3.6 site packages tensorflow python client session.py run fn feed dict, fetch list, target list, options, run metadata 1306 return self. call tf sessionrun 1307 options, feed dict, fetch list, target list, run metadata 1308 anaconda3 lib python3.6 site packages tensorflow python client session.py call tf sessionrun self, options, feed dict, fetch list, target list, run metadata 1408 self. session, options, feed dict, fetch list, target list, 1409 run metadata 1410 else invalidargumenterror must feed value placeholder tensor 'layer 1 input' dtype float shape ?,9 node layer 1 input placeholder dtype dt float, shape ?,9 , device job localhost replica 0 task 0 device cpu 0 handling exception, another exception occurred invalidargumenterror traceback recent call last 64 65 toy2 toynet 2, 5, 100, 50 66 toy2.train x,y crashes end first epoch callbacks 67 toy2.test x test, test train self, x, y, epochs 39 verbose 2, 40 validation split 0.05, 41 callbacks self.logger 42 43 anaconda3 lib python3.6 site packages keras engine training.py fit self, x, y, batch size, epochs, verbose, callbacks, validation split, validation data, shuffle, class weight, sample weight, initial epoch, steps per epoch, validation steps, kwargs 1043 initial epoch initial epoch, 1044 steps per epoch steps per epoch, 1045 validation steps validation steps 1046 1047 def evaluate self, x none, none, anaconda3 lib python3.6 site packages keras engine training arrays.py fit loop model, f, ins, labels, batch size, epochs, verbose, callbacks, val f, val ins, shuffle, callback metrics, initial epoch, steps per epoch, validation steps 215 l, zip labels, val outs 216 epoch logs 'val ' l 217 callbacks.on epoch end epoch, epoch logs 218 callback model.stop training 219 break anaconda3 lib python3.6 site packages keras callbacks.py epoch end self, epoch, logs 75 logs logs 76 callback self.callbacks 77 callback.on epoch end epoch, logs 78 79 def batch begin self, batch, logs none anaconda3 lib python3.6 site packages keras callbacks.py epoch end self, epoch, logs 915 assert len batch val len tensors 916 feed dict dict zip tensors, batch val 917 result self.sess.run self.merged , feed dict feed dict 918 summary str result 0 919 self.writer.add summary summary str, epoch anaconda3 lib python3.6 site packages tensorflow python client session.py run self, fetches, feed dict, options, run metadata 898 try 899 result self. run none, fetches, feed dict, options ptr, 900 run metadata ptr 901 run metadata 902 proto data tf session.tf getbuffer run metadata ptr anaconda3 lib python3.6 site packages tensorflow python client session.py run self, handle, fetches, feed dict, options, run metadata 1133 final fetches final targets handle feed dict tensor 1134 results self. run handle, final targets, final fetches, 1135 feed dict tensor, options, run metadata 1136 else 1137 results anaconda3 lib python3.6 site packages tensorflow python client session.py run self, handle, target list, fetch list, feed dict, options, run metadata 1314 handle none 1315 return self. call run fn, feeds, fetches, targets, options, 1316 run metadata 1317 else 1318 return self. call prun fn, handle, feeds, fetches anaconda3 lib python3.6 site packages tensorflow python client session.py call self, fn, args 1333 except keyerror 1334 pass 1335 raise type e node def, op, message 1336 1337 def extend graph self invalidargumenterror must feed value placeholder tensor 'layer 1 input' dtype float shape ?,9 node layer 1 input placeholder dtype dt float, shape ?,9 , device job localhost replica 0 task 0 device cpu 0 caused op 'layer 1 input', defined file anaconda3 lib python3.6 runpy.py , line 193, run module main main , mod spec file anaconda3 lib python3.6 runpy.py , line 85, run code exec code, run globals file anaconda3 lib python3.6 site packages ipykernel launcher.py , line 16, app.launch new instance file anaconda3 lib python3.6 site packages traitlets config application.py , line 658, launch instance app.start file anaconda3 lib python3.6 site packages ipykernel kernelapp.py , line 486, start self.io loop.start file anaconda3 lib python3.6 site packages tornado platform asyncio.py , line 127, start self.asyncio loop.run forever file anaconda3 lib python3.6 asyncio base events.py , line 422, run forever self. run file anaconda3 lib python3.6 asyncio base events.py , line 1432, run handle. run file anaconda3 lib python3.6 asyncio events.py , line 145, run self. callback self. args file anaconda3 lib python3.6 site packages tornado platform asyncio.py , line 117, handle events handler func fileobj, events file anaconda3 lib python3.6 site packages tornado stack context.py , line 276, null wrapper return fn args, kwargs file anaconda3 lib python3.6 site packages zmq eventloop zmqstream.py , line 450, handle events self. handle recv file anaconda3 lib python3.6 site packages zmq eventloop zmqstream.py , line 480, handle recv self. run callback callback, msg file anaconda3 lib python3.6 site packages zmq eventloop zmqstream.py , line 432, run callback callback args, kwargs file anaconda3 lib python3.6 site packages tornado stack context.py , line 276, null wrapper return fn args, kwargs file anaconda3 lib python3.6 site packages ipykernel kernelbase.py , line 283, dispatcher return self.dispatch shell stream, msg file anaconda3 lib python3.6 site packages ipykernel kernelbase.py , line 233, dispatch shell handler stream, idents, msg file anaconda3 lib python3.6 site packages ipykernel kernelbase.py , line 399, execute request user expressions, allow stdin file anaconda3 lib python3.6 site packages ipykernel ipkernel.py , line 208, execute res shell.run cell code, store history store history, silent silent file anaconda3 lib python3.6 site packages ipykernel zmqshell.py , line 537, run cell return super zmqinteractiveshell, self .run cell args, kwargs file anaconda3 lib python3.6 site packages ipython core interactiveshell.py , line 2662, run cell raw cell, store history, silent, shell futures file anaconda3 lib python3.6 site packages ipython core interactiveshell.py , line 2785, run cell interactivity interactivity, compiler compiler, result result file anaconda3 lib python3.6 site packages ipython core interactiveshell.py , line 2903, run ast nodes self.run code code, result file anaconda3 lib python3.6 site packages ipython core interactiveshell.py , line 2963, run code exec code obj, self.user global ns, self.user ns file , line 61, toy1 toynet 1, 50, 100, 50 file , line 19, init self.model.add dense self.layer 1 nodes, input dim 9, activation 'relu', name 'layer 1' file anaconda3 lib python3.6 site packages keras engine sequential.py , line 160, add name layer.name ' input' file anaconda3 lib python3.6 site packages keras engine input layer.py , line 178, input input tensor tensor file anaconda3 lib python3.6 site packages keras legacy interfaces.py , line 91, wrapper return func args, kwargs file anaconda3 lib python3.6 site packages keras engine input layer.py , line 87, init name self.name file anaconda3 lib python3.6 site packages keras backend tensorflow backend.py , line 517, placeholder x tf.placeholder dtype, shape shape, name name file anaconda3 lib python3.6 site packages tensorflow python ops array ops.py , line 1734, placeholder return gen array ops.placeholder dtype dtype, shape shape, name name file anaconda3 lib python3.6 site packages tensorflow python ops gen array ops.py , line 4924, placeholder placeholder , dtype dtype, shape shape, name name file anaconda3 lib python3.6 site packages tensorflow python framework op def library.py , line 787, apply op helper op def op def file anaconda3 lib python3.6 site packages tensorflow python framework ops.py , line 3414, create op op def op def file anaconda3 lib python3.6 site packages tensorflow python framework ops.py , line 1740, init self. traceback self. graph. extract stack pylint disable protected access invalidargumenterror see traceback must feed value placeholder tensor 'layer 1 input' dtype float shape ?,9 node layer 1 input placeholder dtype dt float, shape ?,9 , device job localhost replica 0 task 0 device cpu 0 data sanity's sake included short sample training testing data below. training data sales data training scaled.csv critic rating,is action,is exclusive us,is portable,is role playing,is sequel,is sports,suitable kids,total earnings,unit price 0.4999999999999999,1.0,1.0,1.0,0.0,1.0,0.0,1.0,0.7991793127668619,1.0 0.16666666666666663,0.0,0.0,0.0,1.0,1.0,0.0,0.0,0.15750170976506905,1.0 0.4999999999999999,0.0,0.0,0.0,0.0,1.0,1.0,0.0,0.18970444169239015,1.0 0.6666666666666666,1.0,1.0,0.0,0.0,1.0,0.0,0.0,0.39223304559989647,0.0 0.0,1.0,0.0,1.0,0.0,1.0,0.0,0.0,0.21546366980277626,1.0 0.4999999999999999,1.0,0.0,1.0,0.0,0.0,0.0,1.0,0.2675699155283636,1.0 0.6666666666666666,1.0,0.0,0.0,0.0,0.0,0.0,1.0,0.2418106874179775,1.0 0.4999999999999999,1.0,1.0,1.0,0.0,1.0,0.0,0.0,0.7090183175911721,1.0 0.6666666666666666,1.0,1.0,0.0,0.0,0.0,0.0,0.0,0.4385279384854254,1.0 0.16666666666666663,0.0,1.0,0.0,1.0,1.0,0.0,1.0,0.3957893569434946,1.0 0.9999999999999999,1.0,0.0,0.0,0.0,1.0,0.0,0.0,0.24197334614886973,0.0 0.6666666666666666,1.0,0.0,0.0,0.0,1.0,0.0,0.0,0.33607142197001905,1.0 0.4999999999999999,0.0,1.0,0.0,0.0,0.0,1.0,0.0,0.26054601578529046,1.0 0.6666666666666666,0.0,0.0,1.0,1.0,1.0,0.0,0.0,0.3501229182455038,1.0 0.8333333333333334,0.0,1.0,1.0,0.0,1.0,1.0,0.0,0.39457681004047984,0.0 0.6666666666666666,0.0,0.0,0.0,0.0,1.0,1.0,0.0,0.2248313339864328,1.0 0.4999999999999999,0.0,0.0,1.0,0.0,1.0,1.0,1.0,0.28864900833625995,1.0 0.4999999999999999,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.05268664165172547,0.0 0.9999999999999999,1.0,0.0,0.0,0.0,1.0,0.0,0.0,0.24431711058945305,0.0 0.16666666666666663,1.0,0.0,1.0,0.0,1.0,0.0,0.0,0.2740097225559601,1.0 0.33333333333333337,0.0,0.0,0.0,1.0,1.0,0.0,0.0,0.20141217352729157,1.0 0.4999999999999999,0.0,1.0,0.0,1.0,1.0,0.0,0.0,0.2802240254339107,0.0 0.16666666666666663,0.0,1.0,0.0,1.0,1.0,0.0,1.0,0.4069129960629193,1.0 0.6666666666666666,0.0,0.0,1.0,1.0,0.0,0.0,0.0,0.21487957708729966,1.0 0.4999999999999999,1.0,0.0,1.0,0.0,1.0,0.0,1.0,0.4250642317147557,1.0 0.4999999999999999,0.0,0.0,1.0,0.0,1.0,1.0,1.0,0.28513336167538494,1.0 0.33333333333333337,1.0,1.0,0.0,0.0,1.0,0.0,0.0,0.40075044823570727,0.5 0.4999999999999999,1.0,0.0,0.0,0.0,1.0,0.0,1.0,0.18811482227685256,0.0 0.33333333333333337,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.1428661207741077,1.0 0.8333333333333334,1.0,0.0,1.0,0.0,0.0,0.0,1.0,0.27292286649045305,0.5 0.16666666666666663,0.0,0.0,0.0,1.0,1.0,0.0,0.0,0.16160144914142066,1.0 0.4999999999999999,1.0,1.0,0.0,0.0,1.0,0.0,0.0,0.5831426406166245,1.0 0.6666666666666666,1.0,0.0,0.0,0.0,1.0,0.0,0.0,0.18850668194672926,0.0 0.8333333333333334,0.0,1.0,0.0,1.0,1.0,0.0,0.0,0.6118297258830706,1.0 0.9999999999999999,0.0,0.0,0.0,1.0,1.0,0.0,0.0,0.18928670449714424,0.0 0.4999999999999999,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.13759449917746436,1.0 0.9999999999999999,0.0,1.0,0.0,0.0,1.0,1.0,0.0,0.34539842147095245,0.0 0.16666666666666663,0.0,0.0,0.0,1.0,1.0,0.0,1.0,0.13583113066301916,0.5 0.9999999999999999,0.0,1.0,1.0,0.0,1.0,1.0,1.0,0.48629415352766125,0.0 0.9999999999999999,0.0,1.0,0.0,1.0,1.0,0.0,1.0,0.7500009241973347,1.0 0.0,0.0,0.0,0.0,1.0,1.0,0.0,0.0,0.04253895491765401,0.0 0.4999999999999999,0.0,0.0,0.0,1.0,1.0,0.0,0.0,0.11630468937727585,0.0 0.16666666666666663,1.0,1.0,0.0,0.0,0.0,0.0,0.0,0.2775253692168352,1.0 0.4999999999999999,0.0,0.0,1.0,0.0,0.0,1.0,0.0,0.10216816694700652,0.5 0.4999999999999999,0.0,1.0,0.0,0.0,1.0,1.0,0.0,0.23650949150662648,0.0 0.33333333333333337,1.0,0.0,1.0,0.0,1.0,0.0,0.0,0.18031089998336447,0.0 0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.05621337868061589,1.0 0.9999999999999999,1.0,1.0,1.0,0.0,1.0,0.0,0.0,0.7949538825530027,0.5 0.4999999999999999,1.0,0.0,0.0,0.0,1.0,0.0,1.0,0.18538289495573096,0.0 0.8333333333333334,0.0,0.0,0.0,0.0,1.0,1.0,0.0,0.2587900408495222,1.0 0.16666666666666663,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.08373227851610877,1.0 0.9999999999999999,1.0,1.0,1.0,0.0,0.0,0.0,1.0,0.7447329993900298,1.0 0.16666666666666663,1.0,0.0,0.0,0.0,1.0,0.0,0.0,0.20902386277517976,1.0 0.6666666666666666,1.0,0.0,0.0,0.0,1.0,0.0,0.0,0.18850668194672926,0.0 0.16666666666666663,0.0,0.0,1.0,0.0,1.0,1.0,0.0,0.12509565442413267,0.5 0.6666666666666666,1.0,0.0,0.0,0.0,1.0,0.0,0.0,0.3313875898781908,1.0 0.33333333333333337,1.0,0.0,1.0,0.0,1.0,0.0,1.0,0.3717861037688767,1.0 0.0,0.0,0.0,0.0,1.0,1.0,0.0,1.0,0.1434502134895843,1.0 0.33333333333333337,1.0,1.0,1.0,0.0,1.0,0.0,0.0,0.6329051219016284,1.0 0.8333333333333334,1.0,1.0,0.0,0.0,1.0,0.0,0.0,0.727753645958485,1.0 0.6666666666666666,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.15144267203933381,0.5 0.8333333333333334,1.0,1.0,1.0,0.0,1.0,0.0,0.0,0.8846601726400621,1.0 0.8333333333333334,0.0,0.0,0.0,1.0,1.0,0.0,0.0,0.30738433670357296,1.0 testing data sales data test scaled.csv critic rating,is action,is exclusive us,is portable,is role playing,is sequel,is sports,suitable kids,total earnings,unit price 0.4999999999999999,1.0,0.0,1.0,0.0,1.0,0.0,0.0,0.3747139609249367,1.0 0.8333333333333334,0.0,0.0,0.0,0.0,1.0,1.0,0.0,0.19242527864549636,0.5 0.33333333333333337,0.0,0.0,0.0,0.0,1.0,1.0,0.0,0.11485185116726125,0.5 0.8333333333333334,1.0,0.0,0.0,0.0,0.0,0.0,1.0,0.14245208036820023,0.0 0.6666666666666666,1.0,0.0,1.0,0.0,1.0,0.0,1.0,0.4806824273118796,1.0 0.6666666666666666,0.0,0.0,0.0,1.0,1.0,0.0,0.0,0.13972015304707863,0.0 0.0,1.0,0.0,0.0,0.0,1.0,0.0,0.0,0.11338792258923126,0.5 0.8333333333333334,0.0,0.0,1.0,1.0,1.0,0.0,1.0,0.44906748488937354,1.0 0.4999999999999999,0.0,0.0,1.0,0.0,0.0,1.0,0.0,0.06127428328496702,0.0 0.16666666666666663,1.0,0.0,0.0,0.0,1.0,0.0,0.0,0.20668009833459638,1.0 0.8333333333333334,1.0,0.0,1.0,0.0,1.0,0.0,0.0,0.4777545701558197,1.0 0.4999999999999999,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.13232657437015954,1.0 0.4999999999999999,0.0,0.0,0.0,1.0,1.0,0.0,0.0,0.17925361823256503,0.5 0.6666666666666666,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.16335742407718895,1.0 0.4999999999999999,0.0,0.0,0.0,1.0,1.0,0.0,0.0,0.23946692297739414,1.0 0.6666666666666666,0.0,0.0,0.0,1.0,1.0,0.0,1.0,0.31206816879540117,1.0 0.4999999999999999,1.0,0.0,0.0,0.0,1.0,0.0,0.0,0.2285281233248923,0.5 0.33333333333333337,1.0,0.0,0.0,0.0,1.0,0.0,1.0,0.29274505092327313,1.0 0.16666666666666663,1.0,1.0,0.0,0.0,1.0,0.0,0.0,0.3480564130053049,0.5 0.9999999999999999,1.0,0.0,0.0,0.0,1.0,0.0,0.0,0.42799208887081575,1.0 0.6666666666666666,0.0,0.0,0.0,0.0,1.0,1.0,0.0,0.2248313339864328,1.0 0.8333333333333334,0.0,0.0,0.0,0.0,0.0,1.0,1.0,0.1280235115801926,0.5 0.16666666666666663,0.0,1.0,0.0,1.0,1.0,0.0,1.0,0.40164507125561455,1.0 0.6666666666666666,1.0,0.0,0.0,0.0,1.0,0.0,1.0,0.2963420269495943,0.5 0.8333333333333334,1.0,0.0,0.0,0.0,1.0,0.0,1.0,0.2486090830114046,0.0 0.8333333333333334,0.0,0.0,0.0,0.0,1.0,1.0,1.0,0.28923310105173655,1.0 0.0,0.0,0.0,0.0,1.0,1.0,0.0,1.0,0.06283432838579693,0.0 0.4999999999999999,1.0,0.0,1.0,0.0,1.0,0.0,0.0,0.29536607456424097,0.5 0.4999999999999999,0.0,0.0,0.0,1.0,1.0,0.0,1.0,0.2693258904641319,1.0 0.0,1.0,0.0,1.0,0.0,0.0,0.0,1.0,0.10460804791038984,0.5 0.4999999999999999,1.0,1.0,0.0,0.0,1.0,0.0,0.0,0.4651485185116726,0.5",0,tensorboard crashes second model run,"tensorboard crashes second model run tensorboard keras crashes deep callbacks end epoch. here's simplified version code code import pandas pd import keras keras.models import sequential keras.layers import dense class toynet def init self, run 1, layer 1 nodes 50, layer 2 nodes 100, layer 3 nodes 50 self.run seq run self.layer 1 nodes layer 1 nodes self.layer 2 nodes layer 2 nodes self.layer 3 nodes layer 3 nodes define model self.model sequential self.model.add dense self.layer 1 nodes, input dim 9, activation 'relu', name 'layer 1' self.model.add dense self.layer 2 nodes, activation 'relu', name 'layer 2' self.model.add dense self.layer 3 nodes, activation 'relu', name 'layer 3' self.model.add dense 1, activation 'linear', name 'output layer' self.model.compile loss 'mean squared error', optimizer 'adam' create tensorboard logger log dir logs .format self.run seq self.logger keras.callbacks.tensorboard log dir log dir, histogram freq 5 def train self, x, y, epochs 50 train model self.model.fit x, y, epochs epochs, shuffle true, verbose 2, validation split 0.05, callbacks self.logger def test self, xt, yt test error rate self.model.evaluate xt, yt, verbose 0 print mean squared error mse test data set .format test error rate name ' main ' training data df pd.read csv sales data training scaled.csv x training data df.drop 'total earnings', axis 1 .values training data df 'total earnings' .values load test data set test data df pd.read csv sales data test scaled.csv x test test data df.drop 'total earnings', axis 1 .values test test data df 'total earnings' .values print run 1 toy1 toynet 1, 50, 100, 50 toy1.train x, toy1.test x test, test print run 2 toy2 toynet 2, 5, 100, 50 toy2.train x,y crashes end first epoch callbacks toy2.test x test, test output anaconda3 lib python3.6 site packages h5py init .py 36 futurewarning conversion second argument issubdtype deprecated. future, treated . . conv import register converters register converters using tensorflow backend. run 1 train 950 samples, validate 50 samples epoch 1 50 1s loss 0.0314 val loss 0.0043 epoch 2 50 0s loss 0.0048 val loss 0.0011 output snipped brevity epoch 50 50 0s loss 2.4237e 05 val loss 5.0361e 05 mean squared error mse test data set 7.575784547952935e 05 run 2 train 950 samples, validate 50 samples epoch 1 50 1s loss 0.0333 val loss 0.0172 invalidargumenterror traceback recent call last anaconda3 lib python3.6 site packages tensorflow python client session.py call self, fn, args 1321 try 1322 return fn args 1323 except errors.operror e anaconda3 lib python3.6 site packages tensorflow python client session.py run fn feed dict, fetch list, target list, options, run metadata 1306 return self. call tf sessionrun 1307 options, feed dict, fetch list, target list, run metadata 1308 anaconda3 lib python3.6 site packages tensorflow python client session.py call tf sessionrun self, options, feed dict, fetch list, target list, run metadata 1408 self. session, options, feed dict, fetch list, target list, 1409 run metadata 1410 else invalidargumenterror must feed value placeholder tensor 'layer 1 input' dtype float shape ?,9 node layer 1 input placeholder dtype dt float, shape ?,9 , device job localhost replica 0 task 0 device cpu 0 handling exception, another exception occurred invalidargumenterror traceback recent call last 64 65 toy2 toynet 2, 5, 100, 50 66 toy2.train x,y crashes end first epoch callbacks 67 toy2.test x test, test train self, x, y, epochs 39 verbose 2, 40 validation split 0.05, 41 callbacks self.logger 42 43 anaconda3 lib python3.6 site packages keras engine training.py fit self, x, y, batch size, epochs, verbose, callbacks, validation split, validation data, shuffle, class weight, sample weight, initial epoch, steps per epoch, validation steps, kwargs 1043 initial epoch initial epoch, 1044 steps per epoch steps per epoch, 1045 validation steps validation steps 1046 1047 def evaluate self, x none, none, anaconda3 lib python3.6 site packages keras engine training arrays.py fit loop model, f, ins, labels, batch size, epochs, verbose, callbacks, val f, val ins, shuffle, callback metrics, initial epoch, steps per epoch, validation steps 215 l, zip labels, val outs 216 epoch logs 'val ' l 217 callbacks.on epoch end epoch, epoch logs 218 callback model.stop training 219 break anaconda3 lib python3.6 site packages keras callbacks.py epoch end self, epoch, logs 75 logs logs 76 callback self.callbacks 77 callback.on epoch end epoch, logs 78 79 def batch begin self, batch, logs none anaconda3 lib python3.6 site packages keras callbacks.py epoch end self, epoch, logs 915 assert len batch val len tensors 916 feed dict dict zip tensors, batch val 917 result self.sess.run self.merged , feed dict feed dict 918 summary str result 0 919 self.writer.add summary summary str, epoch anaconda3 lib python3.6 site packages tensorflow python client session.py run self, fetches, feed dict, options, run metadata 898 try 899 result self. run none, fetches, feed dict, options ptr, 900 run metadata ptr 901 run metadata 902 proto data tf session.tf getbuffer run metadata ptr anaconda3 lib python3.6 site packages tensorflow python client session.py run self, handle, fetches, feed dict, options, run metadata 1133 final fetches final targets handle feed dict tensor 1134 results self. run handle, final targets, final fetches, 1135 feed dict tensor, options, run metadata 1136 else 1137 results anaconda3 lib python3.6 site packages tensorflow python client session.py run self, handle, target list, fetch list, feed dict, options, run metadata 1314 handle none 1315 return self. call run fn, feeds, fetches, targets, options, 1316 run metadata 1317 else 1318 return self. call prun fn, handle, feeds, fetches anaconda3 lib python3.6 site packages tensorflow python client session.py call self, fn, args 1333 except keyerror 1334 pass 1335 raise type e node def, op, message 1336 1337 def extend graph self invalidargumenterror must feed value placeholder tensor 'layer 1 input' dtype float shape ?,9 node layer 1 input placeholder dtype dt float, shape ?,9 , device job localhost replica 0 task 0 device cpu 0 caused op 'layer 1 input', defined file anaconda3 lib python3.6 runpy.py , line 193, run module main main , mod spec file anaconda3 lib python3.6 runpy.py , line 85, run code exec code, run globals file anaconda3 lib python3.6 site packages ipykernel launcher.py , line 16, app.launch new instance file anaconda3 lib python3.6 site packages traitlets config application.py , line 658, launch instance app.start file anaconda3 lib python3.6 site packages ipykernel kernelapp.py , line 486, start self.io loop.start file anaconda3 lib python3.6 site packages tornado platform asyncio.py , line 127, start self.asyncio loop.run forever file anaconda3 lib python3.6 asyncio base events.py , line 422, run forever self. run file anaconda3 lib python3.6 asyncio base events.py , line 1432, run handle. run file anaconda3 lib python3.6 asyncio events.py , line 145, run self. callback self. args file anaconda3 lib python3.6 site packages tornado platform asyncio.py , line 117, handle events handler func fileobj, events file anaconda3 lib python3.6 site packages tornado stack context.py , line 276, null wrapper return fn args, kwargs file anaconda3 lib python3.6 site packages zmq eventloop zmqstream.py , line 450, handle events self. handle recv file anaconda3 lib python3.6 site packages zmq eventloop zmqstream.py , line 480, handle recv self. run callback callback, msg file anaconda3 lib python3.6 site packages zmq eventloop zmqstream.py , line 432, run callback callback args, kwargs file anaconda3 lib python3.6 site packages tornado stack context.py , line 276, null wrapper return fn args, kwargs file anaconda3 lib python3.6 site packages ipykernel kernelbase.py , line 283, dispatcher return self.dispatch shell stream, msg file anaconda3 lib python3.6 site packages ipykernel kernelbase.py , line 233, dispatch shell handler stream, idents, msg file anaconda3 lib python3.6 site packages ipykernel kernelbase.py , line 399, execute request user expressions, allow stdin file anaconda3 lib python3.6 site packages ipykernel ipkernel.py , line 208, execute res shell.run cell code, store history store history, silent silent file anaconda3 lib python3.6 site packages ipykernel zmqshell.py , line 537, run cell return super zmqinteractiveshell, self .run cell args, kwargs file anaconda3 lib python3.6 site packages ipython core interactiveshell.py , line 2662, run cell raw cell, store history, silent, shell futures file anaconda3 lib python3.6 site packages ipython core interactiveshell.py , line 2785, run cell interactivity interactivity, compiler compiler, result result file anaconda3 lib python3.6 site packages ipython core interactiveshell.py , line 2903, run ast nodes self.run code code, result file anaconda3 lib python3.6 site packages ipython core interactiveshell.py , line 2963, run code exec code obj, self.user global ns, self.user ns file , line 61, toy1 toynet 1, 50, 100, 50 file , line 19, init self.model.add dense self.layer 1 nodes, input dim 9, activation 'relu', name 'layer 1' file anaconda3 lib python3.6 site packages keras engine sequential.py , line 160, add name layer.name ' input' file anaconda3 lib python3.6 site packages keras engine input layer.py , line 178, input input tensor tensor file anaconda3 lib python3.6 site packages keras legacy interfaces.py , line 91, wrapper return func args, kwargs file anaconda3 lib python3.6 site packages keras engine input layer.py , line 87, init name self.name file anaconda3 lib python3.6 site packages keras backend tensorflow backend.py , line 517, placeholder x tf.placeholder dtype, shape shape, name name file anaconda3 lib python3.6 site packages tensorflow python ops array ops.py , line 1734, placeholder return gen array ops.placeholder dtype dtype, shape shape, name name file anaconda3 lib python3.6 site packages tensorflow python ops gen array ops.py , line 4924, placeholder placeholder , dtype dtype, shape shape, name name file anaconda3 lib python3.6 site packages tensorflow python framework op def library.py , line 787, apply op helper op def op def file anaconda3 lib python3.6 site packages tensorflow python framework ops.py , line 3414, create op op def op def file anaconda3 lib python3.6 site packages tensorflow python framework ops.py , line 1740, init self. traceback self. graph. extract stack pylint disable protected access invalidargumenterror see traceback must feed value placeholder tensor 'layer 1 input' dtype float shape ?,9 node layer 1 input placeholder dtype dt float, shape ?,9 , device job localhost replica 0 task 0 device cpu 0 data sanity's sake included short sample training testing data below. training data sales data training scaled.csv critic rating,is action,is exclusive us,is portable,is role playing,is sequel,is sports,suitable kids,total earnings,unit price 0.4999999999999999,1.0,1.0,1.0,0.0,1.0,0.0,1.0,0.7991793127668619,1.0 0.16666666666666663,0.0,0.0,0.0,1.0,1.0,0.0,0.0,0.15750170976506905,1.0 0.4999999999999999,0.0,0.0,0.0,0.0,1.0,1.0,0.0,0.18970444169239015,1.0 0.6666666666666666,1.0,1.0,0.0,0.0,1.0,0.0,0.0,0.39223304559989647,0.0 0.0,1.0,0.0,1.0,0.0,1.0,0.0,0.0,0.21546366980277626,1.0 0.4999999999999999,1.0,0.0,1.0,0.0,0.0,0.0,1.0,0.2675699155283636,1.0 0.6666666666666666,1.0,0.0,0.0,0.0,0.0,0.0,1.0,0.2418106874179775,1.0 0.4999999999999999,1.0,1.0,1.0,0.0,1.0,0.0,0.0,0.7090183175911721,1.0 0.6666666666666666,1.0,1.0,0.0,0.0,0.0,0.0,0.0,0.4385279384854254,1.0 0.16666666666666663,0.0,1.0,0.0,1.0,1.0,0.0,1.0,0.3957893569434946,1.0 0.9999999999999999,1.0,0.0,0.0,0.0,1.0,0.0,0.0,0.24197334614886973,0.0 0.6666666666666666,1.0,0.0,0.0,0.0,1.0,0.0,0.0,0.33607142197001905,1.0 0.4999999999999999,0.0,1.0,0.0,0.0,0.0,1.0,0.0,0.26054601578529046,1.0 0.6666666666666666,0.0,0.0,1.0,1.0,1.0,0.0,0.0,0.3501229182455038,1.0 0.8333333333333334,0.0,1.0,1.0,0.0,1.0,1.0,0.0,0.39457681004047984,0.0 0.6666666666666666,0.0,0.0,0.0,0.0,1.0,1.0,0.0,0.2248313339864328,1.0 0.4999999999999999,0.0,0.0,1.0,0.0,1.0,1.0,1.0,0.28864900833625995,1.0 0.4999999999999999,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.05268664165172547,0.0 0.9999999999999999,1.0,0.0,0.0,0.0,1.0,0.0,0.0,0.24431711058945305,0.0 0.16666666666666663,1.0,0.0,1.0,0.0,1.0,0.0,0.0,0.2740097225559601,1.0 0.33333333333333337,0.0,0.0,0.0,1.0,1.0,0.0,0.0,0.20141217352729157,1.0 0.4999999999999999,0.0,1.0,0.0,1.0,1.0,0.0,0.0,0.2802240254339107,0.0 0.16666666666666663,0.0,1.0,0.0,1.0,1.0,0.0,1.0,0.4069129960629193,1.0 0.6666666666666666,0.0,0.0,1.0,1.0,0.0,0.0,0.0,0.21487957708729966,1.0 0.4999999999999999,1.0,0.0,1.0,0.0,1.0,0.0,1.0,0.4250642317147557,1.0 0.4999999999999999,0.0,0.0,1.0,0.0,1.0,1.0,1.0,0.28513336167538494,1.0 0.33333333333333337,1.0,1.0,0.0,0.0,1.0,0.0,0.0,0.40075044823570727,0.5 0.4999999999999999,1.0,0.0,0.0,0.0,1.0,0.0,1.0,0.18811482227685256,0.0 0.33333333333333337,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.1428661207741077,1.0 0.8333333333333334,1.0,0.0,1.0,0.0,0.0,0.0,1.0,0.27292286649045305,0.5 0.16666666666666663,0.0,0.0,0.0,1.0,1.0,0.0,0.0,0.16160144914142066,1.0 0.4999999999999999,1.0,1.0,0.0,0.0,1.0,0.0,0.0,0.5831426406166245,1.0 0.6666666666666666,1.0,0.0,0.0,0.0,1.0,0.0,0.0,0.18850668194672926,0.0 0.8333333333333334,0.0,1.0,0.0,1.0,1.0,0.0,0.0,0.6118297258830706,1.0 0.9999999999999999,0.0,0.0,0.0,1.0,1.0,0.0,0.0,0.18928670449714424,0.0 0.4999999999999999,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.13759449917746436,1.0 0.9999999999999999,0.0,1.0,0.0,0.0,1.0,1.0,0.0,0.34539842147095245,0.0 0.16666666666666663,0.0,0.0,0.0,1.0,1.0,0.0,1.0,0.13583113066301916,0.5 0.9999999999999999,0.0,1.0,1.0,0.0,1.0,1.0,1.0,0.48629415352766125,0.0 0.9999999999999999,0.0,1.0,0.0,1.0,1.0,0.0,1.0,0.7500009241973347,1.0 0.0,0.0,0.0,0.0,1.0,1.0,0.0,0.0,0.04253895491765401,0.0 0.4999999999999999,0.0,0.0,0.0,1.0,1.0,0.0,0.0,0.11630468937727585,0.0 0.16666666666666663,1.0,1.0,0.0,0.0,0.0,0.0,0.0,0.2775253692168352,1.0 0.4999999999999999,0.0,0.0,1.0,0.0,0.0,1.0,0.0,0.10216816694700652,0.5 0.4999999999999999,0.0,1.0,0.0,0.0,1.0,1.0,0.0,0.23650949150662648,0.0 0.33333333333333337,1.0,0.0,1.0,0.0,1.0,0.0,0.0,0.18031089998336447,0.0 0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.05621337868061589,1.0 0.9999999999999999,1.0,1.0,1.0,0.0,1.0,0.0,0.0,0.7949538825530027,0.5 0.4999999999999999,1.0,0.0,0.0,0.0,1.0,0.0,1.0,0.18538289495573096,0.0 0.8333333333333334,0.0,0.0,0.0,0.0,1.0,1.0,0.0,0.2587900408495222,1.0 0.16666666666666663,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.08373227851610877,1.0 0.9999999999999999,1.0,1.0,1.0,0.0,0.0,0.0,1.0,0.7447329993900298,1.0 0.16666666666666663,1.0,0.0,0.0,0.0,1.0,0.0,0.0,0.20902386277517976,1.0 0.6666666666666666,1.0,0.0,0.0,0.0,1.0,0.0,0.0,0.18850668194672926,0.0 0.16666666666666663,0.0,0.0,1.0,0.0,1.0,1.0,0.0,0.12509565442413267,0.5 0.6666666666666666,1.0,0.0,0.0,0.0,1.0,0.0,0.0,0.3313875898781908,1.0 0.33333333333333337,1.0,0.0,1.0,0.0,1.0,0.0,1.0,0.3717861037688767,1.0 0.0,0.0,0.0,0.0,1.0,1.0,0.0,1.0,0.1434502134895843,1.0 0.33333333333333337,1.0,1.0,1.0,0.0,1.0,0.0,0.0,0.6329051219016284,1.0 0.8333333333333334,1.0,1.0,0.0,0.0,1.0,0.0,0.0,0.727753645958485,1.0 0.6666666666666666,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.15144267203933381,0.5 0.8333333333333334,1.0,1.0,1.0,0.0,1.0,0.0,0.0,0.8846601726400621,1.0 0.8333333333333334,0.0,0.0,0.0,1.0,1.0,0.0,0.0,0.30738433670357296,1.0 testing data sales data test scaled.csv critic rating,is action,is exclusive us,is portable,is role playing,is sequel,is sports,suitable kids,total earnings,unit price 0.4999999999999999,1.0,0.0,1.0,0.0,1.0,0.0,0.0,0.3747139609249367,1.0 0.8333333333333334,0.0,0.0,0.0,0.0,1.0,1.0,0.0,0.19242527864549636,0.5 0.33333333333333337,0.0,0.0,0.0,0.0,1.0,1.0,0.0,0.11485185116726125,0.5 0.8333333333333334,1.0,0.0,0.0,0.0,0.0,0.0,1.0,0.14245208036820023,0.0 0.6666666666666666,1.0,0.0,1.0,0.0,1.0,0.0,1.0,0.4806824273118796,1.0 0.6666666666666666,0.0,0.0,0.0,1.0,1.0,0.0,0.0,0.13972015304707863,0.0 0.0,1.0,0.0,0.0,0.0,1.0,0.0,0.0,0.11338792258923126,0.5 0.8333333333333334,0.0,0.0,1.0,1.0,1.0,0.0,1.0,0.44906748488937354,1.0 0.4999999999999999,0.0,0.0,1.0,0.0,0.0,1.0,0.0,0.06127428328496702,0.0 0.16666666666666663,1.0,0.0,0.0,0.0,1.0,0.0,0.0,0.20668009833459638,1.0 0.8333333333333334,1.0,0.0,1.0,0.0,1.0,0.0,0.0,0.4777545701558197,1.0 0.4999999999999999,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.13232657437015954,1.0 0.4999999999999999,0.0,0.0,0.0,1.0,1.0,0.0,0.0,0.17925361823256503,0.5 0.6666666666666666,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.16335742407718895,1.0 0.4999999999999999,0.0,0.0,0.0,1.0,1.0,0.0,0.0,0.23946692297739414,1.0 0.6666666666666666,0.0,0.0,0.0,1.0,1.0,0.0,1.0,0.31206816879540117,1.0 0.4999999999999999,1.0,0.0,0.0,0.0,1.0,0.0,0.0,0.2285281233248923,0.5 0.33333333333333337,1.0,0.0,0.0,0.0,1.0,0.0,1.0,0.29274505092327313,1.0 0.16666666666666663,1.0,1.0,0.0,0.0,1.0,0.0,0.0,0.3480564130053049,0.5 0.9999999999999999,1.0,0.0,0.0,0.0,1.0,0.0,0.0,0.42799208887081575,1.0 0.6666666666666666,0.0,0.0,0.0,0.0,1.0,1.0,0.0,0.2248313339864328,1.0 0.8333333333333334,0.0,0.0,0.0,0.0,0.0,1.0,1.0,0.1280235115801926,0.5 0.16666666666666663,0.0,1.0,0.0,1.0,1.0,0.0,1.0,0.40164507125561455,1.0 0.6666666666666666,1.0,0.0,0.0,0.0,1.0,0.0,1.0,0.2963420269495943,0.5 0.8333333333333334,1.0,0.0,0.0,0.0,1.0,0.0,1.0,0.2486090830114046,0.0 0.8333333333333334,0.0,0.0,0.0,0.0,1.0,1.0,1.0,0.28923310105173655,1.0 0.0,0.0,0.0,0.0,1.0,1.0,0.0,1.0,0.06283432838579693,0.0 0.4999999999999999,1.0,0.0,1.0,0.0,1.0,0.0,0.0,0.29536607456424097,0.5 0.4999999999999999,0.0,0.0,0.0,1.0,1.0,0.0,1.0,0.2693258904641319,1.0 0.0,1.0,0.0,1.0,0.0,0.0,0.0,1.0,0.10460804791038984,0.5 0.4999999999999999,1.0,1.0,0.0,0.0,1.0,0.0,0.0,0.4651485185116726,0.5"
keras,7755,"use following code export keras model tensorflow. getting following error using generated pb file. invalidargumenterror see traceback must feed value placeholder tensor 'batch normalization 1 keras learning phase' dtype bool node batch normalization 1 keras learning phase placeholder dtype dt bool, shape , device job localhost replica 0 task 0 cpu 0",0,invalidargumenterror,"invalidargumenterror use following code export keras model tensorflow. getting following error using generated pb file. invalidargumenterror see traceback must feed value placeholder tensor 'batch normalization 1 keras learning phase' dtype bool node batch normalization 1 keras learning phase placeholder dtype dt bool, shape , device job localhost replica 0 task 0 cpu 0"
keras,10965,"used rnn e.g. lstm , pass initial state either kwarg list tensors argument calling layer , initial state overrides stored states stateful rnn.",0,rnn stateful incompatible initial state,"rnn stateful incompatible initial state used rnn e.g. lstm , pass initial state either kwarg list tensors argument calling layer , initial state overrides stored states stateful rnn."
keras,13391,"system information written custom code opposed using stock example script provided tensorflow os platform distribution e.g., linux ubuntu 16.04 ubuntu 18.04 tensorflow installed source binary built source tensorflow version use command 2.0.0 i.e. release keras version 2.3.0 python version 3.7 conda bazel version compiling tensorflow source 0.26.1 gcc compiler version compiling tensorflow source 7.4.0 cuda cudnn version 10 7.6.4 gpu model memory rtx 2080 ti tesla v100 tried both. error occurs describe current behavior going francois chollet's book deep learning python running code jupyter notebooks tensorflow 2.0.0 backend keras 2.3.0. notebook 6.3, heading 1.6 using recurrent dropout fight overfitting model tensorflow.keras.layers.gru 32, dropout 0.2, recurrent dropout 0.2, input shape none, float data.shape 1 . data read earlier notebook jena climate 2009 2016.csv. get loss 699013271268870062080.0000 first epoch similar figures subsequent epochs. figure simply wrong see . original notebook francois chollet link github https github.com fchollet deep learning python notebooks blob master 6.3 advanced usage recurrent neural networks.ipynb includes correct output. describe expected behavior loss 1 2 epochs supposed around 0.3 code reproduce issue provide reproducible test case bare minimum necessary generate problem. download data follows cd mkdir datasets cd datasets mkdir jena climate cd jena climate wget https s3.amazonaws.com keras datasets jena climate 2009 2016.csv.zip unzip jena climate 2009 2016.csv.zip run jupyter notebook load notebook python 3.7 environment tensorflow 2.0.0 backend keras 2.30. run cell beginning notebook load data create generators get example heading 1.6. try run example. find loss terribly wrong. code heading 1.7 stacking recurrent layers also runs incorrectly. loss produced nan val loss nan around 0.3 . think problem layers.gru reproduced problem running tensorflow.keras tensorflow 2.0.0. problem also occurs running keras 2.3.0 tensorflow 1.1.4 backend. problem occur tensorflow.keras tensorflow 1.1.4.",0,tf2.0.0 keras2.3.0 keras.layers.gru incorrect output model.fit generator trying run francois chollet's notebook 32987,"tf2.0.0 keras2.3.0 keras.layers.gru incorrect output model.fit generator trying run francois chollet's notebook 32987 system information written custom code opposed using stock example script provided tensorflow os platform distribution e.g., linux ubuntu 16.04 ubuntu 18.04 tensorflow installed source binary built source tensorflow version use command 2.0.0 i.e. release keras version 2.3.0 python version 3.7 conda bazel version compiling tensorflow source 0.26.1 gcc compiler version compiling tensorflow source 7.4.0 cuda cudnn version 10 7.6.4 gpu model memory rtx 2080 ti tesla v100 tried both. error occurs describe current behavior going francois chollet's book deep learning python running code jupyter notebooks tensorflow 2.0.0 backend keras 2.3.0. notebook 6.3, heading 1.6 using recurrent dropout fight overfitting model tensorflow.keras.layers.gru 32, dropout 0.2, recurrent dropout 0.2, input shape none, float data.shape 1 . data read earlier notebook jena climate 2009 2016.csv. get loss 699013271268870062080.0000 first epoch similar figures subsequent epochs. figure simply wrong see . original notebook francois chollet link github https github.com fchollet deep learning python notebooks blob master 6.3 advanced usage recurrent neural networks.ipynb includes correct output. describe expected behavior loss 1 2 epochs supposed around 0.3 code reproduce issue provide reproducible test case bare minimum necessary generate problem. download data follows cd mkdir datasets cd datasets mkdir jena climate cd jena climate wget https s3.amazonaws.com keras datasets jena climate 2009 2016.csv.zip unzip jena climate 2009 2016.csv.zip run jupyter notebook load notebook python 3.7 environment tensorflow 2.0.0 backend keras 2.30. run cell beginning notebook load data create generators get example heading 1.6. try run example. find loss terribly wrong. code heading 1.7 stacking recurrent layers also runs incorrectly. loss produced nan val loss nan around 0.3 . think problem layers.gru reproduced problem running tensorflow.keras tensorflow 2.0.0. problem also occurs running keras 2.3.0 tensorflow 1.1.4 backend. problem occur tensorflow.keras tensorflow 1.1.4."
keras,10784,"loading nested models including batchnormalisation 1d convolutions fail following traceback able trim minimum example https gist.github.com dapid cd1be6e9d9d8d61f7f225544f0967e35 inspecting shapes, seems like trying load bias vector conv1d bias kernel, course shapes match. unable trace back original place. model trained tensorflow. possibly related https github.com keras team keras issues 10777 also, inspecting shapes loading weights, possible batch normalisation loaded, would explain https github.com keras team keras issues 10780 among others.",0,wrong loading weights nested models,"wrong loading weights nested models loading nested models including batchnormalisation 1d convolutions fail following traceback able trim minimum example https gist.github.com dapid cd1be6e9d9d8d61f7f225544f0967e35 inspecting shapes, seems like trying load bias vector conv1d bias kernel, course shapes match. unable trace back original place. model trained tensorflow. possibly related https github.com keras team keras issues 10777 also, inspecting shapes loading weights, possible batch normalisation loaded, would explain https github.com keras team keras issues 10780 among others."
keras,10938,"trying build seq2seq model generates sentences classifies them. latter, want feed decoder's softmax output pre trained classifier. classifier however uses keras embedding layer passing raw softmax classifier option. thought could use gumbel softmax get one hot encoding use onehotembedding layer found https github.com keras team keras issues 2505 solve this. eric jang provided tensorflow code gumbel softmax wanted know turn keras layer. particular, interested property ensures vector forward pass strictly categorical backward pass, gradient gumbel softmax output. anyone help please? thanks.",0,gumbel softmax keras,"gumbel softmax keras trying build seq2seq model generates sentences classifies them. latter, want feed decoder's softmax output pre trained classifier. classifier however uses keras embedding layer passing raw softmax classifier option. thought could use gumbel softmax get one hot encoding use onehotembedding layer found https github.com keras team keras issues 2505 solve this. eric jang provided tensorflow code gumbel softmax wanted know turn keras layer. particular, interested property ensures vector forward pass strictly categorical backward pass, gradient gumbel softmax output. anyone help please? thanks."
keras,12383,"please make sure boxes checked submit issue. issue implementation question , please ask question stackoverflow http stackoverflow.com questions tagged keras keras slack channel https keras slack autojoin.herokuapp.com instead opening github issue. thank you! x check date master branch keras. update x check version tensorflow date. installation instructions found https www.tensorflow.org get started os setup . x provide link github gist python script reproduce issue copy script short .",0,keras session.graph.as default attributeerror 'nonetype' object attribute 'graph',"keras session.graph.as default attributeerror 'nonetype' object attribute 'graph' please make sure boxes checked submit issue. issue implementation question , please ask question stackoverflow http stackoverflow.com questions tagged keras keras slack channel https keras slack autojoin.herokuapp.com instead opening github issue. thank you! x check date master branch keras. update x check version tensorflow date. installation instructions found https www.tensorflow.org get started os setup . x provide link github gist python script reproduce issue copy script short ."
keras,13504,"hi, work waymo open dataset https waymo.com open . like make easy possible users get running it. might go adding keras.datasets? thank you!",0,adding waymo open dataset keras.datasets,"adding waymo open dataset keras.datasets hi, work waymo open dataset https waymo.com open . like make easy possible users get running it. might go adding keras.datasets? thank you!"
keras,8065,"x check date master branch keras. update pip install git git github.com fchollet keras.git upgrade deps x running tensorflow, check date latest version. installation instructions found https www.tensorflow.org get started os setup . x provide link github gist python script reproduce issue copy script short . excluding code load preprocess imagenetutils.preprocess input . data kaggle cats dogs data. never gets beyond 50 60 accuracy. vgg16 quickly gets 98 . remove tensorflow.contrib.keras.api. start imports, also gets 98 . something definitely different tf base keras. sure what.",0,"resnet50 tf.keras fine tune, one base keras","resnet50 tf.keras fine tune, one base keras x check date master branch keras. update pip install git git github.com fchollet keras.git upgrade deps x running tensorflow, check date latest version. installation instructions found https www.tensorflow.org get started os setup . x provide link github gist python script reproduce issue copy script short . excluding code load preprocess imagenetutils.preprocess input . data kaggle cats dogs data. never gets beyond 50 60 accuracy. vgg16 quickly gets 98 . remove tensorflow.contrib.keras.api. start imports, also gets 98 . something definitely different tf base keras. sure what."
keras,10058,"current keras code tensorflow backend, non training batch norm operator layer finally run tensorflow old non fused batch norm api tf.nn.batch normalization , result bad performance cpu gpu. tensorflow tf.nn.batch normalization non fused batch norm, computations using several individual ops, tensorflow tf.nn.fused batch norm fused batch norm new implementation comprise several ops one, much better performance. following code shows place calling tensorflow's non fused batch norm happen, suggest call tensorflow's fused batch norm api. output x mean sqrt var epsilon gamma beta thanks.",0,non training batch norm operator bad performance running tensorflow's non fused batch norm api,"non training batch norm operator bad performance running tensorflow's non fused batch norm api current keras code tensorflow backend, non training batch norm operator layer finally run tensorflow old non fused batch norm api tf.nn.batch normalization , result bad performance cpu gpu. tensorflow tf.nn.batch normalization non fused batch norm, computations using several individual ops, tensorflow tf.nn.fused batch norm fused batch norm new implementation comprise several ops one, much better performance. following code shows place calling tensorflow's non fused batch norm happen, suggest call tensorflow's fused batch norm api. output x mean sqrt var epsilon gamma beta thanks."
keras,7681,"using keras tensorflow backend. creating simple 1 layer lstm model. need code give val loss every time train data. running system cpu only. tried numpy.random import seed seed 1337 tensorflow import set random seed set random seed 1337 top code. also initialized kernels recurrent ones model.add lstm 150,input shape none,124 , w regularizer l2 0.001 ,kernel initializer 'ones', recurrent initializer 'ones', bias initializer 'ones' model.add dense 2,kernel initializer 'ones', bias initializer 'ones' model.add activation softmax also set shuffle false model.fit . using rms optimizing. also set pythonhashseed 0. still getting different train loss well val loss epoch rum multiple times. running keras server 56 cpu cores centos. pls help soon tried everything everyone suggested threads !!!!!",0,getting different result training mlp lstm layer keras,"getting different result training mlp lstm layer keras using keras tensorflow backend. creating simple 1 layer lstm model. need code give val loss every time train data. running system cpu only. tried numpy.random import seed seed 1337 tensorflow import set random seed set random seed 1337 top code. also initialized kernels recurrent ones model.add lstm 150,input shape none,124 , w regularizer l2 0.001 ,kernel initializer 'ones', recurrent initializer 'ones', bias initializer 'ones' model.add dense 2,kernel initializer 'ones', bias initializer 'ones' model.add activation softmax also set shuffle false model.fit . using rms optimizing. also set pythonhashseed 0. still getting different train loss well val loss epoch rum multiple times. running keras server 56 cpu cores centos. pls help soon tried everything everyone suggested threads !!!!!"
keras,12602,"use model target values shape running method runs fine. switching target values shape method raises error error checking target expected activation 1 3 dimensions, got array shape 20, 10 model use inp input batch shape batch size, seq size mat embedding num classes, num lstms inp mat lstm num lstms, return sequences true, stateful true mat mat lstm num lstms, return sequences true, stateful true mat mat dropout 0.5 mat mat timedistributed dense vocab size mat 1 activation 'softmax' mat model model inputs inp , outputs 1",0,error checking target expected activation 1 3 dimensions,"error checking target expected activation 1 3 dimensions use model target values shape running method runs fine. switching target values shape method raises error error checking target expected activation 1 3 dimensions, got array shape 20, 10 model use inp input batch shape batch size, seq size mat embedding num classes, num lstms inp mat lstm num lstms, return sequences true, stateful true mat mat lstm num lstms, return sequences true, stateful true mat mat dropout 0.5 mat mat timedistributed dense vocab size mat 1 activation 'softmax' mat model model inputs inp , outputs 1"
keras,13343,"classification network last two layers dense activation respectively. popped layers model activationlayer classifier.layers.pop denselayer classifier.layers.pop pushed layers back model outt1 denselayer classifier.layers 1 .output outt activationlayer outt1 classifier model classifier.input,outt indexing output dense layer get error classifier.layers 2 .output error layer out2 multiple inbound nodes, hence notion layer output ill defined.",0,unable extract layer output pushing layers back model,"unable extract layer output pushing layers back model classification network last two layers dense activation respectively. popped layers model activationlayer classifier.layers.pop denselayer classifier.layers.pop pushed layers back model outt1 denselayer classifier.layers 1 .output outt activationlayer outt1 classifier model classifier.input,outt indexing output dense layer get error classifier.layers 2 .output error layer out2 multiple inbound nodes, hence notion layer output ill defined."
keras,7728,"hi naive way implement group deconvolution3d layer keras 2.0 theano. please give suggestion it? e.g. better solution 1. theano, used compute default deconvolution. actually take number groups argument https github.com theano theano blob master theano tensor nnet abstract conv.py l1163 url 2. keras2.0, update add based update add based , call fix output dimension calculation thanks donglai",0,suggestion implementing group deconvolution3d layer,"suggestion implementing group deconvolution3d layer hi naive way implement group deconvolution3d layer keras 2.0 theano. please give suggestion it? e.g. better solution 1. theano, used compute default deconvolution. actually take number groups argument https github.com theano theano blob master theano tensor nnet abstract conv.py l1163 url 2. keras2.0, update add based update add based , call fix output dimension calculation thanks donglai"
keras,10611,adam recently demonstrated implemented incorrectly several packages including keras. propose fix optimizer using method described https arxiv.org pdf 1711.05101.pdf,0,fix adam optimizer implement paper correctly,fix adam optimizer implement paper correctly adam recently demonstrated implemented incorrectly several packages including keras. propose fix optimizer using method described https arxiv.org pdf 1711.05101.pdf
keras,2370,"could support plan support backends? theano support multiple gpu using opencl cuda, tensorflow seems slower framework using gpu now. could support caffe, leaf, mxnet, cntk torch7? ! image https cloud.githubusercontent.com assets 11470826 14587597 322ccb2e 04e8 11e6 8f82 b4759b5577fe.png ! image https cloud.githubusercontent.com assets 11470826 14587599 39d0fb2a 04e8 11e6 8300 be794b350bf2.png",0,feature request backends,"feature request backends could support plan support backends? theano support multiple gpu using opencl cuda, tensorflow seems slower framework using gpu now. could support caffe, leaf, mxnet, cntk torch7? ! image https cloud.githubusercontent.com assets 11470826 14587597 322ccb2e 04e8 11e6 8f82 b4759b5577fe.png ! image https cloud.githubusercontent.com assets 11470826 14587599 39d0fb2a 04e8 11e6 8300 be794b350bf2.png"
keras,10923,"please make sure boxes checked submit issue. issue implementation question, please ask question stackoverflow http stackoverflow.com questions tagged keras join keras slack channel https keras slack autojoin.herokuapp.com ask instead filing github issue. thank you! x check date master branch keras. update pip install git git github.com keras team keras.git upgrade deps x running tensorflow, check date latest version. installation instructions found https www.tensorflow.org get started os setup . running theano, check date master branch theano. update pip install git git github.com theano theano.git upgrade deps x provide link github gist python script reproduce issue copy script short . hello, collected several models .json format import using model json function. models different inputs outputs tensors respective first final layers. models sequential . api user select input shape data format coming flow generator. so, need modify architecture shape, order take data format user specified. however, spending two days looking alternative ways could find effective way changing architecture input layer. keras supposed intuitive. xp hereby, give example small model architecture generated ideas tried. honestly, really appreciate help this. model goal make receive batch data shape none,32,32,3 , 'channels last' make output batch data shape none,6 attempted 1. using set shape 2. using config 3. changing tensor directly def changeinputsshape model, inputs assert len model.inputs len inputs , model inputs assigned inputs mismatch. range len inputs model.inputs keras.layers.input shape inputs reason 1. set shape update non defined dimensions previously defined tensor shape. 2. setting configuration update input tensors themselves, compiles run. 3. model.build prior compiling 3. compiling note note weights knowledge tranfer problem since model trained relatively simpler. guess not. help would appreciated, andre",0,cannot modified established architecture shape.,"cannot modified established architecture shape. please make sure boxes checked submit issue. issue implementation question, please ask question stackoverflow http stackoverflow.com questions tagged keras join keras slack channel https keras slack autojoin.herokuapp.com ask instead filing github issue. thank you! x check date master branch keras. update pip install git git github.com keras team keras.git upgrade deps x running tensorflow, check date latest version. installation instructions found https www.tensorflow.org get started os setup . running theano, check date master branch theano. update pip install git git github.com theano theano.git upgrade deps x provide link github gist python script reproduce issue copy script short . hello, collected several models .json format import using model json function. models different inputs outputs tensors respective first final layers. models sequential . api user select input shape data format coming flow generator. so, need modify architecture shape, order take data format user specified. however, spending two days looking alternative ways could find effective way changing architecture input layer. keras supposed intuitive. xp hereby, give example small model architecture generated ideas tried. honestly, really appreciate help this. model goal make receive batch data shape none,32,32,3 , 'channels last' make output batch data shape none,6 attempted 1. using set shape 2. using config 3. changing tensor directly def changeinputsshape model, inputs assert len model.inputs len inputs , model inputs assigned inputs mismatch. range len inputs model.inputs keras.layers.input shape inputs reason 1. set shape update non defined dimensions previously defined tensor shape. 2. setting configuration update input tensors themselves, compiles run. 3. model.build prior compiling 3. compiling note note weights knowledge tranfer problem since model trained relatively simpler. guess not. help would appreciated, andre"
keras,10388,"use horovod keras multi gpus job, found use multi gpus process count image num use flow directory, image num different ! https user images.githubusercontent.com 3112825 41193236 f8df388a 6c3b 11e8 8ee7 2b7ac54a2c34.png code could anyone help ? strange, maybe caused multiprocessing ?",0,flow directory error use multi gpus,"flow directory error use multi gpus use horovod keras multi gpus job, found use multi gpus process count image num use flow directory, image num different ! https user images.githubusercontent.com 3112825 41193236 f8df388a 6c3b 11e8 8ee7 2b7ac54a2c34.png code could anyone help ? strange, maybe caused multiprocessing ?"
keras,10689,sometimes modelcheckpoint save model all! why?,0,modelcheckpoint work sometimes.,modelcheckpoint work sometimes. sometimes modelcheckpoint save model all! why?
keras,11957,"tensorflow gpu keras. run tensorboard , uses cpu end epoch tensorboard writes logs. usually gpu used entirety training writing logs also true . issue https github.com keras team keras issues 3358 mentioned previous questions dissimilar issue occurs validation data passed via data generator histograms created reference cpu gpu. use data generator validation histograms created end. notes training carried keras . training dataset passed . validation dataset passed without data generator. edit switching keras crashes attempting write tensorboard logs runs fine . crash log below. notice switches gpu cpu. perhaps could something work with? invalidargumenterror must feed value placeholder tensor 'conv2d 1 input' dtype float shape ?,48,48,1 node conv2d 1 input placeholder dtype dt float, shape ?,48,48,1 , device job localhost replica 0 task 0 device gpu 0 node dense 2 bias read 407 recv client terminated false, recv device job localhost replica 0 task 0 device cpu 0 , send device job localhost replica 0 task 0 device gpu 0 , send device incarnation 1, tensor name edge 335 dense 2 bias read , tensor type dt float, device job localhost replica 0 task 0 device cpu 0 invalidargumenterror must feed value placeholder tensor 'conv2d 1 input' dtype float shape ?,48,48,1 node conv2d 1 input placeholder dtype dt float, shape ?,48,48,1 , device job localhost replica 0 task 0 device gpu 0 node dense 2 bias read 407 recv client terminated false, recv device job localhost replica 0 task 0 device cpu 0 , send device job localhost replica 0 task 0 device gpu 0 , send device incarnation 1, tensor name edge 335 dense 2 bias read , tensor type dt float, device job localhost replica 0 task 0 device cpu 0 anyone aware fix prevent cpu used enabled? ask writing logs cpu make training process five times longer.",0,tensorboard histogram freq switching gpu cpu write logs,"tensorboard histogram freq switching gpu cpu write logs tensorflow gpu keras. run tensorboard , uses cpu end epoch tensorboard writes logs. usually gpu used entirety training writing logs also true . issue https github.com keras team keras issues 3358 mentioned previous questions dissimilar issue occurs validation data passed via data generator histograms created reference cpu gpu. use data generator validation histograms created end. notes training carried keras . training dataset passed . validation dataset passed without data generator. edit switching keras crashes attempting write tensorboard logs runs fine . crash log below. notice switches gpu cpu. perhaps could something work with? invalidargumenterror must feed value placeholder tensor 'conv2d 1 input' dtype float shape ?,48,48,1 node conv2d 1 input placeholder dtype dt float, shape ?,48,48,1 , device job localhost replica 0 task 0 device gpu 0 node dense 2 bias read 407 recv client terminated false, recv device job localhost replica 0 task 0 device cpu 0 , send device job localhost replica 0 task 0 device gpu 0 , send device incarnation 1, tensor name edge 335 dense 2 bias read , tensor type dt float, device job localhost replica 0 task 0 device cpu 0 invalidargumenterror must feed value placeholder tensor 'conv2d 1 input' dtype float shape ?,48,48,1 node conv2d 1 input placeholder dtype dt float, shape ?,48,48,1 , device job localhost replica 0 task 0 device gpu 0 node dense 2 bias read 407 recv client terminated false, recv device job localhost replica 0 task 0 device cpu 0 , send device job localhost replica 0 task 0 device gpu 0 , send device incarnation 1, tensor name edge 335 dense 2 bias read , tensor type dt float, device job localhost replica 0 task 0 device cpu 0 anyone aware fix prevent cpu used enabled? ask writing logs cpu make training process five times longer."
keras,8151,"please make sure boxes checked submit issue. issue implementation question, please ask question stackoverflow http stackoverflow.com questions tagged keras join keras slack channel https keras slack autojoin.herokuapp.com ask instead filing github issue. thank you! check date master branch keras. update pip install git git github.com fchollet keras.git upgrade deps running tensorflow, check date latest version. installation instructions found https www.tensorflow.org get started os setup . running theano, check date master branch theano. update pip install git git github.com theano theano.git upgrade deps provide link github gist python script reproduce issue copy script short .",0,extract weights layer,"extract weights layer please make sure boxes checked submit issue. issue implementation question, please ask question stackoverflow http stackoverflow.com questions tagged keras join keras slack channel https keras slack autojoin.herokuapp.com ask instead filing github issue. thank you! check date master branch keras. update pip install git git github.com fchollet keras.git upgrade deps running tensorflow, check date latest version. installation instructions found https www.tensorflow.org get started os setup . running theano, check date master branch theano. update pip install git git github.com theano theano.git upgrade deps provide link github gist python script reproduce issue copy script short ."
keras,10239,"getting error code create model def baseline model model sequential model.add dense 4, activation 'relu', input shape 4, model.add dense 3, activation 'sigmoid' compile model model.compile optimizer 'adam', loss 'categorical crossentropy',metrics 'accuracy' return model order fit model estimator kerasclassifier build fn baseline model, nb epoch 200, verbose 0, batch size 5 kfold kfold n len x , n folds 10, shuffle true, random state seed results cross val score estimator, x, dummy y, cv kfold print accuracy .2f .2f results.mean 100, results.std 100",0,"valueerror error checking target expected dense 20 shape 3, got array shape 22,","valueerror error checking target expected dense 20 shape 3, got array shape 22, getting error code create model def baseline model model sequential model.add dense 4, activation 'relu', input shape 4, model.add dense 3, activation 'sigmoid' compile model model.compile optimizer 'adam', loss 'categorical crossentropy',metrics 'accuracy' return model order fit model estimator kerasclassifier build fn baseline model, nb epoch 200, verbose 0, batch size 5 kfold kfold n len x , n folds 10, shuffle true, random state seed results cross val score estimator, x, dummy y, cv kfold print accuracy .2f .2f results.mean 100, results.std 100"
keras,11465,"dear keras users, hello everyone. need help dealing activity regularizer keras. know, activity regularizer one adding 'norm output' loss. really similar tikhonov regularization, really working well deep learning? doubt differentiating term output norm loss weight w , becomes zero. actually majored computer science, know know much. want ask help know basic principle it. or, recommend related papers? thank much.",0,principle activity regularizer keras,"principle activity regularizer keras dear keras users, hello everyone. need help dealing activity regularizer keras. know, activity regularizer one adding 'norm output' loss. really similar tikhonov regularization, really working well deep learning? doubt differentiating term output norm loss weight w , becomes zero. actually majored computer science, know know much. want ask help know basic principle it. or, recommend related papers? thank much."
keras,10703,"please make sure boxes checked submit issue. issue implementation question, please ask question stackoverflow http stackoverflow.com questions tagged keras join keras slack channel https keras slack autojoin.herokuapp.com ask instead filing github issue. thank you! x check date master branch keras. update pip install git git github.com keras team keras.git upgrade deps x running tensorflow, check date latest version. installation instructions found https www.tensorflow.org get started os setup . x running theano, check date master branch theano. update pip install git git github.com theano theano.git upgrade deps x provide link github gist python script reproduce issue copy script short .",0,way set different learning rates different different layers cnn model keras?,"way set different learning rates different different layers cnn model keras? please make sure boxes checked submit issue. issue implementation question, please ask question stackoverflow http stackoverflow.com questions tagged keras join keras slack channel https keras slack autojoin.herokuapp.com ask instead filing github issue. thank you! x check date master branch keras. update pip install git git github.com keras team keras.git upgrade deps x running tensorflow, check date latest version. installation instructions found https www.tensorflow.org get started os setup . x running theano, check date master branch theano. update pip install git git github.com theano theano.git upgrade deps x provide link github gist python script reproduce issue copy script short ."
keras,13150,"please make sure bug feature request provide applicable information asked template. issue implementation question , please ask question stackoverflow http stackoverflow.com questions tagged keras keras slack channel https keras slack autojoin.herokuapp.com instead opening github issue. system information written custom code opposed using example directory os platform distribution e.g., linux ubuntu 16.04 linux tensorflow backend yes yes tensorflow version 1.12.0 keras version 2.2.4 python version 3.5.4 cuda cudnn version 8.0, v8.0.61 gpu model memory nvidia pascal p100 describe current behavior getting error tensorflow.python.framework.errors impl.invalidargumenterror must feed value placeholder tensor 'input 1' dtype float shape ?,1,512,20 node input 1 placeholder dtype dt float, shape ?,1,512,20 , device job localhost replica 0 task 0 device gpu 0 code reproduce issue provide reproducible test case bare minimum necessary generate problem. base network info logs file testkerasclean.py , line 329, history model.fit input a,input b ,output trg,batch size 100,shuffle true,epochs 100,class weight class weight val,validation split 0.2,verbose 1,callbacks tensorboard log dir '. graph',write graph true,write grads true,histogram freq 3 file project 6000341 saby2k13 dppi2 venv tensorflow lib python3.5 site packages tensorflow python keras engine training.py , line 1639, fit validation steps validation steps file project 6000341 saby2k13 dppi2 venv tensorflow lib python3.5 site packages tensorflow python keras engine training arrays.py , line 233, fit loop verbose 0 file project 6000341 saby2k13 dppi2 venv tensorflow lib python3.5 site packages tensorflow python keras engine training arrays.py , line 439, test loop batch outs f ins batch file project 6000341 saby2k13 dppi2 venv tensorflow lib python3.5 site packages tensorflow python keras backend.py , line 2986, call run metadata self.run metadata file project 6000341 saby2k13 dppi2 venv tensorflow lib python3.5 site packages tensorflow python client session.py , line 1439, call run metadata ptr file project 6000341 saby2k13 dppi2 venv tensorflow lib python3.5 site packages tensorflow python framework errors impl.py , line 528, exit c api.tf getcode self.status.status tensorflow.python.framework.errors impl.invalidargumenterror must feed value placeholder tensor 'input 1' dtype float shape ?,1,512,20 node input 1 placeholder dtype dt float, shape ?,1,512,20 , device job localhost replica 0 task 0 device gpu 0",0,tensorboard error histogram freq 0,"tensorboard error histogram freq 0 please make sure bug feature request provide applicable information asked template. issue implementation question , please ask question stackoverflow http stackoverflow.com questions tagged keras keras slack channel https keras slack autojoin.herokuapp.com instead opening github issue. system information written custom code opposed using example directory os platform distribution e.g., linux ubuntu 16.04 linux tensorflow backend yes yes tensorflow version 1.12.0 keras version 2.2.4 python version 3.5.4 cuda cudnn version 8.0, v8.0.61 gpu model memory nvidia pascal p100 describe current behavior getting error tensorflow.python.framework.errors impl.invalidargumenterror must feed value placeholder tensor 'input 1' dtype float shape ?,1,512,20 node input 1 placeholder dtype dt float, shape ?,1,512,20 , device job localhost replica 0 task 0 device gpu 0 code reproduce issue provide reproducible test case bare minimum necessary generate problem. base network info logs file testkerasclean.py , line 329, history model.fit input a,input b ,output trg,batch size 100,shuffle true,epochs 100,class weight class weight val,validation split 0.2,verbose 1,callbacks tensorboard log dir '. graph',write graph true,write grads true,histogram freq 3 file project 6000341 saby2k13 dppi2 venv tensorflow lib python3.5 site packages tensorflow python keras engine training.py , line 1639, fit validation steps validation steps file project 6000341 saby2k13 dppi2 venv tensorflow lib python3.5 site packages tensorflow python keras engine training arrays.py , line 233, fit loop verbose 0 file project 6000341 saby2k13 dppi2 venv tensorflow lib python3.5 site packages tensorflow python keras engine training arrays.py , line 439, test loop batch outs f ins batch file project 6000341 saby2k13 dppi2 venv tensorflow lib python3.5 site packages tensorflow python keras backend.py , line 2986, call run metadata self.run metadata file project 6000341 saby2k13 dppi2 venv tensorflow lib python3.5 site packages tensorflow python client session.py , line 1439, call run metadata ptr file project 6000341 saby2k13 dppi2 venv tensorflow lib python3.5 site packages tensorflow python framework errors impl.py , line 528, exit c api.tf getcode self.status.status tensorflow.python.framework.errors impl.invalidargumenterror must feed value placeholder tensor 'input 1' dtype float shape ?,1,512,20 node input 1 placeholder dtype dt float, shape ?,1,512,20 , device job localhost replica 0 task 0 device gpu 0"
keras,13597,"system information written custom code opposed using example directory yes os platform distribution e.g., linux ubuntu 16.04 windows 10 tensorflow backend yes yes tensorflow version 2.0 keras version 2.3.1 python version 3.7 cuda cudnn version none cpu describe current behavior attempting generate confidence intervals using dropout per logic https medium.com hal24k techblog generate neural network confidence intervals keras e4c0b78ebbdf unfortunately keep receiving error above? describe expected behavior expect new model initialised saved config. code reproduce issue provide reproducible test case bare minimum necessary generate problem. adding note mention train test forecast data generated via tf.data.dataset thank help!",0,"valueerror arguments signature arguments match. got 13, expected 14","valueerror arguments signature arguments match. got 13, expected 14 system information written custom code opposed using example directory yes os platform distribution e.g., linux ubuntu 16.04 windows 10 tensorflow backend yes yes tensorflow version 2.0 keras version 2.3.1 python version 3.7 cuda cudnn version none cpu describe current behavior attempting generate confidence intervals using dropout per logic https medium.com hal24k techblog generate neural network confidence intervals keras e4c0b78ebbdf unfortunately keep receiving error above? describe expected behavior expect new model initialised saved config. code reproduce issue provide reproducible test case bare minimum necessary generate problem. adding note mention train test forecast data generated via tf.data.dataset thank help!"
keras,10370,", validation data generator, passed callback parameter makes impossible access validation data use custom callbacks. https github.com keras team keras blob 632560d91286bf278228de72e7ce64f6c5aa530c keras engine training generator.py l92 l98",0,validation data passed param callback.set params fit generator,"validation data passed param callback.set params fit generator , validation data generator, passed callback parameter makes impossible access validation data use custom callbacks. https github.com keras team keras blob 632560d91286bf278228de72e7ce64f6c5aa530c keras engine training generator.py l92 l98"
keras,9684,"dear everyone, found roc auc score function https github.com tflearn tflearn blob master tflearn objectives.py. want write function keras. failed. following code tensortensortensortensortensortensortensortensor problem following 1 valueerror traceback recent call last 149 epochs epochs, 150 validation data validation generator, 151 validation steps validation samples batch size 152 153 model.save weights 'models basic cnn 30 epochs.h5' usr local lib python2.7 dist packages keras legacy interfaces.pyc wrapper args, kwargs 85 warnings.warn 'update call keras 2 api ' signature, stacklevel 2 87 return func args, kwargs 88 wrapper. original function func 89 return wrapper usr local lib python2.7 dist packages keras models.pyc fit generator self, generator, steps per epoch, epochs, verbose, callbacks, validation data, validation steps, class weight, max queue size, workers, use multiprocessing, initial epoch 1119 workers workers, 1120 use multiprocessing use multiprocessing, 1121 initial epoch initial epoch 1122 1123 interfaces.legacy generator methods support usr local lib python2.7 dist packages keras legacy interfaces.pyc wrapper args, kwargs 85 warnings.warn 'update call keras 2 api ' signature, stacklevel 2 87 return func args, kwargs 88 wrapper. original function func 89 return wrapper usr local lib python2.7 dist packages keras engine training.pyc fit generator self, generator, steps per epoch, epochs, verbose, callbacks, validation data, validation steps, class weight, max queue size, workers, use multiprocessing, shuffle, initial epoch 1924 1925 validation bool validation data 1926 self. make train function 1927 validation 1928 self. make test function usr local lib python2.7 dist packages keras engine training.pyc make train function self 958 training updates self.optimizer.get updates 959 params self. collected trainable weights, 960 loss self.total loss 961 updates self.updates training updates 962 gets loss metrics. updates weights call. usr local lib python2.7 dist packages keras legacy interfaces.pyc wrapper args, kwargs 85 warnings.warn 'update call keras 2 api ' signature, stacklevel 2 87 return func args, kwargs 88 wrapper. original function func 89 return wrapper usr local lib python2.7 dist packages keras optimizers.pyc get updates self, loss, params 235 p, g, zip params, grads, accumulators 236 update accumulator 237 new self.rho 1. self.rho k.square g 238 self.updates.append k.update a, new 239 new p p lr g k.sqrt new self.epsilon usr local lib python2.7 dist packages keras backend tensorflow backend.pyc square x 1356 tensor. 1357 1358 return tf.square x 1359 1360 usr local lib python2.7 dist packages tensorflow python ops math ops.pyc square x, name 447 indices x.indices, values x square, dense shape x.dense shape 448 else 449 return gen math ops.square x, name name 450 451 usr local lib python2.7 dist packages tensorflow python ops gen math ops.pyc square x, name 4565 ctx.in graph mode 4566 , , op op def lib. apply op helper 4567 square , x x, name name 4568 result op.outputs 4569 inputs flat op.inputs usr local lib python2.7 dist packages tensorflow python framework op def library.pyc apply op helper self, op type name, name, keywords 526 raise valueerror 527 tried convert ' s' tensor failed. error 528 input name, err 529 prefix input ' s' ' s' op type match 530 input name, op type name, observed valueerror tried convert 'x' tensor failed. error none values supported. finally, anyone check problem. looking forward reply. thanks advanced!",0,write objectives function roc auc score tflearn keras,"write objectives function roc auc score tflearn keras dear everyone, found roc auc score function https github.com tflearn tflearn blob master tflearn objectives.py. want write function keras. failed. following code tensortensortensortensortensortensortensortensor problem following 1 valueerror traceback recent call last 149 epochs epochs, 150 validation data validation generator, 151 validation steps validation samples batch size 152 153 model.save weights 'models basic cnn 30 epochs.h5' usr local lib python2.7 dist packages keras legacy interfaces.pyc wrapper args, kwargs 85 warnings.warn 'update call keras 2 api ' signature, stacklevel 2 87 return func args, kwargs 88 wrapper. original function func 89 return wrapper usr local lib python2.7 dist packages keras models.pyc fit generator self, generator, steps per epoch, epochs, verbose, callbacks, validation data, validation steps, class weight, max queue size, workers, use multiprocessing, initial epoch 1119 workers workers, 1120 use multiprocessing use multiprocessing, 1121 initial epoch initial epoch 1122 1123 interfaces.legacy generator methods support usr local lib python2.7 dist packages keras legacy interfaces.pyc wrapper args, kwargs 85 warnings.warn 'update call keras 2 api ' signature, stacklevel 2 87 return func args, kwargs 88 wrapper. original function func 89 return wrapper usr local lib python2.7 dist packages keras engine training.pyc fit generator self, generator, steps per epoch, epochs, verbose, callbacks, validation data, validation steps, class weight, max queue size, workers, use multiprocessing, shuffle, initial epoch 1924 1925 validation bool validation data 1926 self. make train function 1927 validation 1928 self. make test function usr local lib python2.7 dist packages keras engine training.pyc make train function self 958 training updates self.optimizer.get updates 959 params self. collected trainable weights, 960 loss self.total loss 961 updates self.updates training updates 962 gets loss metrics. updates weights call. usr local lib python2.7 dist packages keras legacy interfaces.pyc wrapper args, kwargs 85 warnings.warn 'update call keras 2 api ' signature, stacklevel 2 87 return func args, kwargs 88 wrapper. original function func 89 return wrapper usr local lib python2.7 dist packages keras optimizers.pyc get updates self, loss, params 235 p, g, zip params, grads, accumulators 236 update accumulator 237 new self.rho 1. self.rho k.square g 238 self.updates.append k.update a, new 239 new p p lr g k.sqrt new self.epsilon usr local lib python2.7 dist packages keras backend tensorflow backend.pyc square x 1356 tensor. 1357 1358 return tf.square x 1359 1360 usr local lib python2.7 dist packages tensorflow python ops math ops.pyc square x, name 447 indices x.indices, values x square, dense shape x.dense shape 448 else 449 return gen math ops.square x, name name 450 451 usr local lib python2.7 dist packages tensorflow python ops gen math ops.pyc square x, name 4565 ctx.in graph mode 4566 , , op op def lib. apply op helper 4567 square , x x, name name 4568 result op.outputs 4569 inputs flat op.inputs usr local lib python2.7 dist packages tensorflow python framework op def library.pyc apply op helper self, op type name, name, keywords 526 raise valueerror 527 tried convert ' s' tensor failed. error 528 input name, err 529 prefix input ' s' ' s' op type match 530 input name, op type name, observed valueerror tried convert 'x' tensor failed. error none values supported. finally, anyone check problem. looking forward reply. thanks advanced!"
keras,12393,"multi gpu machine trying train different models different gpus. trying set gpu programmatically rather use env vars, running issues. here's code however, try run code two separate jupyter notebooks, using ids 0 1, get following error works fine set top script, rather manage this. sense may wrong? using keras 2.2.4 tensorflow 1.13.1 ubuntu 18.04 jupyter 5.2.4 jupyter lab 0.35.4",0,error selecting gpu programmatically jupyter,"error selecting gpu programmatically jupyter multi gpu machine trying train different models different gpus. trying set gpu programmatically rather use env vars, running issues. here's code however, try run code two separate jupyter notebooks, using ids 0 1, get following error works fine set top script, rather manage this. sense may wrong? using keras 2.2.4 tensorflow 1.13.1 ubuntu 18.04 jupyter 5.2.4 jupyter lab 0.35.4"
keras,6810,package pydot can't installed conda python 3.6 platform work,0,issue conda 3.6 pydot plot model,issue conda 3.6 pydot plot model package pydot can't installed conda python 3.6 platform work
keras,11929,"hello! multi gpu model built using , function outputs incorrect information. precisely, following code output however, calling original model e.g. outputs correct architecture expected behaviour? related issue calling parallel model?",0,model.summary output problem multi gpu model,"model.summary output problem multi gpu model hello! multi gpu model built using , function outputs incorrect information. precisely, following code output however, calling original model e.g. outputs correct architecture expected behaviour? related issue calling parallel model?"
keras,9336,"hello, creating following network using cntk backend, getting error 'rnn dropout supported cntk backend using dynamic rnns i.e. non unrolled . either set , set 0, 'or use different backend.' however, cannot unroll, train x lstm.shape 1 1",0,rnn dropout recurrent dropout 1 timestep shows warning cntk,"rnn dropout recurrent dropout 1 timestep shows warning cntk hello, creating following network using cntk backend, getting error 'rnn dropout supported cntk backend using dynamic rnns i.e. non unrolled . either set , set 0, 'or use different backend.' however, cannot unroll, train x lstm.shape 1 1"
keras,10356,making issue comment https github.com keras team keras issues 10080 issuecomment 394640409 10080. converting weights cudnngru gru wrapped timedistributed conversion skipped mistake. similar bidirectional 8908 model sequential 10080 . example failure fails without timedistributed works ok direction plain cudnn . know wrapper layers need conversion?,0,cudnn rnn layers nested timedistributed converted loading,cudnn rnn layers nested timedistributed converted loading making issue comment https github.com keras team keras issues 10080 issuecomment 394640409 10080. converting weights cudnngru gru wrapped timedistributed conversion skipped mistake. similar bidirectional 8908 model sequential 10080 . example failure fails without timedistributed works ok direction plain cudnn . know wrapper layers need conversion?
keras,3424,"research papers http www.clsp.jhu.edu samuel pdfs annealed dropout.pdf showed benefit adjust dropout rate epochs. specifically, used dropout rate current epoch n fixed parameter. achieve using keras? think write call function dropout layer problem get epoch variable.",0,adjust dropout rate epochs,"adjust dropout rate epochs research papers http www.clsp.jhu.edu samuel pdfs annealed dropout.pdf showed benefit adjust dropout rate epochs. specifically, used dropout rate current epoch n fixed parameter. achieve using keras? think write call function dropout layer problem get epoch variable."
keras,3721,"hi all, currently running test simple autoencoders. copied pasted code keras blog entry https blog.keras.io building autoencoders keras.html however, testing different architectures, found even autoencoder zero nodes well, technically even existing first layer appears learning something. loss get comparable loss get bigger architectures. could bug keras autoencoder code might problem dataset quite noisy . intuition learn anything using layer zero nodes. suggestions would helpful! thanks lot!",0,autoencoder 0 nodes learns something,"autoencoder 0 nodes learns something hi all, currently running test simple autoencoders. copied pasted code keras blog entry https blog.keras.io building autoencoders keras.html however, testing different architectures, found even autoencoder zero nodes well, technically even existing first layer appears learning something. loss get comparable loss get bigger architectures. could bug keras autoencoder code might problem dataset quite noisy . intuition learn anything using layer zero nodes. suggestions would helpful! thanks lot!"
keras,7235,"x, y, sample weight x,",0,imagedatagenerator object iterator,"imagedatagenerator object iterator x, y, sample weight x,"
keras,109,"hi, reading tutorial nolearn lasagne http danielnouri.org notes 2014 12 17 using convolutional neural nets detect facial keypoints tutorial , author point possible update learning rate momentum time http danielnouri.org notes 2014 12 17 using convolutional neural nets detect facial keypoints tutorial changing learning rate momentum time . possible keras? coverage speed low, wondering whether possible use technique improve speed.",0,help wanted way update learning rate momentum,"help wanted way update learning rate momentum hi, reading tutorial nolearn lasagne http danielnouri.org notes 2014 12 17 using convolutional neural nets detect facial keypoints tutorial , author point possible update learning rate momentum time http danielnouri.org notes 2014 12 17 using convolutional neural nets detect facial keypoints tutorial changing learning rate momentum time . possible keras? coverage speed low, wondering whether possible use technique improve speed."
keras,703,"mentioned mega issue 100, though would worth separate one. simplest model zoo github wiki page, another option page documentation receive updates via push requests. think model zoo matters models take 6 hours train i.e. large fraction 24h .",0,models zoo,"models zoo mentioned mega issue 100, though would worth separate one. simplest model zoo github wiki page, another option page documentation receive updates via push requests. think model zoo matters models take 6 hours train i.e. large fraction 24h ."
keras,7814,"model looks something like problem ture.shape whenever try access trrue needed calculation inside loss function get true.shape true ?, ?, ?, ? true tensor decoder finalconv target 0 , shape ?, ?, ?, ? , dtype float32 thank you! check date master branch keras. update pip install git git github.com fchollet keras.git upgrade deps running tensorflow, check date latest version. installation instructions found https www.tensorflow.org get started os setup . running theano, check date master branch theano. update pip install git git github.com theano theano.git upgrade deps provide link github gist python script reproduce issue copy script short .",0,"ture.shape expected n,3 yet ?,?,?,? received","ture.shape expected n,3 yet ?,?,?,? received model looks something like problem ture.shape whenever try access trrue needed calculation inside loss function get true.shape true ?, ?, ?, ? true tensor decoder finalconv target 0 , shape ?, ?, ?, ? , dtype float32 thank you! check date master branch keras. update pip install git git github.com fchollet keras.git upgrade deps running tensorflow, check date latest version. installation instructions found https www.tensorflow.org get started os setup . running theano, check date master branch theano. update pip install git git github.com theano theano.git upgrade deps provide link github gist python script reproduce issue copy script short ."
keras,7055,"given variable length input, cntk fixes length first example. following example runs theano tensorflow. cntk fails, since seen first batch, assumes second dimension fixed 100",0,cntk cannot take variable length inputs,"cntk cannot take variable length inputs given variable length input, cntk fixes length first example. following example runs theano tensorflow. cntk fails, since seen first batch, assumes second dimension fixed 100"
keras,5083,"believe bug save load mechanism version 1.2.0. problem exist version 1.1.2. reproduce bug using following code snippet taken keras documentation fine tune inceptionv3 new set classes https keras.io applications traceback recent call last file , line 1, file usr local lib python2.7 dist packages keras models.py , line 143, load model model.load weights hdf5 group f 'model weights' file usr local lib python2.7 dist packages keras engine topology.py , line 2753, load weights hdf5 group str len flattened layers ' layers.' valueerror trying load weight file containing 190 layers model 2 layers.",0,bug save load mechanism version 1.2.0,"bug save load mechanism version 1.2.0 believe bug save load mechanism version 1.2.0. problem exist version 1.1.2. reproduce bug using following code snippet taken keras documentation fine tune inceptionv3 new set classes https keras.io applications traceback recent call last file , line 1, file usr local lib python2.7 dist packages keras models.py , line 143, load model model.load weights hdf5 group f 'model weights' file usr local lib python2.7 dist packages keras engine topology.py , line 2753, load weights hdf5 group str len flattened layers ' layers.' valueerror trying load weight file containing 190 layers model 2 layers."
keras,5088,following code produces error .,0,saving model import backend k breaks model loading.,saving model import backend k breaks model loading. following code produces error .
keras,1722,"would like train graph task, add layer, resume training. best way this? always adding layer end graph. assuming perform first step one graph, save weights, attempt initialize new graph weights learned first step. current code seems rely upon order params list naming weights hdf file. want able make changes graph would change order, need method knowing params set reloading weights. current idea generate unique name param based layer name, store params hdf file according unique names. loading weights modified graph would know params set. sensible approach? would break things? would compatible existing functionality? thanks excellent work",0,load weights learned one network another?,"load weights learned one network another? would like train graph task, add layer, resume training. best way this? always adding layer end graph. assuming perform first step one graph, save weights, attempt initialize new graph weights learned first step. current code seems rely upon order params list naming weights hdf file. want able make changes graph would change order, need method knowing params set reloading weights. current idea generate unique name param based layer name, store params hdf file according unique names. loading weights modified graph would know params set. sensible approach? would break things? would compatible existing functionality? thanks excellent work"
keras,3580,"newbie keras alert! 1. need compute recursive network's error last time step network bptt correct network, done changing objective function?, better way? 2. need recursive layer recursive connections sense neuron knows last output outputs neurons layer, done creating model neuron merging outputs? could force weights remain zero?",0,need custom changes network's behavior.,"need custom changes network's behavior. newbie keras alert! 1. need compute recursive network's error last time step network bptt correct network, done changing objective function?, better way? 2. need recursive layer recursive connections sense neuron knows last output outputs neurons layer, done creating model neuron merging outputs? could force weights remain zero?"
keras,1023,"1 show combining filter lengths around 'best' filter length achieves better sentence classification results using 'best' filter length. possibility specify combination filter lengths 1d convolution? not, feature implemented? zhang, y., wallace, b. 2015 . sensitivity analysis practitioners guide convolutional neural networks sentence classification, 1 . retrieved http arxiv.org abs 1510.03820",0,combining different filter lengths 1d convolutional layers,"combining different filter lengths 1d convolutional layers 1 show combining filter lengths around 'best' filter length achieves better sentence classification results using 'best' filter length. possibility specify combination filter lengths 1d convolution? not, feature implemented? zhang, y., wallace, b. 2015 . sensitivity analysis practitioners guide convolutional neural networks sentence classification, 1 . retrieved http arxiv.org abs 1510.03820"
keras,3295,"fchollet kerasors, know dealing large scale data imagenet, could write customized generator produces batch data often numpy.array disk. could train model . but, want use online data augmentation time, simplest way implement? note would like use method instead method.",0,use data image augmentation fit generator ?,"use data image augmentation fit generator ? fchollet kerasors, know dealing large scale data imagenet, could write customized generator produces batch data often numpy.array disk. could train model . but, want use online data augmentation time, simplest way implement? note would like use method instead method."
keras,1989,"cnn example minst dataset https github.com fchollet keras blob master examples mnist cnn.py tell make good cnn network recognise hand written digits. issue tell predict new digits. example give image, instead telling digits think is, instead gives list 10 numbers presumably probabilities please help!",0,predict values categories?,"predict values categories? cnn example minst dataset https github.com fchollet keras blob master examples mnist cnn.py tell make good cnn network recognise hand written digits. issue tell predict new digits. example give image, instead telling digits think is, instead gives list 10 numbers presumably probabilities please help!"
keras,1917,"hi, jut ran cnn built keras big training set, weird loss values epoch see 66496 511502 ........................... eta 63s loss 8.2800 66528 511502 ........................... eta 63s loss 204433556137039776.0000 345664 511502 .......... eta 23s loss 8.3174 345696 511502 .......... eta 23s loss 39342531075525840.0000 214080 511502 .................. eta 41s loss 8.3406 214112 511502 .................. eta 41s loss 63520753730220536.0000 possible? loss becomes suddenly big value gets bigger double encoding? way avoid it? regards,",0,loss becomes negative,"loss becomes negative hi, jut ran cnn built keras big training set, weird loss values epoch see 66496 511502 ........................... eta 63s loss 8.2800 66528 511502 ........................... eta 63s loss 204433556137039776.0000 345664 511502 .......... eta 23s loss 8.3174 345696 511502 .......... eta 23s loss 39342531075525840.0000 214080 511502 .................. eta 41s loss 8.3406 214112 511502 .................. eta 41s loss 63520753730220536.0000 possible? loss becomes suddenly big value gets bigger double encoding? way avoid it? regards,"
keras,5861,"get file serious limitations, even untar true can't unzip assumes file. changing parameter untar uncompress, something similar what's discussed stackoverflow link identifying compressed files uncomrpressing http stackoverflow.com questions 13044562 python mechanism identify compressed file type uncompress ? parameter could options , , , etc... also, md5 check sha2 check since md5 known insecure. relevant datasets original pascal voc 2012 wget http host.robots.ox.ac.uk pascal voc voc2012 voctrainval 11 may 2012.tar 2 gb berkeley augmented pascal voc wget http www.eecs.berkeley.edu research projects cs vision grouping semantic contours benchmark.tgz 1.3 gb",0,get file reducing limitations,"get file reducing limitations get file serious limitations, even untar true can't unzip assumes file. changing parameter untar uncompress, something similar what's discussed stackoverflow link identifying compressed files uncomrpressing http stackoverflow.com questions 13044562 python mechanism identify compressed file type uncompress ? parameter could options , , , etc... also, md5 check sha2 check since md5 known insecure. relevant datasets original pascal voc 2012 wget http host.robots.ox.ac.uk pascal voc voc2012 voctrainval 11 may 2012.tar 2 gb berkeley augmented pascal voc wget http www.eecs.berkeley.edu research projects cs vision grouping semantic contours benchmark.tgz 1.3 gb"
keras,13459,model.test batch returns total loss instead average keras 2.2.4 tensorflow issue fixed using,0,model.test batch returns total loss instead average keras 2.2.4,model.test batch returns total loss instead average keras 2.2.4 model.test batch returns total loss instead average keras 2.2.4 tensorflow issue fixed using
keras,3975,workings multiple gpu 'device' state does't save nothing load. set default device loading. tried save mode full save via yaml. result same. simple test understanding output without device state.,0,"load model device gpu0, gpu1, loaded","load model device gpu0, gpu1, loaded workings multiple gpu 'device' state does't save nothing load. set default device loading. tried save mode full save via yaml. result same. simple test understanding output without device state."
keras,2232,"want get output lstm layer model0. know get internal output simple model without merging, like however, inputs merging model two styles input. please give suggestions!",0,get internal output left right model?,"get internal output left right model? want get output lstm layer model0. know get internal output simple model without merging, like however, inputs merging model two styles input. please give suggestions!"
keras,10221,"using sample weights important use instead get correct accuracy. however, current implementation load save model take metrics account. current implementations saves metrics, see https github.com keras team keras blob master keras engine saving.py l144 sets metrics, see https github.com keras team keras blob master keras engine saving.py l286 problem occurs loading model trying continue training using one weighted metrics . ideally, weighted metrics also saved subsequently loaded.",0,"weighted metrics loaded using load model, normal metrics set","weighted metrics loaded using load model, normal metrics set using sample weights important use instead get correct accuracy. however, current implementation load save model take metrics account. current implementations saves metrics, see https github.com keras team keras blob master keras engine saving.py l144 sets metrics, see https github.com keras team keras blob master keras engine saving.py l286 problem occurs loading model trying continue training using one weighted metrics . ideally, weighted metrics also saved subsequently loaded."
keras,11204,test wrap combined generator discriminator multiple gpu model api syntax reproduce issue https gist.github.com emilwallner f2c411b83c499c1834fd1be6646f7389 error valueerror name model 1 used 2 times model. layer names unique.,0,can't use multiple gpus gans,can't use multiple gpus gans test wrap combined generator discriminator multiple gpu model api syntax reproduce issue https gist.github.com emilwallner f2c411b83c499c1834fd1be6646f7389 error valueerror name model 1 used 2 times model. layer names unique.
keras,10685,"vgg16 trained fresh hyperspectral dataset input shape like 5, 5, 30 ? used functions. write full code vgg16.",0,input shape problem vgg16,"input shape problem vgg16 vgg16 trained fresh hyperspectral dataset input shape like 5, 5, 30 ? used functions. write full code vgg16."
keras,4238,"hi,all wonder body ever use decovolution3d, unpooling3d ? inverse option convolution3d maxpooling option. thank advance! please make sure boxes checked submit issue. thank you! 1 check date master branch keras. update pip install git git github.com fchollet keras.git upgrade deps 1 running theano, check date master branch theano. update pip install git git github.com theano theano.git upgrade deps 1 provide link github gist python script reproduce issue copy script short .",0,implemente deconv3d,"implemente deconv3d hi,all wonder body ever use decovolution3d, unpooling3d ? inverse option convolution3d maxpooling option. thank advance! please make sure boxes checked submit issue. thank you! 1 check date master branch keras. update pip install git git github.com fchollet keras.git upgrade deps 1 running theano, check date master branch theano. update pip install git git github.com theano theano.git upgrade deps 1 provide link github gist python script reproduce issue copy script short ."
keras,2603,"hi, finished train first rnn keras, two small technical problems. several warning messages apparently something slowing training phase. first problem 'timedistributeddense '. apparently deprecated supposed use 'timedistributed dense ' instead, suggested warning msg itself. surf keras documentation method cited section 'core', code line keras.layers.core import timedistributed gives error. suposed import timedistributed? second problem 'on batch end '. original warning msg following warning warnings module file library frameworks python.framework versions 2.7 lib python2.7 site packages keras callbacks.py , line 66 delta median userwarning method batch end slow compared batch update 0.122654 . check callbacks found another post problem people saying due heavy custom callbacks, define callback own. could problem?",0,slow training due 'on batch end ' 'timedistributeddense ',"slow training due 'on batch end ' 'timedistributeddense ' hi, finished train first rnn keras, two small technical problems. several warning messages apparently something slowing training phase. first problem 'timedistributeddense '. apparently deprecated supposed use 'timedistributed dense ' instead, suggested warning msg itself. surf keras documentation method cited section 'core', code line keras.layers.core import timedistributed gives error. suposed import timedistributed? second problem 'on batch end '. original warning msg following warning warnings module file library frameworks python.framework versions 2.7 lib python2.7 site packages keras callbacks.py , line 66 delta median userwarning method batch end slow compared batch update 0.122654 . check callbacks found another post problem people saying due heavy custom callbacks, define callback own. could problem?"
keras,5481,"suppose want train stateful rnn two samples. first, 100 observations 100 x values 100 values train on. second, 200 observations 100 x values 100 values train on. set stateful rnn batch size 2. start training batches 2. well good first 100 batches. happens 101st? batch size really needs 1 point, believe change batch size mid training. masking intelligent enough handle situation? so, suppose pad first observation 100 sets zeros beginning x series. also pad series 100 zeros. use masking layer mask zeros input data. know masking prevent information x series used, also prevent model anything dummy values? suppose would also possible use batch size 1 throughout simply reset model state ready switch samples. downside little coding overhead ? another best practice this? hope explained well.",0,stateful rnn different lengths sample,"stateful rnn different lengths sample suppose want train stateful rnn two samples. first, 100 observations 100 x values 100 values train on. second, 200 observations 100 x values 100 values train on. set stateful rnn batch size 2. start training batches 2. well good first 100 batches. happens 101st? batch size really needs 1 point, believe change batch size mid training. masking intelligent enough handle situation? so, suppose pad first observation 100 sets zeros beginning x series. also pad series 100 zeros. use masking layer mask zeros input data. know masking prevent information x series used, also prevent model anything dummy values? suppose would also possible use batch size 1 throughout simply reset model state ready switch samples. downside little coding overhead ? another best practice this? hope explained well."
keras,2486,"hi, built layer called dissimilarity returns following error raised buildinf model test code crop right bound boundary 1 crop left crop side 3,0 b dis dissimilarity crop left, crop right bound patch compare model part input, part b input , dis need configure anything else implementation layer?",0,unable output custom layer,"unable output custom layer hi, built layer called dissimilarity returns following error raised buildinf model test code crop right bound boundary 1 crop left crop side 3,0 b dis dissimilarity crop left, crop right bound patch compare model part input, part b input , dis need configure anything else implementation layer?"
keras,2882,try train network like http arxiv.org pdf 1506.02640v5.pdf . idea objective function. implement function return 1 cases depend output return 0 otherwise. ?,0,custom objective function,custom objective function try train network like http arxiv.org pdf 1506.02640v5.pdf . idea objective function. implement function return 1 cases depend output return 0 otherwise. ?
keras,519,"wxs , fchollet trying transplant ctc objective keras. reading codes, noticed function class, lines since weighted objectivemasked truemasked predy truey pred , mean objective function expect shapes masked variables instead unmasked ones?",0,loss weighted compile ?,"loss weighted compile ? wxs , fchollet trying transplant ctc objective keras. reading codes, noticed function class, lines since weighted objectivemasked truemasked predy truey pred , mean objective function expect shapes masked variables instead unmasked ones?"
keras,4237,"please make sure boxes checked submit issue. thank you! x check date master branch keras. update pip install git git github.com fchollet keras.git upgrade deps x running theano, check date master branch theano. update pip install git git github.com theano theano.git upgrade deps x provide link github gist python script reproduce issue copy script short .",0,"keras, import dataset made myself, whats requirements data dataset?","keras, import dataset made myself, whats requirements data dataset? please make sure boxes checked submit issue. thank you! x check date master branch keras. update pip install git git github.com fchollet keras.git upgrade deps x running theano, check date master branch theano. update pip install git git github.com theano theano.git upgrade deps x provide link github gist python script reproduce issue copy script short ."
keras,11094,"time decay drop decay maybe good strategy train model, think better way reduce learning rate valid loss increasing valid acc decreasing. change learning rate passing parameter learningrateschedule. it?",0,reduce learning rate based last epoch valid acc valid loss?,"reduce learning rate based last epoch valid acc valid loss? time decay drop decay maybe good strategy train model, think better way reduce learning rate valid loss increasing valid acc decreasing. change learning rate passing parameter learningrateschedule. it?"
keras,1597,"use lstm sequence labeling task, got acc cal acc epoch. code def modulernn self model sequential model.add lstm output dim 64,input length self.seq len,batch input shape 16,1,200 ,input dim self.embed length,return sequences true,stateful false model.add lstm output dim 16,return sequences true,stateful false model.add dropout 0.2 model.add timedistributeddense output dim self.labs len model.add activation 'softmax' model.compile loss categorical crossentropy , optimizer 'rmsprop' , class mode 'categorical' model.fit self.train,self.train lab,batch size 16,nb epoch 3,verbose 1, validation split 0.1,show accuracy true model.fit self.x train,self.y train,batch size 16,nb epoch 15,verbose 1,show accuracy true,validation split 0.2 score model.evaluate self.x test,self.y test,batch size 16 print score anyone meets problem? please help",0,acc val acc change?,"acc val acc change? use lstm sequence labeling task, got acc cal acc epoch. code def modulernn self model sequential model.add lstm output dim 64,input length self.seq len,batch input shape 16,1,200 ,input dim self.embed length,return sequences true,stateful false model.add lstm output dim 16,return sequences true,stateful false model.add dropout 0.2 model.add timedistributeddense output dim self.labs len model.add activation 'softmax' model.compile loss categorical crossentropy , optimizer 'rmsprop' , class mode 'categorical' model.fit self.train,self.train lab,batch size 16,nb epoch 3,verbose 1, validation split 0.1,show accuracy true model.fit self.x train,self.y train,batch size 16,nb epoch 15,verbose 1,show accuracy true,validation split 0.2 score model.evaluate self.x test,self.y test,batch size 16 print score anyone meets problem? please help"
keras,6296,"hello everyone, confused problem several days... question training time massive difference set batch size 1 20 generator. set batch size 1 , training time 1 epoch approximately 180 200 sec . set batch size 20 , training time 1 epoch approximately 3000 3200 sec . however, horrible difference training times seems abnormal..., since reversed result batch size 1, training time 3000 3200 sec. batch size 20, training time 180 200 sec. input generator file path, numpy arrays already loaded memory via calling np.load . think trade issue exist. using keras 2.0.3 backend tensorflow gpu 1.0.1 seen update merged pr https github.com fchollet keras pull 5879 files , seems change affect anything all. usage original one link https gist.github.com happystorm cb6c22ffec18a8fbb4912e9c79b6d87c gist self defined generator part fit generator. could somebody help explain problem...??? thank much...orz",0,what's difference samples per epoch steps per epoch,"what's difference samples per epoch steps per epoch hello everyone, confused problem several days... question training time massive difference set batch size 1 20 generator. set batch size 1 , training time 1 epoch approximately 180 200 sec . set batch size 20 , training time 1 epoch approximately 3000 3200 sec . however, horrible difference training times seems abnormal..., since reversed result batch size 1, training time 3000 3200 sec. batch size 20, training time 180 200 sec. input generator file path, numpy arrays already loaded memory via calling np.load . think trade issue exist. using keras 2.0.3 backend tensorflow gpu 1.0.1 seen update merged pr https github.com fchollet keras pull 5879 files , seems change affect anything all. usage original one link https gist.github.com happystorm cb6c22ffec18a8fbb4912e9c79b6d87c gist self defined generator part fit generator. could somebody help explain problem...??? thank much...orz"
keras,9783,"noticed weird difference model api. class inherits , changes signature function. reason this? seems like two apis consistent. https github.com keras team keras blob eb97bc385599dec8182963fe263bd958b9ab0057 keras models.py l1355 l1363 vs. https github.com keras team keras blob eb97bc385599dec8182963fe263bd958b9ab0057 keras engine topology.py l2326",0,"get config keras.models.sequential model returns list, get config keras.engine.training.model returns dict","get config keras.models.sequential model returns list, get config keras.engine.training.model returns dict noticed weird difference model api. class inherits , changes signature function. reason this? seems like two apis consistent. https github.com keras team keras blob eb97bc385599dec8182963fe263bd958b9ab0057 keras models.py l1355 l1363 vs. https github.com keras team keras blob eb97bc385599dec8182963fe263bd958b9ab0057 keras engine topology.py l2326"
keras,13024,"system information written custom code opposed using example directory linux ubuntu 16.04 tensorflow backend yes tensorflow version ''v1.13.1 0 g6612da8951' 1.13.1 keras version 2.2.4 python version 3.6.8 cuda cudnn version cuda 10.0 gpu model memory 4 gb tried continues training keras. build keras multiclass classification model, new labels, values. want build new model without retraining. tried continuous train keras. completing save model , want continues training. tried, tried this. thrown error.like previously 10 classes. add new class means error occurred. question is, possible continues training keras multiclass classification new class? thank",0,possible continues training keras multiclass classification new class?,"possible continues training keras multiclass classification new class? system information written custom code opposed using example directory linux ubuntu 16.04 tensorflow backend yes tensorflow version ''v1.13.1 0 g6612da8951' 1.13.1 keras version 2.2.4 python version 3.6.8 cuda cudnn version cuda 10.0 gpu model memory 4 gb tried continues training keras. build keras multiclass classification model, new labels, values. want build new model without retraining. tried continuous train keras. completing save model , want continues training. tried, tried this. thrown error.like previously 10 classes. add new class means error occurred. question is, possible continues training keras multiclass classification new class? thank"
keras,3926,"network predicts images y. output file predicted images batch, monitor progress training? current understanding write callback calls routine, sure actual batch data x going be? case, custom generator creating x, model fit using . anyone know this, examples? thanks!",0,output batch images,"output batch images network predicts images y. output file predicted images batch, monitor progress training? current understanding write callback calls routine, sure actual batch data x going be? case, custom generator creating x, model fit using . anyone know this, examples? thanks!"
keras,6171,"environment typeerrortypeerror value passed parameter 'shape' datatype float32 list allowed values int32, int64 . flatten layer's output float32. however, seems like dense layer accepts int32, int64. error output help appreciated. thank you!",0,"dense layer issue typeerror value passed parameter 'shape' datatype float32 list allowed values int32, int64","dense layer issue typeerror value passed parameter 'shape' datatype float32 list allowed values int32, int64 environment typeerrortypeerror value passed parameter 'shape' datatype float32 list allowed values int32, int64 . flatten layer's output float32. however, seems like dense layer accepts int32, int64. error output help appreciated. thank you!"
keras,7432,"using standard code, found internet. code using scikit learn select parameter using keras. using code http machinelearningmastery.com grid search hyperparameters deep learning models python keras sure problem keras scikit learn. got errors. idea happens.",0,forrtl error 200 program aborting due control c event,"forrtl error 200 program aborting due control c event using standard code, found internet. code using scikit learn select parameter using keras. using code http machinelearningmastery.com grid search hyperparameters deep learning models python keras sure problem keras scikit learn. got errors. idea happens."
keras,4613,"problem configuration using tensorflow backed. computer 1gpu card 12 cpus distributed learning cluster one session, use gpu use cpus. using time. way force keras calling tensorflow gpu cpus run keras cpu run keras gpu write anything, let everything default. since install tensorflow gpu version. assume default using gpu. undocumented trick works far. however, since keras blackbox me, tensorflow structured clear, feel improvement keras better control cpu gpu device keras . know could use keras simplified tensorflow layer constructor. thus possible run everything framework tensorflow rather living world keras. comment alternative ways force keras used cpu gpu, please comment let everyone else know. best, shaowu",0,keras model run specific device?,"keras model run specific device? problem configuration using tensorflow backed. computer 1gpu card 12 cpus distributed learning cluster one session, use gpu use cpus. using time. way force keras calling tensorflow gpu cpus run keras cpu run keras gpu write anything, let everything default. since install tensorflow gpu version. assume default using gpu. undocumented trick works far. however, since keras blackbox me, tensorflow structured clear, feel improvement keras better control cpu gpu device keras . know could use keras simplified tensorflow layer constructor. thus possible run everything framework tensorflow rather living world keras. comment alternative ways force keras used cpu gpu, please comment let everyone else know. best, shaowu"
keras,3420,"axis 1, line 507 broadcastable x.broadcastable 1 x.broadcastable 0 remove correctly? 504 def squeeze x, axis 505 '''remove 1 dimension tensor index axis . 506 ''' 507 broadcastable x.broadcastable axis x.broadcastable axis 1 508 x t.patternbroadcast x, axis range x.type.ndim 509 x t.squeeze x 510 x t.patternbroadcast x, broadcastable 511 return x",0,maybe bug keras backend theano backend.py line 507 function squeeze ??,"maybe bug keras backend theano backend.py line 507 function squeeze ?? axis 1, line 507 broadcastable x.broadcastable 1 x.broadcastable 0 remove correctly? 504 def squeeze x, axis 505 '''remove 1 dimension tensor index axis . 506 ''' 507 broadcastable x.broadcastable axis x.broadcastable axis 1 508 x t.patternbroadcast x, axis range x.type.ndim 509 x t.squeeze x 510 x t.patternbroadcast x, broadcastable 511 return x"
keras,7890,"exception thread thread 8 traceback recent call last file users gpu rohitg1 miniconda2 envs tensorflow lib python2.7 threading.py , line 801, bootstrap inner self.run file users gpu rohitg1 miniconda2 envs tensorflow lib python2.7 threading.py , line 754, run self. target self. args, self. kwargs file users gpu rohitg1 .local lib python2.7 site packages keras utils data utils.py , line 492, run self.sequence.on epoch end file users gpu rohitg1 .local lib python2.7 site packages keras utils data utils.py , line 358, epoch end raise notimplementederror notimplementederror trying train video classification model. gist portion code available https gist.github.com rohit gupta 7668b79389e29598ace41813fc1a50d2 need implement epoch end function sequence object get work ?",0,getting notimplementederror epoch end training using fit generator sequence object,"getting notimplementederror epoch end training using fit generator sequence object exception thread thread 8 traceback recent call last file users gpu rohitg1 miniconda2 envs tensorflow lib python2.7 threading.py , line 801, bootstrap inner self.run file users gpu rohitg1 miniconda2 envs tensorflow lib python2.7 threading.py , line 754, run self. target self. args, self. kwargs file users gpu rohitg1 .local lib python2.7 site packages keras utils data utils.py , line 492, run self.sequence.on epoch end file users gpu rohitg1 .local lib python2.7 site packages keras utils data utils.py , line 358, epoch end raise notimplementederror notimplementederror trying train video classification model. gist portion code available https gist.github.com rohit gupta 7668b79389e29598ace41813fc1a50d2 need implement epoch end function sequence object get work ?"
keras,2808,"confused parameter function keras, work concretely, example, use learning rate , set parameter , anyone help me? thanks advance!",0,parameter decay keras work concretely?,"parameter decay keras work concretely? confused parameter function keras, work concretely, example, use learning rate , set parameter , anyone help me? thanks advance!"
keras,4161,"using keras tensorflow train large number tiny networks 4 layers, less 30 nodes layer . currently tf allocates gpu memory single process therefore prevents opening learning processes parallel. found tf document use this. however, able integrate keras. someone know way initialize tf session keras? thank much!",0,using allow growth keras tensorflow,"using allow growth keras tensorflow using keras tensorflow train large number tiny networks 4 layers, less 30 nodes layer . currently tf allocates gpu memory single process therefore prevents opening learning processes parallel. found tf document use this. however, able integrate keras. someone know way initialize tf session keras? thank much!"
keras,10426,"think uses functions like binary entropy , binary accuracy etc. smoothing used ? description documentation good pointers exact code also help .",0,model.fit calculate loss acc ? documentation helpful.,"model.fit calculate loss acc ? documentation helpful. think uses functions like binary entropy , binary accuracy etc. smoothing used ? description documentation good pointers exact code also help ."
keras,2776,"possible scalar input instance? use get output correctly cannot use input defined keras tensor. way generate scalar input , either directly converting backend tensor object?",0,scalar input,"scalar input possible scalar input instance? use get output correctly cannot use input defined keras tensor. way generate scalar input , either directly converting backend tensor object?"
keras,7945,"traceback recent call last file seq2seq.py , line 90, seq2seq.train seq2seq file seq2seq.py , line 74, train seq2seq model s2s.seq2seq plain file home david downloads seq2seq master seq2seq model.py , line 28, seq2seq plain model.add rnn self.hidden dim, return sequences true , input shape 100, 128 file usr local lib python2.7 dist packages keras models.py , line 327, add output tensor layer self.outputs 0 file usr local lib python2.7 dist packages keras engine topology.py , line 543, call self.build input shapes 0 file usr local lib python2.7 dist packages keras layers recurrent.py , line 763, build self.w k.concatenate self.w i, self.w f, self.w c, self.w file usr local lib python2.7 dist packages keras backend tensorflow backend.py , line 1222, concatenate return tf.concat axis, dense x x tensors file usr local lib python2.7 dist packages tensorflow python ops array ops.py , line 1061, concat dtype dtypes.int32 .get shape file usr local lib python2.7 dist packages tensorflow python framework ops.py , line 611, convert tensor ref false file usr local lib python2.7 dist packages tensorflow python framework ops.py , line 676, internal convert tensor ret conversion func value, dtype dtype, name name, ref ref file usr local lib python2.7 dist packages tensorflow python framework constant op.py , line 121, constant tensor conversion function return constant v, dtype dtype, name name file usr local lib python2.7 dist packages tensorflow python framework constant op.py , line 102, constant tensor util.make tensor proto value, dtype dtype, shape shape, verify shape verify shape file usr local lib python2.7 dist packages tensorflow python framework tensor util.py , line 376, make tensor proto assertcompatible values, dtype file usr local lib python2.7 dist packages tensorflow python framework tensor util.py , line 302, assertcompatible dtype.name, repr mismatch , type mismatch . name typeerror expected int32, got type 'variable' instead.",0,error running seq2seq model tensorflow version 1.3 keras version 1.2.0,"error running seq2seq model tensorflow version 1.3 keras version 1.2.0 traceback recent call last file seq2seq.py , line 90, seq2seq.train seq2seq file seq2seq.py , line 74, train seq2seq model s2s.seq2seq plain file home david downloads seq2seq master seq2seq model.py , line 28, seq2seq plain model.add rnn self.hidden dim, return sequences true , input shape 100, 128 file usr local lib python2.7 dist packages keras models.py , line 327, add output tensor layer self.outputs 0 file usr local lib python2.7 dist packages keras engine topology.py , line 543, call self.build input shapes 0 file usr local lib python2.7 dist packages keras layers recurrent.py , line 763, build self.w k.concatenate self.w i, self.w f, self.w c, self.w file usr local lib python2.7 dist packages keras backend tensorflow backend.py , line 1222, concatenate return tf.concat axis, dense x x tensors file usr local lib python2.7 dist packages tensorflow python ops array ops.py , line 1061, concat dtype dtypes.int32 .get shape file usr local lib python2.7 dist packages tensorflow python framework ops.py , line 611, convert tensor ref false file usr local lib python2.7 dist packages tensorflow python framework ops.py , line 676, internal convert tensor ret conversion func value, dtype dtype, name name, ref ref file usr local lib python2.7 dist packages tensorflow python framework constant op.py , line 121, constant tensor conversion function return constant v, dtype dtype, name name file usr local lib python2.7 dist packages tensorflow python framework constant op.py , line 102, constant tensor util.make tensor proto value, dtype dtype, shape shape, verify shape verify shape file usr local lib python2.7 dist packages tensorflow python framework tensor util.py , line 376, make tensor proto assertcompatible values, dtype file usr local lib python2.7 dist packages tensorflow python framework tensor util.py , line 302, assertcompatible dtype.name, repr mismatch , type mismatch . name typeerror expected int32, got type 'variable' instead."
keras,3041,"model defined like this, issue component another sub model, like get following error batchnormalizationmode 2batchnormalization remove object sub model works fine. obviously intended case this, sub model complicated. x check date master branch keras. update pip install git git github.com fchollet keras.git upgrade deps x running theano, check date master branch theano. update pip install git git github.com theano theano.git upgrade deps x provide link github gist python script reproduce issue copy script short .",0,batchnormalization can't used across timedistributed component sub model,"batchnormalization can't used across timedistributed component sub model model defined like this, issue component another sub model, like get following error batchnormalizationmode 2batchnormalization remove object sub model works fine. obviously intended case this, sub model complicated. x check date master branch keras. update pip install git git github.com fchollet keras.git upgrade deps x running theano, check date master branch theano. update pip install git git github.com theano theano.git upgrade deps x provide link github gist python script reproduce issue copy script short ."
keras,5448,trying use pooling layer 1d 2d border mode 'same' seems change dimensions anyway,0,border mode 'same' pooling layers still changes dimentions,border mode 'same' pooling layers still changes dimentions trying use pooling layer 1d 2d border mode 'same' seems change dimensions anyway
keras,1628,"trying train multi task regression model, outputs complete fact average 1 values per training instance . expected mean squared error non null outputs reasonable objective, however, obviously using keras' mean squared error objective, cost comes , nans propagate. plans supporting sort thing already supported somehow missed it? not, anyone idea hack? tried writing new cost function, like sorry mixing keras theano! evaluates correctly 1d vectors nans true, work keras, even batch size set 1. next plan set nans true equal pred, experienced theano.",0,using outputs missing values,"using outputs missing values trying train multi task regression model, outputs complete fact average 1 values per training instance . expected mean squared error non null outputs reasonable objective, however, obviously using keras' mean squared error objective, cost comes , nans propagate. plans supporting sort thing already supported somehow missed it? not, anyone idea hack? tried writing new cost function, like sorry mixing keras theano! evaluates correctly 1d vectors nans true, work keras, even batch size set 1. next plan set nans true equal pred, experienced theano."
keras,9634,"let's say dataset shape 30, 2 , means 30 samples , 2 features , sales promotion. want build model perform 1 day ahead forecasting sales, based sales last 5 days promotion last 5 days current day . shifting removing na's, could get new dataframe 25 rows, 12 features, are, promotion 5 , sales 5 , promotion 4 , sales 4 , , promotion 1 , sales 1 , promotion , sales . last column treated response , first 11 columns treated x , question shape x, add new column constant shape 25, 6, 2 ? promotion column included x",0,lstm reshape input dataset prediction made based past current features,"lstm reshape input dataset prediction made based past current features let's say dataset shape 30, 2 , means 30 samples , 2 features , sales promotion. want build model perform 1 day ahead forecasting sales, based sales last 5 days promotion last 5 days current day . shifting removing na's, could get new dataframe 25 rows, 12 features, are, promotion 5 , sales 5 , promotion 4 , sales 4 , , promotion 1 , sales 1 , promotion , sales . last column treated response , first 11 columns treated x , question shape x, add new column constant shape 25, 6, 2 ? promotion column included x"
keras,7262,"hi, new keras rnn dataset 1000 videos, video 2000 frames frame 5 features. would like train rnn classify video frame 1 3 categories. example video1 frame1 fc11,fc12,fc13,fc14,fc15 output1 frame2 fc21,fc22,fc23,fc24,fc25 output2 frame3 fc31,fc32,fc33,fc34,fc35 output3 video2 x shape 1000, 2000, 5 shape 1000, 2000, 3 since frame value little confused construct model fit shape",0,shape output rnn,"shape output rnn hi, new keras rnn dataset 1000 videos, video 2000 frames frame 5 features. would like train rnn classify video frame 1 3 categories. example video1 frame1 fc11,fc12,fc13,fc14,fc15 output1 frame2 fc21,fc22,fc23,fc24,fc25 output2 frame3 fc31,fc32,fc33,fc34,fc35 output3 video2 x shape 1000, 2000, 5 shape 1000, 2000, 3 since frame value little confused construct model fit shape"
keras,2586,"hello, got latest pulls keras theano. trying reproduce code blog post http cbonnett.github.io mdn.html . far gist https gist.github.com sergeyf cf20b2759a7d38035f30384769bed9df . issue batch updates order 10 , loss becomes nan. repaste likely culprit negative log likelihood loss function tried alter various parts function, can't spot specific issues. added log sum exp trick prevent underflow there, check values intermediate variable within loss function batches nan loss, seem well behaved. ideas? thanks help, fabulous package.",0,mixture density network quickly gets nan loss,"mixture density network quickly gets nan loss hello, got latest pulls keras theano. trying reproduce code blog post http cbonnett.github.io mdn.html . far gist https gist.github.com sergeyf cf20b2759a7d38035f30384769bed9df . issue batch updates order 10 , loss becomes nan. repaste likely culprit negative log likelihood loss function tried alter various parts function, can't spot specific issues. added log sum exp trick prevent underflow there, check values intermediate variable within loss function batches nan loss, seem well behaved. ideas? thanks help, fabulous package."
keras,2361,"hi, got error fitting model keras 1.0 theano windows 10 x64. previously, fit keras models keras 0.6 theano 0.8 dev fine. simple perceptron code demonstrate error fitting code regression keras another installation error? so, tips debug it? thanks. additional info x date master branch keras x date master branch theano os windows 10 x64 running cpu backend theano import theano",0,importerror fitting keras model,"importerror fitting keras model hi, got error fitting model keras 1.0 theano windows 10 x64. previously, fit keras models keras 0.6 theano 0.8 dev fine. simple perceptron code demonstrate error fitting code regression keras another installation error? so, tips debug it? thanks. additional info x date master branch keras x date master branch theano os windows 10 x64 running cpu backend theano import theano"
keras,6838,"searched high low found satisfying results... so, problem neural network outputs 4, array, predicted labels greater 0.5. problem is, top k categorical accuracy work. course and, tried implement one own, noticed parameter true pred tensors. tried place assignment, making values greater 0.5 become 1 others 0. ideas?",0,customize metrics binary accuracy outputs 0.5,"customize metrics binary accuracy outputs 0.5 searched high low found satisfying results... so, problem neural network outputs 4, array, predicted labels greater 0.5. problem is, top k categorical accuracy work. course and, tried implement one own, noticed parameter true pred tensors. tried place assignment, making values greater 0.5 become 1 others 0. ideas?"
keras,7288,"managed finish experiments using attention mechanism adopted cbaziotis implementation https gist.github.com cbaziotis 7ef97ccf71cbc14366835198c09809d2 confused visualization. really understand heatmap well. guys explain me, means lot 1. heatmap? read them? 2. visualize attention mechanism? weight? 3. code visualize attention mechanism, conjunction cbaziotis' implementation https gist.github.com cbaziotis 7ef97ccf71cbc14366835198c09809d2 thanks advance!",0,visualize attention mechanism classification task?,"visualize attention mechanism classification task? managed finish experiments using attention mechanism adopted cbaziotis implementation https gist.github.com cbaziotis 7ef97ccf71cbc14366835198c09809d2 confused visualization. really understand heatmap well. guys explain me, means lot 1. heatmap? read them? 2. visualize attention mechanism? weight? 3. code visualize attention mechanism, conjunction cbaziotis' implementation https gist.github.com cbaziotis 7ef97ccf71cbc14366835198c09809d2 thanks advance!"
keras,7956,"please make sure boxes checked submit issue. issue implementation question, please ask question stackoverflow http stackoverflow.com questions tagged keras join keras slack channel https keras slack autojoin.herokuapp.com ask instead filing github issue. thank you! check date master branch keras. update pip install git git github.com fchollet keras.git upgrade deps running tensorflow, check date latest version. installation instructions found https www.tensorflow.org get started os setup . running theano, check date master branch theano. update pip install git git github.com theano theano.git upgrade deps provide link github gist python script reproduce issue copy script short .",0,know final value alpha learned prelu?,"know final value alpha learned prelu? please make sure boxes checked submit issue. issue implementation question, please ask question stackoverflow http stackoverflow.com questions tagged keras join keras slack channel https keras slack autojoin.herokuapp.com ask instead filing github issue. thank you! check date master branch keras. update pip install git git github.com fchollet keras.git upgrade deps running tensorflow, check date latest version. installation instructions found https www.tensorflow.org get started os setup . running theano, check date master branch theano. update pip install git git github.com theano theano.git upgrade deps provide link github gist python script reproduce issue copy script short ."
keras,5912,"imdb dataset imdb.npz broken least incompatible provided word index converting reviews string gives nonsense either wrong words mixed word order sure . example output x check date master branch keras. update pip install git git github.com fchollet keras.git upgrade deps x running tensorflow, check date latest version. installation instructions found https www.tensorflow.org get started os setup . x provide link github gist python script reproduce issue copy script short .",0,new version imdb dataset broken word order?,"new version imdb dataset broken word order? imdb dataset imdb.npz broken least incompatible provided word index converting reviews string gives nonsense either wrong words mixed word order sure . example output x check date master branch keras. update pip install git git github.com fchollet keras.git upgrade deps x running tensorflow, check date latest version. installation instructions found https www.tensorflow.org get started os setup . x provide link github gist python script reproduce issue copy script short ."
keras,8121,"consider following scenario would expect that, since compiled, output two calls would same. however, running code actual output expected behavior? addition, experiments get feeling although number trainable weights reported differs two cases, actual number trainable weights expected one, is, example above, even setting , training really update 2,112 parameters 5,344. so, maybe reporting issue.",0,bug number trainable weights seem change model compilation,"bug number trainable weights seem change model compilation consider following scenario would expect that, since compiled, output two calls would same. however, running code actual output expected behavior? addition, experiments get feeling although number trainable weights reported differs two cases, actual number trainable weights expected one, is, example above, even setting , training really update 2,112 parameters 5,344. so, maybe reporting issue."
keras,3241,"vector size size . compute dot product get vector size . however, model summary shape agree expected shape. code reproduce error. x check date master branch keras. update pip install git git github.com fchollet keras.git upgrade deps x running theano, check date master branch theano. update pip install git git github.com theano theano.git upgrade deps x provide link github gist python script reproduce issue copy script short .",0,model summary shape correct.,"model summary shape correct. vector size size . compute dot product get vector size . however, model summary shape agree expected shape. code reproduce error. x check date master branch keras. update pip install git git github.com fchollet keras.git upgrade deps x running theano, check date master branch theano. update pip install git git github.com theano theano.git upgrade deps x provide link github gist python script reproduce issue copy script short ."
keras,1533,"use function pass parameter higher 1 time provide non threadsafe generator first argument, confusing situation occurrs. generator worker fails https github.com fchollet keras blob master keras models.py l953 error case , error. fails silently statement sets event, difficult get what's wrong. older version repo, version line https github.com fchollet keras blob master keras models.py l966 exist therefore, program breaks weird way line https github.com fchollet keras blob master keras models.py l974 . see fixed, still troubles figuring problem generator. good idea remove block altogether least remove catch 'em statement?",0,fit generator workers fail silently,"fit generator workers fail silently use function pass parameter higher 1 time provide non threadsafe generator first argument, confusing situation occurrs. generator worker fails https github.com fchollet keras blob master keras models.py l953 error case , error. fails silently statement sets event, difficult get what's wrong. older version repo, version line https github.com fchollet keras blob master keras models.py l966 exist therefore, program breaks weird way line https github.com fchollet keras blob master keras models.py l974 . see fixed, still troubles figuring problem generator. good idea remove block altogether least remove catch 'em statement?"
keras,12188,hi all! error trying use imagedatagenerator flow directory function transfer learning nasnet model keras.applications . os archlinux tensorflow version 1.12.0 keras version 2.2.4 updated master gpus geforce gtx 1080 ti cuda version 9.0.176 4 cudnn version 7.0.5 2 code output get tried debug code seems like line https github.com keras team keras blob e59570ae26670f788d6c649191031e4a8824f955 keras engine training generator.py l110 statement false due val gen false . code variable val gen could initialized false generator next next functions. normal behavior?,0,typeerror object type 'imagedatagenerator' len,typeerror object type 'imagedatagenerator' len hi all! error trying use imagedatagenerator flow directory function transfer learning nasnet model keras.applications . os archlinux tensorflow version 1.12.0 keras version 2.2.4 updated master gpus geforce gtx 1080 ti cuda version 9.0.176 4 cudnn version 7.0.5 2 code output get tried debug code seems like line https github.com keras team keras blob e59570ae26670f788d6c649191031e4a8824f955 keras engine training generator.py l110 statement false due val gen false . code variable val gen could initialized false generator next next functions. normal behavior?
keras,1384,"hi! implement dag cnns http arxiv.org pdf 1505.05232.pdf models cannot compiled error. cannot found node connect. visualize graph expected graph. errors happen model? traceback recent call last file users tereka programing machinelearningcombinator mlc model keras recipe.py , line 315, model.compile 'sgd', 'output' 'categorical crossentropy' file build bdist.macosx 10.9 x86 64 egg keras models.py , line 1047, compile file build bdist.macosx 10.9 x86 64 egg keras optimizers.py , line 79, get updates file build bdist.macosx 10.9 x86 64 egg keras optimizers.py , line 47, get gradients file build bdist.macosx 10.9 x86 64 egg keras backend theano backend.py , line 373, gradients file users tereka .pyenv versions 2.7.8 lib python2.7 site packages theano gradient.py , line 545, grad handle disconnected elem file users tereka .pyenv versions 2.7.8 lib python2.7 site packages theano gradient.py , line 532, handle disconnected raise disconnectedinputerror message theano.gradient.disconnectedinputerror grad method asked compute gradient respect variable part computational graph cost, used non differentiable operator backtrace node created file build bdist.macosx 10.9 x86 64 egg keras backend theano backend.py , line 34, variable return theano.shared value value, name name, strict false",0,model cannot compiled,"model cannot compiled hi! implement dag cnns http arxiv.org pdf 1505.05232.pdf models cannot compiled error. cannot found node connect. visualize graph expected graph. errors happen model? traceback recent call last file users tereka programing machinelearningcombinator mlc model keras recipe.py , line 315, model.compile 'sgd', 'output' 'categorical crossentropy' file build bdist.macosx 10.9 x86 64 egg keras models.py , line 1047, compile file build bdist.macosx 10.9 x86 64 egg keras optimizers.py , line 79, get updates file build bdist.macosx 10.9 x86 64 egg keras optimizers.py , line 47, get gradients file build bdist.macosx 10.9 x86 64 egg keras backend theano backend.py , line 373, gradients file users tereka .pyenv versions 2.7.8 lib python2.7 site packages theano gradient.py , line 545, grad handle disconnected elem file users tereka .pyenv versions 2.7.8 lib python2.7 site packages theano gradient.py , line 532, handle disconnected raise disconnectedinputerror message theano.gradient.disconnectedinputerror grad method asked compute gradient respect variable part computational graph cost, used non differentiable operator backtrace node created file build bdist.macosx 10.9 x86 64 egg keras backend theano backend.py , line 34, variable return theano.shared value value, name name, strict false"
keras,8924,"please make sure boxes checked submit issue. issue implementation question, please ask question stackoverflow http stackoverflow.com questions tagged keras join keras slack channel https keras slack autojoin.herokuapp.com ask instead filing github issue. thank you! check date master branch keras. update pip install git git github.com keras team keras.git upgrade deps running tensorflow, check date latest version. installation instructions found https www.tensorflow.org get started os setup . running theano, check date master branch theano. update pip install git git github.com theano theano.git upgrade deps provide link github gist python script reproduce issue copy script short .",0,build correct data lstm nn?,"build correct data lstm nn? please make sure boxes checked submit issue. issue implementation question, please ask question stackoverflow http stackoverflow.com questions tagged keras join keras slack channel https keras slack autojoin.herokuapp.com ask instead filing github issue. thank you! check date master branch keras. update pip install git git github.com keras team keras.git upgrade deps running tensorflow, check date latest version. installation instructions found https www.tensorflow.org get started os setup . running theano, check date master branch theano. update pip install git git github.com theano theano.git upgrade deps provide link github gist python script reproduce issue copy script short ."
keras,2155,"docs method say labels, numpy array. unclear, however, wheter vector ints one hot matrix. perhaps one mention exists ? also, following code fail theano tensorflow backend.",0,inconsistent behavior backends missing docs,"inconsistent behavior backends missing docs docs method say labels, numpy array. unclear, however, wheter vector ints one hot matrix. perhaps one mention exists ? also, following code fail theano tensorflow backend."
keras,4822,"hi keras, thank project! seems can't make hidden unit output units simplernn layer diff rent values. doc says argument part output dim dimension internal projections final output. seems unclear internal projections . anyway there's one argument type clear can't set hidden state dimension want. wanted? want big hidden state yo remember lot information unidimensionnal output? merry christmas",0,simplernn hidden output units different dimensions,"simplernn hidden output units different dimensions hi keras, thank project! seems can't make hidden unit output units simplernn layer diff rent values. doc says argument part output dim dimension internal projections final output. seems unclear internal projections . anyway there's one argument type clear can't set hidden state dimension want. wanted? want big hidden state yo remember lot information unidimensionnal output? merry christmas"
keras,1685,"trying work fit generator method graph model. works well set nb worker 1, value higher crashes. seems thread.start automatically triggers stop.is set first thread. python3. tested python2.7 lack skills informative debugging .",0,fit generator,"fit generator trying work fit generator method graph model. works well set nb worker 1, value higher crashes. seems thread.start automatically triggers stop.is set first thread. python3. tested python2.7 lack skills informative debugging ."
keras,4079,"restart ipython interpreter saved model model.save loading isnt working batch size course set. 50. model deeper deconv vae examples, nothing fancy. vae loss example. model fom configsequential.from config config",0,load model working nameerror global name 'batch size' defined????,"load model working nameerror global name 'batch size' defined???? restart ipython interpreter saved model model.save loading isnt working batch size course set. 50. model deeper deconv vae examples, nothing fancy. vae loss example. model fom configsequential.from config config"
keras,1761,"hi, working deep neural network sequential dense , understand training accuracy lower validation accuracy two sets separated roughly distribution . started working machine learning long ago told way one know check network overfitting compare validation train accuracy. validation accuracy start dropping training accuracy continue increase that's concerned. problem validation accuracy higher training accuracy make sense me... sure missed something somewhere maybe training accuracy displayed keras one think way put validation data good one... maybe wrong along. one could give advice, knows way plot visualize data training within keras prevent overfit would help. talking kind thing copy pasting random epoch roughly 50000 50000 32s loss 1.7436 acc. 0.5749 val. loss 1.5925 val. acc. 0.6434 network parameters following sequential dense network kind 20000 1000 1000 1000 1000 trained relu except last softmax. loss categorical crossentropy",0,validation accuracy superior training accuracy,"validation accuracy superior training accuracy hi, working deep neural network sequential dense , understand training accuracy lower validation accuracy two sets separated roughly distribution . started working machine learning long ago told way one know check network overfitting compare validation train accuracy. validation accuracy start dropping training accuracy continue increase that's concerned. problem validation accuracy higher training accuracy make sense me... sure missed something somewhere maybe training accuracy displayed keras one think way put validation data good one... maybe wrong along. one could give advice, knows way plot visualize data training within keras prevent overfit would help. talking kind thing copy pasting random epoch roughly 50000 50000 32s loss 1.7436 acc. 0.5749 val. loss 1.5925 val. acc. 0.6434 network parameters following sequential dense network kind 20000 1000 1000 1000 1000 trained relu except last softmax. loss categorical crossentropy"
keras,12218,"anyone guide use reshape function? encoded ?, 4 encod ?, 2 normalize ?, 2 complex symbols ?, decode ?, concat ?, 2 deco ?, 4 decod ?, 4 size none, 2 layers decode concat need size none,1 use reshape function case? option please make sure boxes checked submit issue. issue implementation question , please ask question stackoverflow http stackoverflow.com questions tagged keras keras slack channel https keras slack autojoin.herokuapp.com instead opening github issue. thank you! check date master branch keras. update check version tensorflow date. installation instructions found https www.tensorflow.org get started os setup . provide link github gist python script reproduce issue copy script short .",0,reshape function,"reshape function anyone guide use reshape function? encoded ?, 4 encod ?, 2 normalize ?, 2 complex symbols ?, decode ?, concat ?, 2 deco ?, 4 decod ?, 4 size none, 2 layers decode concat need size none,1 use reshape function case? option please make sure boxes checked submit issue. issue implementation question , please ask question stackoverflow http stackoverflow.com questions tagged keras keras slack channel https keras slack autojoin.herokuapp.com instead opening github issue. thank you! check date master branch keras. update check version tensorflow date. installation instructions found https www.tensorflow.org get started os setup . provide link github gist python script reproduce issue copy script short ."
keras,5362,"keras model 2 losses. even observations want make use losses, whereas odd observations want use 1st loss. parameter seems allow multiply loss contribution across losses single scalar observation. make odd observations make use 2nd loss?",0,sample loss weights multiple losses,"sample loss weights multiple losses keras model 2 losses. even observations want make use losses, whereas odd observations want use 1st loss. parameter seems allow multiply loss contribution across losses single scalar observation. make odd observations make use 2nd loss?"
keras,3879,"saved model's weight hdf5 file trying load initialize different model different output layer using command model.load weights 'model weights.h5', name true got error typeerror load weights got unexpected keyword argument 'by name' could anyone help?",0,typeerror load weights got unexpected keyword argument 'by name',"typeerror load weights got unexpected keyword argument 'by name' saved model's weight hdf5 file trying load initialize different model different output layer using command model.load weights 'model weights.h5', name true got error typeerror load weights got unexpected keyword argument 'by name' could anyone help?"
keras,7003,"model.fit support multi input output network, data set large enough one use model.fit generator, complicated generate tuple case, plan make simpler like model.fit. network take one input produce two outputs, created generator each, able run network this. input form x, y1, y2 . think need extend generator case ? please make sure boxes checked submit issue. issue implementation question, please ask question stackoverflow http stackoverflow.com questions tagged keras join keras slack channel https keras slack autojoin.herokuapp.com ask instead filing github issue. thank you! x check date master branch keras. update pip install git git github.com fchollet keras.git upgrade deps x running tensorflow, check date latest version. installation instructions found https www.tensorflow.org get started os setup . x running theano, check date master branch theano. update pip install git git github.com theano theano.git upgrade deps x provide link github gist python script reproduce issue copy script short .",0,multi output network generator,"multi output network generator model.fit support multi input output network, data set large enough one use model.fit generator, complicated generate tuple case, plan make simpler like model.fit. network take one input produce two outputs, created generator each, able run network this. input form x, y1, y2 . think need extend generator case ? please make sure boxes checked submit issue. issue implementation question, please ask question stackoverflow http stackoverflow.com questions tagged keras join keras slack channel https keras slack autojoin.herokuapp.com ask instead filing github issue. thank you! x check date master branch keras. update pip install git git github.com fchollet keras.git upgrade deps x running tensorflow, check date latest version. installation instructions found https www.tensorflow.org get started os setup . x running theano, check date master branch theano. update pip install git git github.com theano theano.git upgrade deps x provide link github gist python script reproduce issue copy script short ."
keras,12559,"please make sure boxes checked submit issue. issue implementation question, please ask question stackoverflow http stackoverflow.com questions tagged keras join keras slack channel https keras slack autojoin.herokuapp.com ask instead filing github issue. thank you! check date master branch keras. update pip install git git github.com keras team keras.git upgrade deps running tensorflow, check date latest version. installation instructions found https www.tensorflow.org get started os setup . running theano, check date master branch theano. update pip install git git github.com theano theano.git upgrade deps provide link github gist python script reproduce issue copy script short .",0,"model sequential model.add embedding 10000, embedding size, input length max words model.add conv1d 64, 3, padding 'same' model.add conv1d 32, 3, padding 'same' model.add flatten model.add dropout 0.2 model.add dense 512,activation 'relu' model.add dropout 0.2 model.add dense 1,activation 'softmax","model sequential model.add embedding 10000, embedding size, input length max words model.add conv1d 64, 3, padding 'same' model.add conv1d 32, 3, padding 'same' model.add flatten model.add dropout 0.2 model.add dense 512,activation 'relu' model.add dropout 0.2 model.add dense 1,activation 'softmax please make sure boxes checked submit issue. issue implementation question, please ask question stackoverflow http stackoverflow.com questions tagged keras join keras slack channel https keras slack autojoin.herokuapp.com ask instead filing github issue. thank you! check date master branch keras. update pip install git git github.com keras team keras.git upgrade deps running tensorflow, check date latest version. installation instructions found https www.tensorflow.org get started os setup . running theano, check date master branch theano. update pip install git git github.com theano theano.git upgrade deps provide link github gist python script reproduce issue copy script short ."
keras,6061,getting error ? problem occurred compilation command line c users om anaconda3 envs tensorflow gpu library mingw w64 bin g .exe shared g march broadwell mmmx mno 3dnow msse msse2 msse3 mssse3 mno sse4a mcx16 msahf mmovbe maes mno sha mpclmul mpopcnt mabm mno lwp mfma mno fma4 mno xop mbmi mbmi2 mno tbm mavx mavx2 msse4.2 msse4.1 mlzcnt mrtm mhle mrdrnd mf16c mfsgsbase mrdseed mprfchw mad x mfxsr mxsave mxsaveopt mno avx512f mno avx512er mno avx512cd mno avx512pf mno prefetchwt1 mclflushopt mxsavec mxsaves mno avx512dq mno avx512bw mno avx512vl mno avx512ifm mno avx512vbmi mno clwb mno pcommit mno mwaitx param l1 cache size 32 param l1 cache line size 64 param l2 cache size 6144 mtune generic dnpy deprecated api npy 1 7 api v ersion m64 dms win64 c users om anaconda3 envs tensorflow gpu lib site packages numpy core include c users om anaconda3 envs tensorflow gpu include c users om anaconda3 en vs tensorflow gpu lib site packages theano gof l c users om anaconda3 envs tensorflow gpu libs l c users om anaconda3 envs tensorflow gpu c users om appdata local theano compil edir windows 10 10.0.14393 sp0 intel64 family 6 model 94 stepping 3 genuineintel 3.5.2 64 lazylinker ext lazylinker ext.pyd c users om appdata local theano compiledir windows 10 10.0.143 93 sp0 intel64 family 6 model 94 stepping 3 genuineintel 3.5.2 64 lazylinker ext mod.cpp lpython35 c users om appdata local temp ccuzk0h7.o function imp pyexc importerror' c users om anaconda3 envs tensorflow gpu lib site packages numpy core include numpy multiarray api.h 1461 undefined reference imp pycapsule type' c users om anaconda3 envs tensorflow gpu lib site packages numpy core include numpy multiarray api.h 1467 undefined reference imp pyexc runtimeerror' c users om anaconda3 envs tensorflow gpu lib site packages numpy core include numpy multiarray api.h 1490 undefined reference imp pyexc runtimeerror' c users om anaconda3 envs tensorflow gpu lib site packages numpy core include numpy multiarray api.h 1506 undefined reference imp pyexc runtimeerror' follow c users om appdata local temp ccuzk0h7.o function imp pycapsule type' c users om appdata local temp ccuzk0h7.o function imp p yexc typeerror' c users om appdata local theano compiledir windows 10 10.0.14393 sp0 intel64 family 6 model 94 stepping 3 genuineintel 3.5.2 64 lazylinker ext mod.cpp 58 undefined reference clazylinker init' c users om appdata local theano compiledir windows 10 10.0.14393 sp0 intel64 family 6 model 94 stepping 3 genuineintel 3.5.2 64 lazylinker ext mod.cpp 352 undefined reference imp pyexc indexerror' c users om appdata local theano compiledir windows 10 10.0.14393 sp0 intel64 family 6 model 94 stepping 3 genuineintel 3.5.2 64 lazylinker ext mod.cpp 385 undefined reference imp pyexc indexerror' c users om appdata local temp ccuzk0h7.o c users om appdata local theano compiledir windows 10 10.0.14393 sp0 intel64 family 6 model 94 stepping 3 genuineintel 3.5.2 64 lazylinker ext od.cpp 393 undefined references clazylinker init' c users om appdata local theano compiledir windows 10 10.0.14393 sp0 intel64 family 6 model 94 stepping 3 genuineintel 3.5.2 64 lazylinker ext mod.cpp 405 undefined reference imp py nonestruct' c users om appdata local theano compiledir windows 10 10.0.14393 sp0 intel64 family 6 model 94 stepping 3 genuineintel 3.5.2 64 lazylinker ext mod.cpp 426 undefined reference imp pyexc typeerror' c users om appdata local theano compiledir windows 10 10.0.14393 sp0 intel64 family 6 model 94 stepping 3 genuineintel 3.5.2 64 lazylinker ext mod.cpp 444 undefined reference c call' c users om appdata local theano compiledir windows 10 10.0.14393 sp0 intel64 family 6 model 94 stepping 3 genuineintel 3.5.2 64 lazylinker ext mod.cpp 545 undefined reference imp py nonestruct' c users om appdata local theano compiledir windows 10 10.0.14393 sp0 intel64 family 6 model 94 stepping 3 genuineintel 3.5.2 64 lazylinker ext mod.cpp 545 undefined reference imp py nonestruct' c users om appdata local temp ccuzk0h7.o c users om appdata local theano compiledir windows 10 10.0.14393 sp0 intel64 family 6 model 94 stepping 3 genuineintel 3.5.2 64 lazylinker ext od.cpp 546 undefined references lazy rec eval' c users om appdata local theano compiledir windows 10 10.0.14393 sp0 intel64 family 6 model 94 stepping 3 genuineintel 3.5.2 64 lazylinker ext mod.cpp 618 undefined reference imp pyexc typeerror' c users om appdata local theano compiledir windows 10 10.0.14393 sp0 intel64 family 6 model 94 stepping 3 genuineintel 3.5.2 64 lazylinker ext mod.cpp 649 undefined reference imp pyexc indexerror' c users om appdata local theano compiledir windows 10 10.0.14393 sp0 intel64 family 6 model 94 stepping 3 genuineintel 3.5.2 64 lazylinker ext mod.cpp 708 undefined reference imp pyexc typeerror' c users om appdata local theano compiledir windows 10 10.0.14393 sp0 intel64 family 6 model 94 stepping 3 genuineintel 3.5.2 64 lazylinker ext mod.cpp 721 undefined reference imp py nonestruct' c users om appdata local theano compiledir windows 10 10.0.14393 sp0 intel64 family 6 model 94 stepping 3 genuineintel 3.5.2 64 lazylinker ext mod.cpp 771 undefined reference imp py nonestruct' c users om appdata local temp ccuzk0h7.o function imp pyexc runtimeerror' c users om appdata local theano compiledir windows 10 10.0.14393 sp0 intel64 family 6 model 94 stepping 3 genuineintel 3.5.2 64 lazylinker ext mod.cpp 826 undefined reference imp py nonestruct' c users om appdata local theano compiledir windows 10 10.0.14393 sp0 intel64 family 6 model 94 stepping 3 genuineintel 3.5.2 64 lazylinker ext mod.cpp 839 undefined reference imp py nonestruct' c users om appdata local theano compiledir windows 10 10.0.14393 sp0 intel64 family 6 model 94 stepping 3 genuineintel 3.5.2 64 lazylinker ext mod.cpp 849 undefined reference imp py nonestruct' c users om appdata local temp ccuzk0h7.o c users om appdata local theano compiledir windows 10 10.0.14393 sp0 intel64 family 6 model 94 stepping 3 genuineintel 3.5.2 64 lazylinker ext od.cpp 850 undefined references clazylinker call' c users om appdata local theano compiledir windows 10 10.0.14393 sp0 intel64 family 6 model 94 stepping 3 genuineintel 3.5.2 64 lazylinker ext mod.cpp 894 undefined reference imp py nonestruct' c users om appdata local theano compiledir windows 10 10.0.14393 sp0 intel64 family 6 model 94 stepping 3 genuineintel 3.5.2 64 lazylinker ext mod.cpp 937 undefined reference imp py nonestruct' c users om appdata local temp ccuzk0h7.o function imp pybool type' c users om appdata local theano compiledir windows 10 10.0.14393 sp0 intel64 family 6 model 94 stepping 3 genuineintel 3.5.2 64 lazylinker ext mod.cpp 976 undefined reference . c users om appdata local theano compiledir windows 10 10.0.14393 sp0 intel64 family 6 model 94 stepping 3 genuineintel 3.5.2 64 lazylinker ext mod.cpp 48 undefined reference imp . c users om appdata local theano compiledir windows 10 10.0.14393 sp0 intel64 family 6 model 94 stepping 3 genuineintel 3.5.2 64 lazylinker ext mod.cpp 352 undefined reference . c users om appdata local theano compiledir windows 10 10.0.14393 sp0 intel64 family 6 model 94 stepping 3 genuineintel 3.5.2 64 lazylinker ext mod.cpp 385 undefined reference . c users om appdata local temp ccuzk0h7.o c users om appdata local theano compiledir windows 10 10.0.14393 sp0 intel64 family 6 model 94 stepping 3 genuineintel 3.5.2 64 lazylinker ext . c users om appdata local theano compiledir windows 10 10.0.14393 sp0 intel64 family 6 model 94 stepping 3 genuineintel 3.5.2 64 lazylinker ext mod.cpp 405 undefined reference . c users om appdata local theano compiledir windows 10 10.0.14393 sp0 intel64 family 6 model 94 stepping 3 genuineintel 3.5.2 64 lazylinker ext mod.cpp 426 undefined reference . c users om appdata local theano compiledir windows 10 10.0.14393 sp0 intel64 family 6 model 94 stepping 3 genuineintel 3.5.2 64 lazylinker ext mod.cpp 444 undefined reference . c users om appdata local theano compiledir windows 10 10.0.14393 sp0 intel64 family 6 model 94 stepping 3 genuineintel 3.5.2 64 lazylinker ext mod.cpp 545 undefined reference . c users om appdata local theano compiledir windows 10 10.0.14393 sp0 intel64 family 6 model 94 stepping 3 genuineintel 3.5.2 64 lazylinker ext mod.cpp 546 undefined reference . c users om appdata local theano compiledir windows 10 10.0.14393 sp0 intel64 family 6 model 94 stepping 3 genuineintel 3.5.2 64 lazylinker ext mod.cpp 641 undefined reference . c users om appdata local theano compiledir windows 10 10.0.14393 sp0 intel64 family 6 model 94 stepping 3 genuineintel 3.5.2 64 lazylinker ext mod.cpp 657 undefined reference . c users om appdata local theano compiledir windows 10 10.0.14393 sp0 intel64 family 6 model 94 stepping 3 genuineintel 3.5.2 64 lazylinker ext mod.cpp 715 undefined reference . c users om appdata local theano compiledir windows 10 10.0.14393 sp0 intel64 family 6 model 94 stepping 3 genuineintel 3.5.2 64 lazylinker ext mod.cpp 771 undefined reference . c users om appdata local theano compiledir windows 10 10.0.14393 sp0 intel64 family 6 model 94 stepping 3 genuineintel 3.5.2 64 lazylinker ext mod.cpp 772 undefined reference . c users om appdata local theano compiledir windows 10 10.0.14393 sp0 intel64 family 6 model 94 stepping 3 genuineintel 3.5.2 64 lazylinker ext mod.cpp 826 undefined reference . c users om appdata local theano compiledir windows 10 10.0.14393 sp0 intel64 family 6 model 94 stepping 3 genuineintel 3.5.2 64 lazylinker ext mod.cpp 839 undefined reference . c users om appdata local theano compiledir windows 10 10.0.14393 sp0 intel64 family 6 model 94 stepping 3 genuineintel 3.5.2 64 lazylinker ext mod.cpp 849 undefined reference . c users om appdata local temp ccuzk0h7.o c users om appdata local theano compiledir windows 10 10.0.14393 sp0 intel64 family 6 model 94 stepping 3 genuineintel 3.5.2 64 lazylinker ext . c users om appdata local theano compiledir windows 10 10.0.14393 sp0 intel64 family 6 model 94 stepping 3 genuineintel 3.5.2 64 lazylinker ext mod.cpp 894 undefined reference . c users om appdata local theano compiledir windows 10 10.0.14393 sp0 intel64 family 6 model 94 stepping 3 genuineintel 3.5.2 64 lazylinker ext mod.cpp 937 undefined reference . c users om appdata local theano compiledir windows 10 10.0.14393 sp0 intel64 family 6 model 94 stepping 3 genuineintel 3.5.2 64 lazylinker ext mod.cpp 973 undefined reference . collect2.exe error ld returned 1 exit status,0,"big error , getting ?","big error , getting ? getting error ? problem occurred compilation command line c users om anaconda3 envs tensorflow gpu library mingw w64 bin g .exe shared g march broadwell mmmx mno 3dnow msse msse2 msse3 mssse3 mno sse4a mcx16 msahf mmovbe maes mno sha mpclmul mpopcnt mabm mno lwp mfma mno fma4 mno xop mbmi mbmi2 mno tbm mavx mavx2 msse4.2 msse4.1 mlzcnt mrtm mhle mrdrnd mf16c mfsgsbase mrdseed mprfchw mad x mfxsr mxsave mxsaveopt mno avx512f mno avx512er mno avx512cd mno avx512pf mno prefetchwt1 mclflushopt mxsavec mxsaves mno avx512dq mno avx512bw mno avx512vl mno avx512ifm mno avx512vbmi mno clwb mno pcommit mno mwaitx param l1 cache size 32 param l1 cache line size 64 param l2 cache size 6144 mtune generic dnpy deprecated api npy 1 7 api v ersion m64 dms win64 c users om anaconda3 envs tensorflow gpu lib site packages numpy core include c users om anaconda3 envs tensorflow gpu include c users om anaconda3 en vs tensorflow gpu lib site packages theano gof l c users om anaconda3 envs tensorflow gpu libs l c users om anaconda3 envs tensorflow gpu c users om appdata local theano compil edir windows 10 10.0.14393 sp0 intel64 family 6 model 94 stepping 3 genuineintel 3.5.2 64 lazylinker ext lazylinker ext.pyd c users om appdata local theano compiledir windows 10 10.0.143 93 sp0 intel64 family 6 model 94 stepping 3 genuineintel 3.5.2 64 lazylinker ext mod.cpp lpython35 c users om appdata local temp ccuzk0h7.o function imp pyexc importerror' c users om anaconda3 envs tensorflow gpu lib site packages numpy core include numpy multiarray api.h 1461 undefined reference imp pycapsule type' c users om anaconda3 envs tensorflow gpu lib site packages numpy core include numpy multiarray api.h 1467 undefined reference imp pyexc runtimeerror' c users om anaconda3 envs tensorflow gpu lib site packages numpy core include numpy multiarray api.h 1490 undefined reference imp pyexc runtimeerror' c users om anaconda3 envs tensorflow gpu lib site packages numpy core include numpy multiarray api.h 1506 undefined reference imp pyexc runtimeerror' follow c users om appdata local temp ccuzk0h7.o function imp pycapsule type' c users om appdata local temp ccuzk0h7.o function imp p yexc typeerror' c users om appdata local theano compiledir windows 10 10.0.14393 sp0 intel64 family 6 model 94 stepping 3 genuineintel 3.5.2 64 lazylinker ext mod.cpp 58 undefined reference clazylinker init' c users om appdata local theano compiledir windows 10 10.0.14393 sp0 intel64 family 6 model 94 stepping 3 genuineintel 3.5.2 64 lazylinker ext mod.cpp 352 undefined reference imp pyexc indexerror' c users om appdata local theano compiledir windows 10 10.0.14393 sp0 intel64 family 6 model 94 stepping 3 genuineintel 3.5.2 64 lazylinker ext mod.cpp 385 undefined reference imp pyexc indexerror' c users om appdata local temp ccuzk0h7.o c users om appdata local theano compiledir windows 10 10.0.14393 sp0 intel64 family 6 model 94 stepping 3 genuineintel 3.5.2 64 lazylinker ext od.cpp 393 undefined references clazylinker init' c users om appdata local theano compiledir windows 10 10.0.14393 sp0 intel64 family 6 model 94 stepping 3 genuineintel 3.5.2 64 lazylinker ext mod.cpp 405 undefined reference imp py nonestruct' c users om appdata local theano compiledir windows 10 10.0.14393 sp0 intel64 family 6 model 94 stepping 3 genuineintel 3.5.2 64 lazylinker ext mod.cpp 426 undefined reference imp pyexc typeerror' c users om appdata local theano compiledir windows 10 10.0.14393 sp0 intel64 family 6 model 94 stepping 3 genuineintel 3.5.2 64 lazylinker ext mod.cpp 444 undefined reference c call' c users om appdata local theano compiledir windows 10 10.0.14393 sp0 intel64 family 6 model 94 stepping 3 genuineintel 3.5.2 64 lazylinker ext mod.cpp 545 undefined reference imp py nonestruct' c users om appdata local theano compiledir windows 10 10.0.14393 sp0 intel64 family 6 model 94 stepping 3 genuineintel 3.5.2 64 lazylinker ext mod.cpp 545 undefined reference imp py nonestruct' c users om appdata local temp ccuzk0h7.o c users om appdata local theano compiledir windows 10 10.0.14393 sp0 intel64 family 6 model 94 stepping 3 genuineintel 3.5.2 64 lazylinker ext od.cpp 546 undefined references lazy rec eval' c users om appdata local theano compiledir windows 10 10.0.14393 sp0 intel64 family 6 model 94 stepping 3 genuineintel 3.5.2 64 lazylinker ext mod.cpp 618 undefined reference imp pyexc typeerror' c users om appdata local theano compiledir windows 10 10.0.14393 sp0 intel64 family 6 model 94 stepping 3 genuineintel 3.5.2 64 lazylinker ext mod.cpp 649 undefined reference imp pyexc indexerror' c users om appdata local theano compiledir windows 10 10.0.14393 sp0 intel64 family 6 model 94 stepping 3 genuineintel 3.5.2 64 lazylinker ext mod.cpp 708 undefined reference imp pyexc typeerror' c users om appdata local theano compiledir windows 10 10.0.14393 sp0 intel64 family 6 model 94 stepping 3 genuineintel 3.5.2 64 lazylinker ext mod.cpp 721 undefined reference imp py nonestruct' c users om appdata local theano compiledir windows 10 10.0.14393 sp0 intel64 family 6 model 94 stepping 3 genuineintel 3.5.2 64 lazylinker ext mod.cpp 771 undefined reference imp py nonestruct' c users om appdata local temp ccuzk0h7.o function imp pyexc runtimeerror' c users om appdata local theano compiledir windows 10 10.0.14393 sp0 intel64 family 6 model 94 stepping 3 genuineintel 3.5.2 64 lazylinker ext mod.cpp 826 undefined reference imp py nonestruct' c users om appdata local theano compiledir windows 10 10.0.14393 sp0 intel64 family 6 model 94 stepping 3 genuineintel 3.5.2 64 lazylinker ext mod.cpp 839 undefined reference imp py nonestruct' c users om appdata local theano compiledir windows 10 10.0.14393 sp0 intel64 family 6 model 94 stepping 3 genuineintel 3.5.2 64 lazylinker ext mod.cpp 849 undefined reference imp py nonestruct' c users om appdata local temp ccuzk0h7.o c users om appdata local theano compiledir windows 10 10.0.14393 sp0 intel64 family 6 model 94 stepping 3 genuineintel 3.5.2 64 lazylinker ext od.cpp 850 undefined references clazylinker call' c users om appdata local theano compiledir windows 10 10.0.14393 sp0 intel64 family 6 model 94 stepping 3 genuineintel 3.5.2 64 lazylinker ext mod.cpp 894 undefined reference imp py nonestruct' c users om appdata local theano compiledir windows 10 10.0.14393 sp0 intel64 family 6 model 94 stepping 3 genuineintel 3.5.2 64 lazylinker ext mod.cpp 937 undefined reference imp py nonestruct' c users om appdata local temp ccuzk0h7.o function imp pybool type' c users om appdata local theano compiledir windows 10 10.0.14393 sp0 intel64 family 6 model 94 stepping 3 genuineintel 3.5.2 64 lazylinker ext mod.cpp 976 undefined reference . c users om appdata local theano compiledir windows 10 10.0.14393 sp0 intel64 family 6 model 94 stepping 3 genuineintel 3.5.2 64 lazylinker ext mod.cpp 48 undefined reference imp . c users om appdata local theano compiledir windows 10 10.0.14393 sp0 intel64 family 6 model 94 stepping 3 genuineintel 3.5.2 64 lazylinker ext mod.cpp 352 undefined reference . c users om appdata local theano compiledir windows 10 10.0.14393 sp0 intel64 family 6 model 94 stepping 3 genuineintel 3.5.2 64 lazylinker ext mod.cpp 385 undefined reference . c users om appdata local temp ccuzk0h7.o c users om appdata local theano compiledir windows 10 10.0.14393 sp0 intel64 family 6 model 94 stepping 3 genuineintel 3.5.2 64 lazylinker ext . c users om appdata local theano compiledir windows 10 10.0.14393 sp0 intel64 family 6 model 94 stepping 3 genuineintel 3.5.2 64 lazylinker ext mod.cpp 405 undefined reference . c users om appdata local theano compiledir windows 10 10.0.14393 sp0 intel64 family 6 model 94 stepping 3 genuineintel 3.5.2 64 lazylinker ext mod.cpp 426 undefined reference . c users om appdata local theano compiledir windows 10 10.0.14393 sp0 intel64 family 6 model 94 stepping 3 genuineintel 3.5.2 64 lazylinker ext mod.cpp 444 undefined reference . c users om appdata local theano compiledir windows 10 10.0.14393 sp0 intel64 family 6 model 94 stepping 3 genuineintel 3.5.2 64 lazylinker ext mod.cpp 545 undefined reference . c users om appdata local theano compiledir windows 10 10.0.14393 sp0 intel64 family 6 model 94 stepping 3 genuineintel 3.5.2 64 lazylinker ext mod.cpp 546 undefined reference . c users om appdata local theano compiledir windows 10 10.0.14393 sp0 intel64 family 6 model 94 stepping 3 genuineintel 3.5.2 64 lazylinker ext mod.cpp 641 undefined reference . c users om appdata local theano compiledir windows 10 10.0.14393 sp0 intel64 family 6 model 94 stepping 3 genuineintel 3.5.2 64 lazylinker ext mod.cpp 657 undefined reference . c users om appdata local theano compiledir windows 10 10.0.14393 sp0 intel64 family 6 model 94 stepping 3 genuineintel 3.5.2 64 lazylinker ext mod.cpp 715 undefined reference . c users om appdata local theano compiledir windows 10 10.0.14393 sp0 intel64 family 6 model 94 stepping 3 genuineintel 3.5.2 64 lazylinker ext mod.cpp 771 undefined reference . c users om appdata local theano compiledir windows 10 10.0.14393 sp0 intel64 family 6 model 94 stepping 3 genuineintel 3.5.2 64 lazylinker ext mod.cpp 772 undefined reference . c users om appdata local theano compiledir windows 10 10.0.14393 sp0 intel64 family 6 model 94 stepping 3 genuineintel 3.5.2 64 lazylinker ext mod.cpp 826 undefined reference . c users om appdata local theano compiledir windows 10 10.0.14393 sp0 intel64 family 6 model 94 stepping 3 genuineintel 3.5.2 64 lazylinker ext mod.cpp 839 undefined reference . c users om appdata local theano compiledir windows 10 10.0.14393 sp0 intel64 family 6 model 94 stepping 3 genuineintel 3.5.2 64 lazylinker ext mod.cpp 849 undefined reference . c users om appdata local temp ccuzk0h7.o c users om appdata local theano compiledir windows 10 10.0.14393 sp0 intel64 family 6 model 94 stepping 3 genuineintel 3.5.2 64 lazylinker ext . c users om appdata local theano compiledir windows 10 10.0.14393 sp0 intel64 family 6 model 94 stepping 3 genuineintel 3.5.2 64 lazylinker ext mod.cpp 894 undefined reference . c users om appdata local theano compiledir windows 10 10.0.14393 sp0 intel64 family 6 model 94 stepping 3 genuineintel 3.5.2 64 lazylinker ext mod.cpp 937 undefined reference . c users om appdata local theano compiledir windows 10 10.0.14393 sp0 intel64 family 6 model 94 stepping 3 genuineintel 3.5.2 64 lazylinker ext mod.cpp 973 undefined reference . collect2.exe error ld returned 1 exit status"
keras,10353,"please make sure boxes checked submit issue. issue implementation question, please ask question stackoverflow http stackoverflow.com questions tagged keras join keras slack channel https keras slack autojoin.herokuapp.com ask instead filing github issue. thank you! check date master branch keras. update pip install git git github.com keras team keras.git upgrade deps running tensorflow, check date latest version. installation instructions found https www.tensorflow.org get started os setup . running theano, check date master branch theano. update pip install git git github.com theano theano.git upgrade deps provide link github gist python script reproduce issue copy script short .",0,callbacks,"callbacks please make sure boxes checked submit issue. issue implementation question, please ask question stackoverflow http stackoverflow.com questions tagged keras join keras slack channel https keras slack autojoin.herokuapp.com ask instead filing github issue. thank you! check date master branch keras. update pip install git git github.com keras team keras.git upgrade deps running tensorflow, check date latest version. installation instructions found https www.tensorflow.org get started os setup . running theano, check date master branch theano. update pip install git git github.com theano theano.git upgrade deps provide link github gist python script reproduce issue copy script short ."
keras,3037,"implementing custom layer. try call using following code gives error. right way implement layer multiple inputs? also code call function layer using theano.tensor functions instead functions given keras backend. mean able use functional api anymore? traceback recent call last file users aditya documents qbit logic tbcnn tree.py , line 338, fitlog model.fit trees, connections, n leaves mat , y, batch size 1, nb epoch 50, verbose 1 file users aditya anaconda envs acads lib python3.5 site packages keras models.py , line 409, fit sample weight sample weight file users aditya anaconda envs acads lib python3.5 site packages keras engine training.py , line 1037, fit self. make train function file users aditya anaconda envs acads lib python3.5 site packages keras engine training.py , line 663, make train function training updates self.optimizer.get updates trainable weights, self.constraints, self.total loss file users aditya anaconda envs acads lib python3.5 site packages keras optimizers.py , line 321, get updates grads self.get gradients loss, params file users aditya anaconda envs acads lib python3.5 site packages keras optimizers.py , line 53, get gradients grads k.gradients loss, params file users aditya anaconda envs acads lib python3.5 site packages keras backend theano backend.py , line 532, gradients return t.grad loss, variables file users aditya anaconda envs acads lib python3.5 site packages theano gradient.py , line 545, grad handle disconnected elem file users aditya anaconda envs acads lib python3.5 site packages theano gradient.py , line 532, handle disconnected raise disconnectedinputerror message theano.gradient.disconnectedinputerror grad method asked compute gradient respect variable part computational graph cost, used non differentiable operator backtrace node created file users aditya documents qbit logic tbcnn tree.py , line 336, model.compile 'adam', 'mse' file users aditya anaconda envs acads lib python3.5 site packages keras models.py , line 339, compile kwargs file users aditya anaconda envs acads lib python3.5 site packages keras engine training.py , line 510, compile masks self.compute mask self.inputs, mask none file users aditya anaconda envs acads lib python3.5 site packages keras engine topology.py , line 1914, compute mask output tensors, output masks, output shapes self.run internal graph inputs, masks file users aditya anaconda envs acads lib python3.5 site packages keras engine topology.py , line 2049, run internal graph output tensors list layer.call computed tensors, computed masks file users aditya documents qbit logic tbcnn tree.py , line 178, call self.build x.shape x inputs file users aditya documents qbit logic tbcnn tree.py , line 168, build self.vci, self.wl, self.wr k.variable vci , k.variable wl , k.variable wr file users aditya anaconda envs acads lib python3.5 site packages keras backend theano backend.py , line 31, variable return theano.shared value value, name name, strict false please make sure boxes checked submit issue. thank you! x check date master branch keras. update pip install git git github.com fchollet keras.git upgrade deps x running theano, check date master branch theano. update pip install git git github.com theano theano.git upgrade deps x provide link github gist python script reproduce issue copy script short .",0,implement custom layer multiple inputs input layer trainable weights,"implement custom layer multiple inputs input layer trainable weights implementing custom layer. try call using following code gives error. right way implement layer multiple inputs? also code call function layer using theano.tensor functions instead functions given keras backend. mean able use functional api anymore? traceback recent call last file users aditya documents qbit logic tbcnn tree.py , line 338, fitlog model.fit trees, connections, n leaves mat , y, batch size 1, nb epoch 50, verbose 1 file users aditya anaconda envs acads lib python3.5 site packages keras models.py , line 409, fit sample weight sample weight file users aditya anaconda envs acads lib python3.5 site packages keras engine training.py , line 1037, fit self. make train function file users aditya anaconda envs acads lib python3.5 site packages keras engine training.py , line 663, make train function training updates self.optimizer.get updates trainable weights, self.constraints, self.total loss file users aditya anaconda envs acads lib python3.5 site packages keras optimizers.py , line 321, get updates grads self.get gradients loss, params file users aditya anaconda envs acads lib python3.5 site packages keras optimizers.py , line 53, get gradients grads k.gradients loss, params file users aditya anaconda envs acads lib python3.5 site packages keras backend theano backend.py , line 532, gradients return t.grad loss, variables file users aditya anaconda envs acads lib python3.5 site packages theano gradient.py , line 545, grad handle disconnected elem file users aditya anaconda envs acads lib python3.5 site packages theano gradient.py , line 532, handle disconnected raise disconnectedinputerror message theano.gradient.disconnectedinputerror grad method asked compute gradient respect variable part computational graph cost, used non differentiable operator backtrace node created file users aditya documents qbit logic tbcnn tree.py , line 336, model.compile 'adam', 'mse' file users aditya anaconda envs acads lib python3.5 site packages keras models.py , line 339, compile kwargs file users aditya anaconda envs acads lib python3.5 site packages keras engine training.py , line 510, compile masks self.compute mask self.inputs, mask none file users aditya anaconda envs acads lib python3.5 site packages keras engine topology.py , line 1914, compute mask output tensors, output masks, output shapes self.run internal graph inputs, masks file users aditya anaconda envs acads lib python3.5 site packages keras engine topology.py , line 2049, run internal graph output tensors list layer.call computed tensors, computed masks file users aditya documents qbit logic tbcnn tree.py , line 178, call self.build x.shape x inputs file users aditya documents qbit logic tbcnn tree.py , line 168, build self.vci, self.wl, self.wr k.variable vci , k.variable wl , k.variable wr file users aditya anaconda envs acads lib python3.5 site packages keras backend theano backend.py , line 31, variable return theano.shared value value, name name, strict false please make sure boxes checked submit issue. thank you! x check date master branch keras. update pip install git git github.com fchollet keras.git upgrade deps x running theano, check date master branch theano. update pip install git git github.com theano theano.git upgrade deps x provide link github gist python script reproduce issue copy script short ."
keras,7424,"hi, pointing problem, wondering manage want. explain image x, want predict couple numpy array y1, y2 . y1 38, 60, 18 shaped y2 38, 60, 72 shaped. working big dataset, would like use train batch function. however, function take numpy array x values problem numpy array values. y1 y2 shape, can't stack single array, using list raise exception train batch calls shape. anyone idea trick ? thanks reading",0,train batch multiple predictions,"train batch multiple predictions hi, pointing problem, wondering manage want. explain image x, want predict couple numpy array y1, y2 . y1 38, 60, 18 shaped y2 38, 60, 72 shaped. working big dataset, would like use train batch function. however, function take numpy array x values problem numpy array values. y1 y2 shape, can't stack single array, using list raise exception train batch calls shape. anyone idea trick ? thanks reading"
keras,4891,"hello everyone! could anyone tell me, please, whether built leave one validation exists keras? mean, let's assume training data set consisting 100 records want build model 100 times based 99 records validating remaining one. would like avoid implementing loops, possible. course, process like accumulate error predictions basically, target variable continuous, speeking regression task model model. thank much 4 responses advance! almost forgot happy new year!",0,built leave one validation keras,"built leave one validation keras hello everyone! could anyone tell me, please, whether built leave one validation exists keras? mean, let's assume training data set consisting 100 records want build model 100 times based 99 records validating remaining one. would like avoid implementing loops, possible. course, process like accumulate error predictions basically, target variable continuous, speeking regression task model model. thank much 4 responses advance! almost forgot happy new year!"
keras,9576,https keras.io activations appears last line redundant otherwise contradicts previous paragraphs page equivalent,0,docs correction https keras.io activations,docs correction https keras.io activations https keras.io activations appears last line redundant otherwise contradicts previous paragraphs page equivalent
keras,4045,"hi, using pretrained embedding layer top stateful lstm. word2vec embedding trained larger corpus word2vec corpus lstm training corpus model corpus . mapping word vectors embedding weights using word2vec model. truncated model corpus divided uniform sequences preparing labels train data using lstm model vocab word2vec vocab one hot encoding 0 masking. training model, getting following error model missing something? thanks.",0,index value bound training using embedding stateful lstm,"index value bound training using embedding stateful lstm hi, using pretrained embedding layer top stateful lstm. word2vec embedding trained larger corpus word2vec corpus lstm training corpus model corpus . mapping word vectors embedding weights using word2vec model. truncated model corpus divided uniform sequences preparing labels train data using lstm model vocab word2vec vocab one hot encoding 0 masking. training model, getting following error model missing something? thanks."
keras,7539,"please make sure boxes checked submit issue. issue implementation question, please ask question stackoverflow http stackoverflow.com questions tagged keras join keras slack channel https keras slack autojoin.herokuapp.com ask instead filing github issue. thank you! x check date master branch keras. update pip install git git github.com fchollet keras.git upgrade deps x running tensorflow, check date latest version. installation instructions found https www.tensorflow.org get started os setup . running theano, check date master branch theano. update pip install git git github.com theano theano.git upgrade deps x provide link github gist python script reproduce issue copy script short . asked question stackoverflow, buy received answers it. try fit model theano backend run following error here's full error stack required use theano backend due compatibility issues tensorflow backend university cluster. tested code cpu's personal device tensorflow backend runs error free. load script train script packages",0,unicodedecode error keras fit function theano backend,"unicodedecode error keras fit function theano backend please make sure boxes checked submit issue. issue implementation question, please ask question stackoverflow http stackoverflow.com questions tagged keras join keras slack channel https keras slack autojoin.herokuapp.com ask instead filing github issue. thank you! x check date master branch keras. update pip install git git github.com fchollet keras.git upgrade deps x running tensorflow, check date latest version. installation instructions found https www.tensorflow.org get started os setup . running theano, check date master branch theano. update pip install git git github.com theano theano.git upgrade deps x provide link github gist python script reproduce issue copy script short . asked question stackoverflow, buy received answers it. try fit model theano backend run following error here's full error stack required use theano backend due compatibility issues tensorflow backend university cluster. tested code cpu's personal device tensorflow backend runs error free. load script train script packages"
keras,6842,unable save model trained using kerasregressor wrapper... here's import ideas?,0,'kerasregressor' object attribute 'to json','kerasregressor' object attribute 'to json' unable save model trained using kerasregressor wrapper... here's import ideas?
keras,2607,"examples use metrics 'accuracy' , accuracy always suitable every task. 1. metrics precision, recall on? 2. are, write metrics list order use them? 3. one output, use multiple metrics evaluate different aspect?",0,metrics used keras,"metrics used keras examples use metrics 'accuracy' , accuracy always suitable every task. 1. metrics precision, recall on? 2. are, write metrics list order use them? 3. one output, use multiple metrics evaluate different aspect?"
keras,1415,interesting.. really issue malay language ''hard,0,keras malay,keras malay interesting.. really issue malay language ''hard
keras,3543,"trying implement layer normalization standard fully connected neural network keras, writing new layer. copy nearly code dense layer add function layer normalization corresponding parameters. code fit, got typeerror unorderable types nonetype nonetype . according log message, seems reason trainable weights code building model fit could please tell done wrong fix it? thank advance!",0,set trainable weights properly?,"set trainable weights properly? trying implement layer normalization standard fully connected neural network keras, writing new layer. copy nearly code dense layer add function layer normalization corresponding parameters. code fit, got typeerror unorderable types nonetype nonetype . according log message, seems reason trainable weights code building model fit could please tell done wrong fix it? thank advance!"
keras,3651,"using shown operate higher order inputs, propagate shape information accordingly. however, run code above, see first print statement shows symbolic mask second one gives . sure propagate mask. x check date master branch keras. update pip install git git github.com fchollet keras.git upgrade deps x running theano, check date master branch theano. update pip install git git github.com theano theano.git upgrade deps x provide link github gist python script reproduce issue copy script short .",0,mask propagation timedistributed seem work embedding,"mask propagation timedistributed seem work embedding using shown operate higher order inputs, propagate shape information accordingly. however, run code above, see first print statement shows symbolic mask second one gives . sure propagate mask. x check date master branch keras. update pip install git git github.com fchollet keras.git upgrade deps x running theano, check date master branch theano. update pip install git git github.com theano theano.git upgrade deps x provide link github gist python script reproduce issue copy script short ."
keras,3008,seem possible save load models json yaml there's merge layer lambda.,0,json yaml loading working lambda merge layers,json yaml loading working lambda merge layers seem possible save load models json yaml there's merge layer lambda.
keras,544,"hi guys, updated keras package can't import sequential properly. give following erros file usr lib python2.6 site packages keras 0.1.2 py2.6.egg keras models.py , line 118 model name 'graph', 'sequential' syntaxerror invalid syntax could anyone help me?",0,can't import sequential,"can't import sequential hi guys, updated keras package can't import sequential properly. give following erros file usr lib python2.6 site packages keras 0.1.2 py2.6.egg keras models.py , line 118 model name 'graph', 'sequential' syntaxerror invalid syntax could anyone help me?"
keras,4767,"problem is, writing layer input shape none, 10 , want append value end every row make none, 11 . this? right thinking creating another none, 1 , concatenating them. thanks advance.",0,"create variable shape none, 1 ?","create variable shape none, 1 ? problem is, writing layer input shape none, 10 , want append value end every row make none, 11 . this? right thinking creating another none, 1 , concatenating them. thanks advance."
keras,4776,"hi, sequence labelling task like postagging using keras theano. model use bilstm. training, padding. instead, used batch size 1, specify input length parameter. works well training evaluation use generator version deal memory problem . however, want prediction, following errors think reason first sentence length 42 words, second sentence length 10 words. question is, works well training evaluation, work prediction? code think relevant",0,predict generator function support samples different lengths,"predict generator function support samples different lengths hi, sequence labelling task like postagging using keras theano. model use bilstm. training, padding. instead, used batch size 1, specify input length parameter. works well training evaluation use generator version deal memory problem . however, want prediction, following errors think reason first sentence length 42 words, second sentence length 10 words. question is, works well training evaluation, work prediction? code think relevant"
keras,8070,"hi all, trying split one keras layer using lambda function. follow code snippet model compiles well. feed real training data model, gives error messege generated images,f id generator.predict image batch, c , z file build bdist.linux x86 64 egg keras engine training.py , line 1653, predict file build bdist.linux x86 64 egg keras engine training.py , line 1246, predict loop file build bdist.linux x86 64 egg keras backend tensorflow backend.py , line 2255, call file home vivo anaconda2 envs tensorflow lib python2.7 site packages tensorflow python client session.py , line 767, run run metadata ptr file home vivo anaconda2 envs tensorflow lib python2.7 site packages tensorflow python client session.py , line 965, run feed dict string, options, run metadata file home vivo anaconda2 envs tensorflow lib python2.7 site packages tensorflow python client session.py , line 1015, run target list, options, run metadata file home vivo anaconda2 envs tensorflow lib python2.7 site packages tensorflow python client session.py , line 1035, call raise type e node def, op, message tensorflow.python.framework.errors impl.invalidargumenterror input reshape tensor 8768 values, requested shape requires multiple 146 node dense 4 reshape reshape dt float, tshape dt int32, device job localhost replica 0 task 0 gpu 0 concatenate 1 concat, dense 4 reshape shape node conv2d transpose 3 tanh 53 recv client terminated false, recv device job localhost replica 0 task 0 cpu 0 , send device job localhost replica 0 task 0 gpu 0 , send device incarnation 1, tensor name edge 1 conv2d transpose 3 tanh , tensor type dt float, device job localhost replica 0 task 0 cpu 0 caused op u'dense 4 reshape', defined file gan utd 2nd model.py , line 428, generator model generator latent dim latent dim, input shape input shape, units units file gan utd 2nd model.py , line 240, model generator h dense dense 15 20 64, activation 'relu' h file build bdist.linux x86 64 egg keras engine topology.py , line 602, call output self.call inputs, kwargs file build bdist.linux x86 64 egg keras layers core.py , line 841, call output k.dot inputs, self.kernel file build bdist.linux x86 64 egg keras backend tensorflow backend.py , line 973, dot xt tf.reshape x, 1, x shape 1 file home vivo anaconda2 envs tensorflow lib python2.7 site packages tensorflow python ops gen array ops.py , line 2630, reshape name name file home vivo anaconda2 envs tensorflow lib python2.7 site packages tensorflow python framework op def library.py , line 763, apply op op def op def file home vivo anaconda2 envs tensorflow lib python2.7 site packages tensorflow python framework ops.py , line 2327, create op original op self. default original op, op def op def file home vivo anaconda2 envs tensorflow lib python2.7 site packages tensorflow python framework ops.py , line 1226, init self. traceback extract stack invalidargumenterror see traceback input reshape tensor 8768 values, requested shape requires multiple 146 node dense 4 reshape reshape dt float, tshape dt int32, device job localhost replica 0 task 0 gpu 0 concatenate 1 concat, dense 4 reshape shape node conv2d transpose 3 tanh 53 recv client terminated false, recv device job localhost replica 0 task 0 cpu 0 , send device job localhost replica 0 task 0 gpu 0 , send device incarnation 1, tensor name edge 1 conv2d transpose 3 tanh , tensor type dt float, device job localhost replica 0 task 0 cpu 0 understand especially error message tensorflow.python.framework.errors impl.invalidargumenterror input reshape tensor 8768 values, requested shape requires multiple 146 . output dense layer 19200 units, right following reshape layer... idea?? best",0,layer split use lambda layer gives error,"layer split use lambda layer gives error hi all, trying split one keras layer using lambda function. follow code snippet model compiles well. feed real training data model, gives error messege generated images,f id generator.predict image batch, c , z file build bdist.linux x86 64 egg keras engine training.py , line 1653, predict file build bdist.linux x86 64 egg keras engine training.py , line 1246, predict loop file build bdist.linux x86 64 egg keras backend tensorflow backend.py , line 2255, call file home vivo anaconda2 envs tensorflow lib python2.7 site packages tensorflow python client session.py , line 767, run run metadata ptr file home vivo anaconda2 envs tensorflow lib python2.7 site packages tensorflow python client session.py , line 965, run feed dict string, options, run metadata file home vivo anaconda2 envs tensorflow lib python2.7 site packages tensorflow python client session.py , line 1015, run target list, options, run metadata file home vivo anaconda2 envs tensorflow lib python2.7 site packages tensorflow python client session.py , line 1035, call raise type e node def, op, message tensorflow.python.framework.errors impl.invalidargumenterror input reshape tensor 8768 values, requested shape requires multiple 146 node dense 4 reshape reshape dt float, tshape dt int32, device job localhost replica 0 task 0 gpu 0 concatenate 1 concat, dense 4 reshape shape node conv2d transpose 3 tanh 53 recv client terminated false, recv device job localhost replica 0 task 0 cpu 0 , send device job localhost replica 0 task 0 gpu 0 , send device incarnation 1, tensor name edge 1 conv2d transpose 3 tanh , tensor type dt float, device job localhost replica 0 task 0 cpu 0 caused op u'dense 4 reshape', defined file gan utd 2nd model.py , line 428, generator model generator latent dim latent dim, input shape input shape, units units file gan utd 2nd model.py , line 240, model generator h dense dense 15 20 64, activation 'relu' h file build bdist.linux x86 64 egg keras engine topology.py , line 602, call output self.call inputs, kwargs file build bdist.linux x86 64 egg keras layers core.py , line 841, call output k.dot inputs, self.kernel file build bdist.linux x86 64 egg keras backend tensorflow backend.py , line 973, dot xt tf.reshape x, 1, x shape 1 file home vivo anaconda2 envs tensorflow lib python2.7 site packages tensorflow python ops gen array ops.py , line 2630, reshape name name file home vivo anaconda2 envs tensorflow lib python2.7 site packages tensorflow python framework op def library.py , line 763, apply op op def op def file home vivo anaconda2 envs tensorflow lib python2.7 site packages tensorflow python framework ops.py , line 2327, create op original op self. default original op, op def op def file home vivo anaconda2 envs tensorflow lib python2.7 site packages tensorflow python framework ops.py , line 1226, init self. traceback extract stack invalidargumenterror see traceback input reshape tensor 8768 values, requested shape requires multiple 146 node dense 4 reshape reshape dt float, tshape dt int32, device job localhost replica 0 task 0 gpu 0 concatenate 1 concat, dense 4 reshape shape node conv2d transpose 3 tanh 53 recv client terminated false, recv device job localhost replica 0 task 0 cpu 0 , send device job localhost replica 0 task 0 gpu 0 , send device incarnation 1, tensor name edge 1 conv2d transpose 3 tanh , tensor type dt float, device job localhost replica 0 task 0 cpu 0 understand especially error message tensorflow.python.framework.errors impl.invalidargumenterror input reshape tensor 8768 values, requested shape requires multiple 146 . output dense layer 19200 units, right following reshape layer... idea?? best"
keras,1198,nan,0,display rgb image e.g. cifar keras?,display rgb image e.g. cifar keras? nan
keras,2545,"2 questions 1. discussed sequence sequence learning length input output different problem 2403 concluded use encoder decoder architecture. fundamental question, reason keras support case encoder decoder needed. fundamental problem recurrent neural networks is, anyone explain me? , keras problem support case? 2.when following model consider batch size 1, following statement true a. n prev inputs fed hidden neurons first lstm layer? b. time one prev inputs fed first lstm layer propagate hidden neurons layer?",0,sequence sequence training predicting decoder encoder,"sequence sequence training predicting decoder encoder 2 questions 1. discussed sequence sequence learning length input output different problem 2403 concluded use encoder decoder architecture. fundamental question, reason keras support case encoder decoder needed. fundamental problem recurrent neural networks is, anyone explain me? , keras problem support case? 2.when following model consider batch size 1, following statement true a. n prev inputs fed hidden neurons first lstm layer? b. time one prev inputs fed first lstm layer propagate hidden neurons layer?"
keras,13268,"please make sure bug feature request provide applicable information asked template. issue implementation question , please ask question stackoverflow http stackoverflow.com questions tagged keras keras slack channel https keras slack autojoin.herokuapp.com instead opening github issue. system information written custom code opposed using example directory os platform distribution e.g., linux ubuntu 16.04 linux centos 7.0 tensorflow backend yes yes tensorflow version 1.14 keras version 2.2.0 python version 2.7 cuda cudnn version 8.0 gpu model memory single gpu 16gb memory obtain tensorflow version python c import tensorflow tf print tf.git version, tf.version obtain keras version python c 'import keras k print k. version ' describe current behavior describe expected behavior code reproduce issue provide reproducible test case bare minimum necessary generate problem. info logs include logs source code would helpful diagnose problem. including tracebacks, please include full traceback. large logs files attached. hi ,i found run code weight images got different predictions time ,my code images github binary categorical https github.com yasohasakii binary categorical run script colab, weight file upload google dirver https drive.google.com open?id 1sciagoq7og18ibmhwaoumvsbrziax5fo . codes result twice times.",0,predictions different weight,"predictions different weight please make sure bug feature request provide applicable information asked template. issue implementation question , please ask question stackoverflow http stackoverflow.com questions tagged keras keras slack channel https keras slack autojoin.herokuapp.com instead opening github issue. system information written custom code opposed using example directory os platform distribution e.g., linux ubuntu 16.04 linux centos 7.0 tensorflow backend yes yes tensorflow version 1.14 keras version 2.2.0 python version 2.7 cuda cudnn version 8.0 gpu model memory single gpu 16gb memory obtain tensorflow version python c import tensorflow tf print tf.git version, tf.version obtain keras version python c 'import keras k print k. version ' describe current behavior describe expected behavior code reproduce issue provide reproducible test case bare minimum necessary generate problem. info logs include logs source code would helpful diagnose problem. including tracebacks, please include full traceback. large logs files attached. hi ,i found run code weight images got different predictions time ,my code images github binary categorical https github.com yasohasakii binary categorical run script colab, weight file upload google dirver https drive.google.com open?id 1sciagoq7og18ibmhwaoumvsbrziax5fo . codes result twice times."
keras,5044,"here's script 1 run see exception. one run like . quick glance source, seems like used constructor prevent set , ultimately becomes base . reporters previous bug 4699 reported works create additional directory directory passed . however, think ideal, semantically, class label dealing test images. proposal fix set self.nb sample number valid image files input directory , . second clause condition help prevent breakage code hacked way around issue creating additional directory afraid breaking, . please let know okay this, ideas including calling issue non issue , accordingly prepare quick pr . 1 https gist.github.com yati sagade ff309678a6d6ec849c488b1f9a5fa6b3",0,zerodivisionerror using class mode none imagedatagenerator.flow directory,"zerodivisionerror using class mode none imagedatagenerator.flow directory here's script 1 run see exception. one run like . quick glance source, seems like used constructor prevent set , ultimately becomes base . reporters previous bug 4699 reported works create additional directory directory passed . however, think ideal, semantically, class label dealing test images. proposal fix set self.nb sample number valid image files input directory , . second clause condition help prevent breakage code hacked way around issue creating additional directory afraid breaking, . please let know okay this, ideas including calling issue non issue , accordingly prepare quick pr . 1 https gist.github.com yati sagade ff309678a6d6ec849c488b1f9a5fa6b3"
keras,8073,"written loss function seems get stuck error can't figure out. seems could either sample weight mask, know it. anyone help? ! image https user images.githubusercontent.com 5853644 31272735 49a469ce aa8c 11e7 9d48 80eb1a64b29a.png",0,understanding error,"understanding error written loss function seems get stuck error can't figure out. seems could either sample weight mask, know it. anyone help? ! image https user images.githubusercontent.com 5853644 31272735 49a469ce aa8c 11e7 9d48 80eb1a64b29a.png"
keras,7193,"use keras 2.02 theano backend call attribute 'ctc decode' using k.ctc decode, error thrown. use dir k check avaliable attribute, find 'ctc decode'. problem? thank advance!",0,attributeerror 'module' object attribute 'ctc decode',"attributeerror 'module' object attribute 'ctc decode' use keras 2.02 theano backend call attribute 'ctc decode' using k.ctc decode, error thrown. use dir k check avaliable attribute, find 'ctc decode'. problem? thank advance!"
keras,8827,"hi, wondering correct input shape target size using fit generator flow directory. running keras 2.0.8 tf tensorflow 1.4 backend. images following dimensions width 725 pixel height 180 pixel channels 3 define input shape width, height, channels 725,180,3 , suggested using tensorflow backend. use imagedatagenerator flow directory method get training data. target size may specified. target size either width, height height, width . api defining height, width . problem occurs. input shape width, height, channels target size heigth, width valueerror raised, saying shapes match error checking input expected conv2d 1 input shape none, 725, 180, 3 got array shape 50, 180, 725, 3 . set target size width, height , training starting, plot image using generator, see width height exchanged hence image massively transformed. solve issue? fchollets great example https gist.github.com fchollet 0830affa1f7f19fd47b06d4cf89ed44d target size also set way around api. help appreciated! thank you. code example",0,help wanted input shape target size flow directory,"help wanted input shape target size flow directory hi, wondering correct input shape target size using fit generator flow directory. running keras 2.0.8 tf tensorflow 1.4 backend. images following dimensions width 725 pixel height 180 pixel channels 3 define input shape width, height, channels 725,180,3 , suggested using tensorflow backend. use imagedatagenerator flow directory method get training data. target size may specified. target size either width, height height, width . api defining height, width . problem occurs. input shape width, height, channels target size heigth, width valueerror raised, saying shapes match error checking input expected conv2d 1 input shape none, 725, 180, 3 got array shape 50, 180, 725, 3 . set target size width, height , training starting, plot image using generator, see width height exchanged hence image massively transformed. solve issue? fchollets great example https gist.github.com fchollet 0830affa1f7f19fd47b06d4cf89ed44d target size also set way around api. help appreciated! thank you. code example"
keras,4094,"trying load earlier trained model reproduce outputs, observed different outputs input model loaded different keras versions namely 1.0.5 1.1.0 . model trained keras 1.0.5. tried also verify dummy input data fed model loaded different versions. checked loaded weights match two versions. difference layers defined v 1.0.5 v 1.1.0? noticed least default parameters batch normalization layer slightly different. worth, get similar accuracy train two models parameters two versions 1 change , difference loading model version huge. attached model parameters summary output. model summary.txt https github.com fchollet keras files 533414 model summary.txt",0,"model, different outputs keras versions 1.0.5 1.1.0","model, different outputs keras versions 1.0.5 1.1.0 trying load earlier trained model reproduce outputs, observed different outputs input model loaded different keras versions namely 1.0.5 1.1.0 . model trained keras 1.0.5. tried also verify dummy input data fed model loaded different versions. checked loaded weights match two versions. difference layers defined v 1.0.5 v 1.1.0? noticed least default parameters batch normalization layer slightly different. worth, get similar accuracy train two models parameters two versions 1 change , difference loading model version huge. attached model parameters summary output. model summary.txt https github.com fchollet keras files 533414 model summary.txt"
keras,4412,"hi, trying make model http tinyclouds.org colorize residual encoder.png . relevant code here, want train model bunch images directory input model rgb2gray image op close rgb2uv image . that? also, defined models correctly?",0,use images x,"use images x hi, trying make model http tinyclouds.org colorize residual encoder.png . relevant code here, want train model bunch images directory input model rgb2gray image op close rgb2uv image . that? also, defined models correctly?"
keras,4186,"rnn work batches sequences different length. case return sequences true, average losses batches taking account batch size also different sequence length averaging weights. batch longer sequences stronger effect loss batch batch size shorter sequences. related issue also take account effective length changes due masking.",0,loss averaging batches take account sequence lengths masking return sequences true,"loss averaging batches take account sequence lengths masking return sequences true rnn work batches sequences different length. case return sequences true, average losses batches taking account batch size also different sequence length averaging weights. batch longer sequences stronger effect loss batch batch size shorter sequences. related issue also take account effective length changes due masking."
keras,2257,"please make sure boxes checked submit issue. thank you! check date master branch keras. update pip install git git github.com fchollet keras.git upgrade deps running theano, check date master branch theano. update pip install git git github.com theano theano.git upgrade deps provide link github gist python script reproduce issue copy script short .",0,anyone explain find derivative internal states include keras code ?,"anyone explain find derivative internal states include keras code ? please make sure boxes checked submit issue. thank you! check date master branch keras. update pip install git git github.com fchollet keras.git upgrade deps running theano, check date master branch theano. update pip install git git github.com theano theano.git upgrade deps provide link github gist python script reproduce issue copy script short ."
keras,9042,writing custom loss function find function perform element wise multiplication. suggestion? thanks,0,multiply 2 matrices size element wise using keras backend?,multiply 2 matrices size element wise using keras backend? writing custom loss function find function perform element wise multiplication. suggestion? thanks
keras,11023,"thank you! check date master branch keras. update pip install git git github.com keras team keras.git upgrade deps x running tensorflow, check date latest version. installation instructions found https www.tensorflow.org get started os setup . running theano, check date master branch theano. update pip install git git github.com theano theano.git upgrade deps x provide link github gist python script reproduce issue copy script short . using google colab train cnn save entire model file. code available cnn colab https gist.github.com abhisheksoni27 184c49ca703eb124e1b17eb8dd8af518 model gets saved later try load back, get following error entire output log cnn colab error https gist.github.com abhisheksoni27 732bec240629d2dd721e80130cb2956b",0,cannot load model,"cannot load model thank you! check date master branch keras. update pip install git git github.com keras team keras.git upgrade deps x running tensorflow, check date latest version. installation instructions found https www.tensorflow.org get started os setup . running theano, check date master branch theano. update pip install git git github.com theano theano.git upgrade deps x provide link github gist python script reproduce issue copy script short . using google colab train cnn save entire model file. code available cnn colab https gist.github.com abhisheksoni27 184c49ca703eb124e1b17eb8dd8af518 model gets saved later try load back, get following error entire output log cnn colab error https gist.github.com abhisheksoni27 732bec240629d2dd721e80130cb2956b"
keras,10699,using run 404 error pypi arm7 wheel pyyaml. tried install keras git repo using try import library dependency issues somebody suggest fix install keras tensorflow backend?,0,cannot install keras raspberry pi 3 python 3.5.3,cannot install keras raspberry pi 3 python 3.5.3 using run 404 error pypi arm7 wheel pyyaml. tried install keras git repo using try import library dependency issues somebody suggest fix install keras tensorflow backend?
keras,3048,"hi, trying solve classification problem using time series sequential data using cnn. created features using simple mathematical transformation data. following things tried. 1. 1d conv net features multiple row 2. 2d conv net single channel features rows filter nb rows 1 3. 2d conv net channel number features filter nb rows 1 want know would correct approach. understanding follows 1. 1d conv net multiple rows net treats row different information. 2. 2d 1 channel multiple rows filter nb rows 1 behave 1d conv net multiple rows 3. 2d conv net features different channels help learn new abstractions. appreciate comments approach fits best theoritically. please make sure boxes checked submit issue. thank you! x check date master branch keras. update pip install git git github.com fchollet keras.git upgrade deps x running theano, check date master branch theano. update pip install git git github.com theano theano.git upgrade deps x provide link github gist python script reproduce issue copy script short .",0,classification using 1d 2d conv net time series data multiple features,"classification using 1d 2d conv net time series data multiple features hi, trying solve classification problem using time series sequential data using cnn. created features using simple mathematical transformation data. following things tried. 1. 1d conv net features multiple row 2. 2d conv net single channel features rows filter nb rows 1 3. 2d conv net channel number features filter nb rows 1 want know would correct approach. understanding follows 1. 1d conv net multiple rows net treats row different information. 2. 2d 1 channel multiple rows filter nb rows 1 behave 1d conv net multiple rows 3. 2d conv net features different channels help learn new abstractions. appreciate comments approach fits best theoritically. please make sure boxes checked submit issue. thank you! x check date master branch keras. update pip install git git github.com fchollet keras.git upgrade deps x running theano, check date master branch theano. update pip install git git github.com theano theano.git upgrade deps x provide link github gist python script reproduce issue copy script short ."
keras,7206,"hi, using print 'loaded word vectors.' len embeddings index get following error line f",0,unicodedecodeerror glove,"unicodedecodeerror glove hi, using print 'loaded word vectors.' len embeddings index get following error line f"
keras,12041,"hello guys, would like know cannot use 'categorical crossentropy'. 12 class training validation folder. structure class like database validation p1 database validation database validation p12 make model able compile 'categorical crossentropy'? thanks actual code terminal output categorical crossentropycategorical crossentropycategorical crossentropysparse categorical crossentropy",0,cannot use categorical crossentropy,"cannot use categorical crossentropy hello guys, would like know cannot use 'categorical crossentropy'. 12 class training validation folder. structure class like database validation p1 database validation database validation p12 make model able compile 'categorical crossentropy'? thanks actual code terminal output categorical crossentropycategorical crossentropycategorical crossentropysparse categorical crossentropy"
keras,7352,"found one small bug function 'add ngram' file examples imdb fasttext.py. adding n grams, grams size 1, 2,..., n 1 added according original paper. however, function cannot fully achieve goal due two lines range len new list ngram range 1 ngram value range 2, ngram range 1 . think positions two lines swapped looks like ngram value range 2, ngram range 1 range len new list ngram value 1 . please check issue make updates. thank you!",0,one problem examples imdb fasttext.py,"one problem examples imdb fasttext.py found one small bug function 'add ngram' file examples imdb fasttext.py. adding n grams, grams size 1, 2,..., n 1 added according original paper. however, function cannot fully achieve goal due two lines range len new list ngram range 1 ngram value range 2, ngram range 1 . think positions two lines swapped looks like ngram value range 2, ngram range 1 range len new list ngram value 1 . please check issue make updates. thank you!"
keras,61,"think would much clear easy batch size parameter separately batch normalization layer. directly pass outputs convolution pooling layers it. layers whole coherent. aside, anyone luck batch normalization? tried many times, actually got worse results time.",0,adding batch size explicit parameter batch normalization layer,"adding batch size explicit parameter batch normalization layer think would much clear easy batch size parameter separately batch normalization layer. directly pass outputs convolution pooling layers it. layers whole coherent. aside, anyone luck batch normalization? tried many times, actually got worse results time."
keras,2378,"work institute allowed run workstation overnight, hence split training process multiple days. trained model 10 epochs took approximately 1 day, saved model weights using methods described keras documentation like load model next day like restarted training process initialized training validation loss got earlier day 1st epoch! started accuracy 60 last best accuracy got earlier day, doesn't. also tried call model.compile load weights, well leaving altogether, work either. please help regard. thanks advance.",0,able resume training loading model weights,"able resume training loading model weights work institute allowed run workstation overnight, hence split training process multiple days. trained model 10 epochs took approximately 1 day, saved model weights using methods described keras documentation like load model next day like restarted training process initialized training validation loss got earlier day 1st epoch! started accuracy 60 last best accuracy got earlier day, doesn't. also tried call model.compile load weights, well leaving altogether, work either. please help regard. thanks advance."
keras,2356,"using batch generator fit generator raises error final epoch running tensorflow backend gpu linux reading images disk next generator rather simply taking entries pre populated numpy array. gist issue log https gist.github.com aachkar miovision 89daac9ce598dbcb5a698612a3a42684 use case, possible load images single numpy array cifar10 cnn fit generator example, resort loading images batches within imagedatagenerator. use cv2 perform different transforms involving color conversion, etc., load images batch using cv2.imread . small images, see issue, images order 480x720 pixels see images size related time takes read image? . possible fix would also check attributeerror https github.com fchollet keras blob master keras engine training.py l399 reason line checks valueerror? please make sure boxes checked submit issue. thank you! yes check date master branch keras. update pip install git git github.com fchollet keras.git upgrade deps using tf running theano, check date master branch theano. update pip install git git github.com theano theano.git upgrade deps done provide link github gist python script reproduce issue copy script short .",0,error final epoch using fit generator,"error final epoch using fit generator using batch generator fit generator raises error final epoch running tensorflow backend gpu linux reading images disk next generator rather simply taking entries pre populated numpy array. gist issue log https gist.github.com aachkar miovision 89daac9ce598dbcb5a698612a3a42684 use case, possible load images single numpy array cifar10 cnn fit generator example, resort loading images batches within imagedatagenerator. use cv2 perform different transforms involving color conversion, etc., load images batch using cv2.imread . small images, see issue, images order 480x720 pixels see images size related time takes read image? . possible fix would also check attributeerror https github.com fchollet keras blob master keras engine training.py l399 reason line checks valueerror? please make sure boxes checked submit issue. thank you! yes check date master branch keras. update pip install git git github.com fchollet keras.git upgrade deps using tf running theano, check date master branch theano. update pip install git git github.com theano theano.git upgrade deps done provide link github gist python script reproduce issue copy script short ."
keras,6279,"activations e.g. , typically something like model.add leakyrelu however, there're numbers people struggled work raise warning. e.g. https github.com fchollet keras issues 3816 think maybe add warnings?",0,request add warnings leakyrelu passed activation,"request add warnings leakyrelu passed activation activations e.g. , typically something like model.add leakyrelu however, there're numbers people struggled work raise warning. e.g. https github.com fchollet keras issues 3816 think maybe add warnings?"
keras,5343,"would like experiment meta rl, central idea involves adding previous prediction quality input vector timestep. means writing train loop middle something like this. two problems that. 1. runs prediction step twice, internally training step, give prediction 2. gives scalar loss, vector representing individual loss prediction. basically, would like single function everything one go.",0,meta rl,"meta rl would like experiment meta rl, central idea involves adding previous prediction quality input vector timestep. means writing train loop middle something like this. two problems that. 1. runs prediction step twice, internally training step, give prediction 2. gives scalar loss, vector representing individual loss prediction. basically, would like single function everything one go."
keras,11415,"nameerror traceback recent call last 1 set transfer learning pre trained imagenet inception v3 model remove fully connected layer replace 2 softmax classifying 10 classes 3 incepv3 model inceptionv3 weights 'imagenet', include top false, input shape 299,299,3 4 x incepv3 model.output 5 x flatten name 'flatten' x anaconda3 envs tensorflow venv lib python3.6 site packages keras applications inception v3.py inceptionv3 include top, weights, input tensor, input shape, pooling, classes 362 elif pooling 'max' 363 x layers.globalmaxpooling2d x 364 x flatten name 'flatten' x 365 ensure model takes account 366 potential predecessors . nameerror name 'flatten' defined",0,"name error name 'flatten' defined , try fine tune pretrained model inceptionv3","name error name 'flatten' defined , try fine tune pretrained model inceptionv3 nameerror traceback recent call last 1 set transfer learning pre trained imagenet inception v3 model remove fully connected layer replace 2 softmax classifying 10 classes 3 incepv3 model inceptionv3 weights 'imagenet', include top false, input shape 299,299,3 4 x incepv3 model.output 5 x flatten name 'flatten' x anaconda3 envs tensorflow venv lib python3.6 site packages keras applications inception v3.py inceptionv3 include top, weights, input tensor, input shape, pooling, classes 362 elif pooling 'max' 363 x layers.globalmaxpooling2d x 364 x flatten name 'flatten' x 365 ensure model takes account 366 potential predecessors . nameerror name 'flatten' defined"
keras,6619,"hi, possible specify initial state stateful recurrent layer? example, following code, work? idea need initial flag turned beginning sequence turned off. sure logic correct keras implementation. thanks! farizrahman4u",0,specifying initial state stateful recurrent layers,"specifying initial state stateful recurrent layers hi, possible specify initial state stateful recurrent layer? example, following code, work? idea need initial flag turned beginning sequence turned off. sure logic correct keras implementation. thanks! farizrahman4u"
keras,11000,"trying implement lstm model process 100 texts concatenate together feed dense layer. however comes error, code shows anyone gives help, thanks!",0,keras error input dense layer reshaped layer,"keras error input dense layer reshaped layer trying implement lstm model process 100 texts concatenate together feed dense layer. however comes error, code shows anyone gives help, thanks!"
keras,6011,"following function def transpose dot vects x, vects 2 return k.dot x, k.transpose try evaluate works x k.variable np.array np x k.variable np.array np x obj transpose dot objective output obj x, print ' ' print k.eval objective output result 1. 1. 1. 2. 1. 2. 2. 4. 1. 2. 2. 4. 2. 4. 4. 8. , try use function layer work. np x 1, 0 , 1, 1 , 1, 1 , 2, 2 features np.array np x test input input shape np.array np x .shape dot layer lambda transpose dot, output shape 4,4 test input, test input x model inputs test input, outputs dot layer x.predict features, batch size 1 result self.fn output subset none else valueerror shape mismatch x 2 cols 4 rows 4 rows 2 cols apply node caused error dot22 reshape 2 .0, reshape 2 .0 toposort index 11 inputs types tensortype float32, matrix , tensortype float32, matrix inputs shapes 4, 2 , 4, 2 inputs strides 8, 4 , 8, 4 inputs values 'not shown', 'not shown' outputs clients reshape 4 dot22.0, makevector dtype 'int64' .0 idea missing here? p.s stackoverflow link provide well http stackoverflow.com questions 43049984 keras lambda function dot product mistmatch",0,lambda function transpose operation,"lambda function transpose operation following function def transpose dot vects x, vects 2 return k.dot x, k.transpose try evaluate works x k.variable np.array np x k.variable np.array np x obj transpose dot objective output obj x, print ' ' print k.eval objective output result 1. 1. 1. 2. 1. 2. 2. 4. 1. 2. 2. 4. 2. 4. 4. 8. , try use function layer work. np x 1, 0 , 1, 1 , 1, 1 , 2, 2 features np.array np x test input input shape np.array np x .shape dot layer lambda transpose dot, output shape 4,4 test input, test input x model inputs test input, outputs dot layer x.predict features, batch size 1 result self.fn output subset none else valueerror shape mismatch x 2 cols 4 rows 4 rows 2 cols apply node caused error dot22 reshape 2 .0, reshape 2 .0 toposort index 11 inputs types tensortype float32, matrix , tensortype float32, matrix inputs shapes 4, 2 , 4, 2 inputs strides 8, 4 , 8, 4 inputs values 'not shown', 'not shown' outputs clients reshape 4 dot22.0, makevector dtype 'int64' .0 idea missing here? p.s stackoverflow link provide well http stackoverflow.com questions 43049984 keras lambda function dot product mistmatch"
keras,2091,"new keras trouble shapes, specially comes rnns lstms. running code variable predictor train numpy array 119 inner arrays, one 80 different items. error far found rnn receives 3d tensor shape batch size, timesteps, dimension set input shape batch size usually omitted, provide tuple timesteps, dimension . exactly write code??",0,"simplernn wrong number dimensions, expected 3, got 2 shape 119,80","simplernn wrong number dimensions, expected 3, got 2 shape 119,80 new keras trouble shapes, specially comes rnns lstms. running code variable predictor train numpy array 119 inner arrays, one 80 different items. error far found rnn receives 3d tensor shape batch size, timesteps, dimension set input shape batch size usually omitted, provide tuple timesteps, dimension . exactly write code??"
keras,6761,would nice standard way reference library papers. tensorflow https www.tensorflow.org versions r0.11 resources bib best regards,0,standard bibtex entry reference academic documents,standard bibtex entry reference academic documents would nice standard way reference library papers. tensorflow https www.tensorflow.org versions r0.11 resources bib best regards
keras,6938,"want run different model time. however, cannot get correct answer running time. following code. generate mnist like data, build 2 cnn network training testing. create individual session graph object. sometime got correct answer following however, got error sometimes... wrong implementation? thing notice multi thread work?",0,issue implement multiple cnn models individual thread keras,"issue implement multiple cnn models individual thread keras want run different model time. however, cannot get correct answer running time. following code. generate mnist like data, build 2 cnn network training testing. create individual session graph object. sometime got correct answer following however, got error sometimes... wrong implementation? thing notice multi thread work?"
keras,9232,"facing issue project currently working on. attempting build many many model takes series images classifies them. part relatively straight forward. model built using keras uses convolutional layers inside time distributed wrapper feed lstm works fine. complexity current project comes fact model needs converted coreml deployment. feel wall help provided would life saver. like said previously, current model trainable using time distributed wrapper, seem supported coreml. seen examples using lstm coreml lstm states passed model item sequence. essentially creates recurrent network takes single item sequence well previous predictions lstm states input, rather whole sequence once. lstm state loop lack better term seems best option coreml support sequential image inputs. issue comes training. train network properly sequential data, convert coreml? remove time distribution non lstm layers, model compile missing extra time dimension. essentially, catch can't remove time distribution wrappers model functional without inclusion time steps, can't convert coreml present. anyone ideas this? hope question understandable. quite late working 20 hours straight bit fried moment. thanks advance input, thoughts, ideas provided. cheers! model image input input shape max sequence length, 224, 224, 3 convolutional 1 timedistributed conv2d 64, 3, 3 , activation 'relu', data format 'channels last' image input pooling 1 timedistributed maxpooling2d 2, 2 , strides 1, 1 convolutional 1 convolutional 2 timedistributed conv2d 128, 4,4 , activation 'relu' pooling 1 pooling 2 timedistributed maxpooling2d 2, 2 , strides 2, 2 convolutional 2 convolutional 3 timedistributed conv2d 256, 4,4 , activation 'relu' pooling 2 pooling 3 timedistributed maxpooling2d 2, 2 , strides 2, 2 convolutional 3 flatten 1 timedistributed flatten pooling 3 dropout 1 timedistributed dropout 0.5 flatten 1 lstm 1 lstm 256, return sequences true, return state false, stateful false, dropout 0.5 dropout 1 dense 1 timedistributed dense num classes, activation 'sigmoid' lstm 1 model model inputs image input, outputs dense 1 wanted add seen posts seems people using time distribution wrappers coreml, however try convert model raises error soon hits first wrapper attributeerror layer never called thus defined output shape. modified conversion script keras coreml handle 4d input although able test see works expected can't convert model image sequence, get convert time distribution layers place, would functional. link apple article discussing rnn's coreml https developer.apple.com documentation coreml core ml api making predictions sequence inputs link github repo implementation lstm rnn https github.com akimach gestureai coreml ios",0,use lstm recurrent convolutional network without time distributed wrapper,"use lstm recurrent convolutional network without time distributed wrapper facing issue project currently working on. attempting build many many model takes series images classifies them. part relatively straight forward. model built using keras uses convolutional layers inside time distributed wrapper feed lstm works fine. complexity current project comes fact model needs converted coreml deployment. feel wall help provided would life saver. like said previously, current model trainable using time distributed wrapper, seem supported coreml. seen examples using lstm coreml lstm states passed model item sequence. essentially creates recurrent network takes single item sequence well previous predictions lstm states input, rather whole sequence once. lstm state loop lack better term seems best option coreml support sequential image inputs. issue comes training. train network properly sequential data, convert coreml? remove time distribution non lstm layers, model compile missing extra time dimension. essentially, catch can't remove time distribution wrappers model functional without inclusion time steps, can't convert coreml present. anyone ideas this? hope question understandable. quite late working 20 hours straight bit fried moment. thanks advance input, thoughts, ideas provided. cheers! model image input input shape max sequence length, 224, 224, 3 convolutional 1 timedistributed conv2d 64, 3, 3 , activation 'relu', data format 'channels last' image input pooling 1 timedistributed maxpooling2d 2, 2 , strides 1, 1 convolutional 1 convolutional 2 timedistributed conv2d 128, 4,4 , activation 'relu' pooling 1 pooling 2 timedistributed maxpooling2d 2, 2 , strides 2, 2 convolutional 2 convolutional 3 timedistributed conv2d 256, 4,4 , activation 'relu' pooling 2 pooling 3 timedistributed maxpooling2d 2, 2 , strides 2, 2 convolutional 3 flatten 1 timedistributed flatten pooling 3 dropout 1 timedistributed dropout 0.5 flatten 1 lstm 1 lstm 256, return sequences true, return state false, stateful false, dropout 0.5 dropout 1 dense 1 timedistributed dense num classes, activation 'sigmoid' lstm 1 model model inputs image input, outputs dense 1 wanted add seen posts seems people using time distribution wrappers coreml, however try convert model raises error soon hits first wrapper attributeerror layer never called thus defined output shape. modified conversion script keras coreml handle 4d input although able test see works expected can't convert model image sequence, get convert time distribution layers place, would functional. link apple article discussing rnn's coreml https developer.apple.com documentation coreml core ml api making predictions sequence inputs link github repo implementation lstm rnn https github.com akimach gestureai coreml ios"
keras,9394,"hi, upgrade keras latest version getting error related keras engine training code used work fine beforehand previous version 2.1.0 . sure kind deprectation etc. happening brach dropout bb alpha detecting adversarial examples https github.com yingzhenli dropout bbalpha . unfortunately, coudn't share version due disclusure project. thanks understanding. found add stateful global metrics 9200 https github.com keras team keras pull 9200 files 1ba271554f3cebf6268d382090e7097f075e5794 , track changes. idea solve issue? thanks, shek checked check date master branch keras. update pip install git git github.com keras team keras.git upgrade deps checked running tensorflow, check date latest version. installation instructions found https www.tensorflow.org get started os setup .",0,attributeerror 'model' object attribute 'stateful metric names',"attributeerror 'model' object attribute 'stateful metric names' hi, upgrade keras latest version getting error related keras engine training code used work fine beforehand previous version 2.1.0 . sure kind deprectation etc. happening brach dropout bb alpha detecting adversarial examples https github.com yingzhenli dropout bbalpha . unfortunately, coudn't share version due disclusure project. thanks understanding. found add stateful global metrics 9200 https github.com keras team keras pull 9200 files 1ba271554f3cebf6268d382090e7097f075e5794 , track changes. idea solve issue? thanks, shek checked check date master branch keras. update pip install git git github.com keras team keras.git upgrade deps checked running tensorflow, check date latest version. installation instructions found https www.tensorflow.org get started os setup ."
keras,3538,"generating different initial random state neural network based suggestions forum import numpy np import datetime np.random.seed datetime.datetime.now .microsecond keras import .... setting numpy random seed importing anything keras . now, assume want train evaluate neural network different initial random states, 10 times, take average f scores. run file externally 10 times e.i., python myprog.py make work. assume want inside python range 10 import numpy np import datetime np.random.seed datetime.datetime.now .microsecond keras import .... train evaluate guess it, keras already imported. del np, del keras , inside function works? reload np , reload keras works ? correct way this? thanks advance.",0,reset keras random state many times inside python?,"reset keras random state many times inside python? generating different initial random state neural network based suggestions forum import numpy np import datetime np.random.seed datetime.datetime.now .microsecond keras import .... setting numpy random seed importing anything keras . now, assume want train evaluate neural network different initial random states, 10 times, take average f scores. run file externally 10 times e.i., python myprog.py make work. assume want inside python range 10 import numpy np import datetime np.random.seed datetime.datetime.now .microsecond keras import .... train evaluate guess it, keras already imported. del np, del keras , inside function works? reload np , reload keras works ? correct way this? thanks advance."
keras,6852,consider following example seems cause confusing bugs downstream concatenateconcatenate however examine size seems fine,0,invalid output shape model,invalid output shape model consider following example seems cause confusing bugs downstream concatenateconcatenate however examine size seems fine
keras,13194,"please make sure boxes checked submit issue. issue implementation question, please ask question stackoverflow http stackoverflow.com questions tagged keras join keras slack channel https keras slack autojoin.herokuapp.com ask instead filing github issue. thank you! check date master branch keras. update pip install git git github.com keras team keras.git upgrade deps running tensorflow, check date latest version. installation instructions found https www.tensorflow.org get started os setup . running theano, check date master branch theano. update pip install git git github.com theano theano.git upgrade deps provide link github gist python script reproduce issue copy script short .",0,"hi,","hi, please make sure boxes checked submit issue. issue implementation question, please ask question stackoverflow http stackoverflow.com questions tagged keras join keras slack channel https keras slack autojoin.herokuapp.com ask instead filing github issue. thank you! check date master branch keras. update pip install git git github.com keras team keras.git upgrade deps running tensorflow, check date latest version. installation instructions found https www.tensorflow.org get started os setup . running theano, check date master branch theano. update pip install git git github.com theano theano.git upgrade deps provide link github gist python script reproduce issue copy script short ."
keras,4738,"error seems layer shape shown example 'vgg16 weights.h5' downloaded. error message traceback recent call last model.layers k .set weights weights file usr local lib python3.4 dist packages keras engine topology.py , line 889, set weights 'provided weight shape ' str w.shape exception layer weight shape 3, 3, 128, 64 compatible provided weight shape 64, 3, 3, 3 system configuration running vgg16 example ubuntu 14.04, python 3.4 using tensorflow backend",0,vgg16 example layer weight shape compatible provided weight shape,"vgg16 example layer weight shape compatible provided weight shape error seems layer shape shown example 'vgg16 weights.h5' downloaded. error message traceback recent call last model.layers k .set weights weights file usr local lib python3.4 dist packages keras engine topology.py , line 889, set weights 'provided weight shape ' str w.shape exception layer weight shape 3, 3, 128, 64 compatible provided weight shape 64, 3, 3, 3 system configuration running vgg16 example ubuntu 14.04, python 3.4 using tensorflow backend"
keras,1153,"a744b60 highly useful function disappeared. reason, chance bringing back?",0,print layer shapes gone,"print layer shapes gone a744b60 highly useful function disappeared. reason, chance bringing back?"
keras,3309,"please make sure boxes checked submit issue. thank you! check date master branch keras. update pip install git git github.com fchollet keras.git upgrade deps running theano, check date master branch theano. update pip install git git github.com theano theano.git upgrade deps provide link github gist python script reproduce issue copy script short .",0,"hi,","hi, please make sure boxes checked submit issue. thank you! check date master branch keras. update pip install git git github.com fchollet keras.git upgrade deps running theano, check date master branch theano. update pip install git git github.com theano theano.git upgrade deps provide link github gist python script reproduce issue copy script short ."
keras,5989,"description used transfer learning resnet50, basicly retrain different top network it. worked fine. keras 2 seems takes forever. using code runs gpu ran verify gpu actually used checked backend. getting huge number like 1151049s per epoch. means around 13 days! update got around 4000s per epoch ideas cause that? thanks! details 1 ubuntu 16 lts 2 using fit generator 3 tesla k80 4 tested keras 2.0.1 2.0.2 5 theano update branch master 6 tensorflow 1.0.1 7 running python mnist cnn.py takes aroudn 8s per epoch backend done check date master branch keras. update pip install git git github.com fchollet keras.git upgrade deps done running tensorflow, check date latest version. installation instructions found https www.tensorflow.org get started os setup . done running theano, check date master branch theano. update pip install git git github.com theano theano.git upgrade deps provide link github gist python script reproduce issue copy script short .",0,keras 2. resnet training speed,"keras 2. resnet training speed description used transfer learning resnet50, basicly retrain different top network it. worked fine. keras 2 seems takes forever. using code runs gpu ran verify gpu actually used checked backend. getting huge number like 1151049s per epoch. means around 13 days! update got around 4000s per epoch ideas cause that? thanks! details 1 ubuntu 16 lts 2 using fit generator 3 tesla k80 4 tested keras 2.0.1 2.0.2 5 theano update branch master 6 tensorflow 1.0.1 7 running python mnist cnn.py takes aroudn 8s per epoch backend done check date master branch keras. update pip install git git github.com fchollet keras.git upgrade deps done running tensorflow, check date latest version. installation instructions found https www.tensorflow.org get started os setup . done running theano, check date master branch theano. update pip install git git github.com theano theano.git upgrade deps provide link github gist python script reproduce issue copy script short ."
keras,2845,"hi people. new neural networks keras. working ipython notebook windows 8.1 cpu. getting error trying train network assertionerror abstractconv2d theano optimization failed implementation available supporting requested options. exclude conv dnn conv gemm optimizer? gpu, cudnn available gpu support it? cpu, blas library installed theano link against? looked similar issues theano keras github issues page. almost solutions pointed towards updating theano keras. tried updating them, resolve issue. issue give images training function model.fit ? format 2d grey scale images given training set? could someone please help fixing this. please find link code error https github.com vivek b kaggle blob master digit 20recognition conv 2d 20 20keras.ipynb",0,error abstractconv2d theano optimization failed,"error abstractconv2d theano optimization failed hi people. new neural networks keras. working ipython notebook windows 8.1 cpu. getting error trying train network assertionerror abstractconv2d theano optimization failed implementation available supporting requested options. exclude conv dnn conv gemm optimizer? gpu, cudnn available gpu support it? cpu, blas library installed theano link against? looked similar issues theano keras github issues page. almost solutions pointed towards updating theano keras. tried updating them, resolve issue. issue give images training function model.fit ? format 2d grey scale images given training set? could someone please help fixing this. please find link code error https github.com vivek b kaggle blob master digit 20recognition conv 2d 20 20keras.ipynb"
keras,1741,"hi, guys merge layer exist 'similarity matrx' merge mode? implement simiarity matrix layer, suspect effectiveness? implementation right? def init self, dim1, dim2, layers, init 'uniform', w regularizer none, activity regularizer none, w constraint none, kwargs assert len layers 2 self.layers layers self.dim1 dim1 self.dim2 dim2 self.init initializations.get init self.w constraint constraints.get w constraint self.w regularizer regularizers.get w regularizer self.activity regularizer regularizers.get activity regularizer self.params self.regularizers self.constraints self.updates l self.layers params, regs, consts, updates l.get params self.regularizers regs self.updates updates params constraints size p, c zip params, consts p self.params self.params.append p self.constraints.append c self.w self.init self.dim1, self.dim2 self.params.append self.w self.w regularizer self.w regularizer.set param self.w self.regularizers.append self.w regularizer self.activity regularizer self.activity regularizer.set layer self self.regularizers.append self.activity regularizer super similaritymatrix, self . init kwargs property def output shape self return self.layers 0 .output shape 0 , 1 def get params self return self.params, self.regularizers, self.constraints, self.updates def get input self, train false res range len self.layers self.layers .get input train type list output output res res.append output return res def get output self, train s1 self.layers 0 .get output train s2 self.layers 1 .get output train sim t.sum t.dot s1, self.w s2, axis 1 return sim.dimshuffle 0,'x' property def input self return self.get input def supports masked input self return false def get output mask self, train none return none def get weights self weights l self.layers weights l.get weights return weights def set weights self, weights range len self.layers nb param len self.layers .params self.layers .set weights weights nb param weights weights nb param",0,similairty matrix layer implementation right?,"similairty matrix layer implementation right? hi, guys merge layer exist 'similarity matrx' merge mode? implement simiarity matrix layer, suspect effectiveness? implementation right? def init self, dim1, dim2, layers, init 'uniform', w regularizer none, activity regularizer none, w constraint none, kwargs assert len layers 2 self.layers layers self.dim1 dim1 self.dim2 dim2 self.init initializations.get init self.w constraint constraints.get w constraint self.w regularizer regularizers.get w regularizer self.activity regularizer regularizers.get activity regularizer self.params self.regularizers self.constraints self.updates l self.layers params, regs, consts, updates l.get params self.regularizers regs self.updates updates params constraints size p, c zip params, consts p self.params self.params.append p self.constraints.append c self.w self.init self.dim1, self.dim2 self.params.append self.w self.w regularizer self.w regularizer.set param self.w self.regularizers.append self.w regularizer self.activity regularizer self.activity regularizer.set layer self self.regularizers.append self.activity regularizer super similaritymatrix, self . init kwargs property def output shape self return self.layers 0 .output shape 0 , 1 def get params self return self.params, self.regularizers, self.constraints, self.updates def get input self, train false res range len self.layers self.layers .get input train type list output output res res.append output return res def get output self, train s1 self.layers 0 .get output train s2 self.layers 1 .get output train sim t.sum t.dot s1, self.w s2, axis 1 return sim.dimshuffle 0,'x' property def input self return self.get input def supports masked input self return false def get output mask self, train none return none def get weights self weights l self.layers weights l.get weights return weights def set weights self, weights range len self.layers nb param len self.layers .params self.layers .set weights weights nb param weights weights nb param"
keras,12652,python 3.7 tensorflow 2.0 create model. k.clear session . create model. layer names reset. create example make sense immediately.,0,k.clear session reset layer naming conventions,k.clear session reset layer naming conventions python 3.7 tensorflow 2.0 create model. k.clear session . create model. layer names reset. create example make sense immediately.
keras,895,"sure coming up. included snippet code error message. seems stemming numpy, really confused why. run it, often dip way memory error comes up. feel like missing something obvious. thanks advance! error message snippet code",0,weird memory error,"weird memory error sure coming up. included snippet code error message. seems stemming numpy, really confused why. run it, often dip way memory error comes up. feel like missing something obvious. thanks advance! error message snippet code"
keras,11885,"please make sure boxes checked submit issue. issue implementation question , please ask question stackoverflow http stackoverflow.com questions tagged keras keras slack channel https keras slack autojoin.herokuapp.com instead opening github issue. thank you! x check date master branch keras. update x check version tensorflow date. installation instructions found https www.tensorflow.org get started os setup . x provide link github gist python script reproduce issue copy script short .",0,remove delete specific weights neural network layer dense layer convolution layer keras tensorflow ?,"remove delete specific weights neural network layer dense layer convolution layer keras tensorflow ? please make sure boxes checked submit issue. issue implementation question , please ask question stackoverflow http stackoverflow.com questions tagged keras keras slack channel https keras slack autojoin.herokuapp.com instead opening github issue. thank you! x check date master branch keras. update x check version tensorflow date. installation instructions found https www.tensorflow.org get started os setup . x provide link github gist python script reproduce issue copy script short ."
keras,699,"home page documentation, claim ...building neural turing machine fast . sound implausible, think anyone successfully reproduced results ntm paper anyone know referring specific ntm implementation, so, well implementation worked? http keras.io",0,ntm actually built keras?,"ntm actually built keras? home page documentation, claim ...building neural turing machine fast . sound implausible, think anyone successfully reproduced results ntm paper anyone know referring specific ntm implementation, so, well implementation worked? http keras.io"
keras,5990,"new keras maybe know better yet things noticed. example changes return type scalar list depending whether network one many outputs. wanted create something generic handles cases, forced perform type checking operation end. wanted ask whether possible change thing like simply always return list. something similar, way worse, happens one changes model inputs single, multi channel. noticed change single input network might hand matrix whereas changes input list contain data channel etc. imho would easier simply always demand list way run issue write batch generation logic. complaint. love keras think awesome please forgive wrong suggest would great keep things mind. since .. noticed lot questions getting asked github. lot questions would also fit stackoverflow. sure better enforce point close new issues ask people move general questions stackoverflow? affiliated simply much easier get overview existing questions there. guys think?",0,keras api,"keras api new keras maybe know better yet things noticed. example changes return type scalar list depending whether network one many outputs. wanted create something generic handles cases, forced perform type checking operation end. wanted ask whether possible change thing like simply always return list. something similar, way worse, happens one changes model inputs single, multi channel. noticed change single input network might hand matrix whereas changes input list contain data channel etc. imho would easier simply always demand list way run issue write batch generation logic. complaint. love keras think awesome please forgive wrong suggest would great keep things mind. since .. noticed lot questions getting asked github. lot questions would also fit stackoverflow. sure better enforce point close new issues ask people move general questions stackoverflow? affiliated simply much easier get overview existing questions there. guys think?"
keras,3752,"network model sequential model.add convolution2d 128, 3, 3, border mode 'valid',input shape data.shape 3 model.add activation act model.add convolution2d 128, 3, 3, border mode 'valid' model.add activation act model.add maxpooling2d pool size 2, 2 model.add dropout 0.25 model.add convolution2d 256, 3, 3, border mode 'valid' model.add activation act model.add convolution2d 256, 3, 3, border mode 'valid' model.add activation act model.add maxpooling2d pool size 2, 2 model.add dropout 0.25 model.add convolution2d 512, 3, 3, border mode 'valid' model.add activation act model.add maxpooling2d pool size 2, 2 model.add flatten model.add dense 4096, init 'normal' model.add activation act model.add dropout 0.5 model.add dense 4096, init 'normal' model.add activation act model.add dropout 0.5 model.add dense classnumber, init 'normal' model.add activation 'softmax' first set act 'relu' , loss accuracy does't change. changed act 'tanh' traing again, loss acc normal known issue",0,bug 'relu'?,"bug 'relu'? network model sequential model.add convolution2d 128, 3, 3, border mode 'valid',input shape data.shape 3 model.add activation act model.add convolution2d 128, 3, 3, border mode 'valid' model.add activation act model.add maxpooling2d pool size 2, 2 model.add dropout 0.25 model.add convolution2d 256, 3, 3, border mode 'valid' model.add activation act model.add convolution2d 256, 3, 3, border mode 'valid' model.add activation act model.add maxpooling2d pool size 2, 2 model.add dropout 0.25 model.add convolution2d 512, 3, 3, border mode 'valid' model.add activation act model.add maxpooling2d pool size 2, 2 model.add flatten model.add dense 4096, init 'normal' model.add activation act model.add dropout 0.5 model.add dense 4096, init 'normal' model.add activation act model.add dropout 0.5 model.add dense classnumber, init 'normal' model.add activation 'softmax' first set act 'relu' , loss accuracy does't change. changed act 'tanh' traing again, loss acc normal known issue"
keras,4605,"hey! guys, recently working project training lstm variable length sequences, length sequences 4 12, using one hot representation, following example https github.com erikbern rnn lang model blob master train lstm.py url , problem train network variable length sequences, first , tried use making layer, know code correct pasted major part code training data validation data mylist test list sequences, total vocabulary letters sequences, get one hot representation training data validation data network training second , tried train network one pair data per time, method remove masking layer, test use masking layer correct third , tried use 'pad neutral data ' method following https github.com fchollet keras issues 40 url , paded sequences end symbol tried group method yet, performance try evaluated validation loss batch size 1 method masking padding neutral data question right using masking method? since performance much lower batch size 1 method, think problem using masking layer, anyone help ? thanks advance!",0,training keras lstm model variable length sequence mask pading batch size 1 group ?,"training keras lstm model variable length sequence mask pading batch size 1 group ? hey! guys, recently working project training lstm variable length sequences, length sequences 4 12, using one hot representation, following example https github.com erikbern rnn lang model blob master train lstm.py url , problem train network variable length sequences, first , tried use making layer, know code correct pasted major part code training data validation data mylist test list sequences, total vocabulary letters sequences, get one hot representation training data validation data network training second , tried train network one pair data per time, method remove masking layer, test use masking layer correct third , tried use 'pad neutral data ' method following https github.com fchollet keras issues 40 url , paded sequences end symbol tried group method yet, performance try evaluated validation loss batch size 1 method masking padding neutral data question right using masking method? since performance much lower batch size 1 method, think problem using masking layer, anyone help ? thanks advance!"
keras,12083,"need use model.fit generator method use multiprocessin true workers 1 want parallelize augmentation. keras imagedatagenerator would perfect match model.fit generator. several people found out, causes problems lacking thread safety. simple way make two work together? everyone working keras wants able write data generator scratch. would greatly appreciated would give example documentation. case, images need augmented directories. one class, thanks!",0,multiprocessing model.fit generator data augmentation,"multiprocessing model.fit generator data augmentation need use model.fit generator method use multiprocessin true workers 1 want parallelize augmentation. keras imagedatagenerator would perfect match model.fit generator. several people found out, causes problems lacking thread safety. simple way make two work together? everyone working keras wants able write data generator scratch. would greatly appreciated would give example documentation. case, images need augmented directories. one class, thanks!"
keras,4292,"please make sure boxes checked submit issue. thank you! x check date master branch keras. update pip install git git github.com fchollet keras.git upgrade deps x running theano, check date master branch theano. update pip install git git github.com theano theano.git upgrade deps x provide link github gist python script reproduce issue copy script short . hi guys, wondering implemented ssim structural similarity index used objective. often used measuring similarity two images x . formulation follow ! image https cloud.githubusercontent.com assets 810340 20015848 3a1ed87c a2a4 11e6 825b fac27edcb146.png think must easy implement using generic functions backends theano os tf , familiarized enough.",0,ssim objective,"ssim objective please make sure boxes checked submit issue. thank you! x check date master branch keras. update pip install git git github.com fchollet keras.git upgrade deps x running theano, check date master branch theano. update pip install git git github.com theano theano.git upgrade deps x provide link github gist python script reproduce issue copy script short . hi guys, wondering implemented ssim structural similarity index used objective. often used measuring similarity two images x . formulation follow ! image https cloud.githubusercontent.com assets 810340 20015848 3a1ed87c a2a4 11e6 825b fac27edcb146.png think must easy implement using generic functions backends theano os tf , familiarized enough."
keras,1686,"document faq , get output given specific input layer 0 . want know whether get outpu given intermedia layer input. example, 7 layers totally, give 3th layer's input, get output 7th layers use keras? thanks.",0,get output given intermedia layer input ?,"get output given intermedia layer input ? document faq , get output given specific input layer 0 . want know whether get outpu given intermedia layer input. example, 7 layers totally, give 3th layer's input, get output 7th layers use keras? thanks."
keras,4097,"type checking looking class names rather inheritance example also somewhat old python style . preventing use inheritance based customization patterns. example model pull request https github.com fchollet keras pull 4069 , code would working.",0,use inheritance rather class name equality,"use inheritance rather class name equality type checking looking class names rather inheritance example also somewhat old python style . preventing use inheritance based customization patterns. example model pull request https github.com fchollet keras pull 4069 , code would working."
keras,2790,"using functional api create model try save using following line, gives following error saying file trainer.py , line 48, create training features json string model.to json file usr local lib python2.7 site packages keras engine topology.py , line 2368, json config self.get config file usr local lib python2.7 site packages keras engine topology.py , line 2163, get config new node index node conversion map node key keyerror 'input 1 ib 0' gist https gist.github.com akmahaja ef406b2087b5c50befc1a479989b1921",0,error saving model,"error saving model using functional api create model try save using following line, gives following error saying file trainer.py , line 48, create training features json string model.to json file usr local lib python2.7 site packages keras engine topology.py , line 2368, json config self.get config file usr local lib python2.7 site packages keras engine topology.py , line 2163, get config new node index node conversion map node key keyerror 'input 1 ib 0' gist https gist.github.com akmahaja ef406b2087b5c50befc1a479989b1921"
keras,12741,fit generator . work python 3.6.5 keras 2.1.6 python 3.6.8 keras 2.2.4 unless related version modules.,0,fit generator working keras 2.2.4 python 3.6.8,fit generator working keras 2.2.4 python 3.6.8 fit generator . work python 3.6.5 keras 2.1.6 python 3.6.8 keras 2.2.4 unless related version modules.
keras,337,"forgot initialize self.monitor monitor. line appears like self.monitor error attributeerror 'modelcheckpoint' object attribute 'monitor' would appreciate could fix that, thanks",0,modelcheckpoint bug line 177,"modelcheckpoint bug line 177 forgot initialize self.monitor monitor. line appears like self.monitor error attributeerror 'modelcheckpoint' object attribute 'monitor' would appreciate could fix that, thanks"
keras,4178,"edit following issue minimal example produce error. actual goal use complicated model instead here. executing following script occurs simplest model produces error original architecture, tried distribute complex model . issue occurs replacing layer e.g. , , e.g. . think error boils combination layer model uses learning phase. maybe conceptual problem learning phase input? issues seem somewhat related 3834, 2609, 3686, 2391 full stack trace please make sure boxes checked submit issue. thank you! x check date master branch keras. update pip install git git github.com fchollet keras.git upgrade deps x running theano, check date master branch theano. update pip install git git github.com theano theano.git upgrade deps x provide link github gist python script reproduce issue copy script short .",0,problem timedistributed learning phase,"problem timedistributed learning phase edit following issue minimal example produce error. actual goal use complicated model instead here. executing following script occurs simplest model produces error original architecture, tried distribute complex model . issue occurs replacing layer e.g. , , e.g. . think error boils combination layer model uses learning phase. maybe conceptual problem learning phase input? issues seem somewhat related 3834, 2609, 3686, 2391 full stack trace please make sure boxes checked submit issue. thank you! x check date master branch keras. update pip install git git github.com fchollet keras.git upgrade deps x running theano, check date master branch theano. update pip install git git github.com theano theano.git upgrade deps x provide link github gist python script reproduce issue copy script short ."
keras,312,possible stack merge layers? model compiles train methods breaks,0,stacking merge layers?,stacking merge layers? possible stack merge layers? model compiles train methods breaks
keras,10518,"please make sure boxes checked submit issue. issue implementation question, please ask question stackoverflow http stackoverflow.com questions tagged keras join keras slack channel https keras slack autojoin.herokuapp.com ask instead filing github issue. thank you! check date master branch keras. update pip install git git github.com keras team keras.git upgrade deps running tensorflow, check date latest version. installation instructions found https www.tensorflow.org get started os setup . running theano, check date master branch theano. update pip install git git github.com theano theano.git upgrade deps provide link github gist python script reproduce issue copy script short .",0,"working regressor problem 50,000 rows 20 coloumns ,i want implement cnn find mse different architecture, model sequential model.add conv2d 32, 3, 3 , activation 'relu', input shape 100, 100, 3 model.add conv2d 32, 3, 3 , activation 'relu' model.add maxpooling2d pool size 2, 2 model.add dropout 0.25 model.add conv2d 64, 3, 3 , activation 'relu' model.add conv2d 64, 3, 3 , activation 'relu' model.add maxpooling2d pool size 2, 2 model.add dropout 0.25 model.add flatten model.add dense 256, activation 'relu' model.add dropout 0.5 model.add dense 10, activation 'tanh' sgd sgd lr 0.01, decay 1e 6, momentum 0.9, nesterov true model.compile loss 'mean squared error', optimizer sgd model.fit bx train, fx train, batch size 32, epochs 10 score model.evaluate bx test, fx test, batch size 32 , dimension sholud used input","working regressor problem 50,000 rows 20 coloumns ,i want implement cnn find mse different architecture, model sequential model.add conv2d 32, 3, 3 , activation 'relu', input shape 100, 100, 3 model.add conv2d 32, 3, 3 , activation 'relu' model.add maxpooling2d pool size 2, 2 model.add dropout 0.25 model.add conv2d 64, 3, 3 , activation 'relu' model.add conv2d 64, 3, 3 , activation 'relu' model.add maxpooling2d pool size 2, 2 model.add dropout 0.25 model.add flatten model.add dense 256, activation 'relu' model.add dropout 0.5 model.add dense 10, activation 'tanh' sgd sgd lr 0.01, decay 1e 6, momentum 0.9, nesterov true model.compile loss 'mean squared error', optimizer sgd model.fit bx train, fx train, batch size 32, epochs 10 score model.evaluate bx test, fx test, batch size 32 , dimension sholud used input please make sure boxes checked submit issue. issue implementation question, please ask question stackoverflow http stackoverflow.com questions tagged keras join keras slack channel https keras slack autojoin.herokuapp.com ask instead filing github issue. thank you! check date master branch keras. update pip install git git github.com keras team keras.git upgrade deps running tensorflow, check date latest version. installation instructions found https www.tensorflow.org get started os setup . running theano, check date master branch theano. update pip install git git github.com theano theano.git upgrade deps provide link github gist python script reproduce issue copy script short ."
keras,676,loading image using results error.,0,python 3 compatibility problem image loading,python 3 compatibility problem image loading loading image using results error.
keras,1584,"hi, theano.scan used things like get output graph containers such? would theano able gpu optimize functions scan used instead normal python dictionary iterations?",0,theano.scan unused keras source?,"theano.scan unused keras source? hi, theano.scan used things like get output graph containers such? would theano able gpu optimize functions scan used instead normal python dictionary iterations?"
keras,2711,"model trying train loss go down. custom image set using. images 106 x 106 px black white two 2 classes, bargraph gels. two classes different. run cifar10 dataset reduce loss, confused model always predict one class everything. xtrain numpy array images numpy arrays , ytrain numpy array arrays 0,1 1,0 shapes look like model right small training sets tried 1000 examples well, similar results . also tried rms sdg large small learning rates. else try ? x check date master branch keras. update pip install git git github.com fchollet keras.git upgrade deps x running theano, check date master branch theano. update pip install git git github.com theano theano.git upgrade deps",0,loss changing training,"loss changing training model trying train loss go down. custom image set using. images 106 x 106 px black white two 2 classes, bargraph gels. two classes different. run cifar10 dataset reduce loss, confused model always predict one class everything. xtrain numpy array images numpy arrays , ytrain numpy array arrays 0,1 1,0 shapes look like model right small training sets tried 1000 examples well, similar results . also tried rms sdg large small learning rates. else try ? x check date master branch keras. update pip install git git github.com fchollet keras.git upgrade deps x running theano, check date master branch theano. update pip install git git github.com theano theano.git upgrade deps"
keras,9824,"numpy import array pickle import load keras.preprocessing.text import tokenizer keras.preprocessing.sequence import pad sequences keras.utils import categorical keras.utils import plot model keras.models import model keras.layers import input keras.layers import dense keras.layers import lstm keras.layers import embedding keras.layers import dropout keras.layers.merge import add keras.callbacks import modelcheckpoint import itertools load doc memory def load doc filename open file read file open filename, 'r' read text text file.read close file file.close return text load pre defined list photo identifiers def load set filename doc load doc filename dataset list process line line line doc.split ' n' skip empty lines len line 1 continue get image identifier identifier line.split '.' 0 dataset.append identifier return set dataset load clean descriptions memory def load clean descriptions filename, dataset load document doc load doc filename descriptions dict line doc.split ' n' split line white space tokens line.split split id description image id, image desc tokens 0 , tokens 1 skip images set image id dataset create list image id descriptions descriptions image id list wrap description tokens desc 'startseq ' ' '.join image desc ' endseq' store descriptions image id .append desc return descriptions load photo features def load photo features filename, dataset load features features load open filename, 'rb' filter features features k features k k dataset return features covert dictionary clean descriptions list descriptions def lines descriptions desc list key descriptions.keys desc.append descriptions key return desc fit tokenizer given caption descriptions def create tokenizer descriptions lines lines descriptions tokenizer tokenizer tokenizer.fit texts lines return tokenizer calculate length description words def max length descriptions lines lines descriptions return max len d.split lines create sequences images, input sequences output words image def create sequences tokenizer, max length, descriptions, photos x1, x2, list , list , list walk image identifier key, desc list descriptions.items walk description image desc desc list encode sequence seq tokenizer.texts sequences desc 0 split one sequence multiple x,y pairs range 1, len seq split input output pair seq, seq seq , seq pad input sequence seq pad sequences seq , maxlen max length 0 encode output sequence seq categorical seq , num classes vocab size store key photos.items x1.append photos key 0 x2.append seq y.append seq return array x1 , array x2 , array define captioning model def define model vocab size, max length feature extractor model inputs1 input shape 4096, fe1 dropout 0.5 inputs1 fe2 dense 256, activation 'relu' fe1 sequence model inputs2 input shape max length, se1 embedding vocab size, 256, mask zero true inputs2 se2 dropout 0.5 se1 se3 lstm 256 se2 decoder model decoder1 add fe2, se3 decoder2 dense 256, activation 'relu' decoder1 outputs dense vocab size, activation 'softmax' decoder2 tie together image, seq word model model inputs inputs1, inputs2 , outputs outputs model.compile loss 'categorical crossentropy', optimizer 'adam' summarize model print model.summary plot model model, file 'model.png', show shapes true return model train dataset load training dataset 6k filename 'flickr8k text flickr 8k.trainimages.txt' train load set filename print 'dataset d' len train descriptions train descriptions load clean descriptions 'descriptions.txt', train print 'descriptions train d' len train descriptions photo features train features load photo features 'features.pkl', train print 'photos train d' len train features prepare tokenizer tokenizer create tokenizer train descriptions vocab size len tokenizer.word index 1 print 'vocabulary size d' vocab size determine maximum sequence length max length max length train descriptions print 'description length d' max length print train descriptions.type print train features.type interdesc dict itertools.islice train descriptions.items ,1000 interfeatures dict itertools.islice train features.items ,1000 print train descriptions print interfeatures prepare sequences x1train, x2train, ytrain create sequences tokenizer, max length, interdesc, interfeatures dev dataset load test set filename 'flickr8k text flickr 8k.devimages.txt' test load set filename print 'dataset d' len test descriptions test descriptions load clean descriptions 'descriptions.txt', test print 'descriptions test d' len test descriptions photo features test features load photo features 'features.pkl', test print 'photos test d' len test features prepare sequences x1test, x2test, ytest create sequences tokenizer, max length, test descriptions, test features fit model define model model define model vocab size, max length define checkpoint callback filepath 'model ep epoch 03d loss loss .3f val loss val loss .3f .h5' checkpoint modelcheckpoint filepath, monitor 'val loss', verbose 1, save best true, mode 'min' fit model model.fit x1train, x2train , ytrain, epochs 20, verbose 2, callbacks checkpoint , validation data x1test, x2test , ytest",0,"valueerror error checking input expected input 1 shape none, 4096 got array shape 0, 1","valueerror error checking input expected input 1 shape none, 4096 got array shape 0, 1 numpy import array pickle import load keras.preprocessing.text import tokenizer keras.preprocessing.sequence import pad sequences keras.utils import categorical keras.utils import plot model keras.models import model keras.layers import input keras.layers import dense keras.layers import lstm keras.layers import embedding keras.layers import dropout keras.layers.merge import add keras.callbacks import modelcheckpoint import itertools load doc memory def load doc filename open file read file open filename, 'r' read text text file.read close file file.close return text load pre defined list photo identifiers def load set filename doc load doc filename dataset list process line line line doc.split ' n' skip empty lines len line 1 continue get image identifier identifier line.split '.' 0 dataset.append identifier return set dataset load clean descriptions memory def load clean descriptions filename, dataset load document doc load doc filename descriptions dict line doc.split ' n' split line white space tokens line.split split id description image id, image desc tokens 0 , tokens 1 skip images set image id dataset create list image id descriptions descriptions image id list wrap description tokens desc 'startseq ' ' '.join image desc ' endseq' store descriptions image id .append desc return descriptions load photo features def load photo features filename, dataset load features features load open filename, 'rb' filter features features k features k k dataset return features covert dictionary clean descriptions list descriptions def lines descriptions desc list key descriptions.keys desc.append descriptions key return desc fit tokenizer given caption descriptions def create tokenizer descriptions lines lines descriptions tokenizer tokenizer tokenizer.fit texts lines return tokenizer calculate length description words def max length descriptions lines lines descriptions return max len d.split lines create sequences images, input sequences output words image def create sequences tokenizer, max length, descriptions, photos x1, x2, list , list , list walk image identifier key, desc list descriptions.items walk description image desc desc list encode sequence seq tokenizer.texts sequences desc 0 split one sequence multiple x,y pairs range 1, len seq split input output pair seq, seq seq , seq pad input sequence seq pad sequences seq , maxlen max length 0 encode output sequence seq categorical seq , num classes vocab size store key photos.items x1.append photos key 0 x2.append seq y.append seq return array x1 , array x2 , array define captioning model def define model vocab size, max length feature extractor model inputs1 input shape 4096, fe1 dropout 0.5 inputs1 fe2 dense 256, activation 'relu' fe1 sequence model inputs2 input shape max length, se1 embedding vocab size, 256, mask zero true inputs2 se2 dropout 0.5 se1 se3 lstm 256 se2 decoder model decoder1 add fe2, se3 decoder2 dense 256, activation 'relu' decoder1 outputs dense vocab size, activation 'softmax' decoder2 tie together image, seq word model model inputs inputs1, inputs2 , outputs outputs model.compile loss 'categorical crossentropy', optimizer 'adam' summarize model print model.summary plot model model, file 'model.png', show shapes true return model train dataset load training dataset 6k filename 'flickr8k text flickr 8k.trainimages.txt' train load set filename print 'dataset d' len train descriptions train descriptions load clean descriptions 'descriptions.txt', train print 'descriptions train d' len train descriptions photo features train features load photo features 'features.pkl', train print 'photos train d' len train features prepare tokenizer tokenizer create tokenizer train descriptions vocab size len tokenizer.word index 1 print 'vocabulary size d' vocab size determine maximum sequence length max length max length train descriptions print 'description length d' max length print train descriptions.type print train features.type interdesc dict itertools.islice train descriptions.items ,1000 interfeatures dict itertools.islice train features.items ,1000 print train descriptions print interfeatures prepare sequences x1train, x2train, ytrain create sequences tokenizer, max length, interdesc, interfeatures dev dataset load test set filename 'flickr8k text flickr 8k.devimages.txt' test load set filename print 'dataset d' len test descriptions test descriptions load clean descriptions 'descriptions.txt', test print 'descriptions test d' len test descriptions photo features test features load photo features 'features.pkl', test print 'photos test d' len test features prepare sequences x1test, x2test, ytest create sequences tokenizer, max length, test descriptions, test features fit model define model model define model vocab size, max length define checkpoint callback filepath 'model ep epoch 03d loss loss .3f val loss val loss .3f .h5' checkpoint modelcheckpoint filepath, monitor 'val loss', verbose 1, save best true, mode 'min' fit model model.fit x1train, x2train , ytrain, epochs 20, verbose 2, callbacks checkpoint , validation data x1test, x2test , ytest"
keras,3880,"studying chinese stock market binary classification gru, traing samples balance output fine class 1s 0s, however, want predict real, almost return 0s. strange one mode predict 2 classes oneday, predict output similar training data, fine. mode trained epoches, try use predict day data, predict data like 0.2 0.3, means get 0s? prepare real data code hard explain one day fine others wrong idea?thanks",0,binary classification get 0s gru,"binary classification get 0s gru studying chinese stock market binary classification gru, traing samples balance output fine class 1s 0s, however, want predict real, almost return 0s. strange one mode predict 2 classes oneday, predict output similar training data, fine. mode trained epoches, try use predict day data, predict data like 0.2 0.3, means get 0s? prepare real data code hard explain one day fine others wrong idea?thanks"
keras,13190,nan,0,.,. nan
keras,5963,"siamese network example mnist https github.com fchollet keras blob master examples mnist siamese graph.py uses following function compute accuracy seems flawed me. example surely order predictions important? example above, predictions different labels, yet compute accuracy returns 100 . missing something obvious, mistake. x check date master branch keras. update pip install git git github.com fchollet keras.git upgrade deps x running tensorflow, check date latest version. installation instructions found https www.tensorflow.org get started os setup . x running theano, check date master branch theano. update pip install git git github.com theano theano.git upgrade deps x provide link github gist python script reproduce issue copy script short .",0,bug siamese example accuracy calculation,"bug siamese example accuracy calculation siamese network example mnist https github.com fchollet keras blob master examples mnist siamese graph.py uses following function compute accuracy seems flawed me. example surely order predictions important? example above, predictions different labels, yet compute accuracy returns 100 . missing something obvious, mistake. x check date master branch keras. update pip install git git github.com fchollet keras.git upgrade deps x running tensorflow, check date latest version. installation instructions found https www.tensorflow.org get started os setup . x running theano, check date master branch theano. update pip install git git github.com theano theano.git upgrade deps x provide link github gist python script reproduce issue copy script short ."
keras,6912,"hi, wonder way specify different learning rates different layers cnn. thanks",0,different learning rates different layers cnn.,"different learning rates different layers cnn. hi, wonder way specify different learning rates different layers cnn. thanks"
keras,1093,"hello, try run example, model fails build, throws following error using latest keras , well latest theano github, suggested many times. error obviously occurs even try create backwards lstm layer models. clue might be?",0,lstm go backwards,"lstm go backwards hello, try run example, model fails build, throws following error using latest keras , well latest theano github, suggested many times. error obviously occurs even try create backwards lstm layer models. clue might be?"
keras,9054,"question regarding example file https github.com keras team keras blob master examples mnist siamese.py contrastive loss layer, example file using margin setting training process. would assume margin would parameter threshold accuracy model. 0.5 compute acc function . reason using different threshold?",0,siamese network forward pass test accuracy,"siamese network forward pass test accuracy question regarding example file https github.com keras team keras blob master examples mnist siamese.py contrastive loss layer, example file using margin setting training process. would assume margin would parameter threshold accuracy model. 0.5 compute acc function . reason using different threshold?"
keras,2946,"upgraded keras 1.0.3 since started giving error. earlier 0.3.3, however running fine another system keras 1.0.3 version. unable figure issue.",0,error saving model keras. version 1.0.3,"error saving model keras. version 1.0.3 upgraded keras 1.0.3 since started giving error. earlier 0.3.3, however running fine another system keras 1.0.3 version. unable figure issue."
keras,2160,"want train model parts parameters fixed. tried load saved weights hdf5 file, fails. seems saveweight function save none trainable weights, loadweight function requires them. see example code keras.models import sequential keras.layers.core import merge keras.layers.convolutional import convolution2d model range 5 sequential model range 5.add convolution2d 128,5,5,dim ordering 'tf',activation 'relu',border mode 'same',input shape 256,200,3 ,trainable false model range 4 sequential model range 4.add convolution2d 128,5,5,dim ordering 'tf',activation 'relu',border mode 'same',input shape 256,200,3 test model sequential merge model range 5,model range 4 test model.compile optimizer 'sgd',loss 'mse' test model.save weights 'weight.model',overwrite true test model.load weights 'weight.model'",0,bugs save load weights,"bugs save load weights want train model parts parameters fixed. tried load saved weights hdf5 file, fails. seems saveweight function save none trainable weights, loadweight function requires them. see example code keras.models import sequential keras.layers.core import merge keras.layers.convolutional import convolution2d model range 5 sequential model range 5.add convolution2d 128,5,5,dim ordering 'tf',activation 'relu',border mode 'same',input shape 256,200,3 ,trainable false model range 4 sequential model range 4.add convolution2d 128,5,5,dim ordering 'tf',activation 'relu',border mode 'same',input shape 256,200,3 test model sequential merge model range 5,model range 4 test model.compile optimizer 'sgd',loss 'mse' test model.save weights 'weight.model',overwrite true test model.load weights 'weight.model'"
keras,5176,documentation keras seems scikit learn api supported sequential models. make sense use well using keras functional api?,0,scikit wrapper functional api well?,scikit wrapper functional api well? documentation keras seems scikit learn api supported sequential models. make sense use well using keras functional api?
keras,2905,"hi, guys, come across problem training dataset large split many subset load subset training. subset, split batches training. here, compiling, use function graph.fit many times update weights different subsets?",0,rerun graph.fit many times,"rerun graph.fit many times hi, guys, come across problem training dataset large split many subset load subset training. subset, split batches training. here, compiling, use function graph.fit many times update weights different subsets?"
keras,2521,"minor issue, admittedly, builtin way set call order model's callbacks , ? assume callbacks follow input list's ordering, defaults. specifically progbarlogger, since seems called last, output gets wonky case callback also prints stdout .",0,"setting callbacks order, including default callbacks","setting callbacks order, including default callbacks minor issue, admittedly, builtin way set call order model's callbacks , ? assume callbacks follow input list's ordering, defaults. specifically progbarlogger, since seems called last, output gets wonky case callback also prints stdout ."
keras,2560,users like race condition generator exits generating number samples equal slightly greater . function occasionally fetch queue instead final elements.,0,race condition fit generator generator exits,race condition fit generator generator exits users like race condition generator exits generating number samples equal slightly greater . function occasionally fetch queue instead final elements.
keras,3425,"hi! recurrent neural network lstm, resp. gru behaves way cannot explain. training starts trains well results look quite good suddenly accuracy drops loss rapidly increases training testing metrics. sometimes net goes crazy returns random outputs sometimes last three given examples starts return output inputs . ! image https cloud.githubusercontent.com assets 8523511 17508785 f94aaf50 5e16 11e6 92cd 4d842b4c0e9c.png explanation behavior ? opinion welcome. please, see task description figures below. task word predict word2vec vector input word2vec model normalized feed network word letter letter . pad words see example . example word football want predict word2vec vector 100 dimensions wide. input . three examples behavior single layer lstm ! image https cloud.githubusercontent.com assets 8523511 17507845 43eecbfe 5e12 11e6 8ae3 179ac10d82b7.png single layer gru ! image https cloud.githubusercontent.com assets 8523511 17507923 8ec75934 5e12 11e6 891b a2d2f2c10e39.png double layer lstm ! image https cloud.githubusercontent.com assets 8523511 17507947 a29ad68e 5e12 11e6 9355 bde7711df71f.png also experienced kind behavior another project used similar architecture objective data different. thus reason hidden data particular objective rather architecture.",0,sudden accuracy drop training lstm gru,"sudden accuracy drop training lstm gru hi! recurrent neural network lstm, resp. gru behaves way cannot explain. training starts trains well results look quite good suddenly accuracy drops loss rapidly increases training testing metrics. sometimes net goes crazy returns random outputs sometimes last three given examples starts return output inputs . ! image https cloud.githubusercontent.com assets 8523511 17508785 f94aaf50 5e16 11e6 92cd 4d842b4c0e9c.png explanation behavior ? opinion welcome. please, see task description figures below. task word predict word2vec vector input word2vec model normalized feed network word letter letter . pad words see example . example word football want predict word2vec vector 100 dimensions wide. input . three examples behavior single layer lstm ! image https cloud.githubusercontent.com assets 8523511 17507845 43eecbfe 5e12 11e6 8ae3 179ac10d82b7.png single layer gru ! image https cloud.githubusercontent.com assets 8523511 17507923 8ec75934 5e12 11e6 891b a2d2f2c10e39.png double layer lstm ! image https cloud.githubusercontent.com assets 8523511 17507947 a29ad68e 5e12 11e6 9355 bde7711df71f.png also experienced kind behavior another project used similar architecture objective data different. thus reason hidden data particular objective rather architecture."
keras,2758,"fairly simple question training sequence sequence model, would like accuracy reported training. note score measure exact accuracy, i.e. output symbols need correct one input sequence order prediction correct. words, metric measure zero one loss outputs together, rather per individual output. instance letters correct, prediction counts are. currently, last layers compile using correct, report per symbol accuracy? so, best implement custom metric?",0,accuracy reported sequence output?,"accuracy reported sequence output? fairly simple question training sequence sequence model, would like accuracy reported training. note score measure exact accuracy, i.e. output symbols need correct one input sequence order prediction correct. words, metric measure zero one loss outputs together, rather per individual output. instance letters correct, prediction counts are. currently, last layers compile using correct, report per symbol accuracy? so, best implement custom metric?"
keras,7146,"related 3921 maybe more, think need template merge layer page https keras.io layers merge probably fchollet put input it? examples work understanding things clear. 1. documentation? need inform import merge layers. otherwise, also think need make clear dealing tensors layers already done docstrings really documentation outdated though. python tensor input shape 32, tensor b input shape 32, merged tensor merge tensor a, tensor b , mode 'concat', concat axis 1 concatdotcoscompute output shape 2. examples show clearly tensors within simple functional model maybe use models?",0,merge layers need documentations example?,"merge layers need documentations example? related 3921 maybe more, think need template merge layer page https keras.io layers merge probably fchollet put input it? examples work understanding things clear. 1. documentation? need inform import merge layers. otherwise, also think need make clear dealing tensors layers already done docstrings really documentation outdated though. python tensor input shape 32, tensor b input shape 32, merged tensor merge tensor a, tensor b , mode 'concat', concat axis 1 concatdotcoscompute output shape 2. examples show clearly tensors within simple functional model maybe use models?"
keras,536,like ask could make optimizer's learning rate momentum shared scalars. way could change values training using custom rules. could work pr long guys reason to.,0,learning rate momentum shared scalars,learning rate momentum shared scalars like ask could make optimizer's learning rate momentum shared scalars. way could change values training using custom rules. could work pr long guys reason to.
keras,1524,"hey there, binary cross entropy problem would like solve using one class classification. this, following model... however, returning following error. anyone help?",0,one class classification,"one class classification hey there, binary cross entropy problem would like solve using one class classification. this, following model... however, returning following error. anyone help?"
keras,868,"work. obviously trying load loss function relevant keras module, finding it. adding parameters model.model json? something like would great. generally though, maybe add possibility giving dictionary named functions custom functions model might use activation, loss, optimizer,",0,using custom loss function model json,"using custom loss function model json work. obviously trying load loss function relevant keras module, finding it. adding parameters model.model json? something like would great. generally though, maybe add possibility giving dictionary named functions custom functions model might use activation, loss, optimizer,"
keras,11757,"yes check date master branch keras. update yes check version tensorflow date. installation instructions found https www.tensorflow.org get started os setup . https github.com keras team keras blob master examples tensorboard embeddings mnist.py provide link github gist python script reproduce issue copy script short . using anaconda python 3.6 running example code tensorboard embedding mnist. training epoch, call back called, gives following error. epoch 1 12 60000 60000 8s 139us step loss 0.2665 acc 0.9170 val loss 0.0716 val acc 0.9774 2018 11 29 13 04 24.254467 w tensorflow core framework op kernel.cc 1273 op requires failed save restore v2 ops.cc 137 unknown failed rename . logs keras embedding.ckpt 0.data 00000 00001.tempstate7943206387758954579 . logs keras embedding.ckpt 0.data 00000 00001 access denied. input output error traceback recent call last file c users abalu appdata local continuum anaconda3 envs keras lib site packages tensorflow python client session.py , line 1334, call return fn args file c users abalu appdata local continuum anaconda3 envs keras lib site packages tensorflow python client session.py , line 1319, run fn options, feed dict, fetch list, target list, run metadata file c users abalu appdata local continuum anaconda3 envs keras lib site packages tensorflow python client session.py , line 1407, call tf sessionrun run metadata tensorflow.python.framework.errors impl.unknownerror failed rename . logs keras embedding.ckpt 0.data 00000 00001.tempstate7943206387758954579 . logs keras embedding.ckpt 0.data 00000 00001 access denied. input output error node save savev2 savev2 dtypes dt float , device job localhost replica 0 task 0 device cpu 0 arg save const 0 0, save savev2 tensor names, save savev2 shape slices, features embedding 137 handling exception, another exception occurred traceback recent call last file embedding.py , line 88, validation data x test, test file c users abalu appdata local continuum anaconda3 envs keras lib site packages keras engine training.py , line 1039, fit validation steps validation steps file c users abalu appdata local continuum anaconda3 envs keras lib site packages keras engine training arrays.py , line 217, fit loop callbacks.on epoch end epoch, epoch logs file c users abalu appdata local continuum anaconda3 envs keras lib site packages keras callbacks.py , line 79, epoch end callback.on epoch end epoch, logs file c users abalu appdata local continuum anaconda3 envs keras lib site packages keras callbacks.py , line 981, epoch end epoch file c users abalu appdata local continuum anaconda3 envs keras lib site packages tensorflow python training saver.py , line 1441, save self.saver def.filename tensor name checkpoint file file c users abalu appdata local continuum anaconda3 envs keras lib site packages tensorflow python client session.py , line 929, run run metadata ptr file c users abalu appdata local continuum anaconda3 envs keras lib site packages tensorflow python client session.py , line 1152, run feed dict tensor, options, run metadata file c users abalu appdata local continuum anaconda3 envs keras lib site packages tensorflow python client session.py , line 1328, run run metadata file c users abalu appdata local continuum anaconda3 envs keras lib site packages tensorflow python client session.py , line 1348, call raise type e node def, op, message tensorflow.python.framework.errors impl.unknownerror failed rename . logs keras embedding.ckpt 0.data 00000 00001.tempstate7943206387758954579 . logs keras embedding.ckpt 0.data 00000 00001 access denied. input output error node save savev2 defined c users abalu appdata local continuum anaconda3 envs keras lib site packages keras callbacks.py 887 savev2 dtypes dt float , device job localhost replica 0 task 0 device cpu 0 arg save const 0 0, save savev2 tensor names, save savev2 shape slices, features embedding 137 caused op 'save savev2', defined file embedding.py , line 88, validation data x test, test file c users abalu appdata local continuum anaconda3 envs keras lib site packages keras engine training.py , line 1039, fit validation steps validation steps file c users abalu appdata local continuum anaconda3 envs keras lib site packages keras engine training arrays.py , line 117, fit loop callbacks.set model callback model file c users abalu appdata local continuum anaconda3 envs keras lib site packages keras callbacks.py , line 54, set model callback.set model model file c users abalu appdata local continuum anaconda3 envs keras lib site packages keras callbacks.py , line 887, set model self.saver tf.train.saver list embeddings vars.values file c users abalu appdata local continuum anaconda3 envs keras lib site packages tensorflow python training saver.py , line 1102, init self.build file c users abalu appdata local continuum anaconda3 envs keras lib site packages tensorflow python training saver.py , line 1114, build self. build self. filename, build save true, build restore true file c users abalu appdata local continuum anaconda3 envs keras lib site packages tensorflow python training saver.py , line 1151, build build save build save, build restore build restore file c users abalu appdata local continuum anaconda3 envs keras lib site packages tensorflow python training saver.py , line 792, build internal save tensor self. addsaveops filename tensor, saveables file c users abalu appdata local continuum anaconda3 envs keras lib site packages tensorflow python training saver.py , line 284, addsaveops save self.save op filename tensor, saveables file c users abalu appdata local continuum anaconda3 envs keras lib site packages tensorflow python training saver.py , line 202, save op tensors file c users abalu appdata local continuum anaconda3 envs keras lib site packages tensorflow python ops gen io ops.py , line 1690, save v2 shape slices shape slices, tensors tensors, name name file c users abalu appdata local continuum anaconda3 envs keras lib site packages tensorflow python framework op def library.py , line 787, apply op helper op def op def file c users abalu appdata local continuum anaconda3 envs keras lib site packages tensorflow python util deprecation.py , line 488, new func return func args, kwargs file c users abalu appdata local continuum anaconda3 envs keras lib site packages tensorflow python framework ops.py , line 3274, create op op def op def file c users abalu appdata local continuum anaconda3 envs keras lib site packages tensorflow python framework ops.py , line 1770, init self. traceback tf stack.extract stack unknownerror see traceback failed rename . logs keras embedding.ckpt 0.data 00000 00001.tempstate7943206387758954579 . logs keras embedding.ckpt 0.data 00000 00001 access denied. input output error node save savev2 defined c users abalu appdata local continuum anaconda3 envs keras lib site packages keras callbacks.py 887 savev2 dtypes dt float , device job localhost replica 0 task 0 device cpu 0 arg save const 0 0, save savev2 tensor names, save savev2 shape slices, features embedding 137 searched possible reasons happen, found following issues discussion https github.com balancap ssd tensorflow issues 72 https stackoverflow.com questions 43644893 windows tensorflow could restore checkpoint access denied understanding that, could something behavior windows. know exact issue is, would like contribute someone could let know might issue. thanks",0,keras examples tensorboard embeddings mnist.py gives,"keras examples tensorboard embeddings mnist.py gives yes check date master branch keras. update yes check version tensorflow date. installation instructions found https www.tensorflow.org get started os setup . https github.com keras team keras blob master examples tensorboard embeddings mnist.py provide link github gist python script reproduce issue copy script short . using anaconda python 3.6 running example code tensorboard embedding mnist. training epoch, call back called, gives following error. epoch 1 12 60000 60000 8s 139us step loss 0.2665 acc 0.9170 val loss 0.0716 val acc 0.9774 2018 11 29 13 04 24.254467 w tensorflow core framework op kernel.cc 1273 op requires failed save restore v2 ops.cc 137 unknown failed rename . logs keras embedding.ckpt 0.data 00000 00001.tempstate7943206387758954579 . logs keras embedding.ckpt 0.data 00000 00001 access denied. input output error traceback recent call last file c users abalu appdata local continuum anaconda3 envs keras lib site packages tensorflow python client session.py , line 1334, call return fn args file c users abalu appdata local continuum anaconda3 envs keras lib site packages tensorflow python client session.py , line 1319, run fn options, feed dict, fetch list, target list, run metadata file c users abalu appdata local continuum anaconda3 envs keras lib site packages tensorflow python client session.py , line 1407, call tf sessionrun run metadata tensorflow.python.framework.errors impl.unknownerror failed rename . logs keras embedding.ckpt 0.data 00000 00001.tempstate7943206387758954579 . logs keras embedding.ckpt 0.data 00000 00001 access denied. input output error node save savev2 savev2 dtypes dt float , device job localhost replica 0 task 0 device cpu 0 arg save const 0 0, save savev2 tensor names, save savev2 shape slices, features embedding 137 handling exception, another exception occurred traceback recent call last file embedding.py , line 88, validation data x test, test file c users abalu appdata local continuum anaconda3 envs keras lib site packages keras engine training.py , line 1039, fit validation steps validation steps file c users abalu appdata local continuum anaconda3 envs keras lib site packages keras engine training arrays.py , line 217, fit loop callbacks.on epoch end epoch, epoch logs file c users abalu appdata local continuum anaconda3 envs keras lib site packages keras callbacks.py , line 79, epoch end callback.on epoch end epoch, logs file c users abalu appdata local continuum anaconda3 envs keras lib site packages keras callbacks.py , line 981, epoch end epoch file c users abalu appdata local continuum anaconda3 envs keras lib site packages tensorflow python training saver.py , line 1441, save self.saver def.filename tensor name checkpoint file file c users abalu appdata local continuum anaconda3 envs keras lib site packages tensorflow python client session.py , line 929, run run metadata ptr file c users abalu appdata local continuum anaconda3 envs keras lib site packages tensorflow python client session.py , line 1152, run feed dict tensor, options, run metadata file c users abalu appdata local continuum anaconda3 envs keras lib site packages tensorflow python client session.py , line 1328, run run metadata file c users abalu appdata local continuum anaconda3 envs keras lib site packages tensorflow python client session.py , line 1348, call raise type e node def, op, message tensorflow.python.framework.errors impl.unknownerror failed rename . logs keras embedding.ckpt 0.data 00000 00001.tempstate7943206387758954579 . logs keras embedding.ckpt 0.data 00000 00001 access denied. input output error node save savev2 defined c users abalu appdata local continuum anaconda3 envs keras lib site packages keras callbacks.py 887 savev2 dtypes dt float , device job localhost replica 0 task 0 device cpu 0 arg save const 0 0, save savev2 tensor names, save savev2 shape slices, features embedding 137 caused op 'save savev2', defined file embedding.py , line 88, validation data x test, test file c users abalu appdata local continuum anaconda3 envs keras lib site packages keras engine training.py , line 1039, fit validation steps validation steps file c users abalu appdata local continuum anaconda3 envs keras lib site packages keras engine training arrays.py , line 117, fit loop callbacks.set model callback model file c users abalu appdata local continuum anaconda3 envs keras lib site packages keras callbacks.py , line 54, set model callback.set model model file c users abalu appdata local continuum anaconda3 envs keras lib site packages keras callbacks.py , line 887, set model self.saver tf.train.saver list embeddings vars.values file c users abalu appdata local continuum anaconda3 envs keras lib site packages tensorflow python training saver.py , line 1102, init self.build file c users abalu appdata local continuum anaconda3 envs keras lib site packages tensorflow python training saver.py , line 1114, build self. build self. filename, build save true, build restore true file c users abalu appdata local continuum anaconda3 envs keras lib site packages tensorflow python training saver.py , line 1151, build build save build save, build restore build restore file c users abalu appdata local continuum anaconda3 envs keras lib site packages tensorflow python training saver.py , line 792, build internal save tensor self. addsaveops filename tensor, saveables file c users abalu appdata local continuum anaconda3 envs keras lib site packages tensorflow python training saver.py , line 284, addsaveops save self.save op filename tensor, saveables file c users abalu appdata local continuum anaconda3 envs keras lib site packages tensorflow python training saver.py , line 202, save op tensors file c users abalu appdata local continuum anaconda3 envs keras lib site packages tensorflow python ops gen io ops.py , line 1690, save v2 shape slices shape slices, tensors tensors, name name file c users abalu appdata local continuum anaconda3 envs keras lib site packages tensorflow python framework op def library.py , line 787, apply op helper op def op def file c users abalu appdata local continuum anaconda3 envs keras lib site packages tensorflow python util deprecation.py , line 488, new func return func args, kwargs file c users abalu appdata local continuum anaconda3 envs keras lib site packages tensorflow python framework ops.py , line 3274, create op op def op def file c users abalu appdata local continuum anaconda3 envs keras lib site packages tensorflow python framework ops.py , line 1770, init self. traceback tf stack.extract stack unknownerror see traceback failed rename . logs keras embedding.ckpt 0.data 00000 00001.tempstate7943206387758954579 . logs keras embedding.ckpt 0.data 00000 00001 access denied. input output error node save savev2 defined c users abalu appdata local continuum anaconda3 envs keras lib site packages keras callbacks.py 887 savev2 dtypes dt float , device job localhost replica 0 task 0 device cpu 0 arg save const 0 0, save savev2 tensor names, save savev2 shape slices, features embedding 137 searched possible reasons happen, found following issues discussion https github.com balancap ssd tensorflow issues 72 https stackoverflow.com questions 43644893 windows tensorflow could restore checkpoint access denied understanding that, could something behavior windows. know exact issue is, would like contribute someone could let know might issue. thanks"
keras,11600,"use custom layer trainable swish activation function https arxiv.org abs 1710.05941. set non trainable beta value, quite general. like add keras part next release. quite popular. add keras contrib. add code swish making pull request?",0,add trainable swish layer keras,"add trainable swish layer keras use custom layer trainable swish activation function https arxiv.org abs 1710.05941. set non trainable beta value, quite general. like add keras part next release. quite popular. add keras contrib. add code swish making pull request?"
keras,6657,"using keras 2.x tf seeting. using x batch, batch datagen.flow train, train, batch size 32 example code keras.datasets import mnist keras.preprocessing.image import imagedatagenerator x train, train , x test, test mnist.load data x train x train.reshape x train.shape 0 , 28, 28,1 x test x test.reshape x test.shape 0 , 28, 28,1 x train x train.astype float32 x test x test.astype float32 datagen imagedatagenerator featurewise center true, featurewise std normalization true datagen.fit x train x batch, batch datagen.flow x train, train, batch size 9 question anyone tell why? thanks!",0,datagen.flow question,"datagen.flow question using keras 2.x tf seeting. using x batch, batch datagen.flow train, train, batch size 32 example code keras.datasets import mnist keras.preprocessing.image import imagedatagenerator x train, train , x test, test mnist.load data x train x train.reshape x train.shape 0 , 28, 28,1 x test x test.reshape x test.shape 0 , 28, 28,1 x train x train.astype float32 x test x test.astype float32 datagen imagedatagenerator featurewise center true, featurewise std normalization true datagen.fit x train x batch, batch datagen.flow x train, train, batch size 9 question anyone tell why? thanks!"
keras,7510,"trying import model exported using , get following error file q anaconda2 lib site packages keras models.py , line 325, model json return layer module.deserialize config, custom objects custom objects file q anaconda2 lib site packages keras layers init .py , line 46, deserialize printable module name 'layer' file q anaconda2 lib site packages keras utils generic utils.py , line 140, deserialize keras object list custom objects.items file q anaconda2 lib site packages keras engine topology.py , line 2374, config process layer layer data file q anaconda2 lib site packages keras engine topology.py , line 2343, process layer custom objects custom objects file q anaconda2 lib site packages keras layers init .py , line 46, deserialize printable module name 'layer' file q anaconda2 lib site packages keras utils generic utils.py , line 141, deserialize keras object return cls.from config config 'config' file q anaconda2 lib site packages keras engine topology.py , line 1206, config return cls config file q anaconda2 lib site packages keras legacy interfaces.py , line 88, wrapper return func args, kwargs file q anaconda2 lib site packages keras layers recurrent.py , line 931, init super lstm, self . init kwargs file q anaconda2 lib site packages keras layers recurrent.py , line 181, init super recurrent, self . init kwargs file q anaconda2 lib site packages keras engine topology.py , line 277, init raise typeerror 'keyword argument understood ', kwarg typeerror 'keyword argument understood ', u'return state' problem occurs trying import lstm layer , called keyword args however, throws exception keyword list. however, allowed kwarg per documentation https keras.io layers recurrent . model json serialized using version keras try load it. desired behavior? one might expect model serialized, also deserializable using version keras. json description model found gist https gist.github.com benjaminalt 04f00629cf52b735414d700e2352930c . setup os 64 bit windows 10 python python 2.7.13 anaconda 4.4.0 64 bit keras 2.0.6 theano 0.9.0",0,'keyword argument understood' error importing keras model json,"'keyword argument understood' error importing keras model json trying import model exported using , get following error file q anaconda2 lib site packages keras models.py , line 325, model json return layer module.deserialize config, custom objects custom objects file q anaconda2 lib site packages keras layers init .py , line 46, deserialize printable module name 'layer' file q anaconda2 lib site packages keras utils generic utils.py , line 140, deserialize keras object list custom objects.items file q anaconda2 lib site packages keras engine topology.py , line 2374, config process layer layer data file q anaconda2 lib site packages keras engine topology.py , line 2343, process layer custom objects custom objects file q anaconda2 lib site packages keras layers init .py , line 46, deserialize printable module name 'layer' file q anaconda2 lib site packages keras utils generic utils.py , line 141, deserialize keras object return cls.from config config 'config' file q anaconda2 lib site packages keras engine topology.py , line 1206, config return cls config file q anaconda2 lib site packages keras legacy interfaces.py , line 88, wrapper return func args, kwargs file q anaconda2 lib site packages keras layers recurrent.py , line 931, init super lstm, self . init kwargs file q anaconda2 lib site packages keras layers recurrent.py , line 181, init super recurrent, self . init kwargs file q anaconda2 lib site packages keras engine topology.py , line 277, init raise typeerror 'keyword argument understood ', kwarg typeerror 'keyword argument understood ', u'return state' problem occurs trying import lstm layer , called keyword args however, throws exception keyword list. however, allowed kwarg per documentation https keras.io layers recurrent . model json serialized using version keras try load it. desired behavior? one might expect model serialized, also deserializable using version keras. json description model found gist https gist.github.com benjaminalt 04f00629cf52b735414d700e2352930c . setup os 64 bit windows 10 python python 2.7.13 anaconda 4.4.0 64 bit keras 2.0.6 theano 0.9.0"
keras,1490,keras theano newest. used sudo pip install git git github.com theano theano.git. solve problem? many thanks!,0,attributeerror 'module' object attribute 'relu',attributeerror 'module' object attribute 'relu' keras theano newest. used sudo pip install git git github.com theano theano.git. solve problem? many thanks!
keras,8298,"please make sure boxes checked submit issue. issue implementation question, please ask question stackoverflow http stackoverflow.com questions tagged keras join keras slack channel https keras slack autojoin.herokuapp.com ask instead filing github issue. thank you! check date master branch keras. update pip install git git github.com fchollet keras.git upgrade deps running tensorflow, check date latest version. installation instructions found https www.tensorflow.org get started os setup . running theano, check date master branch theano. update pip install git git github.com theano theano.git upgrade deps provide link github gist python script reproduce issue copy script short .",0,index generator,"index generator please make sure boxes checked submit issue. issue implementation question, please ask question stackoverflow http stackoverflow.com questions tagged keras join keras slack channel https keras slack autojoin.herokuapp.com ask instead filing github issue. thank you! check date master branch keras. update pip install git git github.com fchollet keras.git upgrade deps running tensorflow, check date latest version. installation instructions found https www.tensorflow.org get started os setup . running theano, check date master branch theano. update pip install git git github.com theano theano.git upgrade deps provide link github gist python script reproduce issue copy script short ."
keras,2003,"using keras build train recurrent neural network. array sequences latitude, longitude, temperature , padded length values 0,0,0 . train network, first epoch gives loss 63, increases epochs. causing call later code give values completely training values. example, training values sequence around , rnn outputs values consistently around , causes think something wrong masking layer. training x data looks something like much larger training data looks like",0,loss increasing epoch.,"loss increasing epoch. using keras build train recurrent neural network. array sequences latitude, longitude, temperature , padded length values 0,0,0 . train network, first epoch gives loss 63, increases epochs. causing call later code give values completely training values. example, training values sequence around , rnn outputs values consistently around , causes think something wrong masking layer. training x data looks something like much larger training data looks like"
keras,303,"time bug correctly figured input dimensionality hidden layer that's layered top convolutional max pooling layer. code auto compute input dimensionality non input layers ease use? would saved hours initially, passage works specify output dimensionality .",0,omit input dimensions hidden layers auto calculate specified,"omit input dimensions hidden layers auto calculate specified time bug correctly figured input dimensionality hidden layer that's layered top convolutional max pooling layer. code auto compute input dimensionality non input layers ease use? would saved hours initially, passage works specify output dimensionality ."
keras,579,nan,0,"cnn lstm classify muti categories texts, modify code? thanks!","cnn lstm classify muti categories texts, modify code? thanks! nan"
keras,2943,nan,0,typo,typo nan
keras,197,karpathy's character based rnn torch https github.com karpathy char rnn gotten great deal attention recently blog post http karpathy.github.io 2015 05 21 rnn effectiveness entitled unreasonable effectiveness recurrent neural networks . would nice example thing keras.,0,add character based rnn example.,add character based rnn example. karpathy's character based rnn torch https github.com karpathy char rnn gotten great deal attention recently blog post http karpathy.github.io 2015 05 21 rnn effectiveness entitled unreasonable effectiveness recurrent neural networks . would nice example thing keras.
keras,1641,"hey everyone, trying use custom data lstm model, keeps giving shape errors. reading issues along lines, even tried reshaping input data size nb inputs, timestamps, 1 looks approximately like 4200, 60, 1 , returns error says shape none, 4200, 60, 1 good. thoughts? output using theano backend. loading data... 4130 train sequences 1016 test sequences x train shape 4130l, 60l x test shape 1016l, 60l build model... train... train 4130 samples, validate 1016 samples epoch 1 3 traceback recent call last file main.py , line 52, validation data x test, test , show accuracy true file c miniconda2 lib site packages keras models.py , line 507, fit shuffle shuffle, metrics metrics file c miniconda2 lib site packages keras models.py , line 226, fit outs f ins batch file c miniconda2 lib site packages keras backend theano backend.py , line 357, call return self.function inputs file c miniconda2 lib site packages theano compile function module.py , line 513, call allow downcast s.allow downcast file c miniconda2 lib site packages theano tensor type.py , line 169, filter data.shape typeerror 'bad input argument theano function name c miniconda2 lib site packages keras backend theano backend.py 354 index 0 0 based ', 'wrong number dimensions expected 3, got 2 shape 32l, 60l .'",0,"help 'wrong number dimensions expected 3, got 2 shape 32l, 60l .' lstm model","help 'wrong number dimensions expected 3, got 2 shape 32l, 60l .' lstm model hey everyone, trying use custom data lstm model, keeps giving shape errors. reading issues along lines, even tried reshaping input data size nb inputs, timestamps, 1 looks approximately like 4200, 60, 1 , returns error says shape none, 4200, 60, 1 good. thoughts? output using theano backend. loading data... 4130 train sequences 1016 test sequences x train shape 4130l, 60l x test shape 1016l, 60l build model... train... train 4130 samples, validate 1016 samples epoch 1 3 traceback recent call last file main.py , line 52, validation data x test, test , show accuracy true file c miniconda2 lib site packages keras models.py , line 507, fit shuffle shuffle, metrics metrics file c miniconda2 lib site packages keras models.py , line 226, fit outs f ins batch file c miniconda2 lib site packages keras backend theano backend.py , line 357, call return self.function inputs file c miniconda2 lib site packages theano compile function module.py , line 513, call allow downcast s.allow downcast file c miniconda2 lib site packages theano tensor type.py , line 169, filter data.shape typeerror 'bad input argument theano function name c miniconda2 lib site packages keras backend theano backend.py 354 index 0 0 based ', 'wrong number dimensions expected 3, got 2 shape 32l, 60l .'"
keras,6241,"hi, got problem run sequential model. fit model firstly, want get first fullconnect layer's parameters ,that dense1 model.get layer index 1 .get weights . got wrong attributeerror 'nonetype' object attribute 'get weights' idea this. anyone help me? sincerely",0,attributeerror 'nonetype' object attribute 'get weights',"attributeerror 'nonetype' object attribute 'get weights' hi, got problem run sequential model. fit model firstly, want get first fullconnect layer's parameters ,that dense1 model.get layer index 1 .get weights . got wrong attributeerror 'nonetype' object attribute 'get weights' idea this. anyone help me? sincerely"
keras,2529,"running date keras repo c9f7d970e97d5a following instructions following error happens addition, could small error example supposed line 7 function . x check date master branch keras. update pip install git git github.com fchollet keras.git upgrade deps x running theano, check date master branch theano. update pip install git git github.com theano theano.git upgrade deps x provide link github gist python script reproduce issue copy script short .",0,missing file docs sources layers writing keras layers.md building documentation,"missing file docs sources layers writing keras layers.md building documentation running date keras repo c9f7d970e97d5a following instructions following error happens addition, could small error example supposed line 7 function . x check date master branch keras. update pip install git git github.com fchollet keras.git upgrade deps x running theano, check date master branch theano. update pip install git git github.com theano theano.git upgrade deps x provide link github gist python script reproduce issue copy script short ."
keras,3873,"wanted able show sequential networks clean minimalistic way didactic purpose. graph export enough wanted dimensions, numbers parameters activation functions one place, time without unnecessary overhead. bear mind purposefully make distinction adding activation function keyword argument separate layer vide activations keras documentation https keras.io activations , unlike . code https gist.github.com stared 8411d4e7e457b0f14f39d700afc8511c clean generalise it, part ? comments, remarks sub feature requests ale welcomed! examples proof principle vgg16",0,feature ascii prints sequential models,"feature ascii prints sequential models wanted able show sequential networks clean minimalistic way didactic purpose. graph export enough wanted dimensions, numbers parameters activation functions one place, time without unnecessary overhead. bear mind purposefully make distinction adding activation function keyword argument separate layer vide activations keras documentation https keras.io activations , unlike . code https gist.github.com stared 8411d4e7e457b0f14f39d700afc8511c clean generalise it, part ? comments, remarks sub feature requests ale welcomed! examples proof principle vgg16"
keras,4007,example code want see output right branch model given test input merge layer ? thanks,0,visualize output one two merged layers ?,visualize output one two merged layers ? example code want see output right branch model given test input merge layer ? thanks
keras,6973,"trying use batch normalization, reason, even simplest network, run model.fit even one epoch,the loss nan naturally learning performed. example use simple model like model sequential model.add conv2d 32,kernel size 3,3 ,activation 'relu',input shape 16,16,3 model.add maxpool2d pool size 2, 2 model.add batchnormalization model.add flatten model.add dense 2,activation 'softmax' model.compile loss 'binary crossentropy', optimizer 'adam', metrics 'accuracy' remove batch normalization, everything works great. using keras 2.0.4 theano 0.9.0, cuda 7. tried removing cudnn, got results. tried diffrent axis axis 1 calling bn, although right got result. wrong ? thank you!",0,problem batch normalization layer,"problem batch normalization layer trying use batch normalization, reason, even simplest network, run model.fit even one epoch,the loss nan naturally learning performed. example use simple model like model sequential model.add conv2d 32,kernel size 3,3 ,activation 'relu',input shape 16,16,3 model.add maxpool2d pool size 2, 2 model.add batchnormalization model.add flatten model.add dense 2,activation 'softmax' model.compile loss 'binary crossentropy', optimizer 'adam', metrics 'accuracy' remove batch normalization, everything works great. using keras 2.0.4 theano 0.9.0, cuda 7. tried removing cudnn, got results. tried diffrent axis axis 1 calling bn, although right got result. wrong ? thank you!"
keras,3988,say pertained model like vgg16. want select first convolutional blocks something like way use api slice middle part model including weights ? tried something like got error,0,slicing layers pre trained model,slicing layers pre trained model say pertained model like vgg16. want select first convolutional blocks something like way use api slice middle part model including weights ? tried something like got error
keras,4156,"please make sure boxes checked submit issue. thank you! x check date master branch keras. update pip install git git github.com fchollet keras.git upgrade deps x running theano, check date master branch theano. update pip install git git github.com theano theano.git upgrade deps x provide link github gist python script reproduce issue copy script short . callback model model compilation final csv column 'lr' get always 0.03 instead lr calculated decay. looking callbacks.py think problem 'logs' object passed callback callback itself. hints? could try make pr given guidance",0,callbacks e.g. csv callback receive constant lr lr decay,"callbacks e.g. csv callback receive constant lr lr decay please make sure boxes checked submit issue. thank you! x check date master branch keras. update pip install git git github.com fchollet keras.git upgrade deps x running theano, check date master branch theano. update pip install git git github.com theano theano.git upgrade deps x provide link github gist python script reproduce issue copy script short . callback model model compilation final csv column 'lr' get always 0.03 instead lr calculated decay. looking callbacks.py think problem 'logs' object passed callback callback itself. hints? could try make pr given guidance"
keras,2619,"hi, design network composes shared layers, write code using functional api. however, always errors. paste network structure graph , code , error information below, hope provide clue. thanks! network structure ! network structure jpeg https raw.githubusercontent.com ylqfp attachments bb396650622b37b2a311d29864795aec67301042 cnn2 2 lstm.jpg share layers 1. embedding shared among 3 input. blue color . however, input length different input1 input2 input3. input length input1 input2 lstm maxlen, 1000, input3 query maxlen, 5, eg. max 5 words query. 2. cnn feature learning layer shared input1 input2. green color",0,nested shared layers network work?,"nested shared layers network work? hi, design network composes shared layers, write code using functional api. however, always errors. paste network structure graph , code , error information below, hope provide clue. thanks! network structure ! network structure jpeg https raw.githubusercontent.com ylqfp attachments bb396650622b37b2a311d29864795aec67301042 cnn2 2 lstm.jpg share layers 1. embedding shared among 3 input. blue color . however, input length different input1 input2 input3. input length input1 input2 lstm maxlen, 1000, input3 query maxlen, 5, eg. max 5 words query. 2. cnn feature learning layer shared input1 input2. green color"
keras,5284,precomputing features convolutional layer training fully connected layers using them. use hdf5 file custom generator size dataset quite large. following output loss using fit decreases much quickly using fit generator custom generator. something wrong? looks like bug fit generator.,0,problems using fit generator custom generator,problems using fit generator custom generator precomputing features convolutional layer training fully connected layers using them. use hdf5 file custom generator size dataset quite large. following output loss using fit decreases much quickly using fit generator custom generator. something wrong? looks like bug fit generator.
keras,1592,trying run specific code pc titan x gpu yields following error also versions run code another pc passes! machine without gpu versions could order use good gpu ? fall back keras 3.0 theano problem ? thank advance,0,convolutional neural net bug,convolutional neural net bug trying run specific code pc titan x gpu yields following error also versions run code another pc passes! machine without gpu versions could order use good gpu ? fall back keras 3.0 theano problem ? thank advance
keras,5167,"hi, model uses merge layer. model runs saves. trouble loading model again. error get arg 5 closure must none tuple researching previous issues, saw people potentially similar issues using lambda . get whether solution this. running issue python 2.7 3.5 update changed merge layer. lambda takes one argument, output shape literally defined list output layers models merged. now, try load model, error layer never called thus defined output shape help would appreciated. gerti please make sure boxes checked submit issue. issue implementation question, please ask question stackoverflow http stackoverflow.com questions tagged keras join keras slack channel https keras slack autojoin.herokuapp.com ask instead filing github issue. thank you! x check date master branch keras. update pip install git git github.com fchollet keras.git upgrade deps running tensorflow, check date latest version. installation instructions found https www.tensorflow.org get started os setup . x running theano, check date master branch theano. update pip install git git github.com theano theano.git upgrade deps x provide link github gist python script reproduce issue copy script short .",0,merge layer lambda potential issue reloading model save,"merge layer lambda potential issue reloading model save hi, model uses merge layer. model runs saves. trouble loading model again. error get arg 5 closure must none tuple researching previous issues, saw people potentially similar issues using lambda . get whether solution this. running issue python 2.7 3.5 update changed merge layer. lambda takes one argument, output shape literally defined list output layers models merged. now, try load model, error layer never called thus defined output shape help would appreciated. gerti please make sure boxes checked submit issue. issue implementation question, please ask question stackoverflow http stackoverflow.com questions tagged keras join keras slack channel https keras slack autojoin.herokuapp.com ask instead filing github issue. thank you! x check date master branch keras. update pip install git git github.com fchollet keras.git upgrade deps running tensorflow, check date latest version. installation instructions found https www.tensorflow.org get started os setup . x running theano, check date master branch theano. update pip install git git github.com theano theano.git upgrade deps x provide link github gist python script reproduce issue copy script short ."
keras,8926,nan,0,load modelcrashing,load modelcrashing nan
keras,4966,"trying implement following architecture keras theano backend . first sequential network say s1 takes image input 4 linear output, correspond upper left bottom right coordinates rectangular window input image supposed contain object want identify. four outputs... like actually crop image! thought take image input new sequential network say s2 merge two networks using merge layer function mode. turned bit complicated expected stuck theano errors can't get rid of. here's relevant part code model1 sequential model1.add convolution2d 64, 3, 3, border mode 'same', input shape 1280, 720, 3 , activation 'relu' model.output shape none, 1280, 720, 64 add 3x3 convolution top, 32 output filters model1.add convolution2d 32, 3, 3, border mode 'same',activation 'relu' model.output shape none, 1280, 720, 32 model1.add flatten model1.add dense 4, activation 'linear' model2 sequential create simple input net? model2.add reshape 1280, 720, 3 , input shape 1280, 720, 3 def merger l image l 1 indexes l 0 index 0 indexes 0 index 1 indexes ,1 index 2 indexes ,2 index 3 indexes ,3 cropped image image ,index 0 index 1 1, index 2 index 3 1, return cropped image merged model sequential merged model.add merge model1, model2 , mode merger error merger l 11 index 2 indexes ,2 12 index 3 indexes ,3 13 cropped image image ,index 0 index 1 1, index 2 index 3 1, valueerror 'tensortype could cast 0 dimensions', tensortype float32, vector what's going on?",0,crop dinamically,"crop dinamically trying implement following architecture keras theano backend . first sequential network say s1 takes image input 4 linear output, correspond upper left bottom right coordinates rectangular window input image supposed contain object want identify. four outputs... like actually crop image! thought take image input new sequential network say s2 merge two networks using merge layer function mode. turned bit complicated expected stuck theano errors can't get rid of. here's relevant part code model1 sequential model1.add convolution2d 64, 3, 3, border mode 'same', input shape 1280, 720, 3 , activation 'relu' model.output shape none, 1280, 720, 64 add 3x3 convolution top, 32 output filters model1.add convolution2d 32, 3, 3, border mode 'same',activation 'relu' model.output shape none, 1280, 720, 32 model1.add flatten model1.add dense 4, activation 'linear' model2 sequential create simple input net? model2.add reshape 1280, 720, 3 , input shape 1280, 720, 3 def merger l image l 1 indexes l 0 index 0 indexes 0 index 1 indexes ,1 index 2 indexes ,2 index 3 indexes ,3 cropped image image ,index 0 index 1 1, index 2 index 3 1, return cropped image merged model sequential merged model.add merge model1, model2 , mode merger error merger l 11 index 2 indexes ,2 12 index 3 indexes ,3 13 cropped image image ,index 0 index 1 1, index 2 index 3 1, valueerror 'tensortype could cast 0 dimensions', tensortype float32, vector what's going on?"
keras,3776,"using latest version keras theano backend. short description problem code reproduces problem also shown convolution network 1 channel 2d input. input layer shared two nodes, performing convolution operations. outputs two nodes merged another node, produces final output. expecting get output dimension batch size, 2x4x9 , 15, 72 . however, error message says getting 30, 72 , 2xbatch size, 2x4x9 . seems something wrong merge node looking 2xbatch size . structure model ! capture https cloud.githubusercontent.com assets 22214494 18573430 e74a2964 7bf5 11e6 9d28 82239e29d293.png model error info pasted tried make minimum example simple possible. idea wrong? thanks lot help!",0,"fit generator, unexpected output dimension","fit generator, unexpected output dimension using latest version keras theano backend. short description problem code reproduces problem also shown convolution network 1 channel 2d input. input layer shared two nodes, performing convolution operations. outputs two nodes merged another node, produces final output. expecting get output dimension batch size, 2x4x9 , 15, 72 . however, error message says getting 30, 72 , 2xbatch size, 2x4x9 . seems something wrong merge node looking 2xbatch size . structure model ! capture https cloud.githubusercontent.com assets 22214494 18573430 e74a2964 7bf5 11e6 9d28 82239e29d293.png model error info pasted tried make minimum example simple possible. idea wrong? thanks lot help!"
keras,11943,"minor, second link used reference conv2dtranspose seems invalid. changed https www.matthewzeiler.com pubs cvpr2010 cvpr2010.pdf https www.matthewzeiler.com mattzeiler deconvolutionalnetworks.pdf.",0,link deconv paper broken conv2dtranspose comments,"link deconv paper broken conv2dtranspose comments minor, second link used reference conv2dtranspose seems invalid. changed https www.matthewzeiler.com pubs cvpr2010 cvpr2010.pdf https www.matthewzeiler.com mattzeiler deconvolutionalnetworks.pdf."
keras,5617,"please make sure boxes checked submit issue. issue implementation question, please ask question stackoverflow http stackoverflow.com questions tagged keras join keras slack channel https keras slack autojoin.herokuapp.com ask instead filing github issue. thank you! x check date master branch keras. update pip install git git github.com fchollet keras.git upgrade deps x running tensorflow, check date latest version. installation instructions found https www.tensorflow.org get started os setup . x running theano, check date master branch theano. update pip install git git github.com theano theano.git upgrade deps x provide link github gist python script reproduce issue copy script short . need lstm generate words timestep given input next timestep. done keras?",0,obtain output timestep,"obtain output timestep please make sure boxes checked submit issue. issue implementation question, please ask question stackoverflow http stackoverflow.com questions tagged keras join keras slack channel https keras slack autojoin.herokuapp.com ask instead filing github issue. thank you! x check date master branch keras. update pip install git git github.com fchollet keras.git upgrade deps x running tensorflow, check date latest version. installation instructions found https www.tensorflow.org get started os setup . x running theano, check date master branch theano. update pip install git git github.com theano theano.git upgrade deps x provide link github gist python script reproduce issue copy script short . need lstm generate words timestep given input next timestep. done keras?"
keras,5164,"fchollet came across something make life better. handful tools automatically close issues abandoned. one example plenty options. https github.com twbs carrier configuration like might reasonable send warning 2, 4 6 weeks. close activity 8 weeks warnings , making sure mark resolution something like automatic closure get confused things manually closed. would easy browse automatic closures wanted to. something gets closed accident, someone could always reopen make exceptions issues want pin keep active. currently 1,945 issues including one rising. sound like something reasonable implement? cheers",0,automatically closing abandoned issues,"automatically closing abandoned issues fchollet came across something make life better. handful tools automatically close issues abandoned. one example plenty options. https github.com twbs carrier configuration like might reasonable send warning 2, 4 6 weeks. close activity 8 weeks warnings , making sure mark resolution something like automatic closure get confused things manually closed. would easy browse automatic closures wanted to. something gets closed accident, someone could always reopen make exceptions issues want pin keep active. currently 1,945 issues including one rising. sound like something reasonable implement? cheers"
keras,4982,"would possible support images channels 3 1. currently working project images 8 channels, currently truncate 3 channels. way around this?",0,multichannel images,"multichannel images would possible support images channels 3 1. currently working project images 8 channels, currently truncate 3 channels. way around this?"
keras,1625,"using callback stopping training fixed period time, useful testing different architectures consecutively. worth contributing? one thing noticed stops model epoch, training epochs long might want stop batch.",0,timestopping callback,"timestopping callback using callback stopping training fixed period time, useful testing different architectures consecutively. worth contributing? one thing noticed stops model epoch, training epochs long might want stop batch."
keras,5896,"using ubuntu 16.04, python 3.5.2, keras 2.0.1 tensorflow 1.01. keras tensorflow crash using threads. following code simplified version trying recreates crash. https gist.github.com eyesonlyhack c43dea734f872a9c45da8587eefec581 found way get around issue think probably bug keras. workaround https gist.github.com eyesonlyhack 2f0b20f1e73aaf5e9b83f49415f3601a",0,cannot use keras threads,"cannot use keras threads using ubuntu 16.04, python 3.5.2, keras 2.0.1 tensorflow 1.01. keras tensorflow crash using threads. following code simplified version trying recreates crash. https gist.github.com eyesonlyhack c43dea734f872a9c45da8587eefec581 found way get around issue think probably bug keras. workaround https gist.github.com eyesonlyhack 2f0b20f1e73aaf5e9b83f49415f3601a"
keras,4740,"try run sequential keras found thread running sequential model generated 13 sub threads. control number sub threads? part code model sequential model.add ..... model.compile loss 'binary crossentropy', optimizer params 'optimizer' model.fit ..... please help problem. thanks advance!",0,control number threads running sequential keras,"control number threads running sequential keras try run sequential keras found thread running sequential model generated 13 sub threads. control number sub threads? part code model sequential model.add ..... model.compile loss 'binary crossentropy', optimizer params 'optimizer' model.fit ..... please help problem. thanks advance!"
keras,3486,"merge, merge merge forwards, backwards , mode 'max' traceback recent call last file , line 1, file home job analyse env lib python2.7 site packages keras engine topology.py , line 1490, merge name name file home job analyse env lib python2.7 site packages keras engine topology.py , line 1148, init self.add inbound node layers, node indices, tensor indices file home job analyse env lib python2.7 site packages keras engine topology.py , line 543, add inbound node node.create node self, inbound layers, node indices, tensor indices file home job analyse env lib python2.7 site packages keras engine topology.py , line 154, create node output masks list outbound layer.compute mask input tensors, input masks file home job analyse env lib python2.7 site packages keras engine topology.py , line 1372, compute mask raise exception 'invalid merge mode '.format self.mode exception invalid merge mode max",0,max mode used merge function ? ?,"max mode used merge function ? ? merge, merge merge forwards, backwards , mode 'max' traceback recent call last file , line 1, file home job analyse env lib python2.7 site packages keras engine topology.py , line 1490, merge name name file home job analyse env lib python2.7 site packages keras engine topology.py , line 1148, init self.add inbound node layers, node indices, tensor indices file home job analyse env lib python2.7 site packages keras engine topology.py , line 543, add inbound node node.create node self, inbound layers, node indices, tensor indices file home job analyse env lib python2.7 site packages keras engine topology.py , line 154, create node output masks list outbound layer.compute mask input tensors, input masks file home job analyse env lib python2.7 site packages keras engine topology.py , line 1372, compute mask raise exception 'invalid merge mode '.format self.mode exception invalid merge mode max"
keras,4862,"looks like commit https github.com fchollet keras commit 2a3d4722c21d99d882b2cbc2da451108147fe1c4 introduced bug least . rolling back commit fixes problem, sure whether breaks something somewhere else.",0,k.int shape bug,"k.int shape bug looks like commit https github.com fchollet keras commit 2a3d4722c21d99d882b2cbc2da451108147fe1c4 introduced bug least . rolling back commit fixes problem, sure whether breaks something somewhere else."
keras,6412,"hello still exploring keras past months new here. like much thanks fchollet . 3 problem now. 1 create new loss function? better example example 4 pairs coordinate allocated 1x8 array output network. want create euclidean loos function compare 4 distance result comparing every point array. problem know must use keras backend operation. proble iterate tensor? example taking mean 4 distance still error, know maybe tensor must use backend operation, know do. 2 create new metrics, example define custom metric accuracy? like case 3 monitor using new metrics save best weight using custom metrics? suggestion welcomed. thank you.",0,new loss function metric monitor best weights,"new loss function metric monitor best weights hello still exploring keras past months new here. like much thanks fchollet . 3 problem now. 1 create new loss function? better example example 4 pairs coordinate allocated 1x8 array output network. want create euclidean loos function compare 4 distance result comparing every point array. problem know must use keras backend operation. proble iterate tensor? example taking mean 4 distance still error, know maybe tensor must use backend operation, know do. 2 create new metrics, example define custom metric accuracy? like case 3 monitor using new metrics save best weight using custom metrics? suggestion welcomed. thank you."
keras,1872,saved model weights epoch using callbacks.modelcheckpoint. want train last epoch. set model.fit command start previous epoch?,0,resume training previous epoch,resume training previous epoch saved model weights epoch using callbacks.modelcheckpoint. want train last epoch. set model.fit command start previous epoch?
keras,4869,please advice object detection using keras?,0,object detection using keras,object detection using keras please advice object detection using keras?
keras,4479,predicting give different vectors,0,model gives answers,model gives answers predicting give different vectors
keras,1643,"hello, actually new python, wondering way use taps keras? e.g. say, want compute, t.dot w xi, x t.dot w hi1, h tm1 t.dot w hi2,h tm2 h tm1 h 1 h tm2 h 2 quite straightforward theano, use output info, specify taps.",0,"help way set taps scan op keras, lstm application","help way set taps scan op keras, lstm application hello, actually new python, wondering way use taps keras? e.g. say, want compute, t.dot w xi, x t.dot w hi1, h tm1 t.dot w hi2,h tm2 h tm1 h 1 h tm2 h 2 quite straightforward theano, use output info, specify taps."
keras,8642,use different sizes strides max pooling 2x2 . use problem changes happen model like accuracy prediction changes?,0,use 2x2 strides size maxpooling model?,use 2x2 strides size maxpooling model? use different sizes strides max pooling 2x2 . use problem changes happen model like accuracy prediction changes?
keras,1103,can't figure fix travis config install tf run tests tf backend python 2.7 . travis experts challenge? see branch https github.com fchollet keras blob backend .travis.yml,0,fixing travis config tensorflow,fixing travis config tensorflow can't figure fix travis config install tf run tests tf backend python 2.7 . travis experts challenge? see branch https github.com fchollet keras blob backend .travis.yml
keras,4965,"using latest versions keras theano macbook pro discrete amd graphics. works fine cpu, getting error try use opencl backend description exception 30000 characters long removed let know useful code",0,pygpu.gpuarray.gpuarrayexception theano opencl backend,"pygpu.gpuarray.gpuarrayexception theano opencl backend using latest versions keras theano macbook pro discrete amd graphics. works fine cpu, getting error try use opencl backend description exception 30000 characters long removed let know useful code"
keras,3866,"developed ml app local machine wanted deployment aws ec2 cpu instance time talk predict flow installation packages specifically keras theano backend, able call python ubuntu 14 04 python version 2.7.6 packages nicely. so, example, interactively call python import keras, get standard print 'using theano backend.' desired set keras.json file now, installed flask server apache2 server working wsgi package. test server several types routes returns working nicley. flask file looking essentially like predict.py file python directory looks like so, start apache server look logs logs tell 'using tensorflow backend.' want. anybody give hint happening many thanks peter",0,keras aws ec2,"keras aws ec2 developed ml app local machine wanted deployment aws ec2 cpu instance time talk predict flow installation packages specifically keras theano backend, able call python ubuntu 14 04 python version 2.7.6 packages nicely. so, example, interactively call python import keras, get standard print 'using theano backend.' desired set keras.json file now, installed flask server apache2 server working wsgi package. test server several types routes returns working nicley. flask file looking essentially like predict.py file python directory looks like so, start apache server look logs logs tell 'using tensorflow backend.' want. anybody give hint happening many thanks peter"
keras,4168,"possible get training set predictions model end epoch inside function epoch end end callback without call self.model.predict every time, since model already computed them?",0,get current epoch predictions inside callback epoch end,"get current epoch predictions inside callback epoch end possible get training set predictions model end epoch inside function epoch end end callback without call self.model.predict every time, since model already computed them?"
keras,1556,e.g. https travis ci.org fchollet keras jobs 104864770,0,travis ci tests failing due connection errors data possible aws problems?,travis ci tests failing due connection errors data possible aws problems? e.g. https travis ci.org fchollet keras jobs 104864770
keras,649,"noticed tests convolution1d, side effect code coverage metrics.",0,test coverage keras,"test coverage keras noticed tests convolution1d, side effect code coverage metrics."
keras,5792,"hello, wondering line 411 413 correct ? https github.com fchollet keras blob master keras layers local.py l412 l413 output k.reshape output, self.output row, self.output col, 1, filters output dimension output row x output col x batch size x filters output k.permute dimensions output, 2, 0, 1, 3 1,2,0,3 ? thanks !",0,small bugs local.py locallyconnected2d?,"small bugs local.py locallyconnected2d? hello, wondering line 411 413 correct ? https github.com fchollet keras blob master keras layers local.py l412 l413 output k.reshape output, self.output row, self.output col, 1, filters output dimension output row x output col x batch size x filters output k.permute dimensions output, 2, 0, 1, 3 1,2,0,3 ? thanks !"
keras,131,fix probably change line 188.,0,model.fit shuffle false gives typeerror,model.fit shuffle false gives typeerror fix probably change line 188.
keras,7453,deep deep model 200 layers giving error,0,"node index deep model, recursion depth exceeded","node index deep model, recursion depth exceeded deep deep model 200 layers giving error"
keras,4010,"hi, trying model stateful lstm tts system. added however, getting following setup ideas? thanks!",0,stateful lstm dimensions using input batch shape?,"stateful lstm dimensions using input batch shape? hi, trying model stateful lstm tts system. added however, getting following setup ideas? thanks!"
keras,12724,"system information written custom code opposed using example directory yes os platform distribution e.g., linux ubuntu 16.04 ubuntu 18.04 tensorflow backend yes yes tensorflow version 1.13.1 keras version 2.4.4 bug exists latest master well python version 3.6.8 cuda cudnn version using cuda gpu model memory using cuda describe current behavior set random seeds, use cpu, disable cpu multiprocessing, run experiment 10 times, loss second call comes one two different values time. describe expected behavior loss bit perfect reproducible situation. code reproduce issue check reproducibility following command python3 code example.py 2 dev null tail n 1 done result info logs variety things affect whether training bit perfect reproducible loss always consistent first . training three steps exhibits identical behavior training two steps losses come one two values commenting various layers restores reproducibility see comments code example eliminating decay optimizer using optimizer decay restores reproducibility metric selection affects reproducibility. metrics break reproducibility included twice, one metric, breaks reproducibility included all. noticed change behavior used git master branch. inserting graph, using rmsprop decay, restore reproducibility. patch",0,keras appears generate non deterministic tensorflow graph,"keras appears generate non deterministic tensorflow graph system information written custom code opposed using example directory yes os platform distribution e.g., linux ubuntu 16.04 ubuntu 18.04 tensorflow backend yes yes tensorflow version 1.13.1 keras version 2.4.4 bug exists latest master well python version 3.6.8 cuda cudnn version using cuda gpu model memory using cuda describe current behavior set random seeds, use cpu, disable cpu multiprocessing, run experiment 10 times, loss second call comes one two different values time. describe expected behavior loss bit perfect reproducible situation. code reproduce issue check reproducibility following command python3 code example.py 2 dev null tail n 1 done result info logs variety things affect whether training bit perfect reproducible loss always consistent first . training three steps exhibits identical behavior training two steps losses come one two values commenting various layers restores reproducibility see comments code example eliminating decay optimizer using optimizer decay restores reproducibility metric selection affects reproducibility. metrics break reproducibility included twice, one metric, breaks reproducibility included all. noticed change behavior used git master branch. inserting graph, using rmsprop decay, restore reproducibility. patch"
keras,5880,,0,"hello, ask build 3d convolution autoencoder?","hello, ask build 3d convolution autoencoder? "
keras,11881,"hi, wanted train u net, everything works fine laptop tensorflow cpu version. however, cluster gpu, create compile it, call method get following error traceback recent call last file cluster.py , line 46, unet.fit generator gen, steps per epoch gen.getstepsperepoch , epochs 101 file home exacloud tempwork changlab guillaume python lib python3.5 site packages keras legacy interfaces.py , line 91, wrapper return func args, kwargs file home exacloud tempwork changlab guillaume python lib python3.5 site packages keras engine training.py , line 1418, fit generator initial epoch initial epoch file home exacloud tempwork changlab guillaume python lib python3.5 site packages keras engine training generator.py , line 40, fit generator model. make train function file home exacloud tempwork changlab guillaume python lib python3.5 site packages keras engine training.py , line 509, make train function loss self.total loss file home exacloud tempwork changlab guillaume python lib python3.5 site packages keras legacy interfaces.py , line 91, wrapper return func args, kwargs file home exacloud tempwork changlab guillaume python lib python3.5 site packages keras optimizers.py , line 410, get updates self.updates.append k.update a, new file home exacloud tempwork changlab guillaume python lib python3.5 site packages keras backend tensorflow backend.py , line 973, update return tf.assign x, new x file home exacloud tempwork changlab guillaume conda dl35 lib python3.5 site packages tensorflow python ops state ops.py , line 277, assign return ref.assign value attributeerror 'tensor' object attribute 'assign' exact code runs perfectly laptop cpu version tensorflow, gpu one. idea issue may be? thanks advance!",0,code runs fine cpu attributeerror gpu,"code runs fine cpu attributeerror gpu hi, wanted train u net, everything works fine laptop tensorflow cpu version. however, cluster gpu, create compile it, call method get following error traceback recent call last file cluster.py , line 46, unet.fit generator gen, steps per epoch gen.getstepsperepoch , epochs 101 file home exacloud tempwork changlab guillaume python lib python3.5 site packages keras legacy interfaces.py , line 91, wrapper return func args, kwargs file home exacloud tempwork changlab guillaume python lib python3.5 site packages keras engine training.py , line 1418, fit generator initial epoch initial epoch file home exacloud tempwork changlab guillaume python lib python3.5 site packages keras engine training generator.py , line 40, fit generator model. make train function file home exacloud tempwork changlab guillaume python lib python3.5 site packages keras engine training.py , line 509, make train function loss self.total loss file home exacloud tempwork changlab guillaume python lib python3.5 site packages keras legacy interfaces.py , line 91, wrapper return func args, kwargs file home exacloud tempwork changlab guillaume python lib python3.5 site packages keras optimizers.py , line 410, get updates self.updates.append k.update a, new file home exacloud tempwork changlab guillaume python lib python3.5 site packages keras backend tensorflow backend.py , line 973, update return tf.assign x, new x file home exacloud tempwork changlab guillaume conda dl35 lib python3.5 site packages tensorflow python ops state ops.py , line 277, assign return ref.assign value attributeerror 'tensor' object attribute 'assign' exact code runs perfectly laptop cpu version tensorflow, gpu one. idea issue may be? thanks advance!"
keras,3518,"sometimes need set threshold early stopping. threshold minimum amount improvement considered new maximum performance. please add comment think would need it, too!",0,survey need threshold early stopping?,"survey need threshold early stopping? sometimes need set threshold early stopping. threshold minimum amount improvement considered new maximum performance. please add comment think would need it, too!"
keras,2801,"lambda layer like split tensor two opposite k.concatenate, essentially perform different operations two parts, concatenating again. thoughts split keras backend?",0,split tensor,"split tensor lambda layer like split tensor two opposite k.concatenate, essentially perform different operations two parts, concatenating again. thoughts split keras backend?"
keras,9941,"hello, created layer follows keras import backend k keras.engine.topology import layer import numpy np import cv2 class medianlayer layer def init self, output dim, input shape, kwargs self.output dim output dim super medianlayer, self . init kwargs def build self, input shape create trainable weight variable layer. self.kernel self.add weight name 'kernel', shape input shape 1 , self.output dim , initializer 'uniform', trainable true super medianlayer, self .build input shape sure call somewhere! def call self, x return cv2.medianblur x,5 x def compute output shape self, input shape return input shape 0 , self.output dim this, following k sequential layer medianlayer output dim 100,100 , input shape 100,100 lc layer.get config x np.random.randn 100, 100 lc input shape x.shape lc output dim x.shape print lc prints 'name' 'median layer 3', 'trainable' true, 'input shape' 100, 100 , 'output dim' 100, 100 expected. now, instantiate new layer object old config layer1 layer.from config lc print layer1.get config print lc get following 'name' 'median layer 3', 'trainable' true new layer config 'name' 'median layer 3', 'trainable' true, 'input shape' 100, 100 , 'output dim' 100, 100 old layer config, used input instantiation. instantiation another layer object using existing layer config failing clues highly appreciated. thank you. kumar",0,new layer instantiation config failing.,"new layer instantiation config failing. hello, created layer follows keras import backend k keras.engine.topology import layer import numpy np import cv2 class medianlayer layer def init self, output dim, input shape, kwargs self.output dim output dim super medianlayer, self . init kwargs def build self, input shape create trainable weight variable layer. self.kernel self.add weight name 'kernel', shape input shape 1 , self.output dim , initializer 'uniform', trainable true super medianlayer, self .build input shape sure call somewhere! def call self, x return cv2.medianblur x,5 x def compute output shape self, input shape return input shape 0 , self.output dim this, following k sequential layer medianlayer output dim 100,100 , input shape 100,100 lc layer.get config x np.random.randn 100, 100 lc input shape x.shape lc output dim x.shape print lc prints 'name' 'median layer 3', 'trainable' true, 'input shape' 100, 100 , 'output dim' 100, 100 expected. now, instantiate new layer object old config layer1 layer.from config lc print layer1.get config print lc get following 'name' 'median layer 3', 'trainable' true new layer config 'name' 'median layer 3', 'trainable' true, 'input shape' 100, 100 , 'output dim' 100, 100 old layer config, used input instantiation. instantiation another layer object using existing layer config failing clues highly appreciated. thank you. kumar"
keras,12986,"keras provide way get output nested model? way flatten nested model? model architecture below, 'get layer1' 'layer2'? compile m2, see 'layer3' timedistributed layer. system information written custom code opposed using example directory os platform distribution e.g., linux ubuntu 16.04 mac os tensorflow backend yes yes tensorflow version b'v1.13.0 rc2 5 g6612da8951' 1.13.1 keras version 2.2.4 python version 3.6.5 cuda cudnn version gpu model memory",0,obtain layer output nested model keras,"obtain layer output nested model keras keras provide way get output nested model? way flatten nested model? model architecture below, 'get layer1' 'layer2'? compile m2, see 'layer3' timedistributed layer. system information written custom code opposed using example directory os platform distribution e.g., linux ubuntu 16.04 mac os tensorflow backend yes yes tensorflow version b'v1.13.0 rc2 5 g6612da8951' 1.13.1 keras version 2.2.4 python version 3.6.5 cuda cudnn version gpu model memory"
keras,1309,"hi, added batchnormalization layers model suddenly took much time train. takes 561 seconds one epoch epoch 1 1 4096 4096 561s loss 0.0946 acc 0.9006 comment batchnorm layers 186 seconds epoch 1 1 4096 4096 186s loss 4.5043 acc 0.5933 wrote user group someone informed adding dropout slows further, even without dropout much slower 500 seconds . cpu related issue? slowdown expected? model.",0,slow batchnormalization layers cpu tested,"slow batchnormalization layers cpu tested hi, added batchnormalization layers model suddenly took much time train. takes 561 seconds one epoch epoch 1 1 4096 4096 561s loss 0.0946 acc 0.9006 comment batchnorm layers 186 seconds epoch 1 1 4096 4096 186s loss 4.5043 acc 0.5933 wrote user group someone informed adding dropout slows further, even without dropout much slower 500 seconds . cpu related issue? slowdown expected? model."
keras,6243,"since upgrading keras 2 latest theano last updated jan '17 , train batch call takes 10 second pause approximately first 10 batches convolutional layers. reason pause, way get around it?",0,"inefficient, newly introduced pause train batch","inefficient, newly introduced pause train batch since upgrading keras 2 latest theano last updated jan '17 , train batch call takes 10 second pause approximately first 10 batches convolutional layers. reason pause, way get around it?"
keras,6849,"trying use keras' implementation vgg16 grayscale images, channels first. training data shaped follows build model throws error input shape ' str input shape 'input shape 1, 128, 128 seems odd since using still seems expect 3 channel tensor passing shape , seems inflexible allowing anything 3 channels see . desirable? keep get shape mismatch error beginning training epoch 1. keras 2.0.4 python 3.5",0,error checking vgg16 input shape include top false,"error checking vgg16 input shape include top false trying use keras' implementation vgg16 grayscale images, channels first. training data shaped follows build model throws error input shape ' str input shape 'input shape 1, 128, 128 seems odd since using still seems expect 3 channel tensor passing shape , seems inflexible allowing anything 3 channels see . desirable? keep get shape mismatch error beginning training epoch 1. keras 2.0.4 python 3.5"
keras,6893,"run keras examples babi rnn.py script using runs fine, wanted export model according example code found https blog.keras.io keras simplified interface tensorflow tutorial.html exporting model tensorflow serving get following error. know what's going wrong?",0,error exporting model keras examples babi rnn.py,"error exporting model keras examples babi rnn.py run keras examples babi rnn.py script using runs fine, wanted export model according example code found https blog.keras.io keras simplified interface tensorflow tutorial.html exporting model tensorflow serving get following error. know what's going wrong?"
keras,1854,getting error trying change argument inside lstm function always exist typo changed ? keras documentation. thanks!,0,lstm argument,lstm argument getting error trying change argument inside lstm function always exist typo changed ? keras documentation. thanks!
keras,11410,"getting error try load resnet50 try load xception get input shape 48, 48, 3 please check code sample https gist.github.com hasibzunair 606c9037abed51836e5dc6059141bfff",0,connectionreseterror errno 104 connection reset peer,"connectionreseterror errno 104 connection reset peer getting error try load resnet50 try load xception get input shape 48, 48, 3 please check code sample https gist.github.com hasibzunair 606c9037abed51836e5dc6059141bfff"
keras,2293,"master branch seems compatible previous version's model dump file, using json read old model file, there's bug",0,master branch seems compatible,"master branch seems compatible master branch seems compatible previous version's model dump file, using json read old model file, there's bug"
keras,11673,"current documentation callbacks https keras.io callbacks showing bullet points correctly arguments section models. here's example filepath string, path save model file. monitor quantity monitor. verbose verbosity mode, 0 1. save best save best true, latest best model according quantity monitored overwritten. mode one auto, min, max . save best true, decision overwrite current save file made based either maximization minimization monitored quantity. val acc, max, val loss min, etc. auto mode, direction automatically inferred name monitored quantity. save weights true, model's weights saved model.save weights filepath , else full model saved model.save filepath . period interval number epochs checkpoints. looking source code, docstring seems organized correctly https github.com keras team keras blob dc9e510192d0a8a6f6943cd46e9554364d4dcdd2 keras callbacks.py l371 l390 however showing correctly models, e.g. arguments count mode one steps samples . whether progress bar count samples seen steps batches seen. stateful metrics iterable string names metrics averaged epoch. metrics list logged is. others averaged time e.g. loss, etc .",0,callbacks documentation showing bullet points correctly,"callbacks documentation showing bullet points correctly current documentation callbacks https keras.io callbacks showing bullet points correctly arguments section models. here's example filepath string, path save model file. monitor quantity monitor. verbose verbosity mode, 0 1. save best save best true, latest best model according quantity monitored overwritten. mode one auto, min, max . save best true, decision overwrite current save file made based either maximization minimization monitored quantity. val acc, max, val loss min, etc. auto mode, direction automatically inferred name monitored quantity. save weights true, model's weights saved model.save weights filepath , else full model saved model.save filepath . period interval number epochs checkpoints. looking source code, docstring seems organized correctly https github.com keras team keras blob dc9e510192d0a8a6f6943cd46e9554364d4dcdd2 keras callbacks.py l371 l390 however showing correctly models, e.g. arguments count mode one steps samples . whether progress bar count samples seen steps batches seen. stateful metrics iterable string names metrics averaged epoch. metrics list logged is. others averaged time e.g. loss, etc ."
keras,3667,hi there! way could fix error? caused batchnormalization layer.. happens run following code machine configuration operating system opensuse theano version '0.9.0dev2' keras version 1.0.8 cudnn version v5.1 cuda version 7.5 thanks!,0,valueerror caused batchnormalization layer,valueerror caused batchnormalization layer hi there! way could fix error? caused batchnormalization layer.. happens run following code machine configuration operating system opensuse theano version '0.9.0dev2' keras version 1.0.8 cudnn version v5.1 cuda version 7.5 thanks!
keras,8823,"model work lstm layer. works fine encoder inputs input shape none, num encoder tokens encoder lstm latent dim, return state true encoder outputs, state h, state c encoder encoder inputs add wrap bidirectional layer encoder inputs input shape none, num encoder tokens encoder bidirectional lstm latent dim, return state true ,merge mode mul encoder outputs, state h, state c encoder encoder inputs lead error output rev typeerror can't multiply sequence non int type 'list'. get output, state memory bidirectional layer ? remove , also works",0,get state memory lstm layer wrap bidirectional layer ?,"get state memory lstm layer wrap bidirectional layer ? model work lstm layer. works fine encoder inputs input shape none, num encoder tokens encoder lstm latent dim, return state true encoder outputs, state h, state c encoder encoder inputs add wrap bidirectional layer encoder inputs input shape none, num encoder tokens encoder bidirectional lstm latent dim, return state true ,merge mode mul encoder outputs, state h, state c encoder encoder inputs lead error output rev typeerror can't multiply sequence non int type 'list'. get output, state memory bidirectional layer ? remove , also works"
keras,4630,"short see rescale option imagedatagenerator. following pertained vgg16 network needs channels values preprocessed follows cv2.resize cv2.imread 'cat.jpg' , 224, 224 .astype np.float32 , ,0 103.939 , ,1 116.779 , ,2 123.68 im.transpose 2,0,1 np.expand dims im, axis 0 specify options imagedatagenerator? thanks dr",0,allow preprocessing image channel values imagedatagenerator flow directory?,"allow preprocessing image channel values imagedatagenerator flow directory? short see rescale option imagedatagenerator. following pertained vgg16 network needs channels values preprocessed follows cv2.resize cv2.imread 'cat.jpg' , 224, 224 .astype np.float32 , ,0 103.939 , ,1 116.779 , ,2 123.68 im.transpose 2,0,1 np.expand dims im, axis 0 specify options imagedatagenerator? thanks dr"
keras,1852,"theory dropout layer keras hinton g., n. srivastava, a. krizhevsky, i. sutskever, r. salakhutdinov. 2012. improving neural networks preventing co adaptation feature detectors. researchgate. ?",0,"theory dropout layer keras refer hinton et al., 2012 ?","theory dropout layer keras refer hinton et al., 2012 ? theory dropout layer keras hinton g., n. srivastava, a. krizhevsky, i. sutskever, r. salakhutdinov. 2012. improving neural networks preventing co adaptation feature detectors. researchgate. ?"
keras,12354,"getting weird error try create network using upsampling layer, manually set interpolate keyword bilinear. leave out, go default 'nearest neighbour works fine. anyone know what's up? tf 1.12.0 keras 2.2.2 os ubuntu 18.04 code model. error thrown layer 'up1' chnl4 input input shape 368, 256, 4 chnl3 input input shape 736, 512, 3 conv1 conv2d 26, self.kernel size, activation 'relu', padding 'same' chnl4 input conv2 conv2d 26, self.kernel size, strides 2, 2 , activation 'relu', padding 'same' conv1 conv5 conv2d 64, self.kernel size, activation 'relu', padding 'same' conv2 conv6 conv2d 64, self.kernel size, activation 'relu', padding 'same' conv5 up1 concatenate upsampling2d size 2, 2 , interpolation 'bilinear' conv6 , conv1 , axis 1 conv7 conv2d 64, self.kernel size, activation 'relu', padding 'same' up1 conv8 conv2d 64, self.kernel size, activation 'relu', padding 'same' conv7 conv9 conv2d 64, self.kernel size, activation 'relu', padding 'same' conv8 conv11 conv2d 64, self.kernel size, activation 'relu', padding 'same' conv9 conv12 conv2d 64, self.kernel size, activation 'relu', padding 'same' conv11 up3 concatenate upsampling2d size 2, 2 , interpolation 'bilinear' conv12 , chnl3 input , axis 1 conv13 conv2d 67, self.kernel size, activation 'relu', padding 'same' up3 conv14 conv2d 67, self.kernel size, activation 'relu', padding 'same' conv13 conv15 conv2d 32, self.kernel size, activation 'relu', padding 'same' conv14 conv16 conv2d 3, self.kernel size, activation 'relu', padding 'same' conv15 conv16 self.model model inputs chnl4 input, chnl3 input , outputs self.model.compile optimizer self.optimizer func, loss self.loss func self.model.name 'unet' return self.modele errror typeerror 'keyword argument understood ', 'interpolation'",0,upsampling2d throwing error keyword 'interpolation',"upsampling2d throwing error keyword 'interpolation' getting weird error try create network using upsampling layer, manually set interpolate keyword bilinear. leave out, go default 'nearest neighbour works fine. anyone know what's up? tf 1.12.0 keras 2.2.2 os ubuntu 18.04 code model. error thrown layer 'up1' chnl4 input input shape 368, 256, 4 chnl3 input input shape 736, 512, 3 conv1 conv2d 26, self.kernel size, activation 'relu', padding 'same' chnl4 input conv2 conv2d 26, self.kernel size, strides 2, 2 , activation 'relu', padding 'same' conv1 conv5 conv2d 64, self.kernel size, activation 'relu', padding 'same' conv2 conv6 conv2d 64, self.kernel size, activation 'relu', padding 'same' conv5 up1 concatenate upsampling2d size 2, 2 , interpolation 'bilinear' conv6 , conv1 , axis 1 conv7 conv2d 64, self.kernel size, activation 'relu', padding 'same' up1 conv8 conv2d 64, self.kernel size, activation 'relu', padding 'same' conv7 conv9 conv2d 64, self.kernel size, activation 'relu', padding 'same' conv8 conv11 conv2d 64, self.kernel size, activation 'relu', padding 'same' conv9 conv12 conv2d 64, self.kernel size, activation 'relu', padding 'same' conv11 up3 concatenate upsampling2d size 2, 2 , interpolation 'bilinear' conv12 , chnl3 input , axis 1 conv13 conv2d 67, self.kernel size, activation 'relu', padding 'same' up3 conv14 conv2d 67, self.kernel size, activation 'relu', padding 'same' conv13 conv15 conv2d 32, self.kernel size, activation 'relu', padding 'same' conv14 conv16 conv2d 3, self.kernel size, activation 'relu', padding 'same' conv15 conv16 self.model model inputs chnl4 input, chnl3 input , outputs self.model.compile optimizer self.optimizer func, loss self.loss func self.model.name 'unet' return self.modele errror typeerror 'keyword argument understood ', 'interpolation'"
keras,4262,"trying build custom function keras shown https github.com fchollet keras blob master keras objectives.py however, get error ipython python 2.7.12 anaconda 4.2.0 x86 64 default, jul 2 2016, 17 43 17 type copyright , credits license information. ipython 5.1.0 enhanced interactive python. ? introduction overview ipython's features. quickref quick reference. help python's help system. object? details 'object', use 'object??' extra details. 1 import keras using tensorflow backend. 2 . import backend k valueerror traceback recent call last 1 . import backend k valueerror attempted relative import non package help? thank much.",0,cannot import backend keras,"cannot import backend keras trying build custom function keras shown https github.com fchollet keras blob master keras objectives.py however, get error ipython python 2.7.12 anaconda 4.2.0 x86 64 default, jul 2 2016, 17 43 17 type copyright , credits license information. ipython 5.1.0 enhanced interactive python. ? introduction overview ipython's features. quickref quick reference. help python's help system. object? details 'object', use 'object??' extra details. 1 import keras using tensorflow backend. 2 . import backend k valueerror traceback recent call last 1 . import backend k valueerror attempted relative import non package help? thank much."
keras,10693,"hello. new keras. using tensorflow backend. like load many models use different threads. different threads, use different graphs, use different sessions. right? trimmed version code reproduce error. wrong? using keras 2.2.0 tensorflow 1.8.0 without gpu ubuntu 16.04.4. import tensorflow tf import numpy np keras.models import sequential keras.layers import masking def predict model sequential layer masking batch input shape none, 1 model.add layer x np.array 0 , dtype 'int32' model.predict x, batch size 1 name ' main ' tf.session predict tf.session predict",0,segmentation fault core dumped happened use one session.,"segmentation fault core dumped happened use one session. hello. new keras. using tensorflow backend. like load many models use different threads. different threads, use different graphs, use different sessions. right? trimmed version code reproduce error. wrong? using keras 2.2.0 tensorflow 1.8.0 without gpu ubuntu 16.04.4. import tensorflow tf import numpy np keras.models import sequential keras.layers import masking def predict model sequential layer masking batch input shape none, 1 model.add layer x np.array 0 , dtype 'int32' model.predict x, batch size 1 name ' main ' tf.session predict tf.session predict"
keras,13006,"system information written custom code opposed using example directory yes os platform distribution linux ubuntu 4.15.0 51 generic 16 18.04.1 ubuntu smp tensorflow backend yes tensorflow version 1.13.1 keras version 2.2.4 python version 3.6.8 used model provided https medium.com ksusorokina image classification convolutional neural networks 496815db12a8 trains model two categories pictures tries classify them. furthermore, force network use seeds training get comparable results. also create close tf sessions read may also cause problems. describe current behaviour time test validation accuracy converge around 0.5 loss stays exactly value every epoch. run code, without changing it, several times row get problem rarely get run neural network trained properly rises 0.9 accuracy training validation. describe expected behaviour seeds run produce least similar results i.e. stuck around 0.5 accuracy. code reproduce issue edit tried different tutorial https www.geeksforgeeks.org python image classification using keras get result. changes done code change image dimension size images, changed image directories turn image augmentation, well changed number images. half runs produced output half produced numbers always exactly . though maybe bad images tried use images tutorials like https github.com perseus784 bvs got results half runs useless converge 0.5 accuracy. what's going on?",0,keras training validation accuracy always converges 0.5,"keras training validation accuracy always converges 0.5 system information written custom code opposed using example directory yes os platform distribution linux ubuntu 4.15.0 51 generic 16 18.04.1 ubuntu smp tensorflow backend yes tensorflow version 1.13.1 keras version 2.2.4 python version 3.6.8 used model provided https medium.com ksusorokina image classification convolutional neural networks 496815db12a8 trains model two categories pictures tries classify them. furthermore, force network use seeds training get comparable results. also create close tf sessions read may also cause problems. describe current behaviour time test validation accuracy converge around 0.5 loss stays exactly value every epoch. run code, without changing it, several times row get problem rarely get run neural network trained properly rises 0.9 accuracy training validation. describe expected behaviour seeds run produce least similar results i.e. stuck around 0.5 accuracy. code reproduce issue edit tried different tutorial https www.geeksforgeeks.org python image classification using keras get result. changes done code change image dimension size images, changed image directories turn image augmentation, well changed number images. half runs produced output half produced numbers always exactly . though maybe bad images tried use images tutorials like https github.com perseus784 bvs got results half runs useless converge 0.5 accuracy. what's going on?"
keras,4696,"hi, new keras trying implements model part includes encoding sentences using convolution network. import statements keras.layers.embeddings import embedding keras.layers.wrappers import timedistributed keras.layers.convolutional import convolution1d keras.layers.pooling import maxpooling1d keras.engine.topology import merge keras.layers.core import dropout, repeatvector keras.layers import lstm, input, dense keras.models import model, sequential ipython.display import svg def conv encoder nb filters, filter len, shape model encode sentence particular filter size nb filters number filters, filter len size filter, shape input shape , sent len, word len model sequential model.add convolution1d nb filters, filter len, border mode 'same', input shape shape model.add maxpooling1d pool length 2, stride none, border mode 'same' return model def make sentence encoder nb filters, filter lens, shape model encode sentence different filter sizes basically, learns different types representations based filter sizes like unigram, bigram, trigram etc. models flt filter lens models.append conv encoder nb filters, flt, shape merged model sequential merged model.add merge models, mode 'sum', concat axis 1 return merged model def encode sentences vocab size, dense emb dim, doc maxlen, sent maxlen, nb filters, filter lens, shape model encodes sentences using conv nets. returns encoded set sentences. input document, shape , doc len, sent len output shape , doc len, new sent len setup sentence encoder sent encoder make sentence encoder nb filters, filter lens, shape embed input sequence sequence vectors sentences encoder sequential initialize embedding layer wordvec sentences encoder.add timedistributed embedding input dim vocab size, output dim dense emb dim, mask zero true , input shape doc maxlen, sent maxlen , input dtype 'int32' sentences encoder.add timedistributed dropout 0.3 sentences encoder.add timedistributed sent encoder return sentences encoder test code inputs input shape 50, 170 sents enc encode sentences 1000, 60, 50, 170, 32, 1, 2, 3 , 170, 60 print sents enc.output shape sents enc inputs model model input inputs, output line method source problem. guess takes input list size equal len filter lens i.e total number different filters used. know confused pass here. help highly appreciated. thanks. error thrown",0,"assertionerror could compute output elemwise add,no inplace .0","assertionerror could compute output elemwise add,no inplace .0 hi, new keras trying implements model part includes encoding sentences using convolution network. import statements keras.layers.embeddings import embedding keras.layers.wrappers import timedistributed keras.layers.convolutional import convolution1d keras.layers.pooling import maxpooling1d keras.engine.topology import merge keras.layers.core import dropout, repeatvector keras.layers import lstm, input, dense keras.models import model, sequential ipython.display import svg def conv encoder nb filters, filter len, shape model encode sentence particular filter size nb filters number filters, filter len size filter, shape input shape , sent len, word len model sequential model.add convolution1d nb filters, filter len, border mode 'same', input shape shape model.add maxpooling1d pool length 2, stride none, border mode 'same' return model def make sentence encoder nb filters, filter lens, shape model encode sentence different filter sizes basically, learns different types representations based filter sizes like unigram, bigram, trigram etc. models flt filter lens models.append conv encoder nb filters, flt, shape merged model sequential merged model.add merge models, mode 'sum', concat axis 1 return merged model def encode sentences vocab size, dense emb dim, doc maxlen, sent maxlen, nb filters, filter lens, shape model encodes sentences using conv nets. returns encoded set sentences. input document, shape , doc len, sent len output shape , doc len, new sent len setup sentence encoder sent encoder make sentence encoder nb filters, filter lens, shape embed input sequence sequence vectors sentences encoder sequential initialize embedding layer wordvec sentences encoder.add timedistributed embedding input dim vocab size, output dim dense emb dim, mask zero true , input shape doc maxlen, sent maxlen , input dtype 'int32' sentences encoder.add timedistributed dropout 0.3 sentences encoder.add timedistributed sent encoder return sentences encoder test code inputs input shape 50, 170 sents enc encode sentences 1000, 60, 50, 170, 32, 1, 2, 3 , 170, 60 print sents enc.output shape sents enc inputs model model input inputs, output line method source problem. guess takes input list size equal len filter lens i.e total number different filters used. know confused pass here. help highly appreciated. thanks. error thrown"
keras,4395,"hello, sure proper place ask question keras. apologize advance not. currently trying reproduce publication using recurrent neural nets predict chemical activities. goulon, a., t. picot, a. duprat, g. dreyfus. predicting activities without computing descriptors graph machines qsar. sar qsar environmental research 18, no. 1 2 january 1, 2007 141 53. doi 10.1080 10629360601054313. two words autors represent molecule directed acyclic graph. node graph atom edge chemical bond. neural network applied exit node called root node publication . neural network calls parents root node provide prediction whole molecule. think understand training works graph machine molecule, gradient cost function calculated backpropagation molecule. gradient batch molecules weighted average gradient calculated molecule composing batch. gradient descent algorithm used minimize cost function. main trouble implement training procedure keras ? usually, provide list x list corresponding use model.compile model.fit . understand, possible molecule keras.model keras even best solution ? best regards,",0,question graph machines keras,"question graph machines keras hello, sure proper place ask question keras. apologize advance not. currently trying reproduce publication using recurrent neural nets predict chemical activities. goulon, a., t. picot, a. duprat, g. dreyfus. predicting activities without computing descriptors graph machines qsar. sar qsar environmental research 18, no. 1 2 january 1, 2007 141 53. doi 10.1080 10629360601054313. two words autors represent molecule directed acyclic graph. node graph atom edge chemical bond. neural network applied exit node called root node publication . neural network calls parents root node provide prediction whole molecule. think understand training works graph machine molecule, gradient cost function calculated backpropagation molecule. gradient batch molecules weighted average gradient calculated molecule composing batch. gradient descent algorithm used minimize cost function. main trouble implement training procedure keras ? usually, provide list x list corresponding use model.compile model.fit . understand, possible molecule keras.model keras even best solution ? best regards,"
keras,10179,"import numpy np import tensorflow tf tf.set random seed 49999 maxlen 2 word dim 2 n rnn 3 x np.array range 4 , dtype np.float32 .reshape 1, 2, 2 x r np.flip x, axis 1 inputs r tf.constant x r, dtype tf.float32 seq len tf.constant 2 , dtype tf.int32 cell fw tf.nn.rnn cell.lstmcell n rnn, name fw cell bw tf.nn.rnn cell.lstmcell n rnn, name bw tf forward tf hs fw, tf c fw, tf h fw tf.nn.dynamic rnn cell fw, inputs r, seq len, dtype tf.float32 tf backward tf hs bw, tf c bw, tf h bw tf.nn.dynamic rnn cell bw, inputs r, seq len, dtype tf.float32 sess tf.interactivesession init tf.global variables initializer sess.run init print tf.trainable variables tf w v.eval v tf.trainable variables v tf h fw tf h fw.eval 0 v tf h bw tf h bw.eval 0 def tf2keras params, reshape dim 0, 2, 1, 3 , dim 2, rnn dim 3 params 5, 12 , 12, , 5, 12 , 12, assert dim rnn dim params 0 .shape 0 len params 2 tf lstm params 0 dim, , params 0 dim , , params 1 else 4 tf lstm params 0 dim, , params 0 dim , , params 1 , params 2 dim, , params 2 dim , , params 3 tf lstm 2, 12 , 3, 12 , 12, , 2, 12 , 3, 12 , 12, def dim recombination x, reshape dim i, j, f, i, f, c, j c rst none len x.shape 1 i, j, f, tmp x 0 rnn dim 1 , x rnn dim 1 rnn dim 2 , x rnn dim 2 rnn dim 3 , x rnn dim 3 rnn dim 4 rst np.hstack tmp reshape dim elif len x.shape 2 tmp x , 0 rnn dim 1 , x , rnn dim 1 rnn dim 2 , x , rnn dim 2 rnn dim 3 , x , rnn dim 3 rnn dim 4 rst np.hstack tmp reshape dim else print xxxxx return rst keras w dim recombination v, reshape dim v tf lstm return keras w k params tf2keras tf w, 0, 2, 1, 3 keras keras.layers import input keras.layers import lstm keras.models import model keras input input shape maxlen, word dim , dtype 'float32', name 'input layer' k lstm1 lstm n rnn, recurrent activation 'sigmoid', return sequences true, return state true, name lstm1 keras input, training false k lstm2 lstm n rnn, recurrent activation 'sigmoid', return sequences true, return state true, name lstm2 keras input, training false m1 model inputs keras input, outputs k lstm1 m1.set weights k params 3 k hs fw, k h fw, k c fw m1.predict x r v k h fw k h fw 0 m2 model inputs keras input, outputs k lstm2 m2.set weights k params 3 k hs bw, k h bw, k c bw m2.predict x r v k h bw k h bw 0 print v k h fw v tf h fw expect 0, 0, 0 get 0.0370398 0.02871804 0.00494046 print v k h bw v tf h bw expect 0, 0, 0 get 0.03148754 0.0340828 0.05445436",0,"feed lstm weight get tensorflow lstm keras, failed repeat result!","feed lstm weight get tensorflow lstm keras, failed repeat result! import numpy np import tensorflow tf tf.set random seed 49999 maxlen 2 word dim 2 n rnn 3 x np.array range 4 , dtype np.float32 .reshape 1, 2, 2 x r np.flip x, axis 1 inputs r tf.constant x r, dtype tf.float32 seq len tf.constant 2 , dtype tf.int32 cell fw tf.nn.rnn cell.lstmcell n rnn, name fw cell bw tf.nn.rnn cell.lstmcell n rnn, name bw tf forward tf hs fw, tf c fw, tf h fw tf.nn.dynamic rnn cell fw, inputs r, seq len, dtype tf.float32 tf backward tf hs bw, tf c bw, tf h bw tf.nn.dynamic rnn cell bw, inputs r, seq len, dtype tf.float32 sess tf.interactivesession init tf.global variables initializer sess.run init print tf.trainable variables tf w v.eval v tf.trainable variables v tf h fw tf h fw.eval 0 v tf h bw tf h bw.eval 0 def tf2keras params, reshape dim 0, 2, 1, 3 , dim 2, rnn dim 3 params 5, 12 , 12, , 5, 12 , 12, assert dim rnn dim params 0 .shape 0 len params 2 tf lstm params 0 dim, , params 0 dim , , params 1 else 4 tf lstm params 0 dim, , params 0 dim , , params 1 , params 2 dim, , params 2 dim , , params 3 tf lstm 2, 12 , 3, 12 , 12, , 2, 12 , 3, 12 , 12, def dim recombination x, reshape dim i, j, f, i, f, c, j c rst none len x.shape 1 i, j, f, tmp x 0 rnn dim 1 , x rnn dim 1 rnn dim 2 , x rnn dim 2 rnn dim 3 , x rnn dim 3 rnn dim 4 rst np.hstack tmp reshape dim elif len x.shape 2 tmp x , 0 rnn dim 1 , x , rnn dim 1 rnn dim 2 , x , rnn dim 2 rnn dim 3 , x , rnn dim 3 rnn dim 4 rst np.hstack tmp reshape dim else print xxxxx return rst keras w dim recombination v, reshape dim v tf lstm return keras w k params tf2keras tf w, 0, 2, 1, 3 keras keras.layers import input keras.layers import lstm keras.models import model keras input input shape maxlen, word dim , dtype 'float32', name 'input layer' k lstm1 lstm n rnn, recurrent activation 'sigmoid', return sequences true, return state true, name lstm1 keras input, training false k lstm2 lstm n rnn, recurrent activation 'sigmoid', return sequences true, return state true, name lstm2 keras input, training false m1 model inputs keras input, outputs k lstm1 m1.set weights k params 3 k hs fw, k h fw, k c fw m1.predict x r v k h fw k h fw 0 m2 model inputs keras input, outputs k lstm2 m2.set weights k params 3 k hs bw, k h bw, k c bw m2.predict x r v k h bw k h bw 0 print v k h fw v tf h fw expect 0, 0, 0 get 0.0370398 0.02871804 0.00494046 print v k h bw v tf h bw expect 0, 0, 0 get 0.03148754 0.0340828 0.05445436"
keras,8139,"two changes proposed issue make usable final testing phase change 1 imagedatagenerator exhaused documentation imagedatagenerator https keras.io preprocessing image says generate batches tensor image data real time data augmentation. data looped batches indefinitely. seems that, loops indefinitely implies that, image data generator mainly used training process, final test need one epoch testing . sure achieve one epoch purpose using , relies implementation documented behaviour. propose 1. add keyword argument controls whether iterator yields indefinitely not. 2. alternative 1, also make mentioned documented behaviour, also document final step, image visited exactly once. change 2 add new yields filenames batch current implementation allows recover current batch's filenames using property, relies implementation documented. propose add new yields filenames current batch, like let user slice property get current batch's class. filename useful least making kaggle submissions . changes backward compatible.",0,feature request make imagedatagenerator suitable test,"feature request make imagedatagenerator suitable test two changes proposed issue make usable final testing phase change 1 imagedatagenerator exhaused documentation imagedatagenerator https keras.io preprocessing image says generate batches tensor image data real time data augmentation. data looped batches indefinitely. seems that, loops indefinitely implies that, image data generator mainly used training process, final test need one epoch testing . sure achieve one epoch purpose using , relies implementation documented behaviour. propose 1. add keyword argument controls whether iterator yields indefinitely not. 2. alternative 1, also make mentioned documented behaviour, also document final step, image visited exactly once. change 2 add new yields filenames batch current implementation allows recover current batch's filenames using property, relies implementation documented. propose add new yields filenames current batch, like let user slice property get current batch's class. filename useful least making kaggle submissions . changes backward compatible."
keras,5267,"like modify categorical accuracy take account non padded values, code come keep getting error message below. tested function made examples seems work fine, pass categorical accuracy metric model, doesn't. help? thanks much advance! metric result metric fn true, pred file .. codeswitch pos tagger trainer.py , line 27, categorical accuracy",0,modifying categorical accuracy,"modifying categorical accuracy like modify categorical accuracy take account non padded values, code come keep getting error message below. tested function made examples seems work fine, pass categorical accuracy metric model, doesn't. help? thanks much advance! metric result metric fn true, pred file .. codeswitch pos tagger trainer.py , line 27, categorical accuracy"
keras,10439,apologies,0,accidental submission,accidental submission apologies
keras,5760,"keras 1.2.2 file resnet50.py , line 39, keras.applications.imagenet utils import obtain input shape importerror cannot import name ' obtain input shape'",0,deep learning models importerror cannot import name ' obtain input shape',"deep learning models importerror cannot import name ' obtain input shape' keras 1.2.2 file resnet50.py , line 39, keras.applications.imagenet utils import obtain input shape importerror cannot import name ' obtain input shape'"
keras,1679,trying create convolutional autoencoder temporal sequences using keras. here's code getting following error 3rd line decoder initialized please help resolve issue,0,convolutional autoencoder temporal sequences,convolutional autoencoder temporal sequences trying create convolutional autoencoder temporal sequences using keras. here's code getting following error 3rd line decoder initialized please help resolve issue
keras,9245,"fchollet, recently wrote factorized version https github.com taehoonlee tensornets blob master tensornets resnets.py resnet variants pre trained weights including resnet, resnetv2, resnext, wideresnet total 11 models . added keras, hard manage cause problems? useful? think adding 11 resnet models?",0,add resnet variants,"add resnet variants fchollet, recently wrote factorized version https github.com taehoonlee tensornets blob master tensornets resnets.py resnet variants pre trained weights including resnet, resnetv2, resnext, wideresnet total 11 models . added keras, hard manage cause problems? useful? think adding 11 resnet models?"
keras,4142,"hello all, using fit generator train model, trying batch implementation similar test multiprocessing.py https github.com fchollet keras blob master tests keras test multiprocessing.py output clear output multiple process trying access samples see intersection start,end different process batches . also tried implementation suggested 1638 luck, also working similarly. batch generator implementation work correctly intersection batches long possible. case always, samples samples per epoch nb worker 1 pickle safe true. ? tia.",0,using fit generator nb worker 1 pickle safe true,"using fit generator nb worker 1 pickle safe true hello all, using fit generator train model, trying batch implementation similar test multiprocessing.py https github.com fchollet keras blob master tests keras test multiprocessing.py output clear output multiple process trying access samples see intersection start,end different process batches . also tried implementation suggested 1638 luck, also working similarly. batch generator implementation work correctly intersection batches long possible. case always, samples samples per epoch nb worker 1 pickle safe true. ? tia."
keras,5700,"regarding patch wise training image classification segmentation, need put multiple patches corresponding single image single mini batch training process. keras? ensure multiple training patches single mini batch belong training image?",0,regarding putting multiple patches single image single mini batch,"regarding putting multiple patches single image single mini batch regarding patch wise training image classification segmentation, need put multiple patches corresponding single image single mini batch training process. keras? ensure multiple training patches single mini batch belong training image?"
keras,5418,"see backend implementation batchwise dot product. function batchwise outer product? tried implementing outer product layer own, getting stuck dealing batches would appreciate pointers help. thanks advance.",0,batchwise outer product?,"batchwise outer product? see backend implementation batchwise dot product. function batchwise outer product? tried implementing outer product layer own, getting stuck dealing batches would appreciate pointers help. thanks advance."
keras,1821,"really issue, question ensure others output function multi label classification problem interpreted correctly. here's toy problem , using basic keras model outputs interpret output? used using classifiers, return probability output multi label problems shape number test instances 2 0, 1 . think model outputting shape samples test instances classes unique number classes . could someone clarify output me?",0,interprect .predict proba multi label classification problem?,"interprect .predict proba multi label classification problem? really issue, question ensure others output function multi label classification problem interpreted correctly. here's toy problem , using basic keras model outputs interpret output? used using classifiers, return probability output multi label problems shape number test instances 2 0, 1 . think model outputting shape samples test instances classes unique number classes . could someone clarify output me?"
keras,4271,"really issue, found documentation access hidden state lstm rnn. context want capture semantic meaning sentence, feeding glove word embedding vectors https github.com fchollet keras blob master examples pretrained word embeddings.py url rnn sentence ends, final hidden state captures called sentence embedding. want access vector obtained final hidden state.",0,semantic meaning sentence,"semantic meaning sentence really issue, found documentation access hidden state lstm rnn. context want capture semantic meaning sentence, feeding glove word embedding vectors https github.com fchollet keras blob master examples pretrained word embeddings.py url rnn sentence ends, final hidden state captures called sentence embedding. want access vector obtained final hidden state."
keras,987,"already installed keras 2.0 successfully. but, print cnmem disabled , tell what's meaning note.",0,cnmem disabled,"cnmem disabled already installed keras 2.0 successfully. but, print cnmem disabled , tell what's meaning note."
keras,4733,"hi guys. trying augment image data. however, labels also images. need augmentation operations image label. would like save image somehow, use datagen.flow, save label, image. also insert code using https github.com adkoadko ostruvky blob master keras augmentation way save also label? missed something? thank",0,imagedatagenerator save label image,"imagedatagenerator save label image hi guys. trying augment image data. however, labels also images. need augmentation operations image label. would like save image somehow, use datagen.flow, save label, image. also insert code using https github.com adkoadko ostruvky blob master keras augmentation way save also label? missed something? thank"
keras,4220,"follow guide integrate tensorflow workflow https blog.keras.io keras simplified interface tensorflow tutorial.html others, cannot access weight variable building model shown guide. merely using layers. need compile use simplified interface tensorflow. access weights variables ? use tensorflow like guide, call merely use layers build.",0,access variables layers?,"access variables layers? follow guide integrate tensorflow workflow https blog.keras.io keras simplified interface tensorflow tutorial.html others, cannot access weight variable building model shown guide. merely using layers. need compile use simplified interface tensorflow. access weights variables ? use tensorflow like guide, call merely use layers build."
keras,4727,built model multiple outputs need balance losses other. theano backend via parameter . unfortunately using tensorflow. keras deal multiple losses add take derivative sum ? optimize individually ? idea implement losses need multiply weighting factor loss manually. would work optimizing sum losses.,0,weigh multiple losses,weigh multiple losses built model multiple outputs need balance losses other. theano backend via parameter . unfortunately using tensorflow. keras deal multiple losses add take derivative sum ? optimize individually ? idea implement losses need multiply weighting factor loss manually. would work optimizing sum losses.
keras,1926,"wanted reproduce issues cited post https github.com fchollet keras issues 1917 . wanted train neural network using keras, training start, happens really weird situations. portion code run code terminal using , everything goes fine training starts. run code terminal using , , redirect outputs code , program stall method, learning start. displayed, nothing more, training. took look seems program keeps calling function ideas start redirect outputs?",0,learning start,"learning start wanted reproduce issues cited post https github.com fchollet keras issues 1917 . wanted train neural network using keras, training start, happens really weird situations. portion code run code terminal using , everything goes fine training starts. run code terminal using , , redirect outputs code , program stall method, learning start. displayed, nothing more, training. took look seems program keeps calling function ideas start redirect outputs?"
keras,230,"added layer local keras convolutional layers class globalpooling layer ''' apply global pooling output. ''' def init self, pooling function t.mean super globalpooling,self . init self.pooling function pooling function def get output self, train x self.get input train return self.pooling function x.flatten 3 , axis 2 seems work fine, please add support? thanks!",0,add global pooling layer?,"add global pooling layer? added layer local keras convolutional layers class globalpooling layer ''' apply global pooling output. ''' def init self, pooling function t.mean super globalpooling,self . init self.pooling function pooling function def get output self, train x self.get input train return self.pooling function x.flatten 3 , axis 2 seems work fine, please add support? thanks!"
keras,9140,"trying understand code given ! conv lstm https github.com keras team keras blob master examples conv lstm.py . implemented whole code jupyter python notebook linked ! lstm https github.com anirudh257 audio files extraction blob master understanding 20lstm 20.ipynb start training network, notebook becomes unresponsive, firefox browser freezes crashes sometime. kernel dies too. reason this? 2 arrays noisy movies shifted movies large notebook process? anybody else experiencing issue well? would glad someone help.",0,notebook dying often.,"notebook dying often. trying understand code given ! conv lstm https github.com keras team keras blob master examples conv lstm.py . implemented whole code jupyter python notebook linked ! lstm https github.com anirudh257 audio files extraction blob master understanding 20lstm 20.ipynb start training network, notebook becomes unresponsive, firefox browser freezes crashes sometime. kernel dies too. reason this? 2 arrays noisy movies shifted movies large notebook process? anybody else experiencing issue well? would glad someone help."
keras,3854,"saved model using model.save 'my model.h5' working. however, try load model different project get error message valueerror tensor cond pred id 0 , dtype bool must graph tensor dropout 1 mul 1 0 , shape ?, 1, 256 , dtype float32 . could bug? idea?",0,convolutional neural network model could loaded,"convolutional neural network model could loaded saved model using model.save 'my model.h5' working. however, try load model different project get error message valueerror tensor cond pred id 0 , dtype bool must graph tensor dropout 1 mul 1 0 , shape ?, 1, 256 , dtype float32 . could bug? idea?"
keras,2372,"hi, thanks making wonderful tool! using keras 1.0. want save load model arch parameters. use method faq. code. load model use model.predict , error attributeerror 'nonetype' object attribute 'predict' know why. load model file, train model use it, everything seems ok. checked issues, people need load parameters. possible load architecture, overwrite old model loose model.predict ? thanks making keras! ben",0,problem save load model,"problem save load model hi, thanks making wonderful tool! using keras 1.0. want save load model arch parameters. use method faq. code. load model use model.predict , error attributeerror 'nonetype' object attribute 'predict' know why. load model file, train model use it, everything seems ok. checked issues, people need load parameters. possible load architecture, overwrite old model loose model.predict ? thanks making keras! ben"
keras,5191,"edit docs state below, unroll parameter used backends. unroll tensorflow rnn always unrolled, theano use boolean flag unroll rnn. theano backend use scan tf use loop, even tf also supports scan. also, anyone know using scan instead loop performance benefits? cheers, ben please make sure boxes checked submit issue. issue implementation question, please ask question stackoverflow http stackoverflow.com questions tagged keras join keras slack channel https keras slack autojoin.herokuapp.com ask instead filing github issue. thank you! x check date master branch keras. update pip install git git github.com fchollet keras.git upgrade deps x running tensorflow, check date latest version. installation instructions found https www.tensorflow.org get started os setup . x running theano, check date master branch theano. update pip install git git github.com theano theano.git upgrade deps x provide link github gist python script reproduce issue copy script short .",0,fix k.rnn documentation unroll parameter,"fix k.rnn documentation unroll parameter edit docs state below, unroll parameter used backends. unroll tensorflow rnn always unrolled, theano use boolean flag unroll rnn. theano backend use scan tf use loop, even tf also supports scan. also, anyone know using scan instead loop performance benefits? cheers, ben please make sure boxes checked submit issue. issue implementation question, please ask question stackoverflow http stackoverflow.com questions tagged keras join keras slack channel https keras slack autojoin.herokuapp.com ask instead filing github issue. thank you! x check date master branch keras. update pip install git git github.com fchollet keras.git upgrade deps x running tensorflow, check date latest version. installation instructions found https www.tensorflow.org get started os setup . x running theano, check date master branch theano. update pip install git git github.com theano theano.git upgrade deps x provide link github gist python script reproduce issue copy script short ."
