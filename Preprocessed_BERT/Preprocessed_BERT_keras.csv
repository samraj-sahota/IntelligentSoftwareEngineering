Repository,Number,Body,class,Title,Combined_Text
keras,11776,"Improve performance (GPU utilization) of  for small models (see [this analysis](https://github.com/rossumai/keras-multi-gpu/blob/master/blog/docs/measurements.md)).

Note created by @fchollet in the ""Requests for contributions"".

",1,Improve performance (GPU utilization) of `multi_gpu_model` for small models.,"Improve performance (GPU utilization) of `multi_gpu_model` for small models. Improve performance (GPU utilization) of  for small models (see [this analysis](https://github.com/rossumai/keras-multi-gpu/blob/master/blog/docs/measurements.md)).

Note created by @fchollet in the ""Requests for contributions"".

"
keras,8707,[Deleted],1,Good accuracy on test data but bad prediction behaviour with LSTM,Good accuracy on test data but bad prediction behaviour with LSTM [Deleted]
keras,11267,"I have a similar problem of this old [issue](https://github.com/keras-team/keras/issues/7723)(wrongly closed because stale), in all epochs, the validation calculation is slower than the epoch training part.
Both validation and training sets have the same number of examples.

The two phases take the same time with a 18 cores Xeon CPU, but validation takes 4 times the training time with Intel Phi architeclure (and tensorflow MKL binary).

I think/suspect that model evaluation for validation calculation does not take advantage of parallelization/multi-threading. This could be the core of the problem. Please check.

Regards

Please make sure that the boxes below are checked before you submit your issue.
If your issue is an **implementation question**, please ask your question on [StackOverflow](http://stackoverflow.com/questions/tagged/keras) or [on the Keras Slack channel](https://keras-slack-autojoin.herokuapp.com/) instead of opening a GitHub issue.

Thank you!

- [x ] Check that you are up-to-date with the master branch of Keras. You can update with:


- [x ] Check that your version of TensorFlow is up-to-date. The installation instructions can be found [here](https://www.tensorflow.org/get_started/os_setup).

- [x] THE LINKED OLD ISSUE ALREADY HAS IT. Provide a link to a GitHub Gist of a Python script that can reproduce your issue (or just copy the script here if it is short). 
",1,"model.fit very slow validation, more evident with Intel Phi","model.fit very slow validation, more evident with Intel Phi I have a similar problem of this old [issue](https://github.com/keras-team/keras/issues/7723)(wrongly closed because stale), in all epochs, the validation calculation is slower than the epoch training part.
Both validation and training sets have the same number of examples.

The two phases take the same time with a 18 cores Xeon CPU, but validation takes 4 times the training time with Intel Phi architeclure (and tensorflow MKL binary).

I think/suspect that model evaluation for validation calculation does not take advantage of parallelization/multi-threading. This could be the core of the problem. Please check.

Regards

Please make sure that the boxes below are checked before you submit your issue.
If your issue is an **implementation question**, please ask your question on [StackOverflow](http://stackoverflow.com/questions/tagged/keras) or [on the Keras Slack channel](https://keras-slack-autojoin.herokuapp.com/) instead of opening a GitHub issue.

Thank you!

- [x ] Check that you are up-to-date with the master branch of Keras. You can update with:


- [x ] Check that your version of TensorFlow is up-to-date. The installation instructions can be found [here](https://www.tensorflow.org/get_started/os_setup).

- [x] THE LINKED OLD ISSUE ALREADY HAS IT. Provide a link to a GitHub Gist of a Python script that can reproduce your issue (or just copy the script here if it is short). 
"
keras,511,"I've been trying all the MNIST examples from the Keras documentation as well as the cifar10 and they just don't work if I use the GPU.

The Accuracy is always below 0.1 or NaN.

If I do it on the CPU it works correctly and if I do it in the GPU with float64 it also works correctly although slowly.

I've tried to restart the laptop |(like another user suggests) and also cleaning and purging theano-cache, but the problem remains.

My system is a Mac OS X 10.10. With a GTX 750M.

Theano without Keras works properly in GPU float32 with other libraries/examples.
",1,Very low or NaN accuracy in the examples when running on a GPU with float32,"Very low or NaN accuracy in the examples when running on a GPU with float32 I've been trying all the MNIST examples from the Keras documentation as well as the cifar10 and they just don't work if I use the GPU.

The Accuracy is always below 0.1 or NaN.

If I do it on the CPU it works correctly and if I do it in the GPU with float64 it also works correctly although slowly.

I've tried to restart the laptop |(like another user suggests) and also cleaning and purging theano-cache, but the problem remains.

My system is a Mac OS X 10.10. With a GTX 750M.

Theano without Keras works properly in GPU float32 with other libraries/examples.
"
keras,12812,"Training a GRU autoencoder on EEG data, I discover the model predicts lot more poorly than train loss (mse) suggests; _evaluate_, _test_on_batch_, _predict_ with hand-coded loss, and _fit_ all agree the loss to be x10 the train values; some results below.

Exploring various predictions, it often seems that the network performs well _except_ for a bias (offset). Aside this, I'm without a clue to the cause underlying the discrepancy; disabling dropout & batch norm doesn't help.

Any clues? Help is appreciated.

<hr>
<strong>Additional details</strong>:<br>

 - CuDNNGRU stateful implementation, TensorFlow backend <br>
 - Layers:  --  -- <br>
 - Output: <br>
 -  for all layers<br>
 -  +  at (=after) input,  at encoder,  at latent<br>
 -  between encoder and latent, latent and decoder<br>
 -  - 25 *separate*, 10-min sequences fed 400 timesteps
 (=1 sec) at a time (as 10*60=600 'windows' in parallel, non-shuffled)
 -   applied before testing on new x25 10-min sequences
 -  for training, but results don't differ from 
- Keras 2.2.4, Python 3.6, Spyder 3.3.4 via Anaconda

<hr><strong>Results</strong>

    model.fit(x,x,validation_data=(x,x))

    25/25 [==============================] - 1s 42ms/step - loss: 0.1780 - val_loss: 2.9175
    25/25 [==============================] - 1s 39ms/step - loss: 0.1794 - val_loss: 2.9380

[![enter image description here][1]][1]




  [1]: https://i.stack.imgur.com/1nYU5.png",1,"Validation loss >> train loss, same data","Validation loss >> train loss, same data Training a GRU autoencoder on EEG data, I discover the model predicts lot more poorly than train loss (mse) suggests; _evaluate_, _test_on_batch_, _predict_ with hand-coded loss, and _fit_ all agree the loss to be x10 the train values; some results below.

Exploring various predictions, it often seems that the network performs well _except_ for a bias (offset). Aside this, I'm without a clue to the cause underlying the discrepancy; disabling dropout & batch norm doesn't help.

Any clues? Help is appreciated.

<hr>
<strong>Additional details</strong>:<br>

 - CuDNNGRU stateful implementation, TensorFlow backend <br>
 - Layers:  --  -- <br>
 - Output: <br>
 -  for all layers<br>
 -  +  at (=after) input,  at encoder,  at latent<br>
 -  between encoder and latent, latent and decoder<br>
 -  - 25 *separate*, 10-min sequences fed 400 timesteps
 (=1 sec) at a time (as 10*60=600 'windows' in parallel, non-shuffled)
 -   applied before testing on new x25 10-min sequences
 -  for training, but results don't differ from 
- Keras 2.2.4, Python 3.6, Spyder 3.3.4 via Anaconda

<hr><strong>Results</strong>

    model.fit(x,x,validation_data=(x,x))

    25/25 [==============================] - 1s 42ms/step - loss: 0.1780 - val_loss: 2.9175
    25/25 [==============================] - 1s 39ms/step - loss: 0.1794 - val_loss: 2.9380

[![enter image description here][1]][1]




  [1]: https://i.stack.imgur.com/1nYU5.png"
keras,4108,"Hi,

I wrote a custom loss function that just uses the built-in binary crossentropy loss:



However, the behavior is different when I use this function instead of passing 'binary_crossentropy' to the model's compile method. This function yields worse results. I suspect it might be a numerical problem but I don't know how to fix it.

In [this gist,](https://gist.github.com/EhsanEI/90203acd0026d915699a7cbb9af584a9) I get ~99% validation accuracy on MNIST with the built-in function and ~96% validation accuracy with my_loss after the first epoch. The difference is much bigger on my own dataset.
",1,Different behavior with custom loss function.,"Different behavior with custom loss function. Hi,

I wrote a custom loss function that just uses the built-in binary crossentropy loss:



However, the behavior is different when I use this function instead of passing 'binary_crossentropy' to the model's compile method. This function yields worse results. I suspect it might be a numerical problem but I don't know how to fix it.

In [this gist,](https://gist.github.com/EhsanEI/90203acd0026d915699a7cbb9af584a9) I get ~99% validation accuracy on MNIST with the built-in function and ~96% validation accuracy with my_loss after the first epoch. The difference is much bigger on my own dataset.
"
keras,4633,"Hello there, 

I don't have an exact message on me now as I am in the middle of training my model under TF but this is an issue i've had on two setups:

Setup 1: 32Gb RAM, no GPU
Setup 2: p2.xlarge instance - 60GB RAM, 1/2 Tesla K80

Model goes like this: LSTM 2, LSTM16, LSTM8, Dense1 ( but this happens on any lstm/gru network) 
Data is 3GB CSV (4*0.75GB)
Memory usage on Theano is like:
![15321436_1588947471135425_2108629537_o png](https://cloud.githubusercontent.com/assets/9210039/20986714/125561f4-bcca-11e6-9c38-47bfb33ad91b.jpeg)

What happens there roughly is theano invokes g++ and each compilation blows up (OOM)
What happens there with TF, it plateaus on 35GB of memory and runs happily, so even though Setup 1 might be out of question (leave optimization), Setup 2 works like a charm.

   
I think it'd be worth to investigating.
Theano==0.9.0.dev3
Keras==1.1.0
 ",1,Theano memory issues. ,"Theano memory issues.  Hello there, 

I don't have an exact message on me now as I am in the middle of training my model under TF but this is an issue i've had on two setups:

Setup 1: 32Gb RAM, no GPU
Setup 2: p2.xlarge instance - 60GB RAM, 1/2 Tesla K80

Model goes like this: LSTM 2, LSTM16, LSTM8, Dense1 ( but this happens on any lstm/gru network) 
Data is 3GB CSV (4*0.75GB)
Memory usage on Theano is like:
![15321436_1588947471135425_2108629537_o png](https://cloud.githubusercontent.com/assets/9210039/20986714/125561f4-bcca-11e6-9c38-47bfb33ad91b.jpeg)

What happens there roughly is theano invokes g++ and each compilation blows up (OOM)
What happens there with TF, it plateaus on 35GB of memory and runs happily, so even though Setup 1 might be out of question (leave optimization), Setup 2 works like a charm.

   
I think it'd be worth to investigating.
Theano==0.9.0.dev3
Keras==1.1.0
 "
keras,6688,"I am trying to build a binary classification algorithm (output is 0 or 1) on a dataset that contains normal and malicious network packets. The dataset shape (after converting IP @'s and hexa to decimal) is:

![capture1](https://cloud.githubusercontent.com/assets/18170760/26241844/86366da2-3c86-11e7-9620-24e6dd6f74e0.PNG)

Note: The final column is the output.

And the Keras model is:



However, I tried different optimizers, activation functions, number of layers, but the accuracy is reaching 0.5 at most:

![capture](https://cloud.githubusercontent.com/assets/18170760/26241854/8c530380-3c86-11e7-92c6-c9afd759eac0.PNG)

Even I tried Grid search for searching the best parameters, but the maximum is 0.5. Does anyone knows why the output is always like that? and how can I enhance it. Thanks in advance!",1,Keras accuracy is not increasing over 50%,"Keras accuracy is not increasing over 50% I am trying to build a binary classification algorithm (output is 0 or 1) on a dataset that contains normal and malicious network packets. The dataset shape (after converting IP @'s and hexa to decimal) is:

![capture1](https://cloud.githubusercontent.com/assets/18170760/26241844/86366da2-3c86-11e7-9620-24e6dd6f74e0.PNG)

Note: The final column is the output.

And the Keras model is:



However, I tried different optimizers, activation functions, number of layers, but the accuracy is reaching 0.5 at most:

![capture](https://cloud.githubusercontent.com/assets/18170760/26241854/8c530380-3c86-11e7-92c6-c9afd759eac0.PNG)

Even I tried Grid search for searching the best parameters, but the maximum is 0.5. Does anyone knows why the output is always like that? and how can I enhance it. Thanks in advance!"
keras,9762,"when I turn on shuffle on training, the speed become 1/3 .
I use fully connected network, with 5m sample, 300 feature",1,shuffle is very slow when using fcn ,"shuffle is very slow when using fcn  when I turn on shuffle on training, the speed become 1/3 .
I use fully connected network, with 5m sample, 300 feature"
keras,9766,"Hi,

I am new to Keras with two trials centered around MNIST sequential model. The first one I used traditional approach while in the second I used functional one.
Performance-wise, the first was more than 10 fold faster than the functional
Even with that, the first gave accuracy about 97% while the second was about 50%

I am using CPU-based tensorflow backend. Any clues?",1,learning (fiting) performance,"learning (fiting) performance Hi,

I am new to Keras with two trials centered around MNIST sequential model. The first one I used traditional approach while in the second I used functional one.
Performance-wise, the first was more than 10 fold faster than the functional
Even with that, the first gave accuracy about 97% while the second was about 50%

I am using CPU-based tensorflow backend. Any clues?"
keras,5671,"I have put my loss function within my metric:



Yet my metric is entirely different, for example here is some training output:



Note that my output per sample is two (coordinate for x and y). I can't work out why this it's so different. This different is consistent even when a new epoch starts.

I train with , like so:



Where my generator outputs multiple inputs



Where shape of  is of shape , which is similar to the inputs.  in this instance is 128.",1,Loss and metric are completely different,"Loss and metric are completely different I have put my loss function within my metric:



Yet my metric is entirely different, for example here is some training output:



Note that my output per sample is two (coordinate for x and y). I can't work out why this it's so different. This different is consistent even when a new epoch starts.

I train with , like so:



Where my generator outputs multiple inputs



Where shape of  is of shape , which is similar to the inputs.  in this instance is 128."
keras,1063,"Hello, I am training a single LSTM layer with these parameters :
number of examples : 27,000
size of sample : (2500, 1 , dtype=float64)
size of target : (250, 1 , dtype=float64)
batch size : 64
activation : linear

I use a GeForce GTX 970 GPU and the training is really slow : a single epoch takes 678 seconds, which seems really slow. Any idea of what could be happening/how I can speed things up?  

thanks!
",1,LSTM training is really slow,"LSTM training is really slow Hello, I am training a single LSTM layer with these parameters :
number of examples : 27,000
size of sample : (2500, 1 , dtype=float64)
size of target : (250, 1 , dtype=float64)
batch size : 64
activation : linear

I use a GeForce GTX 970 GPU and the training is really slow : a single epoch takes 678 seconds, which seems really slow. Any idea of what could be happening/how I can speed things up?  

thanks!
"
keras,12843,"### Configuration

| Library                        | Version  |
| ------------------------------ | -------- |
| python                         | 3.6.8    |
| GCC                            | 7.3.0    |
| tensorflow-base/tensorflow-gpu | 1.13.1   |
| keras-gpu/keras-base           | 2.2.4    |
| theano                         | 1.0.4    |
| cudnn                          | 7.3.1    |
| cudatoolkit                    | 10.0.130 |

Machine Configuration：

Ubuntu 18.04.2 LTS

Intel(R) Xeon(R) CPU E5-2640 v4 @ 2.40GHz 128GB RAM

CUDA Version: 10.2 

Double GeForce GTX 1080ti 

---

### Description

I used the same pre-trained keras model (a LeNet structure on fashion-mnist datasets) on tensorflow and theano backends respectively, and noticed that there is a big difference between the two backends. The accuracy on tensorflow is 91.6% while the accuracy on theano is just 40.6%.

![acc](https://user-images.githubusercontent.com/17698785/58078171-abfebd00-7be0-11e9-8980-a3e2686cac16.png)

The prediction results on the two backends are shown as below, respectively:

![tensorflow-result](https://user-images.githubusercontent.com/17698785/58078197-bcaf3300-7be0-11e9-91a5-2ad46142c1ef.png)
![theano-result](https://user-images.githubusercontent.com/17698785/58078200-bcaf3300-7be0-11e9-83f3-4ed88c93991d.png)

Based on the results, the problem seems to occur at the backend of Theano. I also tried to localize the problem, and found that the inconsistency may happen at the first layer of the model, which is a convolutional layer:


I compared the outputs of the layer from the two backends, and found that many values are not close (or the same).

This layer is also used in many other models but there is no such inconsistency. Therefore, I suspect this problem happens under some specific context (such as parameters). In particular, I constructed a model only with this layer (using the same parameters and shuffling the weights), and the inconsistency still exists.

I attched the code and the pretrained model (.h5 file) to help reproduce this inconsistency.

[inconsistency_1.zip](https://github.com/keras-team/keras/files/3201344/inconsistency_1.zip)
",1,Inconsistency for the convolutional layer between Tensorflow and Theano backends,"Inconsistency for the convolutional layer between Tensorflow and Theano backends ### Configuration

| Library                        | Version  |
| ------------------------------ | -------- |
| python                         | 3.6.8    |
| GCC                            | 7.3.0    |
| tensorflow-base/tensorflow-gpu | 1.13.1   |
| keras-gpu/keras-base           | 2.2.4    |
| theano                         | 1.0.4    |
| cudnn                          | 7.3.1    |
| cudatoolkit                    | 10.0.130 |

Machine Configuration：

Ubuntu 18.04.2 LTS

Intel(R) Xeon(R) CPU E5-2640 v4 @ 2.40GHz 128GB RAM

CUDA Version: 10.2 

Double GeForce GTX 1080ti 

---

### Description

I used the same pre-trained keras model (a LeNet structure on fashion-mnist datasets) on tensorflow and theano backends respectively, and noticed that there is a big difference between the two backends. The accuracy on tensorflow is 91.6% while the accuracy on theano is just 40.6%.

![acc](https://user-images.githubusercontent.com/17698785/58078171-abfebd00-7be0-11e9-8980-a3e2686cac16.png)

The prediction results on the two backends are shown as below, respectively:

![tensorflow-result](https://user-images.githubusercontent.com/17698785/58078197-bcaf3300-7be0-11e9-91a5-2ad46142c1ef.png)
![theano-result](https://user-images.githubusercontent.com/17698785/58078200-bcaf3300-7be0-11e9-83f3-4ed88c93991d.png)

Based on the results, the problem seems to occur at the backend of Theano. I also tried to localize the problem, and found that the inconsistency may happen at the first layer of the model, which is a convolutional layer:


I compared the outputs of the layer from the two backends, and found that many values are not close (or the same).

This layer is also used in many other models but there is no such inconsistency. Therefore, I suspect this problem happens under some specific context (such as parameters). In particular, I constructed a model only with this layer (using the same parameters and shuffling the weights), and the inconsistency still exists.

I attched the code and the pretrained model (.h5 file) to help reproduce this inconsistency.

[inconsistency_1.zip](https://github.com/keras-team/keras/files/3201344/inconsistency_1.zip)
"
keras,7724,"I have an input like this:


which  means increase in one metric and  means decrease in it and  means no change in the metric. Each array has 83 items for 83 fields and the output(labels) for each array is a categorical array that shows effect of these metrics on a single metric:



I used  and  in the following code:

 
but the loss after 100 epochs is:


How can I decrease this loss percentage?",1,Decrease loss in keras training usnig lstm,"Decrease loss in keras training usnig lstm I have an input like this:


which  means increase in one metric and  means decrease in it and  means no change in the metric. Each array has 83 items for 83 fields and the output(labels) for each array is a categorical array that shows effect of these metrics on a single metric:



I used  and  in the following code:

 
but the loss after 100 epochs is:


How can I decrease this loss percentage?"
keras,12343,"Hi! I have strange behavior, trying make transfer learning on MobileNetV2 in Keras. I use keras.utils.Sequence class-inheritant as data-generators. One for train, one for validation. But while train loss decreases, validation loss increases since training start. Even if I use single generator for training and for validation (absolutely same data).
My model: https://gist.github.com/dzubape/efeefa77fdb1901d8f99c201b54381df
It seems, that validation loss could be different to training loss, but not like that: 2.5 vs 0.01
Where is my way of thinking wrong?)",1,"fit_generator: same data, different loss","fit_generator: same data, different loss Hi! I have strange behavior, trying make transfer learning on MobileNetV2 in Keras. I use keras.utils.Sequence class-inheritant as data-generators. One for train, one for validation. But while train loss decreases, validation loss increases since training start. Even if I use single generator for training and for validation (absolutely same data).
My model: https://gist.github.com/dzubape/efeefa77fdb1901d8f99c201b54381df
It seems, that validation loss could be different to training loss, but not like that: 2.5 vs 0.01
Where is my way of thinking wrong?)"
keras,6199,"For my application i need 2 output from the trained network (one at final layer and one at intermediate layer). Since model.predict() gives output from only last layer, i wrote theano function to fetch output at both intermediate layer and final layer. It works fine, but it is almost 20 times slower as compared to model.predict(). 

Can anybody help me to understand why theano function is slower and how can i make it faster.",1,teano.function is slower as compared to model.predict,"teano.function is slower as compared to model.predict For my application i need 2 output from the trained network (one at final layer and one at intermediate layer). Since model.predict() gives output from only last layer, i wrote theano function to fetch output at both intermediate layer and final layer. It works fine, but it is almost 20 times slower as compared to model.predict(). 

Can anybody help me to understand why theano function is slower and how can i make it faster."
keras,4171,"I'm trying to train a simple MLP model on numerical data for binary classification, and when I train it this is what I see:



This continues forever...

You can view the code in question here, without the data loading portion:



If I look at the output of the model, I can see that this is what it predicts:



**Things I have tried:**

I have tried on several machines with different hardware and Ubuntu versions, with different combinations of Keras (1.0.7, 1.0.8, latest commit) & Theano (0.8.2 & latest commit) versions, and both CPU and GPU training - with the same result in every case.

I've tried removing the sigmoid activation and optimising MSE instead, but that results in a loss about 10 digits long. I've tried having a single dense neuron with no activation (and other various simplified architecures), and I've tried several different optimizers (Adam, sgd, rmsprop). No matter how I change the model, I cannot seem to get anything else.

I've verified that there are no NaNs or Infs in the matrix, and that the y values are binary. The matrix passed to Keras is a numpy array.

Any help would be very much appreciated, as I have been labouring over this problem for hours without being able to fix it.
",1,Very high loss (~7) when doing binary classification,"Very high loss (~7) when doing binary classification I'm trying to train a simple MLP model on numerical data for binary classification, and when I train it this is what I see:



This continues forever...

You can view the code in question here, without the data loading portion:



If I look at the output of the model, I can see that this is what it predicts:



**Things I have tried:**

I have tried on several machines with different hardware and Ubuntu versions, with different combinations of Keras (1.0.7, 1.0.8, latest commit) & Theano (0.8.2 & latest commit) versions, and both CPU and GPU training - with the same result in every case.

I've tried removing the sigmoid activation and optimising MSE instead, but that results in a loss about 10 digits long. I've tried having a single dense neuron with no activation (and other various simplified architecures), and I've tried several different optimizers (Adam, sgd, rmsprop). No matter how I change the model, I cannot seem to get anything else.

I've verified that there are no NaNs or Infs in the matrix, and that the y values are binary. The matrix passed to Keras is a numpy array.

Any help would be very much appreciated, as I have been labouring over this problem for hours without being able to fix it.
"
keras,13389,"**System information**  
- Have I written custom code (as opposed to using example directory):  Yes, custom Keras Sequence
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Ubuntu 16.04
- TensorFlow backend (yes / no):  yes
- TensorFlow version:  1.14.0
- Keras version:  2.3.0
- Python version:  Python 3.6.8
- CUDA/cuDNN version:  10.1
- GPU model and memory:  ASUS Dual GeForce® RTX 2080 Ti  (2 pieces)

You can obtain the TensorFlow version with:  
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""  
You can obtain the Keras version with:  
python -c 'import keras as k; print(k.__version__)'  

**Describe the current behavior**  
I have a model with multiple outputs, and hence there is loss for each of the output and an overall weighted loss. The weighted sum of all the validation loss of the different outputs doesn't add up to the overall validation loss, val_loss.

**Describe the expected behavior**  
Expecting val_loss to be a weighted sum of all the validation loss of model output.

**Code to reproduce the issue**  


**Other info / logs**  
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.  
",1,val_loss not making sense with multiple outputs and weighted loss,"val_loss not making sense with multiple outputs and weighted loss **System information**  
- Have I written custom code (as opposed to using example directory):  Yes, custom Keras Sequence
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Ubuntu 16.04
- TensorFlow backend (yes / no):  yes
- TensorFlow version:  1.14.0
- Keras version:  2.3.0
- Python version:  Python 3.6.8
- CUDA/cuDNN version:  10.1
- GPU model and memory:  ASUS Dual GeForce® RTX 2080 Ti  (2 pieces)

You can obtain the TensorFlow version with:  
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""  
You can obtain the Keras version with:  
python -c 'import keras as k; print(k.__version__)'  

**Describe the current behavior**  
I have a model with multiple outputs, and hence there is loss for each of the output and an overall weighted loss. The weighted sum of all the validation loss of the different outputs doesn't add up to the overall validation loss, val_loss.

**Describe the expected behavior**  
Expecting val_loss to be a weighted sum of all the validation loss of model output.

**Code to reproduce the issue**  


**Other info / logs**  
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.  
"
keras,10832,"
I recently bought a second GPU in order to speed up my neural network computations. I use Keras 2.2.2 with  Tensorflow 1.9.0. When I run a normal model it is sped up by one GPU as might be expected. It makes my model run about 60 times faster compared to the CPU (AMD 1950X).


When I use the instruction  the combined power of both cards results in a slowdown by about a **factor 100**. That happens to all my models: RNN, CNN and Dense. Both cards seem to work, however, they seem to alternate the work between each other. Also the CPU activity does increase quite a lot. When using both cards the CPU activity of one core is about 50%, using one card this is about 8%-10%. 

When selecting each GPU separately by using  the usual speed up occurs. I cannot use multi_gpu_model in that case. When I select both GPU’s with   and multi_gpu_model then the slow down occurs. I printed the topology (see below) but could not find anything strange, but I am far from an expert on this. 


This is not the same issue as issue #9204. That issue discusses why some models cause some slowdown when using multiple GPU's. As far as I can see my issue is not related to any specific model.  
Does anyone have an idea what exactly is wrong in my setup?

Keras 2.2.2, Tensorflow 1.9.0, python 3.5.5, Ubuntu Mate 18.04 and Nvidia drivers 390.48 (same problems with Ubuntu Mate 16.04.4 and Nvidia drivers 390.130).

Below is some simple MNIST code that I used to benchmark my setup.

",1,Slowdown by factor 100 when adding second GPU,"Slowdown by factor 100 when adding second GPU 
I recently bought a second GPU in order to speed up my neural network computations. I use Keras 2.2.2 with  Tensorflow 1.9.0. When I run a normal model it is sped up by one GPU as might be expected. It makes my model run about 60 times faster compared to the CPU (AMD 1950X).


When I use the instruction  the combined power of both cards results in a slowdown by about a **factor 100**. That happens to all my models: RNN, CNN and Dense. Both cards seem to work, however, they seem to alternate the work between each other. Also the CPU activity does increase quite a lot. When using both cards the CPU activity of one core is about 50%, using one card this is about 8%-10%. 

When selecting each GPU separately by using  the usual speed up occurs. I cannot use multi_gpu_model in that case. When I select both GPU’s with   and multi_gpu_model then the slow down occurs. I printed the topology (see below) but could not find anything strange, but I am far from an expert on this. 


This is not the same issue as issue #9204. That issue discusses why some models cause some slowdown when using multiple GPU's. As far as I can see my issue is not related to any specific model.  
Does anyone have an idea what exactly is wrong in my setup?

Keras 2.2.2, Tensorflow 1.9.0, python 3.5.5, Ubuntu Mate 18.04 and Nvidia drivers 390.48 (same problems with Ubuntu Mate 16.04.4 and Nvidia drivers 390.130).

Below is some simple MNIST code that I used to benchmark my setup.

"
keras,8273,"Any idea about **why** our **training loss** is **smooth** and our **validation loss** is that **noisy** across epochs?

![image](https://user-images.githubusercontent.com/4671752/32121045-4b16b5b8-bb31-11e7-86e0-8690ce9f867c.png)

 We are implementing a deep learning model for diabetic retinopathy detection (**binary classification**) using the data set of fundus photographs provided by [this Kaggle competition][2]. We are using **Keras 2.0** with **Tensorflow** backend.

As the data set is too big to fit in memory, we are using , with  randomly taking images from training and validation folders:

    # TRAIN THE MODEL
    model.fit_generator(
        train_generator,
        steps_per_epoch= train_generator.samples // training_batch_size,
        epochs=int(config['training']['epochs']),
        validation_data=validation_generator,
        validation_steps= validation_generator.samples // validation_batch_size,
        class_weight=None)

Our CNN architecture is VGG16 with dropout = 0.5 in the last two fully connected layers, batch normalization only before the first fully connected layer, and data augmentation (consisting on flipping the images horizontally and vertically). Our training and validation samples are normalized using the training set mean and standard deviation. Batch size is 32. Our activation is a  and the loss function is the . [You can find our implementation in Github][3]

It definitely has nothing to do with overfitting, as we tried with a highly regularized model and the behavior was quite the same. **Is it related with the sampling from the validation set?** Has any of you had a similar problem before?

Thanks!!

  [2]: https://www.kaggle.com/c/diabetic-retinopathy-detection
  [3]: https://github.com/ignaciorlando/cnn-dr-kaggle",1,Noisy validation loss in Keras when using fit_generator,"Noisy validation loss in Keras when using fit_generator Any idea about **why** our **training loss** is **smooth** and our **validation loss** is that **noisy** across epochs?

![image](https://user-images.githubusercontent.com/4671752/32121045-4b16b5b8-bb31-11e7-86e0-8690ce9f867c.png)

 We are implementing a deep learning model for diabetic retinopathy detection (**binary classification**) using the data set of fundus photographs provided by [this Kaggle competition][2]. We are using **Keras 2.0** with **Tensorflow** backend.

As the data set is too big to fit in memory, we are using , with  randomly taking images from training and validation folders:

    # TRAIN THE MODEL
    model.fit_generator(
        train_generator,
        steps_per_epoch= train_generator.samples // training_batch_size,
        epochs=int(config['training']['epochs']),
        validation_data=validation_generator,
        validation_steps= validation_generator.samples // validation_batch_size,
        class_weight=None)

Our CNN architecture is VGG16 with dropout = 0.5 in the last two fully connected layers, batch normalization only before the first fully connected layer, and data augmentation (consisting on flipping the images horizontally and vertically). Our training and validation samples are normalized using the training set mean and standard deviation. Batch size is 32. Our activation is a  and the loss function is the . [You can find our implementation in Github][3]

It definitely has nothing to do with overfitting, as we tried with a highly regularized model and the behavior was quite the same. **Is it related with the sampling from the validation set?** Has any of you had a similar problem before?

Thanks!!

  [2]: https://www.kaggle.com/c/diabetic-retinopathy-detection
  [3]: https://github.com/ignaciorlando/cnn-dr-kaggle"
keras,11858,"Hi,

Not sure this can be labelled as a bug, but it's problematic. BatchNormalization seems to silently produce NaN weights when training a  if the training dataset size is not a multiple of .

For example, with a training dataset (1825, 401, 401, 3), validation dataset (140, 401, 401, 3), , , 



The training apparently goes fine



but the weights have NaNs, e.g.



I think this happens because the training dataset of 1825 gets split into sets of . So there's going to be a set of 1 training image, and maybe that doesn't work with BatchNormalization.

Inference with the trained model gives



A solution is to make sure that the number of training images is a multiple of .",1,BatchNormalization produces NaN weights without NaN loss,"BatchNormalization produces NaN weights without NaN loss Hi,

Not sure this can be labelled as a bug, but it's problematic. BatchNormalization seems to silently produce NaN weights when training a  if the training dataset size is not a multiple of .

For example, with a training dataset (1825, 401, 401, 3), validation dataset (140, 401, 401, 3), , , 



The training apparently goes fine



but the weights have NaNs, e.g.



I think this happens because the training dataset of 1825 gets split into sets of . So there's going to be a set of 1 training image, and maybe that doesn't work with BatchNormalization.

Inference with the trained model gives



A solution is to make sure that the number of training images is a multiple of ."
keras,8792,"Please make sure that the boxes below are checked before you submit your issue. If your issue is an implementation question, please ask your question on [StackOverflow](http://stackoverflow.com/questions/tagged/keras) or [join the Keras Slack channel](https://keras-slack-autojoin.herokuapp.com/) and ask there instead of filing a GitHub issue.

Thank you!

- [x] Check that you are up-to-date with the master branch of Keras. You can update with:
pip install git+git://github.com/fchollet/keras.git --upgrade --no-deps

- [x] If running on TensorFlow, check that you are up-to-date with the latest version. The installation instructions can be found [here](https://www.tensorflow.org/get_started/os_setup).

- [x] If running on Theano, check that you are up-to-date with the master branch of Theano. You can update with:
pip install git+git://github.com/Theano/Theano.git --upgrade --no-deps

- [x] Provide a link to a GitHub Gist of a Python script that can reproduce your issue (or just copy the script here if it is short).

When running under both Keras 2.1.2 as well as Keras from GitHub master the validation accuracy achieved in the dogs vs. cats example in Chapter 5.3 of the DL w/Python book (https://github.com/fchollet/deep-learning-with-python-notebooks/blob/master/5.3-using-a-pretrained-convnet.ipynb) is ~ 90%.

However, the validation accuracy when running under Keras 2.0.9 is ~ 96%.

The code (as well as the cats vs. dogs images) required to reproduce this is in the following GitHub repo (the script is also inline below):

https://github.com/jjallaire/keras-vgg16-accuracy

One other noteworthy thing is that with Keras 2.1.2 case training takes ~ 22 seconds/epoch whereas with Keras 2.0.9 it's ~ 50 seconds per epoch (this on an Amazon p2.xlarge instance).


 
",1,regression in training accuracy with VGG16 application,"regression in training accuracy with VGG16 application Please make sure that the boxes below are checked before you submit your issue. If your issue is an implementation question, please ask your question on [StackOverflow](http://stackoverflow.com/questions/tagged/keras) or [join the Keras Slack channel](https://keras-slack-autojoin.herokuapp.com/) and ask there instead of filing a GitHub issue.

Thank you!

- [x] Check that you are up-to-date with the master branch of Keras. You can update with:
pip install git+git://github.com/fchollet/keras.git --upgrade --no-deps

- [x] If running on TensorFlow, check that you are up-to-date with the latest version. The installation instructions can be found [here](https://www.tensorflow.org/get_started/os_setup).

- [x] If running on Theano, check that you are up-to-date with the master branch of Theano. You can update with:
pip install git+git://github.com/Theano/Theano.git --upgrade --no-deps

- [x] Provide a link to a GitHub Gist of a Python script that can reproduce your issue (or just copy the script here if it is short).

When running under both Keras 2.1.2 as well as Keras from GitHub master the validation accuracy achieved in the dogs vs. cats example in Chapter 5.3 of the DL w/Python book (https://github.com/fchollet/deep-learning-with-python-notebooks/blob/master/5.3-using-a-pretrained-convnet.ipynb) is ~ 90%.

However, the validation accuracy when running under Keras 2.0.9 is ~ 96%.

The code (as well as the cats vs. dogs images) required to reproduce this is in the following GitHub repo (the script is also inline below):

https://github.com/jjallaire/keras-vgg16-accuracy

One other noteworthy thing is that with Keras 2.1.2 case training takes ~ 22 seconds/epoch whereas with Keras 2.0.9 it's ~ 50 seconds per epoch (this on an Amazon p2.xlarge instance).


 
"
keras,5720,"I am trying to train a segmentation network. I am running into issues when trying to use image augmentation. Below is attached my function that sets the generator, fits it (though I dont think I really need to), it then zips the label and image generators. 

It is when trying to zip these that my program hangs and never moves on. Playing around it seems that it has to do with the flow function. Whenever I call the flow function it just runs on it's own indefinitely. 


I have made some changes to the image.py file to allow for 3D manipulations, but I have check/doublechecked / even gone back and re-implemented my changes in a clean copy of image.py  to make sure I wasn't changing anything that should effect this process. 
[image.py.zip](https://github.com/fchollet/keras/files/836158/image.py.zip)


I've attached a zip of my updated image.py. Most of the functions at the start are left as the 2D counterparts, its within the random_transform that most of the 3D manipulations are located. 

Does anyone have any thoughts? 

Thanks, 

Anthony. 



",1,Image Augmentation- flow - hanging forever and not training,"Image Augmentation- flow - hanging forever and not training I am trying to train a segmentation network. I am running into issues when trying to use image augmentation. Below is attached my function that sets the generator, fits it (though I dont think I really need to), it then zips the label and image generators. 

It is when trying to zip these that my program hangs and never moves on. Playing around it seems that it has to do with the flow function. Whenever I call the flow function it just runs on it's own indefinitely. 


I have made some changes to the image.py file to allow for 3D manipulations, but I have check/doublechecked / even gone back and re-implemented my changes in a clean copy of image.py  to make sure I wasn't changing anything that should effect this process. 
[image.py.zip](https://github.com/fchollet/keras/files/836158/image.py.zip)


I've attached a zip of my updated image.py. Most of the functions at the start are left as the 2D counterparts, its within the random_transform that most of the 3D manipulations are located. 

Does anyone have any thoughts? 

Thanks, 

Anthony. 



"
keras,11866,"I am using 2D data in a classification problem using keras.
So I am defining a keras model as following:




which returns a compiled model with the following number of parameters parameters


Reading the documentation and inspecting the [code](https://github.com/keras-team/keras/blob/88af7d0c97497b5c3a198ee9416b2accfbc72c36/keras/layers/core.py#L880) if the input shape has more than 1D it is going to use the last dimension as input dimension. 
Thus the layer has 1100 parameters = 10*100+ 100 (weights + bias).

What I don't understand is how the dot product between the input and the weights is performed. Because if the data is 2D with 5\*10 data points, and I am using only 100\*10 weights how are the weights and input values multiplied? If I change the input data from (5,10) --> (500,10) something changes?

I always have the idea that the output of the neuron k was something like:

where the x_1, x_2 are ALL the previous outpus, however in this case that is 2D I don't know what is happening here.


",1,Number of parameters of a dense layer with a 2D input,"Number of parameters of a dense layer with a 2D input I am using 2D data in a classification problem using keras.
So I am defining a keras model as following:




which returns a compiled model with the following number of parameters parameters


Reading the documentation and inspecting the [code](https://github.com/keras-team/keras/blob/88af7d0c97497b5c3a198ee9416b2accfbc72c36/keras/layers/core.py#L880) if the input shape has more than 1D it is going to use the last dimension as input dimension. 
Thus the layer has 1100 parameters = 10*100+ 100 (weights + bias).

What I don't understand is how the dot product between the input and the weights is performed. Because if the data is 2D with 5\*10 data points, and I am using only 100\*10 weights how are the weights and input values multiplied? If I change the input data from (5,10) --> (500,10) something changes?

I always have the idea that the output of the neuron k was something like:

where the x_1, x_2 are ALL the previous outpus, however in this case that is 2D I don't know what is happening here.


"
keras,3675,"Hi, 

I have noticed that when I use fit generator with pickle safe to True, number of workers > 1, and max generator queue > 2 the python processes accumulate memory leading to hanging of the execution due to no more available memory (I have 40GB of RAM).

This does not happening when setting pickle safe to False. 
",1,Memory consumption when using fit generator,"Memory consumption when using fit generator Hi, 

I have noticed that when I use fit generator with pickle safe to True, number of workers > 1, and max generator queue > 2 the python processes accumulate memory leading to hanging of the execution due to no more available memory (I have 40GB of RAM).

This does not happening when setting pickle safe to False. 
"
keras,11356,"When training LSTM models, switching to CuDNNLSTM improves the training speed significantly on GPU machines. Training ConvLSTM2D models is extremely slow, is there a plan (or is it even possible) to make sth like CuDNNConvLSTM2D?",1,ConvLSTM2D very slow during training,"ConvLSTM2D very slow during training When training LSTM models, switching to CuDNNLSTM improves the training speed significantly on GPU machines. Training ConvLSTM2D models is extremely slow, is there a plan (or is it even possible) to make sth like CuDNNConvLSTM2D?"
keras,8289,"I found use adam optimizer ,train loss  only at the beginning of each epoch it drops soon and then slows down。


like this：
epoch 1/100： loss 70.0 -> 60.0 ->55. -> ...... ->40.0  ->39.95 ->39.90->39.89->.....
epoch 2/100： loss 30 -> 25 ->20 -> ...... ->18.0  ->17.9  ->.....->17.75 ->17.73
epoch 3/100： loss 10 -> 8 ->7 -> ...... ->6  ->6.9  ->.....
...








",1,"Why use adam optimizer ,train loss only at the beginning of each epoch it drops soon and then slows down？","Why use adam optimizer ,train loss only at the beginning of each epoch it drops soon and then slows down？ I found use adam optimizer ,train loss  only at the beginning of each epoch it drops soon and then slows down。


like this：
epoch 1/100： loss 70.0 -> 60.0 ->55. -> ...... ->40.0  ->39.95 ->39.90->39.89->.....
epoch 2/100： loss 30 -> 25 ->20 -> ...... ->18.0  ->17.9  ->.....->17.75 ->17.73
epoch 3/100： loss 10 -> 8 ->7 -> ...... ->6  ->6.9  ->.....
...








"
keras,8802,"When sharing  layers Siamese-style, I wasn't able to synchronize dropped units. For example, in the code below,  has no effect.  parameter has no effect either. Shared  layers should synchronize dropped units by default, otherwise they are not shared in any meaningful way.

Output:


Note that
1. during inference nothing is dropped, so loss is 0, as expected
2. if dropout rate=0, the loss is 0 during training, as expected
3. Note that on Epochs 2,3,7,8,9 the loss is 4, which I don't understand at all. Maybe another bug, I need to investigate further.",1,"When Dropout layer is shared Siamese-style, dropped units are not synchronized.","When Dropout layer is shared Siamese-style, dropped units are not synchronized. When sharing  layers Siamese-style, I wasn't able to synchronize dropped units. For example, in the code below,  has no effect.  parameter has no effect either. Shared  layers should synchronize dropped units by default, otherwise they are not shared in any meaningful way.

Output:


Note that
1. during inference nothing is dropped, so loss is 0, as expected
2. if dropout rate=0, the loss is 0 during training, as expected
3. Note that on Epochs 2,3,7,8,9 the loss is 4, which I don't understand at all. Maybe another bug, I need to investigate further."
keras,1133,"When i use the cnn with new keras, eg keras 0.3.0, i get a quite slow cnn training using mnist_cnn.py 2x faster than cpu version. Well, keras 0.2.0 got a almost 60x faster than cpu. 
I don't understand.
What happend in Dec 01, 2015?
",1,Why the performance of new keras reduce so much?,"Why the performance of new keras reduce so much? When i use the cnn with new keras, eg keras 0.3.0, i get a quite slow cnn training using mnist_cnn.py 2x faster than cpu version. Well, keras 0.2.0 got a almost 60x faster than cpu. 
I don't understand.
What happend in Dec 01, 2015?
"
keras,10350,"Hi,

I used the basic character-level seq2seq model [example](https://github.com/keras-team/keras/blob/master/examples/lstm_seq2seq.py) for French-to-English translation.

The accuracy measure seems to comparing the entire decoder output, i.e max_decoder_output_length.
and I would like the accuracy to represent only the characters till first occurrence of the stop character ('\n' in this case) and not compare the predicted output to padded target output.

For example:
Consider the example that gives a 63 character long decoder sequence:

i.e **decoder_target_output** is One Hot Representation of ( '' empty string represents a zero padded input vector)
['C', 'o', 'u', 'r', 'e', 'z', '\xe2', '\x80', '\xaf', '!', '\n', '', '', '', '' '', '', '', '', '', '', '' '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '' '', '', '', '', '' '', '', '', '', '', '', '', '', '', '', '', '' '', '', '', '', '', '', '', '']

after fitting, the **predicted_decoder_output** gives output using Argmax on One Hot Representation as

['C', 'o', 'u', 'r', 'e', 'z', '\xe2', '\x80', '\xaf', '!', '\n', 'i', 'r', ' ', '?', '\n', 'i', 'l', '\xc3', '\xa9', ' ', '?', '\n', 'i', ' ', 'd', 'e', 'u', 's', ' ', 'c', 'o', 'u', 'r', 'e', '\xe2', '\x80', '\xaf', '!', '\n', '\n', '\n', ' ', '!', '\n', '\n', 'i', ' ', 'c', 'h', 'i', 'e', '\xe2', '\x80', '\xaf', '!', '\n', '\n', '\n', ' ', 'd', 'i', 'r', 'e'] 

keras_accuracy = 11/63

however, the relevant accuracy is = 11/11 as I don't need the model to consider accuracy for beyond the stop character.
Is there a way to get correct accuracy by ignoring the padded output ?
",1,Incorrect accuracy measure for padded target sequence in seq2seq model,"Incorrect accuracy measure for padded target sequence in seq2seq model Hi,

I used the basic character-level seq2seq model [example](https://github.com/keras-team/keras/blob/master/examples/lstm_seq2seq.py) for French-to-English translation.

The accuracy measure seems to comparing the entire decoder output, i.e max_decoder_output_length.
and I would like the accuracy to represent only the characters till first occurrence of the stop character ('\n' in this case) and not compare the predicted output to padded target output.

For example:
Consider the example that gives a 63 character long decoder sequence:

i.e **decoder_target_output** is One Hot Representation of ( '' empty string represents a zero padded input vector)
['C', 'o', 'u', 'r', 'e', 'z', '\xe2', '\x80', '\xaf', '!', '\n', '', '', '', '' '', '', '', '', '', '', '' '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '' '', '', '', '', '' '', '', '', '', '', '', '', '', '', '', '', '' '', '', '', '', '', '', '', '']

after fitting, the **predicted_decoder_output** gives output using Argmax on One Hot Representation as

['C', 'o', 'u', 'r', 'e', 'z', '\xe2', '\x80', '\xaf', '!', '\n', 'i', 'r', ' ', '?', '\n', 'i', 'l', '\xc3', '\xa9', ' ', '?', '\n', 'i', ' ', 'd', 'e', 'u', 's', ' ', 'c', 'o', 'u', 'r', 'e', '\xe2', '\x80', '\xaf', '!', '\n', '\n', '\n', ' ', '!', '\n', '\n', 'i', ' ', 'c', 'h', 'i', 'e', '\xe2', '\x80', '\xaf', '!', '\n', '\n', '\n', ' ', 'd', 'i', 'r', 'e'] 

keras_accuracy = 11/63

however, the relevant accuracy is = 11/11 as I don't need the model to consider accuracy for beyond the stop character.
Is there a way to get correct accuracy by ignoring the padded output ?
"
keras,4728,"I am new to keras and deep learning in general. I am trying to implementation Visual Attention based Image Caption generation based on [Xu et. al](https://arxiv.org/pdf/1502.03044.pdf) I have created a new class AttentionLSTM based on the existing LSTM class. I want to retrieve the value of one of the states (alpha - the weights of features vectors), however whenever I access it (at the end of each batch), it is always comes up as an all-zero tensor. My model is as follows:

	SEQUENCE_LENGTH = 45
	MAX_SENTENCE_LENGTH = SEQUENCE_LENGTH - 3 # 1 for image, 1 for start token, 1 for end token
	OUTPUT_DIM = 512
	ANNOTATION_DIM = 512
	WORD_DIM = 512
	ANNOTATION_SIZE=196

	x_inp = Input(shape=(SEQUENCE_LENGTH-1, VOCAB_COUNT))
	z_inp = Input(shape=(ANNOTATION_SIZE, ANNOTATION_DIM,))
	z_mean = Input(shape=(ANNOTATION_DIM,))
	h_Dense = Dense(OUTPUT_DIM, input_dim=ANNOTATION_DIM, activation='softmax')(z_mean)
	c_Dense = Dense(OUTPUT_DIM, input_dim=ANNOTATION_DIM, activation='softmax')(z_mean)
	xt_dense = TimeDistributed(Dense(WORD_DIM))(x_inp)
	aLstm_Layer = AttentionLSTM(output_dim=WORD_DIM, z_dim=ANNOTATION_DIM, W_regularizer=l2(0.01), U_regularizer=l2(0.01), Z_regularizer=l2(0.01), dropout_W=0.3, dropout_U=0.3, dropout_Z=0.3, return_sequences=True)
	aLstm = aLstm_Layer([xt_dense, h_Dense, c_Dense, z_inp])
	tdense = TimeDistributed(Dense(VOCAB_COUNT))(aLstm)
	act = Activation('softmax')(tdense)
	model = Model(input=[x_inp, z_inp, z_mean], output=act)

	model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])
	model.fit(x_train, y_train, batch_size=1, nb_epoch=1, verbose=1)

My attention has the following code in  function

	def step(self, x, states):
		prev_h1 = states[0]
		prev_c1 = states[1]
		proj_z = states[2]
		alphaz = states[3]
		B_U = states[4]
		B_W = states[5]
		B_Z = states[6]

		proj_state = K.dot(prev_h1, self.Wd_att)
		proj_z = proj_z + proj_state[:, None, :]
		proj_list = []
		proj_list.append(proj_z)
		proj_z = K.tanh(proj_z)

		alpha = K.dot(proj_z, self.U_att ) + self.b2_att
		alpha_shape = alpha.shape
		alpha = K.softmax(alpha.reshape((alpha_shape[0], alpha_shape[1])))

		alphaz = alpha
		self.alphaz = alpha

		z = (self.initial_z * alpha[:, :, None]).sum(1)
                #Remaing code same as LSTM.step()

To get the alpha value, I have defined the following function:
    alphaz = aLstm_Layer.states[3]
    alpha_func = K.function([x_inp, z_inp, z_mean], alphaz)
    al = alpha_func(x_train)
    print(al)

The above print statement always returns


I am setting alpha to zero in  and . 

Am I doing something wrong (with the model or the way I retrieve alpha) ? Is there a better way to get the value of  ? (I am doing this because I don't know if there's a way to make a layer give multiple outputs)

- [x] Check that you are up-to-date with the master branch of Keras. You can update with:
pip install git+git://github.com/fchollet/keras.git --upgrade --no-deps

- [x] If running on Theano, check that you are up-to-date with the master branch of Theano. You can update with:
pip install git+git://github.com/Theano/Theano.git --upgrade --no-deps

- [x] Provide a link to a GitHub Gist of a Python script that can reproduce your issue (or just copy the script here if it is short).",1,Layer state value always zero,"Layer state value always zero I am new to keras and deep learning in general. I am trying to implementation Visual Attention based Image Caption generation based on [Xu et. al](https://arxiv.org/pdf/1502.03044.pdf) I have created a new class AttentionLSTM based on the existing LSTM class. I want to retrieve the value of one of the states (alpha - the weights of features vectors), however whenever I access it (at the end of each batch), it is always comes up as an all-zero tensor. My model is as follows:

	SEQUENCE_LENGTH = 45
	MAX_SENTENCE_LENGTH = SEQUENCE_LENGTH - 3 # 1 for image, 1 for start token, 1 for end token
	OUTPUT_DIM = 512
	ANNOTATION_DIM = 512
	WORD_DIM = 512
	ANNOTATION_SIZE=196

	x_inp = Input(shape=(SEQUENCE_LENGTH-1, VOCAB_COUNT))
	z_inp = Input(shape=(ANNOTATION_SIZE, ANNOTATION_DIM,))
	z_mean = Input(shape=(ANNOTATION_DIM,))
	h_Dense = Dense(OUTPUT_DIM, input_dim=ANNOTATION_DIM, activation='softmax')(z_mean)
	c_Dense = Dense(OUTPUT_DIM, input_dim=ANNOTATION_DIM, activation='softmax')(z_mean)
	xt_dense = TimeDistributed(Dense(WORD_DIM))(x_inp)
	aLstm_Layer = AttentionLSTM(output_dim=WORD_DIM, z_dim=ANNOTATION_DIM, W_regularizer=l2(0.01), U_regularizer=l2(0.01), Z_regularizer=l2(0.01), dropout_W=0.3, dropout_U=0.3, dropout_Z=0.3, return_sequences=True)
	aLstm = aLstm_Layer([xt_dense, h_Dense, c_Dense, z_inp])
	tdense = TimeDistributed(Dense(VOCAB_COUNT))(aLstm)
	act = Activation('softmax')(tdense)
	model = Model(input=[x_inp, z_inp, z_mean], output=act)

	model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])
	model.fit(x_train, y_train, batch_size=1, nb_epoch=1, verbose=1)

My attention has the following code in  function

	def step(self, x, states):
		prev_h1 = states[0]
		prev_c1 = states[1]
		proj_z = states[2]
		alphaz = states[3]
		B_U = states[4]
		B_W = states[5]
		B_Z = states[6]

		proj_state = K.dot(prev_h1, self.Wd_att)
		proj_z = proj_z + proj_state[:, None, :]
		proj_list = []
		proj_list.append(proj_z)
		proj_z = K.tanh(proj_z)

		alpha = K.dot(proj_z, self.U_att ) + self.b2_att
		alpha_shape = alpha.shape
		alpha = K.softmax(alpha.reshape((alpha_shape[0], alpha_shape[1])))

		alphaz = alpha
		self.alphaz = alpha

		z = (self.initial_z * alpha[:, :, None]).sum(1)
                #Remaing code same as LSTM.step()

To get the alpha value, I have defined the following function:
    alphaz = aLstm_Layer.states[3]
    alpha_func = K.function([x_inp, z_inp, z_mean], alphaz)
    al = alpha_func(x_train)
    print(al)

The above print statement always returns


I am setting alpha to zero in  and . 

Am I doing something wrong (with the model or the way I retrieve alpha) ? Is there a better way to get the value of  ? (I am doing this because I don't know if there's a way to make a layer give multiple outputs)

- [x] Check that you are up-to-date with the master branch of Keras. You can update with:
pip install git+git://github.com/fchollet/keras.git --upgrade --no-deps

- [x] If running on Theano, check that you are up-to-date with the master branch of Theano. You can update with:
pip install git+git://github.com/Theano/Theano.git --upgrade --no-deps

- [x] Provide a link to a GitHub Gist of a Python script that can reproduce your issue (or just copy the script here if it is short)."
keras,11897,"With reference to https://stackoverflow.com/questions/53400472/keras-model-weights-for-some-layers-become-all-nans , what I have noticed that the ""NaNs"" are present only in the ""batch_norm"" layers. Why must this be happening?

Is there some problem with the training - like Vanishing Gradient or Gradients exploding? or are the NaNs only placeholders for biases?

(Just a prior - I am trying to train a Siamese Network with Triplet Loss as mentioned here https://stackoverflow.com/questions/53400472/keras-model-weights-for-some-layers-become-all-nans (architecture and loss) ) 

Sample Weights listed here-
https://stackoverflow.com/questions/53830547/batch-norm-layer-weights-parameters-turning-nans-during-training-of-a-siamese-ne

Link to the Gist for the code I am using for training - https://gist.github.com/sidgairo18/dca347edd4588484237a231d7dab9a63

Please make sure that the boxes below are checked before you submit your issue.
If your issue is an **implementation question**, please ask your question on [StackOverflow](http://stackoverflow.com/questions/tagged/keras) or [on the Keras Slack channel](https://keras-slack-autojoin.herokuapp.com/) instead of opening a GitHub issue.

Thank you!

- [Yes] Check that you are up-to-date with the master branch of Keras. You can update with:


- [Yes] Check that your version of TensorFlow is up-to-date. The installation instructions can be found [here](https://www.tensorflow.org/get_started/os_setup).

- [Yes] Provide a link to a GitHub Gist of a Python script that can reproduce your issue (or just copy the script here if it is short).",1,Batch Norm Layer weights/parameters turning NaNs during training of a Siamese Network with Triplet Loss,"Batch Norm Layer weights/parameters turning NaNs during training of a Siamese Network with Triplet Loss With reference to https://stackoverflow.com/questions/53400472/keras-model-weights-for-some-layers-become-all-nans , what I have noticed that the ""NaNs"" are present only in the ""batch_norm"" layers. Why must this be happening?

Is there some problem with the training - like Vanishing Gradient or Gradients exploding? or are the NaNs only placeholders for biases?

(Just a prior - I am trying to train a Siamese Network with Triplet Loss as mentioned here https://stackoverflow.com/questions/53400472/keras-model-weights-for-some-layers-become-all-nans (architecture and loss) ) 

Sample Weights listed here-
https://stackoverflow.com/questions/53830547/batch-norm-layer-weights-parameters-turning-nans-during-training-of-a-siamese-ne

Link to the Gist for the code I am using for training - https://gist.github.com/sidgairo18/dca347edd4588484237a231d7dab9a63

Please make sure that the boxes below are checked before you submit your issue.
If your issue is an **implementation question**, please ask your question on [StackOverflow](http://stackoverflow.com/questions/tagged/keras) or [on the Keras Slack channel](https://keras-slack-autojoin.herokuapp.com/) instead of opening a GitHub issue.

Thank you!

- [Yes] Check that you are up-to-date with the master branch of Keras. You can update with:


- [Yes] Check that your version of TensorFlow is up-to-date. The installation instructions can be found [here](https://www.tensorflow.org/get_started/os_setup).

- [Yes] Provide a link to a GitHub Gist of a Python script that can reproduce your issue (or just copy the script here if it is short)."
keras,2172,"Hi, 

I am modelling a simple text classification task with 3 target labels. I am using word2vec embeddings as input to the LSTM layer. 
 is the size of each sequence.  is the word2vec dimension. 
 is a 3D tensor of the shape 
 is a simple vector of labels like [0,2,1,0,1,0,...]



But while training this model, I always get accuracy scores of 1 like this:



What could be the issue here?
PS: I am using Keras 0.3.2
",1,Accuracy always 1 when using LSTM,"Accuracy always 1 when using LSTM Hi, 

I am modelling a simple text classification task with 3 target labels. I am using word2vec embeddings as input to the LSTM layer. 
 is the size of each sequence.  is the word2vec dimension. 
 is a 3D tensor of the shape 
 is a simple vector of labels like [0,2,1,0,1,0,...]



But while training this model, I always get accuracy scores of 1 like this:



What could be the issue here?
PS: I am using Keras 0.3.2
"
keras,3713,"Hi,

I am training a deep recurrent model on batches of a dataset at a time, by manually slicing the data and calling model.fit() on it. I noticed that when I use a larger dataset (same batch size, but more validation data), training slows down quite a bit, and monitoring CPU usage I can see that the process seems to be IO bound in some way (one core is constantly at 100%, and the other cores sporadically spike slightly, seemingly on minibatch start and end).

For reference, here is the some of the relevant part of my training code:



Just to clarify: 
- the model gets the same amount of training data in each  call, and that data is not a view but a copy of the relevant chunk of the big matrix)
- My systems RAM is far from being fully used
- I supply 2.5% of the whole dataset as validation data, so the amount of val data is higher if I use a larger dataset (note though that val testing does not seem to be what is slowing down training, as the CPU usage is poor during the epoch, not during testing)
- the larger the dataset, the more time one chunk takes to train. At 200k samples, it's around 240 seconds per  call, and at 2 million samples, it goes to around 1000 seconds.
- the time each epoch takes seems to be increasing with each call to 
- I've tried both RMSprop and SGD as optimizers, SGD seems to slow down less, but CPU usage is still bad on the large dataset

What could be slowing this down? If I drop the dataset size so that there are only around 2 chunks, CPU usage is basically perfect, and RAM does not seem to be the problem.

EDIT: the training time definitely appears to slow down with each epoch:


",1,Training slows down when using larger dataset,"Training slows down when using larger dataset Hi,

I am training a deep recurrent model on batches of a dataset at a time, by manually slicing the data and calling model.fit() on it. I noticed that when I use a larger dataset (same batch size, but more validation data), training slows down quite a bit, and monitoring CPU usage I can see that the process seems to be IO bound in some way (one core is constantly at 100%, and the other cores sporadically spike slightly, seemingly on minibatch start and end).

For reference, here is the some of the relevant part of my training code:



Just to clarify: 
- the model gets the same amount of training data in each  call, and that data is not a view but a copy of the relevant chunk of the big matrix)
- My systems RAM is far from being fully used
- I supply 2.5% of the whole dataset as validation data, so the amount of val data is higher if I use a larger dataset (note though that val testing does not seem to be what is slowing down training, as the CPU usage is poor during the epoch, not during testing)
- the larger the dataset, the more time one chunk takes to train. At 200k samples, it's around 240 seconds per  call, and at 2 million samples, it goes to around 1000 seconds.
- the time each epoch takes seems to be increasing with each call to 
- I've tried both RMSprop and SGD as optimizers, SGD seems to slow down less, but CPU usage is still bad on the large dataset

What could be slowing this down? If I drop the dataset size so that there are only around 2 chunks, CPU usage is basically perfect, and RAM does not seem to be the problem.

EDIT: the training time definitely appears to slow down with each epoch:


"
keras,9350,"I have been running LSTM keras default program using Sentiment corpus in jupyter notebook for 5 hours but not getting results. It show continue processing.
There is no gup error recovered.

The details of my system are as under:
 Linux-x86_64 operating system, 
NVIDIA Driver Version: 384.111, 
GPUs: GeForce GTX 960M (GPU 0)s 

I run example code of keras lstm first  to know the process than I shall run this code on my own corpus

datafile:
data = pd.read_csv('/home/mazhar/Downloads/Sentiment.csv')
# Keeping only the neccessary columns
data = data[['text','sentiment']]

Model:

embed_dim = 128
lstm_out = 196

model = Sequential()
model.add(Embedding(max_fatures, embed_dim,input_length = X.shape[1], dropout=0.2))
model.add(LSTM(lstm_out, dropout_U=0.2, dropout_W=0.2))
model.add(Dense(2,activation='softmax'))
model.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])
print(model.summary())

Please help and guide me to resolve my problem",1,LSTM is responding slow,"LSTM is responding slow I have been running LSTM keras default program using Sentiment corpus in jupyter notebook for 5 hours but not getting results. It show continue processing.
There is no gup error recovered.

The details of my system are as under:
 Linux-x86_64 operating system, 
NVIDIA Driver Version: 384.111, 
GPUs: GeForce GTX 960M (GPU 0)s 

I run example code of keras lstm first  to know the process than I shall run this code on my own corpus

datafile:
data = pd.read_csv('/home/mazhar/Downloads/Sentiment.csv')
# Keeping only the neccessary columns
data = data[['text','sentiment']]

Model:

embed_dim = 128
lstm_out = 196

model = Sequential()
model.add(Embedding(max_fatures, embed_dim,input_length = X.shape[1], dropout=0.2))
model.add(LSTM(lstm_out, dropout_U=0.2, dropout_W=0.2))
model.add(Dense(2,activation='softmax'))
model.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])
print(model.summary())

Please help and guide me to resolve my problem"
keras,8856,"I found that there is a shift for the  training loss, and when I tried to see why there is a shift, I set my own model to  non trainable:
![image](https://user-images.githubusercontent.com/13257051/34256650-4dd550c6-e691-11e7-8942-5bd249f5c1ed.png)
Then I  choose ten images as my training samples(they were also be used as the validation data), and set the batch-size to 5 and shuffle the data.Before I training my model, I do evaluate for my samples, the result is as below:
![image](https://user-images.githubusercontent.com/13257051/34256707-93529636-e691-11e7-8f7f-fa3b04684517.png)
Then I trained my model(actually it could not be trained)， and print the fit history, the result is as below:
![image](https://user-images.githubusercontent.com/13257051/34256735-bace98cc-e691-11e7-8b58-47f349909d8e.png)
It can be found that even the model is not be trained, and the validation loss never change, but the training loss were different among three epochs. And I found that if I set the shuffle to false, the result will be just right. I wanna know if this is a bug or the shuffle will make some difference when training. 
",1,Why the training loss changed even the trainable-params is zero？,"Why the training loss changed even the trainable-params is zero？ I found that there is a shift for the  training loss, and when I tried to see why there is a shift, I set my own model to  non trainable:
![image](https://user-images.githubusercontent.com/13257051/34256650-4dd550c6-e691-11e7-8942-5bd249f5c1ed.png)
Then I  choose ten images as my training samples(they were also be used as the validation data), and set the batch-size to 5 and shuffle the data.Before I training my model, I do evaluate for my samples, the result is as below:
![image](https://user-images.githubusercontent.com/13257051/34256707-93529636-e691-11e7-8f7f-fa3b04684517.png)
Then I trained my model(actually it could not be trained)， and print the fit history, the result is as below:
![image](https://user-images.githubusercontent.com/13257051/34256735-bace98cc-e691-11e7-8b58-47f349909d8e.png)
It can be found that even the model is not be trained, and the validation loss never change, but the training loss were different among three epochs. And I found that if I set the shuffle to false, the result will be just right. I wanna know if this is a bug or the shuffle will make some difference when training. 
"
keras,6808,"Hi everyone, 

I recently updated Keras to version 2.0.4 and I saw a big drop in performance compared to Keras 1.2.2.

I'm basically trying to find a mapping between movie titles and movie plot summaries (called synopses) using seq2seq LSTMs. While my model was pretty fast with Keras 1.2.2, it took a lots of time to train in the last version.

    model = Sequential()
    model.add(Embedding(vocab_size, EMBEDDING_DIM, input_length=MAX_LENGTH, mask_zero=True))
    model.add(LSTM(1024, return_sequences=True))
    model.add(LSTM(1024, return_sequences=True))
    model.add(TimeDistributed(Dense(vocab_size, activation='softmax')))
    model.compile(loss='sparse_categorical_crossentropy', optimizer=(Adam()), metrics=['accuracy'])

I did not change anything to this model or to the code in general between Keras 1.2.2 and Keras 2.0.4, trained on a Nvidia Quadro K6000 with Theano up-to-date in both cases and simply ran:  and  to update it.

However, the training is much slower.

Using Keras 1.2.2:


Using Keras 2.0.4: 


In the example above, the delta is only about a few seconds but when training on the whole dataset, it is much much slower.

Does anyone know what I'm doing wrong and could point me in the right direction ?",1,Training much slower on Keras 2.0.4 than on Keras 1.2.2,"Training much slower on Keras 2.0.4 than on Keras 1.2.2 Hi everyone, 

I recently updated Keras to version 2.0.4 and I saw a big drop in performance compared to Keras 1.2.2.

I'm basically trying to find a mapping between movie titles and movie plot summaries (called synopses) using seq2seq LSTMs. While my model was pretty fast with Keras 1.2.2, it took a lots of time to train in the last version.

    model = Sequential()
    model.add(Embedding(vocab_size, EMBEDDING_DIM, input_length=MAX_LENGTH, mask_zero=True))
    model.add(LSTM(1024, return_sequences=True))
    model.add(LSTM(1024, return_sequences=True))
    model.add(TimeDistributed(Dense(vocab_size, activation='softmax')))
    model.compile(loss='sparse_categorical_crossentropy', optimizer=(Adam()), metrics=['accuracy'])

I did not change anything to this model or to the code in general between Keras 1.2.2 and Keras 2.0.4, trained on a Nvidia Quadro K6000 with Theano up-to-date in both cases and simply ran:  and  to update it.

However, the training is much slower.

Using Keras 1.2.2:


Using Keras 2.0.4: 


In the example above, the delta is only about a few seconds but when training on the whole dataset, it is much much slower.

Does anyone know what I'm doing wrong and could point me in the right direction ?"
keras,10906,"I came to a weird problem when use Keras LSTM model. I build a single layer LSTM and try to play with it. I found the the output of model is different between single input and multiple inputs, as shown in the following code.
   
    def lstmTest(training, latent_dim=10):
            _, time_dim, input_dim = training.shape

            # Define an input sequence and process it.
            encoder_inputs = Input(shape=(time_dim, input_dim), name='input')
            encoder = LSTM(latent_dim, return_state=False, name='lstm')
            encoder_outputs = encoder(encoder_inputs)

            model = Model(encoder_inputs, encoder_outputs)

            return model

    def trainingTest(model, training, nb_epoch=10, batch_size=300):
            model.compile(optimizer='adam', loss='mse', metrics=['acc'])
            history = model.fit(training, training[:, -1, :10],
                                epochs=nb_epoch,
                                batch_size=batch_size,
                                shuffle=True,
                                verbose=1,
                                ).history
        return history

    myVector = [[[i]*20]*8 for i in range(100)]
    myVector = np.array(myVector)

    lstmTest = lstmTest(myVector)
    history = trainingTest(lstmTest, myVector)

    vector = myVector[:2]
    res1 = lstmTest.predict(vector)

    vector = myVector[:1]
    res2 = lstmTest.predict(vector)

    res2[0] - res1[0]


    array([0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 5.8207661e-11,
           0.0000000e+00, 2.3283064e-10, 0.0000000e+00, 0.0000000e+00,
           0.0000000e+00, 0.0000000e+00], dtype=float32)
`
But If I change res2 to the same as res1 I got the expected result

    array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)

I use tensorflow as the backend ",1,Keras LSTM has different output for single input,"Keras LSTM has different output for single input I came to a weird problem when use Keras LSTM model. I build a single layer LSTM and try to play with it. I found the the output of model is different between single input and multiple inputs, as shown in the following code.
   
    def lstmTest(training, latent_dim=10):
            _, time_dim, input_dim = training.shape

            # Define an input sequence and process it.
            encoder_inputs = Input(shape=(time_dim, input_dim), name='input')
            encoder = LSTM(latent_dim, return_state=False, name='lstm')
            encoder_outputs = encoder(encoder_inputs)

            model = Model(encoder_inputs, encoder_outputs)

            return model

    def trainingTest(model, training, nb_epoch=10, batch_size=300):
            model.compile(optimizer='adam', loss='mse', metrics=['acc'])
            history = model.fit(training, training[:, -1, :10],
                                epochs=nb_epoch,
                                batch_size=batch_size,
                                shuffle=True,
                                verbose=1,
                                ).history
        return history

    myVector = [[[i]*20]*8 for i in range(100)]
    myVector = np.array(myVector)

    lstmTest = lstmTest(myVector)
    history = trainingTest(lstmTest, myVector)

    vector = myVector[:2]
    res1 = lstmTest.predict(vector)

    vector = myVector[:1]
    res2 = lstmTest.predict(vector)

    res2[0] - res1[0]


    array([0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 5.8207661e-11,
           0.0000000e+00, 2.3283064e-10, 0.0000000e+00, 0.0000000e+00,
           0.0000000e+00, 0.0000000e+00], dtype=float32)
`
But If I change res2 to the same as res1 I got the expected result

    array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)

I use tensorflow as the backend "
keras,667,"Hi,

I was wondering if there is an example for an lstm layer that eventually feeds into a regression output layer. I am having a hard time putting one together.

I am hoping to feed in one sample at a time, where sample looks like this: 

input: [[x,x,x,x],[x,x,x,x],[[x,x,x,x],] output: [y]

This way, I would expect an lstm layer that takes in 4 inputs and after 3 iterations produces an output h to feed into a regression layer.

Cheers
",1,Regression output with LSTM input layer,"Regression output with LSTM input layer Hi,

I was wondering if there is an example for an lstm layer that eventually feeds into a regression output layer. I am having a hard time putting one together.

I am hoping to feed in one sample at a time, where sample looks like this: 

input: [[x,x,x,x],[x,x,x,x],[[x,x,x,x],] output: [y]

This way, I would expect an lstm layer that takes in 4 inputs and after 3 iterations produces an output h to feed into a regression layer.

Cheers
"
keras,6300,"There is a regression in predicted values for a simple Dense model between Keras v1.2.2 and v2.0. Code to reproduce is below.

Using Keras v1.2.2 commit 4fa7e5d
Using Keras April 17 v2.0 commit 73bf06fb023a8b37ddf2e2a168bbf920c7a6c766
Using Theano April 17 TOT commit 388805f946685e86225cdf602eb8a4f0059f9667
Running on Theano.Cuda with GeForce GT 750M, CuDNN v5105

",1,Regression in initial Dense model prediction between Keras v1.2.2 and v2.0,"Regression in initial Dense model prediction between Keras v1.2.2 and v2.0 There is a regression in predicted values for a simple Dense model between Keras v1.2.2 and v2.0. Code to reproduce is below.

Using Keras v1.2.2 commit 4fa7e5d
Using Keras April 17 v2.0 commit 73bf06fb023a8b37ddf2e2a168bbf920c7a6c766
Using Theano April 17 TOT commit 388805f946685e86225cdf602eb8a4f0059f9667
Running on Theano.Cuda with GeForce GT 750M, CuDNN v5105

"
keras,2718,"Does anyone have any ideas as to why the validation accuracy curve would be this noisy?

![screen shot 2016-05-13 at 10 22 04 am](https://cloud.githubusercontent.com/assets/6086781/15250847/92ada084-18f4-11e6-9527-b0871e776065.png)

For completeness, I'll mention that the ""validation set"" is systematically different than the training set (this is a transfer learning situation).  I can provide more details as needed about the specific application and model, but was interested if there was some kind of generic explanation for the noisy accuracy curve.
",1,Noisy Validation Accuracy,"Noisy Validation Accuracy Does anyone have any ideas as to why the validation accuracy curve would be this noisy?

![screen shot 2016-05-13 at 10 22 04 am](https://cloud.githubusercontent.com/assets/6086781/15250847/92ada084-18f4-11e6-9527-b0871e776065.png)

For completeness, I'll mention that the ""validation set"" is systematically different than the training set (this is a transfer learning situation).  I can provide more details as needed about the specific application and model, but was interested if there was some kind of generic explanation for the noisy accuracy curve.
"
keras,4256,"Theano [recently introduced](https://github.com/Theano/Theano/pull/4915) cuDNN5 support for recurrent networks and I decided to give it a spin with a Keras LSTM network. However, I noticed only a marginal benefit that seems more likely noise than anything else. Any reason I shouldn't be seeing bigger performance improvements?

cuDNN 4, theano 0.8.2, keras 1.1.0
Train on 3000000 samples, validate on 857222 samples
Epoch 1/100
3000000/3000000 [==============================] - 781s - loss: 2.2539 - acc: 0.6299 - val_loss: 1.6206 - val_acc: 0.7274

cuDNN 5, theano 0.9.0, keras 1.1.0
Train on 3000000 samples, validate on 857222 samples
Epoch 1/100
3000000/3000000 [==============================] - 765s - loss: 2.2588 - acc: 0.6289 - val_loss: 1.6214 - val_acc: 0.7274

cuDNN 5, theano 0.9.0, keras 1.1.1
Train on 3000000 samples, validate on 857222 samples
Epoch 1/100
3000000/3000000 [==============================] - 761s - loss: 2.2578 - acc: 0.6292 - val_loss: 1.6234 - val_acc: 0.7271
",1,cuDNN 5 and Theano 0.9.0 not improving LSTM performance,"cuDNN 5 and Theano 0.9.0 not improving LSTM performance Theano [recently introduced](https://github.com/Theano/Theano/pull/4915) cuDNN5 support for recurrent networks and I decided to give it a spin with a Keras LSTM network. However, I noticed only a marginal benefit that seems more likely noise than anything else. Any reason I shouldn't be seeing bigger performance improvements?

cuDNN 4, theano 0.8.2, keras 1.1.0
Train on 3000000 samples, validate on 857222 samples
Epoch 1/100
3000000/3000000 [==============================] - 781s - loss: 2.2539 - acc: 0.6299 - val_loss: 1.6206 - val_acc: 0.7274

cuDNN 5, theano 0.9.0, keras 1.1.0
Train on 3000000 samples, validate on 857222 samples
Epoch 1/100
3000000/3000000 [==============================] - 765s - loss: 2.2588 - acc: 0.6289 - val_loss: 1.6214 - val_acc: 0.7274

cuDNN 5, theano 0.9.0, keras 1.1.1
Train on 3000000 samples, validate on 857222 samples
Epoch 1/100
3000000/3000000 [==============================] - 761s - loss: 2.2578 - acc: 0.6292 - val_loss: 1.6234 - val_acc: 0.7271
"
keras,5285,"I have a lot of sample images in three classes, but some of the labels of them are wrong, so the trained classifier can not achieve good performance, and correct the wrong labels needs to consume a lot of time.
To eliminate the effect of error labels, I intend to redefine the loss function like that: When calculating the losses of a batch of data, the largest loss is removed from the others at this batch. And then calculate the mean of the remaining elements as the loss value of this batch.
What should I do to achieve this goal, or is there any other better way to achieve the same purpose?",1,Customize loss function in a way that ignore the samples with large loss,"Customize loss function in a way that ignore the samples with large loss I have a lot of sample images in three classes, but some of the labels of them are wrong, so the trained classifier can not achieve good performance, and correct the wrong labels needs to consume a lot of time.
To eliminate the effect of error labels, I intend to redefine the loss function like that: When calculating the losses of a batch of data, the largest loss is removed from the others at this batch. And then calculate the mean of the remaining elements as the loss value of this batch.
What should I do to achieve this goal, or is there any other better way to achieve the same purpose?"
keras,6825,"

If you comment out the  lines, or change  to a smaller number then it works fine (MSE loss is 1.0).",1,loss function doesn't ignore large masked values,"loss function doesn't ignore large masked values 

If you comment out the  lines, or change  to a smaller number then it works fine (MSE loss is 1.0)."
keras,2730,"I have found fit_generator to be quite slow.
This makes the disk read speed a major bottleneck when the network is relatively shallow.

Using the multiprocessing module instead of the threading one, I was able to get significant improvements in speed (half the time) as shown in this [gist example](https://gist.github.com/tdeboissiere/195dde7fddfcf622a82a895b90d2c800).

I could look into making a pull request with this new implementation if you think the speed gain is worth it.
",1,Improving the speed of fit_generator,"Improving the speed of fit_generator I have found fit_generator to be quite slow.
This makes the disk read speed a major bottleneck when the network is relatively shallow.

Using the multiprocessing module instead of the threading one, I was able to get significant improvements in speed (half the time) as shown in this [gist example](https://gist.github.com/tdeboissiere/195dde7fddfcf622a82a895b90d2c800).

I could look into making a pull request with this new implementation if you think the speed gain is worth it.
"
keras,3755,"I am training a deep CNN (4 layers) on my data. I used ""categorical_crossentropy"" as the loss function.
During training, the training loss keeps decreasing and training accuracy keeps increasing until convergence.  But the validation loss started increasing while the validation accuracy is still improving. 
The curves of loss and accuracy are shown in the following figures:
![acc_cnn32p-32p-32p-32p_adam1e-4_5000](https://cloud.githubusercontent.com/assets/8138843/18455543/3b15fc32-7910-11e6-93a5-72374837a78d.png)
![loss_cnn32p-32p-32p-32p_adam1e-4_5000](https://cloud.githubusercontent.com/assets/8138843/18455542/3b14ebee-7910-11e6-873b-ca2a4dc3dfe7.png)

It also seems that the validation loss will keep going up if I train the model for more epochs. Does anyone have idea what's going on here? 

Thanks a lot!
",1,Validation loss increases while validation accuracy is still improving,"Validation loss increases while validation accuracy is still improving I am training a deep CNN (4 layers) on my data. I used ""categorical_crossentropy"" as the loss function.
During training, the training loss keeps decreasing and training accuracy keeps increasing until convergence.  But the validation loss started increasing while the validation accuracy is still improving. 
The curves of loss and accuracy are shown in the following figures:
![acc_cnn32p-32p-32p-32p_adam1e-4_5000](https://cloud.githubusercontent.com/assets/8138843/18455543/3b15fc32-7910-11e6-93a5-72374837a78d.png)
![loss_cnn32p-32p-32p-32p_adam1e-4_5000](https://cloud.githubusercontent.com/assets/8138843/18455542/3b14ebee-7910-11e6-873b-ca2a4dc3dfe7.png)

It also seems that the validation loss will keep going up if I train the model for more epochs. Does anyone have idea what's going on here? 

Thanks a lot!
"
keras,12977,"the model below is from  [website][1] and it behaves exactly as expected. It is defined with . I want to convert it to be defined with  to make it more flexible for my future use. But after my conversion, the performance plummeted. 

The original model you can find on  website:


The following code is my conversion:

The only difference I can see is  doesn't count  while  counts it, but I don't believe they make the model structure different. However, the performance of  is:

[![enter image description here][2]][2]


While the performance of the  I converted is:

[![enter image description here][3]][3]


  [1]: https://www.tensorflow.org/tutorials/keras/basic_regression
  [2]: https://i.stack.imgur.com/PdtrA.png
  [3]: https://i.stack.imgur.com/vfvxp.png

### Can anyone tell me what I did wrong?
### Some other context:
I have read this [post](https://github.com/keras-team/keras/issues/8001), but my code are all run on CPU in Google Colab





Code to plot the losses:

Code to train the model(it's exact same for both models):


Any suggestion is appreciated!",1,training performance are different with exact same data and architecture. The only difference is using .Sequential() or .Model(),"training performance are different with exact same data and architecture. The only difference is using .Sequential() or .Model() the model below is from  [website][1] and it behaves exactly as expected. It is defined with . I want to convert it to be defined with  to make it more flexible for my future use. But after my conversion, the performance plummeted. 

The original model you can find on  website:


The following code is my conversion:

The only difference I can see is  doesn't count  while  counts it, but I don't believe they make the model structure different. However, the performance of  is:

[![enter image description here][2]][2]


While the performance of the  I converted is:

[![enter image description here][3]][3]


  [1]: https://www.tensorflow.org/tutorials/keras/basic_regression
  [2]: https://i.stack.imgur.com/PdtrA.png
  [3]: https://i.stack.imgur.com/vfvxp.png

### Can anyone tell me what I did wrong?
### Some other context:
I have read this [post](https://github.com/keras-team/keras/issues/8001), but my code are all run on CPU in Google Colab





Code to plot the losses:

Code to train the model(it's exact same for both models):


Any suggestion is appreciated!"
keras,3766,"I run the example code for LSTM networks that uses imdb dataset in Keras. One can find the code in the following link. [https://github.com/fchollet/keras/blob/master/examples/imdb_lstm.py](url)

My problem is that as code progresses the training loss decreases and training accuracy increases as expected but validation accuracy fluctuates in an interval and validation loss increases to a high value. I attach a part of the log of the training phase below. Even I observe that when training loss is very small (~ 0.01-0.03) sometimes it increases in the next epoch and then it decreases again. What I mention can be seen in epochs 75-77. But in general it decreases. Is this behaviour expected at higher number of epochs? (I mean the fluctuations in the train_loss and train_acc.)

What I expect is that training accuracy always increases up to 0.99-1 and training loss always decreases. Moreover, the validation accuracy should start from maybe 0.4 and raise to for example 0.8 in the end. If validation accuracy does not improve over epochs what is the point of waiting during epochs? Also the test accuracy is close 0.81 at the end. I checked overfitting. But I could not come up with an explanation for fluctuations in val_acc with overfitting. What may be causing these fluctuations?

I also tried with my own data and came up with same situation. I processed my data in a similar way. I mean my training, validation and test points are processed in same logic as the ones in this example code.

Besides, I think the code takes the last output from LSTM for all time steps in a sample before feeding it to a dense layer.  What if I want to take the mean or max of all outputs for all time steps in LSTM layer before dense layer? How will I apply these operations in Keras? As far as I know return_sequence=True must be used. But how to combine outputs for all time steps in a sample to obtain a single representation of a sentence before dense layer?

Any help would be appreciated.

Using Theano backend.
Loading data...
25000 train sequences
25000 test sequences
Pad sequences (samples x time)
X_train shape: (25000, 80)
X_test shape: (25000, 80)
Build model...
Train...
Train on 22500 samples, validate on 2500 samples
Epoch 1/100
22500/22500 [==============================] - 236s - loss: 0.5438 - acc: 0.7209 - val_loss: 0.4305 - val_acc: 0.8076
Epoch 2/100
22500/22500 [==============================] - 237s - loss: 0.3843 - acc: 0.8346 - val_loss: 0.3791 - val_acc: 0.8332
Epoch 3/100
22500/22500 [==============================] - 245s - loss: 0.3099 - acc: 0.8716 - val_loss: 0.3736 - val_acc: 0.8440
Epoch 4/100
22500/22500 [==============================] - 243s - loss: 0.2458 - acc: 0.9023 - val_loss: 0.4206 - val_acc: 0.8372
Epoch 5/100
22500/22500 [==============================] - 239s - loss: 0.2120 - acc: 0.9138 - val_loss: 0.3844 - val_acc: 0.8384
....
....
Epoch 75/100
22500/22500 [==============================] - 238s - loss: 0.0134 - acc: 0.9868 - val_loss: 0.9045 - val_acc: 0.8132
Epoch 76/100
22500/22500 [==============================] - 241s - loss: 0.0156 - acc: 0.9845 - val_loss: 0.9078 - val_acc: 0.8211
Epoch 77/100
22500/22500 [==============================] - 235s - loss: 0.0129 - acc: 0.9883 - val_loss: 0.9105 - val_acc: 0.8234
",1,Fluctuating validation accuracy in LSTM Network and Different ways of sentence representation,"Fluctuating validation accuracy in LSTM Network and Different ways of sentence representation I run the example code for LSTM networks that uses imdb dataset in Keras. One can find the code in the following link. [https://github.com/fchollet/keras/blob/master/examples/imdb_lstm.py](url)

My problem is that as code progresses the training loss decreases and training accuracy increases as expected but validation accuracy fluctuates in an interval and validation loss increases to a high value. I attach a part of the log of the training phase below. Even I observe that when training loss is very small (~ 0.01-0.03) sometimes it increases in the next epoch and then it decreases again. What I mention can be seen in epochs 75-77. But in general it decreases. Is this behaviour expected at higher number of epochs? (I mean the fluctuations in the train_loss and train_acc.)

What I expect is that training accuracy always increases up to 0.99-1 and training loss always decreases. Moreover, the validation accuracy should start from maybe 0.4 and raise to for example 0.8 in the end. If validation accuracy does not improve over epochs what is the point of waiting during epochs? Also the test accuracy is close 0.81 at the end. I checked overfitting. But I could not come up with an explanation for fluctuations in val_acc with overfitting. What may be causing these fluctuations?

I also tried with my own data and came up with same situation. I processed my data in a similar way. I mean my training, validation and test points are processed in same logic as the ones in this example code.

Besides, I think the code takes the last output from LSTM for all time steps in a sample before feeding it to a dense layer.  What if I want to take the mean or max of all outputs for all time steps in LSTM layer before dense layer? How will I apply these operations in Keras? As far as I know return_sequence=True must be used. But how to combine outputs for all time steps in a sample to obtain a single representation of a sentence before dense layer?

Any help would be appreciated.

Using Theano backend.
Loading data...
25000 train sequences
25000 test sequences
Pad sequences (samples x time)
X_train shape: (25000, 80)
X_test shape: (25000, 80)
Build model...
Train...
Train on 22500 samples, validate on 2500 samples
Epoch 1/100
22500/22500 [==============================] - 236s - loss: 0.5438 - acc: 0.7209 - val_loss: 0.4305 - val_acc: 0.8076
Epoch 2/100
22500/22500 [==============================] - 237s - loss: 0.3843 - acc: 0.8346 - val_loss: 0.3791 - val_acc: 0.8332
Epoch 3/100
22500/22500 [==============================] - 245s - loss: 0.3099 - acc: 0.8716 - val_loss: 0.3736 - val_acc: 0.8440
Epoch 4/100
22500/22500 [==============================] - 243s - loss: 0.2458 - acc: 0.9023 - val_loss: 0.4206 - val_acc: 0.8372
Epoch 5/100
22500/22500 [==============================] - 239s - loss: 0.2120 - acc: 0.9138 - val_loss: 0.3844 - val_acc: 0.8384
....
....
Epoch 75/100
22500/22500 [==============================] - 238s - loss: 0.0134 - acc: 0.9868 - val_loss: 0.9045 - val_acc: 0.8132
Epoch 76/100
22500/22500 [==============================] - 241s - loss: 0.0156 - acc: 0.9845 - val_loss: 0.9078 - val_acc: 0.8211
Epoch 77/100
22500/22500 [==============================] - 235s - loss: 0.0129 - acc: 0.9883 - val_loss: 0.9105 - val_acc: 0.8234
"
keras,10943,"Hi,
I have a pre-trained model that I try to use. It has an h5 weight file that I loaded into Keras.
But when I run it using Theano, the output is not the same as TensorFlow/CNTK (TensorFlow and CNTK agree with each other)

On further investigation, I found out that the attribute original_backend in this file is set to ""plaidml"" which from what I understand is one of the computational engines for TensorFlow. But in the function ""_need_convert_kernel"" in engine/saving.py, it does not convert if the original_backend is not in the lookup table. Which mean when I sue Theano we need to convert the weight but Keras does not which lead to inconsistent output. I have top1 accuracy for TensorFlow and CNTK around 91% but only 44% for Theano.",1,Inconsistent run when using pre-trained h5 weight file on Theano vs TensorFlow/CNTK,"Inconsistent run when using pre-trained h5 weight file on Theano vs TensorFlow/CNTK Hi,
I have a pre-trained model that I try to use. It has an h5 weight file that I loaded into Keras.
But when I run it using Theano, the output is not the same as TensorFlow/CNTK (TensorFlow and CNTK agree with each other)

On further investigation, I found out that the attribute original_backend in this file is set to ""plaidml"" which from what I understand is one of the computational engines for TensorFlow. But in the function ""_need_convert_kernel"" in engine/saving.py, it does not convert if the original_backend is not in the lookup table. Which mean when I sue Theano we need to convert the weight but Keras does not which lead to inconsistent output. I have top1 accuracy for TensorFlow and CNTK around 91% but only 44% for Theano."
keras,6339,"I’ve upgraded to the new verison of Keras and I’m noticing a huge a drop in the performance: from 200 seconds per epoch on the old Keras to 40,000 seconds per epoch on the new Keras.

Attached are two shots showing training on the old Keras and on the new. The models have same architecture are confirmed to both be utilizing the same GPU during training. They are both coded with the functional API. The problem does not change when using different tensorflow backends (0.12 and 1.0). 

After doing these side-by-side tests, I’m thinking this may be a bug with Keras.

Training Output of Keras 1.1.0 (270 sec/epoch):
![kerasold](https://cloud.githubusercontent.com/assets/25262161/25255979/b878359e-25fb-11e7-857a-2c37d7be4561.png)

Training Output of Keras 2.0.3 (38000 sec/epoch):

![kerasnew](https://cloud.githubusercontent.com/assets/25262161/25256006/d7bdf786-25fb-11e7-8df3-93f0bdbf1ea3.png)

",1,Large Differences in Performance between Keras 2.0.3 and Keras 1.1.0 – Potential Bug?,"Large Differences in Performance between Keras 2.0.3 and Keras 1.1.0 – Potential Bug? I’ve upgraded to the new verison of Keras and I’m noticing a huge a drop in the performance: from 200 seconds per epoch on the old Keras to 40,000 seconds per epoch on the new Keras.

Attached are two shots showing training on the old Keras and on the new. The models have same architecture are confirmed to both be utilizing the same GPU during training. They are both coded with the functional API. The problem does not change when using different tensorflow backends (0.12 and 1.0). 

After doing these side-by-side tests, I’m thinking this may be a bug with Keras.

Training Output of Keras 1.1.0 (270 sec/epoch):
![kerasold](https://cloud.githubusercontent.com/assets/25262161/25255979/b878359e-25fb-11e7-857a-2c37d7be4561.png)

Training Output of Keras 2.0.3 (38000 sec/epoch):

![kerasnew](https://cloud.githubusercontent.com/assets/25262161/25256006/d7bdf786-25fb-11e7-8df3-93f0bdbf1ea3.png)

"
keras,10947,"Hi there,

Running on my Macbook Pro (no GPU).

I have a simple CNN very similar to [this](https://github.com/keras-team/keras/blob/master/examples/mnist_cnn.py) example. What I'm finding is that doing:



Is faster than the total time when running each example individually. 



Any reason for this behavior? Is it always faster to run in batches like this? 

 ",1,model.predict() runs faster when test examples are passed in as batch vs one by one.,"model.predict() runs faster when test examples are passed in as batch vs one by one. Hi there,

Running on my Macbook Pro (no GPU).

I have a simple CNN very similar to [this](https://github.com/keras-team/keras/blob/master/examples/mnist_cnn.py) example. What I'm finding is that doing:



Is faster than the total time when running each example individually. 



Any reason for this behavior? Is it always faster to run in batches like this? 

 "
keras,707,"As far as I understand, it seems that accuracy doesn't take into account possible timesteps masking (through ).

In  model, categorical accuracy is computed as:



while a more correct approach will take into account :



However, I see in [theano documentation](http://deeplearning.net/software/theano/library/tensor/basic.html#indexing) that it won't work, and indeed it doesn't :)

Any intention/idea how to fix it?
",1,Wrong accuracy with masking,"Wrong accuracy with masking As far as I understand, it seems that accuracy doesn't take into account possible timesteps masking (through ).

In  model, categorical accuracy is computed as:



while a more correct approach will take into account :



However, I see in [theano documentation](http://deeplearning.net/software/theano/library/tensor/basic.html#indexing) that it won't work, and indeed it doesn't :)

Any intention/idea how to fix it?
"
keras,6341,"I am trying to understand the behavior of fit_generator so that I can use it with dataset too large to fit in my memory, so I use the following generator function to imitate the behavior of fit (keras version 1.2.0):

1. load x_data_train and y_data_train into the memory

2.  create a simple sequential model

3. define the generator function in the following way:
     

4. use the generator function in fit_generator

The fit_generator works fine with my simple toy data, but for true data, the result is totally different from using fit with the same batch_size and the same dataset: the loss either does not get better at all or generates some crazy output. In addition, the fit_generator is faster even though I expect it to be slower. To make things more weird, fit_generator is faster than fit even if I am loading dataset from hard disk with similar generator function.

I do not see any difference between this fit_generator and fit expect my generator is sampling with replacement, but I do not think it will create such a big difference in behaviour. ",1,Different Behaviour Between fit and fit_generator,"Different Behaviour Between fit and fit_generator I am trying to understand the behavior of fit_generator so that I can use it with dataset too large to fit in my memory, so I use the following generator function to imitate the behavior of fit (keras version 1.2.0):

1. load x_data_train and y_data_train into the memory

2.  create a simple sequential model

3. define the generator function in the following way:
     

4. use the generator function in fit_generator

The fit_generator works fine with my simple toy data, but for true data, the result is totally different from using fit with the same batch_size and the same dataset: the loss either does not get better at all or generates some crazy output. In addition, the fit_generator is faster even though I expect it to be slower. To make things more weird, fit_generator is faster than fit even if I am loading dataset from hard disk with similar generator function.

I do not see any difference between this fit_generator and fit expect my generator is sampling with replacement, but I do not think it will create such a big difference in behaviour. "
keras,198,"Training a binary classification net with the  loss function, I found that when the  method has managed to correctly classify all test cases, the error will become 0 and thus the loss function becomes NaN. Any further training steps cause all future calls to  to return only NaNs as well, which is certainly not intended.

Ideally, the net would stop training once this occurs. At the very least, the case of the loss function becoming NaN should be handled gracefully.
",1,NaN when accuracy reaches 1 with logistic loss,"NaN when accuracy reaches 1 with logistic loss Training a binary classification net with the  loss function, I found that when the  method has managed to correctly classify all test cases, the error will become 0 and thus the loss function becomes NaN. Any further training steps cause all future calls to  to return only NaNs as well, which is certainly not intended.

Ideally, the net would stop training once this occurs. At the very least, the case of the loss function becoming NaN should be handled gracefully.
"
keras,3782,"I'm using Theano backend on Keras running on jupyter ipython notebook. Everything was working fine couples of days ago, but today I notice that it is running very very slow !, I think running it on CPU is much faster than that. I'm running [ mnist example](https://github.com/fchollet/keras/blob/master/examples/mnist_cnn.py) without making any changes. The batch (128 size) takes too long to process, around 5 minutes or something to finish, the next batch takes the same time almost or longer, it used to run mnist example very very fast (170 seconds for the whole epoch). 

Please help me how could I figure out the problem
Note: I tried it using anaconda then I tried it using winpython and the same problem occur.

I'm using windows 10, 16GB ram, Nvidia GTX 660 2 GB, cuDNN either enabled or disabled the same problem occur

[Theano issues thread](https://github.com/Theano/Theano/issues/4974)
",1,Theano is running very slow on GPU,"Theano is running very slow on GPU I'm using Theano backend on Keras running on jupyter ipython notebook. Everything was working fine couples of days ago, but today I notice that it is running very very slow !, I think running it on CPU is much faster than that. I'm running [ mnist example](https://github.com/fchollet/keras/blob/master/examples/mnist_cnn.py) without making any changes. The batch (128 size) takes too long to process, around 5 minutes or something to finish, the next batch takes the same time almost or longer, it used to run mnist example very very fast (170 seconds for the whole epoch). 

Please help me how could I figure out the problem
Note: I tried it using anaconda then I tried it using winpython and the same problem occur.

I'm using windows 10, 16GB ram, Nvidia GTX 660 2 GB, cuDNN either enabled or disabled the same problem occur

[Theano issues thread](https://github.com/Theano/Theano/issues/4974)
"
keras,13521,"Hello,

I noticed I always got different results between fit_generator and fit.
Specifically, I noticed I always got good result **faster** with fit compared to using fit_generator. By faster I mean given the same epoch training the model with fit always gives better than fit_generator. I should also note that I make sure I implemented generator properly including shuffling with fit_generator.

Also I often noticed using fit gets better result compared to using fit_generator.

I am just curious: Do you observe the same behavior? If so, what is in fit that fit_generator does not have?",1,Got different results between using fit and fit_generator,"Got different results between using fit and fit_generator Hello,

I noticed I always got different results between fit_generator and fit.
Specifically, I noticed I always got good result **faster** with fit compared to using fit_generator. By faster I mean given the same epoch training the model with fit always gives better than fit_generator. I should also note that I make sure I implemented generator properly including shuffling with fit_generator.

Also I often noticed using fit gets better result compared to using fit_generator.

I am just curious: Do you observe the same behavior? If so, what is in fit that fit_generator does not have?"
keras,2259,"Pretrained models are often finetuned on small datasets. For these use cases it can be relevant to add dropout to prevent overfitting. However, adding recurrent dropout to a loaded model (and recompiling the model) has no effect at all.

I've modified the  to demonstrate this. With recurrent dropout of 0.999, the model should not be able to learn anything, but it actually starts overfitting dramatically (it gets an accuracy of ~99% after 3 epochs). See the code below.


",1,Bug: Recurrent dropout fails silenty when set on loaded model,"Bug: Recurrent dropout fails silenty when set on loaded model Pretrained models are often finetuned on small datasets. For these use cases it can be relevant to add dropout to prevent overfitting. However, adding recurrent dropout to a loaded model (and recompiling the model) has no effect at all.

I've modified the  to demonstrate this. With recurrent dropout of 0.999, the model should not be able to learn anything, but it actually starts overfitting dramatically (it gets an accuracy of ~99% after 3 epochs). See the code below.


"
keras,724,"I have a list of sequential values. I want to feed them into a RNN to predict the next value in the sequence.

[ 0.43589744  0.44230769  0.49358974 ...,  0.71153846  0.70833333 0.69230769]

I keep getting an accuracy of 1.0. I found a similar issue for classification but no methods used there worked for me.

I get a decreasing loss but my accuracy is always 1.0

How can I fix this?

model = Sequential()
model.add(SimpleRNN(1, 100))
model.add(Dense(100, 1, activation = ""sigmoid""))
model.compile(loss=""mean_squared_error"", optimizer = ""sgd"")

Epoch 0
1517/1517 [==============================] - 0s - loss: 0.0726 - acc: 1.0000 - val_loss: 0.0636 - val_acc: 1.0000
Epoch 1
1517/1517 [==============================] - 0s - loss: 0.0720 - acc: 1.0000 - val_loss: 0.0629 - val_acc: 1.0000
...
",1,Predictions using RNNs - Accuracy always 1.0,"Predictions using RNNs - Accuracy always 1.0 I have a list of sequential values. I want to feed them into a RNN to predict the next value in the sequence.

[ 0.43589744  0.44230769  0.49358974 ...,  0.71153846  0.70833333 0.69230769]

I keep getting an accuracy of 1.0. I found a similar issue for classification but no methods used there worked for me.

I get a decreasing loss but my accuracy is always 1.0

How can I fix this?

model = Sequential()
model.add(SimpleRNN(1, 100))
model.add(Dense(100, 1, activation = ""sigmoid""))
model.compile(loss=""mean_squared_error"", optimizer = ""sgd"")

Epoch 0
1517/1517 [==============================] - 0s - loss: 0.0726 - acc: 1.0000 - val_loss: 0.0636 - val_acc: 1.0000
Epoch 1
1517/1517 [==============================] - 0s - loss: 0.0720 - acc: 1.0000 - val_loss: 0.0629 - val_acc: 1.0000
...
"
keras,1237,"Hi, 
I'm trying to build a bi-directional GRU
 to do gender recognition on text. My model is as follows 

model = Graph()
    model.add_input(name = 'input', input_shape = (None,max_features), dtype = 'float')
    model.add_node(GRU(100, activation='sigmoid', inner_activation='hard_sigmoid'), name='forward', input='input')
    #backwards in time direction
    model.add_node(GRU(100, activation='sigmoid', inner_activation='hard_sigmoid', go_backwards = True), name='backward', input='input')
    model.add_node(Dropout(0.5), name = 'first_dropout', inputs = ['forward','backward'])
    model.add_node(Dense(1, activation='sigmoid'), name='sigmoid', input='first_dropout')
    model.add_output(name='output', input='sigmoid')
    model.compile('SGD',  {'output': 'binary_crossentropy'})

My data is a 3D numpy array (x,y,z) where x is the number of samples, y is the time dimension (Words in the document example) and z is the word2vec encoding of this word (input dimension). The x and y are ints and the z are floats. The labels are binary.

Unfortunately, when I train, the cross entropy loss seems to jump all over the place, and I end up with a test accuracy of 0. Do you have any idea why this is? 
<img width=""458"" alt=""screen shot 2015-12-10 at 8 48 14 am"" src=""https://cloud.githubusercontent.com/assets/3630063/11716984/67eca744-9f1b-11e5-941c-3ca033196ba4.png"">

Thank you very much!
",1,loss increases as training progresses,"loss increases as training progresses Hi, 
I'm trying to build a bi-directional GRU
 to do gender recognition on text. My model is as follows 

model = Graph()
    model.add_input(name = 'input', input_shape = (None,max_features), dtype = 'float')
    model.add_node(GRU(100, activation='sigmoid', inner_activation='hard_sigmoid'), name='forward', input='input')
    #backwards in time direction
    model.add_node(GRU(100, activation='sigmoid', inner_activation='hard_sigmoid', go_backwards = True), name='backward', input='input')
    model.add_node(Dropout(0.5), name = 'first_dropout', inputs = ['forward','backward'])
    model.add_node(Dense(1, activation='sigmoid'), name='sigmoid', input='first_dropout')
    model.add_output(name='output', input='sigmoid')
    model.compile('SGD',  {'output': 'binary_crossentropy'})

My data is a 3D numpy array (x,y,z) where x is the number of samples, y is the time dimension (Words in the document example) and z is the word2vec encoding of this word (input dimension). The x and y are ints and the z are floats. The labels are binary.

Unfortunately, when I train, the cross entropy loss seems to jump all over the place, and I end up with a test accuracy of 0. Do you have any idea why this is? 
<img width=""458"" alt=""screen shot 2015-12-10 at 8 48 14 am"" src=""https://cloud.githubusercontent.com/assets/3630063/11716984/67eca744-9f1b-11e5-941c-3ca033196ba4.png"">

Thank you very much!
"
keras,8929,"I am posting this issue here since no body in stackoverflow seems to know what it is the problem. I assume then that there is a bug and post here.

I am implementing a variant of  the CNN described by [this paper][1].
My problem is that the loss isn't decreasing and I don't understand why. Same have to be said concerning accuracy(stuck at 0.5 more or less).

This a problem of 2 classes classification. I am using the data from this  [website][2]:

I suspected the optimizer so I changed it whithout any improvements. I am pretty sure the data I am using is ok because I used it on an LSTM and the classifier was fine. 


Here is my code:

    from keras.layers import Embedding
    from keras.layers import Conv2D
    from keras.models import Sequential
    from keras.layers import MaxPooling2D
    from keras.layers import Reshape
    from keras.layers import Flatten
    from keras.layers import Dense
    from keras.layers import Dropout
    from keras.layers import Input
    from keras import backend as K
    
    from keras.models import Model
    import tensorflow as tf
    from sklearn import preprocessing
    import keras
    import numpy as np
    import pdb
    
    #using multiple filters
    config = tf.ConfigProto()
     
    # Don't pre-allocate memory; allocate as-needed
    config.gpu_options.allow_growth = True
     
    # Only allow a total of half the GPU memory to be allocated
    config.gpu_options.per_process_gpu_memory_fraction = 0.7
     
    # Create a session with the above options specified.
    config.gpu_options.per_process_gpu_memory_fraction = 0.65
    K.tensorflow_backend.set_session(tf.Session(config=config))
    class Classifier():
        def __init__(self,Vocab,maxlen=75):
            self.model=Sequential()
            self.EMBED_DIM=30
            self.batch_size=30
            self.MAX_SEQUENCE_LENGTH=maxlen
            self.nb_labels=2
            self.model = Sequential()
            self.Vocab=Vocab
        def fit(self, X,y):
            #pdb.set_trace()
            
            mainIn=Input(shape=(self.MAX_SEQUENCE_LENGTH,), dtype='int32', name='main_input')
            
            x=Embedding(len(self.Vocab)+2,self.EMBED_DIM,input_length=self.MAX_SEQUENCE_LENGTH)(mainIn)
            
            x=Reshape((1,self.MAX_SEQUENCE_LENGTH, self.EMBED_DIM))(x)
            
            x1=Conv2D(128, strides=2,kernel_size=5 ,activation=""relu"", padding='same')(x)
    
            x1=MaxPooling2D((self.MAX_SEQUENCE_LENGTH-5+1,1),padding='same')(x1)
            x1=Flatten()(x1)
            x2=Conv2D(128, strides=2, kernel_size=4, activation=""sigmoid"", padding='same')(x)
            x2=MaxPooling2D((self.MAX_SEQUENCE_LENGTH-4+1,1),padding='same')(x2)
            
            x2=Flatten()(x2)
            x3=Conv2D(128, strides=2, kernel_size=3, activation=""tanh"", padding='same')(x)
            x3=MaxPooling2D((self.MAX_SEQUENCE_LENGTH-3+1,1),padding='same')(x3)
            
            x3=Flatten()(x3)
            
            combinedX=keras.layers.concatenate([x1,x2,x3],axis=1)
            
            
            combinedX=Dense(64, activation=""relu"")(combinedX)
            combinedX=Dropout(0.2)(combinedX)
            #output=Dense(self.nb_labels, activation=""sigmoid"")(combinedX)
            #output=Dense(2, activation=""softmax"")(combinedX)
            output=Dense(1, activation=""sigmoid"")(combinedX)
    
            
            #encoder =preprocessing.LabelEncoder()
            #encoder.fit(y)
            #encoded_Y = encoder.transform(y)
            #labels=keras.utils.to_categorical(encoded_Y, num_classes=2)
            labels=y
            pdb.set_trace()
            inputs2=X
            self.model = Model(inputs=mainIn, outputs=output)
            
           # self.model.compile(loss='binary_crossentropy',
           #                 optimizer='adam',
           #                 metrics=['acc'])
            
            self.model.compile(loss='binary_crossentropy',
                            optimizer='rmsprop',
                            metrics=['acc'])
            
            self.model.fit(inputs2,labels,epochs=7, batch_size=self.batch_size)
        def predict(self, X):
            return self.model.predict(np.array(X))
    
        def predict_proba(self, X):
            
            return self.model.predict(np.array(X), self.batch_size)
 
Here is my code to preprocess the data:

  

         #loading the file
    file2=pd.read_csv(""Sentiment_Analysis Dataset.csv"",error_bad_lines=False)
            #splitting into train et test set
    
    from sklearn.model_selection import train_test_split
    Text=list(file2.SentimentText)
    file2.groupby('Sentiment').count()
    train_data,test_data,train_label,test_label=train_test_split(Text, file2.Sentiment, test_size=0.4, random_state=42)
    #Buidling the dictionary
    
    vocabDic=dict()
    for document in train_data:
        document=document.split("" "")
        for word in document:
            if word not in vocabDic:
                vocabDic[word]=len(vocabDic)+1
    vocabDic['unk']=len(vocabDic)+1
    
    #coding the documents
    def codeDocuments(documents,dictionnary):
        documentsArray=list()
        for i,document in enumerate(documents):
            tempList=list()
            document=document.split("" "")
            for word in document:
                if word in vocabDic:
                    word=vocabDic[word]
                else:
                    word=vocabDic['unk']
                tempList.append(word)
            documentsArray.append(tempList)
        return np.array(documentsArray)
    train_docs=codeDocuments(train_data,vocabDic)
    test_docs=codeDocuments(test_data,vocabDic)
    
    #padding the documents
    
    
    from keras.preprocessing import sequence
    
    maxlen=75
    train_set = sequence.pad_sequences(train_docs, maxlen=maxlen)
    test_set = sequence.pad_sequences(test_docs, maxlen=maxlen)
    #calling the model
    model=Classifier(vocabDic,maxlen)
    model.fit(train_set[:50000],train_label[:50000])





  [1]: https://arxiv.org/abs/1408.5882
  [2]: http://thinknook.com/twitter-sentiment-analysis-training-corpus-dataset-2012-09-22/

",1,Loss and accuracy are not decreasing,"Loss and accuracy are not decreasing I am posting this issue here since no body in stackoverflow seems to know what it is the problem. I assume then that there is a bug and post here.

I am implementing a variant of  the CNN described by [this paper][1].
My problem is that the loss isn't decreasing and I don't understand why. Same have to be said concerning accuracy(stuck at 0.5 more or less).

This a problem of 2 classes classification. I am using the data from this  [website][2]:

I suspected the optimizer so I changed it whithout any improvements. I am pretty sure the data I am using is ok because I used it on an LSTM and the classifier was fine. 


Here is my code:

    from keras.layers import Embedding
    from keras.layers import Conv2D
    from keras.models import Sequential
    from keras.layers import MaxPooling2D
    from keras.layers import Reshape
    from keras.layers import Flatten
    from keras.layers import Dense
    from keras.layers import Dropout
    from keras.layers import Input
    from keras import backend as K
    
    from keras.models import Model
    import tensorflow as tf
    from sklearn import preprocessing
    import keras
    import numpy as np
    import pdb
    
    #using multiple filters
    config = tf.ConfigProto()
     
    # Don't pre-allocate memory; allocate as-needed
    config.gpu_options.allow_growth = True
     
    # Only allow a total of half the GPU memory to be allocated
    config.gpu_options.per_process_gpu_memory_fraction = 0.7
     
    # Create a session with the above options specified.
    config.gpu_options.per_process_gpu_memory_fraction = 0.65
    K.tensorflow_backend.set_session(tf.Session(config=config))
    class Classifier():
        def __init__(self,Vocab,maxlen=75):
            self.model=Sequential()
            self.EMBED_DIM=30
            self.batch_size=30
            self.MAX_SEQUENCE_LENGTH=maxlen
            self.nb_labels=2
            self.model = Sequential()
            self.Vocab=Vocab
        def fit(self, X,y):
            #pdb.set_trace()
            
            mainIn=Input(shape=(self.MAX_SEQUENCE_LENGTH,), dtype='int32', name='main_input')
            
            x=Embedding(len(self.Vocab)+2,self.EMBED_DIM,input_length=self.MAX_SEQUENCE_LENGTH)(mainIn)
            
            x=Reshape((1,self.MAX_SEQUENCE_LENGTH, self.EMBED_DIM))(x)
            
            x1=Conv2D(128, strides=2,kernel_size=5 ,activation=""relu"", padding='same')(x)
    
            x1=MaxPooling2D((self.MAX_SEQUENCE_LENGTH-5+1,1),padding='same')(x1)
            x1=Flatten()(x1)
            x2=Conv2D(128, strides=2, kernel_size=4, activation=""sigmoid"", padding='same')(x)
            x2=MaxPooling2D((self.MAX_SEQUENCE_LENGTH-4+1,1),padding='same')(x2)
            
            x2=Flatten()(x2)
            x3=Conv2D(128, strides=2, kernel_size=3, activation=""tanh"", padding='same')(x)
            x3=MaxPooling2D((self.MAX_SEQUENCE_LENGTH-3+1,1),padding='same')(x3)
            
            x3=Flatten()(x3)
            
            combinedX=keras.layers.concatenate([x1,x2,x3],axis=1)
            
            
            combinedX=Dense(64, activation=""relu"")(combinedX)
            combinedX=Dropout(0.2)(combinedX)
            #output=Dense(self.nb_labels, activation=""sigmoid"")(combinedX)
            #output=Dense(2, activation=""softmax"")(combinedX)
            output=Dense(1, activation=""sigmoid"")(combinedX)
    
            
            #encoder =preprocessing.LabelEncoder()
            #encoder.fit(y)
            #encoded_Y = encoder.transform(y)
            #labels=keras.utils.to_categorical(encoded_Y, num_classes=2)
            labels=y
            pdb.set_trace()
            inputs2=X
            self.model = Model(inputs=mainIn, outputs=output)
            
           # self.model.compile(loss='binary_crossentropy',
           #                 optimizer='adam',
           #                 metrics=['acc'])
            
            self.model.compile(loss='binary_crossentropy',
                            optimizer='rmsprop',
                            metrics=['acc'])
            
            self.model.fit(inputs2,labels,epochs=7, batch_size=self.batch_size)
        def predict(self, X):
            return self.model.predict(np.array(X))
    
        def predict_proba(self, X):
            
            return self.model.predict(np.array(X), self.batch_size)
 
Here is my code to preprocess the data:

  

         #loading the file
    file2=pd.read_csv(""Sentiment_Analysis Dataset.csv"",error_bad_lines=False)
            #splitting into train et test set
    
    from sklearn.model_selection import train_test_split
    Text=list(file2.SentimentText)
    file2.groupby('Sentiment').count()
    train_data,test_data,train_label,test_label=train_test_split(Text, file2.Sentiment, test_size=0.4, random_state=42)
    #Buidling the dictionary
    
    vocabDic=dict()
    for document in train_data:
        document=document.split("" "")
        for word in document:
            if word not in vocabDic:
                vocabDic[word]=len(vocabDic)+1
    vocabDic['unk']=len(vocabDic)+1
    
    #coding the documents
    def codeDocuments(documents,dictionnary):
        documentsArray=list()
        for i,document in enumerate(documents):
            tempList=list()
            document=document.split("" "")
            for word in document:
                if word in vocabDic:
                    word=vocabDic[word]
                else:
                    word=vocabDic['unk']
                tempList.append(word)
            documentsArray.append(tempList)
        return np.array(documentsArray)
    train_docs=codeDocuments(train_data,vocabDic)
    test_docs=codeDocuments(test_data,vocabDic)
    
    #padding the documents
    
    
    from keras.preprocessing import sequence
    
    maxlen=75
    train_set = sequence.pad_sequences(train_docs, maxlen=maxlen)
    test_set = sequence.pad_sequences(test_docs, maxlen=maxlen)
    #calling the model
    model=Classifier(vocabDic,maxlen)
    model.fit(train_set[:50000],train_label[:50000])





  [1]: https://arxiv.org/abs/1408.5882
  [2]: http://thinknook.com/twitter-sentiment-analysis-training-corpus-dataset-2012-09-22/

"
keras,7394,"https://github.com/fchollet/keras/blob/master/keras/engine/training.py#L463 is strange.

The code used for weighting is:

        # apply sample weighting
        if weights is not None:
            # reduce score_array to same ndim as weight array
            ndim = K.ndim(score_array)
            weight_ndim = K.ndim(weights)
            score_array = K.mean(score_array, axis=list(range(weight_ndim, ndim)))
            score_array *= weights
            score_array /= K.mean(K.cast(K.not_equal(weights, 0), K.floatx()))
        return K.mean(score_array)

I think the line 

            score_array /= K.mean(K.cast(K.not_equal(weights, 0), K.floatx()))

should become

            score_array /= K.mean(weights)

This would make the weighted loss be normalized. For instance, with N samples and weights w[i] with K non-zero weights, the weighted loss is sum(loss[i] * w[i] for i in range(N)) / K.

I think it should instead be sum(loss[i] * w[i] for i in range(N)) / sum(w).",1,Normalization of training weights,"Normalization of training weights https://github.com/fchollet/keras/blob/master/keras/engine/training.py#L463 is strange.

The code used for weighting is:

        # apply sample weighting
        if weights is not None:
            # reduce score_array to same ndim as weight array
            ndim = K.ndim(score_array)
            weight_ndim = K.ndim(weights)
            score_array = K.mean(score_array, axis=list(range(weight_ndim, ndim)))
            score_array *= weights
            score_array /= K.mean(K.cast(K.not_equal(weights, 0), K.floatx()))
        return K.mean(score_array)

I think the line 

            score_array /= K.mean(K.cast(K.not_equal(weights, 0), K.floatx()))

should become

            score_array /= K.mean(weights)

This would make the weighted loss be normalized. For instance, with N samples and weights w[i] with K non-zero weights, the weighted loss is sum(loss[i] * w[i] for i in range(N)) / K.

I think it should instead be sum(loss[i] * w[i] for i in range(N)) / sum(w)."
keras,7404,"z_mean and z_log_var look like same.
.....
x = Input(batch_shape=(batch_size, original_dim))
h = Dense(intermediate_dim, activation='relu')(x)
z_mean = Dense(latent_dim)(h)
z_log_var = Dense(latent_dim)(h)
.....
How is it possible?",1,z_mean and z_log_var in variational_autoencoder.py,"z_mean and z_log_var in variational_autoencoder.py z_mean and z_log_var look like same.
.....
x = Input(batch_shape=(batch_size, original_dim))
h = Dense(intermediate_dim, activation='relu')(x)
z_mean = Dense(latent_dim)(h)
z_log_var = Dense(latent_dim)(h)
.....
How is it possible?"
keras,7409,"platform: i5 7500, 1080ti, newest keras and backends, windows 10
Model(MNIST, CNN):
[https://github.com/minimaxir/keras-cntk-benchmark/blob/master/test_files/mnist_cnn.py](url)
model = Sequential()
model.add(Conv2D(20, (5, 5), activation='relu',
                 input_shape=input_shape))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Conv2D(40, (5, 5), activation='relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Dropout(0.5))
model.add(Flatten())
model.add(Dense(128, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(num_classes, activation='softmax'))

this model takes 2s per epoch with tensorflow, 3s with cntk and 11s with theano.
I have my own simple code to train MNIST and takes 1.3s per epoch with theano, 2.0s with tensorflow and 3.1s with cntk. ",1,keras is very slow with theano,"keras is very slow with theano platform: i5 7500, 1080ti, newest keras and backends, windows 10
Model(MNIST, CNN):
[https://github.com/minimaxir/keras-cntk-benchmark/blob/master/test_files/mnist_cnn.py](url)
model = Sequential()
model.add(Conv2D(20, (5, 5), activation='relu',
                 input_shape=input_shape))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Conv2D(40, (5, 5), activation='relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Dropout(0.5))
model.add(Flatten())
model.add(Dense(128, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(num_classes, activation='softmax'))

this model takes 2s per epoch with tensorflow, 3s with cntk and 11s with theano.
I have my own simple code to train MNIST and takes 1.3s per epoch with theano, 2.0s with tensorflow and 3.1s with cntk. "
keras,3826,"I have created feature vectors from a irregular multivariate time series that additionally contains static metadata features. I have then converted all values into a one hot encoded representation (continuous time series values via binning to established thresholds and then one hot encoding them). I then time order them (irregular intervals) and concatenate them together like the following simplified example:



I then pad them to be the same length with zeros:



My fundamental question is if this actually an appropriate data representation?  I have read about approach to creating ""stationary"" time series representations that usually involve creating standardized distance metrics.  I have not run into lit similar to this simple an approach.  I have tried LSTM models more appropriate for time series (are very slow and low accuracy in my case), but I find treating this as a traditional binary classification problem gives me really exceptional accuracy on my task (even with dropout, l2 regularization applied). Any advice or insight would be appreciated!
",1,Keras Binary Classification of Time Series with Static Metadata,"Keras Binary Classification of Time Series with Static Metadata I have created feature vectors from a irregular multivariate time series that additionally contains static metadata features. I have then converted all values into a one hot encoded representation (continuous time series values via binning to established thresholds and then one hot encoding them). I then time order them (irregular intervals) and concatenate them together like the following simplified example:



I then pad them to be the same length with zeros:



My fundamental question is if this actually an appropriate data representation?  I have read about approach to creating ""stationary"" time series representations that usually involve creating standardized distance metrics.  I have not run into lit similar to this simple an approach.  I have tried LSTM models more appropriate for time series (are very slow and low accuracy in my case), but I find treating this as a traditional binary classification problem gives me really exceptional accuracy on my task (even with dropout, l2 regularization applied). Any advice or insight would be appreciated!
"
keras,249,"The MNIST example seems to be running about 2x slower for me on a GPU (Tesla K10.G1.8GB) than on CPU.

When I run it with profiling it seems that it is spending a large amount of time in GpuFromHost.  This would indicate that the data is not being loaded onto the GPU with shared variables (and in fact I can't really tell where in the Keras code this would be taking place) and thus each minibatch must be transferred to the GPU at each iteration of training.

Yet all the other examples give instructions for running on GPU, and thus Keras is presumably optimized for GPU. Am I missing something here?
",1,MNIST example 2x slower on GPU,"MNIST example 2x slower on GPU The MNIST example seems to be running about 2x slower for me on a GPU (Tesla K10.G1.8GB) than on CPU.

When I run it with profiling it seems that it is spending a large amount of time in GpuFromHost.  This would indicate that the data is not being loaded onto the GPU with shared variables (and in fact I can't really tell where in the Keras code this would be taking place) and thus each minibatch must be transferred to the GPU at each iteration of training.

Yet all the other examples give instructions for running on GPU, and thus Keras is presumably optimized for GPU. Am I missing something here?
"
keras,6906,"For my application i need 2 output from the trained network (one at final layer and one at intermediate layer). Since model.predict() gives output from only last layer, i wrote theano function to fetch output at both intermediate layer and final layer. It works fine, but it is almost 20 times slower as compared to model.predict().

Can anybody help me to understand why theano function is slower and how can i make it faster.",1,teano.function is slower as compared to model.predict,"teano.function is slower as compared to model.predict For my application i need 2 output from the trained network (one at final layer and one at intermediate layer). Since model.predict() gives output from only last layer, i wrote theano function to fetch output at both intermediate layer and final layer. It works fine, but it is almost 20 times slower as compared to model.predict().

Can anybody help me to understand why theano function is slower and how can i make it faster."
keras,3325,"I am one of the many users having issues with passing the pre-trained embedding matrix to the Keras Embedding layer. My code currently looks like the following:



The first row of the pre-trained matrix  contains a random vector for the special token 'PAD'. This token is assigned the index 0 in the corpus and I assume that this index has special meaning in the lookup. The second row in this matrix contains a trained vector for the 'UNK' token which is returned whenever a word is not found in the vocabulary ('PAD' != 'UNK').

In the Embedding layer the input_dim is , which is the same number of rows in the  matrix. The output_dim is , which is the dimension of the word vectors. The input_length is  which is the (fixed) number of words in the padded sentences (padding with 0). The mask_zero argument is set to False because the matrix  already has a vector assigned in the first row for the 'PAD' token (index 0).

After running this on a very small training set of ~200 labeled sentences (3 labels) the accuracy is around 55% in both training and validation, but the prediction completely fails even on the training set. Almost all sentences therein are labeled the same.

So now it comes the questions:
1. Is it reasonable to use such a small training set to predict labels on sentences given a pre-trained word embeddnig matrix?
2. We have tried **not** passing the pre-trained embedding matrix and it produces the ""same"" results in terms of accuracy and predicted labels. How can we make sure this matrix is being used by the LSTM classifier?
3. Is it necessary to have a zero vector for the 'PAD' token or it can be random like we did?
4. We observe that the accuracy is highly correlated to the proportion of the labels in the training set. Do you think this information can help further debug the issue?
5. Could you add a complete example to the documentation with multiclass classification of sentences given word vectors? It seems that many people are struggling to get it working.

Please let me know if you need anything else in order to help with this issue.
",1,Misclassification on small training set with pre-trained word vectors,"Misclassification on small training set with pre-trained word vectors I am one of the many users having issues with passing the pre-trained embedding matrix to the Keras Embedding layer. My code currently looks like the following:



The first row of the pre-trained matrix  contains a random vector for the special token 'PAD'. This token is assigned the index 0 in the corpus and I assume that this index has special meaning in the lookup. The second row in this matrix contains a trained vector for the 'UNK' token which is returned whenever a word is not found in the vocabulary ('PAD' != 'UNK').

In the Embedding layer the input_dim is , which is the same number of rows in the  matrix. The output_dim is , which is the dimension of the word vectors. The input_length is  which is the (fixed) number of words in the padded sentences (padding with 0). The mask_zero argument is set to False because the matrix  already has a vector assigned in the first row for the 'PAD' token (index 0).

After running this on a very small training set of ~200 labeled sentences (3 labels) the accuracy is around 55% in both training and validation, but the prediction completely fails even on the training set. Almost all sentences therein are labeled the same.

So now it comes the questions:
1. Is it reasonable to use such a small training set to predict labels on sentences given a pre-trained word embeddnig matrix?
2. We have tried **not** passing the pre-trained embedding matrix and it produces the ""same"" results in terms of accuracy and predicted labels. How can we make sure this matrix is being used by the LSTM classifier?
3. Is it necessary to have a zero vector for the 'PAD' token or it can be random like we did?
4. We observe that the accuracy is highly correlated to the proportion of the labels in the training set. Do you think this information can help further debug the issue?
5. Could you add a complete example to the documentation with multiclass classification of sentences given word vectors? It seems that many people are struggling to get it working.

Please let me know if you need anything else in order to help with this issue.
"
keras,13053,"

I'm trying to build LSTM architecture to predict sickness rate. I'm actually stuck in 40% accuracy, I'm new in machine learning and I tried several tips like changing the optimzer, the layer node number and the dropout value without any improving. So could you guys help me with some advice.

the x array is composed of 10 columns

the y array is only one column the sickness rate

here is my model 

this is the output of . evaluate()

and thank you in advance",1,improve LSTM accuracy ,"improve LSTM accuracy  

I'm trying to build LSTM architecture to predict sickness rate. I'm actually stuck in 40% accuracy, I'm new in machine learning and I tried several tips like changing the optimzer, the layer node number and the dropout value without any improving. So could you guys help me with some advice.

the x array is composed of 10 columns

the y array is only one column the sickness rate

here is my model 

this is the output of . evaluate()

and thank you in advance"
keras,11006,"Hi,
I am using Keras LSTM on time series prediction (let time series is y[n]). Network input is time shifted vector y[0], y[-1], y[-2] etc.... After network training, I calculate one step prediction, then add prediction to input vector on first place and release last value. So i get prediction to e.g. 50 future points. Problem is, that predict method is very slow for this case. I noticed, that prediction time does change only a little with changing network size (for small networks - e.g.  1-50 LSTM cells). I know, that prediction of whole array of input of vectors is significantly faster, but that is not what i want. I tried to change Keras backend to CNTK or Theano, but it was worse than Tensorflow. 
Is there possibility to speed it up? What about of adding fast predict method for only one input vector? Would be there a way to use Tensorflow or CNTK directly without running through Keras?

Model I use (example):









My function for multiple step prediction (I know, converting array to lists an backwards is not optimal):












`
Keras version is 2.2.2
Tensorflow version is 1.5.0 (I have problems with newer versions)

Profiling shows, that bottle neck ist in _pywrap_tensorflow_internal.TF_Run method.


",1,Predict point by point is very slow,"Predict point by point is very slow Hi,
I am using Keras LSTM on time series prediction (let time series is y[n]). Network input is time shifted vector y[0], y[-1], y[-2] etc.... After network training, I calculate one step prediction, then add prediction to input vector on first place and release last value. So i get prediction to e.g. 50 future points. Problem is, that predict method is very slow for this case. I noticed, that prediction time does change only a little with changing network size (for small networks - e.g.  1-50 LSTM cells). I know, that prediction of whole array of input of vectors is significantly faster, but that is not what i want. I tried to change Keras backend to CNTK or Theano, but it was worse than Tensorflow. 
Is there possibility to speed it up? What about of adding fast predict method for only one input vector? Would be there a way to use Tensorflow or CNTK directly without running through Keras?

Model I use (example):









My function for multiple step prediction (I know, converting array to lists an backwards is not optimal):












`
Keras version is 2.2.2
Tensorflow version is 1.5.0 (I have problems with newer versions)

Profiling shows, that bottle neck ist in _pywrap_tensorflow_internal.TF_Run method.


"
keras,11014,"Using model with multiple outputs I am getting nan loss despite all outputs loss seems valid. screenshot attached.
This behavior remains when I change loss functions (I have tried standard 'mae', 'mse', 'categorical_crossentropy' and some customized loss functions as well).
This behavior remains when I change optimizers (I have tries Adam and SGD with momentum).

Any advice would be appreciated.

multiple loss do seem to converge though :-| (screenshot attached).
![image](https://user-images.githubusercontent.com/25052915/44705551-a2a85000-aaa7-11e8-9b94-be5a7b3f0311.png)

![image](https://user-images.githubusercontent.com/25052915/44705587-bb186a80-aaa7-11e8-818b-7f7a4478c5cc.png)

loss & val loss are nan -
![image](https://user-images.githubusercontent.com/25052915/44705694-06cb1400-aaa8-11e8-8a43-5ee17f353eed.png)

",1,nan loss although multiple outputs loss not nan,"nan loss although multiple outputs loss not nan Using model with multiple outputs I am getting nan loss despite all outputs loss seems valid. screenshot attached.
This behavior remains when I change loss functions (I have tried standard 'mae', 'mse', 'categorical_crossentropy' and some customized loss functions as well).
This behavior remains when I change optimizers (I have tries Adam and SGD with momentum).

Any advice would be appreciated.

multiple loss do seem to converge though :-| (screenshot attached).
![image](https://user-images.githubusercontent.com/25052915/44705551-a2a85000-aaa7-11e8-9b94-be5a7b3f0311.png)

![image](https://user-images.githubusercontent.com/25052915/44705587-bb186a80-aaa7-11e8-818b-7f7a4478c5cc.png)

loss & val loss are nan -
![image](https://user-images.githubusercontent.com/25052915/44705694-06cb1400-aaa8-11e8-8a43-5ee17f353eed.png)

"
keras,4365,"I've noticed that the Embedding layer with TensorFlow backend is converting sparse gradient updates to dense ones and killing the performance, as well as gobbling up lots of memory.  This is making it unusable for a large scale problem with a large Embedding layer.

Here is a script that makes a model with single large embedding layer using Keras and TensorFlow directly.  In Keras, it takes about 2.3 seconds / batch and uses > 9 GB of memory while training.  In TensorFlow it only takes 20 ms / batch (100X faster) and uses < 4 G of memory.

This is using TensorFlow 0.11.0rc2 and the master branch of Keras.



Outputs:



With TensorFlow:


 
Outputs:

",1,Embedding with TensorFlow very slow: converts indices to dense gradients,"Embedding with TensorFlow very slow: converts indices to dense gradients I've noticed that the Embedding layer with TensorFlow backend is converting sparse gradient updates to dense ones and killing the performance, as well as gobbling up lots of memory.  This is making it unusable for a large scale problem with a large Embedding layer.

Here is a script that makes a model with single large embedding layer using Keras and TensorFlow directly.  In Keras, it takes about 2.3 seconds / batch and uses > 9 GB of memory while training.  In TensorFlow it only takes 20 ms / batch (100X faster) and uses < 4 G of memory.

This is using TensorFlow 0.11.0rc2 and the master branch of Keras.



Outputs:



With TensorFlow:


 
Outputs:

"
keras,270,"As you can see by the code below, the behavior of batch normalization seems to be dependent on the batch of samples. Any given sample should always be classified exactly the same (in mode 0 at least). Note that for the single item batch, the output of model.predict(arr1[:1]) is identical to the output of model.predict(arr1[1:]).



I've read the source code a thousand times and cannot find a reason for this to happen. I've got a working theory that there is an issue with running mean and running std being mutable within get_output(). Those values are also not inspectable as they are not stored as a tensor value:



This issue makes batch normalization fairly useless to me as I cannot guarantee that a sample will always generate the same prediction.
",1,Inconsistent behavior of batch normalization layer,"Inconsistent behavior of batch normalization layer As you can see by the code below, the behavior of batch normalization seems to be dependent on the batch of samples. Any given sample should always be classified exactly the same (in mode 0 at least). Note that for the single item batch, the output of model.predict(arr1[:1]) is identical to the output of model.predict(arr1[1:]).



I've read the source code a thousand times and cannot find a reason for this to happen. I've got a working theory that there is an issue with running mean and running std being mutable within get_output(). Those values are also not inspectable as they are not stored as a tensor value:



This issue makes batch normalization fairly useless to me as I cannot guarantee that a sample will always generate the same prediction.
"
keras,13582,"Hi! I'm training a small Keras model (just 32k params) with a dataset composed of 50000 samples, each of them is stored on disk (HDD). Each sample has a size of 249.1 kB. The validation dataset has 8000 samples.
I use a custom data generator during training that unpickles data from disk and feeds it to the model. Each epoch last 390 iterations with a batch_size of 128 (plus 64 iters for validation). I set  for the fit_generator.
I also use threadsafe iterations.

I'm using Ubuntu 16.04 with Python 3, keras 2.2.4  and tensorflow-gpu 1.13.1.
My PC has 32 GB of RAM (15 used) and 32 GB of swapping memory (5 used).

THE PROBLEM:
During training, the first ~250 iterations of the first epoch last just a few seconds and later the training is slowed down. Then the GPU usage is ~0% and the disk usage is: . The first epoch lasts ~550 sec. and the second one ~1700 sec.

Can this problem be related to the HDD disk usage? I cannot find the reason for this slowing down.",1,Training slows down after a fixed num of iterations,"Training slows down after a fixed num of iterations Hi! I'm training a small Keras model (just 32k params) with a dataset composed of 50000 samples, each of them is stored on disk (HDD). Each sample has a size of 249.1 kB. The validation dataset has 8000 samples.
I use a custom data generator during training that unpickles data from disk and feeds it to the model. Each epoch last 390 iterations with a batch_size of 128 (plus 64 iters for validation). I set  for the fit_generator.
I also use threadsafe iterations.

I'm using Ubuntu 16.04 with Python 3, keras 2.2.4  and tensorflow-gpu 1.13.1.
My PC has 32 GB of RAM (15 used) and 32 GB of swapping memory (5 used).

THE PROBLEM:
During training, the first ~250 iterations of the first epoch last just a few seconds and later the training is slowed down. Then the GPU usage is ~0% and the disk usage is: . The first epoch lasts ~550 sec. and the second one ~1700 sec.

Can this problem be related to the HDD disk usage? I cannot find the reason for this slowing down."
keras,13585,"**System information**  
- Have I written custom code (as opposed to using example directory):  
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  CentOS Linux release 7.2.1511 (Core)
- TensorFlow backend (yes / no):  YES
- TensorFlow version:  v1.12.3-0-g41e0a4f56c 1.12.3
- Keras version:  2.1.6-tf
- Python version:  python 3.6.5
- CUDA/cuDNN version:  9.0.176/7.6.3.30
- GPU model and memory:  Tesla P100-PCIE 16GB x4

My model is shown below, with many BN layers.



My test set is (128 * n, 40, 40, 1), batch_size = 128
It's normal for me to run my code on a single GPU
However, when I use multiple GPUs（multi_gpu_num）, the weight of the BN layer becomes NAN.
Strangely, this seems to be related only to GPU_NUM :
When GPU_NUM = 2, the weights become Nan at the 129th step of training,
When GPU_NUM = 4, the weights become Nan at the 65th step of training,
Whether I change the batch_size to 64, 128, or 512, the above behaviors still exist

code:



How can I solve this problem？",1,BatchNormalization produces NaN weights without NaN loss[NOT duplicate],"BatchNormalization produces NaN weights without NaN loss[NOT duplicate] **System information**  
- Have I written custom code (as opposed to using example directory):  
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  CentOS Linux release 7.2.1511 (Core)
- TensorFlow backend (yes / no):  YES
- TensorFlow version:  v1.12.3-0-g41e0a4f56c 1.12.3
- Keras version:  2.1.6-tf
- Python version:  python 3.6.5
- CUDA/cuDNN version:  9.0.176/7.6.3.30
- GPU model and memory:  Tesla P100-PCIE 16GB x4

My model is shown below, with many BN layers.



My test set is (128 * n, 40, 40, 1), batch_size = 128
It's normal for me to run my code on a single GPU
However, when I use multiple GPUs（multi_gpu_num）, the weight of the BN layer becomes NAN.
Strangely, this seems to be related only to GPU_NUM :
When GPU_NUM = 2, the weights become Nan at the 129th step of training,
When GPU_NUM = 4, the weights become Nan at the 65th step of training,
Whether I change the batch_size to 64, 128, or 512, the above behaviors still exist

code:



How can I solve this problem？"
keras,7954,"# Conclusion: Observation of keras cosine proximity stuck as -1/3 #
As noted by numerous post, Keras seriously currently has an issue with cosine proximity:

https://github.com/fchollet/keras/issues/3031
https://github.com/fchollet/keras/issues/5046

Here is the code in jupyter notebook for simple test:




The printed result is 


**So the true cosine proximity is actually 0.9986, but keras shows near -1/3. Of course keras would use the negative of cosine proximity for minimization purpose, but it should be -0.9986.., in any case, don't trust the outcome of metric in keras cosine proximity**
",1,Wrong result for cosine proximity: keras 2.0.8,"Wrong result for cosine proximity: keras 2.0.8 # Conclusion: Observation of keras cosine proximity stuck as -1/3 #
As noted by numerous post, Keras seriously currently has an issue with cosine proximity:

https://github.com/fchollet/keras/issues/3031
https://github.com/fchollet/keras/issues/5046

Here is the code in jupyter notebook for simple test:




The printed result is 


**So the true cosine proximity is actually 0.9986, but keras shows near -1/3. Of course keras would use the negative of cosine proximity for minimization purpose, but it should be -0.9986.., in any case, don't trust the outcome of metric in keras cosine proximity**
"
keras,13601,"**System information**  
- TensorFlow backend (yes / no):   yes
- TensorFlow version:  1.13.2
- Keras version:  2.3.1
- Python version:  3.6
- CUDA/cuDNN version:  10.0

**Describe the current behavior**  
While training a network with multiple output layers (3 in my case), i observe wrong validation loss:


**Describe the expected behavior**  
The validation loss should look like the one of the training. Even for the training, the loss is not the sum of all outputs, but it has the same dimension.
I don't have this problem with keras 2.2.4. This Problem pop up from version 2.2.5 upwards. Unfortunately, i can't go back to 2.2.4, because I need the new on_test_begin callback.
**Code to reproduce the issue**  
Provide a reproducible test case that is the bare minimum necessary to generate the problem.  

**Other info / logs**  



",1,Wrong Validation Loss with Multi-Output Architecture,"Wrong Validation Loss with Multi-Output Architecture **System information**  
- TensorFlow backend (yes / no):   yes
- TensorFlow version:  1.13.2
- Keras version:  2.3.1
- Python version:  3.6
- CUDA/cuDNN version:  10.0

**Describe the current behavior**  
While training a network with multiple output layers (3 in my case), i observe wrong validation loss:


**Describe the expected behavior**  
The validation loss should look like the one of the training. Even for the training, the loss is not the sum of all outputs, but it has the same dimension.
I don't have this problem with keras 2.2.4. This Problem pop up from version 2.2.5 upwards. Unfortunately, i can't go back to 2.2.4, because I need the new on_test_begin callback.
**Code to reproduce the issue**  
Provide a reproducible test case that is the bare minimum necessary to generate the problem.  

**Other info / logs**  



"
keras,6447,"I'm trying to learn a regression problem. The data is mostly one-hot encoded categorical variables, one continuous. The target output is a probability (0-1). Here is the code:


It sure seems to be learning _something_:
![hist](https://cloud.githubusercontent.com/assets/3537118/25560256/935f61a0-2d04-11e7-8406-14281273edb4.png)

But print(preds[0:10]) gives:

Even though print(evals) gives a loss and mse of:


It even does that when I call  on training data.

I've tried no regularization, more regularization, different optimizers, different learning rates, mean/std normalization, less depth, more depth, all with the same result. 

Any ideas?",1,model.predict() gives same output for all inputs,"model.predict() gives same output for all inputs I'm trying to learn a regression problem. The data is mostly one-hot encoded categorical variables, one continuous. The target output is a probability (0-1). Here is the code:


It sure seems to be learning _something_:
![hist](https://cloud.githubusercontent.com/assets/3537118/25560256/935f61a0-2d04-11e7-8406-14281273edb4.png)

But print(preds[0:10]) gives:

Even though print(evals) gives a loss and mse of:


It even does that when I call  on training data.

I've tried no regularization, more regularization, different optimizers, different learning rates, mean/std normalization, less depth, more depth, all with the same result. 

Any ideas?"
keras,4404,"Hello,

I run the example of mnist_cnn by CPU on my win7. The result is very bad, with fixed test accuracy of 0.098 even after 12 epochs. 
I am new in keras, and want to know how to debug it.
Thx for any suggestion.

The packages have been updated to latest version:
Keras version: 1.1.1
Theano version: 0.8.2

Here is the log
Train on 60000 samples, validate on 10000 samples
Epoch 1/12
60000/60000 [==============================] - 5584s - loss: nan - acc: 0.0988 - val_loss: nan - val_acc: 0.0980
Epoch 2/12
60000/60000 [==============================] - 5578s - loss: nan - acc: 0.0987 - val_loss: nan - val_acc: 0.0980
Epoch 3/12
60000/60000 [==============================] - 5577s - loss: nan - acc: 0.0987 - val_loss: nan - val_acc: 0.0980
Epoch 4/12
60000/60000 [==============================] - 5556s - loss: nan - acc: 0.0987 - val_loss: nan - val_acc: 0.0980
Epoch 5/12
60000/60000 [==============================] - 5559s - loss: nan - acc: 0.0987 - val_loss: nan - val_acc: 0.0980
Epoch 6/12
60000/60000 [==============================] - 5549s - loss: nan - acc: 0.0987 - val_loss: nan - val_acc: 0.0980
Epoch 7/12
60000/60000 [==============================] - 5550s - loss: nan - acc: 0.0987 - val_loss: nan - val_acc: 0.0980
Epoch 8/12
60000/60000 [==============================] - 5546s - loss: nan - acc: 0.0987 - val_loss: nan - val_acc: 0.0980
Epoch 9/12
60000/60000 [==============================] - 5548s - loss: nan - acc: 0.0987 - val_loss: nan - val_acc: 0.0980
Epoch 10/12
60000/60000 [==============================] - 5550s - loss: nan - acc: 0.0987 - val_loss: nan - val_acc: 0.0980
Epoch 11/12
60000/60000 [==============================] - 5549s - loss: nan - acc: 0.0987 - val_loss: nan - val_acc: 0.0980
Epoch 12/12
60000/60000 [==============================] - 5565s - loss: nan - acc: 0.0987 - val_loss: nan - val_acc: 0.0980
Test score: nan
Test accuracy: 0.098",1,too low accuracy in the example of mnist_cnn,"too low accuracy in the example of mnist_cnn Hello,

I run the example of mnist_cnn by CPU on my win7. The result is very bad, with fixed test accuracy of 0.098 even after 12 epochs. 
I am new in keras, and want to know how to debug it.
Thx for any suggestion.

The packages have been updated to latest version:
Keras version: 1.1.1
Theano version: 0.8.2

Here is the log
Train on 60000 samples, validate on 10000 samples
Epoch 1/12
60000/60000 [==============================] - 5584s - loss: nan - acc: 0.0988 - val_loss: nan - val_acc: 0.0980
Epoch 2/12
60000/60000 [==============================] - 5578s - loss: nan - acc: 0.0987 - val_loss: nan - val_acc: 0.0980
Epoch 3/12
60000/60000 [==============================] - 5577s - loss: nan - acc: 0.0987 - val_loss: nan - val_acc: 0.0980
Epoch 4/12
60000/60000 [==============================] - 5556s - loss: nan - acc: 0.0987 - val_loss: nan - val_acc: 0.0980
Epoch 5/12
60000/60000 [==============================] - 5559s - loss: nan - acc: 0.0987 - val_loss: nan - val_acc: 0.0980
Epoch 6/12
60000/60000 [==============================] - 5549s - loss: nan - acc: 0.0987 - val_loss: nan - val_acc: 0.0980
Epoch 7/12
60000/60000 [==============================] - 5550s - loss: nan - acc: 0.0987 - val_loss: nan - val_acc: 0.0980
Epoch 8/12
60000/60000 [==============================] - 5546s - loss: nan - acc: 0.0987 - val_loss: nan - val_acc: 0.0980
Epoch 9/12
60000/60000 [==============================] - 5548s - loss: nan - acc: 0.0987 - val_loss: nan - val_acc: 0.0980
Epoch 10/12
60000/60000 [==============================] - 5550s - loss: nan - acc: 0.0987 - val_loss: nan - val_acc: 0.0980
Epoch 11/12
60000/60000 [==============================] - 5549s - loss: nan - acc: 0.0987 - val_loss: nan - val_acc: 0.0980
Epoch 12/12
60000/60000 [==============================] - 5565s - loss: nan - acc: 0.0987 - val_loss: nan - val_acc: 0.0980
Test score: nan
Test accuracy: 0.098"
keras,13621,"Following are the last two lines of my model:


If I use predictions as the model output, the model does not learn anything, but if I remove the predictions layer and add the activations parameter to outputs Dense layer, it works fine. ",1,Using output as a separate acitvation layer causing very poor accuracy.,"Using output as a separate acitvation layer causing very poor accuracy. Following are the last two lines of my model:


If I use predictions as the model output, the model does not learn anything, but if I remove the predictions layer and add the activations parameter to outputs Dense layer, it works fine. "
keras,3894,"I got a strange question. I train a two layers CNN using .flow_from_directory(), the training accuracy is very high, while the validation accuracy is very low. following is my code ,very simple.
from keras.preprocessing.image import ImageDataGenerator

from keras.models import Sequential
from keras.layers import Convolution2D, MaxPooling2D
from keras.layers import Activation, Dropout, Flatten, Dense
from keras.callbacks import EarlyStopping

model=Sequential()
model.add(Convolution2D(32, 5,5, input_shape=(3,28,28)))
model.add(Activation('relu'))
model.add(MaxPooling2D(pool_size=(2,2)))

model.add(Convolution2D(32, 3,3))
model.add(Activation('relu'))
model.add(MaxPooling2D(pool_size=(2,2)))

model.add(Flatten())
model.add(Dense(128))
model.add(Activation('relu'))
model.add(Dropout(0.5))

model.add(Dense(10))
model.add(Activation('softmax'))

train_datagen=ImageDataGenerator(rescale=1./255,
                                 shear_range=0.2,
                                 zoom_range=0.2,
                                 horizontal_flip=True)
test_datagen=ImageDataGenerator(rescale=1./255)

train_generator=train_datagen.flow_from_directory(
    r'C:\Users\zhx\Desktop\mnist\train',
    target_size=(28,28),
    classes=['0','1','2','3','4','5','6','7','8','9'],
    batch_size=60,
    class_mode='categorical',
    shuffle=True)

validation_generator=test_datagen.flow_from_directory(
    r'C:\Users\zhx\Desktop\mnist\test',
    target_size=(28, 28),
    classes=['0','1','2','3','4','5','6','7','8','9'],
    batch_size=100,
    class_mode='categorical',
    shuffle=True)

model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])
early_stopping =EarlyStopping(monitor='val_loss', patience=2)
model.fit_generator(train_generator, samples_per_epoch=60000, nb_epoch=10, validation_data=validation_generator, callbacks=[early_stopping],nb_val_samples=10000)

json_string=model.to_json()
open(r'C:\Users\zhx\Desktop\mnistmodel\mnistcnn_arc.json','w').write(json_string)
model.save_weights(r'C:\Users\zhx\Desktop\mnistmodel\mnistcnn_weights.h5')
score=model.evaluate_generator(validation_generator, 10000)

print('Test score:', score[0])
print('Test accuracy:', score[1])

Here is the log(only the last )

Using Theano backend.
Found 60000 images belonging to 10 classes.
Found 10000 images belonging to 10 classes.
Epoch 1/10

   60/60000 [..............................] - ETA: 812s - loss: 2.2756 - acc: 0.1667
  120/60000 [..............................] - ETA: 690s - loss: 2.2897 - acc: 0.1667
  180/60000 [..............................] - ETA: 688s - loss: 2.2728 - acc: 0.1833
  240/60000 [..............................] - ETA: 721s - loss: 2.2692 - acc: 0.1792
  300/60000 [..............................] - ETA: 768s - loss: 2.2633 - acc: 0.1800
  360/60000 [..............................] - ETA: 833s - loss: 2.2561 - acc: 0.1750
  420/60000 [..............................] - ETA: 884s - loss: 2.2437 - acc: 0.1786
Warning (from warnings module):
  File ""C:\WinPython-64bit-3.4.4.4Qt5\python-3.4.4.amd64\lib\site-packages\keras\callbacks.py"", line 67
    % delta_t_median)
UserWarning: Method on_batch_end() is slow compared to the batch update (0.484438). Check your callbacks.

  480/60000 [..............................] - ETA: 1002s - loss: 2.2285 - acc: 0.2042
  540/60000 [..............................] - ETA: 978s - loss: 2.2102 - acc: 0.2241 
  600/60000 [..............................] - ETA: 979s - loss: 2.1920 - acc: 0.2317
  660/60000 [..............................] - ETA: 1013s - loss: 2.1655 - acc: 0.2439
Warning (from warnings module):
  File ""C:\WinPython-64bit-3.4.4.4Qt5\python-3.4.4.amd64\lib\site-packages\keras\callbacks.py"", line 67
    % delta_t_median)
UserWarning: Method on_batch_end() is slow compared to the batch update (0.578522). Check your callbacks.

  720/60000 [..............................] - ETA: 1057s - loss: 2.1447 - acc: 0.2514
  780/60000 [..............................] - ETA: 1038s - loss: 2.1188 - acc: 0.2577
  840/60000 [..............................] - ETA: 1029s - loss: 2.1058 - acc: 0.2631

.........

59400/60000 [============================>.] - ETA: 11s - loss: 0.1535 - acc: 0.9568
59460/60000 [============================>.] - ETA: 10s - loss: 0.1537 - acc: 0.9567
59520/60000 [============================>.] - ETA: 9s - loss: 0.1537 - acc: 0.9568 
Warning (from warnings module):
  File ""C:\WinPython-64bit-3.4.4.4Qt5\python-3.4.4.amd64\lib\site-packages\keras\callbacks.py"", line 67
    % delta_t_median)
UserWarning: Method on_batch_end() is slow compared to the batch update (0.705150). Check your callbacks.

59580/60000 [============================>.] - ETA: 8s - loss: 0.1536 - acc: 0.9567
Warning (from warnings module):
  File ""C:\WinPython-64bit-3.4.4.4Qt5\python-3.4.4.amd64\lib\site-packages\keras\callbacks.py"", line 67
    % delta_t_median)
UserWarning: Method on_batch_end() is slow compared to the batch update (0.606548). Check your callbacks.

59640/60000 [============================>.] - ETA: 6s - loss: 0.1536 - acc: 0.9567
59700/60000 [============================>.] - ETA: 5s - loss: 0.1535 - acc: 0.9568
59760/60000 [============================>.] - ETA: 4s - loss: 0.1535 - acc: 0.9568
59820/60000 [============================>.] - ETA: 3s - loss: 0.1535 - acc: 0.9568
Warning (from warnings module):
  File ""C:\WinPython-64bit-3.4.4.4Qt5\python-3.4.4.amd64\lib\site-packages\keras\callbacks.py"", line 67
    % delta_t_median)
UserWarning: Method on_batch_end() is slow compared to the batch update (0.461924). Check your callbacks.

59880/60000 [============================>.] - ETA: 2s - loss: 0.1535 - acc: 0.9567
59940/60000 [============================>.] - ETA: 1s - loss: 0.1535 - acc: 0.9567
60000/60000 [==============================] - 1189s - loss: 0.1535 - acc: 0.9567 - val_loss: 8.0909 - val_acc: 0.1783
Test score: 8.09085823536
Test accuracy: 0.178299999908

Can any one give me a suggestion? thanks.
",1,"The training accuracy is very high, while the validation accuracy is very low?","The training accuracy is very high, while the validation accuracy is very low? I got a strange question. I train a two layers CNN using .flow_from_directory(), the training accuracy is very high, while the validation accuracy is very low. following is my code ,very simple.
from keras.preprocessing.image import ImageDataGenerator

from keras.models import Sequential
from keras.layers import Convolution2D, MaxPooling2D
from keras.layers import Activation, Dropout, Flatten, Dense
from keras.callbacks import EarlyStopping

model=Sequential()
model.add(Convolution2D(32, 5,5, input_shape=(3,28,28)))
model.add(Activation('relu'))
model.add(MaxPooling2D(pool_size=(2,2)))

model.add(Convolution2D(32, 3,3))
model.add(Activation('relu'))
model.add(MaxPooling2D(pool_size=(2,2)))

model.add(Flatten())
model.add(Dense(128))
model.add(Activation('relu'))
model.add(Dropout(0.5))

model.add(Dense(10))
model.add(Activation('softmax'))

train_datagen=ImageDataGenerator(rescale=1./255,
                                 shear_range=0.2,
                                 zoom_range=0.2,
                                 horizontal_flip=True)
test_datagen=ImageDataGenerator(rescale=1./255)

train_generator=train_datagen.flow_from_directory(
    r'C:\Users\zhx\Desktop\mnist\train',
    target_size=(28,28),
    classes=['0','1','2','3','4','5','6','7','8','9'],
    batch_size=60,
    class_mode='categorical',
    shuffle=True)

validation_generator=test_datagen.flow_from_directory(
    r'C:\Users\zhx\Desktop\mnist\test',
    target_size=(28, 28),
    classes=['0','1','2','3','4','5','6','7','8','9'],
    batch_size=100,
    class_mode='categorical',
    shuffle=True)

model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])
early_stopping =EarlyStopping(monitor='val_loss', patience=2)
model.fit_generator(train_generator, samples_per_epoch=60000, nb_epoch=10, validation_data=validation_generator, callbacks=[early_stopping],nb_val_samples=10000)

json_string=model.to_json()
open(r'C:\Users\zhx\Desktop\mnistmodel\mnistcnn_arc.json','w').write(json_string)
model.save_weights(r'C:\Users\zhx\Desktop\mnistmodel\mnistcnn_weights.h5')
score=model.evaluate_generator(validation_generator, 10000)

print('Test score:', score[0])
print('Test accuracy:', score[1])

Here is the log(only the last )

Using Theano backend.
Found 60000 images belonging to 10 classes.
Found 10000 images belonging to 10 classes.
Epoch 1/10

   60/60000 [..............................] - ETA: 812s - loss: 2.2756 - acc: 0.1667
  120/60000 [..............................] - ETA: 690s - loss: 2.2897 - acc: 0.1667
  180/60000 [..............................] - ETA: 688s - loss: 2.2728 - acc: 0.1833
  240/60000 [..............................] - ETA: 721s - loss: 2.2692 - acc: 0.1792
  300/60000 [..............................] - ETA: 768s - loss: 2.2633 - acc: 0.1800
  360/60000 [..............................] - ETA: 833s - loss: 2.2561 - acc: 0.1750
  420/60000 [..............................] - ETA: 884s - loss: 2.2437 - acc: 0.1786
Warning (from warnings module):
  File ""C:\WinPython-64bit-3.4.4.4Qt5\python-3.4.4.amd64\lib\site-packages\keras\callbacks.py"", line 67
    % delta_t_median)
UserWarning: Method on_batch_end() is slow compared to the batch update (0.484438). Check your callbacks.

  480/60000 [..............................] - ETA: 1002s - loss: 2.2285 - acc: 0.2042
  540/60000 [..............................] - ETA: 978s - loss: 2.2102 - acc: 0.2241 
  600/60000 [..............................] - ETA: 979s - loss: 2.1920 - acc: 0.2317
  660/60000 [..............................] - ETA: 1013s - loss: 2.1655 - acc: 0.2439
Warning (from warnings module):
  File ""C:\WinPython-64bit-3.4.4.4Qt5\python-3.4.4.amd64\lib\site-packages\keras\callbacks.py"", line 67
    % delta_t_median)
UserWarning: Method on_batch_end() is slow compared to the batch update (0.578522). Check your callbacks.

  720/60000 [..............................] - ETA: 1057s - loss: 2.1447 - acc: 0.2514
  780/60000 [..............................] - ETA: 1038s - loss: 2.1188 - acc: 0.2577
  840/60000 [..............................] - ETA: 1029s - loss: 2.1058 - acc: 0.2631

.........

59400/60000 [============================>.] - ETA: 11s - loss: 0.1535 - acc: 0.9568
59460/60000 [============================>.] - ETA: 10s - loss: 0.1537 - acc: 0.9567
59520/60000 [============================>.] - ETA: 9s - loss: 0.1537 - acc: 0.9568 
Warning (from warnings module):
  File ""C:\WinPython-64bit-3.4.4.4Qt5\python-3.4.4.amd64\lib\site-packages\keras\callbacks.py"", line 67
    % delta_t_median)
UserWarning: Method on_batch_end() is slow compared to the batch update (0.705150). Check your callbacks.

59580/60000 [============================>.] - ETA: 8s - loss: 0.1536 - acc: 0.9567
Warning (from warnings module):
  File ""C:\WinPython-64bit-3.4.4.4Qt5\python-3.4.4.amd64\lib\site-packages\keras\callbacks.py"", line 67
    % delta_t_median)
UserWarning: Method on_batch_end() is slow compared to the batch update (0.606548). Check your callbacks.

59640/60000 [============================>.] - ETA: 6s - loss: 0.1536 - acc: 0.9567
59700/60000 [============================>.] - ETA: 5s - loss: 0.1535 - acc: 0.9568
59760/60000 [============================>.] - ETA: 4s - loss: 0.1535 - acc: 0.9568
59820/60000 [============================>.] - ETA: 3s - loss: 0.1535 - acc: 0.9568
Warning (from warnings module):
  File ""C:\WinPython-64bit-3.4.4.4Qt5\python-3.4.4.amd64\lib\site-packages\keras\callbacks.py"", line 67
    % delta_t_median)
UserWarning: Method on_batch_end() is slow compared to the batch update (0.461924). Check your callbacks.

59880/60000 [============================>.] - ETA: 2s - loss: 0.1535 - acc: 0.9567
59940/60000 [============================>.] - ETA: 1s - loss: 0.1535 - acc: 0.9567
60000/60000 [==============================] - 1189s - loss: 0.1535 - acc: 0.9567 - val_loss: 8.0909 - val_acc: 0.1783
Test score: 8.09085823536
Test accuracy: 0.178299999908

Can any one give me a suggestion? thanks.
"
keras,11070,"Hi all,

**I've found that the problem doesn't occur when TensorFlow is forced to use the CPU** -- (I think) this implies that it's a TensorFlow bug and not a Keras bug, so maybe this issue can be closed.

First off, I'm using the Keras that's distributed with TensorFlow 1.10.0 so let me know if I should have opened the issue on their repo instead.

I'm using a sequence-to-sequence model based on the [Keras blogpost](https://blog.keras.io/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras.html) which I've wrapped into a fairly complicated object (although the issue also occurs with a simplified version linked below). When I create a new model (which I have to do for gridsearch and for clearing the TF session when the graph gets too big and slows down training) it starts with accuracy of either 0% or 70%.

Here are a pair of screenshots that show what I mean:
 * Good: https://i.imgur.com/7mT5Siv.png
 * Bad: https://i.imgur.com/MZ3NdCB.png

You can see that in the first screenshot, the accuracy is low but trending upwards. In the second, the accuracy of two models starts at 70% and doesn't increase (another model starts at 3% and also doesn't increase).

This happens whether I create new, blank models or load pretrained weights into new models with .

- [ ] Check that you are up-to-date with the master branch of Keras. You can update with:
pip install git+git://github.com/keras-team/keras.git --upgrade --no-deps

- [X] If running on TensorFlow, check that you are up-to-date with the latest version. The installation instructions can be found [here](https://www.tensorflow.org/get_started/os_setup).

- [ ] If running on Theano, check that you are up-to-date with the master branch of Theano. You can update with:
pip install git+git://github.com/Theano/Theano.git --upgrade --no-deps

- [X] Provide a link to a GitHub Gist of a Python script that can reproduce your issue (or just copy the script here if it is short).

Here is a minimal Gist which reproduces the problem: https://gist.github.com/ChrisSwinchatt/97304761e9f875dfd34e3339891a5475",1,Accuracy oscillates between ~0% and ~70% when creating new models,"Accuracy oscillates between ~0% and ~70% when creating new models Hi all,

**I've found that the problem doesn't occur when TensorFlow is forced to use the CPU** -- (I think) this implies that it's a TensorFlow bug and not a Keras bug, so maybe this issue can be closed.

First off, I'm using the Keras that's distributed with TensorFlow 1.10.0 so let me know if I should have opened the issue on their repo instead.

I'm using a sequence-to-sequence model based on the [Keras blogpost](https://blog.keras.io/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras.html) which I've wrapped into a fairly complicated object (although the issue also occurs with a simplified version linked below). When I create a new model (which I have to do for gridsearch and for clearing the TF session when the graph gets too big and slows down training) it starts with accuracy of either 0% or 70%.

Here are a pair of screenshots that show what I mean:
 * Good: https://i.imgur.com/7mT5Siv.png
 * Bad: https://i.imgur.com/MZ3NdCB.png

You can see that in the first screenshot, the accuracy is low but trending upwards. In the second, the accuracy of two models starts at 70% and doesn't increase (another model starts at 3% and also doesn't increase).

This happens whether I create new, blank models or load pretrained weights into new models with .

- [ ] Check that you are up-to-date with the master branch of Keras. You can update with:
pip install git+git://github.com/keras-team/keras.git --upgrade --no-deps

- [X] If running on TensorFlow, check that you are up-to-date with the latest version. The installation instructions can be found [here](https://www.tensorflow.org/get_started/os_setup).

- [ ] If running on Theano, check that you are up-to-date with the master branch of Theano. You can update with:
pip install git+git://github.com/Theano/Theano.git --upgrade --no-deps

- [X] Provide a link to a GitHub Gist of a Python script that can reproduce your issue (or just copy the script here if it is short).

Here is a minimal Gist which reproduces the problem: https://gist.github.com/ChrisSwinchatt/97304761e9f875dfd34e3339891a5475"
keras,6470,"Guys I've upgraded my keras and theano to keras 2.0 and theano 9.0. I've tested GPU and it works fine, the test script is ok. I've also installed cuDNN 5110 and it works fine too.,but the script runs too slowly. I've even used some configuration in theano to make the performance better:Forced to use dnn, to use float32 (warn if float is 64), made some optimizations, but the learning process goes too long anyway. I had a CNN with about 16 layers (2 FC layers).**With keras 1, the 1 epoch took ~1m, but with keras2.0 it takes ~3.24m**.Is there any idea how I can make the speed better? almost all tests related with theano and gpu run without any problem.",1,"Keras 2.0 is too slow, even with theano 0.9","Keras 2.0 is too slow, even with theano 0.9 Guys I've upgraded my keras and theano to keras 2.0 and theano 9.0. I've tested GPU and it works fine, the test script is ok. I've also installed cuDNN 5110 and it works fine too.,but the script runs too slowly. I've even used some configuration in theano to make the performance better:Forced to use dnn, to use float32 (warn if float is 64), made some optimizations, but the learning process goes too long anyway. I had a CNN with about 16 layers (2 FC layers).**With keras 1, the 1 epoch took ~1m, but with keras2.0 it takes ~3.24m**.Is there any idea how I can make the speed better? almost all tests related with theano and gpu run without any problem."
keras,9543,"Hi everyone,

On a personal project, the bi-gpu was slower than mono-gpu. Therefore, I wanted to check if the problem was my project or something else. Using the cifar10_resnet.py model, I just added

Just before the model compilation annnnnnd:

1 GPU, 1 epoch = 27 sec
2 GPUs, 1 epoch = 32 sec

Any ideas ?

Thanks you !
",1,cifar10_resnet.py Multi-gpu is slower than mono gpu ,"cifar10_resnet.py Multi-gpu is slower than mono gpu  Hi everyone,

On a personal project, the bi-gpu was slower than mono-gpu. Therefore, I wanted to check if the problem was my project or something else. Using the cifar10_resnet.py model, I just added

Just before the model compilation annnnnnd:

1 GPU, 1 epoch = 27 sec
2 GPUs, 1 epoch = 32 sec

Any ideas ?

Thanks you !
"
keras,5960,"Hi guys,
I have executed an LSTM in my dataset for about 7 epochs.

       inputs = Input(shape = (40,) , dtype = 'int32')
        Embs = Embedding(19449, embedding_vecor_length, input_length=40, weights = [w1],mask_zero=True,trainable=False)
        Embsyo = Embs(inputs)
         lstm1 = LSTM(300,activation='tanh', recurrent_activation='sigmoid')(Embsyo)
         out = Dense(5, activation = 'softmax')(lstm1)
         model = Model(input = inputs, output = out)
         model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])

However, each time I run the model, I get a different accuracy on my test dataset. Like one time I get 32% and the next time I get 26%. I keep the epochs fixed at 7 and even removed dropouts to avoid randomness. Is there any way I can get Keras to give me the same or atleast very similar accuracy percentage each time I run the code? Thanks in advance ",1,lack of consistency in Keras Model,"lack of consistency in Keras Model Hi guys,
I have executed an LSTM in my dataset for about 7 epochs.

       inputs = Input(shape = (40,) , dtype = 'int32')
        Embs = Embedding(19449, embedding_vecor_length, input_length=40, weights = [w1],mask_zero=True,trainable=False)
        Embsyo = Embs(inputs)
         lstm1 = LSTM(300,activation='tanh', recurrent_activation='sigmoid')(Embsyo)
         out = Dense(5, activation = 'softmax')(lstm1)
         model = Model(input = inputs, output = out)
         model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])

However, each time I run the model, I get a different accuracy on my test dataset. Like one time I get 32% and the next time I get 26%. I keep the epochs fixed at 7 and even removed dropouts to avoid randomness. Is there any way I can get Keras to give me the same or atleast very similar accuracy percentage each time I run the code? Thanks in advance "
keras,10063,"I want to use VGG in my code. But accuracy and loss are not changing. How can i  solve this problem?
My dataset is medical image(Predicting IDC or non IDC) 
Here is my code. 
-------------------------------------



And result is here, 
Epoch 1/8
1299/1298 [==============================] - 27s 21ms/step - loss: 0.5580 - acc: 0.7216 - val_loss: 0.5227 - val_acc: 0.7386
Epoch 2/8
1299/1298 [==============================] - 27s 21ms/step - loss: 0.5260 - acc: 0.7466 - val_loss: 0.5321 - val_acc: 0.7298
Epoch 3/8
1299/1298 [==============================] - 27s 21ms/step - loss: 0.5175 - acc: 0.7512 - val_loss: 0.5170 - val_acc: 0.7412
Epoch 4/8
1299/1298 [==============================] - 27s 21ms/step - loss: 0.5166 - acc: 0.7556 - val_loss: 0.5086 - val_acc: 0.7528
Epoch 5/8
1299/1298 [==============================] - 27s 21ms/step - loss: 0.5141 - acc: 0.7562 - val_loss: 0.5017 - val_acc: 0.7572
Epoch 6/8
1299/1298 [==============================] - 27s 21ms/step - loss: 0.5119 - acc: 0.7602 - val_loss: 0.5061 - val_acc: 0.7515
Epoch 7/8
1299/1298 [==============================] - 27s 21ms/step - loss: 0.5090 - acc: 0.7591 - val_loss: 0.4999 - val_acc: 0.7611
Epoch 8/8
1299/1298 [==============================] - 27s 21ms/step - loss: 0.5100 - acc: 0.7624 - val_loss: 0.5043 - val_acc: 0.7539

Keras CNN #1C - accuracy: 0.7538994800234126 




Please make sure that the boxes below are checked before you submit your issue. If your issue is an implementation question, please ask your question on [StackOverflow](http://stackoverflow.com/questions/tagged/keras) or [join the Keras Slack channel](https://keras-slack-autojoin.herokuapp.com/) and ask there instead of filing a GitHub issue.

Thank you!

- [x] Check that you are up-to-date with the master branch of Keras. You can update with:
pip install git+git://github.com/keras-team/keras.git --upgrade --no-deps

- [x] If running on TensorFlow, check that you are up-to-date with the latest version. The installation instructions can be found [here](https://www.tensorflow.org/get_started/os_setup).

- [x] If running on Theano, check that you are up-to-date with the master branch of Theano. You can update with:
pip install git+git://github.com/Theano/Theano.git --upgrade --no-deps

- [x] Provide a link to a GitHub Gist of a Python script that can reproduce your issue (or just copy the script here if it is short).
",1,accuracy and loss are not changing ,"accuracy and loss are not changing  I want to use VGG in my code. But accuracy and loss are not changing. How can i  solve this problem?
My dataset is medical image(Predicting IDC or non IDC) 
Here is my code. 
-------------------------------------



And result is here, 
Epoch 1/8
1299/1298 [==============================] - 27s 21ms/step - loss: 0.5580 - acc: 0.7216 - val_loss: 0.5227 - val_acc: 0.7386
Epoch 2/8
1299/1298 [==============================] - 27s 21ms/step - loss: 0.5260 - acc: 0.7466 - val_loss: 0.5321 - val_acc: 0.7298
Epoch 3/8
1299/1298 [==============================] - 27s 21ms/step - loss: 0.5175 - acc: 0.7512 - val_loss: 0.5170 - val_acc: 0.7412
Epoch 4/8
1299/1298 [==============================] - 27s 21ms/step - loss: 0.5166 - acc: 0.7556 - val_loss: 0.5086 - val_acc: 0.7528
Epoch 5/8
1299/1298 [==============================] - 27s 21ms/step - loss: 0.5141 - acc: 0.7562 - val_loss: 0.5017 - val_acc: 0.7572
Epoch 6/8
1299/1298 [==============================] - 27s 21ms/step - loss: 0.5119 - acc: 0.7602 - val_loss: 0.5061 - val_acc: 0.7515
Epoch 7/8
1299/1298 [==============================] - 27s 21ms/step - loss: 0.5090 - acc: 0.7591 - val_loss: 0.4999 - val_acc: 0.7611
Epoch 8/8
1299/1298 [==============================] - 27s 21ms/step - loss: 0.5100 - acc: 0.7624 - val_loss: 0.5043 - val_acc: 0.7539

Keras CNN #1C - accuracy: 0.7538994800234126 




Please make sure that the boxes below are checked before you submit your issue. If your issue is an implementation question, please ask your question on [StackOverflow](http://stackoverflow.com/questions/tagged/keras) or [join the Keras Slack channel](https://keras-slack-autojoin.herokuapp.com/) and ask there instead of filing a GitHub issue.

Thank you!

- [x] Check that you are up-to-date with the master branch of Keras. You can update with:
pip install git+git://github.com/keras-team/keras.git --upgrade --no-deps

- [x] If running on TensorFlow, check that you are up-to-date with the latest version. The installation instructions can be found [here](https://www.tensorflow.org/get_started/os_setup).

- [x] If running on Theano, check that you are up-to-date with the master branch of Theano. You can update with:
pip install git+git://github.com/Theano/Theano.git --upgrade --no-deps

- [x] Provide a link to a GitHub Gist of a Python script that can reproduce your issue (or just copy the script here if it is short).
"
keras,13136,"I am trying to train a CNN using frames that portray me shooting a ball through a basket. And my aim is for the network to be able to classify the result( hit or miss) correctly. When I train the network, the training accuracy increases slowly until it reaches 100%, while the validation accuracy remains around 65% (It is important to mention here that 65% is the percentage of shots that have a Miss label)   Does anyone have experience with a similar problem?
I am using PyTorch and Resnet18 ( have tried other architectures as well but they all gave the same result). My frames are jpg images of sie 224.
As an optimizer, both Adam and SGD gave the same result 
Thank you in advance ",1,"Training accuracy increases, val accuracy stays the same","Training accuracy increases, val accuracy stays the same I am trying to train a CNN using frames that portray me shooting a ball through a basket. And my aim is for the network to be able to classify the result( hit or miss) correctly. When I train the network, the training accuracy increases slowly until it reaches 100%, while the validation accuracy remains around 65% (It is important to mention here that 65% is the percentage of shots that have a Miss label)   Does anyone have experience with a similar problem?
I am using PyTorch and Resnet18 ( have tried other architectures as well but they all gave the same result). My frames are jpg images of sie 224.
As an optimizer, both Adam and SGD gave the same result 
Thank you in advance "
keras,12625,"
80% my GPU memory get's full after loading pre-trained Xception model. but after deleting my model , memory doesn't get empty or flush.
I've also used codes like : K.clear_session() , gc.collect() , tf.reset_default_graph() , del model but none of them worked. Gpu properties say's 85% of memory is full. 

Nothing flush gpu memory except numba.cuda.close() but won't allow me to use my gpu again. The only way to clear it is restarting kernel and rerun my code.

I'm looking for any script code to add my code allow me to use my code in for loop and clear gpu in every loop.

Part of my code :

image_input = Input(shape=(224, 224, 3))
base_model = Xception(input_tensor=image_input, include_top=False,weights='imagenet')
base_model.compile(loss='categorical_crossentropy',optimizer='adadelta',metrics=['accuracy'])
hist = base_model.fit(X,Y,epochs=2)

System information

    Have I written custom code :
    Windows 10 64-bit
    TensorFlow installed from conda install tensorflow-gpu
    TensorFlow version: 1.3
    Python version: 3.6
    CUDA/cuDNN version: 9.2
    GPU model and memory: Asus GTX 1060 6gb
",1,Clearing GPU memory in Keras,"Clearing GPU memory in Keras 
80% my GPU memory get's full after loading pre-trained Xception model. but after deleting my model , memory doesn't get empty or flush.
I've also used codes like : K.clear_session() , gc.collect() , tf.reset_default_graph() , del model but none of them worked. Gpu properties say's 85% of memory is full. 

Nothing flush gpu memory except numba.cuda.close() but won't allow me to use my gpu again. The only way to clear it is restarting kernel and rerun my code.

I'm looking for any script code to add my code allow me to use my code in for loop and clear gpu in every loop.

Part of my code :

image_input = Input(shape=(224, 224, 3))
base_model = Xception(input_tensor=image_input, include_top=False,weights='imagenet')
base_model.compile(loss='categorical_crossentropy',optimizer='adadelta',metrics=['accuracy'])
hist = base_model.fit(X,Y,epochs=2)

System information

    Have I written custom code :
    Windows 10 64-bit
    TensorFlow installed from conda install tensorflow-gpu
    TensorFlow version: 1.3
    Python version: 3.6
    CUDA/cuDNN version: 9.2
    GPU model and memory: Asus GTX 1060 6gb
"
keras,1360,"I'm a theano and keras fresher, and want to learn them , which I think very interesting and helpful.
The following question confuses me about for one week. But I can't work it out after try some ways mentioned before.
I want to do sentiment analysis for texts to three classes. And I train word2vec(dim = 600) with gensim.
My train data is 10475 sequences in different length. label shape is [10475,3] After setting maxlen of sequence 200, every sequence are converted to 200*600 2D array.If some sequence's length is less than 200, then the remaining values is filled with 0(padding), resulting some rows are all zeroes. And then I feed them into LSTM, 
## LSTM code as following:



model.fit(train,label,batch_size=100,nb_epoch=4,verbose=1,shuffle=True,validation_split=0.1,show_accuracy=True)
## But Getting:
## loss: nan

Train on 9430 samples, validate on 1048 samples
Epoch 1/4
9430/9430 [==============================] - 99s - loss: nan - acc: 0.2992 - val_loss: nan - val_acc: 0.1355
Epoch 2/4
9430/9430 [==============================] - 96s - loss: nan - acc: 0.2992 - val_loss: nan - val_acc: 0.1355
Epoch 3/4
9430/9430 [==============================] - 96s - loss: nan - acc: 0.2992 - val_loss: nan - val_acc: 0.1355
Epoch 4/4
1600/9430 [====>.........................] - ETA: 75s - loss: nan - acc: 0.3038

I test different optimizer,also improve epsilon value, set clipnorm(in optimizer above) and different loss functions('mean_squared_error', 'categorical_crossentropy') and so on, but failed.
#### Also in cpu or gpu mode, loss value is also nan.
# ##Even I switch to Convolution2D:


### The loss values remain nan
### Ways to solve?

So I'm wondering what's the real reason for the NaN loss value? How to solve or debug it?
Is the word2vec data wrong , padding method wrong or other?
If keras can't solve, I have to choose another deep learning package, or the reason is theano?
what can I do then? please help.
",1,why lstm loss is NaN for pre-trained word2vec,"why lstm loss is NaN for pre-trained word2vec I'm a theano and keras fresher, and want to learn them , which I think very interesting and helpful.
The following question confuses me about for one week. But I can't work it out after try some ways mentioned before.
I want to do sentiment analysis for texts to three classes. And I train word2vec(dim = 600) with gensim.
My train data is 10475 sequences in different length. label shape is [10475,3] After setting maxlen of sequence 200, every sequence are converted to 200*600 2D array.If some sequence's length is less than 200, then the remaining values is filled with 0(padding), resulting some rows are all zeroes. And then I feed them into LSTM, 
## LSTM code as following:



model.fit(train,label,batch_size=100,nb_epoch=4,verbose=1,shuffle=True,validation_split=0.1,show_accuracy=True)
## But Getting:
## loss: nan

Train on 9430 samples, validate on 1048 samples
Epoch 1/4
9430/9430 [==============================] - 99s - loss: nan - acc: 0.2992 - val_loss: nan - val_acc: 0.1355
Epoch 2/4
9430/9430 [==============================] - 96s - loss: nan - acc: 0.2992 - val_loss: nan - val_acc: 0.1355
Epoch 3/4
9430/9430 [==============================] - 96s - loss: nan - acc: 0.2992 - val_loss: nan - val_acc: 0.1355
Epoch 4/4
1600/9430 [====>.........................] - ETA: 75s - loss: nan - acc: 0.3038

I test different optimizer,also improve epsilon value, set clipnorm(in optimizer above) and different loss functions('mean_squared_error', 'categorical_crossentropy') and so on, but failed.
#### Also in cpu or gpu mode, loss value is also nan.
# ##Even I switch to Convolution2D:


### The loss values remain nan
### Ways to solve?

So I'm wondering what's the real reason for the NaN loss value? How to solve or debug it?
Is the word2vec data wrong , padding method wrong or other?
If keras can't solve, I have to choose another deep learning package, or the reason is theano?
what can I do then? please help.
"
keras,8020,"Hello everyone.
I've been reading many discussions here and in google groups regarding the tf backend issues and I found them too **wordy** and without a bottom-line for a question, that at least in my opinion should has a short answer.

I'm using the CNN mnist model proposed here: [https://github.com/fchollet/keras/blob/master/examples/mnist_cnn.py](url)
To run some performances tests.

I'm using a p2.xlarge amazon instance with a Tesla K80 single GPU.
My python version is 3.6
Keras==2.0.8 (conda keras-gpu version)
tensorflow==1.2.0
Theano==0.9.0

My keras.json file at ~/.keras/keras.json:

and my theanorc file contains:


When I run the code I get that Theano and Tensorflow are roughly equal (Theano 8s per epoch, and Tf is 10s per epoch but loads the data faster)

But when I'm ruuning this on my model whose input shape is 224x224x3x30000 images
**Theano outperforms Tenserflow by a factor of 2.5!** (although it takes about 5 minutes for theano to initialize the images)

**Tensorflow backend:**
![image](https://user-images.githubusercontent.com/31940058/31015242-c0ba6c6c-a527-11e7-9751-c072b40aafe7.png)

**Theano backend:**
![image](https://user-images.githubusercontent.com/31940058/31018700-aa694f2e-a535-11e7-8b9a-9c81a65eb6db.png)


One more thing - when I execute ""import keras"" with tensorflow backend I receive:


While theano as backend:



**Thanks for help!**",1,Tensorflow backend is slower than Theano in CNN models,"Tensorflow backend is slower than Theano in CNN models Hello everyone.
I've been reading many discussions here and in google groups regarding the tf backend issues and I found them too **wordy** and without a bottom-line for a question, that at least in my opinion should has a short answer.

I'm using the CNN mnist model proposed here: [https://github.com/fchollet/keras/blob/master/examples/mnist_cnn.py](url)
To run some performances tests.

I'm using a p2.xlarge amazon instance with a Tesla K80 single GPU.
My python version is 3.6
Keras==2.0.8 (conda keras-gpu version)
tensorflow==1.2.0
Theano==0.9.0

My keras.json file at ~/.keras/keras.json:

and my theanorc file contains:


When I run the code I get that Theano and Tensorflow are roughly equal (Theano 8s per epoch, and Tf is 10s per epoch but loads the data faster)

But when I'm ruuning this on my model whose input shape is 224x224x3x30000 images
**Theano outperforms Tenserflow by a factor of 2.5!** (although it takes about 5 minutes for theano to initialize the images)

**Tensorflow backend:**
![image](https://user-images.githubusercontent.com/31940058/31015242-c0ba6c6c-a527-11e7-9751-c072b40aafe7.png)

**Theano backend:**
![image](https://user-images.githubusercontent.com/31940058/31018700-aa694f2e-a535-11e7-8b9a-9c81a65eb6db.png)


One more thing - when I execute ""import keras"" with tensorflow backend I receive:


While theano as backend:



**Thanks for help!**"
keras,1369,"Currently, I am using Bidirectional RNN/LSTM(BRNN/BLSTM) for sequence classification (return_sequence= True, i.e., classification of each time-step). According to the official provided example, the BRNN is required to implemented by Graph (while the BidirectionalRNN API does not support mask input). Besides, before mini-batch input, I do a preprocessing (pad with 0) to make the input to (batch_size, input_length, input_features), but it seems that no explicit mask input for keras. So I listed my implementation as 











As no **show_accuracy=True** for modellstm.fit (fit of Graph does not support), the accuracy 
 doesn't take the input_mask into consideration. Besides, for example, **without explicit input_mask**, **the output result of classification np.argmax([0 0 0 0])=0 (i.e., class=1) for input np.array([0 0 ... 0]), which is indeed wrong**. Actually input_mask for input np.array([0 0 ... 0]) should be **0**, which means all 0 input and output should not be calculated for accuracy. How could I modify me code to accomplish this code. Thanks a lot for your nice assistance. @fchollet @farizrahman4u
",1,The accuracy of using Graph to implement Bidirectional RNN for classification is note correct,"The accuracy of using Graph to implement Bidirectional RNN for classification is note correct Currently, I am using Bidirectional RNN/LSTM(BRNN/BLSTM) for sequence classification (return_sequence= True, i.e., classification of each time-step). According to the official provided example, the BRNN is required to implemented by Graph (while the BidirectionalRNN API does not support mask input). Besides, before mini-batch input, I do a preprocessing (pad with 0) to make the input to (batch_size, input_length, input_features), but it seems that no explicit mask input for keras. So I listed my implementation as 











As no **show_accuracy=True** for modellstm.fit (fit of Graph does not support), the accuracy 
 doesn't take the input_mask into consideration. Besides, for example, **without explicit input_mask**, **the output result of classification np.argmax([0 0 0 0])=0 (i.e., class=1) for input np.array([0 0 ... 0]), which is indeed wrong**. Actually input_mask for input np.array([0 0 ... 0]) should be **0**, which means all 0 input and output should not be calculated for accuracy. How could I modify me code to accomplish this code. Thanks a lot for your nice assistance. @fchollet @farizrahman4u
"
keras,8027,"# **Update: finally, I found the problem. It was related with the activation function and it was my fault. Sorry for that. I closed the issue.**

I'm addressing a sentence-level binary classification task. My data consists of 3 subarrays of tokens: left context, core, and right context.

I used Keras to devise several alternatives of Convolutional Neural Networks and validate which one best fit my problem.

I'm a newbie in Python and Keras and I decided to start with simpler solutions in order to test which changes improve my metrics (accuracy, precision, recall, f1 and auc-roc). The first simplification was regarding input data: I decided to ignore contexts to create a Sequential model of Keras:


As you can see, I use a fixed size of inputs so I applied a padding preprocessing. I also used an embedding layer with a Word2Vec model.

This model returns the following results:


I wished to implement how to select a subarray of input data inside my CNN by means of Lambda layers. I use the following definition of my Lambda layer:

Lambda(lambda x: x[:, 1], output_shape=(500,))(input)
And this is the summary of my new CNN (as you can see it's almost the same than the prior):


But the results were disgusting because accuracy stops at 60% and obviously, precision, recall and f1 were too low (< 0.10) regarding the first model results.

I don't know what's happening and I don't know if these networks are more different that I thought.

Any clue regarding this issue?

**Note**: I asked this question in Stackoverflow but I think it could be an issue regarding Keras implementation. Link [here](https://stackoverflow.com/questions/46491418/why-2-almost-equal-keras-cnn-returns-2-quite-different-results)",1,Why 2 almost equal CNN returns 2 quite different results,"Why 2 almost equal CNN returns 2 quite different results # **Update: finally, I found the problem. It was related with the activation function and it was my fault. Sorry for that. I closed the issue.**

I'm addressing a sentence-level binary classification task. My data consists of 3 subarrays of tokens: left context, core, and right context.

I used Keras to devise several alternatives of Convolutional Neural Networks and validate which one best fit my problem.

I'm a newbie in Python and Keras and I decided to start with simpler solutions in order to test which changes improve my metrics (accuracy, precision, recall, f1 and auc-roc). The first simplification was regarding input data: I decided to ignore contexts to create a Sequential model of Keras:


As you can see, I use a fixed size of inputs so I applied a padding preprocessing. I also used an embedding layer with a Word2Vec model.

This model returns the following results:


I wished to implement how to select a subarray of input data inside my CNN by means of Lambda layers. I use the following definition of my Lambda layer:

Lambda(lambda x: x[:, 1], output_shape=(500,))(input)
And this is the summary of my new CNN (as you can see it's almost the same than the prior):


But the results were disgusting because accuracy stops at 60% and obviously, precision, recall and f1 were too low (< 0.10) regarding the first model results.

I don't know what's happening and I don't know if these networks are more different that I thought.

Any clue regarding this issue?

**Note**: I asked this question in Stackoverflow but I think it could be an issue regarding Keras implementation. Link [here](https://stackoverflow.com/questions/46491418/why-2-almost-equal-keras-cnn-returns-2-quite-different-results)"
keras,863,"I noticed that when I include weight regularization such as L2 weight regularization, it significantly slows down training. Can someone provide an explanation for why that is?
",1,Keras slow with regularization?,"Keras slow with regularization? I noticed that when I include weight regularization such as L2 weight regularization, it significantly slows down training. Can someone provide an explanation for why that is?
"
keras,866,"With the latest updates to scan in Theano, anybody else experiencing dramatically different training times? In particular, LSTM is faster, but GRU/JZS1-3 is much, much slower:

A relative comparison on a test network, exchanging the recurrent layer:



For this same network, LSTM pre Theano scan changes was ~24s.

Anybody have some insight into this?
",1,"with latest Theano updates, LSTM faster but GRU much slower","with latest Theano updates, LSTM faster but GRU much slower With the latest updates to scan in Theano, anybody else experiencing dramatically different training times? In particular, LSTM is faster, but GRU/JZS1-3 is much, much slower:

A relative comparison on a test network, exchanging the recurrent layer:



For this same network, LSTM pre Theano scan changes was ~24s.

Anybody have some insight into this?
"
keras,867,"Hello, 

Is it normal that when i use a MLP to make a regression (with time series like sunspot) they are only 'loss' and 'val_loss' that decrease during the training ( acc and val_acc are always at 1.0000) ?

for information, I put a linear activation on output layer (which have only one neural) with mse and each hiden layer have 'relu' activation and dropout

thank you to anybody can give me a good reason :) 
",1,Regression with MLP,"Regression with MLP Hello, 

Is it normal that when i use a MLP to make a regression (with time series like sunspot) they are only 'loss' and 'val_loss' that decrease during the training ( acc and val_acc are always at 1.0000) ?

for information, I put a linear activation on output layer (which have only one neural) with mse and each hiden layer have 'relu' activation and dropout

thank you to anybody can give me a good reason :) 
"
keras,11119,"I've noticed that the process of saving and loading saved models slows down as I generate more models. To clarify, say I have to load 10 models from hdf5 files. The 1st model I load will be loaded quickly, but every successive load will be progressively slower. The models are of the same size, and memory is not an issue. I've attached a python script to reproduce the issue. 

OS: Ubuntu 16.04
Backend: Tensorflow
Using CPU only

Thank you

### Reproducing code example:



### Example output:


",1,Progressively slower model save/load times,"Progressively slower model save/load times I've noticed that the process of saving and loading saved models slows down as I generate more models. To clarify, say I have to load 10 models from hdf5 files. The 1st model I load will be loaded quickly, but every successive load will be progressively slower. The models are of the same size, and memory is not an issue. I've attached a python script to reproduce the issue. 

OS: Ubuntu 16.04
Backend: Tensorflow
Using CPU only

Thank you

### Reproducing code example:



### Example output:


"
keras,10096,"We have observed some massive slowdowns in some Keras/tensorflow models with tensorflow versions newer than 1.4.1.
Not sure if the issue is with tensorflow or with the way keras creates the tensorflow models, so I am cross-posting these issue to both repos.

Here is a script reproducing the issue:

## Setup


## Fitting



Here are some timings for the fit method:
* Tensorflow 1.4.1:  **2.91 s ± 452 ms** per loop  (obtained using ipython's  magic, 7 loops)
* Tensorflow 1.5.0:  CPU times: user **2min 19s**, sys: 5min 22s, total: 7min 41s Wall time: 1min 2s
* Tensorflow 1.6.0: CPU times: user **5min 5s**, sys: 12min 31s, total: 17min 36s Wall time: 2min 37s
* Tensorflow 1.7.0: CPU times: user **5min 5s**, sys: 12min 39s, total: 17min 45s Wall time: 2min 39s

So, it seems there was a massive slowdown in version 1,5, and then a further one in 1.6 (which similar speed in 1.7). All the tests are run on a conda environment with python 3.6.5 and keras 2.1.5, with the corresponding tensorflow versions all coming from the anaconda  channel.

The GPU accelerated version of keras/tensorflow ( conda package) does not present the issue.

Thanks in advance!",1,Speed regression with change of tensorflow backend version,"Speed regression with change of tensorflow backend version We have observed some massive slowdowns in some Keras/tensorflow models with tensorflow versions newer than 1.4.1.
Not sure if the issue is with tensorflow or with the way keras creates the tensorflow models, so I am cross-posting these issue to both repos.

Here is a script reproducing the issue:

## Setup


## Fitting



Here are some timings for the fit method:
* Tensorflow 1.4.1:  **2.91 s ± 452 ms** per loop  (obtained using ipython's  magic, 7 loops)
* Tensorflow 1.5.0:  CPU times: user **2min 19s**, sys: 5min 22s, total: 7min 41s Wall time: 1min 2s
* Tensorflow 1.6.0: CPU times: user **5min 5s**, sys: 12min 31s, total: 17min 36s Wall time: 2min 37s
* Tensorflow 1.7.0: CPU times: user **5min 5s**, sys: 12min 39s, total: 17min 45s Wall time: 2min 39s

So, it seems there was a massive slowdown in version 1,5, and then a further one in 1.6 (which similar speed in 1.7). All the tests are run on a conda environment with python 3.6.5 and keras 2.1.5, with the corresponding tensorflow versions all coming from the anaconda  channel.

The GPU accelerated version of keras/tensorflow ( conda package) does not present the issue.

Thanks in advance!"
keras,13172,"Hello everyone, there might be a problem with the metrics during training when using a generator.

**System information**  
- Have I written custom code (as opposed to using example directory):  **yes**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  **Windows 10**
- TensorFlow backend (yes / no):  **yes**
- TensorFlow version:  **1.10.0**
- Keras version:  **2.2.4**
- Python version:  **3.6.8**
- CUDA/cuDNN version:  **None**
- GPU model and memory:  **None**

**Current behavior**
During training,  shows very low scores.


**Expected behavior**  
After training, the model performs very well. The actual performance of the trained model on the task largely outclasses the suggested performance estimate during training.
Therefore I expect the metrics during training to show better values.

**Code to reproduce the issue**  



**Other info / logs**  
This code is just the generator. The false metrics came up when I introduced the generator to the training.
Did anyone have similar experiences?
Can anyone tell from the generator if something obvious and important is missing?

Thank you very much. 
",1,Low scores during training but good perormance on real data with fit_generator,"Low scores during training but good perormance on real data with fit_generator Hello everyone, there might be a problem with the metrics during training when using a generator.

**System information**  
- Have I written custom code (as opposed to using example directory):  **yes**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  **Windows 10**
- TensorFlow backend (yes / no):  **yes**
- TensorFlow version:  **1.10.0**
- Keras version:  **2.2.4**
- Python version:  **3.6.8**
- CUDA/cuDNN version:  **None**
- GPU model and memory:  **None**

**Current behavior**
During training,  shows very low scores.


**Expected behavior**  
After training, the model performs very well. The actual performance of the trained model on the task largely outclasses the suggested performance estimate during training.
Therefore I expect the metrics during training to show better values.

**Code to reproduce the issue**  



**Other info / logs**  
This code is just the generator. The false metrics came up when I introduced the generator to the training.
Did anyone have similar experiences?
Can anyone tell from the generator if something obvious and important is missing?

Thank you very much. 
"
keras,10624,"The post in Keras blog [here](https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html) yields a 95%  validation accuracy after 2 epochs. Given the example code, I can't reproduce this (in fact the validation accuracy will not go over 75% and loss is very high as well). 

I'm following the example code here:

https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html

...with the exception that I open the GloVe vector file with encoding='utf-8' as it fails otherwise. The link is provided in the example blog post itself. ",1,accuracy issue (using the 20 newsgroups example model),"accuracy issue (using the 20 newsgroups example model) The post in Keras blog [here](https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html) yields a 95%  validation accuracy after 2 epochs. Given the example code, I can't reproduce this (in fact the validation accuracy will not go over 75% and loss is very high as well). 

I'm following the example code here:

https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html

...with the exception that I open the GloVe vector file with encoding='utf-8' as it fails otherwise. The link is provided in the example blog post itself. "
keras,7558,"


the cost2 (categorical_crossentropy) causes the poor results finally, but the results are good using cost1.

I used these in the mnist cnn algorithm, as follows:

",1,issue in categorical_crossentropy (keras) and softmax_cross_entropy_with_logits (tensorflow),"issue in categorical_crossentropy (keras) and softmax_cross_entropy_with_logits (tensorflow) 


the cost2 (categorical_crossentropy) causes the poor results finally, but the results are good using cost1.

I used these in the mnist cnn algorithm, as follows:

"
keras,10632,"I train a few resnet v2 110 model with cifar10, and only get 92.x% accuracy on the test set. In the table, it shows test accuracy is about 93.x%. I wonder how to get that? 
",1,Low test accuracy with resnet 110 cifar10,"Low test accuracy with resnet 110 cifar10 I train a few resnet v2 110 model with cifar10, and only get 92.x% accuracy on the test set. In the table, it shows test accuracy is about 93.x%. I wonder how to get that? 
"
keras,11665,"- [ x] Check that you are up-to-date with the master branch of Keras. You can update with:


- [ x] Check that your version of TensorFlow is up-to-date. The installation instructions can be found [here](https://www.tensorflow.org/get_started/os_setup).

- [x ] Provide a link to a GitHub Gist of a Python script that can reproduce your issue (or just copy the script here if it is short).

I am performing batch matrix multiplies of two tensors of size (batch, N, M) and (batch, M, K) to get a tensor of size (batch, N, K), with the matrix products. This behavior can be done with both tf.matmul and K.batch_dot with the default axis arguments.

However in K.batch_dot, the elementwise multiplication in the line https://github.com/keras-team/keras/blob/75a35032e194a2d065b0071a9e786adf6cee83ea/keras/backend/tensorflow_backend.py#L1248 eats up a lot of memory. The elementwise multiplication followed by summing over an axis is of course mathematically equivalent to the matrix multiply, but in the two-step implementation, Tensorflow assigns memory to the intermediate very large tensor.

In this simple example, my small GPU (Nvidia 970) is able to perform the calculation using tf.matmul, but using K.batch_dot Tensorflow fails with an OOM error.



This fails when it tries to assign a tensor of size (100, 10000, 500, 32) in the elementwise multiply in batch_dot (the dimension of 10000 not being strictly necessary in this case since we are only interested in the sum).",1,Poor memory performance of K.batch_dot under tensorflow backend relative to batched tf.matmul,"Poor memory performance of K.batch_dot under tensorflow backend relative to batched tf.matmul - [ x] Check that you are up-to-date with the master branch of Keras. You can update with:


- [ x] Check that your version of TensorFlow is up-to-date. The installation instructions can be found [here](https://www.tensorflow.org/get_started/os_setup).

- [x ] Provide a link to a GitHub Gist of a Python script that can reproduce your issue (or just copy the script here if it is short).

I am performing batch matrix multiplies of two tensors of size (batch, N, M) and (batch, M, K) to get a tensor of size (batch, N, K), with the matrix products. This behavior can be done with both tf.matmul and K.batch_dot with the default axis arguments.

However in K.batch_dot, the elementwise multiplication in the line https://github.com/keras-team/keras/blob/75a35032e194a2d065b0071a9e786adf6cee83ea/keras/backend/tensorflow_backend.py#L1248 eats up a lot of memory. The elementwise multiplication followed by summing over an axis is of course mathematically equivalent to the matrix multiply, but in the two-step implementation, Tensorflow assigns memory to the intermediate very large tensor.

In this simple example, my small GPU (Nvidia 970) is able to perform the calculation using tf.matmul, but using K.batch_dot Tensorflow fails with an OOM error.



This fails when it tries to assign a tensor of size (100, 10000, 500, 32) in the elementwise multiply in batch_dot (the dimension of 10000 not being strictly necessary in this case since we are only interested in the sum)."
keras,3996,"Using TF with the following model, when I run model.fit it takes about 10 minutes before it actually starts doing anything. [out.profile.zip](https://github.com/fchollet/keras/files/517347/out.profile.zip)
Any ideas?


",1,Model.fit takes a long time to start,"Model.fit takes a long time to start Using TF with the following model, when I run model.fit it takes about 10 minutes before it actually starts doing anything. [out.profile.zip](https://github.com/fchollet/keras/files/517347/out.profile.zip)
Any ideas?


"
keras,13212,"The data are loaded with . I watched this issue https://github.com/keras-team/keras/issues/3477, and applied the tweaks:
- No shuffle
- Good order of the images on the FS
- Remove transformations

I don't have a test dataset and want to print the result of the prediction to draw the confusion matrix. I use the same datas (Validation) during the training and the prediction.

During the training, the model achieve 91.5% of accuracy. The  gives me the same accuracy. But during the prediction, I have a completely different accuracy, which is only 86%.

Training Python code:


Evaluate python code:


Prediction python code:


It runs on jupyter notebook with python 3. Is it an issue or am I missing something ?",1,The accuracy on the validation data is not the same during training and prediction,"The accuracy on the validation data is not the same during training and prediction The data are loaded with . I watched this issue https://github.com/keras-team/keras/issues/3477, and applied the tweaks:
- No shuffle
- Good order of the images on the FS
- Remove transformations

I don't have a test dataset and want to print the result of the prediction to draw the confusion matrix. I use the same datas (Validation) during the training and the prediction.

During the training, the model achieve 91.5% of accuracy. The  gives me the same accuracy. But during the prediction, I have a completely different accuracy, which is only 86%.

Training Python code:


Evaluate python code:


Prediction python code:


It runs on jupyter notebook with python 3. Is it an issue or am I missing something ?"
keras,1438,"In a discussion in https://groups.google.com/a/tensorflow.org/d/msg/discuss/V6aeBw4nlaE/VUAgE-nXEwAJ Mark Daoust suggested an impl for leaky relu as:



Wouldn't be better to change the current impl from:



to something like:



I think the two are identical at least for alpha <= 1.

Is it preferable to use tf.nn.relu because it will be faster?

In one place you use tf.constant to cast alpha to the correct type and in other place you use tf.cast  to cast max_value. Which one is better or are they equivalent?

Thanks
",1,"Simplify relu(x, alpha=0., max_value=None) impl in tensorflow_backend.py","Simplify relu(x, alpha=0., max_value=None) impl in tensorflow_backend.py In a discussion in https://groups.google.com/a/tensorflow.org/d/msg/discuss/V6aeBw4nlaE/VUAgE-nXEwAJ Mark Daoust suggested an impl for leaky relu as:



Wouldn't be better to change the current impl from:



to something like:



I think the two are identical at least for alpha <= 1.

Is it preferable to use tf.nn.relu because it will be faster?

In one place you use tf.constant to cast alpha to the correct type and in other place you use tf.cast  to cast max_value. Which one is better or are they equivalent?

Thanks
"
keras,415,"Hi, I just started using keras. Awesome work!
I tried to use LSTM with the following code



And I compared the efficiency with [char-rnn](https://github.com/karpathy/char-rnn), and found that the implementation in keras is about 4 times slower than Karpathy's (with the same batchsize).
Am I doing something wrong? 
I've attached the theano profile [result](https://gist.github.com/ffmpbgrnn/842e1910f216b1e00e27)
Thanks you!
",1,slow training of LSTM,"slow training of LSTM Hi, I just started using keras. Awesome work!
I tried to use LSTM with the following code



And I compared the efficiency with [char-rnn](https://github.com/karpathy/char-rnn), and found that the implementation in keras is about 4 times slower than Karpathy's (with the same batchsize).
Am I doing something wrong? 
I've attached the theano profile [result](https://gist.github.com/ffmpbgrnn/842e1910f216b1e00e27)
Thanks you!
"
keras,9122,"I am trying to train a simple CNN on windows with tensorflow as backend on windows. My inputs are quite large (256x256) but I can train the network on my GTX 1080.

Once the model has been trained and saved, I get OOM errors when restoring it. In order to be able to resume training, I need to divide my batch size by 4.

Is there a reason for the increased memory usage after reloading the model and a way to avoid it? 


[1st_run_output.txt](https://github.com/keras-team/keras/files/1645667/1st_run_output.txt)
[2nd_run_output.txt](https://github.com/keras-team/keras/files/1645668/2nd_run_output.txt)

",1,Increased GPU memory usage after restoring saved model,"Increased GPU memory usage after restoring saved model I am trying to train a simple CNN on windows with tensorflow as backend on windows. My inputs are quite large (256x256) but I can train the network on my GTX 1080.

Once the model has been trained and saved, I get OOM errors when restoring it. In order to be able to resume training, I need to divide my batch size by 4.

Is there a reason for the increased memory usage after reloading the model and a way to avoid it? 


[1st_run_output.txt](https://github.com/keras-team/keras/files/1645667/1st_run_output.txt)
[2nd_run_output.txt](https://github.com/keras-team/keras/files/1645668/2nd_run_output.txt)

"
keras,933,"Hello, 

I am learning to use Graph as it seems more powerful so I implemented one of my previous model which uses Sequential. Here is the model using sequential (number of dimension set in random):



The model works fine and below is my reimplementation using Graph:



My impression is that they are exactly the same model (grateful if somebody spotted something wrong there). But the model based on Graph gives a loss of 3.6 while the loss for the other one is around 0.002. 

Is there a reason for this please ?

Thank you for your help
",1,Same model but graph gives bad performance,"Same model but graph gives bad performance Hello, 

I am learning to use Graph as it seems more powerful so I implemented one of my previous model which uses Sequential. Here is the model using sequential (number of dimension set in random):



The model works fine and below is my reimplementation using Graph:



My impression is that they are exactly the same model (grateful if somebody spotted something wrong there). But the model based on Graph gives a loss of 3.6 while the loss for the other one is around 0.002. 

Is there a reason for this please ?

Thank you for your help
"
keras,11692,"My CNN model learning very slow after some epochs and testing accuracy is not increasing. I'm using **learning rate = 0.0003** and **dropout = 0.55**. **Batch Size = 50**, **Training Samples = 8000** grayscale images and **Testing Samples = 1600** grayscale images. The model is given below. 



![image](https://user-images.githubusercontent.com/17239812/48775610-61fb5400-ecef-11e8-9cc3-57ee1df8f657.png)

How do I increase training and testing accuracy?

![image](https://user-images.githubusercontent.com/17239812/48775842-0aa9b380-ecf0-11e8-908a-3f6df616d0e4.png)

This is the accuracy and loss I'm getting after 30th epoch. Training accuracy is on 0.77 for last 5 epochs!!",1,Slow Learning and Overfitting - CNN,"Slow Learning and Overfitting - CNN My CNN model learning very slow after some epochs and testing accuracy is not increasing. I'm using **learning rate = 0.0003** and **dropout = 0.55**. **Batch Size = 50**, **Training Samples = 8000** grayscale images and **Testing Samples = 1600** grayscale images. The model is given below. 



![image](https://user-images.githubusercontent.com/17239812/48775610-61fb5400-ecef-11e8-9cc3-57ee1df8f657.png)

How do I increase training and testing accuracy?

![image](https://user-images.githubusercontent.com/17239812/48775842-0aa9b380-ecf0-11e8-908a-3f6df616d0e4.png)

This is the accuracy and loss I'm getting after 30th epoch. Training accuracy is on 0.77 for last 5 epochs!!"
keras,8114,"Hi there,

I'm trying to utilise ""bottleneck features"" produced from VGG16 for a new classification task with limited data.

Right now, I have the following (please, excuse code quality):



Which returns the following output:

ModelModel(outputs=Softmax.0, inputs=/input_3)

I'm fairly accustom to Keras and deep learning in general, but I've yet to use this ""bottleneck"" technique. I believe the problem is likely to be related to the corresponding of predicted output to informative labels, but I'm struggling to find a solution.

Any help would be fantastic!

Thanks,

Keiron.",1,"Prediction score extremely low, despite high (~94%) validation accuracy with bottleneck features (VGG16)","Prediction score extremely low, despite high (~94%) validation accuracy with bottleneck features (VGG16) Hi there,

I'm trying to utilise ""bottleneck features"" produced from VGG16 for a new classification task with limited data.

Right now, I have the following (please, excuse code quality):



Which returns the following output:

ModelModel(outputs=Softmax.0, inputs=/input_3)

I'm fairly accustom to Keras and deep learning in general, but I've yet to use this ""bottleneck"" technique. I believe the problem is likely to be related to the corresponding of predicted output to informative labels, but I'm struggling to find a solution.

Any help would be fantastic!

Thanks,

Keiron."
keras,3508,"hi everyone:
   I tried all the examples of MNIST and when i use cpu all of them are work well (tensorflow and theano all tried), but **when i use gpu (tensorflow  backend) mnist_cnn.py is not working, it has very low accuracy**, i guess there's some wrong with cnn on gpu.
([issue 511](https://github.com/fchollet/keras/issues/511) can not solve the problem.)
## my configuration is:
### 1、.keras/keras.json

{
    ""image_dim_ordering"": ""tf"",
    ""epsilon"": 1e-05,
    ""floatx"": ""float32"",
    ""backend"": ""tensorflow""
}
### 2、source code is changed like this：

line 35：X_train = X_train.reshape(X_train.shape[0], img_rows, img_cols, 1)
line 36：X_test = X_test.reshape(X_test.shape[0], img_rows, img_cols, 1)
line 51：model.add(Convolution2D(nb_filters, nb_conv, nb_conv,
line 52：                          border_mode='valid',
line 53：                          input_shape=(img_rows, img_cols, 1)))
### 3、keras version：1.0.7,  GPU:Geforce GTX1070, Cuda toolkit version：7.5, cuDNN version：6.5，tensorflow version：0.9r
## detail information
### 1、on gpu

(tensorflow):~/dllearning/mnist$ python mnist_cnn.py 
I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcublas.so locally
I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcudnn.so locally
I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcufft.so locally
I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcuda.so locally
I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcurand.so locally
Using TensorFlow backend.
X_train shape: (60000, 28, 28, 1)
60000 train samples
10000 test samples
I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:924] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
I tensorflow/core/common_runtime/gpu/gpu_init.cc:102] Found device 0 with properties: 
name: GeForce GTX 1070
major: 6 minor: 1 memoryClockRate (GHz) 1.7845
pciBusID 0000:01:00.0
Total memory: 7.92GiB
Free memory: 1.93GiB
I tensorflow/core/common_runtime/gpu/gpu_init.cc:126] DMA: 0 
I tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 0:   Y 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:806] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 1070, pci bus id: 0000:01:00.0)
Train on 60000 samples, validate on 10000 samples
Epoch 1/12
60000/60000 [==============================] - 12s - loss: 2.3065 - acc: 0.1103 - val_loss: 2.3011 - val_acc: 0.1137
Epoch 2/12
60000/60000 [==============================] - 6s - loss: 2.3022 - acc: 0.1122 - val_loss: 2.3011 - val_acc: 0.1135
Epoch 3/12
60000/60000 [==============================] - 6s - loss: 2.3014 - acc: 0.1123 - val_loss: 2.3011 - val_acc: 0.1135
Epoch 4/12
60000/60000 [==============================] - 6s - loss: 2.3025 - acc: 0.1121 - val_loss: 2.3010 - val_acc: 0.1135
Epoch 5/12
60000/60000 [==============================] - 5s - loss: 2.3013 - acc: 0.1124 - val_loss: 2.3010 - val_acc: 0.1135
Epoch 6/12
60000/60000 [==============================] - 5s - loss: 2.3013 - acc: 0.1124 - val_loss: 2.3010 - val_acc: 0.1135
Epoch 7/12
60000/60000 [==============================] - 6s - loss: 2.3013 - acc: 0.1124 - val_loss: 2.3021 - val_acc: 0.1135
Epoch 8/12
60000/60000 [==============================] - 6s - loss: 2.3013 - acc: 0.1124 - val_loss: 2.3017 - val_acc: 0.1135
Epoch 9/12
60000/60000 [==============================] - 6s - loss: 2.3013 - acc: 0.1123 - val_loss: 2.3010 - val_acc: 0.1135
Epoch 10/12
60000/60000 [==============================] - 5s - loss: 2.3012 - acc: 0.1123 - val_loss: 2.3011 - val_acc: 0.1135
Epoch 11/12
60000/60000 [==============================] - 6s - loss: 2.3013 - acc: 0.1124 - val_loss: 2.3011 - val_acc: 0.1135
Epoch 12/12
60000/60000 [==============================] - 6s - loss: 2.3013 - acc: 0.1124 - val_loss: 2.3011 - val_acc: 0.1135
Test score: 2.30111371231
Test accuracy: 0.1135
### 2、on cpu

Train on 60000 samples, validate on 10000 samples
Epoch 1/12
60000/60000 [==============================] - 71s - loss: 0.3994 - acc: 0.8764 - val_loss: 0.1060 - val_acc: 0.9674
Epoch 2/12
60000/60000 [==============================] - 72s - loss: 0.1495 - acc: 0.9557 - val_loss: 0.0677 - val_acc: 0.9785
Epoch 3/12
60000/60000 [==============================] - 72s - loss: 0.1119 - acc: 0.9675 - val_loss: 0.0537 - val_acc: 0.9823
Epoch 4/12
60000/60000 [==============================] - 100s - loss: 0.0914 - acc: 0.9724 - val_loss: 0.0474 - val_acc: 0.9839
Epoch 5/12
60000/60000 [==============================] - 114s - loss: 0.0805 - acc: 0.9758 - val_loss: 0.0393 - val_acc: 0.9874
Epoch 6/12
60000/60000 [==============================] - 117s - loss: 0.0721 - acc: 0.9783 - val_loss: 0.0381 - val_acc: 0.9873
Epoch 7/12
60000/60000 [==============================] - 118s - loss: 0.0664 - acc: 0.9806 - val_loss: 0.0376 - val_acc: 0.9874
Epoch 8/12
60000/60000 [==============================] - 121s - loss: 0.0621 - acc: 0.9815 - val_loss: 0.0338 - val_acc: 0.9883
Epoch 9/12
60000/60000 [==============================] - 121s - loss: 0.0565 - acc: 0.9834 - val_loss: 0.0322 - val_acc: 0.9887
Epoch 10/12
60000/60000 [==============================] - 124s - loss: 0.0542 - acc: 0.9840 - val_loss: 0.0304 - val_acc: 0.9906
Epoch 11/12
60000/60000 [==============================] - 121s - loss: 0.0491 - acc: 0.9850 - val_loss: 0.0314 - val_acc: 0.9896
Epoch 12/12
60000/60000 [==============================] - 116s - loss: 0.0485 - acc: 0.9857 - val_loss: 0.0294 - val_acc: 0.9898
Test score: 0.0294442383148
Test accuracy: 0.9898
",1,Very low accuracy in the mnist_cnn when running on a GPU using tensorflow  backend,"Very low accuracy in the mnist_cnn when running on a GPU using tensorflow  backend hi everyone:
   I tried all the examples of MNIST and when i use cpu all of them are work well (tensorflow and theano all tried), but **when i use gpu (tensorflow  backend) mnist_cnn.py is not working, it has very low accuracy**, i guess there's some wrong with cnn on gpu.
([issue 511](https://github.com/fchollet/keras/issues/511) can not solve the problem.)
## my configuration is:
### 1、.keras/keras.json

{
    ""image_dim_ordering"": ""tf"",
    ""epsilon"": 1e-05,
    ""floatx"": ""float32"",
    ""backend"": ""tensorflow""
}
### 2、source code is changed like this：

line 35：X_train = X_train.reshape(X_train.shape[0], img_rows, img_cols, 1)
line 36：X_test = X_test.reshape(X_test.shape[0], img_rows, img_cols, 1)
line 51：model.add(Convolution2D(nb_filters, nb_conv, nb_conv,
line 52：                          border_mode='valid',
line 53：                          input_shape=(img_rows, img_cols, 1)))
### 3、keras version：1.0.7,  GPU:Geforce GTX1070, Cuda toolkit version：7.5, cuDNN version：6.5，tensorflow version：0.9r
## detail information
### 1、on gpu

(tensorflow):~/dllearning/mnist$ python mnist_cnn.py 
I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcublas.so locally
I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcudnn.so locally
I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcufft.so locally
I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcuda.so locally
I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcurand.so locally
Using TensorFlow backend.
X_train shape: (60000, 28, 28, 1)
60000 train samples
10000 test samples
I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:924] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
I tensorflow/core/common_runtime/gpu/gpu_init.cc:102] Found device 0 with properties: 
name: GeForce GTX 1070
major: 6 minor: 1 memoryClockRate (GHz) 1.7845
pciBusID 0000:01:00.0
Total memory: 7.92GiB
Free memory: 1.93GiB
I tensorflow/core/common_runtime/gpu/gpu_init.cc:126] DMA: 0 
I tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 0:   Y 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:806] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 1070, pci bus id: 0000:01:00.0)
Train on 60000 samples, validate on 10000 samples
Epoch 1/12
60000/60000 [==============================] - 12s - loss: 2.3065 - acc: 0.1103 - val_loss: 2.3011 - val_acc: 0.1137
Epoch 2/12
60000/60000 [==============================] - 6s - loss: 2.3022 - acc: 0.1122 - val_loss: 2.3011 - val_acc: 0.1135
Epoch 3/12
60000/60000 [==============================] - 6s - loss: 2.3014 - acc: 0.1123 - val_loss: 2.3011 - val_acc: 0.1135
Epoch 4/12
60000/60000 [==============================] - 6s - loss: 2.3025 - acc: 0.1121 - val_loss: 2.3010 - val_acc: 0.1135
Epoch 5/12
60000/60000 [==============================] - 5s - loss: 2.3013 - acc: 0.1124 - val_loss: 2.3010 - val_acc: 0.1135
Epoch 6/12
60000/60000 [==============================] - 5s - loss: 2.3013 - acc: 0.1124 - val_loss: 2.3010 - val_acc: 0.1135
Epoch 7/12
60000/60000 [==============================] - 6s - loss: 2.3013 - acc: 0.1124 - val_loss: 2.3021 - val_acc: 0.1135
Epoch 8/12
60000/60000 [==============================] - 6s - loss: 2.3013 - acc: 0.1124 - val_loss: 2.3017 - val_acc: 0.1135
Epoch 9/12
60000/60000 [==============================] - 6s - loss: 2.3013 - acc: 0.1123 - val_loss: 2.3010 - val_acc: 0.1135
Epoch 10/12
60000/60000 [==============================] - 5s - loss: 2.3012 - acc: 0.1123 - val_loss: 2.3011 - val_acc: 0.1135
Epoch 11/12
60000/60000 [==============================] - 6s - loss: 2.3013 - acc: 0.1124 - val_loss: 2.3011 - val_acc: 0.1135
Epoch 12/12
60000/60000 [==============================] - 6s - loss: 2.3013 - acc: 0.1124 - val_loss: 2.3011 - val_acc: 0.1135
Test score: 2.30111371231
Test accuracy: 0.1135
### 2、on cpu

Train on 60000 samples, validate on 10000 samples
Epoch 1/12
60000/60000 [==============================] - 71s - loss: 0.3994 - acc: 0.8764 - val_loss: 0.1060 - val_acc: 0.9674
Epoch 2/12
60000/60000 [==============================] - 72s - loss: 0.1495 - acc: 0.9557 - val_loss: 0.0677 - val_acc: 0.9785
Epoch 3/12
60000/60000 [==============================] - 72s - loss: 0.1119 - acc: 0.9675 - val_loss: 0.0537 - val_acc: 0.9823
Epoch 4/12
60000/60000 [==============================] - 100s - loss: 0.0914 - acc: 0.9724 - val_loss: 0.0474 - val_acc: 0.9839
Epoch 5/12
60000/60000 [==============================] - 114s - loss: 0.0805 - acc: 0.9758 - val_loss: 0.0393 - val_acc: 0.9874
Epoch 6/12
60000/60000 [==============================] - 117s - loss: 0.0721 - acc: 0.9783 - val_loss: 0.0381 - val_acc: 0.9873
Epoch 7/12
60000/60000 [==============================] - 118s - loss: 0.0664 - acc: 0.9806 - val_loss: 0.0376 - val_acc: 0.9874
Epoch 8/12
60000/60000 [==============================] - 121s - loss: 0.0621 - acc: 0.9815 - val_loss: 0.0338 - val_acc: 0.9883
Epoch 9/12
60000/60000 [==============================] - 121s - loss: 0.0565 - acc: 0.9834 - val_loss: 0.0322 - val_acc: 0.9887
Epoch 10/12
60000/60000 [==============================] - 124s - loss: 0.0542 - acc: 0.9840 - val_loss: 0.0304 - val_acc: 0.9906
Epoch 11/12
60000/60000 [==============================] - 121s - loss: 0.0491 - acc: 0.9850 - val_loss: 0.0314 - val_acc: 0.9896
Epoch 12/12
60000/60000 [==============================] - 116s - loss: 0.0485 - acc: 0.9857 - val_loss: 0.0294 - val_acc: 0.9898
Test score: 0.0294442383148
Test accuracy: 0.9898
"
keras,8629,"I created a siamese like network.

When training with keras loss it does not converge at all.

With raw tensorflow loss function and training_op it converges quickly.

https://gist.github.com/Aelphy/c73aa56bf2f410b1423c63bd7087142e",1,"Siamese network do not converge, while with raw tf the same network converges.","Siamese network do not converge, while with raw tf the same network converges. I created a siamese like network.

When training with keras loss it does not converge at all.

With raw tensorflow loss function and training_op it converges quickly.

https://gist.github.com/Aelphy/c73aa56bf2f410b1423c63bd7087142e"
keras,949,"hello, I'm trying to code something like that http://arxiv.org/abs/1506.04214
to start i'm trying to implement a simple conv 1D on time series (forex). 

the goal it's to predict the 10th value of the last value of a temporal window (60 values in window and I want to predict the 70th) To have my data compatible with conv 1D I prepare my data into a array of 5019 matrix of 60 raw and 1 column

my code : 

 javascript
sizeTraining=0.9
sizeTest=0.1
window=60
h=10

print('Loading data ...')
forex = FOREX()
(X_train, y_train),(X_test,y_test) = forex.load_data(sizeTraining, sizeTest, window, h)

Xtrain_mean = X_train.mean()
Xtrain_ecart_type = X_train.std()
X_train=(X_train-Xtrain_mean)/Xtrain_ecart_type

Xtest_mean = X_test.mean()
Xtest_ecart_type = X_test.std()
X_test=(X_test-Xtest_mean)/Xtest_ecart_type

ytrain_mean = y_train.mean()
ytrain_ecart_type = y_train.std()
y_train=(y_train-ytrain_mean)/ytrain_ecart_type

ytest_mean = y_test.mean()
ytest_ecart_type = y_test.std()
y_test=(y_test-ytest_mean)/ytest_ecart_type

print('x reshape')
X_train = np.reshape(X_train, (-1,window,1))
X_test = np.reshape(X_test, (-1,window,1))
print(X_train.shape)
print(X_train)

print('y reshapet')
y_train = np.reshape(y_train, (-1,1))
y_test = np.reshape(y_train, (-1,1))
print(y_train.shape)

model = Sequential()
#premiere couche de convolution
model.add(Convolution1D(nb_filter=60,filter_length=3,border_mode=""full"",activation=""relu"",subsample_length=1,input_dim=1,input_length=60))
conv1= Activation('relu')
model.add(conv1)
#premiere couche de subsampling
model.add(MaxPooling1D(pool_length=2))
model.add(Dropout(0.25))

model.add(Convolution1D(nb_filter=60,filter_length=3,border_mode=""full"",activation=""relu"",subsample_length=1))
conv1= Activation('relu')
model.add(conv1)
#premiere couche de subsampling
model.add(MaxPooling1D(pool_length=2))
model.add(Dropout(0.25))

#on applatit la sortie pour la presenter au MLP
model.add(Flatten())

#premiere couche du MLP avec activation de type maxoutdense
model.add(Dense(output_dim=500))
model.add(Dropout(0.5))

model.add(Dense(output_dim=500))
model.add(Dropout(0.5))

model.add(Dense(1))
model.add(Activation('linear'))

model.compile(loss='mse', optimizer='adadelta')

history = model.fit(X_train, y_train, batch_size=128, nb_epoch=2, show_accuracy=False, verbose=1, validation_data=(X_test, y_test))
score = model.evaluate(X_test, y_test, show_accuracy=False, verbose=0)
'''

this my problem : 

`
",1,loss : nan after training second batch size with convolution 1D,"loss : nan after training second batch size with convolution 1D hello, I'm trying to code something like that http://arxiv.org/abs/1506.04214
to start i'm trying to implement a simple conv 1D on time series (forex). 

the goal it's to predict the 10th value of the last value of a temporal window (60 values in window and I want to predict the 70th) To have my data compatible with conv 1D I prepare my data into a array of 5019 matrix of 60 raw and 1 column

my code : 

 javascript
sizeTraining=0.9
sizeTest=0.1
window=60
h=10

print('Loading data ...')
forex = FOREX()
(X_train, y_train),(X_test,y_test) = forex.load_data(sizeTraining, sizeTest, window, h)

Xtrain_mean = X_train.mean()
Xtrain_ecart_type = X_train.std()
X_train=(X_train-Xtrain_mean)/Xtrain_ecart_type

Xtest_mean = X_test.mean()
Xtest_ecart_type = X_test.std()
X_test=(X_test-Xtest_mean)/Xtest_ecart_type

ytrain_mean = y_train.mean()
ytrain_ecart_type = y_train.std()
y_train=(y_train-ytrain_mean)/ytrain_ecart_type

ytest_mean = y_test.mean()
ytest_ecart_type = y_test.std()
y_test=(y_test-ytest_mean)/ytest_ecart_type

print('x reshape')
X_train = np.reshape(X_train, (-1,window,1))
X_test = np.reshape(X_test, (-1,window,1))
print(X_train.shape)
print(X_train)

print('y reshapet')
y_train = np.reshape(y_train, (-1,1))
y_test = np.reshape(y_train, (-1,1))
print(y_train.shape)

model = Sequential()
#premiere couche de convolution
model.add(Convolution1D(nb_filter=60,filter_length=3,border_mode=""full"",activation=""relu"",subsample_length=1,input_dim=1,input_length=60))
conv1= Activation('relu')
model.add(conv1)
#premiere couche de subsampling
model.add(MaxPooling1D(pool_length=2))
model.add(Dropout(0.25))

model.add(Convolution1D(nb_filter=60,filter_length=3,border_mode=""full"",activation=""relu"",subsample_length=1))
conv1= Activation('relu')
model.add(conv1)
#premiere couche de subsampling
model.add(MaxPooling1D(pool_length=2))
model.add(Dropout(0.25))

#on applatit la sortie pour la presenter au MLP
model.add(Flatten())

#premiere couche du MLP avec activation de type maxoutdense
model.add(Dense(output_dim=500))
model.add(Dropout(0.5))

model.add(Dense(output_dim=500))
model.add(Dropout(0.5))

model.add(Dense(1))
model.add(Activation('linear'))

model.compile(loss='mse', optimizer='adadelta')

history = model.fit(X_train, y_train, batch_size=128, nb_epoch=2, show_accuracy=False, verbose=1, validation_data=(X_test, y_test))
score = model.evaluate(X_test, y_test, show_accuracy=False, verbose=0)
'''

this my problem : 

`
"
keras,1465,"I use Keras to run a forward pass with weights pre-trained on another network, but the forward pass is terribly slow. While it takes only 2~3 seconds on [Darknet](https://github.com/pjreddie/darknet), It takes me 27 seconds to run a forward pass. The result is as expected, so I don't know what I've done wrong here. Here is my model in keras:



It's a network similar to GoogleNet with 27 layers in total. I am using a 3GB Quadro K4000 
",1,My impletation of feedforward is too slow,"My impletation of feedforward is too slow I use Keras to run a forward pass with weights pre-trained on another network, but the forward pass is terribly slow. While it takes only 2~3 seconds on [Darknet](https://github.com/pjreddie/darknet), It takes me 27 seconds to run a forward pass. The result is as expected, so I don't know what I've done wrong here. Here is my model in keras:



It's a network similar to GoogleNet with 27 layers in total. I am using a 3GB Quadro K4000 
"
keras,4030,"I have a regression CNN model for object detection tasks.
When I use one output and one loss function, the result is very good.


Now I add another output  with **ZERO** loss weight on top of :



The performance is much worse. Note that I tried different loss weight values for , which all gave bad results.
My thought is that if loss weight of  is zero, it should has no contribution in the back-propagation updates. And because  does not alter the sub-network from  to , the learning shouldn't change either.
Any idea where I was doing wrong?
Thanks a lot!
",1,Performance issue with multiple loss functions,"Performance issue with multiple loss functions I have a regression CNN model for object detection tasks.
When I use one output and one loss function, the result is very good.


Now I add another output  with **ZERO** loss weight on top of :



The performance is much worse. Note that I tried different loss weight values for , which all gave bad results.
My thought is that if loss weight of  is zero, it should has no contribution in the back-propagation updates. And because  does not alter the sub-network from  to , the learning shouldn't change either.
Any idea where I was doing wrong?
Thanks a lot!
"
keras,8640,"I am trying to train my model on a GPU instead of a CPU on an AWS p2.xlarge instance from my Jupyter Notebook. I am using tensorflow-gpu backend (only  was installed and mentioned in  and not ).

I am not seeing any speed improvements when training models on these instances compared to using a CPU, infact I am getting training speeds per epoch that is almost same as I am getting on my 4-core  laptop CPU (p2.xlarge also has 4 vCPUs with a Tesla K80 GPU). I am not sure if i need to do some changes to my code to accommodate faster/parallel processing that GPU can offer. I am pasting below my code for my model:



Also interestingly the GPU seems to be utilizing between 50%-60% of its processing power and almost all of its memory every time I check for GPU status using  (but both fall to 0% and 1MiB respectively when not training):

Also if you's like to see my logs about using the GPU from Jupyter Notebook:

Please suggest what could be the problem. Thanks a ton for looking at this anyways!",1,No Improvements to training speed with GPU (partial GPU usage?!),"No Improvements to training speed with GPU (partial GPU usage?!) I am trying to train my model on a GPU instead of a CPU on an AWS p2.xlarge instance from my Jupyter Notebook. I am using tensorflow-gpu backend (only  was installed and mentioned in  and not ).

I am not seeing any speed improvements when training models on these instances compared to using a CPU, infact I am getting training speeds per epoch that is almost same as I am getting on my 4-core  laptop CPU (p2.xlarge also has 4 vCPUs with a Tesla K80 GPU). I am not sure if i need to do some changes to my code to accommodate faster/parallel processing that GPU can offer. I am pasting below my code for my model:



Also interestingly the GPU seems to be utilizing between 50%-60% of its processing power and almost all of its memory every time I check for GPU status using  (but both fall to 0% and 1MiB respectively when not training):

Also if you's like to see my logs about using the GPU from Jupyter Notebook:

Please suggest what could be the problem. Thanks a ton for looking at this anyways!"
keras,9672,"I have a similar problem with this Kaggle tutorial: https://www.kaggle.com/eliotbarr/text-mining-with-sklearn-keras-mlp-lstm-cnn, so I will refer to it.

If you look to the code block number 30 and 31:



and 



I suppose the accuracy scores should be the same, but in fact, they are different. How can it be possible?
One accuracy is calculated by model.evaluate and other one is calculated by accuracy_score (sklearn).",1,Different accuracy score between keras.model.evaluate and sklearn.accuracy_score,"Different accuracy score between keras.model.evaluate and sklearn.accuracy_score I have a similar problem with this Kaggle tutorial: https://www.kaggle.com/eliotbarr/text-mining-with-sklearn-keras-mlp-lstm-cnn, so I will refer to it.

If you look to the code block number 30 and 31:



and 



I suppose the accuracy scores should be the same, but in fact, they are different. How can it be possible?
One accuracy is calculated by model.evaluate and other one is calculated by accuracy_score (sklearn)."
keras,9674,"Hi! I am working on transferring NASNet Mobile weights from Keras to Pytorch.
After transferring the weights and making predictions of **the same image using both frameworks** I am getting absolutely **the same values** both in confidence and in any middle layers of the network.

However, the evaluation of the imagenet using transferred weights in Pytorch gave me
**Acc@1 38.110 Acc@5 60.844**

Also, I tested the script that I am using for evaluation on ImageNet and it works correct, as I got correct values for other architectures.

I was wondering if you could double check the weight that you provide for NASNet Mobile, probably there is some issue with them! 

",1,NASNet Mobile gives low accuracy on ImageNet,"NASNet Mobile gives low accuracy on ImageNet Hi! I am working on transferring NASNet Mobile weights from Keras to Pytorch.
After transferring the weights and making predictions of **the same image using both frameworks** I am getting absolutely **the same values** both in confidence and in any middle layers of the network.

However, the evaluation of the imagenet using transferred weights in Pytorch gave me
**Acc@1 38.110 Acc@5 60.844**

Also, I tested the script that I am using for evaluation on ImageNet and it works correct, as I got correct values for other architectures.

I was wondering if you could double check the weight that you provide for NASNet Mobile, probably there is some issue with them! 

"
keras,13266,"I tested the example of ""keras/examples/lstm_seq2seq.py"" with ""accuracy"" metrics. As a result that the model didn't achieve high accuracy. I found that one-hot encoding after tokenizing wasn't applied for the back padding of sentences.",1,low accuracy with incorrect one-hot encoding in lstm_seq2seq.py example,"low accuracy with incorrect one-hot encoding in lstm_seq2seq.py example I tested the example of ""keras/examples/lstm_seq2seq.py"" with ""accuracy"" metrics. As a result that the model didn't achieve high accuracy. I found that one-hot encoding after tokenizing wasn't applied for the back padding of sentences."
keras,4563,"I am training an LSTM model for multiple time-series regression. But, my losses are always either very high or Nan. I have tried several optimizers such as ,  and . Here's the script:


 is of the shape (N_SAMPLES_TRAIN, MAX_TIMESTEPS, MAX_FEATURES)
 is of the shape (N_SAMPLES)

I should add that the Y values I am trying to predict are very high. Any idea where I might be going wroing?",1,Very high / Nan loss when performing multiple sequence time series regression,"Very high / Nan loss when performing multiple sequence time series regression I am training an LSTM model for multiple time-series regression. But, my losses are always either very high or Nan. I have tried several optimizers such as ,  and . Here's the script:


 is of the shape (N_SAMPLES_TRAIN, MAX_TIMESTEPS, MAX_FEATURES)
 is of the shape (N_SAMPLES)

I should add that the Y values I am trying to predict are very high. Any idea where I might be going wroing?"
keras,10706,"This is a reproducible example: 

consider this 



Both RMSE and MAE are the same. This happens when i placed them together. Is there a bug?

the output is:

![image](https://user-images.githubusercontent.com/22788747/42806109-39fc16be-89e0-11e8-80d8-5e73ba8b2541.png)
",1,custom metric MAE and RMSE are the same,"custom metric MAE and RMSE are the same This is a reproducible example: 

consider this 



Both RMSE and MAE are the same. This happens when i placed them together. Is there a bug?

the output is:

![image](https://user-images.githubusercontent.com/22788747/42806109-39fc16be-89e0-11e8-80d8-5e73ba8b2541.png)
"
keras,6108,"image_ocr example on kears was 10 times faster then on keras2
Why that ?
What has changed ?",1,image_ocr example performance,"image_ocr example performance image_ocr example on kears was 10 times faster then on keras2
Why that ?
What has changed ?"
keras,6625,"I am very new to Keras. That said, I am trying to avoid using Scikit's pipeline code since it is relatively slower than using the keras network.



How can you achieve the same result using Keras's implementation? Can I achieve the same using NormalizationLayer? I could not figure out how but I tried the following:



I am using Keras 2.0.4 with Tensorflow 0.12.1.

",1,Replace ScikitLearn's pipelines with Keras' layers,"Replace ScikitLearn's pipelines with Keras' layers I am very new to Keras. That said, I am trying to avoid using Scikit's pipeline code since it is relatively slower than using the keras network.



How can you achieve the same result using Keras's implementation? Can I achieve the same using NormalizationLayer? I could not figure out how but I tried the following:



I am using Keras 2.0.4 with Tensorflow 0.12.1.

"
keras,12770,"Please make sure that the boxes below are checked before you submit your issue. If your issue is an implementation question, please ask your question on [StackOverflow](http://stackoverflow.com/questions/tagged/keras) or [join the Keras Slack channel](https://keras-slack-autojoin.herokuapp.com/) and ask there instead of filing a GitHub issue.

Thank you!

- [x] Check that you are up-to-date with the master branch of Keras. You can update with:
pip install git+git://github.com/keras-team/keras.git --upgrade --no-deps

- [x] If running on TensorFlow, check that you are up-to-date with the latest version. The installation instructions can be found [here](https://www.tensorflow.org/get_started/os_setup).

- [ ] If running on Theano, check that you are up-to-date with the master branch of Theano. You can update with:
pip install git+git://github.com/Theano/Theano.git --upgrade --no-deps

- [x] Provide a link to a GitHub Gist of a Python script that can reproduce your issue (or just copy the script here if it is short).

###################################
###################################

So I am building a Siamese Network using VGG19 as my base network. Everything went fine throughout training and testing. However, I noticed that during training, my accuracy is quite high (only took 2 epochs to reach 100% accuracy, which was amazing), but when testing, the accuracy is around 0% (sometimes even lower, as shown below). The code is in  and  located in my repo [here](https://github.com/ayaz-amin/AyazNet). 

When I run , this is the output:


When I run , this is the output:

I was expecting the output to be showing ones (0 means no similarity, 1 meaning full similarity). I am pretty sure their is nothing wrong with this code. Am I doing anything wrong? Or is it a bug? Also, I am using tf.keras from Tensorflow 2.0.
",1,Siamese Network Performance Issues,"Siamese Network Performance Issues Please make sure that the boxes below are checked before you submit your issue. If your issue is an implementation question, please ask your question on [StackOverflow](http://stackoverflow.com/questions/tagged/keras) or [join the Keras Slack channel](https://keras-slack-autojoin.herokuapp.com/) and ask there instead of filing a GitHub issue.

Thank you!

- [x] Check that you are up-to-date with the master branch of Keras. You can update with:
pip install git+git://github.com/keras-team/keras.git --upgrade --no-deps

- [x] If running on TensorFlow, check that you are up-to-date with the latest version. The installation instructions can be found [here](https://www.tensorflow.org/get_started/os_setup).

- [ ] If running on Theano, check that you are up-to-date with the master branch of Theano. You can update with:
pip install git+git://github.com/Theano/Theano.git --upgrade --no-deps

- [x] Provide a link to a GitHub Gist of a Python script that can reproduce your issue (or just copy the script here if it is short).

###################################
###################################

So I am building a Siamese Network using VGG19 as my base network. Everything went fine throughout training and testing. However, I noticed that during training, my accuracy is quite high (only took 2 epochs to reach 100% accuracy, which was amazing), but when testing, the accuracy is around 0% (sometimes even lower, as shown below). The code is in  and  located in my repo [here](https://github.com/ayaz-amin/AyazNet). 

When I run , this is the output:


When I run , this is the output:

I was expecting the output to be showing ones (0 means no similarity, 1 meaning full similarity). I am pretty sure their is nothing wrong with this code. Am I doing anything wrong? Or is it a bug? Also, I am using tf.keras from Tensorflow 2.0.
"
keras,4580,"Hi all,

I am running a model which contains some GRU modules in an RNN with keras. While training, the CPU spikes between ~400% to ~950%, when every spike ends after several seconds and returns to 99% CPU usage baseline. Ideally I would like the CPU to stay steady in > 700%, as the spiking translates to inefficient runtime. My .theanorc file:



Any suggestions?",1,CPU usage spikes when using keras,"CPU usage spikes when using keras Hi all,

I am running a model which contains some GRU modules in an RNN with keras. While training, the CPU spikes between ~400% to ~950%, when every spike ends after several seconds and returns to 99% CPU usage baseline. Ideally I would like the CPU to stay steady in > 700%, as the spiking translates to inefficient runtime. My .theanorc file:



Any suggestions?"
keras,11236,"Hi,

I'm facing an increase in validation loss when using multi_gpu_model to train a CNN. I kept the all other parameters constant (batchsize, learning rate).

This is how I implemented it:

 I created the single-gpu model:
  
      if num_gpus > 1:
        import tensorflow as tf
        with tf.device('/cpu:0'):
            model = create_model()

Then I converted it into a parallel model:

    if num_gpus > 1:
        parallel_model = multi_gpu_model(model, gpus=num_gpus)

Then I compile it and run the training:

    parallel_model.compile(...)
    parallel_model.fit_generator(...)

Any ideas what might be the source of error?

I'm using the latest Keras (current master) with the latest tensorflow (1.10.1).

Best,
Thorsten
",1,Higher validation loss (but same training loss) when using multi_gpu_model,"Higher validation loss (but same training loss) when using multi_gpu_model Hi,

I'm facing an increase in validation loss when using multi_gpu_model to train a CNN. I kept the all other parameters constant (batchsize, learning rate).

This is how I implemented it:

 I created the single-gpu model:
  
      if num_gpus > 1:
        import tensorflow as tf
        with tf.device('/cpu:0'):
            model = create_model()

Then I converted it into a parallel model:

    if num_gpus > 1:
        parallel_model = multi_gpu_model(model, gpus=num_gpus)

Then I compile it and run the training:

    parallel_model.compile(...)
    parallel_model.fit_generator(...)

Any ideas what might be the source of error?

I'm using the latest Keras (current master) with the latest tensorflow (1.10.1).

Best,
Thorsten
"
keras,10214,"Hello everyone!

I'm using Keras 2.1.6 and TF 1.8. I want to fine-tune the pre-trained InceptionV3 on a new set of classes as shown here: https://keras.io/applications/

My code is literally copy-paste from the documentation. While doing that, I noticed that the accuracy on my training dataset is very high while the one on the validation set is very low. Immediately I suspected over-fitting but after wasting several hours I could not find anything. Then I tried to fit/evaluate on the same dataset. To my surprise the accuracy reported by fit() was 87.2% while the one reported by evaluate() was 62.9% on the SAME dataset.

How is this possible? Is there a bug or something? I tried also Keras 2.1.5 and 2.1.4 but I get the same problem. Am I doing something wrong? I following the documentation.",1,Extremely low accuracy after finetuning InceptionV3 (not overfitting),"Extremely low accuracy after finetuning InceptionV3 (not overfitting) Hello everyone!

I'm using Keras 2.1.6 and TF 1.8. I want to fine-tune the pre-trained InceptionV3 on a new set of classes as shown here: https://keras.io/applications/

My code is literally copy-paste from the documentation. While doing that, I noticed that the accuracy on my training dataset is very high while the one on the validation set is very low. Immediately I suspected over-fitting but after wasting several hours I could not find anything. Then I tried to fit/evaluate on the same dataset. To my surprise the accuracy reported by fit() was 87.2% while the one reported by evaluate() was 62.9% on the SAME dataset.

How is this possible? Is there a bug or something? I tried also Keras 2.1.5 and 2.1.4 but I get the same problem. Am I doing something wrong? I following the documentation."
keras,3562,"I suppose I'm missing something obvious but the progress bar reports a much higher training loss than validation loss when the data is the same. Why is that? Shouldn't they be pretty much the same?

With split data (80/20, for example) everything looks fine, with a slightly worse validation loss than training loss, as expected.
",1,"If training data and validation data is the same, how come the losses differ?","If training data and validation data is the same, how come the losses differ? I suppose I'm missing something obvious but the progress bar reports a much higher training loss than validation loss when the data is the same. Why is that? Shouldn't they be pretty much the same?

With split data (80/20, for example) everything looks fine, with a slightly worse validation loss than training loss, as expected.
"
keras,3563,"I  copied the code o mnist_cnn.py exactly except that modified the print lines to be compatible with Python 2.7. The np.random.seed(1337) was kept as well for reproducibility. However, I can only get 98.78%. Any clues why I couldn't reach 99.25% test accuracy? 
",1,mnist_cnn.py: couldn't reach 99.25% test accuracy,"mnist_cnn.py: couldn't reach 99.25% test accuracy I  copied the code o mnist_cnn.py exactly except that modified the print lines to be compatible with Python 2.7. The np.random.seed(1337) was kept as well for reproducibility. However, I can only get 98.78%. Any clues why I couldn't reach 99.25% test accuracy? 
"
keras,6636,"Snippet from the Keras MNIST (LeNet) example ([source](https://github.com/fchollet/keras/blob/master/examples/mnist_cnn.py#L57)) :

Implementation of  uses  ([source](https://github.com/fchollet/keras/blob/master/keras/backend/tensorflow_backend.py#L2750)):

But the Tensorflow doc on  ([source](https://www.tensorflow.org/api_docs/python/tf/nn/softmax_cross_entropy_with_logits)) states:
> **WARNING**: This op expects unscaled logits, since it performs a softmax on logits internally for efficiency. Do not call this op with the output of softmax, as it will produce incorrect results.

Am I missing something or are we violating the above warning due to  calling the softmax activation?",1,MNIST_CNN example uses double softmax?,"MNIST_CNN example uses double softmax? Snippet from the Keras MNIST (LeNet) example ([source](https://github.com/fchollet/keras/blob/master/examples/mnist_cnn.py#L57)) :

Implementation of  uses  ([source](https://github.com/fchollet/keras/blob/master/keras/backend/tensorflow_backend.py#L2750)):

But the Tensorflow doc on  ([source](https://www.tensorflow.org/api_docs/python/tf/nn/softmax_cross_entropy_with_logits)) states:
> **WARNING**: This op expects unscaled logits, since it performs a softmax on logits internally for efficiency. Do not call this op with the output of softmax, as it will produce incorrect results.

Am I missing something or are we violating the above warning due to  calling the softmax activation?"
keras,13293,"I imported mobilenetv2 but the accuracy is incredibly low (close to 0% correct)

My code is as below

from keras.applications.mobilenet_v2 import MobileNetV2
model1 = MobileNetV2(weights='imagenet')

import cv2
x = cv2.imread('image_input.jpg')
x = cv2.resize(x,(224,224))
y = model1.predict(x[np.newaxis,:,:,:])


Wheras if I import VGG (or resnet)
from keras.applications.vgg16 import VGG16
model2 = MobileNetV2(weights='imagenet')
y = model2.predict(x[np.newaxis,:,:,:])


VGG has a much much better accuracy

Has keras trained mobilenetv2 properly.",1,mobilenetv2 poor accuracy,"mobilenetv2 poor accuracy I imported mobilenetv2 but the accuracy is incredibly low (close to 0% correct)

My code is as below

from keras.applications.mobilenet_v2 import MobileNetV2
model1 = MobileNetV2(weights='imagenet')

import cv2
x = cv2.imread('image_input.jpg')
x = cv2.resize(x,(224,224))
y = model1.predict(x[np.newaxis,:,:,:])


Wheras if I import VGG (or resnet)
from keras.applications.vgg16 import VGG16
model2 = MobileNetV2(weights='imagenet')
y = model2.predict(x[np.newaxis,:,:,:])


VGG has a much much better accuracy

Has keras trained mobilenetv2 properly."
keras,1006,"I've been using keras to build convolution neural networks for binary classification. Using the same input data, I've tried to vary the model structure (i.e. filter size, number of filters, number of hidden layer neurons) for better performance. I noticed that for certain models, the training accuracy remains unchanged at a low value through all 50 training epochs. One of the model structure is as follows:



I used the whole training set as validation so I can get the training accuracy at the end of each epoch. The output after each epoch looks like:



I plotted the filter weights of the convolution layer at each epoch and they stayed the same for most of the epochs and had minor changes rarely.
at the beginning of epoch 1:
![epoch1_batch1_layer0_weight_img](https://cloud.githubusercontent.com/assets/7233438/11153096/32e26b80-89fc-11e5-8baa-71a2c2b2ef53.png)
at the beginning of epoch 3:
![epoch3_batch1_layer0_weight_img](https://cloud.githubusercontent.com/assets/7233438/11153097/32ee8bf4-89fc-11e5-8252-6bedd64d4888.png)
at the beginning of epoch 5:
![epoch5_batch1_layer0_weight_img](https://cloud.githubusercontent.com/assets/7233438/11153095/32e1ff56-89fc-11e5-9f39-e5e471a69538.png)

I also tried rmsprop as optimizer, and the accuracy at each epoch was exactly the same 0.4473 . However the filter weights formed different pattern at the end of epoch 1 but remained unchanged after that.

What may I have missed? I tried this model structure directly with Theano and the problem persisted, so I guess this problem was not from keras.

To identify the cause of this problem, I think it would be helpful to track the activation and gradient values of each mini-batch. How do I do these with Keras or Theano? Or what would be the right direction to look into?

Thanks a lot.
Cheng
",1,accuracy stop improving in simple convolution network,"accuracy stop improving in simple convolution network I've been using keras to build convolution neural networks for binary classification. Using the same input data, I've tried to vary the model structure (i.e. filter size, number of filters, number of hidden layer neurons) for better performance. I noticed that for certain models, the training accuracy remains unchanged at a low value through all 50 training epochs. One of the model structure is as follows:



I used the whole training set as validation so I can get the training accuracy at the end of each epoch. The output after each epoch looks like:



I plotted the filter weights of the convolution layer at each epoch and they stayed the same for most of the epochs and had minor changes rarely.
at the beginning of epoch 1:
![epoch1_batch1_layer0_weight_img](https://cloud.githubusercontent.com/assets/7233438/11153096/32e26b80-89fc-11e5-8baa-71a2c2b2ef53.png)
at the beginning of epoch 3:
![epoch3_batch1_layer0_weight_img](https://cloud.githubusercontent.com/assets/7233438/11153097/32ee8bf4-89fc-11e5-8252-6bedd64d4888.png)
at the beginning of epoch 5:
![epoch5_batch1_layer0_weight_img](https://cloud.githubusercontent.com/assets/7233438/11153095/32e1ff56-89fc-11e5-9f39-e5e471a69538.png)

I also tried rmsprop as optimizer, and the accuracy at each epoch was exactly the same 0.4473 . However the filter weights formed different pattern at the end of epoch 1 but remained unchanged after that.

What may I have missed? I tried this model structure directly with Theano and the problem persisted, so I guess this problem was not from keras.

To identify the cause of this problem, I think it would be helpful to track the activation and gradient values of each mini-batch. How do I do these with Keras or Theano? Or what would be the right direction to look into?

Thanks a lot.
Cheng
"
keras,3572,"I am trying to test the fit generator but found the speed is rather slow to around less 100 examples processing per second, I am wondering anything wrong in my code?

 

the train.txt file format:
785 floats separate by comma, with the last as label.
",1,fit generator slow ,"fit generator slow  I am trying to test the fit generator but found the speed is rather slow to around less 100 examples processing per second, I am wondering anything wrong in my code?

 

the train.txt file format:
785 floats separate by comma, with the last as label.
"
keras,3576,"I am training a simple neural network in Keras with Theano backend consisting of 4 dense layers connected to a Merge layer and then to a softmax classifier layer. Using Adam for training, the first few epochs train in about 60s each (in the CPU) but, after that, the training time per epoch starts increasing, taking more than 400s by epoch 70, making it unusable.

Is there anything wrong with my code? Is this supposed to happen or is it a bug?

This only happens when using Adam, not with sgd, adadelta, rmsprop or adagrad.  I'd use any of the other methods but Adam produces far better results.

The code:


",1,Training with Adam gets slower each epoch,"Training with Adam gets slower each epoch I am training a simple neural network in Keras with Theano backend consisting of 4 dense layers connected to a Merge layer and then to a softmax classifier layer. Using Adam for training, the first few epochs train in about 60s each (in the CPU) but, after that, the training time per epoch starts increasing, taking more than 400s by epoch 70, making it unusable.

Is there anything wrong with my code? Is this supposed to happen or is it a bug?

This only happens when using Adam, not with sgd, adadelta, rmsprop or adagrad.  I'd use any of the other methods but Adam produces far better results.

The code:


"
keras,3578,"Hi,
  I am training a 2 hidden layer NN and notice that the convergence is faster when using model.fit than using model.fit_generator. The optimizer is the same in both cases ('sgd') and my batch size is 32  in both cases. val_loss decreases much rapidly when using fit. Is this to be expected? Is val_loss calculated differently for fit_generator?

Thanks in advance
",1,Fit generator slow to converge than model.fit,"Fit generator slow to converge than model.fit Hi,
  I am training a 2 hidden layer NN and notice that the convergence is faster when using model.fit than using model.fit_generator. The optimizer is the same in both cases ('sgd') and my batch size is 32  in both cases. val_loss decreases much rapidly when using fit. Is this to be expected? Is val_loss calculated differently for fit_generator?

Thanks in advance
"
keras,13311,"   Hi everyone, I tried to train a dnn model with Keras, but the acc and val_acc I got were very low, could someone give me some advice about how to solve it? Thank you very much in advance! Below is my code.
 
import keras
import numpy as np
import pandas as pd
from keras.models import Sequential
from keras.models import Model
from keras.layers import Dense, Dropout, Activation, Input
from keras import optimizers, losses
from keras.layers.normalization import BatchNormalization
from keras.callbacks import EarlyStopping
from keras.utils import np_utils, generic_utils
from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import StandardScaler

def load_data_train(path, train=True):
    df = pd.read_csv(path)
    x = df.values.copy()
    if train:
        np.random.shuffle(x) 
        x, labels = x[:, 1:-1].astype(np.float32), x[:, -1]
        return x, labels
    else:
        x, ids = x[:, 1:].astype(np.float32), x[:, 0].astype(str)
        return x, ids

def load_data_test(path):
    df = pd.read_csv(path)
    x = df.values.copy()
    x, labels = x[:, 1:-1].astype(np.float32), x[:, -1]
    return x, labels

def preprocess_data(x, scaler=None):
    if not scaler:
        scaler = StandardScaler()
        scaler.fit(x)
    x = scaler.transform(x)   # another type x = StanderScaler().fit_transferom(x)
    return x, scaler

def preprocess_labels(labels, encoder=None, categorical=True):
    if not encoder:
        encoder = LabelEncoder()
        encoder.fit(labels)
    y = encoder.transform(labels).astype(np.int32)
    if categorical:
        y = np_utils.to_categorical(y)
    return y, encoder

print(""Loading data..."")
x_train, labels = load_data_train('train.csv', train=True)
x_train, scaler = preprocess_data(x_train)
y_train, encoder = preprocess_labels(labels)

x_test, labels = load_data_test('dev.csv')
x_test, scaler = preprocess_data(x_test)
y_test, encoder = preprocess_labels(labels)
nb_classes = y_train.shape[1]
print(nb_classes, 'classes')
dims = x_train.shape[1]
print(dims, 'dims')

print(""Building model..."")
model = Sequential()
model.add(Dense(128, activation='relu', input_dim=dims)) 
model.add(BatchNormalization())
model.add(Dropout(0.2))
model.add(Dense(nb_classes, activation='softmax'))

early_stopping = EarlyStopping(monitor='val_loss', patience=30, mode='auto', verbose=2)
sgd = optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True) 
model.compile(optimizer=sgd, loss=losses.mean_squared_logarithmic_error, metrics=['accuracy'])
print(""Training Model... "")
hist = model.fit(x_train, y_train, epochs=200, batch_size=200, validation_split=0.1) 
score = model.evaluate(x_test, y_test, verbose=2)
classlabel = model.predict_classes(x_test)

print(""Generating submission..."")
print('Test score: ', score[0])
print('Test accuracy: ', score[1])
print(model.summary())


",1,Very low accuracy,"Very low accuracy    Hi everyone, I tried to train a dnn model with Keras, but the acc and val_acc I got were very low, could someone give me some advice about how to solve it? Thank you very much in advance! Below is my code.
 
import keras
import numpy as np
import pandas as pd
from keras.models import Sequential
from keras.models import Model
from keras.layers import Dense, Dropout, Activation, Input
from keras import optimizers, losses
from keras.layers.normalization import BatchNormalization
from keras.callbacks import EarlyStopping
from keras.utils import np_utils, generic_utils
from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import StandardScaler

def load_data_train(path, train=True):
    df = pd.read_csv(path)
    x = df.values.copy()
    if train:
        np.random.shuffle(x) 
        x, labels = x[:, 1:-1].astype(np.float32), x[:, -1]
        return x, labels
    else:
        x, ids = x[:, 1:].astype(np.float32), x[:, 0].astype(str)
        return x, ids

def load_data_test(path):
    df = pd.read_csv(path)
    x = df.values.copy()
    x, labels = x[:, 1:-1].astype(np.float32), x[:, -1]
    return x, labels

def preprocess_data(x, scaler=None):
    if not scaler:
        scaler = StandardScaler()
        scaler.fit(x)
    x = scaler.transform(x)   # another type x = StanderScaler().fit_transferom(x)
    return x, scaler

def preprocess_labels(labels, encoder=None, categorical=True):
    if not encoder:
        encoder = LabelEncoder()
        encoder.fit(labels)
    y = encoder.transform(labels).astype(np.int32)
    if categorical:
        y = np_utils.to_categorical(y)
    return y, encoder

print(""Loading data..."")
x_train, labels = load_data_train('train.csv', train=True)
x_train, scaler = preprocess_data(x_train)
y_train, encoder = preprocess_labels(labels)

x_test, labels = load_data_test('dev.csv')
x_test, scaler = preprocess_data(x_test)
y_test, encoder = preprocess_labels(labels)
nb_classes = y_train.shape[1]
print(nb_classes, 'classes')
dims = x_train.shape[1]
print(dims, 'dims')

print(""Building model..."")
model = Sequential()
model.add(Dense(128, activation='relu', input_dim=dims)) 
model.add(BatchNormalization())
model.add(Dropout(0.2))
model.add(Dense(nb_classes, activation='softmax'))

early_stopping = EarlyStopping(monitor='val_loss', patience=30, mode='auto', verbose=2)
sgd = optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True) 
model.compile(optimizer=sgd, loss=losses.mean_squared_logarithmic_error, metrics=['accuracy'])
print(""Training Model... "")
hist = model.fit(x_train, y_train, epochs=200, batch_size=200, validation_split=0.1) 
score = model.evaluate(x_test, y_test, verbose=2)
classlabel = model.predict_classes(x_test)

print(""Generating submission..."")
print('Test score: ', score[0])
print('Test accuracy: ', score[1])
print(model.summary())


"
keras,10232,,0,how to use multicore cpu to do prediction?,
keras,12148,"![image](https://user-images.githubusercontent.com/32533059/51807246-9ecf6280-22bf-11e9-817a-d312001698d2.png)



hello,i have some question  as show as picture ，i want using this ""https://github.com/yhenon/keras-frcnn""'s vgg model , but i want some change their model,but when i search more day ,i can't find out this question solustion 

they are some discuusion:
https://stats.stackexchange.com/questions/282282/how-is-spatial-dropout-in-2d-implemented

exactly what i need is Spatial Dropout2D on define ""pixel "",this mean i need to provide the 
[0,0,0,0,0,0,0]
[0,1,0,1,0,1,0]
[0,0,0,0,0,0,0]
[0,1,0,1,0,1,0]
[0,0,0,0,0,0,0]
[0,1,0,1,0,1,0]
[0,0,0,0,0,0,0] to like mask_drop 



",0,keras can do  sparse dense?  ,"keras can do  sparse dense?   ![image](https://user-images.githubusercontent.com/32533059/51807246-9ecf6280-22bf-11e9-817a-d312001698d2.png)



hello,i have some question  as show as picture ，i want using this ""https://github.com/yhenon/keras-frcnn""'s vgg model , but i want some change their model,but when i search more day ,i can't find out this question solustion 

they are some discuusion:
https://stats.stackexchange.com/questions/282282/how-is-spatial-dropout-in-2d-implemented

exactly what i need is Spatial Dropout2D on define ""pixel "",this mean i need to provide the 
[0,0,0,0,0,0,0]
[0,1,0,1,0,1,0]
[0,0,0,0,0,0,0]
[0,1,0,1,0,1,0]
[0,0,0,0,0,0,0]
[0,1,0,1,0,1,0]
[0,0,0,0,0,0,0] to like mask_drop 



"
keras,12715,"**System information**  
- Ubuntu 16.04)
- TensorFlow backend (yes / no):  yes
- TensorFlow version:  v1.10.0-0-g656e7a2b34 1.10.0
- Keras version:  2.2.4
- Python version:  3.5

**Describe the current behavior**  
I am training a model with shared weights using concatenate function and using a pretrained VGG16 network which has some non trainable layers as seen below




I save the model without any error, but when try to do  then the following error appears:



If I set all layers in VGG as trainable, then the error does not appear.

**Describe the expected behavior**  
How could I load the model?

**Code to reproduce the issue**  




**Other info**  
If I save  and then load it, the error does not appear. Error is only shown when trying to load the entire model. 

",0,Error when loading a model with shared layers and pretrained network,"Error when loading a model with shared layers and pretrained network **System information**  
- Ubuntu 16.04)
- TensorFlow backend (yes / no):  yes
- TensorFlow version:  v1.10.0-0-g656e7a2b34 1.10.0
- Keras version:  2.2.4
- Python version:  3.5

**Describe the current behavior**  
I am training a model with shared weights using concatenate function and using a pretrained VGG16 network which has some non trainable layers as seen below




I save the model without any error, but when try to do  then the following error appears:



If I set all layers in VGG as trainable, then the error does not appear.

**Describe the expected behavior**  
How could I load the model?

**Code to reproduce the issue**  




**Other info**  
If I save  and then load it, the error does not appear. Error is only shown when trying to load the entire model. 

"
keras,9591,"Hi All,
I have created CNN successfully, when i tried implement that model into the iOS project i got the following error

The size of the output layer 'chairConfidence' in the neural network does not match the number of classes in the classifier.

code is

Keras predict it properly, iOS app i got the error",0, The size of the output layer 'chairConfidence' in the neural network does not match the number of classes in the classifier.," The size of the output layer 'chairConfidence' in the neural network does not match the number of classes in the classifier. Hi All,
I have created CNN successfully, when i tried implement that model into the iOS project i got the following error

The size of the output layer 'chairConfidence' in the neural network does not match the number of classes in the classifier.

code is

Keras predict it properly, iOS app i got the error"
keras,11430,"I implemented some tf operations with lambda layer, but I found this layer has no weights in my model.
So how can I train this lambda layer and load its weights?
I just want to use lambda layer to transform tf tensor into Keras tensor",0,Why lambda layer has no weights?,"Why lambda layer has no weights? I implemented some tf operations with lambda layer, but I found this layer has no weights in my model.
So how can I train this lambda layer and load its weights?
I just want to use lambda layer to transform tf tensor into Keras tensor"
keras,11304,"from the source code, I find the constraints of weights in keras just apply after each gradient descent updating. could I apply it at weights directly?

for example, at Dense layer, kernel_ constraint=unit_norm, can I define x\*unit_norm(W) rather that x\*W?",0,how to apply constraints before construct graph?,"how to apply constraints before construct graph? from the source code, I find the constraints of weights in keras just apply after each gradient descent updating. could I apply it at weights directly?

for example, at Dense layer, kernel_ constraint=unit_norm, can I define x\*unit_norm(W) rather that x\*W?"
keras,11315,"hello,I have a question can we implement LSTM encoding and decoding of text data in Sequential model API  instead of functional API.

",0,keras sequential model,"keras sequential model hello,I have a question can we implement LSTM encoding and decoding of text data in Sequential model API  instead of functional API.

"
keras,12598,This is not supported for kernel weights but is for bias weights.  Propose to add capability for kernel weights to be consistant across the API.,0,kernel_initializer = 'zeros' doesnt work for Dense layers,kernel_initializer = 'zeros' doesnt work for Dense layers This is not supported for kernel weights but is for bias weights.  Propose to add capability for kernel weights to be consistant across the API.
keras,11867,"Dear,
Scenario Description 

Step #1:
I replaced the backbone in yoloV3 from Darknet53 to MobileNet. The MobileNet weights is loaded from the Imagenet, by using
.

Step #2:
I also want to load pre-trained weights for detector yolov3, so I download the .cfg and .weights files from the https://pjreddie.com/darknet/yolo/ where we can find the file links as shown below in the original page.
----------------------------------------------------------------------------------
YOLOv3-416 | COCO trainval | test-dev | 55.3 | 65.86 Bn | 35 | cfg | weights
----------------------------------------------------------------------------------
Obviously, the .weights and .cfg files for YOLOv3-416 include the weights for both yoloV3 detector, and its backbone, Darknet53.

Step #3 :
when we develop the new yolo body (yolo body = backbone + yolo Detector), by using, for example, 


where  ""weights_path"" is the place where we save the  .weights file in Step #2(Of course, the .cfg and .weights have been converted to .h5 file).

Now comes the issue: the new yolo body is composed of mobilenet + yoloV3, while the loaded pre-trained .h5 weights  is for Darknet53+yoloV3. How can we guarantee that we load the correct weights to the correct model, in this scenario, obviously, I just want to load the yoloV3 detector part to the new yolo body, and the Darknet53 weights in the .h5 file will be automatically dropped. 

How to guarantee this, in a formal way? Just by simply setting ""skip_mismatch=True"" ?",0,"How to guarantee that Keras model loading the pretrained weights correctly, by setting  skip_mismatch=True ?","How to guarantee that Keras model loading the pretrained weights correctly, by setting  skip_mismatch=True ? Dear,
Scenario Description 

Step #1:
I replaced the backbone in yoloV3 from Darknet53 to MobileNet. The MobileNet weights is loaded from the Imagenet, by using
.

Step #2:
I also want to load pre-trained weights for detector yolov3, so I download the .cfg and .weights files from the https://pjreddie.com/darknet/yolo/ where we can find the file links as shown below in the original page.
----------------------------------------------------------------------------------
YOLOv3-416 | COCO trainval | test-dev | 55.3 | 65.86 Bn | 35 | cfg | weights
----------------------------------------------------------------------------------
Obviously, the .weights and .cfg files for YOLOv3-416 include the weights for both yoloV3 detector, and its backbone, Darknet53.

Step #3 :
when we develop the new yolo body (yolo body = backbone + yolo Detector), by using, for example, 


where  ""weights_path"" is the place where we save the  .weights file in Step #2(Of course, the .cfg and .weights have been converted to .h5 file).

Now comes the issue: the new yolo body is composed of mobilenet + yoloV3, while the loaded pre-trained .h5 weights  is for Darknet53+yoloV3. How can we guarantee that we load the correct weights to the correct model, in this scenario, obviously, I just want to load the yoloV3 detector part to the new yolo body, and the Darknet53 weights in the .h5 file will be automatically dropped. 

How to guarantee this, in a formal way? Just by simply setting ""skip_mismatch=True"" ?"
keras,9877,"**Feature Request**

[This Keras blog][1] explains nicely, how a small dataset can be augmented by the following code:

    from keras.preprocessing.image import ImageDataGenerator
    
    datagen = ImageDataGenerator(
            rotation_range=40,
            width_shift_range=0.2,
            height_shift_range=0.2,
            rescale=1./255,
            shear_range=0.2,
            zoom_range=0.2,
            horizontal_flip=True,
            fill_mode='nearest')

I am sure the vanilla example introduced in this blog works well, for similarly simple scenarios. 

In a much more complicated scenario, I want to use the weights of models pretrained on the famous [COCO dataset for object detection][2], to transfer learn new classes, for which I have only a very limited amount of data (<=1000).

The labeling granularity in such datasets is not per image, but per objects inside the images. I.e., each image may contain one or more objects which are marked by polygonical bounding boxes and these bounding boxes are labeled according to the object names they contain. This complex labeling information is encoded in json format, like in the following example:

    {
	""info"": {
		""year"": 2018,
		""version"": null,
		""description"": ""Peaches"",
		""contributor"": ""ralph@r4robotics.com.au"",
		""url"": ""labelbox.io"",
		""date_created"": ""2018-04-07T10:08:51.409340+00:00""
	},
	""images"": [{
		""id"": ""cjfp6vz7xfwz20198ixce9la4"",
		""width"": 274,
		""height"": 184,
		""file_name"": ""https://firebasestorage.googleapis.com/v0/b/labelbox-193903.appspot.com/o/cjfp6hjghfuvd01147d130984%2F5a7fdf5d-201a-40d0-bfef-c36d6ed02212%2Fpeach8.jpg?alt=media&token=11337eaa-4ffd-4dfb-b3ec-9c4ee6bd2f17"",
		""license"": null,
		""flickr_url"": ""https://firebasestorage.googleapis.com/v0/b/labelbox-193903.appspot.com/o/cjfp6hjghfuvd01147d130984%2F5a7fdf5d-201a-40d0-bfef-c36d6ed02212%2Fpeach8.jpg?alt=media&token=11337eaa-4ffd-4dfb-b3ec-9c4ee6bd2f17"",
		""coco_url"": ""https://firebasestorage.googleapis.com/v0/b/labelbox-193903.appspot.com/o/cjfp6hjghfuvd01147d130984%2F5a7fdf5d-201a-40d0-bfef-c36d6ed02212%2Fpeach8.jpg?alt=media&token=11337eaa-4ffd-4dfb-b3ec-9c4ee6bd2f17"",
		""date_captured"": null
	}, {
		""id"": ""cjfp6wqfhfwyu0107il09db3p"",
		""width"": 275,
		""height"": 183,
		""file_name"": ""https://firebasestorage.googleapis.com/v0/b/labelbox-193903.appspot.com/o/cjfp6hjghfuvd01147d130984%2F5a7fdf5d-201a-40d0-bfef-c36d6ed02212%2Fpeach9.jpg?alt=media&token=39dd5e97-c411-43e9-9ba3-9f51a334c7c7"",
		""license"": null,
		""flickr_url"": ""https://firebasestorage.googleapis.com/v0/b/labelbox-193903.appspot.com/o/cjfp6hjghfuvd01147d130984%2F5a7fdf5d-201a-40d0-bfef-c36d6ed02212%2Fpeach9.jpg?alt=media&token=39dd5e97-c411-43e9-9ba3-9f51a334c7c7"",
		""coco_url"": ""https://firebasestorage.googleapis.com/v0/b/labelbox-193903.appspot.com/o/cjfp6hjghfuvd01147d130984%2F5a7fdf5d-201a-40d0-bfef-c36d6ed02212%2Fpeach9.jpg?alt=media&token=39dd5e97-c411-43e9-9ba3-9f51a334c7c7"",
		""date_captured"": null
	}],
	""annotations"": [ {
		""id"": 23,
		""image_id"": ""cjfp6vz7xfwz20198ixce9la4"",
		""category_id"": 1,
		""segmentation"": [
			[31.0, 72.0, 63.0, 84.0, 75.0, 105.0, 67.0, 134.0, 68.0, 158.0, 44.0, 174.0, 24.0, 178.0, 2.0, 172.0, 2.0, 82.0, 31.0, 72.0]
		],
		""area"": 6301.0,
		""bbox"": [2.0, 6.0, 73.0, 106.0],
		""iscrowd"": 0
	}, {
		""id"": 24,
		""image_id"": ""cjfp6vz7xfwz20198ixce9la4"",
		""category_id"": 1,
		""segmentation"": [
			[75.0, 103.0, 108.0, 76.0, 137.0, 74.0, 166.0, 89.0, 182.0, 104.0, 188.0, 145.0, 179.0, 171.0, 167.0, 183.0, 92.0, 183.0, 72.0, 158.0, 68.0, 134.0, 75.0, 103.0]
		],
		""area"": 10652.5,
		""bbox"": [68.0, 1.0, 120.0, 109.0],
		""iscrowd"": 0
	}, {
		""id"": 25,
		""image_id"": ""cjfp6vz7xfwz20198ixce9la4"",
		""category_id"": 1,
		""segmentation"": [
			[169.0, 92.0, 182.0, 66.0, 211.0, 53.0, 246.0, 66.0, 262.0, 80.0, 268.0, 95.0, 261.0, 129.0, 241.0, 145.0, 216.0, 153.0, 188.0, 143.0, 184.0, 105.0, 169.0, 92.0]
		],
		""area"": 6838.5,
		""bbox"": [169.0, 31.0, 99.0, 100.0],
		""iscrowd"": 0
	}, {
		""id"": 26,
		""image_id"": ""cjfp6wqfhfwyu0107il09db3p"",
		""category_id"": 1,
		""segmentation"": [
			[86.0, 54.0, 109.0, 56.0, 119.0, 73.0, 113.0, 92.0, 93.0, 101.0, 76.0, 92.0, 70.0, 77.0, 71.0, 63.0, 86.0, 54.0]
		],
		""area"": 1715.0,
		""bbox"": [70.0, 82.0, 49.0, 47.0],
		""iscrowd"": 0
	}, {
		""id"": 27,
		""image_id"": ""cjfp6wqfhfwyu0107il09db3p"",
		""category_id"": 1,
		""segmentation"": [
			[117.0, 95.0, 123.0, 110.0, 136.0, 118.0, 153.0, 113.0, 159.0, 99.0, 158.0, 87.0, 145.0, 79.0, 132.0, 76.0, 123.0, 84.0, 117.0, 95.0]
		],
		""area"": 1260.0,
		""bbox"": [117.0, 65.0, 42.0, 42.0],
		""iscrowd"": 0
	}, {
		""id"": 28,
		""image_id"": ""cjfp6wqfhfwyu0107il09db3p"",
		""category_id"": 1,
		""segmentation"": [
			[109.0, 54.0, 115.0, 40.0, 133.0, 32.0, 146.0, 34.0, 157.0, 43.0, 161.0, 58.0, 152.0, 72.0, 133.0, 76.0, 119.0, 71.0, 109.0, 54.0]
		],
		""area"": 1660.5,
		""bbox"": [109.0, 107.0, 52.0, 44.0],
		""iscrowd"": 0
	}],
	""licenses"": [],
	""categories"": [{
		""supercategory"": ""Peach"",
		""id"": 1,
		""name"": ""Peach""
	}]
}

Obviously, augmentation in this scenario is a bit more complicated, since  not only the images have to be distorted and rotated, but also the bounding boxes.   It would just make Keras so much more powerful and save users a lot of painstaking labeling work, if such a dataset could be augmented within a Keras pipeline.  And with the increasing relevance of Object Detection, this might be a feature to consider.

  [1]: https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html
  [2]: http://cocodataset.org/#home
",0,Data Augmentation for Object Detection with Fully Convolutional Networks ,"Data Augmentation for Object Detection with Fully Convolutional Networks  **Feature Request**

[This Keras blog][1] explains nicely, how a small dataset can be augmented by the following code:

    from keras.preprocessing.image import ImageDataGenerator
    
    datagen = ImageDataGenerator(
            rotation_range=40,
            width_shift_range=0.2,
            height_shift_range=0.2,
            rescale=1./255,
            shear_range=0.2,
            zoom_range=0.2,
            horizontal_flip=True,
            fill_mode='nearest')

I am sure the vanilla example introduced in this blog works well, for similarly simple scenarios. 

In a much more complicated scenario, I want to use the weights of models pretrained on the famous [COCO dataset for object detection][2], to transfer learn new classes, for which I have only a very limited amount of data (<=1000).

The labeling granularity in such datasets is not per image, but per objects inside the images. I.e., each image may contain one or more objects which are marked by polygonical bounding boxes and these bounding boxes are labeled according to the object names they contain. This complex labeling information is encoded in json format, like in the following example:

    {
	""info"": {
		""year"": 2018,
		""version"": null,
		""description"": ""Peaches"",
		""contributor"": ""ralph@r4robotics.com.au"",
		""url"": ""labelbox.io"",
		""date_created"": ""2018-04-07T10:08:51.409340+00:00""
	},
	""images"": [{
		""id"": ""cjfp6vz7xfwz20198ixce9la4"",
		""width"": 274,
		""height"": 184,
		""file_name"": ""https://firebasestorage.googleapis.com/v0/b/labelbox-193903.appspot.com/o/cjfp6hjghfuvd01147d130984%2F5a7fdf5d-201a-40d0-bfef-c36d6ed02212%2Fpeach8.jpg?alt=media&token=11337eaa-4ffd-4dfb-b3ec-9c4ee6bd2f17"",
		""license"": null,
		""flickr_url"": ""https://firebasestorage.googleapis.com/v0/b/labelbox-193903.appspot.com/o/cjfp6hjghfuvd01147d130984%2F5a7fdf5d-201a-40d0-bfef-c36d6ed02212%2Fpeach8.jpg?alt=media&token=11337eaa-4ffd-4dfb-b3ec-9c4ee6bd2f17"",
		""coco_url"": ""https://firebasestorage.googleapis.com/v0/b/labelbox-193903.appspot.com/o/cjfp6hjghfuvd01147d130984%2F5a7fdf5d-201a-40d0-bfef-c36d6ed02212%2Fpeach8.jpg?alt=media&token=11337eaa-4ffd-4dfb-b3ec-9c4ee6bd2f17"",
		""date_captured"": null
	}, {
		""id"": ""cjfp6wqfhfwyu0107il09db3p"",
		""width"": 275,
		""height"": 183,
		""file_name"": ""https://firebasestorage.googleapis.com/v0/b/labelbox-193903.appspot.com/o/cjfp6hjghfuvd01147d130984%2F5a7fdf5d-201a-40d0-bfef-c36d6ed02212%2Fpeach9.jpg?alt=media&token=39dd5e97-c411-43e9-9ba3-9f51a334c7c7"",
		""license"": null,
		""flickr_url"": ""https://firebasestorage.googleapis.com/v0/b/labelbox-193903.appspot.com/o/cjfp6hjghfuvd01147d130984%2F5a7fdf5d-201a-40d0-bfef-c36d6ed02212%2Fpeach9.jpg?alt=media&token=39dd5e97-c411-43e9-9ba3-9f51a334c7c7"",
		""coco_url"": ""https://firebasestorage.googleapis.com/v0/b/labelbox-193903.appspot.com/o/cjfp6hjghfuvd01147d130984%2F5a7fdf5d-201a-40d0-bfef-c36d6ed02212%2Fpeach9.jpg?alt=media&token=39dd5e97-c411-43e9-9ba3-9f51a334c7c7"",
		""date_captured"": null
	}],
	""annotations"": [ {
		""id"": 23,
		""image_id"": ""cjfp6vz7xfwz20198ixce9la4"",
		""category_id"": 1,
		""segmentation"": [
			[31.0, 72.0, 63.0, 84.0, 75.0, 105.0, 67.0, 134.0, 68.0, 158.0, 44.0, 174.0, 24.0, 178.0, 2.0, 172.0, 2.0, 82.0, 31.0, 72.0]
		],
		""area"": 6301.0,
		""bbox"": [2.0, 6.0, 73.0, 106.0],
		""iscrowd"": 0
	}, {
		""id"": 24,
		""image_id"": ""cjfp6vz7xfwz20198ixce9la4"",
		""category_id"": 1,
		""segmentation"": [
			[75.0, 103.0, 108.0, 76.0, 137.0, 74.0, 166.0, 89.0, 182.0, 104.0, 188.0, 145.0, 179.0, 171.0, 167.0, 183.0, 92.0, 183.0, 72.0, 158.0, 68.0, 134.0, 75.0, 103.0]
		],
		""area"": 10652.5,
		""bbox"": [68.0, 1.0, 120.0, 109.0],
		""iscrowd"": 0
	}, {
		""id"": 25,
		""image_id"": ""cjfp6vz7xfwz20198ixce9la4"",
		""category_id"": 1,
		""segmentation"": [
			[169.0, 92.0, 182.0, 66.0, 211.0, 53.0, 246.0, 66.0, 262.0, 80.0, 268.0, 95.0, 261.0, 129.0, 241.0, 145.0, 216.0, 153.0, 188.0, 143.0, 184.0, 105.0, 169.0, 92.0]
		],
		""area"": 6838.5,
		""bbox"": [169.0, 31.0, 99.0, 100.0],
		""iscrowd"": 0
	}, {
		""id"": 26,
		""image_id"": ""cjfp6wqfhfwyu0107il09db3p"",
		""category_id"": 1,
		""segmentation"": [
			[86.0, 54.0, 109.0, 56.0, 119.0, 73.0, 113.0, 92.0, 93.0, 101.0, 76.0, 92.0, 70.0, 77.0, 71.0, 63.0, 86.0, 54.0]
		],
		""area"": 1715.0,
		""bbox"": [70.0, 82.0, 49.0, 47.0],
		""iscrowd"": 0
	}, {
		""id"": 27,
		""image_id"": ""cjfp6wqfhfwyu0107il09db3p"",
		""category_id"": 1,
		""segmentation"": [
			[117.0, 95.0, 123.0, 110.0, 136.0, 118.0, 153.0, 113.0, 159.0, 99.0, 158.0, 87.0, 145.0, 79.0, 132.0, 76.0, 123.0, 84.0, 117.0, 95.0]
		],
		""area"": 1260.0,
		""bbox"": [117.0, 65.0, 42.0, 42.0],
		""iscrowd"": 0
	}, {
		""id"": 28,
		""image_id"": ""cjfp6wqfhfwyu0107il09db3p"",
		""category_id"": 1,
		""segmentation"": [
			[109.0, 54.0, 115.0, 40.0, 133.0, 32.0, 146.0, 34.0, 157.0, 43.0, 161.0, 58.0, 152.0, 72.0, 133.0, 76.0, 119.0, 71.0, 109.0, 54.0]
		],
		""area"": 1660.5,
		""bbox"": [109.0, 107.0, 52.0, 44.0],
		""iscrowd"": 0
	}],
	""licenses"": [],
	""categories"": [{
		""supercategory"": ""Peach"",
		""id"": 1,
		""name"": ""Peach""
	}]
}

Obviously, augmentation in this scenario is a bit more complicated, since  not only the images have to be distorted and rotated, but also the bounding boxes.   It would just make Keras so much more powerful and save users a lot of painstaking labeling work, if such a dataset could be augmented within a Keras pipeline.  And with the increasing relevance of Object Detection, this might be a feature to consider.

  [1]: https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html
  [2]: http://cocodataset.org/#home
"
keras,11779,"
Note created by @fchollet in the ""Requests for contributions"".
",0,Add masking support to convolution layers (or at least determine whether it is feasible).,"Add masking support to convolution layers (or at least determine whether it is feasible). 
Note created by @fchollet in the ""Requests for contributions"".
"
keras,11055,"![tim 20180901192728](https://user-images.githubusercontent.com/5326601/44947933-66fde500-ae1d-11e8-96fd-43ace664425f.png)
",0,"When using pycharm with remote run, the progress bar won't refresh in place.","When using pycharm with remote run, the progress bar won't refresh in place. ![tim 20180901192728](https://user-images.githubusercontent.com/5326601/44947933-66fde500-ae1d-11e8-96fd-43ace664425f.png)
"
keras,10612,"Please make sure that the boxes below are checked before you submit your issue. If your issue is an implementation question, please ask your question on [StackOverflow](http://stackoverflow.com/questions/tagged/keras) or [join the Keras Slack channel](https://keras-slack-autojoin.herokuapp.com/) and ask there instead of filing a GitHub issue.

Thank you!

- [x ] Check that you are up-to-date with the master branch of Keras. You can update with:
pip install git+git://github.com/keras-team/keras.git --upgrade --no-deps

- [x ] If running on TensorFlow, check that you are up-to-date with the latest version. The installation instructions can be found [here](https://www.tensorflow.org/get_started/os_setup).

- [x ] Provide a link to a GitHub Gist of a Python script that can reproduce your issue (or just copy the script here if it is short).

When attempting to use multiple sessions with , the kernel crashes without error.  Possibly an OOM problem, but I currently do not know.  The code below crashes around i=3 or i=4 for me, which is not very good.

OS: Windows 10
running on CPU only


",0,Keras crashes with multiple sessions,"Keras crashes with multiple sessions Please make sure that the boxes below are checked before you submit your issue. If your issue is an implementation question, please ask your question on [StackOverflow](http://stackoverflow.com/questions/tagged/keras) or [join the Keras Slack channel](https://keras-slack-autojoin.herokuapp.com/) and ask there instead of filing a GitHub issue.

Thank you!

- [x ] Check that you are up-to-date with the master branch of Keras. You can update with:
pip install git+git://github.com/keras-team/keras.git --upgrade --no-deps

- [x ] If running on TensorFlow, check that you are up-to-date with the latest version. The installation instructions can be found [here](https://www.tensorflow.org/get_started/os_setup).

- [x ] Provide a link to a GitHub Gist of a Python script that can reproduce your issue (or just copy the script here if it is short).

When attempting to use multiple sessions with , the kernel crashes without error.  Possibly an OOM problem, but I currently do not know.  The code below crashes around i=3 or i=4 for me, which is not very good.

OS: Windows 10
running on CPU only


"
keras,8657,,0,Introduction of global metrics (precision and recall),
keras,9186,"Hello, 
So I wanted to use  the  option when using . 
My y object was one hot encoded, so I did not know which keys I should use for 
the dict. 
Turns out that on , the mapping is determined using using the column index of the output:

Should we make this clearer in the documentation of the  function?
Thank you very much, 
Pierre",0,clearer doc for class_weights,"clearer doc for class_weights Hello, 
So I wanted to use  the  option when using . 
My y object was one hot encoded, so I did not know which keys I should use for 
the dict. 
Turns out that on , the mapping is determined using using the column index of the output:

Should we make this clearer in the documentation of the  function?
Thank you very much, 
Pierre"
keras,11371,"I was looking over the Keras documentation, when I noticed that the default for keras.callbacks.EarlyStopping is restore_best_weights=False.

In the documentation, the described behaviour is:
restore_best_weights: whether to restore model weights from the epoch with the best value of the monitored quantity. If False, the model weights obtained at the last step of training are used.

I find this a bit confusing because I think the principle of early stopping is to obtain the best results on the validation set by stopping the optimisation when the validation loss increases, which is the case without setting the patience parameter.

However, in early stopping it makes sense to set the patience parameter because you might get spurious overfittings due to momentary local optima. In that case, you would definitely want to restore the previous weights, because you might severely overfit after the end of your patience period.
Furthermore, I could not think of any reason why you would not want restore best weights in Early Stopping. The only case I can think of when you want to stop your training when there is no significant improvement on your loss, in order to reduce training time, but this is not the purpose of early stopping.
  
I'm wondering if there is any use case at all to make the False the default behaviour and if not, would not it make sense to change this?",0,Why weight restoring is not the default option in EarlyStopping?,"Why weight restoring is not the default option in EarlyStopping? I was looking over the Keras documentation, when I noticed that the default for keras.callbacks.EarlyStopping is restore_best_weights=False.

In the documentation, the described behaviour is:
restore_best_weights: whether to restore model weights from the epoch with the best value of the monitored quantity. If False, the model weights obtained at the last step of training are used.

I find this a bit confusing because I think the principle of early stopping is to obtain the best results on the validation set by stopping the optimisation when the validation loss increases, which is the case without setting the patience parameter.

However, in early stopping it makes sense to set the patience parameter because you might get spurious overfittings due to momentary local optima. In that case, you would definitely want to restore the previous weights, because you might severely overfit after the end of your patience period.
Furthermore, I could not think of any reason why you would not want restore best weights in Early Stopping. The only case I can think of when you want to stop your training when there is no significant improvement on your loss, in order to reduce training time, but this is not the purpose of early stopping.
  
I'm wondering if there is any use case at all to make the False the default behaviour and if not, would not it make sense to change this?"
keras,9284,"Why doesn't the Keras team have an ""official"" and ""large"" model zoo like Caffe2 [https://github.com/caffe2/caffe2/wiki/Model-Zoo]?

I see the following repositories on Github but they are indeed insufficient:

https://github.com/albertomontesg/keras-model-zoo
https://github.com/Yoctol/yoctol-keras-layer-zoo
https://github.com/david-vazquez/keras_zoo
https://github.com/GKalliatakis/Keras-Application-Zoo

What is the best way to get people to start sharing pretrained models in terms of the following:
1. opening up an issue
2. discussing this over the mailing list
3. having you send an email to the major contributors of keras or even the committers

Please let me know if you have any further questions or concerns.

Thanks!",0,Large Model Zoo for Keras along like Caffe2 but even better for the community of contributors,"Large Model Zoo for Keras along like Caffe2 but even better for the community of contributors Why doesn't the Keras team have an ""official"" and ""large"" model zoo like Caffe2 [https://github.com/caffe2/caffe2/wiki/Model-Zoo]?

I see the following repositories on Github but they are indeed insufficient:

https://github.com/albertomontesg/keras-model-zoo
https://github.com/Yoctol/yoctol-keras-layer-zoo
https://github.com/david-vazquez/keras_zoo
https://github.com/GKalliatakis/Keras-Application-Zoo

What is the best way to get people to start sharing pretrained models in terms of the following:
1. opening up an issue
2. discussing this over the mailing list
3. having you send an email to the major contributors of keras or even the committers

Please let me know if you have any further questions or concerns.

Thanks!"
keras,11194,"Please give a warning when using default  parameter in .

I just spent an hour debugging a magic number '32' in the model, which did not occur anywhere in my code.  
After successfully training, the following failed on the *same* input sample:  
 

finally found the culprit:  
https://keras.io/models/model/#predict  
batch_size: Integer. If unspecified, it will default to 32.


 should either use the shape of the sample as batch_size, or the same one as the model in training or use '1' as default (usually we just want to predict one item anyways), but don't fallback to an arbitrary default value without a warning please.
",0,batch_size dangerous default,"batch_size dangerous default Please give a warning when using default  parameter in .

I just spent an hour debugging a magic number '32' in the model, which did not occur anywhere in my code.  
After successfully training, the following failed on the *same* input sample:  
 

finally found the culprit:  
https://keras.io/models/model/#predict  
batch_size: Integer. If unspecified, it will default to 32.


 should either use the shape of the sample as batch_size, or the same one as the model in training or use '1' as default (usually we just want to predict one item anyways), but don't fallback to an arbitrary default value without a warning please.
"
keras,10366,In https://keras.io/preprocessing/image/#imagedatagenerator-class brightness_range is not documented.,0,Brightness range not documented,Brightness range not documented In https://keras.io/preprocessing/image/#imagedatagenerator-class brightness_range is not documented.
keras,12879,"I'm currently working on a regression problem where I need to preprocess the raw input data in a certain way (what's begin done is not relevant) before I can feed it to my neural network. I'm aware that preprocessing is usually done outside the model, but as I also need the gradients of the output of the network wrt. the raw input data (used in the loss function), I need the preprocessing to be included in the computational graph.

From what I've gathered, I need to feed the resulting tensor as input to the neural network in order for the two parts to be connected in the graph. An important note is that the preprocessing is done on the entire training set, meaning it only needs to be done once.

I've successfully implemented this using Keras backend functions, but the problem is that the preprocessing part seems to be performed at every forward pass of the model, which is both really unnecessary, and really slow. My hope was that the input tensor would be treated as some static (but differentiable) input to the network, with the forward-backward propagation simply flowing between the network input and output during training, but it doesn't seem to be working that way.

As my code is quite large and intricate, I'll leave a ""semi-pseudic"" code to simply show the gist of what I'm trying to do.



In the preprocessing function I have added a print-statement to the tensor that prints whenever the function is called, and it does indeed print at every epoch during training. This also happens even if I don't include the gradient in the loss function, showing that it most likely prints in the forward pass. The model does seem to train and predict as intended however, so the implementation seems to be correct.

There might be some flaws in my logic/understanding of how the graph works, and I'm hoping there is a simple fix to my problem.

To summarize:

1.  How do I feed a ""constant"", differentiable tensor as input to a neural network, without the tensor being calculated anew during each forward pass through the network?

As the preprocessed tensor contains all training samples, I would also like to split and train on smaller batches, so:

2.  Given there exists a solution to 1., how does batch-training tie into this? Do I have to create a generator and fit with fit_generator, or is it possible to let Keras handle it with fit?

Hope the problem was clear enough, and I'm grateful for all help/leads!
",0,Get gradient of preprocessing without forward passing each time ,"Get gradient of preprocessing without forward passing each time  I'm currently working on a regression problem where I need to preprocess the raw input data in a certain way (what's begin done is not relevant) before I can feed it to my neural network. I'm aware that preprocessing is usually done outside the model, but as I also need the gradients of the output of the network wrt. the raw input data (used in the loss function), I need the preprocessing to be included in the computational graph.

From what I've gathered, I need to feed the resulting tensor as input to the neural network in order for the two parts to be connected in the graph. An important note is that the preprocessing is done on the entire training set, meaning it only needs to be done once.

I've successfully implemented this using Keras backend functions, but the problem is that the preprocessing part seems to be performed at every forward pass of the model, which is both really unnecessary, and really slow. My hope was that the input tensor would be treated as some static (but differentiable) input to the network, with the forward-backward propagation simply flowing between the network input and output during training, but it doesn't seem to be working that way.

As my code is quite large and intricate, I'll leave a ""semi-pseudic"" code to simply show the gist of what I'm trying to do.



In the preprocessing function I have added a print-statement to the tensor that prints whenever the function is called, and it does indeed print at every epoch during training. This also happens even if I don't include the gradient in the loss function, showing that it most likely prints in the forward pass. The model does seem to train and predict as intended however, so the implementation seems to be correct.

There might be some flaws in my logic/understanding of how the graph works, and I'm hoping there is a simple fix to my problem.

To summarize:

1.  How do I feed a ""constant"", differentiable tensor as input to a neural network, without the tensor being calculated anew during each forward pass through the network?

As the preprocessed tensor contains all training samples, I would also like to split and train on smaller batches, so:

2.  Given there exists a solution to 1., how does batch-training tie into this? Do I have to create a generator and fit with fit_generator, or is it possible to let Keras handle it with fit?

Hope the problem was clear enough, and I'm grateful for all help/leads!
"
keras,11579,"I am new to the keras world and i am trying to build a feedfoward net which output should be a vector of parameters to be used in a linear ensembler, i want to minimize the MSE for the ensembler, then, i created the following loss function,



g_data is my matrix for single estimators predictions, which multiplyed to theta (net output) gives me the real predictions. Next, i create the model as follows,



Of course it does not run, cause y_train is a single output meanwhile i want to train a vector of outputs, as well, i would have to pass the sample index for each batch in order to use the proper rows of g_data in the loss function calculation.

Any ideas on how to working around this?",0,Custom output layer / loss function,"Custom output layer / loss function I am new to the keras world and i am trying to build a feedfoward net which output should be a vector of parameters to be used in a linear ensembler, i want to minimize the MSE for the ensembler, then, i created the following loss function,



g_data is my matrix for single estimators predictions, which multiplyed to theta (net output) gives me the real predictions. Next, i create the model as follows,



Of course it does not run, cause y_train is a single output meanwhile i want to train a vector of outputs, as well, i would have to pass the sample index for each batch in order to use the proper rows of g_data in the loss function calculation.

Any ideas on how to working around this?"
keras,3510,"In the mnist siamese example, two mnist images are mapped onto a vector. The two vectors are merged using euclidian distance. The distance should be low if the images belong to the same number and be high if the two images represent different images.

Questions: In line 59, the labels are set for positive and negative examples. If the euclidian distance for the same numbers should be low, why is the target 1 and why is it 0 for the negative examples? Also, the vectors are created using relu which normally does not have a max_value. How is the euclidian distance kept between 0 and 1?
",0,Question about mnist siamese example,"Question about mnist siamese example In the mnist siamese example, two mnist images are mapped onto a vector. The two vectors are merged using euclidian distance. The distance should be low if the images belong to the same number and be high if the two images represent different images.

Questions: In line 59, the labels are set for positive and negative examples. If the euclidian distance for the same numbers should be low, why is the target 1 and why is it 0 for the negative examples? Also, the vectors are created using relu which normally does not have a max_value. How is the euclidian distance kept between 0 and 1?
"
keras,7488,"Documentation of an Embedding Layer is bit not clear...

Does embedding layer performs word2vec or something different?
If it performs word2vec, I would suggest to mention it in [documentation](https://keras.io/layers/embeddings/))

And to add some relevant reference.
In opinion current reference  [dropout in RNN](https://arxiv.org/abs/1512.05287) is not directly connected to   word2vec.


keras/keras/layers/embeddings.py
https://github.com/fchollet/keras/blob/master/keras/layers/embeddings.py#L11",0,Embedding layer (Documentation),"Embedding layer (Documentation) Documentation of an Embedding Layer is bit not clear...

Does embedding layer performs word2vec or something different?
If it performs word2vec, I would suggest to mention it in [documentation](https://keras.io/layers/embeddings/))

And to add some relevant reference.
In opinion current reference  [dropout in RNN](https://arxiv.org/abs/1512.05287) is not directly connected to   word2vec.


keras/keras/layers/embeddings.py
https://github.com/fchollet/keras/blob/master/keras/layers/embeddings.py#L11"
keras,53,"Hey guys, cool project. The theano interface itself was really horrific and off-putting. 

Maybe I'm doing it wrong but is there any way to access the activations of different layers? Similar to predict but only computed half way. Would be really useful for analysis and the likes. 
",0,Accessing internal states,"Accessing internal states Hey guys, cool project. The theano interface itself was really horrific and off-putting. 

Maybe I'm doing it wrong but is there any way to access the activations of different layers? Similar to predict but only computed half way. Would be really useful for analysis and the likes. 
"
keras,2398,"Hi,

If we implement fully convolutional net in Keras, we want it to be able to work with input of arbitrary size.
In our case of 1D data - with arbitrary number of timesteps.

We can use None as an instruction that input could be of any size on the 1st dimension, e.g.:


But in the last layer(s) of  fully CN model we have to use actual length of the sequence as the filter_length in order to obtain sequence of labels correctly distributed along original timespan, e.g.:



As soon as we don't know actual sequence length before runtime, we have the following options:
1. Omit this parameter - but it's mandatory and it will not work
2. Use None value for the filter_length in order to tell Keras that this value will be available on runtime only (see input shape above) - it will cause an error (see output below)
3. Use some kind of placeholder value during network buildup and hope that it will be magically replaced by the real sequence length during runtime. We use this option and it looks like it works, but I wonder if it's correct and how other developers deal with this issue?

May be it's worth implementing filter_length = None as an option for arbitrary sequence?

Example code is here - https://gist.github.com/lukovkin/e57dc3d40c9148a65c2bf40ea6360e45
Environment - Python 3.4, TF 0.8, Keras 1.0.1

> AttributeError                            Traceback (most recent call last)
> /root/miniconda2/envs/tf/lib/python3.4/site-packages/numpy/core/fromnumeric.py in prod(a, axis, dtype, out, keepdims)
>    2488         try:
> -> 2489             prod = a.prod
>    2490         except AttributeError:
> 
> AttributeError: 'tuple' object has no attribute 'prod'
> 
> During handling of the above exception, another exception occurred:
> 
> TypeError                                 Traceback (most recent call last)
> <ipython-input-5-ae9ea8f75655> in <module>()
>       1 model_d = ufcnn_model_deconv(regression = False, output_dim=3, features=4, 
> ----> 2                                    loss=""categorical_crossentropy"", sequence_length=500, optimizer=rmsprop )
> 
> /notebook/ufcnn-keras/models/UFCNN_functional.py in ufcnn_model_deconv(sequence_length, features, nb_filter, filter_length, output_dim, optimizer, loss, regression, class_mode, activation, init)
>     285 
>     286     else:
> --> 287         conv9 = Convolution1D(nb_filter=output_dim, filter_length=None, border_mode='same', init=init, name='conv9')(relu8)
>     288         activation = Activation('softmax', name='activation')(conv9)
>     289         output = activation
> 
> /root/miniconda2/envs/tf/lib/python3.4/site-packages/Keras-1.0.1-py3.4.egg/keras/engine/topology.py in **call**(self, x, mask)
>     456                                     '')
>     457             if len(input_shapes) == 1:
> --> 458                 self.build(input_shapes[0])
>     459             else:
>     460                 self.build(input_shapes)
> 
> /root/miniconda2/envs/tf/lib/python3.4/site-packages/Keras-1.0.1-py3.4.egg/keras/layers/convolutional.py in build(self, input_shape)
>     118         input_dim = input_shape[2]
>     119         self.W_shape = (self.nb_filter, input_dim, self.filter_length, 1)
> --> 120         self.W = self.init(self.W_shape, name='{}_W'.format(self.name))
>     121         self.b = K.zeros((self.nb_filter,), name='{}_b'.format(self.name))
>     122         self.trainable_weights = [self.W, self.b]
> 
> /root/miniconda2/envs/tf/lib/python3.4/site-packages/Keras-1.0.1-py3.4.egg/keras/initializations.py in lecun_uniform(shape, name, dim_ordering)
>      41         http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf
>      42     '''
> ---> 43     fan_in, fan_out = get_fans(shape, dim_ordering=dim_ordering)
>      44     scale = np.sqrt(3. / fan_in)
>      45     return uniform(shape, scale, name=name)
> 
> /root/miniconda2/envs/tf/lib/python3.4/site-packages/Keras-1.0.1-py3.4.egg/keras/initializations.py in get_fans(shape, dim_ordering)
>      13         # TF kernel shape: (..., input_depth, depth)
>      14         if dim_ordering == 'th':
> ---> 15             fan_in = np.prod(shape[1:])
>      16             fan_out = shape[0]
>      17         elif dim_ordering == 'tf':
> 
> /root/miniconda2/envs/tf/lib/python3.4/site-packages/numpy/core/fromnumeric.py in prod(a, axis, dtype, out, keepdims)
>    2490         except AttributeError:
>    2491             return _methods._prod(a, axis=axis, dtype=dtype,
> -> 2492                                   out=out, keepdims=keepdims)
>    2493         return prod(axis=axis, dtype=dtype, out=out)
>    2494     else:
> 
> /root/miniconda2/envs/tf/lib/python3.4/site-packages/numpy/core/_methods.py in _prod(a, axis, dtype, out, keepdims)
>      33 
>      34 def _prod(a, axis=None, dtype=None, out=None, keepdims=False):
> ---> 35     return umr_prod(a, axis, dtype, out, keepdims)
>      36 
>      37 def _any(a, axis=None, dtype=None, out=None, keepdims=False):
> 
> TypeError: unsupported operand type(s) for *: 'int' and 'NoneType'

Please make sure that the boxes below are checked before you submit your issue. Thank you!
- [x] Check that you are up-to-date with the master branch of Keras. You can update with:
  pip install git+git://github.com/fchollet/keras.git --upgrade --no-deps
- [x] If running on Theano, check that you are up-to-date with the master branch of Theano. You can update with:
  pip install git+git://github.com/Theano/Theano.git --upgrade --no-deps
- [x] Provide a link to a GitHub Gist of a Python script that can reproduce your issue (or just copy the script here if it is short).
",0,Correct value for 'filter_length' for final layer(s) in Fully Convolutional Net,"Correct value for 'filter_length' for final layer(s) in Fully Convolutional Net Hi,

If we implement fully convolutional net in Keras, we want it to be able to work with input of arbitrary size.
In our case of 1D data - with arbitrary number of timesteps.

We can use None as an instruction that input could be of any size on the 1st dimension, e.g.:


But in the last layer(s) of  fully CN model we have to use actual length of the sequence as the filter_length in order to obtain sequence of labels correctly distributed along original timespan, e.g.:



As soon as we don't know actual sequence length before runtime, we have the following options:
1. Omit this parameter - but it's mandatory and it will not work
2. Use None value for the filter_length in order to tell Keras that this value will be available on runtime only (see input shape above) - it will cause an error (see output below)
3. Use some kind of placeholder value during network buildup and hope that it will be magically replaced by the real sequence length during runtime. We use this option and it looks like it works, but I wonder if it's correct and how other developers deal with this issue?

May be it's worth implementing filter_length = None as an option for arbitrary sequence?

Example code is here - https://gist.github.com/lukovkin/e57dc3d40c9148a65c2bf40ea6360e45
Environment - Python 3.4, TF 0.8, Keras 1.0.1

> AttributeError                            Traceback (most recent call last)
> /root/miniconda2/envs/tf/lib/python3.4/site-packages/numpy/core/fromnumeric.py in prod(a, axis, dtype, out, keepdims)
>    2488         try:
> -> 2489             prod = a.prod
>    2490         except AttributeError:
> 
> AttributeError: 'tuple' object has no attribute 'prod'
> 
> During handling of the above exception, another exception occurred:
> 
> TypeError                                 Traceback (most recent call last)
> <ipython-input-5-ae9ea8f75655> in <module>()
>       1 model_d = ufcnn_model_deconv(regression = False, output_dim=3, features=4, 
> ----> 2                                    loss=""categorical_crossentropy"", sequence_length=500, optimizer=rmsprop )
> 
> /notebook/ufcnn-keras/models/UFCNN_functional.py in ufcnn_model_deconv(sequence_length, features, nb_filter, filter_length, output_dim, optimizer, loss, regression, class_mode, activation, init)
>     285 
>     286     else:
> --> 287         conv9 = Convolution1D(nb_filter=output_dim, filter_length=None, border_mode='same', init=init, name='conv9')(relu8)
>     288         activation = Activation('softmax', name='activation')(conv9)
>     289         output = activation
> 
> /root/miniconda2/envs/tf/lib/python3.4/site-packages/Keras-1.0.1-py3.4.egg/keras/engine/topology.py in **call**(self, x, mask)
>     456                                     '')
>     457             if len(input_shapes) == 1:
> --> 458                 self.build(input_shapes[0])
>     459             else:
>     460                 self.build(input_shapes)
> 
> /root/miniconda2/envs/tf/lib/python3.4/site-packages/Keras-1.0.1-py3.4.egg/keras/layers/convolutional.py in build(self, input_shape)
>     118         input_dim = input_shape[2]
>     119         self.W_shape = (self.nb_filter, input_dim, self.filter_length, 1)
> --> 120         self.W = self.init(self.W_shape, name='{}_W'.format(self.name))
>     121         self.b = K.zeros((self.nb_filter,), name='{}_b'.format(self.name))
>     122         self.trainable_weights = [self.W, self.b]
> 
> /root/miniconda2/envs/tf/lib/python3.4/site-packages/Keras-1.0.1-py3.4.egg/keras/initializations.py in lecun_uniform(shape, name, dim_ordering)
>      41         http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf
>      42     '''
> ---> 43     fan_in, fan_out = get_fans(shape, dim_ordering=dim_ordering)
>      44     scale = np.sqrt(3. / fan_in)
>      45     return uniform(shape, scale, name=name)
> 
> /root/miniconda2/envs/tf/lib/python3.4/site-packages/Keras-1.0.1-py3.4.egg/keras/initializations.py in get_fans(shape, dim_ordering)
>      13         # TF kernel shape: (..., input_depth, depth)
>      14         if dim_ordering == 'th':
> ---> 15             fan_in = np.prod(shape[1:])
>      16             fan_out = shape[0]
>      17         elif dim_ordering == 'tf':
> 
> /root/miniconda2/envs/tf/lib/python3.4/site-packages/numpy/core/fromnumeric.py in prod(a, axis, dtype, out, keepdims)
>    2490         except AttributeError:
>    2491             return _methods._prod(a, axis=axis, dtype=dtype,
> -> 2492                                   out=out, keepdims=keepdims)
>    2493         return prod(axis=axis, dtype=dtype, out=out)
>    2494     else:
> 
> /root/miniconda2/envs/tf/lib/python3.4/site-packages/numpy/core/_methods.py in _prod(a, axis, dtype, out, keepdims)
>      33 
>      34 def _prod(a, axis=None, dtype=None, out=None, keepdims=False):
> ---> 35     return umr_prod(a, axis, dtype, out, keepdims)
>      36 
>      37 def _any(a, axis=None, dtype=None, out=None, keepdims=False):
> 
> TypeError: unsupported operand type(s) for *: 'int' and 'NoneType'

Please make sure that the boxes below are checked before you submit your issue. Thank you!
- [x] Check that you are up-to-date with the master branch of Keras. You can update with:
  pip install git+git://github.com/fchollet/keras.git --upgrade --no-deps
- [x] If running on Theano, check that you are up-to-date with the master branch of Theano. You can update with:
  pip install git+git://github.com/Theano/Theano.git --upgrade --no-deps
- [x] Provide a link to a GitHub Gist of a Python script that can reproduce your issue (or just copy the script here if it is short).
"
keras,2608,"Hello everybody,

I would like to implement the ResNet network with the shortcut connections that add zero entries when features/channels dimensions mismatch according to the original paper:

> When the dimensions increase (dotted line shortcuts
> in Fig. 3), we consider two options: (A) The shortcut still
> performs identity mapping, with extra zero entries padded
> for increasing dimensions ...
> http://arxiv.org/pdf/1512.03385v1.pdf

However wasn't able to implement it and I can't seem to find an answer on the web or on the source code. All the implementations that I found use the 1x1 convolution trick for shortcut connections when dimensions mismatch.

The layer I would like to implement would basically concatenate the input tensor with a tensor with an all zeros tensor to compensate for the dimension mismatch.

The idea would be something like this, but I could not get it working:



Does anyone has an idea on how to implement such a layer ?

Thanks a lot 
",0,Zero-padding for ResNet shortcut connections when channel number increase,"Zero-padding for ResNet shortcut connections when channel number increase Hello everybody,

I would like to implement the ResNet network with the shortcut connections that add zero entries when features/channels dimensions mismatch according to the original paper:

> When the dimensions increase (dotted line shortcuts
> in Fig. 3), we consider two options: (A) The shortcut still
> performs identity mapping, with extra zero entries padded
> for increasing dimensions ...
> http://arxiv.org/pdf/1512.03385v1.pdf

However wasn't able to implement it and I can't seem to find an answer on the web or on the source code. All the implementations that I found use the 1x1 convolution trick for shortcut connections when dimensions mismatch.

The layer I would like to implement would basically concatenate the input tensor with a tensor with an all zeros tensor to compensate for the dimension mismatch.

The idea would be something like this, but I could not get it working:



Does anyone has an idea on how to implement such a layer ?

Thanks a lot 
"
keras,8446,"Please make sure that the boxes below are checked before you submit your issue. If your issue is an implementation question, please ask your question on [StackOverflow](http://stackoverflow.com/questions/tagged/keras) or [join the Keras Slack channel](https://keras-slack-autojoin.herokuapp.com/) and ask there instead of filing a GitHub issue.

Thank you!

- [x] Check that you are up-to-date with the master branch of Keras. You can update with:
pip install git+git://github.com/fchollet/keras.git --upgrade --no-deps

- [x] If running on TensorFlow, check that you are up-to-date with the latest version. The installation instructions can be found [here](https://www.tensorflow.org/get_started/os_setup).

This is a short and simple issue. Following upgrading to Keras 2.0.9 I have been using the multi_gpu_model utility but I can seem to save my models or best weights using model.save.

The error I get is



I suspect there is some problem gaining access to the model object. Is there a work around this issue?",0,Can not save model using model.save following multi_gpu_model,"Can not save model using model.save following multi_gpu_model Please make sure that the boxes below are checked before you submit your issue. If your issue is an implementation question, please ask your question on [StackOverflow](http://stackoverflow.com/questions/tagged/keras) or [join the Keras Slack channel](https://keras-slack-autojoin.herokuapp.com/) and ask there instead of filing a GitHub issue.

Thank you!

- [x] Check that you are up-to-date with the master branch of Keras. You can update with:
pip install git+git://github.com/fchollet/keras.git --upgrade --no-deps

- [x] If running on TensorFlow, check that you are up-to-date with the latest version. The installation instructions can be found [here](https://www.tensorflow.org/get_started/os_setup).

This is a short and simple issue. Following upgrading to Keras 2.0.9 I have been using the multi_gpu_model utility but I can seem to save my models or best weights using model.save.

The error I get is



I suspect there is some problem gaining access to the model object. Is there a work around this issue?"
keras,9496,"In the [VAE](https://github.com/keras-team/keras/blob/master/examples/variational_autoencoder.py#L58) example, the model is compiled like this: 
In the latest master branch, the compile method is required to have the arguments optimizer and loss passed.",0,Outdated VAE example,"Outdated VAE example In the [VAE](https://github.com/keras-team/keras/blob/master/examples/variational_autoencoder.py#L58) example, the model is compiled like this: 
In the latest master branch, the compile method is required to have the arguments optimizer and loss passed."
keras,5615,"
error:

TypeError: The added layer must be an instance of class Layer. Found: Tensor(""input_1:0"", shape=(?, 784), |dtype=float32)
",0,why input layer requires to be an instance of a class layer?,"why input layer requires to be an instance of a class layer? 
error:

TypeError: The added layer must be an instance of class Layer. Found: Tensor(""input_1:0"", shape=(?, 784), |dtype=float32)
"
keras,13341,"**System information**  
- Have I written custom code (as opposed to using example directory):  
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  
- TensorFlow backend (yes / no):  
- TensorFlow version: 1.14.0 
- Keras version:  2.3.0
- Python version:  
- CUDA/cuDNN version:  
- GPU model and memory:  


**Describe the current behavior**  

  File ""/usr/local/lib/python3.6/dist-packages/keras/engine/training.py"", line 222, in compile
    masks=masks)
  File ""/usr/local/lib/python3.6/dist-packages/keras/engine/training.py"", line 871, in _handle_metrics
    self._per_output_metrics[i], target, output, output_mask)
  File ""/usr/local/lib/python3.6/dist-packages/keras/engine/training.py"", line 842, in _handle_per_output_metrics
    metric_fn, y_true, y_pred, weights=weights, mask=mask)
  File ""/usr/local/lib/python3.6/dist-packages/keras/engine/training_utils.py"", line 1022, in call_metric_function
    mask = math_ops.cast(mask, y_pred.dtype)
NameError: name 'math_ops' is not defined

**Describe the expected behavior**  

**Code to reproduce the issue**  

**Other info / logs**  
",0,NameError: name 'math_ops' is not defined,"NameError: name 'math_ops' is not defined **System information**  
- Have I written custom code (as opposed to using example directory):  
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  
- TensorFlow backend (yes / no):  
- TensorFlow version: 1.14.0 
- Keras version:  2.3.0
- Python version:  
- CUDA/cuDNN version:  
- GPU model and memory:  


**Describe the current behavior**  

  File ""/usr/local/lib/python3.6/dist-packages/keras/engine/training.py"", line 222, in compile
    masks=masks)
  File ""/usr/local/lib/python3.6/dist-packages/keras/engine/training.py"", line 871, in _handle_metrics
    self._per_output_metrics[i], target, output, output_mask)
  File ""/usr/local/lib/python3.6/dist-packages/keras/engine/training.py"", line 842, in _handle_per_output_metrics
    metric_fn, y_true, y_pred, weights=weights, mask=mask)
  File ""/usr/local/lib/python3.6/dist-packages/keras/engine/training_utils.py"", line 1022, in call_metric_function
    mask = math_ops.cast(mask, y_pred.dtype)
NameError: name 'math_ops' is not defined

**Describe the expected behavior**  

**Code to reproduce the issue**  

**Other info / logs**  
"
keras,2820,,0,   self.pool_size = tuple(pool_size)  TypeError: 'int' object is not iterable,
keras,4015," imported by 
when I use callbacks in 



it's got who know why is it?
I will be appreciate.
",0, keras callbacks does not seem to work by import," keras callbacks does not seem to work by import  imported by 
when I use callbacks in 



it's got who know why is it?
I will be appreciate.
"
keras,5646,"I am working on Brain tumor Segmentation MRI images using CNN. but i don't know how to implement segmentation in cnn ?

Please help me with this.

Thanks
KJ",0,Image Segmentation,"Image Segmentation I am working on Brain tumor Segmentation MRI images using CNN. but i don't know how to implement segmentation in cnn ?

Please help me with this.

Thanks
KJ"
keras,11460,"
![img_20181023_154623](https://user-images.githubusercontent.com/25239334/47344598-75f56b00-d6db-11e8-86a5-a81732ee33d0.jpg)
Hi, i have a network like this. And  i want to stop the backpropagation from CNN  to Inception. The Inception just get the gradients from LSTM. What should i do ? Thank you for your help.",0,stop the backpropagation,"stop the backpropagation 
![img_20181023_154623](https://user-images.githubusercontent.com/25239334/47344598-75f56b00-d6db-11e8-86a5-a81732ee33d0.jpg)
Hi, i have a network like this. And  i want to stop the backpropagation from CNN  to Inception. The Inception just get the gradients from LSTM. What should i do ? Thank you for your help."
keras,6651,"Hi everyone,
I have a reasonably big dataset of images, and I'm trying to train the Inception Model on it.
The problem is, when I try to use the Image Generator (To apply random transformations), I get a MemoryError.

This is weird, because, when I fit my model without the generator (So without transformations) I have no problem.
Possibly, the generator has some memory leaks ?
(I train the model with train_on_batch method and yield batch_size from the generator)

If there are no solutions, is there a way to apply transformations to images without the generator ?
(By each images possibly ?)
Thanks!",0,Keras Image Generator MemoryError,"Keras Image Generator MemoryError Hi everyone,
I have a reasonably big dataset of images, and I'm trying to train the Inception Model on it.
The problem is, when I try to use the Image Generator (To apply random transformations), I get a MemoryError.

This is weird, because, when I fit my model without the generator (So without transformations) I have no problem.
Possibly, the generator has some memory leaks ?
(I train the model with train_on_batch method and yield batch_size from the generator)

If there are no solutions, is there a way to apply transformations to images without the generator ?
(By each images possibly ?)
Thanks!"
keras,9198,"I'm trying to fine-tune a modified InceptionV3 model in Keras.

I follow the example ""Fine-tune InceptionV3 on a new set of classes"" on [this page][1].

So I first trained the top dense layers that were added to the InceptionV3 base model with the following code:

    model = Model(inputs=base_model.input, outputs=predictions)

    for layer in base_model.layers:
        layer.trainable = False

    parallel_model = multi_gpu_model(model, gpus=2)
    
    parallel_model.compile(optimizer='rmsprop', loss='categorical_crossentropy')
    
    history = parallel_model.fit_generator(generate_batches(path), steps_per_epoch = num_images/batch_size, epochs = num_epochs)

After that, I try to fine-tune the top 2 inception blocks from InceptionV3. And according to the example, what I should do is:

    for layer in model.layers[:249]:
       layer.trainable = False
    for layer in model.layers[249:]:
       layer.trainable = True
    
    model.compile(optimizer=SGD(lr=0.0001, momentum=0.9), loss='categorical_crossentropy')

    model.fit_generator(...)

But I'm using the , so I don't know how to freeze the first 249 layers. 

I mean, if I freeze the layers in the no-gpu model (like the example), and use  to freeze the layers in the , then the weights in the top dense layers that were just trained and contained in the  will be overwritten, right?

On the other hand, I tried to directly use , but when I checked the layers in the , it showed:

    for i, layer in enumerate(parallel_model.layers):
       print(i, layer.name)

    (0, 'input_1')
    (1, 'lambda_1')
    (2, 'lambda_2')
    (3, 'model_1')
    (4, 'dense_3')

So what are the 'lambda_1', 'lambda_2' and 'model_1' layers and why it only shows 5 layers in the ?

More importantly, how to freeze the layers in the ?

  [1]: https://keras.io/applications/",0,Freeze layers with the multi_gpu_model in Keras,"Freeze layers with the multi_gpu_model in Keras I'm trying to fine-tune a modified InceptionV3 model in Keras.

I follow the example ""Fine-tune InceptionV3 on a new set of classes"" on [this page][1].

So I first trained the top dense layers that were added to the InceptionV3 base model with the following code:

    model = Model(inputs=base_model.input, outputs=predictions)

    for layer in base_model.layers:
        layer.trainable = False

    parallel_model = multi_gpu_model(model, gpus=2)
    
    parallel_model.compile(optimizer='rmsprop', loss='categorical_crossentropy')
    
    history = parallel_model.fit_generator(generate_batches(path), steps_per_epoch = num_images/batch_size, epochs = num_epochs)

After that, I try to fine-tune the top 2 inception blocks from InceptionV3. And according to the example, what I should do is:

    for layer in model.layers[:249]:
       layer.trainable = False
    for layer in model.layers[249:]:
       layer.trainable = True
    
    model.compile(optimizer=SGD(lr=0.0001, momentum=0.9), loss='categorical_crossentropy')

    model.fit_generator(...)

But I'm using the , so I don't know how to freeze the first 249 layers. 

I mean, if I freeze the layers in the no-gpu model (like the example), and use  to freeze the layers in the , then the weights in the top dense layers that were just trained and contained in the  will be overwritten, right?

On the other hand, I tried to directly use , but when I checked the layers in the , it showed:

    for i, layer in enumerate(parallel_model.layers):
       print(i, layer.name)

    (0, 'input_1')
    (1, 'lambda_1')
    (2, 'lambda_2')
    (3, 'model_1')
    (4, 'dense_3')

So what are the 'lambda_1', 'lambda_2' and 'model_1' layers and why it only shows 5 layers in the ?

More importantly, how to freeze the layers in the ?

  [1]: https://keras.io/applications/"
keras,12782,"
**System information**  
- Have I written custom code (as opposed to using example directory):  yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Windows 10 Pro
- TensorFlow backend (yes / no):  yes
- TensorFlow version:  1.13.1
- Keras version:  2.2.4
- Python version: 3.7.3
- CUDA/cuDNN version:  NA
- GPU model and memory:  NA

**Describe the current behavior**  
When creating a LSTM layer using the functional moded below error is thrown


**Describe the expected behavior**  
LSTM layer is created successfully

**Code to reproduce the issue**  


**Error Log**  
  

**The below version of the code which uses the Sequential model runs successfully**



**Success log**

",0,issue while using LSTM in functional mode,"issue while using LSTM in functional mode 
**System information**  
- Have I written custom code (as opposed to using example directory):  yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Windows 10 Pro
- TensorFlow backend (yes / no):  yes
- TensorFlow version:  1.13.1
- Keras version:  2.2.4
- Python version: 3.7.3
- CUDA/cuDNN version:  NA
- GPU model and memory:  NA

**Describe the current behavior**  
When creating a LSTM layer using the functional moded below error is thrown


**Describe the expected behavior**  
LSTM layer is created successfully

**Code to reproduce the issue**  


**Error Log**  
  

**The below version of the code which uses the Sequential model runs successfully**



**Success log**

"
keras,916,"https://github.com/fchollet/keras/blob/master/examples/mnist_mlp.py#L52

Might want to update your example ;-) 
",0,test data used for validation,"test data used for validation https://github.com/fchollet/keras/blob/master/examples/mnist_mlp.py#L52

Might want to update your example ;-) 
"
keras,8897,"Hi
I have successfully installed theano, tensorflow and keras but I always get this error each time I try to import keras.

My OS is windows 10
I am now using python 3.5 (was 3.63 but had to reinstall anaconda with python 3.5 when troubleshooting)

Kindly help

Thanks
",0, 'ModuleNotFoundError : no module named tensorflow'," 'ModuleNotFoundError : no module named tensorflow' Hi
I have successfully installed theano, tensorflow and keras but I always get this error each time I try to import keras.

My OS is windows 10
I am now using python 3.5 (was 3.63 but had to reinstall anaconda with python 3.5 when troubleshooting)

Kindly help

Thanks
"
keras,9235,"This code works in keras 2.1.2 but not on keras 2.1.3:



",0,Can't apply Inception on input tensor,"Can't apply Inception on input tensor This code works in keras 2.1.2 but not on keras 2.1.3:



"
keras,3120,"In other words, I don't know if the Keras install will use Theano or TensorFlow, but I'd still like to have control over whether the CPU or GPU is being used. Is this possible? Could we add a flag to  for this purpose otherwise?
",0,Is it possible to set CPU / GPU usage without knowing the backend?,"Is it possible to set CPU / GPU usage without knowing the backend? In other words, I don't know if the Keras install will use Theano or TensorFlow, but I'd still like to have control over whether the CPU or GPU is being used. Is this possible? Could we add a flag to  for this purpose otherwise?
"
keras,3457,"when I load a model that was saved in hdf5 that was using a custom loss function called  I get the following exception

Even if I have defined the function in the current script it still raises this exeption.

Is this the normal behaviour? Do I have to install keras defining my custom loss in the source code?
",0,Loading saved model with custom loss,"Loading saved model with custom loss when I load a model that was saved in hdf5 that was using a custom loss function called  I get the following exception

Even if I have defined the function in the current script it still raises this exeption.

Is this the normal behaviour? Do I have to install keras defining my custom loss in the source code?
"
keras,2712,"Hi, I am new to keras and neural network, and I am wondering if model.fit() is accumulative?

So basically, I accidentally killed the program yesterday, but however I do save weights on every epoch. So I was wondering if I could load the last weight into the model and then call model.fit()?
",0,is model.fit() accumulative?,"is model.fit() accumulative? Hi, I am new to keras and neural network, and I am wondering if model.fit() is accumulative?

So basically, I accidentally killed the program yesterday, but however I do save weights on every epoch. So I was wondering if I could load the last weight into the model and then call model.fit()?
"
keras,3323,"I see a lot of variables are named like ""nb_xx"", such as ""nb_classes"", ""nb_samples"", etc. So what does ""nb"" means here?
",0,"What does ""nb"" mean in Keras?","What does ""nb"" mean in Keras? I see a lot of variables are named like ""nb_xx"", such as ""nb_classes"", ""nb_samples"", etc. So what does ""nb"" means here?
"
keras,11358,"Please make sure that the boxes below are checked before you submit your issue. If your issue is an implementation question, please ask your question on [StackOverflow](http://stackoverflow.com/questions/tagged/keras) or [join the Keras Slack channel](https://keras-slack-autojoin.herokuapp.com/) and ask there instead of filing a GitHub issue.

Thank you!

- [x] Check that you are up-to-date with the master branch of Keras. You can update with:
pip install git+git://github.com/keras-team/keras.git --upgrade --no-deps

- [ ] If running on TensorFlow, check that you are up-to-date with the latest version. The installation instructions can be found [here](https://www.tensorflow.org/get_started/os_setup).

- [ ] If running on Theano, check that you are up-to-date with the master branch of Theano. You can update with:
pip install git+git://github.com/Theano/Theano.git --upgrade --no-deps

- [ ] Provide a link to a GitHub Gist of a Python script that can reproduce your issue (or just copy the script here if it is short).
",0,Error in load model,"Error in load model Please make sure that the boxes below are checked before you submit your issue. If your issue is an implementation question, please ask your question on [StackOverflow](http://stackoverflow.com/questions/tagged/keras) or [join the Keras Slack channel](https://keras-slack-autojoin.herokuapp.com/) and ask there instead of filing a GitHub issue.

Thank you!

- [x] Check that you are up-to-date with the master branch of Keras. You can update with:
pip install git+git://github.com/keras-team/keras.git --upgrade --no-deps

- [ ] If running on TensorFlow, check that you are up-to-date with the latest version. The installation instructions can be found [here](https://www.tensorflow.org/get_started/os_setup).

- [ ] If running on Theano, check that you are up-to-date with the master branch of Theano. You can update with:
pip install git+git://github.com/Theano/Theano.git --upgrade --no-deps

- [ ] Provide a link to a GitHub Gist of a Python script that can reproduce your issue (or just copy the script here if it is short).
"
keras,8373,"I ran this code several times on different PCs recent days, but the code was not able to correctly generate samples. All the samples that generated are all pure black, that, only a black image.

I ran the code for 50 epoches, on keras 2(recent updated) , both theano and tensorflow backend were tried. 

And I noticed that in the code, the set ""discriminator.trainable = False"" in done in line 160, but seems never set to be True in the ""for"" loop of training. So is the discriminator is initialized but never trained during the process?

![window](https://user-images.githubusercontent.com/15433614/32375122-5d0e4396-c098-11e7-97f2-48bb5e69d1d8.png)
",0,Meeting some trouble when running examples/mnist_acgan.py(ACGAN model),"Meeting some trouble when running examples/mnist_acgan.py(ACGAN model) I ran this code several times on different PCs recent days, but the code was not able to correctly generate samples. All the samples that generated are all pure black, that, only a black image.

I ran the code for 50 epoches, on keras 2(recent updated) , both theano and tensorflow backend were tried. 

And I noticed that in the code, the set ""discriminator.trainable = False"" in done in line 160, but seems never set to be True in the ""for"" loop of training. So is the discriminator is initialized but never trained during the process?

![window](https://user-images.githubusercontent.com/15433614/32375122-5d0e4396-c098-11e7-97f2-48bb5e69d1d8.png)
"
keras,5360,"
I want to implement LSTM/GRU language model by Keras. But it seems one challenge.
How to generate softmax over all the vocabularies? I.e. I cannot access weight in Embedding layer in either my own implemented loss function as well as function in Lambda layer.",0,How to implement LSTM/GRU language model through Keras?,"How to implement LSTM/GRU language model through Keras? 
I want to implement LSTM/GRU language model by Keras. But it seems one challenge.
How to generate softmax over all the vocabularies? I.e. I cannot access weight in Embedding layer in either my own implemented loss function as well as function in Lambda layer."
keras,2145,"The Optimizer's class get_config method returns a dictionary with single key ""name"". However, Optimizer can also receive ""clipnorm"" and ""clipvalue"" keyword arguments. It would be nice to also add them to the dictionary returned by get_config if they were set.
- [x] Check that you are up-to-date with the master branch of Keras. You can update with:
  pip install git+git://github.com/fchollet/keras.git --upgrade --no-deps
- [x] If running on Theano, check that you are up-to-date with the master branch of Theano. You can update with:
  pip install git+git://github.com/Theano/Theano.git --upgrade --no-deps
- [x] Provide a link to a GitHub Gist of a Python script that can reproduce your issue (or just copy the script here if it is short).

I'd be happy to implement this feature.
",0,Include clipnorm/clipvalue in Optimizer.get_config output,"Include clipnorm/clipvalue in Optimizer.get_config output The Optimizer's class get_config method returns a dictionary with single key ""name"". However, Optimizer can also receive ""clipnorm"" and ""clipvalue"" keyword arguments. It would be nice to also add them to the dictionary returned by get_config if they were set.
- [x] Check that you are up-to-date with the master branch of Keras. You can update with:
  pip install git+git://github.com/fchollet/keras.git --upgrade --no-deps
- [x] If running on Theano, check that you are up-to-date with the master branch of Theano. You can update with:
  pip install git+git://github.com/Theano/Theano.git --upgrade --no-deps
- [x] Provide a link to a GitHub Gist of a Python script that can reproduce your issue (or just copy the script here if it is short).

I'd be happy to implement this feature.
"
keras,6156,"Please make sure that the boxes below are checked before you submit your issue. If your issue is an implementation question, please ask your question on [StackOverflow](http://stackoverflow.com/questions/tagged/keras) or [join the Keras Slack channel](https://keras-slack-autojoin.herokuapp.com/) and ask there instead of filing a GitHub issue.

Thank you!

- [ ] Check that you are up-to-date with the master branch of Keras. You can update with:
pip install git+git://github.com/fchollet/keras.git --upgrade --no-deps

- [ ] If running on TensorFlow, check that you are up-to-date with the latest version. The installation instructions can be found [here](https://www.tensorflow.org/get_started/os_setup).

- [ ] If running on Theano, check that you are up-to-date with the master branch of Theano. You can update with:
pip install git+git://github.com/Theano/Theano.git --upgrade --no-deps

- [ ] Provide a link to a GitHub Gist of a Python script that can reproduce your issue (or just copy the script here if it is short).
",0,"I want to use seq2seq model for Q&A,but I don't know the output of the LSTM(decoder),is it a array of probability??","I want to use seq2seq model for Q&A,but I don't know the output of the LSTM(decoder),is it a array of probability?? Please make sure that the boxes below are checked before you submit your issue. If your issue is an implementation question, please ask your question on [StackOverflow](http://stackoverflow.com/questions/tagged/keras) or [join the Keras Slack channel](https://keras-slack-autojoin.herokuapp.com/) and ask there instead of filing a GitHub issue.

Thank you!

- [ ] Check that you are up-to-date with the master branch of Keras. You can update with:
pip install git+git://github.com/fchollet/keras.git --upgrade --no-deps

- [ ] If running on TensorFlow, check that you are up-to-date with the latest version. The installation instructions can be found [here](https://www.tensorflow.org/get_started/os_setup).

- [ ] If running on Theano, check that you are up-to-date with the master branch of Theano. You can update with:
pip install git+git://github.com/Theano/Theano.git --upgrade --no-deps

- [ ] Provide a link to a GitHub Gist of a Python script that can reproduce your issue (or just copy the script here if it is short).
"
keras,7089,"@fchollet can you enable autocancelation on Travis? Haven't tested it out myself but it should cancel redundant builds (for example, if you push twice to a pr). I don't think it is enabled now but I could be wrong.

https://blog.travis-ci.com/2017-03-22-introducing-auto-cancellation

Travis seems to be doing a lot of work recently, especially with the new backend. Could help to cut back a little bit.

Cheers",0,Autocancelation,"Autocancelation @fchollet can you enable autocancelation on Travis? Haven't tested it out myself but it should cancel redundant builds (for example, if you push twice to a pr). I don't think it is enabled now but I could be wrong.

https://blog.travis-ci.com/2017-03-22-introducing-auto-cancellation

Travis seems to be doing a lot of work recently, especially with the new backend. Could help to cut back a little bit.

Cheers"
keras,100,"Here are the key features that we will have in Keras by the time it hits v1.0. 
- Visualization tools, possibly built on top of Bokeh. You should be able to see everything that's going on in your experiment, and maybe even manage your experiments from a GUI. Total visualization is key to doing good research.
- Easy support for Spearmint for hyperparameter search.
- Complete unit tests. The recent regularizers/constraints debacle that left Kaggle users confused highlights once more that reliable unit tests should be an absolute priority. We want to be able to develop quality code safely and with confidence. 
- Better convnet features, including FFT convolutions, and maybe new padding options. In the medium term we should look into incorporating support for Nervana Systems' fast convolution kernels. Let's stay state of the art ;-)
- Support for non-sequential models, by the way of:
  - [DONE] a Merge container that takes a list of Sequential models and turns them into a single output
  - a Fork container that replicates the output of a Sequential model to a list of Sequential models
    (both would be trainable end-to-end, of course).

If you see anything you would like to work on, please post here with your thoughts on how you would incorporate it into the current architecture / data structures. This is our opportunity to make a big contribution to the deep learning ecosystem! : )
",0,Roadmap,"Roadmap Here are the key features that we will have in Keras by the time it hits v1.0. 
- Visualization tools, possibly built on top of Bokeh. You should be able to see everything that's going on in your experiment, and maybe even manage your experiments from a GUI. Total visualization is key to doing good research.
- Easy support for Spearmint for hyperparameter search.
- Complete unit tests. The recent regularizers/constraints debacle that left Kaggle users confused highlights once more that reliable unit tests should be an absolute priority. We want to be able to develop quality code safely and with confidence. 
- Better convnet features, including FFT convolutions, and maybe new padding options. In the medium term we should look into incorporating support for Nervana Systems' fast convolution kernels. Let's stay state of the art ;-)
- Support for non-sequential models, by the way of:
  - [DONE] a Merge container that takes a list of Sequential models and turns them into a single output
  - a Fork container that replicates the output of a Sequential model to a list of Sequential models
    (both would be trainable end-to-end, of course).

If you see anything you would like to work on, please post here with your thoughts on how you would incorporate it into the current architecture / data structures. This is our opportunity to make a big contribution to the deep learning ecosystem! : )
"
keras,8227,"Paramater to GeneratorEnqueuer is called 'random_seed', while everywhere else throughout Keras, it's called 'seed'. I can rename it, but should I provide backward compatibility or not?
https://github.com/fchollet/keras/blob/master/keras/utils/data_utils.py#L556",0,[API Inconsistency] GeneratorEnqueuer(random_seed),"[API Inconsistency] GeneratorEnqueuer(random_seed) Paramater to GeneratorEnqueuer is called 'random_seed', while everywhere else throughout Keras, it's called 'seed'. I can rename it, but should I provide backward compatibility or not?
https://github.com/fchollet/keras/blob/master/keras/utils/data_utils.py#L556"
keras,2326,"Hi,

I was using the following piece to report the training progress



This was working before, but after upgrading to latest , it looks  isn't supplied anymore. It gives a KeyError. (I suspect it's related with  being deprecated.)

It looks documentation in http://keras.io/callbacks/ should be updated with the keys as well. 

Thank you very much. 
",0,`on_epoch_end` in Callback doesn't receive `acc` key in `logs` argument anymore?,"`on_epoch_end` in Callback doesn't receive `acc` key in `logs` argument anymore? Hi,

I was using the following piece to report the training progress



This was working before, but after upgrading to latest , it looks  isn't supplied anymore. It gives a KeyError. (I suspect it's related with  being deprecated.)

It looks documentation in http://keras.io/callbacks/ should be updated with the keys as well. 

Thank you very much. 
"
keras,5291,"I am quite new to Keras. What I am trying to accomplish is to train a word level text generation module.
To overcome the problem of varying length of sentences in my training data, I set my batch_size to 1 and trained the model with statefull = true. 
Now it will predict next word given a word. Then I tried to concatenate the generated word to the input to the predict function but in that case I get output shape as a array of predictions for each word in the input ( What I wished for was a single prediction for the next word, considering all the previous words)

I want to design something like if i give 
START_TOKEN
Generate say : What
then i concatenate it to START_TOKEN  to get ""START_TOKEN What"" (ie add a new row to input vector)
Then the next word generated should consider both START_TOKEN and word ""What"" to generate the next word and so on....

My current code:




Here the batch function generates a x and y arrays representing with shape (len_corpus, 1, vocabulary)
y is one word ahead of x 
like x= ""how are you....""
 then y =""are you.....

Thanks in advance",0,Text Generation With Kearas LSTM with varying time step,"Text Generation With Kearas LSTM with varying time step I am quite new to Keras. What I am trying to accomplish is to train a word level text generation module.
To overcome the problem of varying length of sentences in my training data, I set my batch_size to 1 and trained the model with statefull = true. 
Now it will predict next word given a word. Then I tried to concatenate the generated word to the input to the predict function but in that case I get output shape as a array of predictions for each word in the input ( What I wished for was a single prediction for the next word, considering all the previous words)

I want to design something like if i give 
START_TOKEN
Generate say : What
then i concatenate it to START_TOKEN  to get ""START_TOKEN What"" (ie add a new row to input vector)
Then the next word generated should consider both START_TOKEN and word ""What"" to generate the next word and so on....

My current code:




Here the batch function generates a x and y arrays representing with shape (len_corpus, 1, vocabulary)
y is one word ahead of x 
like x= ""how are you....""
 then y =""are you.....

Thanks in advance"
keras,7548," returns  as a parameter name which  does not accept.

To reproduce:





Raises:


",0,KerasRegressor/KerasClassifier get_params and set_params incompatible,"KerasRegressor/KerasClassifier get_params and set_params incompatible  returns  as a parameter name which  does not accept.

To reproduce:





Raises:


"
keras,6580,"Hi,

I'm not sure but the documentation about categorical_crossentropy seems to be not compatible with the tensorflow one.

I mean just had a look at the documentation about loss functions i. e.
https://keras.io/losses/#available-loss-functions

and at the end there is a note about the categorical_crossentropy.
There is such a sentence:

""Note: when using the categorical_crossentropy loss, your targets should be in categorical format (e.g. if you have 10 classes, the target for each sample should be a 10-dimensional vector that is all-zeros expect for a 1 at the index corresponding to the class of the sample).""

So I understand that the only possible target is e.g. [0, 0, 1, 0, 0]?

But in the tensorflow documentation about softmax_cross_entropy_with_logits  we have (https://www.tensorflow.org/api_docs/python/tf/nn/softmax_cross_entropy_with_logits)

""While the classes are mutually exclusive, their probabilities need not be. All that is required is that each row of labels is a valid probability distribution.""

which I understand means that such a target is possible [0.0, 0.75, 0.25, 0.0, 0.0]?

categorical_crossentropy is defined in https://github.com/tensorflow/tensorflow/blob/r1.1/tensorflow/contrib/keras/python/keras/backend.py and indeed returns softmax_cross_entropy_with_logits.

Am I missing something (if yes then I'm really sorry)?

Best wishes
Dawid
",0,Note about the categorical_crossentropy?,"Note about the categorical_crossentropy? Hi,

I'm not sure but the documentation about categorical_crossentropy seems to be not compatible with the tensorflow one.

I mean just had a look at the documentation about loss functions i. e.
https://keras.io/losses/#available-loss-functions

and at the end there is a note about the categorical_crossentropy.
There is such a sentence:

""Note: when using the categorical_crossentropy loss, your targets should be in categorical format (e.g. if you have 10 classes, the target for each sample should be a 10-dimensional vector that is all-zeros expect for a 1 at the index corresponding to the class of the sample).""

So I understand that the only possible target is e.g. [0, 0, 1, 0, 0]?

But in the tensorflow documentation about softmax_cross_entropy_with_logits  we have (https://www.tensorflow.org/api_docs/python/tf/nn/softmax_cross_entropy_with_logits)

""While the classes are mutually exclusive, their probabilities need not be. All that is required is that each row of labels is a valid probability distribution.""

which I understand means that such a target is possible [0.0, 0.75, 0.25, 0.0, 0.0]?

categorical_crossentropy is defined in https://github.com/tensorflow/tensorflow/blob/r1.1/tensorflow/contrib/keras/python/keras/backend.py and indeed returns softmax_cross_entropy_with_logits.

Am I missing something (if yes then I'm really sorry)?

Best wishes
Dawid
"
keras,3806,"Hello , i want to ask your opinion why this is happening. To explain myself until yesterday, my models as simple as they were they were working until today. my code is as follows :
The above code words for 100 or 200 embedding size but not for the above setup. It produces the following error.

The same error is produced and when i use cnn-lstm with word2vec vectors. The weird part is that the models were working yesterday for any value i had test them.
thanks,
nick
",0,"Lstm input shape error,","Lstm input shape error, Hello , i want to ask your opinion why this is happening. To explain myself until yesterday, my models as simple as they were they were working until today. my code is as follows :
The above code words for 100 or 200 embedding size but not for the above setup. It produces the following error.

The same error is produced and when i use cnn-lstm with word2vec vectors. The weird part is that the models were working yesterday for any value i had test them.
thanks,
nick
"
keras,3647,"## Error Message is below (Full Code  is attached)

Using Theano backend.
Using gpu device 0: GeForce GTX 1060 3GB (CNMeM is enabled with initial size: 81.0% of memory, cuDNN 5005)
(6L, 3L, 150L, 150L) <- this is data for predict

Traceback (most recent call last):
  File ""C:\Program Files (x86)\JetBrains\PyCharm Edu 2.0.4\helpers\pydev\pydevd.py"", line 2411, in <module>
    globals = debugger.run(setup['file'], None, None, is_module)
  File ""C:\Program Files (x86)\JetBrains\PyCharm Edu 2.0.4\helpers\pydev\pydevd.py"", line 1802, in run
    launch(file, globals, locals)  # execute the script
  File ""C:/Users/byoru/PycharmProjects/MailLession/FirstNN/Canvas.py"", line 50, in <module>
    output = model.predict(testarrnp, batch_size=2,verbose=1)
  File ""c:\program files (x86)\microsoft visual studio 12.0\vc\theano\keras\keras\models.py"", line 664, in predict
    return self.model.predict(x, batch_size=batch_size, verbose=verbose)
  File ""c:\program files (x86)\microsoft visual studio 12.0\vc\theano\keras\keras\engine\training.py"", line 1180, in predict
    batch_size=batch_size, verbose=verbose)
  File ""c:\program files (x86)\microsoft visual studio 12.0\vc\theano\keras\keras\engine\training.py"", line 879, in _predict_loop
    batch_outs = f(ins_batch)
  File ""c:\program files (x86)\microsoft visual studio 12.0\vc\theano\keras\keras\backend\theano_backend.py"", line 655, in __call__
    return self.function(*inputs)
  File ""c:\program files (x86)\microsoft visual studio 12.0\vc\theano\theano\compile\function_module.py"", line 879, in **call**
    storage_map=getattr(self.fn, 'storage_map', None))
  File ""c:\program files (x86)\microsoft visual studio 12.0\vc\theano\theano\gof\link.py"", line 325, in raise_with_op
    reraise(exc_type, exc_value, exc_trace)
  File ""c:\program files (x86)\microsoft visual studio 12.0\vc\theano\theano\compile\function_module.py"", line 866, in **call**
    self.fn() if output_subset is None else\
  File ""c:\program files (x86)\microsoft visual studio 12.0\vc\theano\theano\gof\op.py"", line 908, in rval
    r = p(n, [x[0] for x in i], o)
  File ""c:\program files (x86)\microsoft visual studio 12.0\vc\theano\theano\tensor\nnet\abstract_conv.py"", line 848, in perform
    conv_out = self.conv2d(img, kern, mode=""valid"", dilation=self.filter_dilation)
  File ""c:\program files (x86)\microsoft visual studio 12.0\vc\theano\theano\tensor\nnet\abstract_conv.py"", line 775, in conv2d
    dilated_kern[n, im0, ...],
IndexError: index 1 is out of bounds for axis 1 with size 1
Apply node that caused the error: AbstractConv2d{border_mode='valid', subsample=(1, 1), filter_flip=True, imshp=(None, None, None, None), kshp=(32, 3, 3, 3), filter_dilation=(1, 1)}(convolution2d_input_1, HostFromGpu.0)
Toposort index: 33
Inputs types: [TensorType(float32, 4D), TensorType(float32, 4D)]
Inputs shapes: [(2L, 3L, 150L, 150L), (32L, 1L, 3L, 3L)]
Inputs strides: [(270000L, 90000L, 600L, 4L), (36L, 36L, 12L, 4L)]
Inputs values: ['not shown', 'not shown']
Inputs type_num: [11, 11]
Outputs clients: [[Elemwise{add,no_inplace}(AbstractConv2d{border_mode='valid', subsample=(1, 1), filter_flip=True, imshp=(None, None, None, None), kshp=(32, 3, 3, 3), filter_dilation=(1, 1)}.0, Reshape{4}.0)]]

Backtrace when the node is created(use Theano flag traceback.limit=N to make it longer):
  File ""C:/Users/byoru/PycharmProjects/MailLession/FirstNN/Canvas.py"", line 11, in <module>
    model.add(Convolution2D(32, 3, 3, input_shape=(3, 150, 150)))
  File ""c:\program files (x86)\microsoft visual studio 12.0\vc\theano\keras\keras\models.py"", line 275, in add
    layer.create_input_layer(batch_input_shape, input_dtype)
  File ""c:\program files (x86)\microsoft visual studio 12.0\vc\theano\keras\keras\engine\topology.py"", line 367, in create_input_layer
    self(x)
  File ""c:\program files (x86)\microsoft visual studio 12.0\vc\theano\keras\keras\engine\topology.py"", line 511, in **call**
    self.add_inbound_node(inbound_layers, node_indices, tensor_indices)
  File ""c:\program files (x86)\microsoft visual studio 12.0\vc\theano\keras\keras\engine\topology.py"", line 569, in add_inbound_node
    Node.create_node(self, inbound_layers, node_indices, tensor_indices)
  File ""c:\program files (x86)\microsoft visual studio 12.0\vc\theano\keras\keras\engine\topology.py"", line 150, in create_node
    output_tensors = to_list(outbound_layer.call(input_tensors[0], mask=input_masks[0]))
  File ""c:\program files (x86)\microsoft visual studio 12.0\vc\theano\keras\keras\layers\convolutional.py"", line 353, in call
    filter_shape=self.W_shape)
  File ""c:\program files (x86)\microsoft visual studio 12.0\vc\theano\keras\keras\backend\theano_backend.py"", line 1073, in conv2d
    filter_shape=filter_shape)

Debugprint of the apply node: 
AbstractConv2d{border_mode='valid', subsample=(1, 1), filter_flip=True, imshp=(None, None, None, None), kshp=(32, 3, 3, 3), filter_dilation=(1, 1)} [id A] <TensorType(float32, 4D)> ''  
 |convolution2d_input_1 [id B] <TensorType(float32, 4D)>
 |HostFromGpu [id C] <TensorType(float32, 4D)> ''  
   |convolution2d_1_W [id D] <CudaNdarrayType(float32, 4D)>

Storage map footprint:
- dense_1_W, Shared Input, Shape: (18496, 64), ElemSize: 4 Byte(s), TotalSize: 4734976 Byte(s)
- convolution2d_input_1, Input, Shape: (2L, 3L, 150L, 150L), ElemSize: 4 Byte(s), TotalSize: 540000 Byte(s)
- <CudaNdarrayType(float32, vector)>, Shared Input, Shape: (92160,), ElemSize: 4 Byte(s), TotalSize: 368640 Byte(s)
- <CudaNdarrayType(float32, vector)>, Shared Input, Shape: (92160,), ElemSize: 4 Byte(s), TotalSize: 368640 Byte(s)
- <CudaNdarrayType(float32, vector)>, Shared Input, Shape: (92160,), ElemSize: 4 Byte(s), TotalSize: 368640 Byte(s)
- convolution2d_3_W, Shared Input, Shape: (64, 32, 3, 3), ElemSize: 4 Byte(s), TotalSize: 73728 Byte(s)
- convolution2d_2_W, Shared Input, Shape: (32, 32, 3, 3), ElemSize: 4 Byte(s), TotalSize: 36864 Byte(s)
- HostFromGpu.0, Shape: (32L, 1L, 3L, 3L), ElemSize: 4 Byte(s), TotalSize: 1152 Byte(s)
- convolution2d_1_W, Shared Input, Shape: (32, 1, 3, 3), ElemSize: 4 Byte(s), TotalSize: 1152 Byte(s)
- convolution2d_3_b, Shared Input, Shape: (64,), ElemSize: 4 Byte(s), TotalSize: 256 Byte(s)
- dense_1_b, Shared Input, Shape: (64,), ElemSize: 4 Byte(s), TotalSize: 256 Byte(s)
- dense_2_W, Shared Input, Shape: (64, 1), ElemSize: 4 Byte(s), TotalSize: 256 Byte(s)
- convolution2d_1_b, Shared Input, Shape: (32,), ElemSize: 4 Byte(s), TotalSize: 128 Byte(s)
- convolution2d_2_b, Shared Input, Shape: (32,), ElemSize: 4 Byte(s), TotalSize: 128 Byte(s)
- TensorConstant{[ 1 32  1  1]}, Shape: (4L,), ElemSize: 4 Byte(s), TotalSize: 16 Byte(s)
- TensorConstant{[ 1 64  1  1]}, Shape: (4L,), ElemSize: 4 Byte(s), TotalSize: 16 Byte(s)
- TensorConstant{[ 1 32  1  1]}, Shape: (4L,), ElemSize: 4 Byte(s), TotalSize: 16 Byte(s)
- Constant{0}, Shape: (), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)
- Constant{0}, Shape: (), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)
- TensorConstant{0.5}, Shape: (), ElemSize: 4 Byte(s), TotalSize: 4.0 Byte(s)
- dense_2_b, Shared Input, Shape: (1,), ElemSize: 4 Byte(s), TotalSize: 4 Byte(s)
- TensorConstant{0.5}, Shape: (), ElemSize: 4 Byte(s), TotalSize: 4.0 Byte(s)
- DimShuffle{x,x,x,x}.0, Shape: (1L, 1L, 1L, 1L), ElemSize: 4 Byte(s), TotalSize: 4 Byte(s)
- TensorConstant{0.800000011921}, Shape: (), ElemSize: 4 Byte(s), TotalSize: 4.0 Byte(s)
- TensorConstant{0.800000011921}, Shape: (), ElemSize: 4 Byte(s), TotalSize: 4.0 Byte(s)
- TensorConstant{0.5}, Shape: (), ElemSize: 4 Byte(s), TotalSize: 4.0 Byte(s)
- DimShuffle{x,x}.0, Shape: (1L, 1L), ElemSize: 4 Byte(s), TotalSize: 4 Byte(s)
- TensorConstant{0.5}, Shape: (), ElemSize: 4 Byte(s), TotalSize: 4.0 Byte(s)
- TensorConstant{0.5}, Shape: (), ElemSize: 4 Byte(s), TotalSize: 4.0 Byte(s)
- TensorConstant{1.0}, Shape: (), ElemSize: 4 Byte(s), TotalSize: 4.0 Byte(s)
- TensorConstant{0.5}, Shape: (), ElemSize: 4 Byte(s), TotalSize: 4.0 Byte(s)
- DimShuffle{x,x,x,x}.0, Shape: (1L, 1L, 1L, 1L), ElemSize: 4 Byte(s), TotalSize: 4 Byte(s)
- DimShuffle{x,x,x,x}.0, Shape: (1L, 1L, 1L, 1L), ElemSize: 4 Byte(s), TotalSize: 4 Byte(s)
- TensorConstant{0.800000011921}, Shape: (), ElemSize: 4 Byte(s), TotalSize: 4.0 Byte(s)
- TensorConstant{0.800000011921}, Shape: (), ElemSize: 4 Byte(s), TotalSize: 4.0 Byte(s)
- TensorConstant{0.0}, Shape: (), ElemSize: 4 Byte(s), TotalSize: 4.0 Byte(s)
- DimShuffle{x,x,x,x}.0, Shape: (1L, 1L, 1L, 1L), ElemSize: 1 Byte(s), TotalSize: 1 Byte(s)
- DimShuffle{x,x,x,x}.0, Shape: (1L, 1L, 1L, 1L), ElemSize: 1 Byte(s), TotalSize: 1 Byte(s)
- DimShuffle{x,x}.0, Shape: (1L, 1L), ElemSize: 1 Byte(s), TotalSize: 1 Byte(s)
- keras_learning_phase, Input, Shape: (), ElemSize: 1 Byte(s), TotalSize: 1.0 Byte(s)
  TotalSize: 6494952.0 Byte(s) 0.006 GB
  TotalSize inputs: 6493781.0 Byte(s) 0.006 GB

---

---------- Full Code ----------------------------
import numpy as np
import os
from PIL import Image
from keras.models import Sequential
from keras.layers import Convolution2D, MaxPooling2D
from keras.layers import Activation, Dropout, Flatten, Dense

from keras.preprocessing.image import ImageDataGenerator

model = Sequential()
model.add(Convolution2D(32, 3, 3, input_shape=(3, 150, 150)))
model.add(Activation('relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))

model.add(Convolution2D(32, 3, 3))
model.add(Activation('relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Dropout(0.2))

model.add(Convolution2D(64, 3, 3))
model.add(Activation('relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Dropout(0.2))

model.add(Flatten())  # this converts our 3D feature maps to 1D feature vectors
model.add(Dense(64))
model.add(Activation('relu'))
model.add(Dropout(0.5))
model.add(Dense(1))
model.add(Activation('sigmoid'))

model.load_weights('weight1.h5')

model.compile(loss='binary_crossentropy',
              optimizer='rmsprop',
              metrics=['accuracy'])
path = 'test'
testarr = []

for item in os.listdir(path):
    imgpath = path + '\' + item
    img = Image.open(imgpath)
    resizedimg = img.resize((150,150),Image.ANTIALIAS)
    data = np.array( resizedimg )
    data2 = data.transpose(2,0,1)
    testarr.append(data2)
testarrnp = np.asarray(testarr)

print testarrnp.shape
output = model.predict(testarrnp, batch_size=2,verbose=1)
print output
",0,Theanio Flag Error when  Predict on CNN ,"Theanio Flag Error when  Predict on CNN  ## Error Message is below (Full Code  is attached)

Using Theano backend.
Using gpu device 0: GeForce GTX 1060 3GB (CNMeM is enabled with initial size: 81.0% of memory, cuDNN 5005)
(6L, 3L, 150L, 150L) <- this is data for predict

Traceback (most recent call last):
  File ""C:\Program Files (x86)\JetBrains\PyCharm Edu 2.0.4\helpers\pydev\pydevd.py"", line 2411, in <module>
    globals = debugger.run(setup['file'], None, None, is_module)
  File ""C:\Program Files (x86)\JetBrains\PyCharm Edu 2.0.4\helpers\pydev\pydevd.py"", line 1802, in run
    launch(file, globals, locals)  # execute the script
  File ""C:/Users/byoru/PycharmProjects/MailLession/FirstNN/Canvas.py"", line 50, in <module>
    output = model.predict(testarrnp, batch_size=2,verbose=1)
  File ""c:\program files (x86)\microsoft visual studio 12.0\vc\theano\keras\keras\models.py"", line 664, in predict
    return self.model.predict(x, batch_size=batch_size, verbose=verbose)
  File ""c:\program files (x86)\microsoft visual studio 12.0\vc\theano\keras\keras\engine\training.py"", line 1180, in predict
    batch_size=batch_size, verbose=verbose)
  File ""c:\program files (x86)\microsoft visual studio 12.0\vc\theano\keras\keras\engine\training.py"", line 879, in _predict_loop
    batch_outs = f(ins_batch)
  File ""c:\program files (x86)\microsoft visual studio 12.0\vc\theano\keras\keras\backend\theano_backend.py"", line 655, in __call__
    return self.function(*inputs)
  File ""c:\program files (x86)\microsoft visual studio 12.0\vc\theano\theano\compile\function_module.py"", line 879, in **call**
    storage_map=getattr(self.fn, 'storage_map', None))
  File ""c:\program files (x86)\microsoft visual studio 12.0\vc\theano\theano\gof\link.py"", line 325, in raise_with_op
    reraise(exc_type, exc_value, exc_trace)
  File ""c:\program files (x86)\microsoft visual studio 12.0\vc\theano\theano\compile\function_module.py"", line 866, in **call**
    self.fn() if output_subset is None else\
  File ""c:\program files (x86)\microsoft visual studio 12.0\vc\theano\theano\gof\op.py"", line 908, in rval
    r = p(n, [x[0] for x in i], o)
  File ""c:\program files (x86)\microsoft visual studio 12.0\vc\theano\theano\tensor\nnet\abstract_conv.py"", line 848, in perform
    conv_out = self.conv2d(img, kern, mode=""valid"", dilation=self.filter_dilation)
  File ""c:\program files (x86)\microsoft visual studio 12.0\vc\theano\theano\tensor\nnet\abstract_conv.py"", line 775, in conv2d
    dilated_kern[n, im0, ...],
IndexError: index 1 is out of bounds for axis 1 with size 1
Apply node that caused the error: AbstractConv2d{border_mode='valid', subsample=(1, 1), filter_flip=True, imshp=(None, None, None, None), kshp=(32, 3, 3, 3), filter_dilation=(1, 1)}(convolution2d_input_1, HostFromGpu.0)
Toposort index: 33
Inputs types: [TensorType(float32, 4D), TensorType(float32, 4D)]
Inputs shapes: [(2L, 3L, 150L, 150L), (32L, 1L, 3L, 3L)]
Inputs strides: [(270000L, 90000L, 600L, 4L), (36L, 36L, 12L, 4L)]
Inputs values: ['not shown', 'not shown']
Inputs type_num: [11, 11]
Outputs clients: [[Elemwise{add,no_inplace}(AbstractConv2d{border_mode='valid', subsample=(1, 1), filter_flip=True, imshp=(None, None, None, None), kshp=(32, 3, 3, 3), filter_dilation=(1, 1)}.0, Reshape{4}.0)]]

Backtrace when the node is created(use Theano flag traceback.limit=N to make it longer):
  File ""C:/Users/byoru/PycharmProjects/MailLession/FirstNN/Canvas.py"", line 11, in <module>
    model.add(Convolution2D(32, 3, 3, input_shape=(3, 150, 150)))
  File ""c:\program files (x86)\microsoft visual studio 12.0\vc\theano\keras\keras\models.py"", line 275, in add
    layer.create_input_layer(batch_input_shape, input_dtype)
  File ""c:\program files (x86)\microsoft visual studio 12.0\vc\theano\keras\keras\engine\topology.py"", line 367, in create_input_layer
    self(x)
  File ""c:\program files (x86)\microsoft visual studio 12.0\vc\theano\keras\keras\engine\topology.py"", line 511, in **call**
    self.add_inbound_node(inbound_layers, node_indices, tensor_indices)
  File ""c:\program files (x86)\microsoft visual studio 12.0\vc\theano\keras\keras\engine\topology.py"", line 569, in add_inbound_node
    Node.create_node(self, inbound_layers, node_indices, tensor_indices)
  File ""c:\program files (x86)\microsoft visual studio 12.0\vc\theano\keras\keras\engine\topology.py"", line 150, in create_node
    output_tensors = to_list(outbound_layer.call(input_tensors[0], mask=input_masks[0]))
  File ""c:\program files (x86)\microsoft visual studio 12.0\vc\theano\keras\keras\layers\convolutional.py"", line 353, in call
    filter_shape=self.W_shape)
  File ""c:\program files (x86)\microsoft visual studio 12.0\vc\theano\keras\keras\backend\theano_backend.py"", line 1073, in conv2d
    filter_shape=filter_shape)

Debugprint of the apply node: 
AbstractConv2d{border_mode='valid', subsample=(1, 1), filter_flip=True, imshp=(None, None, None, None), kshp=(32, 3, 3, 3), filter_dilation=(1, 1)} [id A] <TensorType(float32, 4D)> ''  
 |convolution2d_input_1 [id B] <TensorType(float32, 4D)>
 |HostFromGpu [id C] <TensorType(float32, 4D)> ''  
   |convolution2d_1_W [id D] <CudaNdarrayType(float32, 4D)>

Storage map footprint:
- dense_1_W, Shared Input, Shape: (18496, 64), ElemSize: 4 Byte(s), TotalSize: 4734976 Byte(s)
- convolution2d_input_1, Input, Shape: (2L, 3L, 150L, 150L), ElemSize: 4 Byte(s), TotalSize: 540000 Byte(s)
- <CudaNdarrayType(float32, vector)>, Shared Input, Shape: (92160,), ElemSize: 4 Byte(s), TotalSize: 368640 Byte(s)
- <CudaNdarrayType(float32, vector)>, Shared Input, Shape: (92160,), ElemSize: 4 Byte(s), TotalSize: 368640 Byte(s)
- <CudaNdarrayType(float32, vector)>, Shared Input, Shape: (92160,), ElemSize: 4 Byte(s), TotalSize: 368640 Byte(s)
- convolution2d_3_W, Shared Input, Shape: (64, 32, 3, 3), ElemSize: 4 Byte(s), TotalSize: 73728 Byte(s)
- convolution2d_2_W, Shared Input, Shape: (32, 32, 3, 3), ElemSize: 4 Byte(s), TotalSize: 36864 Byte(s)
- HostFromGpu.0, Shape: (32L, 1L, 3L, 3L), ElemSize: 4 Byte(s), TotalSize: 1152 Byte(s)
- convolution2d_1_W, Shared Input, Shape: (32, 1, 3, 3), ElemSize: 4 Byte(s), TotalSize: 1152 Byte(s)
- convolution2d_3_b, Shared Input, Shape: (64,), ElemSize: 4 Byte(s), TotalSize: 256 Byte(s)
- dense_1_b, Shared Input, Shape: (64,), ElemSize: 4 Byte(s), TotalSize: 256 Byte(s)
- dense_2_W, Shared Input, Shape: (64, 1), ElemSize: 4 Byte(s), TotalSize: 256 Byte(s)
- convolution2d_1_b, Shared Input, Shape: (32,), ElemSize: 4 Byte(s), TotalSize: 128 Byte(s)
- convolution2d_2_b, Shared Input, Shape: (32,), ElemSize: 4 Byte(s), TotalSize: 128 Byte(s)
- TensorConstant{[ 1 32  1  1]}, Shape: (4L,), ElemSize: 4 Byte(s), TotalSize: 16 Byte(s)
- TensorConstant{[ 1 64  1  1]}, Shape: (4L,), ElemSize: 4 Byte(s), TotalSize: 16 Byte(s)
- TensorConstant{[ 1 32  1  1]}, Shape: (4L,), ElemSize: 4 Byte(s), TotalSize: 16 Byte(s)
- Constant{0}, Shape: (), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)
- Constant{0}, Shape: (), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)
- TensorConstant{0.5}, Shape: (), ElemSize: 4 Byte(s), TotalSize: 4.0 Byte(s)
- dense_2_b, Shared Input, Shape: (1,), ElemSize: 4 Byte(s), TotalSize: 4 Byte(s)
- TensorConstant{0.5}, Shape: (), ElemSize: 4 Byte(s), TotalSize: 4.0 Byte(s)
- DimShuffle{x,x,x,x}.0, Shape: (1L, 1L, 1L, 1L), ElemSize: 4 Byte(s), TotalSize: 4 Byte(s)
- TensorConstant{0.800000011921}, Shape: (), ElemSize: 4 Byte(s), TotalSize: 4.0 Byte(s)
- TensorConstant{0.800000011921}, Shape: (), ElemSize: 4 Byte(s), TotalSize: 4.0 Byte(s)
- TensorConstant{0.5}, Shape: (), ElemSize: 4 Byte(s), TotalSize: 4.0 Byte(s)
- DimShuffle{x,x}.0, Shape: (1L, 1L), ElemSize: 4 Byte(s), TotalSize: 4 Byte(s)
- TensorConstant{0.5}, Shape: (), ElemSize: 4 Byte(s), TotalSize: 4.0 Byte(s)
- TensorConstant{0.5}, Shape: (), ElemSize: 4 Byte(s), TotalSize: 4.0 Byte(s)
- TensorConstant{1.0}, Shape: (), ElemSize: 4 Byte(s), TotalSize: 4.0 Byte(s)
- TensorConstant{0.5}, Shape: (), ElemSize: 4 Byte(s), TotalSize: 4.0 Byte(s)
- DimShuffle{x,x,x,x}.0, Shape: (1L, 1L, 1L, 1L), ElemSize: 4 Byte(s), TotalSize: 4 Byte(s)
- DimShuffle{x,x,x,x}.0, Shape: (1L, 1L, 1L, 1L), ElemSize: 4 Byte(s), TotalSize: 4 Byte(s)
- TensorConstant{0.800000011921}, Shape: (), ElemSize: 4 Byte(s), TotalSize: 4.0 Byte(s)
- TensorConstant{0.800000011921}, Shape: (), ElemSize: 4 Byte(s), TotalSize: 4.0 Byte(s)
- TensorConstant{0.0}, Shape: (), ElemSize: 4 Byte(s), TotalSize: 4.0 Byte(s)
- DimShuffle{x,x,x,x}.0, Shape: (1L, 1L, 1L, 1L), ElemSize: 1 Byte(s), TotalSize: 1 Byte(s)
- DimShuffle{x,x,x,x}.0, Shape: (1L, 1L, 1L, 1L), ElemSize: 1 Byte(s), TotalSize: 1 Byte(s)
- DimShuffle{x,x}.0, Shape: (1L, 1L), ElemSize: 1 Byte(s), TotalSize: 1 Byte(s)
- keras_learning_phase, Input, Shape: (), ElemSize: 1 Byte(s), TotalSize: 1.0 Byte(s)
  TotalSize: 6494952.0 Byte(s) 0.006 GB
  TotalSize inputs: 6493781.0 Byte(s) 0.006 GB

---

---------- Full Code ----------------------------
import numpy as np
import os
from PIL import Image
from keras.models import Sequential
from keras.layers import Convolution2D, MaxPooling2D
from keras.layers import Activation, Dropout, Flatten, Dense

from keras.preprocessing.image import ImageDataGenerator

model = Sequential()
model.add(Convolution2D(32, 3, 3, input_shape=(3, 150, 150)))
model.add(Activation('relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))

model.add(Convolution2D(32, 3, 3))
model.add(Activation('relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Dropout(0.2))

model.add(Convolution2D(64, 3, 3))
model.add(Activation('relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Dropout(0.2))

model.add(Flatten())  # this converts our 3D feature maps to 1D feature vectors
model.add(Dense(64))
model.add(Activation('relu'))
model.add(Dropout(0.5))
model.add(Dense(1))
model.add(Activation('sigmoid'))

model.load_weights('weight1.h5')

model.compile(loss='binary_crossentropy',
              optimizer='rmsprop',
              metrics=['accuracy'])
path = 'test'
testarr = []

for item in os.listdir(path):
    imgpath = path + '\' + item
    img = Image.open(imgpath)
    resizedimg = img.resize((150,150),Image.ANTIALIAS)
    data = np.array( resizedimg )
    data2 = data.transpose(2,0,1)
    testarr.append(data2)
testarrnp = np.asarray(testarr)

print testarrnp.shape
output = model.predict(testarrnp, batch_size=2,verbose=1)
print output
"
keras,6129,"Keras throws an error when the first layer is an  layer with  (this is a pretty unusual use case, but it should work). This seems to be because the definition of that layer is simply:

    def linear(x):
        return x

So the topology is not properly created.
Here's a simple script to reproduce the error:


	from keras.models import Sequential
	from keras.layers import Dense, Activation
	import numpy as np

	def build_net_bug(activation):
		model = Sequential()
		model.add(Activation(activation,input_shape=(12,)))
		model.add(Dense(2))
		model.compile(loss='categorical_crossentropy', optimizer='sgd')
		return model

	model = build_net_bug('linear')
	model.train_on_batch(np.zeros((32,12)),np.zeros((32,2)))
	print('Ok')

Producted the following error:

    /path/experimental/keras/keras/engine/topology.py:1516: UserWarning: Model inputs must come from a Keras Input layer, they cannot be the output of a previous non-Input layer. Here, a tensor specified as input to ""sequential_1_model"" was not an Input tensor, it was generated by layer activation_1.
    Note that input tensors are instantiated via .
    The tensor that caused the issue was: activation_1_input:0
      str(x.name))
    Traceback (most recent call last):
      File ""test_keras_bug_2.py"", line 12, in <module>
        model = build_net_bug('linear')
      File ""test_keras_bug_2.py"", line 9, in build_net_bug
        model.compile(loss='categorical_crossentropy', optimizer='sgd')
      File ""/path/experimental/keras/keras/models.py"", line 761, in compile
        self.build()
      File ""/path/experimental/keras/keras/models.py"", line 520, in build
        name=self.name + '_model')
      File ""/path/experimental/keras/keras/legacy/interfaces.py"", line 88, in wrapper
        return func(*args, **kwargs)
      File ""/path/experimental/keras/keras/engine/topology.py"", line 1569, in __init__
        if layer.is_placeholder:
    AttributeError: 'Activation' object has no attribute 'is_placeholder'


Replacting  with any other activation fixes the issue.
Any suggestions on the best way to fix this (preferably without impacting the performance of  ? ",0,Bug: error when using Activation('linear') as first layer,"Bug: error when using Activation('linear') as first layer Keras throws an error when the first layer is an  layer with  (this is a pretty unusual use case, but it should work). This seems to be because the definition of that layer is simply:

    def linear(x):
        return x

So the topology is not properly created.
Here's a simple script to reproduce the error:


	from keras.models import Sequential
	from keras.layers import Dense, Activation
	import numpy as np

	def build_net_bug(activation):
		model = Sequential()
		model.add(Activation(activation,input_shape=(12,)))
		model.add(Dense(2))
		model.compile(loss='categorical_crossentropy', optimizer='sgd')
		return model

	model = build_net_bug('linear')
	model.train_on_batch(np.zeros((32,12)),np.zeros((32,2)))
	print('Ok')

Producted the following error:

    /path/experimental/keras/keras/engine/topology.py:1516: UserWarning: Model inputs must come from a Keras Input layer, they cannot be the output of a previous non-Input layer. Here, a tensor specified as input to ""sequential_1_model"" was not an Input tensor, it was generated by layer activation_1.
    Note that input tensors are instantiated via .
    The tensor that caused the issue was: activation_1_input:0
      str(x.name))
    Traceback (most recent call last):
      File ""test_keras_bug_2.py"", line 12, in <module>
        model = build_net_bug('linear')
      File ""test_keras_bug_2.py"", line 9, in build_net_bug
        model.compile(loss='categorical_crossentropy', optimizer='sgd')
      File ""/path/experimental/keras/keras/models.py"", line 761, in compile
        self.build()
      File ""/path/experimental/keras/keras/models.py"", line 520, in build
        name=self.name + '_model')
      File ""/path/experimental/keras/keras/legacy/interfaces.py"", line 88, in wrapper
        return func(*args, **kwargs)
      File ""/path/experimental/keras/keras/engine/topology.py"", line 1569, in __init__
        if layer.is_placeholder:
    AttributeError: 'Activation' object has no attribute 'is_placeholder'


Replacting  with any other activation fixes the issue.
Any suggestions on the best way to fix this (preferably without impacting the performance of  ? "
keras,2554,"It was a very useful feature. Why was it removed? Could it be readded?
",0,Multiple workers gone from fit_generator,"Multiple workers gone from fit_generator It was a very useful feature. Why was it removed? Could it be readded?
"
keras,2028,"Hi,everybody. 
I have a convolution network, i want to use an autoencode above the convolution network out, in order to learn the condensed features.

the convolution part is below, how should i do next?
### covolution part

convolutionpart=Sequential()
convolutionpart.add(Convolution1d(100,3,input_shape=(trian[1].shape,300)))
convolutionpart.add(MaxPooling1D(pool_length=4))
### autoencoder here
",0,How to build autoencode on other layers?,"How to build autoencode on other layers? Hi,everybody. 
I have a convolution network, i want to use an autoencode above the convolution network out, in order to learn the condensed features.

the convolution part is below, how should i do next?
### covolution part

convolutionpart=Sequential()
convolutionpart.add(Convolution1d(100,3,input_shape=(trian[1].shape,300)))
convolutionpart.add(MaxPooling1D(pool_length=4))
### autoencoder here
"
keras,1614,"Dear all,

I would like to use ImageDataGenerator with a regression problem (detect facial key points - https://www.kaggle.com/c/facial-keypoints-detection). By looking at the code I've got the feeling that_ y_ value is not modified when images are transformed. 
Is that true ? 
If so are you planning to support a kind of regression mode in parallel to the classification one 
",0,ImageDataGenerator with regression problem,"ImageDataGenerator with regression problem Dear all,

I would like to use ImageDataGenerator with a regression problem (detect facial key points - https://www.kaggle.com/c/facial-keypoints-detection). By looking at the code I've got the feeling that_ y_ value is not modified when images are transformed. 
Is that true ? 
If so are you planning to support a kind of regression mode in parallel to the classification one 
"
keras,6213,"Keras = 2.0.2
TensorFlow = 1.0.0
Python3
Mac OSX

Error:

Using tensorflow backend here is my network:

     convolutionInput = Input(shape=(1, maxRow, cols), name='convolutional_input')
     x = Conv2D(32, (3, cols), input_shape=(1, maxRow, cols), data_format='channels_first')(convolutionInput)
     x = MaxPooling2D(pool_size=(2, 1))(x)
     x = Dropout(.5)(x)
     convolutionOutput = Flatten()(x)
     additionalInput = Input(shape=(1,), name='additional_input')
     x = Concatenate([convolutionOutput, additionalInput], axis=1)
     x = Dense(64, activation='relu')(x)
     x = Dense(64, activation='relu')(x)
     finalOutput = Dense(2, activation='softmax')(x)
     convoNet = Model(inputs=[convolutionInput, additionalInput], outputs=finalOutput)
     convoNet.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])
     convoNet.fit(x={'convolutional_input': trainingSet[0], 'additional_input': trainingSet[1]}, y=trainLabels, epochs=20, batch_size=10)

And I get this error:

      x = Concatenate([convolutionOutput, additionalInput], axis=1)
     TypeError: __init__() got multiple values for argument 'axis'

I also tried not including the axis keyword argument at all and got this error:

     Traceback (most recent call last):
       File ""/Users/bl755p/Documents/WRT_NLP.py"", line 683, in <module>
         x = Dense(64, activation='relu')(x)
       File ""/Users/bl755p/anaconda/envs/ATT_NLP-Keras2/lib/python3.5/sitepackages/keras/engine/topology.py"", line 511, in __call__
         self.assert_input_compatibility(inputs)
       File ""/Users/bl755p/anaconda/envs/ATT_NLP-Keras2/lib/python3.5/site-packages/keras/engine/topology.py"", line 423, in assert_input_compatibility
         ndim = K.ndim(x)
       File ""/Users/bl755p/anaconda/envs/ATT_NLP-Keras2/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py"", line 437, in ndim
         dims = x.get_shape()._dims
     AttributeError: 'Concatenate' object has no attribute 'get_shape'
",0,Concatenate Layer Errors,"Concatenate Layer Errors Keras = 2.0.2
TensorFlow = 1.0.0
Python3
Mac OSX

Error:

Using tensorflow backend here is my network:

     convolutionInput = Input(shape=(1, maxRow, cols), name='convolutional_input')
     x = Conv2D(32, (3, cols), input_shape=(1, maxRow, cols), data_format='channels_first')(convolutionInput)
     x = MaxPooling2D(pool_size=(2, 1))(x)
     x = Dropout(.5)(x)
     convolutionOutput = Flatten()(x)
     additionalInput = Input(shape=(1,), name='additional_input')
     x = Concatenate([convolutionOutput, additionalInput], axis=1)
     x = Dense(64, activation='relu')(x)
     x = Dense(64, activation='relu')(x)
     finalOutput = Dense(2, activation='softmax')(x)
     convoNet = Model(inputs=[convolutionInput, additionalInput], outputs=finalOutput)
     convoNet.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])
     convoNet.fit(x={'convolutional_input': trainingSet[0], 'additional_input': trainingSet[1]}, y=trainLabels, epochs=20, batch_size=10)

And I get this error:

      x = Concatenate([convolutionOutput, additionalInput], axis=1)
     TypeError: __init__() got multiple values for argument 'axis'

I also tried not including the axis keyword argument at all and got this error:

     Traceback (most recent call last):
       File ""/Users/bl755p/Documents/WRT_NLP.py"", line 683, in <module>
         x = Dense(64, activation='relu')(x)
       File ""/Users/bl755p/anaconda/envs/ATT_NLP-Keras2/lib/python3.5/sitepackages/keras/engine/topology.py"", line 511, in __call__
         self.assert_input_compatibility(inputs)
       File ""/Users/bl755p/anaconda/envs/ATT_NLP-Keras2/lib/python3.5/site-packages/keras/engine/topology.py"", line 423, in assert_input_compatibility
         ndim = K.ndim(x)
       File ""/Users/bl755p/anaconda/envs/ATT_NLP-Keras2/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py"", line 437, in ndim
         dims = x.get_shape()._dims
     AttributeError: 'Concatenate' object has no attribute 'get_shape'
"
keras,7197,"A new paper from [Lee et al.](http://www.kentonl.com/pub/llz.2017.pdf) details a new, simplified recurrent neural network dubbed the ""Recurrent Additive Network."" It seems to do well on language modeling tasks when compared to the LSTM, despite using simpler computation with less parameters. This model would be easy to implement in Keras as a subclass of Recurrent, and I'd be happy to do it. However, my question is: is this something that would be pulled into the main repo? What is the standard policy for adding new sorts of RNNs?",0,Feature Wanted? Recurrent Additive Networks. I'd be happy to implement/PR but...,"Feature Wanted? Recurrent Additive Networks. I'd be happy to implement/PR but... A new paper from [Lee et al.](http://www.kentonl.com/pub/llz.2017.pdf) details a new, simplified recurrent neural network dubbed the ""Recurrent Additive Network."" It seems to do well on language modeling tasks when compared to the LSTM, despite using simpler computation with less parameters. This model would be easy to implement in Keras as a subclass of Recurrent, and I'd be happy to do it. However, my question is: is this something that would be pulled into the main repo? What is the standard policy for adding new sorts of RNNs?"
keras,3311,"Here is a modified version of the  file. It does downsampling in the input and upsampling in the output it compiles and converges as expected but the estimated output shape is wrong it says  instead of . There may be an error in the deconv layer output shape estimation

I'll look into this now @yaringal , but if you already have the solution let me know.

Copy paste the code below to reproduce results

Lambda(sampling)([z_mean, z_log_var])
",0,Error in deconv output shape estimation,"Error in deconv output shape estimation Here is a modified version of the  file. It does downsampling in the input and upsampling in the output it compiles and converges as expected but the estimated output shape is wrong it says  instead of . There may be an error in the deconv layer output shape estimation

I'll look into this now @yaringal , but if you already have the solution let me know.

Copy paste the code below to reproduce results

Lambda(sampling)([z_mean, z_log_var])
"
keras,2931,"I am running a deep net model on Keras code. The training model was running perfectly fne on image size 76_76, but it not working on image dimension 128_128.

I got the below error :

  <i>File ""/usr/local/lib/python2.7/dist-packages/theano/tensor/opt.py"", line 4262, in get_num_denum
    pairs = [self.get_num_denum(input2) for input2 in parent.inputs]
RuntimeError: maximum recursion depth exceeded

if isinstance(c, graph.Constant) and (len(c.clients) <= 1):
RuntimeError: ('maximum recursion depth exceeded while calling a Python object', 'Please, report this to theano-dev mailing list. As a temporary work around, you can raise Python stack limit with: import sys; sys.setrecursionlimit(10000)')</i>

I am using a GPU machine (NVIDIA GF110GL [Tesla M2075]) with fairly good configuration. I don't want to proceed with temporary work-around because in that case, the code is getting stuck before training. Kindly suggest some better way to handle this issue. 
",0,Maximum Recursion Depth error in Model.fit,"Maximum Recursion Depth error in Model.fit I am running a deep net model on Keras code. The training model was running perfectly fne on image size 76_76, but it not working on image dimension 128_128.

I got the below error :

  <i>File ""/usr/local/lib/python2.7/dist-packages/theano/tensor/opt.py"", line 4262, in get_num_denum
    pairs = [self.get_num_denum(input2) for input2 in parent.inputs]
RuntimeError: maximum recursion depth exceeded

if isinstance(c, graph.Constant) and (len(c.clients) <= 1):
RuntimeError: ('maximum recursion depth exceeded while calling a Python object', 'Please, report this to theano-dev mailing list. As a temporary work around, you can raise Python stack limit with: import sys; sys.setrecursionlimit(10000)')</i>

I am using a GPU machine (NVIDIA GF110GL [Tesla M2075]) with fairly good configuration. I don't want to proceed with temporary work-around because in that case, the code is getting stuck before training. Kindly suggest some better way to handle this issue. 
"
keras,3809,"I'm not quite understand how fit_generator works. In particular what does mean parameter samples_per_epoch? As described [here](https://github.com/fchollet/keras/issues/1627) samples_per_epoch = batch_size \* numbers of batches and fit_generator will read exactly samples_per_epoch samples from generator. And there also said that we control batch size by our generator yieldings. But here begins what I'm really missing.

Let's assume that generator return batch of 5 single samples and we want to train model on 10 batches per epoch so we have training on 50 single samples per epoch. But if we set samples_per_epochs with given formula we get 50 samples taken by fit_generator and each sample is a BATCH yielded by generator so we get 5*50=250 single samples.  I see that we can get samples number in a batch by dividing input length by length of input shape's zero axis. Therefore we can calculate how much times we need to ask for a batch. But it seems too intricately for me.

Ok. Then let's suppose that the formula is wrong and in this case we set samples_per_epoch to our batch_size. It looks more confident for me. Then generator will take exactly sample_per_epoch batches from generator. Again batch size in units of single samples can be computed by dividing.

So the question is what is actually right?
",0,How does fit_generator works,"How does fit_generator works I'm not quite understand how fit_generator works. In particular what does mean parameter samples_per_epoch? As described [here](https://github.com/fchollet/keras/issues/1627) samples_per_epoch = batch_size \* numbers of batches and fit_generator will read exactly samples_per_epoch samples from generator. And there also said that we control batch size by our generator yieldings. But here begins what I'm really missing.

Let's assume that generator return batch of 5 single samples and we want to train model on 10 batches per epoch so we have training on 50 single samples per epoch. But if we set samples_per_epochs with given formula we get 50 samples taken by fit_generator and each sample is a BATCH yielded by generator so we get 5*50=250 single samples.  I see that we can get samples number in a batch by dividing input length by length of input shape's zero axis. Therefore we can calculate how much times we need to ask for a batch. But it seems too intricately for me.

Ok. Then let's suppose that the formula is wrong and in this case we set samples_per_epoch to our batch_size. It looks more confident for me. Then generator will take exactly sample_per_epoch batches from generator. Again batch size in units of single samples can be computed by dividing.

So the question is what is actually right?
"
keras,6205,"I want use keras develop a Face Recognition project, I think maybe need Transfer Learning and have some way can use Transfer Learning on keras 2.0.2
thank you",0,How use Transfer Learning on keras 2.0.2,"How use Transfer Learning on keras 2.0.2 I want use keras develop a Face Recognition project, I think maybe need Transfer Learning and have some way can use Transfer Learning on keras 2.0.2
thank you"
keras,571,"I am running the keras eample-imdb. I am a fresh man, leaning DL not long. I like the keras framework. It looks so easy. Besides the question of title, I also want to know how to save the output of each layer. Thanks!
",0,How to save the loss and acc of each batch  to the file? And how to output the results of each layer?,"How to save the loss and acc of each batch  to the file? And how to output the results of each layer? I am running the keras eample-imdb. I am a fresh man, leaning DL not long. I like the keras framework. It looks so easy. Besides the question of title, I also want to know how to save the output of each layer. Thanks!
"
keras,1787,"

This page now returns a 404: https://github.com/fchollet/keras/blob/master/examples/skipgram_word_embeddings.py

Was this code taken out of keras, or just moved somewhere else?

Thanks,

Zach

---

Please make sure that the boxes below are checked before you submit your issue. Thank you!
- [x] Check that you are up-to-date with the master branch of Keras. You can update with:
  pip install git+git://github.com/fchollet/keras.git --upgrade --no-deps
- [x] If running on Theano, check that you are up-to-date with the master branch of Theano. You can update with:
  pip install git+git://github.com/Theano/Theano.git --upgrade --no-deps
- [x] Provide a link to a GitHub Gist of a Python script that can reproduce your issue (or just copy the script here if it is short).
",0,What happened to WordContextProduct?,"What happened to WordContextProduct? 

This page now returns a 404: https://github.com/fchollet/keras/blob/master/examples/skipgram_word_embeddings.py

Was this code taken out of keras, or just moved somewhere else?

Thanks,

Zach

---

Please make sure that the boxes below are checked before you submit your issue. Thank you!
- [x] Check that you are up-to-date with the master branch of Keras. You can update with:
  pip install git+git://github.com/fchollet/keras.git --upgrade --no-deps
- [x] If running on Theano, check that you are up-to-date with the master branch of Theano. You can update with:
  pip install git+git://github.com/Theano/Theano.git --upgrade --no-deps
- [x] Provide a link to a GitHub Gist of a Python script that can reproduce your issue (or just copy the script here if it is short).
"
keras,3310,"Currently I'm trying to use multiple vgg16 models from the keras model zoo to classify videos.
At this stage I'm taking only a few frames from each video to classify the whole video.
What I do is I take a VGG model, popout the last two layers and merge a few of these models togethers.

But this method isn't working right now, when I try to train the network. I get an error from Theano about disconnected inputs, These inputs are apparently the inputs that I popped off the vgg model. So I think that might be the problem.  Also I don't get this error if I don't load the weights for the vgg model.

Here is a gist that reproduces this error
https://gist.github.com/m1sk/b8ed6d43a5ea86ae51f193d5fc2c01b3

Also any advice with a better way to build this network will be appreciated.

Specs:
Latest Keras and Theano (from git) - updated today
Running on Windows 8.1 with Anaconda

The full text of the error:

> ---
> 
> DisconnectedInputError                    Traceback (most recent call last)
> <ipython-input-22-77cc237b2259> in <module>()
>      79 y_train = np.random.randint(2, size=examples)
>      80 
> ---> 81 merged_model.fit(x_train,y_train)
> 
> N:\Programs\Anaconda\lib\site-packages\keras-1.0.6-py2.7.egg\keras\models.pyc in fit(self, x, y, batch_size, nb_epoch, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, **kwargs)
>     430                               shuffle=shuffle,
>     431                               class_weight=class_weight,
> --> 432                               sample_weight=sample_weight)
>     433 
>     434     def evaluate(self, x, y, batch_size=32, verbose=1,
> 
> N:\Programs\Anaconda\lib\site-packages\keras-1.0.6-py2.7.egg\keras\engine\training.pyc in fit(self, x, y, batch_size, nb_epoch, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight)
>    1077         else:
>    1078             ins = x + y + sample_weights
> -> 1079         self._make_train_function()
>    1080         f = self.train_function
>    1081 
> 
> N:\Programs\Anaconda\lib\site-packages\keras-1.0.6-py2.7.egg\keras\engine\training.pyc in _make_train_function(self)
>     694             # get trainable weights
>     695             trainable_weights = collect_trainable_weights(self)
> --> 696             training_updates = self.optimizer.get_updates(trainable_weights, self.constraints, self.total_loss)
>     697             updates = self.updates + training_updates
>     698 
> 
> N:\Programs\Anaconda\lib\site-packages\keras-1.0.6-py2.7.egg\keras\optimizers.pyc in get_updates(self, params, constraints, loss)
>     119 
>     120     def get_updates(self, params, constraints, loss):
> --> 121         grads = self.get_gradients(loss, params)
>     122         lr = self.lr \* (1. / (1. + self.decay \* self.iterations))
>     123         self.updates = [K.update_add(self.iterations, 1)]
> 
> N:\Programs\Anaconda\lib\site-packages\keras-1.0.6-py2.7.egg\keras\optimizers.pyc in get_gradients(self, loss, params)
>      51 
>      52     def get_gradients(self, loss, params):
> ---> 53         grads = K.gradients(loss, params)
>      54         if hasattr(self, 'clipnorm') and self.clipnorm > 0:
>      55             norm = K.sqrt(sum([K.sum(K.square(g)) for g in grads]))
> 
> N:\Programs\Anaconda\lib\site-packages\keras-1.0.6-py2.7.egg\keras\backend\theano_backend.pyc in gradients(loss, variables)
>     655 
>     656 def gradients(loss, variables):
> --> 657     return T.grad(loss, variables)
>     658 
>     659 
> 
> N:\Programs\Anaconda\lib\site-packages\theano-0.9.0.dev2-py2.7.egg\theano\gradient.pyc in grad(cost, wrt, consider_constant, disconnected_inputs, add_names, known_grads, return_disconnected, null_gradients)
>     531         if elem not in var_to_app_to_idx and elem is not cost \
>     532                 and elem not in grad_dict:
> --> 533             handle_disconnected(elem)
>     534             grad_dict[elem] = disconnected_type()
>     535 
> 
> N:\Programs\Anaconda\lib\site-packages\theano-0.9.0.dev2-py2.7.egg\theano\gradient.pyc in handle_disconnected(var)
>     518             elif disconnected_inputs == 'raise':
>     519                 message = utils.get_variable_trace_string(var)
> --> 520                 raise DisconnectedInputError(message)
>     521             else:
>     522                 raise ValueError(""Invalid value for keyword ""
> 
> DisconnectedInputError:  
> Backtrace when that variable is created:
> 
>   File ""kerasmodelzoo\vgg16.py"", line 53, in model
>     vgg16_model.add(Dense(1000, activation='softmax'))
>   File ""N:\Programs\Anaconda\lib\site-packages\keras-1.0.6-py2.7.egg\keras\models.py"", line 146, in add
>     output_tensor = layer(self.outputs[0])
>   File ""N:\Programs\Anaconda\lib\site-packages\keras-1.0.6-py2.7.egg\keras\engine\topology.py"", line 458, in **call**
>     self.build(input_shapes[0])
>   File ""N:\Programs\Anaconda\lib\site-packages\keras-1.0.6-py2.7.egg\keras\layers\core.py"", line 604, in build
>     name='{}_W'.format(self.name))
>   File ""N:\Programs\Anaconda\lib\site-packages\keras-1.0.6-py2.7.egg\keras\initializations.py"", line 59, in glorot_uniform
>     return uniform(shape, s, name=name)
>   File ""N:\Programs\Anaconda\lib\site-packages\keras-1.0.6-py2.7.egg\keras\initializations.py"", line 32, in uniform
>     return K.random_uniform_variable(shape, -scale, scale, name=name)
>   File ""N:\Programs\Anaconda\lib\site-packages\keras-1.0.6-py2.7.egg\keras\backend\theano_backend.py"", line 111, in random_uniform_variable
>     dtype=dtype, name=name)
>   File ""N:\Programs\Anaconda\lib\site-packages\keras-1.0.6-py2.7.egg\keras\backend\theano_backend.py"", line 40, in variable
>     return theano.shared(value=value, name=name, strict=False)
",0,DisconnectedInputError: When Popping Layers and Adding on Top,"DisconnectedInputError: When Popping Layers and Adding on Top Currently I'm trying to use multiple vgg16 models from the keras model zoo to classify videos.
At this stage I'm taking only a few frames from each video to classify the whole video.
What I do is I take a VGG model, popout the last two layers and merge a few of these models togethers.

But this method isn't working right now, when I try to train the network. I get an error from Theano about disconnected inputs, These inputs are apparently the inputs that I popped off the vgg model. So I think that might be the problem.  Also I don't get this error if I don't load the weights for the vgg model.

Here is a gist that reproduces this error
https://gist.github.com/m1sk/b8ed6d43a5ea86ae51f193d5fc2c01b3

Also any advice with a better way to build this network will be appreciated.

Specs:
Latest Keras and Theano (from git) - updated today
Running on Windows 8.1 with Anaconda

The full text of the error:

> ---
> 
> DisconnectedInputError                    Traceback (most recent call last)
> <ipython-input-22-77cc237b2259> in <module>()
>      79 y_train = np.random.randint(2, size=examples)
>      80 
> ---> 81 merged_model.fit(x_train,y_train)
> 
> N:\Programs\Anaconda\lib\site-packages\keras-1.0.6-py2.7.egg\keras\models.pyc in fit(self, x, y, batch_size, nb_epoch, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, **kwargs)
>     430                               shuffle=shuffle,
>     431                               class_weight=class_weight,
> --> 432                               sample_weight=sample_weight)
>     433 
>     434     def evaluate(self, x, y, batch_size=32, verbose=1,
> 
> N:\Programs\Anaconda\lib\site-packages\keras-1.0.6-py2.7.egg\keras\engine\training.pyc in fit(self, x, y, batch_size, nb_epoch, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight)
>    1077         else:
>    1078             ins = x + y + sample_weights
> -> 1079         self._make_train_function()
>    1080         f = self.train_function
>    1081 
> 
> N:\Programs\Anaconda\lib\site-packages\keras-1.0.6-py2.7.egg\keras\engine\training.pyc in _make_train_function(self)
>     694             # get trainable weights
>     695             trainable_weights = collect_trainable_weights(self)
> --> 696             training_updates = self.optimizer.get_updates(trainable_weights, self.constraints, self.total_loss)
>     697             updates = self.updates + training_updates
>     698 
> 
> N:\Programs\Anaconda\lib\site-packages\keras-1.0.6-py2.7.egg\keras\optimizers.pyc in get_updates(self, params, constraints, loss)
>     119 
>     120     def get_updates(self, params, constraints, loss):
> --> 121         grads = self.get_gradients(loss, params)
>     122         lr = self.lr \* (1. / (1. + self.decay \* self.iterations))
>     123         self.updates = [K.update_add(self.iterations, 1)]
> 
> N:\Programs\Anaconda\lib\site-packages\keras-1.0.6-py2.7.egg\keras\optimizers.pyc in get_gradients(self, loss, params)
>      51 
>      52     def get_gradients(self, loss, params):
> ---> 53         grads = K.gradients(loss, params)
>      54         if hasattr(self, 'clipnorm') and self.clipnorm > 0:
>      55             norm = K.sqrt(sum([K.sum(K.square(g)) for g in grads]))
> 
> N:\Programs\Anaconda\lib\site-packages\keras-1.0.6-py2.7.egg\keras\backend\theano_backend.pyc in gradients(loss, variables)
>     655 
>     656 def gradients(loss, variables):
> --> 657     return T.grad(loss, variables)
>     658 
>     659 
> 
> N:\Programs\Anaconda\lib\site-packages\theano-0.9.0.dev2-py2.7.egg\theano\gradient.pyc in grad(cost, wrt, consider_constant, disconnected_inputs, add_names, known_grads, return_disconnected, null_gradients)
>     531         if elem not in var_to_app_to_idx and elem is not cost \
>     532                 and elem not in grad_dict:
> --> 533             handle_disconnected(elem)
>     534             grad_dict[elem] = disconnected_type()
>     535 
> 
> N:\Programs\Anaconda\lib\site-packages\theano-0.9.0.dev2-py2.7.egg\theano\gradient.pyc in handle_disconnected(var)
>     518             elif disconnected_inputs == 'raise':
>     519                 message = utils.get_variable_trace_string(var)
> --> 520                 raise DisconnectedInputError(message)
>     521             else:
>     522                 raise ValueError(""Invalid value for keyword ""
> 
> DisconnectedInputError:  
> Backtrace when that variable is created:
> 
>   File ""kerasmodelzoo\vgg16.py"", line 53, in model
>     vgg16_model.add(Dense(1000, activation='softmax'))
>   File ""N:\Programs\Anaconda\lib\site-packages\keras-1.0.6-py2.7.egg\keras\models.py"", line 146, in add
>     output_tensor = layer(self.outputs[0])
>   File ""N:\Programs\Anaconda\lib\site-packages\keras-1.0.6-py2.7.egg\keras\engine\topology.py"", line 458, in **call**
>     self.build(input_shapes[0])
>   File ""N:\Programs\Anaconda\lib\site-packages\keras-1.0.6-py2.7.egg\keras\layers\core.py"", line 604, in build
>     name='{}_W'.format(self.name))
>   File ""N:\Programs\Anaconda\lib\site-packages\keras-1.0.6-py2.7.egg\keras\initializations.py"", line 59, in glorot_uniform
>     return uniform(shape, s, name=name)
>   File ""N:\Programs\Anaconda\lib\site-packages\keras-1.0.6-py2.7.egg\keras\initializations.py"", line 32, in uniform
>     return K.random_uniform_variable(shape, -scale, scale, name=name)
>   File ""N:\Programs\Anaconda\lib\site-packages\keras-1.0.6-py2.7.egg\keras\backend\theano_backend.py"", line 111, in random_uniform_variable
>     dtype=dtype, name=name)
>   File ""N:\Programs\Anaconda\lib\site-packages\keras-1.0.6-py2.7.egg\keras\backend\theano_backend.py"", line 40, in variable
>     return theano.shared(value=value, name=name, strict=False)
"
keras,4239,"The following snippet:

    from keras.layers import Input, LSTM, Dense
    from keras.layers.wrappers import TimeDistributed
    from keras.models import Model

    batch_size = 32
    time_steps = 10
    input_dim = 5
    num_classes = 3
    hidden_size = 16
    inputs = Input(batch_shape=(batch_size, time_steps, input_dim))
    y = LSTM(hidden_size, return_sequences=True)(inputs)
    y = TimeDistributed(Dense(num_classes, activation='softmax'))(y)  # Exception raised here
    model = Model(input=inputs, output=y)
    model.compile(optimizer='rmsprop')

Raises the following exception in the indicated line when using TensorFlow backend:

    ValueError: The shape for while_1/Merge_2:0 is not an invariant for the loop. It enters
    the loop with shape (10, 16), but has shape (32, 3) after one iteration. Provide shape
    invariants using either the  argument of tf.while_loop or set_shape()
    on the loop variables.

This seems to be a compatibility issue with TensorFlow 0.11:
* TensorFlow 0.10rc0 + Keras 1.1.0: Works
* TensorFlow 0.11rc0 + Keras 1.1.0: Fails
* TensorFlow 0.11rc1 + Keras 1.1.0: Fails

I have tested this with Python 3 on Debian Jessie and Ubuntu 14.04.5 LTS.",0,Shape invariance error using `TimeDistributed` layers with TensorFlow,"Shape invariance error using `TimeDistributed` layers with TensorFlow The following snippet:

    from keras.layers import Input, LSTM, Dense
    from keras.layers.wrappers import TimeDistributed
    from keras.models import Model

    batch_size = 32
    time_steps = 10
    input_dim = 5
    num_classes = 3
    hidden_size = 16
    inputs = Input(batch_shape=(batch_size, time_steps, input_dim))
    y = LSTM(hidden_size, return_sequences=True)(inputs)
    y = TimeDistributed(Dense(num_classes, activation='softmax'))(y)  # Exception raised here
    model = Model(input=inputs, output=y)
    model.compile(optimizer='rmsprop')

Raises the following exception in the indicated line when using TensorFlow backend:

    ValueError: The shape for while_1/Merge_2:0 is not an invariant for the loop. It enters
    the loop with shape (10, 16), but has shape (32, 3) after one iteration. Provide shape
    invariants using either the  argument of tf.while_loop or set_shape()
    on the loop variables.

This seems to be a compatibility issue with TensorFlow 0.11:
* TensorFlow 0.10rc0 + Keras 1.1.0: Works
* TensorFlow 0.11rc0 + Keras 1.1.0: Fails
* TensorFlow 0.11rc1 + Keras 1.1.0: Fails

I have tested this with Python 3 on Debian Jessie and Ubuntu 14.04.5 LTS."
keras,1606,"I'm looking for a way to do something that seems conceptually simple, but I haven't found a way to do it in Keras yet. I have a CNN with dropout applied in various places in the network. After training, I'd like to compute the forward pass on some data and have the dropout mask applied such that each time I compute the forward pass a different dropout mask is applied. 

If I understand things correctly, this is what is done during training. However, if I use the standard model.predict(X_test) after training, no dropout is applied and the outputs are multiplied by the dropout probability, yielding a non-stochastic set of predictions. 

Does anyone know how I can achieve this? 
",0,Forward pass with dropout,"Forward pass with dropout I'm looking for a way to do something that seems conceptually simple, but I haven't found a way to do it in Keras yet. I have a CNN with dropout applied in various places in the network. After training, I'd like to compute the forward pass on some data and have the dropout mask applied such that each time I compute the forward pass a different dropout mask is applied. 

If I understand things correctly, this is what is done during training. However, if I use the standard model.predict(X_test) after training, no dropout is applied and the outputs are multiplied by the dropout probability, yielding a non-stochastic set of predictions. 

Does anyone know how I can achieve this? 
"
keras,1434,"when i try to run the code i my output is a pile of c++ code(1009 lines) in its first lines it says ""support code"" besides that i get another long error about g++ and gcc this is the error log(im using windows and i run the code in powershell) : 
[log.txt](https://github.com/fchollet/keras/files/83558/log.txt)

thank you.
",0,lstm_text_generation.py doesnt work,"lstm_text_generation.py doesnt work when i try to run the code i my output is a pile of c++ code(1009 lines) in its first lines it says ""support code"" besides that i get another long error about g++ and gcc this is the error log(im using windows and i run the code in powershell) : 
[log.txt](https://github.com/fchollet/keras/files/83558/log.txt)

thank you.
"
keras,1907,"Hi,
I'm wondering where i can find an explanation of why we need to apply pad_sentences to text for the embedding layer and why the 2D transformation ? Thanks!
",0,pad_sentences,"pad_sentences Hi,
I'm wondering where i can find an explanation of why we need to apply pad_sentences to text for the embedding layer and why the 2D transformation ? Thanks!
"
keras,4919,"This issue has been raised before in #953, I believe the fix c4f3155d192935c7cb659cac6d38c76b15ec971e did not solve the issue. 
Here is a code to reproduce:


",0,saving/loading models with frozen layers still not working with 1.2.0,"saving/loading models with frozen layers still not working with 1.2.0 This issue has been raised before in #953, I believe the fix c4f3155d192935c7cb659cac6d38c76b15ec971e did not solve the issue. 
Here is a code to reproduce:


"
keras,6631,"Hi, 

I've been attempting to implement the EntropySGD in this paper (https://arxiv.org/pdf/1611.01838.pdf). However the inner loop of the optimizer requires L gradient updates to estimate the exponential decayed mus which are used to update the parameters on one batch of data. It looks like  'get_updates' is called from training.py. My question is:

1. Is there a way to update the network params, compute a new loss and get new gradients on the same batch of data in the optimizer class to run the L iterations for the Langevin dynamics?
2. If not, can this be implemented in other ways or can this be added as a future feature?

Thanks",0,EntropySGD: gradient updates for Langevin dynamics,"EntropySGD: gradient updates for Langevin dynamics Hi, 

I've been attempting to implement the EntropySGD in this paper (https://arxiv.org/pdf/1611.01838.pdf). However the inner loop of the optimizer requires L gradient updates to estimate the exponential decayed mus which are used to update the parameters on one batch of data. It looks like  'get_updates' is called from training.py. My question is:

1. Is there a way to update the network params, compute a new loss and get new gradients on the same batch of data in the optimizer class to run the L iterations for the Langevin dynamics?
2. If not, can this be implemented in other ways or can this be added as a future feature?

Thanks"
keras,4857,"Currently, the callbacks  and  perform the same general function of carrying out some action after some number of epochs of no improvement on a monitored metric. These classes could be rewritten as subclasses of a more general class that monitors a quantity of interest and calls an  function once this quantity ceases to improve. This would make it easy to implement other callbacks that are dependent on training progress in the future.",0,Implement general callback to perform an action after some epochs of no improvement on a monitored metric,"Implement general callback to perform an action after some epochs of no improvement on a monitored metric Currently, the callbacks  and  perform the same general function of carrying out some action after some number of epochs of no improvement on a monitored metric. These classes could be rewritten as subclasses of a more general class that monitors a quantity of interest and calls an  function once this quantity ceases to improve. This would make it easy to implement other callbacks that are dependent on training progress in the future."
keras,6860,"Hi, I have more csv files, where each file contains a time-series for my forecasting problem.
The problem is thet whenever I fit the csv file for each file, the precedent fit will be,obviously, forget.
There is a method for don't forget it? I insert an image with my problem.
![problema](https://cloud.githubusercontent.com/assets/18617527/26780123/205af932-49e0-11e7-8e9b-e634c86f77c3.jpg)
My idea is using the LSTM, and for the testing I want to insert one single input for time, and predict the value",0,Training LSTM with more .csv files,"Training LSTM with more .csv files Hi, I have more csv files, where each file contains a time-series for my forecasting problem.
The problem is thet whenever I fit the csv file for each file, the precedent fit will be,obviously, forget.
There is a method for don't forget it? I insert an image with my problem.
![problema](https://cloud.githubusercontent.com/assets/18617527/26780123/205af932-49e0-11e7-8e9b-e634c86f77c3.jpg)
My idea is using the LSTM, and for the testing I want to insert one single input for time, and predict the value"
keras,379,"I'm building a network using the new Graph model which takes two sequences as inputs, feeds them through an RNN, concatenates the final output vectors, and passes that on to a dense layer. I expected that the input size of dense layer should be twice the output dimensionality of the RNNs but this doesn't seem to be working. 

Does anyone know what I may be doing wrong? 



The error that I get is:



It seems like the two RNN outputs are not actually getting merged into a 10 dimensional vector, is that expected?

_edit_: This error goes away (and the model trains successfully) when using the Theano CPU backend. 

_edit 2.0_: The error goes away even on the GPU if I switch the JZS1 RNN to an LSTM. 

_edit 3.0_ Actually, the error persists with LSTMs but only if I'm using floatX=32. Current hypothesis: merging of input variables doesn't get handled correctly when calling an external matrix multiply routine. 
",0,"merge_mode=""concat"" behaving unexpectedly? (with the GPU backend)","merge_mode=""concat"" behaving unexpectedly? (with the GPU backend) I'm building a network using the new Graph model which takes two sequences as inputs, feeds them through an RNN, concatenates the final output vectors, and passes that on to a dense layer. I expected that the input size of dense layer should be twice the output dimensionality of the RNNs but this doesn't seem to be working. 

Does anyone know what I may be doing wrong? 



The error that I get is:



It seems like the two RNN outputs are not actually getting merged into a 10 dimensional vector, is that expected?

_edit_: This error goes away (and the model trains successfully) when using the Theano CPU backend. 

_edit 2.0_: The error goes away even on the GPU if I switch the JZS1 RNN to an LSTM. 

_edit 3.0_ Actually, the error persists with LSTMs but only if I'm using floatX=32. Current hypothesis: merging of input variables doesn't get handled correctly when calling an external matrix multiply routine. 
"
keras,9116,"I am trying a simple model, which  consists of two separate inputs (each with two different embedded inputs). The problem is a toy scenario which forms the framework of a more complex system i need to create with multiple inputs and multple embeddings, however i need to get this basic system working. I have no problem using multiple embeddings in the one input case, it is only when i try using multiple inputs that i get the follow error:

> __main__:1: UserWarning: The  function is deprecated and will be removed after 08/2017. Use instead layers from , e.g. , , etc.
> /services/tools/anaconda3/4.0.0/lib/python3.5/site-packages/keras/legacy/layers.py:460: UserWarning: The  layer is deprecated and will be removed after 08/2017. Use instead layers from , e.g. , , etc.
>   name=name)
> Traceback (most recent call last):
> 
>   File ""<ipython-input-676-0f3f0da1e7a7>"", line 3, in <module>
>     model = Model(inputs=[[q1 ,q2],[q3 , q4]], outputs=MC_MergeOut)
> 
>   File ""/services/tools/anaconda3/4.0.0/lib/python3.5/site-packages/keras/legacy/interfaces.py"", line 88, in wrapper
>     return func(*args, **kwargs)
> 
>   File ""/services/tools/anaconda3/4.0.0/lib/python3.5/site-packages/keras/engine/topology.py"", line 1485, in __init__
>     inputs_set = set(self.inputs)
> 
> TypeError: unhashable type: 'list'

Has anyone tried multi-input and multiple embeddings for each input? Any help would be greatly appreciated as i'm on a time deadline. Thanks in advance

",0,Multiple Inputs with Multiple Embeddings (Not working),"Multiple Inputs with Multiple Embeddings (Not working) I am trying a simple model, which  consists of two separate inputs (each with two different embedded inputs). The problem is a toy scenario which forms the framework of a more complex system i need to create with multiple inputs and multple embeddings, however i need to get this basic system working. I have no problem using multiple embeddings in the one input case, it is only when i try using multiple inputs that i get the follow error:

> __main__:1: UserWarning: The  function is deprecated and will be removed after 08/2017. Use instead layers from , e.g. , , etc.
> /services/tools/anaconda3/4.0.0/lib/python3.5/site-packages/keras/legacy/layers.py:460: UserWarning: The  layer is deprecated and will be removed after 08/2017. Use instead layers from , e.g. , , etc.
>   name=name)
> Traceback (most recent call last):
> 
>   File ""<ipython-input-676-0f3f0da1e7a7>"", line 3, in <module>
>     model = Model(inputs=[[q1 ,q2],[q3 , q4]], outputs=MC_MergeOut)
> 
>   File ""/services/tools/anaconda3/4.0.0/lib/python3.5/site-packages/keras/legacy/interfaces.py"", line 88, in wrapper
>     return func(*args, **kwargs)
> 
>   File ""/services/tools/anaconda3/4.0.0/lib/python3.5/site-packages/keras/engine/topology.py"", line 1485, in __init__
>     inputs_set = set(self.inputs)
> 
> TypeError: unhashable type: 'list'

Has anyone tried multi-input and multiple embeddings for each input? Any help would be greatly appreciated as i'm on a time deadline. Thanks in advance

"
keras,1348,"Hi everyone,

I have a dataset with 24 inputs, and 1 output, which is categorical value.

So my X_train.shape is _(20000,24)_ and X_test.shape is _(5000,24)_ and the output should be only one 1 six values: a, b, c, d, e, f. So the Y_train.shape is _(20000,6)_.

Adapting from https://github.com/fchollet/keras/issues/579



but I met the problem



at line



Could you help to point out what I did wrong?

Thanks,
",0,Input shape error with ConvNet 1D,"Input shape error with ConvNet 1D Hi everyone,

I have a dataset with 24 inputs, and 1 output, which is categorical value.

So my X_train.shape is _(20000,24)_ and X_test.shape is _(5000,24)_ and the output should be only one 1 six values: a, b, c, d, e, f. So the Y_train.shape is _(20000,6)_.

Adapting from https://github.com/fchollet/keras/issues/579



but I met the problem



at line



Could you help to point out what I did wrong?

Thanks,
"
keras,1130,"After the fix for issue  #1125, I'm trying



but I get this error



This is the traceback:



Theano is up-to-date. Am I missing anything?
",0,Type error when stateful,"Type error when stateful After the fix for issue  #1125, I'm trying



but I get this error



This is the traceback:



Theano is up-to-date. Am I missing anything?
"
keras,1587,"I'm training a model with Theano/CUDA, and if I attempt to specify a large batch_size (1024 in my case), it reports an out of memory error, which is understandable. However, if I change it back to a size that previously worked (I'm doing it in a notebook), it will still be out of memory, as if it didn't attempt to free whatever it allocated for the previous attempt, so I'm forced to restart the Python process (and reload all data/recompile models).

I can provide model code if needed, its on a laptop that does not have internet access currently.
",0,Out of memory issues (GPU),"Out of memory issues (GPU) I'm training a model with Theano/CUDA, and if I attempt to specify a large batch_size (1024 in my case), it reports an out of memory error, which is understandable. However, if I change it back to a size that previously worked (I'm doing it in a notebook), it will still be out of memory, as if it didn't attempt to free whatever it allocated for the previous attempt, so I'm forced to restart the Python process (and reload all data/recompile models).

I can provide model code if needed, its on a laptop that does not have internet access currently.
"
keras,1723,"Hi all,

First of all thanks for this wonderful framework and great work you guys have done.

I have been trying to use Keras to build a simple LSTM language model with a vocabulary of size 50,000. Considering one-hot representation of words, this translates into 50,000 classes. 
To avoid padding, I have been dividing my training data (~80 million sentences) into chunks where each chunk includes sentences of the same length. Model compiles well and training loss is decreasing but I have two main problems:

1) Using ""  "" function results in huge memory (and computation?) waste. I tried """" to avoid it but got a rank mismatch error from Theano. Is there any work around for this? 

2) I guess when the number of classes is huge the running time becomes very high. Is there any solution for this?
E.g., in my language model experiment, with just one LSTM layer and a Softmax layer on the top, 512 cells for LSTM, maximum sentence length of 15, vocabulary size 50,000 and mini-batch size = 1024 sentences, it takes 4 sec / mini-batch on GPU, which is too long. 
It also consumes up to 7 GB of GPU memory which I believe is because of  ""  "" function.

Thanks!
Hamid
",0,"Keras for many number of classes, e.g., language modelling with vocabulary size 50,000.","Keras for many number of classes, e.g., language modelling with vocabulary size 50,000. Hi all,

First of all thanks for this wonderful framework and great work you guys have done.

I have been trying to use Keras to build a simple LSTM language model with a vocabulary of size 50,000. Considering one-hot representation of words, this translates into 50,000 classes. 
To avoid padding, I have been dividing my training data (~80 million sentences) into chunks where each chunk includes sentences of the same length. Model compiles well and training loss is decreasing but I have two main problems:

1) Using ""  "" function results in huge memory (and computation?) waste. I tried """" to avoid it but got a rank mismatch error from Theano. Is there any work around for this? 

2) I guess when the number of classes is huge the running time becomes very high. Is there any solution for this?
E.g., in my language model experiment, with just one LSTM layer and a Softmax layer on the top, 512 cells for LSTM, maximum sentence length of 15, vocabulary size 50,000 and mini-batch size = 1024 sentences, it takes 4 sec / mini-batch on GPU, which is too long. 
It also consumes up to 7 GB of GPU memory which I believe is because of  ""  "" function.

Thanks!
Hamid
"
keras,7927,"Hello,

I'm in trouble because I'm not able to manage correctly the CNN and maybe the LSTM-RNN. 

I'm trying to solve the following problem. I've 20000 chunks of data that each of them are composed by a sequence of 20 images. The size of each image is 40x40. The images are in grayscale mode. For each chunk of data, we have two possible labels, 0 or 1. We have to classify each chunk of data in two possible sets: 0 or 1. As they are numpy arrays, the shape of each one is:
- Data shape: (20000, 20, 40, 40)
- Labels shape: (20000, 1)

Well, I want to train a CNN to predict if the label of a chunk of data is 0 or 1. Remember that each chunk of data is composed by 20 images of 40x40 in grayscale. And the most important thing is that the order of that 20 images matter. I mean, they have to be managed as a 'little' video file of 20 images. If we change the order of that 20 images in a chunk of data the label should be different.

I don't know if is mandatory to use a RNN. I've read some papers where the researchers have used CNN without LSTM and with LSTM for trying to solve similar problems. I think that it isn't mandatory but I think that I've to use the combination of CNN with TimeDistributed. Am I right?

Thank you so much in advance.

Kind regards,
Rubén



",0,CNN to manage sequences of grayscale images like a video files,"CNN to manage sequences of grayscale images like a video files Hello,

I'm in trouble because I'm not able to manage correctly the CNN and maybe the LSTM-RNN. 

I'm trying to solve the following problem. I've 20000 chunks of data that each of them are composed by a sequence of 20 images. The size of each image is 40x40. The images are in grayscale mode. For each chunk of data, we have two possible labels, 0 or 1. We have to classify each chunk of data in two possible sets: 0 or 1. As they are numpy arrays, the shape of each one is:
- Data shape: (20000, 20, 40, 40)
- Labels shape: (20000, 1)

Well, I want to train a CNN to predict if the label of a chunk of data is 0 or 1. Remember that each chunk of data is composed by 20 images of 40x40 in grayscale. And the most important thing is that the order of that 20 images matter. I mean, they have to be managed as a 'little' video file of 20 images. If we change the order of that 20 images in a chunk of data the label should be different.

I don't know if is mandatory to use a RNN. I've read some papers where the researchers have used CNN without LSTM and with LSTM for trying to solve similar problems. I think that it isn't mandatory but I think that I've to use the combination of CNN with TimeDistributed. Am I right?

Thank you so much in advance.

Kind regards,
Rubén



"
keras,11989,"If you try to use the pre-trained models in float16 mode, they just don't load. I think fixing this can be useful, especially for object detection use cases.




",0,Pre-trained models don't work in float16 mode,"Pre-trained models don't work in float16 mode If you try to use the pre-trained models in float16 mode, they just don't load. I think fixing this can be useful, especially for object detection use cases.




"
keras,1573,"Hello,

I'm trying to add weights to the classes during training right before the final  layer, the output shape of the layer is  with one-hot-label of the same size, and I'm using  loss here. I added a class balancing layer before  which did:



but it didn't seem to help... any suggestions will be greatly appreciated!

Update: I've tried to use  in  and send class weight as dictionary: , but got the error: 
",0,Regarding Class Balancing,"Regarding Class Balancing Hello,

I'm trying to add weights to the classes during training right before the final  layer, the output shape of the layer is  with one-hot-label of the same size, and I'm using  loss here. I added a class balancing layer before  which did:



but it didn't seem to help... any suggestions will be greatly appreciated!

Update: I've tried to use  in  and send class weight as dictionary: , but got the error: 
"
keras,4585,"As the title states, when I run the mnist_acgan example the discriminator loss quickly goes to zero and the generator loss increases over time. The generated images at the end of each epoch are black. I'm digging into this myself, but GANs aren't my area so I expect progress to be slow. Any suggestions on where to look? Is anyone able to run this example without any problems?

Link to the example: https://github.com/fchollet/keras/blob/master/examples/mnist_acgan.py

- [x] Check that you are up-to-date with the master branch of Keras. You can update with:
pip install git+git://github.com/fchollet/keras.git --upgrade --no-deps

- [x] If running on Theano, check that you are up-to-date with the master branch of Theano. You can update with:
pip install git+git://github.com/Theano/Theano.git --upgrade --no-deps

- [x] Provide a link to a GitHub Gist of a Python script that can reproduce your issue (or just copy the script here if it is short).",0,generator in mnist_acgan example collapses to black images,"generator in mnist_acgan example collapses to black images As the title states, when I run the mnist_acgan example the discriminator loss quickly goes to zero and the generator loss increases over time. The generated images at the end of each epoch are black. I'm digging into this myself, but GANs aren't my area so I expect progress to be slow. Any suggestions on where to look? Is anyone able to run this example without any problems?

Link to the example: https://github.com/fchollet/keras/blob/master/examples/mnist_acgan.py

- [x] Check that you are up-to-date with the master branch of Keras. You can update with:
pip install git+git://github.com/fchollet/keras.git --upgrade --no-deps

- [x] If running on Theano, check that you are up-to-date with the master branch of Theano. You can update with:
pip install git+git://github.com/Theano/Theano.git --upgrade --no-deps

- [x] Provide a link to a GitHub Gist of a Python script that can reproduce your issue (or just copy the script here if it is short)."
keras,2366,"I'm using the ModelCheckpoint as described in keras.io with a sequential model.  When I save the model using the code described in the FAQ and then try to reload it based on the weights saved by ModelCheckpoint, it tells me:

model.load_weights(os.path.join('checkpoints')+'//weights.hdf5')
Traceback (most recent call last):

  File ""<ipython-input-15-ca39381e1009>"", line 1, in <module>
    model.load_weights(os.path.join('checkpoints')+'//weights.hdf5')

  File ""C:\WinPython\WinPython-64bit-3.4.4.1\python-3.4.4.amd64\lib\site-packages\keras\engine\topology.py"", line 2286, in load_weights
    str(len(flattened_layers)) + ' layers.')

**Exception: You are trying to load a weight file containing 31 layers into a model with 30 layers.**

Is there an error in the way the ModelCheckpoint saves weights, for instance including input layer weights of 1?
",0,ModelCheckpoint saves one extra layer of weights,"ModelCheckpoint saves one extra layer of weights I'm using the ModelCheckpoint as described in keras.io with a sequential model.  When I save the model using the code described in the FAQ and then try to reload it based on the weights saved by ModelCheckpoint, it tells me:

model.load_weights(os.path.join('checkpoints')+'//weights.hdf5')
Traceback (most recent call last):

  File ""<ipython-input-15-ca39381e1009>"", line 1, in <module>
    model.load_weights(os.path.join('checkpoints')+'//weights.hdf5')

  File ""C:\WinPython\WinPython-64bit-3.4.4.1\python-3.4.4.amd64\lib\site-packages\keras\engine\topology.py"", line 2286, in load_weights
    str(len(flattened_layers)) + ' layers.')

**Exception: You are trying to load a weight file containing 31 layers into a model with 30 layers.**

Is there an error in the way the ModelCheckpoint saves weights, for instance including input layer weights of 1?
"
keras,9262," doesn't respect the default float precision in Keras configuration file. In my case, the default is  while the actual is . Related StackOverflow [thread](https://stackoverflow.com/q/48486775/1348273).

- [x] Check that you are up-to-date with the master branch of Keras. You can update with:
pip install git+git://github.com/keras-team/keras.git --upgrade --no-deps

- [x] If running on TensorFlow, check that you are up-to-date with the latest version. The installation instructions can be found [here](https://www.tensorflow.org/get_started/os_setup).

- [ ] If running on Theano, check that you are up-to-date with the master branch of Theano. You can update with:
pip install git+git://github.com/Theano/Theano.git --upgrade --no-deps

- [x] Provide a link to a GitHub Gist of a Python script that can reproduce your issue (or just copy the script here if it is short).
~~~python
import numpy as np
from keras.utils import to_categorical
print(to_categorical(np.ones(2), 2).dtype)
~~~

",0,to_categorical() doesn't respect default float precision in Keras configuration file,"to_categorical() doesn't respect default float precision in Keras configuration file  doesn't respect the default float precision in Keras configuration file. In my case, the default is  while the actual is . Related StackOverflow [thread](https://stackoverflow.com/q/48486775/1348273).

- [x] Check that you are up-to-date with the master branch of Keras. You can update with:
pip install git+git://github.com/keras-team/keras.git --upgrade --no-deps

- [x] If running on TensorFlow, check that you are up-to-date with the latest version. The installation instructions can be found [here](https://www.tensorflow.org/get_started/os_setup).

- [ ] If running on Theano, check that you are up-to-date with the master branch of Theano. You can update with:
pip install git+git://github.com/Theano/Theano.git --upgrade --no-deps

- [x] Provide a link to a GitHub Gist of a Python script that can reproduce your issue (or just copy the script here if it is short).
~~~python
import numpy as np
from keras.utils import to_categorical
print(to_categorical(np.ones(2), 2).dtype)
~~~

"
keras,7355,"I go this error  in mnist_sklearn_wrapper.py
",0,mnist_sklearn_wrapper.py,"mnist_sklearn_wrapper.py I go this error  in mnist_sklearn_wrapper.py
"
keras,4962,"I am doing text classification. Also I am using my pre-trained word embeddings and i have a  layer on top with a  at the end.

Pretty simple. Now I want to add attention to the model,  but i don't know how to do it.
 
My understanding is that i have to set  so as the attention layer will weigh each timestep accordingly. This way the LSTM will return a 3D Tensor, right?
After that what do i have to do?
Is there a way to easily implement a model with attention using Keras Layers or do i have to write my own custom layer?

If this can be done with the available Keras Layers, I would really appreciate an example.
",0,How to add Attention on top of a Recurrent Layer (Text Classification),"How to add Attention on top of a Recurrent Layer (Text Classification) I am doing text classification. Also I am using my pre-trained word embeddings and i have a  layer on top with a  at the end.

Pretty simple. Now I want to add attention to the model,  but i don't know how to do it.
 
My understanding is that i have to set  so as the attention layer will weigh each timestep accordingly. This way the LSTM will return a 3D Tensor, right?
After that what do i have to do?
Is there a way to easily implement a model with attention using Keras Layers or do i have to write my own custom layer?

If this can be done with the available Keras Layers, I would really appreciate an example.
"
keras,3657,"Please make sure that the boxes below are checked before you submit your issue. Thank you!
- [X] Check that you are up-to-date with the master branch of Keras. You can update with:
  pip install git+git://github.com/fchollet/keras.git --upgrade --no-deps
- [X] If running on Theano, check that you are up-to-date with the master branch of Theano. You can update with:
  pip install git+git://github.com/Theano/Theano.git --upgrade --no-deps
- [X] Provide a link to a GitHub Gist of a Python script that can reproduce your issue (or just copy the script here if it is short).

I am performing batch learning and after a few batches I get this error from this line of code:



I have no idea why I get this error, It seems to happen randomly, can anyone point me in the right direction?

Here is the module of code that I am running:


",0,AttributeError: 'ProgbarLogger' object has no attribute 'log_values',"AttributeError: 'ProgbarLogger' object has no attribute 'log_values' Please make sure that the boxes below are checked before you submit your issue. Thank you!
- [X] Check that you are up-to-date with the master branch of Keras. You can update with:
  pip install git+git://github.com/fchollet/keras.git --upgrade --no-deps
- [X] If running on Theano, check that you are up-to-date with the master branch of Theano. You can update with:
  pip install git+git://github.com/Theano/Theano.git --upgrade --no-deps
- [X] Provide a link to a GitHub Gist of a Python script that can reproduce your issue (or just copy the script here if it is short).

I am performing batch learning and after a few batches I get this error from this line of code:



I have no idea why I get this error, It seems to happen randomly, can anyone point me in the right direction?

Here is the module of code that I am running:


"
keras,6655,"Edit: A new example proposal + details is in https://github.com/fchollet/keras/pull/6891#issuecomment-307659460.

Discussion about a dense prediction API such as image segmentation in https://github.com/fchollet/keras/issues/6538 brought up the possibility of a functional preprocessing API, which could move preprocessing steps into the Keras backend APIs and generalize preprocessing to more network designs. [Dropout](https://github.com/fchollet/keras/blob/master/keras/layers/core.py#L72) provides a precedent for such augmentation layers.

If the layers could be designed much like dropout, and I would expect them to be configured so that the augmentation operations could be applied identically to one or more image inputs as well as one or more image label data, useful for dense prediction tasks.

This could have the advantages of being easy to use, easily applied consistently for arbitrary data inputs, and make it possible to use the [TF backend image augmentation APIs](https://www.tensorflow.org/api_guides/python/image) thus improving performance.

What would be the pros/cons and barriers to a Functional Preprocessing API?


Example usage, label augmentation optional for dense prediction tasks:

",0,Functional Preprocessing and Augmentation API,"Functional Preprocessing and Augmentation API Edit: A new example proposal + details is in https://github.com/fchollet/keras/pull/6891#issuecomment-307659460.

Discussion about a dense prediction API such as image segmentation in https://github.com/fchollet/keras/issues/6538 brought up the possibility of a functional preprocessing API, which could move preprocessing steps into the Keras backend APIs and generalize preprocessing to more network designs. [Dropout](https://github.com/fchollet/keras/blob/master/keras/layers/core.py#L72) provides a precedent for such augmentation layers.

If the layers could be designed much like dropout, and I would expect them to be configured so that the augmentation operations could be applied identically to one or more image inputs as well as one or more image label data, useful for dense prediction tasks.

This could have the advantages of being easy to use, easily applied consistently for arbitrary data inputs, and make it possible to use the [TF backend image augmentation APIs](https://www.tensorflow.org/api_guides/python/image) thus improving performance.

What would be the pros/cons and barriers to a Functional Preprocessing API?


Example usage, label augmentation optional for dense prediction tasks:

"
keras,1315,"/usr/bin/python2.7 /home/dell/DLTest/cifar_test/Residul/cifar_residul_net.py
Using Theano backend.
Using gpu device 1: GeForce GTX TITAN X (CNMeM is disabled)
('X_train shape:', (50000, 3, 32, 32))
(50000, 'train samples')
(10000, 'test samples')
Traceback (most recent call last):
  File ""/home/dell/DLTest/cifar_test/Residul/cifar_residul_net.py"", line 87, in <module>
    model.compile(loss='categorical_crossentropy', optimizer=sgd)
  File ""/home/dell/.local/lib/python2.7/site-packages/keras/models.py"", line 372, in compile
    self.y_train = self.get_output(train=True)
  File ""/home/dell/.local/lib/python2.7/site-packages/keras/layers/containers.py"", line 73, in get_output
    return self.layers[-1].get_output(train)
  File ""/home/dell/.local/lib/python2.7/site-packages/keras/layers/core.py"", line 681, in get_output
    X = self.get_input(train)
  File ""/home/dell/.local/lib/python2.7/site-packages/keras/layers/core.py"", line 102, in get_input
    return self.previous.get_output(train=train)
  File ""/home/dell/.local/lib/python2.7/site-packages/keras/layers/core.py"", line 591, in get_output
    X = self.get_input(train)
  File ""/home/dell/.local/lib/python2.7/site-packages/keras/layers/core.py"", line 102, in get_input
    return self.previous.get_output(train=train)
  File ""/home/dell/.local/lib/python2.7/site-packages/keras/layers/containers.py"", line 216, in get_output
    return self.outputs[self.output_order[0]].get_output(train)
  File ""/home/dell/.local/lib/python2.7/site-packages/keras/layers/core.py"", line 389, in get_output
    s = self.layers[0].get_output(train)
  File ""/home/dell/.local/lib/python2.7/site-packages/keras/layers/convolutional.py"", line 212, in get_output
    X = self.get_input(train)
  File ""/home/dell/.local/lib/python2.7/site-packages/keras/layers/core.py"", line 102, in get_input
    return self.previous.get_output(train=train)
  File ""/home/dell/.local/lib/python2.7/site-packages/keras/layers/core.py"", line 98, in get_output
    return self.get_input(train)
  File ""/home/dell/.local/lib/python2.7/site-packages/keras/layers/core.py"", line 102, in get_input
    return self.previous.get_output(train=train)
  File ""/home/dell/.local/lib/python2.7/site-packages/keras/layers/containers.py"", line 216, in get_output
    return self.outputs[self.output_order[0]].get_output(train)
  File ""/home/dell/.local/lib/python2.7/site-packages/keras/layers/core.py"", line 389, in get_output
    s = self.layers[0].get_output(train)
  File ""/home/dell/.local/lib/python2.7/site-packages/keras/layers/convolutional.py"", line 215, in get_output
    dim_ordering=self.dim_ordering)
  File ""/home/dell/.local/lib/python2.7/site-packages/keras/backend/theano_backend.py"", line 543, in conv2d
    border_mode=(pad_x, pad_y))
  File ""/usr/local/lib/python2.7/dist-packages/theano/sandbox/cuda/dnn.py"", line 1191, in dnn_conv
    conv_mode=conv_mode)(img.shape, kerns.shape)
  File ""/usr/local/lib/python2.7/dist-packages/theano/sandbox/cuda/dnn.py"", line 251, in **init**
    border_mode = tuple(map(int, border_mode))
TypeError: int() argument must be a string or a number, not 'TensorVariable'

I got a bug after i delete /usr/local/cuda/lib64/libcudnn\*  and /usr/local/cuda/include/cudnn.h

Then add the same files of cudnn7-5v3 into the same path 
",0,Bug in /theano/sandbox/cuda/dnn.py,"Bug in /theano/sandbox/cuda/dnn.py /usr/bin/python2.7 /home/dell/DLTest/cifar_test/Residul/cifar_residul_net.py
Using Theano backend.
Using gpu device 1: GeForce GTX TITAN X (CNMeM is disabled)
('X_train shape:', (50000, 3, 32, 32))
(50000, 'train samples')
(10000, 'test samples')
Traceback (most recent call last):
  File ""/home/dell/DLTest/cifar_test/Residul/cifar_residul_net.py"", line 87, in <module>
    model.compile(loss='categorical_crossentropy', optimizer=sgd)
  File ""/home/dell/.local/lib/python2.7/site-packages/keras/models.py"", line 372, in compile
    self.y_train = self.get_output(train=True)
  File ""/home/dell/.local/lib/python2.7/site-packages/keras/layers/containers.py"", line 73, in get_output
    return self.layers[-1].get_output(train)
  File ""/home/dell/.local/lib/python2.7/site-packages/keras/layers/core.py"", line 681, in get_output
    X = self.get_input(train)
  File ""/home/dell/.local/lib/python2.7/site-packages/keras/layers/core.py"", line 102, in get_input
    return self.previous.get_output(train=train)
  File ""/home/dell/.local/lib/python2.7/site-packages/keras/layers/core.py"", line 591, in get_output
    X = self.get_input(train)
  File ""/home/dell/.local/lib/python2.7/site-packages/keras/layers/core.py"", line 102, in get_input
    return self.previous.get_output(train=train)
  File ""/home/dell/.local/lib/python2.7/site-packages/keras/layers/containers.py"", line 216, in get_output
    return self.outputs[self.output_order[0]].get_output(train)
  File ""/home/dell/.local/lib/python2.7/site-packages/keras/layers/core.py"", line 389, in get_output
    s = self.layers[0].get_output(train)
  File ""/home/dell/.local/lib/python2.7/site-packages/keras/layers/convolutional.py"", line 212, in get_output
    X = self.get_input(train)
  File ""/home/dell/.local/lib/python2.7/site-packages/keras/layers/core.py"", line 102, in get_input
    return self.previous.get_output(train=train)
  File ""/home/dell/.local/lib/python2.7/site-packages/keras/layers/core.py"", line 98, in get_output
    return self.get_input(train)
  File ""/home/dell/.local/lib/python2.7/site-packages/keras/layers/core.py"", line 102, in get_input
    return self.previous.get_output(train=train)
  File ""/home/dell/.local/lib/python2.7/site-packages/keras/layers/containers.py"", line 216, in get_output
    return self.outputs[self.output_order[0]].get_output(train)
  File ""/home/dell/.local/lib/python2.7/site-packages/keras/layers/core.py"", line 389, in get_output
    s = self.layers[0].get_output(train)
  File ""/home/dell/.local/lib/python2.7/site-packages/keras/layers/convolutional.py"", line 215, in get_output
    dim_ordering=self.dim_ordering)
  File ""/home/dell/.local/lib/python2.7/site-packages/keras/backend/theano_backend.py"", line 543, in conv2d
    border_mode=(pad_x, pad_y))
  File ""/usr/local/lib/python2.7/dist-packages/theano/sandbox/cuda/dnn.py"", line 1191, in dnn_conv
    conv_mode=conv_mode)(img.shape, kerns.shape)
  File ""/usr/local/lib/python2.7/dist-packages/theano/sandbox/cuda/dnn.py"", line 251, in **init**
    border_mode = tuple(map(int, border_mode))
TypeError: int() argument must be a string or a number, not 'TensorVariable'

I got a bug after i delete /usr/local/cuda/lib64/libcudnn\*  and /usr/local/cuda/include/cudnn.h

Then add the same files of cudnn7-5v3 into the same path 
"
keras,741,"I need  train a multi-label softmax classifier, but there is  a lot of one-hot code labels in examples, so how to change code to do it?
",0,How to train a multi-label Classifier,"How to train a multi-label Classifier I need  train a multi-label softmax classifier, but there is  a lot of one-hot code labels in examples, so how to change code to do it?
"
keras,13105,"**System information**  
- Have I written custom code (as opposed to using example directory):  
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Arch Linux
- TensorFlow backend (yes / no):  yes
- TensorFlow version:  1.13.1
- Keras version:  2.1.6
- Python version:  3.7
- CUDA/cuDNN version:  10
- GPU model and memory:  GTX 1070

Hello, I am using Faster-RCNN with ResNet50 as base network for object detection. It uses a custom BatchNormalization layer named 'FixedBatchNormalization' in ResNet. I want to know what's the difference between this custom layer and the official BatchNormalization layer in keras. I know that there is an ongoing issue with fine-tuning ResNet when freezing layers. So does the 'FixedBatchNormalization' in this repo solve that issue and does this anyway affect the training and evaluating?

And this layer works only with keras version 2.1.6.
Link to the Faster-RCNN repo:
[https://github.com/kbardool/keras-frcnn](url)

**FixedBatchNormalization code:**



",0,Custom BatchNormalization layer in ResNet,"Custom BatchNormalization layer in ResNet **System information**  
- Have I written custom code (as opposed to using example directory):  
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Arch Linux
- TensorFlow backend (yes / no):  yes
- TensorFlow version:  1.13.1
- Keras version:  2.1.6
- Python version:  3.7
- CUDA/cuDNN version:  10
- GPU model and memory:  GTX 1070

Hello, I am using Faster-RCNN with ResNet50 as base network for object detection. It uses a custom BatchNormalization layer named 'FixedBatchNormalization' in ResNet. I want to know what's the difference between this custom layer and the official BatchNormalization layer in keras. I know that there is an ongoing issue with fine-tuning ResNet when freezing layers. So does the 'FixedBatchNormalization' in this repo solve that issue and does this anyway affect the training and evaluating?

And this layer works only with keras version 2.1.6.
Link to the Faster-RCNN repo:
[https://github.com/kbardool/keras-frcnn](url)

**FixedBatchNormalization code:**



"
keras,2466,"I got this error


here is my code


as you can see when I printed trainX shape I got (28709L, 48L, 48L) so my data dim is correct
",0,"error: Wrong number of dimensions: expected 4, got 3 with shape","error: Wrong number of dimensions: expected 4, got 3 with shape I got this error


here is my code


as you can see when I printed trainX shape I got (28709L, 48L, 48L) so my data dim is correct
"
keras,4232,"The recurrence formula in my RNN is , where  is the classification output (Softmax output) of the RNN at time . How can I access this output at the next time step?
I guess I have to write a custom RNN, but I am a bit confused by all the recurrent functions in recurrent.py in Keras and I am not sure which parts should be modified. ",0,Accessing the softmax output of previous RNN state,"Accessing the softmax output of previous RNN state The recurrence formula in my RNN is , where  is the classification output (Softmax output) of the RNN at time . How can I access this output at the next time step?
I guess I have to write a custom RNN, but I am a bit confused by all the recurrent functions in recurrent.py in Keras and I am not sure which parts should be modified. "
keras,6168,"Please consider this simple example
    
    nb_samples = 100000
    X = np.random.randn(nb_samples)
    Y = X[1:]
    X = X[:-1]
    X = X.reshape((len(Y), 1, 1))
    Y = Y.reshape((len(Y), 1))

So we have basically 

    Y[i] = X[i-1]


and the model is simply a lag operator. 

Now I try to learn this model with a stateful LSTM, by giving the pairs of values  one by one ()

    model = Sequential()
    model.add(LSTM(batch_input_shape=(1, 1, 1),
                   output_dim =10,
                   activation='tanh', stateful=True
                  )
            )
    model.add(Dense(output_dim=1, activation='linear'))
    model.compile(loss='mse', optimizer='adam')
    
    
    for epoch in range(10000):
        model.reset_states()
        train_loss = 0
        for i in range(Y_train.shape[0]):
            train_loss += model.train_on_batch(X_train[i:i+1],
                             Y_train[i:i+1],
                             )
        print '# epoch', epoch, '  loss ', train_loss/float(Y_train.shape[0])


but I am seeing a mean loss around 1, which is the standard deviation of my randomly generated data, so the model does not seem to learn.

Am I having something wrong?",0,Simple stateful LSTM example,"Simple stateful LSTM example Please consider this simple example
    
    nb_samples = 100000
    X = np.random.randn(nb_samples)
    Y = X[1:]
    X = X[:-1]
    X = X.reshape((len(Y), 1, 1))
    Y = Y.reshape((len(Y), 1))

So we have basically 

    Y[i] = X[i-1]


and the model is simply a lag operator. 

Now I try to learn this model with a stateful LSTM, by giving the pairs of values  one by one ()

    model = Sequential()
    model.add(LSTM(batch_input_shape=(1, 1, 1),
                   output_dim =10,
                   activation='tanh', stateful=True
                  )
            )
    model.add(Dense(output_dim=1, activation='linear'))
    model.compile(loss='mse', optimizer='adam')
    
    
    for epoch in range(10000):
        model.reset_states()
        train_loss = 0
        for i in range(Y_train.shape[0]):
            train_loss += model.train_on_batch(X_train[i:i+1],
                             Y_train[i:i+1],
                             )
        print '# epoch', epoch, '  loss ', train_loss/float(Y_train.shape[0])


but I am seeing a mean loss around 1, which is the standard deviation of my randomly generated data, so the model does not seem to learn.

Am I having something wrong?"
keras,3421,"Im getting this weird error, while other keras modules load without a problem.
Im just running: variational_autoencoder_deconv.py
Ideas?


",0,ImportError: cannot import name 'Deconvolution2D' (install from source),"ImportError: cannot import name 'Deconvolution2D' (install from source) Im getting this weird error, while other keras modules load without a problem.
Im just running: variational_autoencoder_deconv.py
Ideas?


"
keras,11379,"
I'm trying to implement a dynamic zero padding to keep the second dimension of a constant tensor after going through convolutional layers that have stride> 1, the input tensor has the following shape (batch_size, time_step, 50), I need the time step dimension not be changed by the convolutional layers. I tried to use 'same' padding, however when stride> 1, this does not work, so I created a custom layer for ZeroPadding, it is working for tensors with the shape (None, 100,50), (None, 120,50), (None, 60,50), but does not work for dynamic shapes of type (None, None, 50), I get the following error:



I've added my custom class to an imdb example, to make it easier to reproduce the errors. Change model.add(Embedding(max_features, embedding_dims, input_length = None)) to model.add(Embedding(max_features, embedding_dims, input_length = 400)) and dynamic padding will work, however it needs to work for dimension of type None. The code:

(left_pad, right_pad)(batch, axis_to_pad, features)(batch, padded_axis, features)

I searched and saw that I have to use K.shape (inputs), to get the correct shape at runtime, instead of None, but I could not make it work with Keras, could anyone help me?

If you have another solution to solve the problem of zero dynamic padding, it is very welcome.
The question is also in StackOverflow: [ZeroPadding Dynamic for step> 1 and access the actual shape of a tensor of dimension None in Keras](https://stackoverflow.com/questions/52788133/zeropadding-dynamic-for-step-1-and-access-the-actual-shape-of-a-tensor-of-dimen) 

Thank you in advance for your attention.


Thank you!

- [ V ] Check that you are up-to-date with the master branch of Keras. You can update with:


- [ V ] Check that your version of TensorFlow is up-to-date. The installation instructions can be found [here](https://www.tensorflow.org/get_started/os_setup).

- [ V ] Provide a link to a GitHub Gist of a Python script that can reproduce your issue (or just copy the script here if it is short).
",0,ZeroPadding Dynamic for step> 1 and access the actual shape of a tensor of dimension None in Keras,"ZeroPadding Dynamic for step> 1 and access the actual shape of a tensor of dimension None in Keras 
I'm trying to implement a dynamic zero padding to keep the second dimension of a constant tensor after going through convolutional layers that have stride> 1, the input tensor has the following shape (batch_size, time_step, 50), I need the time step dimension not be changed by the convolutional layers. I tried to use 'same' padding, however when stride> 1, this does not work, so I created a custom layer for ZeroPadding, it is working for tensors with the shape (None, 100,50), (None, 120,50), (None, 60,50), but does not work for dynamic shapes of type (None, None, 50), I get the following error:



I've added my custom class to an imdb example, to make it easier to reproduce the errors. Change model.add(Embedding(max_features, embedding_dims, input_length = None)) to model.add(Embedding(max_features, embedding_dims, input_length = 400)) and dynamic padding will work, however it needs to work for dimension of type None. The code:

(left_pad, right_pad)(batch, axis_to_pad, features)(batch, padded_axis, features)

I searched and saw that I have to use K.shape (inputs), to get the correct shape at runtime, instead of None, but I could not make it work with Keras, could anyone help me?

If you have another solution to solve the problem of zero dynamic padding, it is very welcome.
The question is also in StackOverflow: [ZeroPadding Dynamic for step> 1 and access the actual shape of a tensor of dimension None in Keras](https://stackoverflow.com/questions/52788133/zeropadding-dynamic-for-step-1-and-access-the-actual-shape-of-a-tensor-of-dimen) 

Thank you in advance for your attention.


Thank you!

- [ V ] Check that you are up-to-date with the master branch of Keras. You can update with:


- [ V ] Check that your version of TensorFlow is up-to-date. The installation instructions can be found [here](https://www.tensorflow.org/get_started/os_setup).

- [ V ] Provide a link to a GitHub Gist of a Python script that can reproduce your issue (or just copy the script here if it is short).
"
keras,5326,"Below I provide a toy example which throws an out-of-memory exception while training VGG19 or VGG16 models on GPU. The batch-size and the dataset used here are tiny and my graphics card should handle them. If I use a ResNet50 architecture instead, I get no error and I am able to use really big datasets & batch-sizes. Training the models on CPU works fine for all network architectures. 

The problem first appeared on a more complex pipeline that runs on p2.16xlarge AWS instances. I can reproduce the problem using Ubuntu 14.04, Keras 1.2.1 and Tensorflow 0.12.1. Can anyone reproduce the problem? Any thoughts?

For the shake of completeness I also uploaded the dataset [here](https://ufile.io/c3fbf).



Error message using VGG19:


Successful execution with ResNet50:
",0,GPU runs out of memory for VGG19 & VGG16 but not for ResNet50,"GPU runs out of memory for VGG19 & VGG16 but not for ResNet50 Below I provide a toy example which throws an out-of-memory exception while training VGG19 or VGG16 models on GPU. The batch-size and the dataset used here are tiny and my graphics card should handle them. If I use a ResNet50 architecture instead, I get no error and I am able to use really big datasets & batch-sizes. Training the models on CPU works fine for all network architectures. 

The problem first appeared on a more complex pipeline that runs on p2.16xlarge AWS instances. I can reproduce the problem using Ubuntu 14.04, Keras 1.2.1 and Tensorflow 0.12.1. Can anyone reproduce the problem? Any thoughts?

For the shake of completeness I also uploaded the dataset [here](https://ufile.io/c3fbf).



Error message using VGG19:


Successful execution with ResNet50:
"
keras,11311,"Keras 2.2.4; TensorFlow 1.11.0; Python 3.6.6.

I'm trying to set up a test harness for an LSTM RNN. As a part of this, I want to test the effectiveness of statefulness. Since the LSTM is the first layer and using statefulness requires using batch_input_shape attribute rather than the input_shape attribute, I am attempting to define the layer first, then set the correct attributes. When I run the function below, I get an AttributeError: ""'LSTM' object has no attribute 'dtype'."" I've traced this specifically to adding the batch_input_shape attribute.

",0,AttributeError when attempting to set batch_input_shape using setattr,"AttributeError when attempting to set batch_input_shape using setattr Keras 2.2.4; TensorFlow 1.11.0; Python 3.6.6.

I'm trying to set up a test harness for an LSTM RNN. As a part of this, I want to test the effectiveness of statefulness. Since the LSTM is the first layer and using statefulness requires using batch_input_shape attribute rather than the input_shape attribute, I am attempting to define the layer first, then set the correct attributes. When I run the function below, I get an AttributeError: ""'LSTM' object has no attribute 'dtype'."" I've traced this specifically to adding the batch_input_shape attribute.

"
keras,459,"I have a 2D array consisting of time (nsecs), latitude, longitude and velocity as features which should be the input to simpleRNN. Its shape is (33336,4) as (nb_samples, input_dim). But there's a 3D array (nb_samples, timesteps, input_dim) needed. Of course, I could simply use numpy.expand_dims() but then I would have something like (33336,1,4). But my time features are representing those timesteps. So now I'm confused how to actually create a decent 3D array with latitude, longitude and velocity and the time feature as required timesteps. I tried out different things like first creating a 3D array and then adding the data where its needed. But then I get some memory errors because of its size. Have you guys any ideas how I could create a reasonable input?
",0,Problem with the sequence input to SimpleRNN,"Problem with the sequence input to SimpleRNN I have a 2D array consisting of time (nsecs), latitude, longitude and velocity as features which should be the input to simpleRNN. Its shape is (33336,4) as (nb_samples, input_dim). But there's a 3D array (nb_samples, timesteps, input_dim) needed. Of course, I could simply use numpy.expand_dims() but then I would have something like (33336,1,4). But my time features are representing those timesteps. So now I'm confused how to actually create a decent 3D array with latitude, longitude and velocity and the time feature as required timesteps. I tried out different things like first creating a 3D array and then adding the data where its needed. But then I get some memory errors because of its size. Have you guys any ideas how I could create a reasonable input?
"
keras,5425,"Hello, thanks in advance for your help and for the developers of Keras!

I am working with LSTM networks, actually I am trying to create a CNN+LSTM network that takes as inputs images with 3 channels. I have been reading a lot, but I still have several doubts about how LSTM layers really work, because the results I am obtaining in my experiments are horrible, and this networks are told to give great results.
I have read #4149 and #2403 and I have clear my mind enough to know that I have to learn a lot.

First I will told you my task and then enumerate the doubts I have about recurrent layers.

My inputs are images with 3 channels, I have reshape my data set with the following code in order to have sequences in time:











And that gives me structures with the following shapes:






So if I am right, that means that I have nb_samples=2126 (number of samples), each sample is a sequence of length 2 and each element of that sequence is an image of 3 channels and dimensions 10x8, am I right?

My outputs is a matrix of dimensions , so each input image has associated 3 numbers as outputs. What I want is to feed my net with my input sequences of image so each sequence has the image for t-1 and t, I want my net to give me as output the 3 numbers associated with the image at time t. I have read a lot about problems where the sequences are t-1,t and the output is t+1, but I want the output that correspond to the last element of my input sequence.

Having this in mind, I don't know if that is what I am doing with this net:















So as long as I knew, with  I make sure that the convolutional layer is applayed to each element in the sequence separated. And I added  in the first LSTM layer to connect with the second LSTM layer. Finally the Dense layer is to obtain the 3 outputs I need.

And here comes my doubts (generals and about my problem):

1. With this network, am I calculating the output of the last element of the sequences or the output of the future instant t+1?

2. Are my data reshape right?

3. I don't really understand all the parameters the LSTM and recurrent layers have (I have read the keras documentation but it is not clear to me). Moreover, I don't understand the difference between the cases in this image:
![e4cdf91c-063f-11e6-8844-c89a9e134339](https://cloud.githubusercontent.com/assets/25105487/23065901/1db65978-f518-11e6-8915-2c57668e714e.png)

I don't understand the difference and I don't know how can I programm the layer to obtain the different cases.

4. I have read that is recommended to use  instead of  to connect the cnn layer with the lstm layer, but to me, the reshape is not working.

5. Am I using well the  layer? 
I have read this : [https://github.com/fchollet/keras/blob/master/examples/imdb_cnn_lstm.py](url)
I know is 1D instead of 2D, but in that example they don't use TimeDistributed nor  layer.


I think that's all for the moment. Sorry about the big post, and hope some of you could help me.",0,"LSTM different case of sequences, doubts in general and CNN+LSTM network for regression problem","LSTM different case of sequences, doubts in general and CNN+LSTM network for regression problem Hello, thanks in advance for your help and for the developers of Keras!

I am working with LSTM networks, actually I am trying to create a CNN+LSTM network that takes as inputs images with 3 channels. I have been reading a lot, but I still have several doubts about how LSTM layers really work, because the results I am obtaining in my experiments are horrible, and this networks are told to give great results.
I have read #4149 and #2403 and I have clear my mind enough to know that I have to learn a lot.

First I will told you my task and then enumerate the doubts I have about recurrent layers.

My inputs are images with 3 channels, I have reshape my data set with the following code in order to have sequences in time:











And that gives me structures with the following shapes:






So if I am right, that means that I have nb_samples=2126 (number of samples), each sample is a sequence of length 2 and each element of that sequence is an image of 3 channels and dimensions 10x8, am I right?

My outputs is a matrix of dimensions , so each input image has associated 3 numbers as outputs. What I want is to feed my net with my input sequences of image so each sequence has the image for t-1 and t, I want my net to give me as output the 3 numbers associated with the image at time t. I have read a lot about problems where the sequences are t-1,t and the output is t+1, but I want the output that correspond to the last element of my input sequence.

Having this in mind, I don't know if that is what I am doing with this net:















So as long as I knew, with  I make sure that the convolutional layer is applayed to each element in the sequence separated. And I added  in the first LSTM layer to connect with the second LSTM layer. Finally the Dense layer is to obtain the 3 outputs I need.

And here comes my doubts (generals and about my problem):

1. With this network, am I calculating the output of the last element of the sequences or the output of the future instant t+1?

2. Are my data reshape right?

3. I don't really understand all the parameters the LSTM and recurrent layers have (I have read the keras documentation but it is not clear to me). Moreover, I don't understand the difference between the cases in this image:
![e4cdf91c-063f-11e6-8844-c89a9e134339](https://cloud.githubusercontent.com/assets/25105487/23065901/1db65978-f518-11e6-8915-2c57668e714e.png)

I don't understand the difference and I don't know how can I programm the layer to obtain the different cases.

4. I have read that is recommended to use  instead of  to connect the cnn layer with the lstm layer, but to me, the reshape is not working.

5. Am I using well the  layer? 
I have read this : [https://github.com/fchollet/keras/blob/master/examples/imdb_cnn_lstm.py](url)
I know is 1D instead of 2D, but in that example they don't use TimeDistributed nor  layer.


I think that's all for the moment. Sorry about the big post, and hope some of you could help me."
keras,3089,"For a given data $X \in N*M$ (N is the data size, M is the input dim), the reconstruction of autoencoder and the corresponding representation (the output of encoder) are $Y$ and $H$. I hope to enforce some constraints on $H$, and my loss function is: 

$(X-Y)^{2} + lambda*(H-CH)^{2}$, where C is a pre-defined matrix. My code is as follows:

X = Input(shape=(784,), name='X')
H = Dense(10, activation='tank')(X)
Y = Dense(784, activation='tanh')(H)
model = Model(input=X, output=Y)
model.compile(optimizer='sgd', loss=my_loss(C, H, 0.2)) #  C is the given matrix, whose shape is N*N, where N is the size of X.
model.fit(X, X, nb_epoch=100, batch_size=32)
# the custom loss function

def my_loss(C, H, lmd): #  C is given, whose shape is N*N, where N is the size of X.
    global_loss = K.mean(K.square(encoded - K.dot(C, encoded)), axis=-1)
    def loss(y_true, y_pred):
        local_loss = K.mean(K.square(y_true - y_pred), axis=-1)
        return local_loss + lmd \* global_loss
    return loss
## I got the following errors:

AssertionError: Theano Assert failed!
Apply node that caused the error: Assert{msg='Theano Assert failed!'}(Elemwise{Composite{(i0 \* (Abs(i1) + i2 + i3))}}[(0, 2)].0, Elemwise{eq,no_inplace}.0)
Toposort index: 46
Inputs types: [TensorType(float32, matrix), TensorType(int8, scalar)]
Inputs shapes: [(32, 10), ()]
Inputs strides: [(40, 4), ()]
Inputs values: ['not shown', array(0, dtype=int8)]
Outputs clients: [[Elemwise{sub,no_inplace}(Assert{msg='Theano Assert failed!'}.0, InplaceDimShuffle{x,x}.0)]]

HINT: Re-running with most Theano optimization disabled could give you a back-trace of when this node was created. This can be done with by setting the Theano flag 'optimizer=fast_compile'. If that does not work, Theano optimizations can be disabled with 'optimizer=None'.
## HINT: Use the Theano flag 'exception_verbosity=high' for a debugprint and storage map footprint of this apply node.

I guess that the obtained error may result from passing the fix-sized matrix C into my loss. Anybody can kindly help me to solve this issue? Thank you very much. 
",0,passing a fixed matrix into the custom loss,"passing a fixed matrix into the custom loss For a given data $X \in N*M$ (N is the data size, M is the input dim), the reconstruction of autoencoder and the corresponding representation (the output of encoder) are $Y$ and $H$. I hope to enforce some constraints on $H$, and my loss function is: 

$(X-Y)^{2} + lambda*(H-CH)^{2}$, where C is a pre-defined matrix. My code is as follows:

X = Input(shape=(784,), name='X')
H = Dense(10, activation='tank')(X)
Y = Dense(784, activation='tanh')(H)
model = Model(input=X, output=Y)
model.compile(optimizer='sgd', loss=my_loss(C, H, 0.2)) #  C is the given matrix, whose shape is N*N, where N is the size of X.
model.fit(X, X, nb_epoch=100, batch_size=32)
# the custom loss function

def my_loss(C, H, lmd): #  C is given, whose shape is N*N, where N is the size of X.
    global_loss = K.mean(K.square(encoded - K.dot(C, encoded)), axis=-1)
    def loss(y_true, y_pred):
        local_loss = K.mean(K.square(y_true - y_pred), axis=-1)
        return local_loss + lmd \* global_loss
    return loss
## I got the following errors:

AssertionError: Theano Assert failed!
Apply node that caused the error: Assert{msg='Theano Assert failed!'}(Elemwise{Composite{(i0 \* (Abs(i1) + i2 + i3))}}[(0, 2)].0, Elemwise{eq,no_inplace}.0)
Toposort index: 46
Inputs types: [TensorType(float32, matrix), TensorType(int8, scalar)]
Inputs shapes: [(32, 10), ()]
Inputs strides: [(40, 4), ()]
Inputs values: ['not shown', array(0, dtype=int8)]
Outputs clients: [[Elemwise{sub,no_inplace}(Assert{msg='Theano Assert failed!'}.0, InplaceDimShuffle{x,x}.0)]]

HINT: Re-running with most Theano optimization disabled could give you a back-trace of when this node was created. This can be done with by setting the Theano flag 'optimizer=fast_compile'. If that does not work, Theano optimizations can be disabled with 'optimizer=None'.
## HINT: Use the Theano flag 'exception_verbosity=high' for a debugprint and storage map footprint of this apply node.

I guess that the obtained error may result from passing the fix-sized matrix C into my loss. Anybody can kindly help me to solve this issue? Thank you very much. 
"
keras,7614,"I Am developing a **classification based-model** to predict 12 probability for each pixel in the image , I have built the architecture , but I am not sure whether I am right or not , I am a newbie in deep learning.


_The following is the function for my baseline architecture :_

   

I will explain my architecture , the input is about a 64 * 64 * 1 graysale image , followed by many convolution layers and then it is flattened then there is two different hidden layer U & V , U represents U Channel in the CIELUV color space and V represents V Channel in the same color space, then the U&V channels each of them is reshaped into 64 * 64 * 12 , lemme convert it to 4096 * 12 so , in the 4096 pixel each of them should have a 12 corresponding probabilities and the sum of the 12 probability should be = 1 but that doesn't happen the sum of the overall matrix ( 4096 * 12 ) = 1 , how can I do that make every 12 node have a sum of 1 probability ? **thanks in advance**











",0,Predicting N probabailities for each pixel in Keras,"Predicting N probabailities for each pixel in Keras I Am developing a **classification based-model** to predict 12 probability for each pixel in the image , I have built the architecture , but I am not sure whether I am right or not , I am a newbie in deep learning.


_The following is the function for my baseline architecture :_

   

I will explain my architecture , the input is about a 64 * 64 * 1 graysale image , followed by many convolution layers and then it is flattened then there is two different hidden layer U & V , U represents U Channel in the CIELUV color space and V represents V Channel in the same color space, then the U&V channels each of them is reshaped into 64 * 64 * 12 , lemme convert it to 4096 * 12 so , in the 4096 pixel each of them should have a 12 corresponding probabilities and the sum of the 12 probability should be = 1 but that doesn't happen the sum of the overall matrix ( 4096 * 12 ) = 1 , how can I do that make every 12 node have a sum of 1 probability ? **thanks in advance**











"
keras,11108,"Hi,
I have an auto-encoder and as we know, it has two parts, encoder and decoder. the output of my encoder part is a 28X28 image and I want to add another 28X28 image to it and send 28X28X2 filter to decoder part during learning. I want to know, is it possible or not? if yes, how? please guide me completely due to I am a beginner. I attached my code here too. I do not know using this ""merge_encoded_w=cv2.merge(encoded,w)"" for adding w to encoder output is true or not?thanks
channels_firstchannels_firstchannels_firstchannels_first",0,adding one filter to existing filter in auto encoder during learning,"adding one filter to existing filter in auto encoder during learning Hi,
I have an auto-encoder and as we know, it has two parts, encoder and decoder. the output of my encoder part is a 28X28 image and I want to add another 28X28 image to it and send 28X28X2 filter to decoder part during learning. I want to know, is it possible or not? if yes, how? please guide me completely due to I am a beginner. I attached my code here too. I do not know using this ""merge_encoded_w=cv2.merge(encoded,w)"" for adding w to encoder output is true or not?thanks
channels_firstchannels_firstchannels_firstchannels_first"
keras,8657,,0,Introduction of global metrics (precision and recall),
keras,7393,"Hi, 
I am currently doing experiments on a dataset classifying text document using Embedding, Conv1D and Dense layers. 



Everything is ok, but while I am running the python script I obtain the following error related to native code in C/C++.



It's the first time that I got that error, I had problem before but due to incompatibility among different shapes, not because of the compile phase.

Can someone give me an hint on how to solve this problem?
Thanks",0,Error with Keras while running a Python script,"Error with Keras while running a Python script Hi, 
I am currently doing experiments on a dataset classifying text document using Embedding, Conv1D and Dense layers. 



Everything is ok, but while I am running the python script I obtain the following error related to native code in C/C++.



It's the first time that I got that error, I had problem before but due to incompatibility among different shapes, not because of the compile phase.

Can someone give me an hint on how to solve this problem?
Thanks"
keras,4746,"I am working on guided backprop for activation maximization. Instead of implementing rmsprop, Adam etc., I want to reuse optimizers defined in keras.
",0,How can i use keras optimizer for backprop-ing on my own loss functions,"How can i use keras optimizer for backprop-ing on my own loss functions I am working on guided backprop for activation maximization. Instead of implementing rmsprop, Adam etc., I want to reuse optimizers defined in keras.
"
keras,13016,"<em>Please make sure that this is a Bug or a Feature Request and provide all applicable information asked by the template.
If your issue is an **implementation question**, please ask your question on [StackOverflow](http://stackoverflow.com/questions/tagged/keras) or [on the Keras Slack channel](https://keras-slack-autojoin.herokuapp.com/) instead of opening a GitHub issue.</em>  

**System information**  
- Have I written custom code (as opposed to using example directory):  
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  
- TensorFlow backend (yes / no):  
- TensorFlow version:  
- Keras version:  
- Python version:  
- CUDA/cuDNN version:  
- GPU model and memory:  

You can obtain the TensorFlow version with:  
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""  
You can obtain the Keras version with:  
python -c 'import keras as k; print(k.__version__)'  

**Describe the current behavior**  

**Describe the expected behavior**  

**Code to reproduce the issue**  
Provide a reproducible test case that is the bare minimum necessary to generate the problem.  

**Other info / logs**  
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.  
",0,something wrong with U-NET,"something wrong with U-NET <em>Please make sure that this is a Bug or a Feature Request and provide all applicable information asked by the template.
If your issue is an **implementation question**, please ask your question on [StackOverflow](http://stackoverflow.com/questions/tagged/keras) or [on the Keras Slack channel](https://keras-slack-autojoin.herokuapp.com/) instead of opening a GitHub issue.</em>  

**System information**  
- Have I written custom code (as opposed to using example directory):  
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  
- TensorFlow backend (yes / no):  
- TensorFlow version:  
- Keras version:  
- Python version:  
- CUDA/cuDNN version:  
- GPU model and memory:  

You can obtain the TensorFlow version with:  
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""  
You can obtain the Keras version with:  
python -c 'import keras as k; print(k.__version__)'  

**Describe the current behavior**  

**Describe the expected behavior**  

**Code to reproduce the issue**  
Provide a reproducible test case that is the bare minimum necessary to generate the problem.  

**Other info / logs**  
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.  
"
keras,1732,"I have defined a custom objective which can be used to optimize auc directly, but the roc_auc_score() function is from sklearn which need to feed numpy array as args. But both the y_true and y_pred are tensor variable：



the function above gives an error: 
**theano.gof.fg.MissingInputError: ('Undeclared input',<TensorType(float32, matrix)>)**
",0,How to optimize AUC directly？,"How to optimize AUC directly？ I have defined a custom objective which can be used to optimize auc directly, but the roc_auc_score() function is from sklearn which need to feed numpy array as args. But both the y_true and y_pred are tensor variable：



the function above gives an error: 
**theano.gof.fg.MissingInputError: ('Undeclared input',<TensorType(float32, matrix)>)**
"
keras,12897,"<em>Please make sure that this is a Bug or a Feature Request and provide all applicable information asked by the template.
If your issue is an **implementation question**, please ask your question on [StackOverflow](http://stackoverflow.com/questions/tagged/keras) or [on the Keras Slack channel](https://keras-slack-autojoin.herokuapp.com/) instead of opening a GitHub issue.</em>  

**System information**  
- Have I written custom code (as opposed to using example directory):  
>> I used the default code exactly as written.

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  
Microsoft Windows 10 Enterprise

- TensorFlow backend (yes / no): 
yes
 
- TensorFlow version:  
1.13.1

- Keras version:  
2.2.4

- Python version:  
Python 3.7.3

- CUDA/cuDNN version:  
None.  I'm using cpu-only vanilla keras installed by Anaconda. 

- GPU model and memory:  
NVidia Quadra P1000


**Describe the current behavior**  
It crashes when I hit F5 in Spyder.  It talks about pickle error.



**Describe the expected behavior**  

I would expect it to get to line 29 and print something about the training sequence count.

Line 28, where it is choking, is 


It doesn't get to print ""train sequences"" after attempting to load data, so it isn't getting past line 29.  

When I re-select line 28, and hit F9 (run highlighted) it gives the same error again.

**Code to reproduce the issue**  
You already have it.  It was literally copy-paste-run-crash

**Other info / logs**  
Traceback is included in what it did.
",0,Run Default code for imdb_cnn.py it gives pickle-errors,"Run Default code for imdb_cnn.py it gives pickle-errors <em>Please make sure that this is a Bug or a Feature Request and provide all applicable information asked by the template.
If your issue is an **implementation question**, please ask your question on [StackOverflow](http://stackoverflow.com/questions/tagged/keras) or [on the Keras Slack channel](https://keras-slack-autojoin.herokuapp.com/) instead of opening a GitHub issue.</em>  

**System information**  
- Have I written custom code (as opposed to using example directory):  
>> I used the default code exactly as written.

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  
Microsoft Windows 10 Enterprise

- TensorFlow backend (yes / no): 
yes
 
- TensorFlow version:  
1.13.1

- Keras version:  
2.2.4

- Python version:  
Python 3.7.3

- CUDA/cuDNN version:  
None.  I'm using cpu-only vanilla keras installed by Anaconda. 

- GPU model and memory:  
NVidia Quadra P1000


**Describe the current behavior**  
It crashes when I hit F5 in Spyder.  It talks about pickle error.



**Describe the expected behavior**  

I would expect it to get to line 29 and print something about the training sequence count.

Line 28, where it is choking, is 


It doesn't get to print ""train sequences"" after attempting to load data, so it isn't getting past line 29.  

When I re-select line 28, and hit F9 (run highlighted) it gives the same error again.

**Code to reproduce the issue**  
You already have it.  It was literally copy-paste-run-crash

**Other info / logs**  
Traceback is included in what it did.
"
keras,1990,"Has anyone else ever had their machine throw a CPU error and shut off while fitting a model on the GPU?

This has happened to me ~10 times, both on my Macbook Pro (GeForce GT 750M) and on a Dell server (Tesla M2070-Q).
",0,CPU Error causing machine to shut down,"CPU Error causing machine to shut down Has anyone else ever had their machine throw a CPU error and shut off while fitting a model on the GPU?

This has happened to me ~10 times, both on my Macbook Pro (GeForce GT 750M) and on a Dell server (Tesla M2070-Q).
"
keras,8757,"Hello,

I would like to ask for support in HDF5Matrix to select data from multiple tables at the same time.
This is mainly due to constrains on the width of tables in hdf.
(Storing waveforms  in my case with roughly 1k data points each so I have to split them into multiple tables)

Example:
Hdf file:
/data/table1
/data/table2

m = Model()
a = HDF5Matrix(""file.hdf5"", ""/data/table1"")
b = HDF5Matrix(""file.hdf5"", ""/data/table2"")
a.fit(a+b)


",0,Support for Multitable input from hdf,"Support for Multitable input from hdf Hello,

I would like to ask for support in HDF5Matrix to select data from multiple tables at the same time.
This is mainly due to constrains on the width of tables in hdf.
(Storing waveforms  in my case with roughly 1k data points each so I have to split them into multiple tables)

Example:
Hdf file:
/data/table1
/data/table2

m = Model()
a = HDF5Matrix(""file.hdf5"", ""/data/table1"")
b = HDF5Matrix(""file.hdf5"", ""/data/table2"")
a.fit(a+b)


"
keras,8816,"I tried merging an Input Layer and stacked CNNs and it worked:



But I couldn't achive combining an Input Layer and LSTM, it gives dimension mismatch error in the Merge step, what can I do to match shapes?



Thanks!",0,How to merge LSTM and Input layer?,"How to merge LSTM and Input layer? I tried merging an Input Layer and stacked CNNs and it worked:



But I couldn't achive combining an Input Layer and LSTM, it gives dimension mismatch error in the Merge step, what can I do to match shapes?



Thanks!"
keras,10875,"Hello.
My task is to create cetrain amount of copies of the same network and run each in a separate thread, where they are waiting for data given batch at a time and once they received the data they should make a training step. I am using Keras + Tensorflow on GPU.

First problem that I ran into that simply creating a model in one thread and then trying to train it in another has to be handled with setting a graph and a session, otherwise I was getting an exception of trying to work with nodes from one graph summing up to nodes of another.
When I created separate sessions and graphs for each of the nodes and was specifying it with session.as_default and graph.as_default it was working, but the more processes I am running, the more TF sessions are created and they are completely slowing down the calculations. So I returned to the idea of having one session only, that I am getting via keras.backend.get_session() and using everywhere together with graph of this session.
But here the behavior becomes completely random. It might run without errors (for 1 and 2 workers), it might give an error of initialization


 or 



depending on how fast it starts the threads.

Network creation:



Then the update step looks like this:



What is the correct solution for using sessions and graphs in this situation?
",0,Multi-thread online training for multiple copies of a model,"Multi-thread online training for multiple copies of a model Hello.
My task is to create cetrain amount of copies of the same network and run each in a separate thread, where they are waiting for data given batch at a time and once they received the data they should make a training step. I am using Keras + Tensorflow on GPU.

First problem that I ran into that simply creating a model in one thread and then trying to train it in another has to be handled with setting a graph and a session, otherwise I was getting an exception of trying to work with nodes from one graph summing up to nodes of another.
When I created separate sessions and graphs for each of the nodes and was specifying it with session.as_default and graph.as_default it was working, but the more processes I am running, the more TF sessions are created and they are completely slowing down the calculations. So I returned to the idea of having one session only, that I am getting via keras.backend.get_session() and using everywhere together with graph of this session.
But here the behavior becomes completely random. It might run without errors (for 1 and 2 workers), it might give an error of initialization


 or 



depending on how fast it starts the threads.

Network creation:



Then the update step looks like this:



What is the correct solution for using sessions and graphs in this situation?
"
keras,7695,"Hi Folks,

I am using Keras and TF.
I am passing padded sequences as input (pad value=-1) and masking the input with a  layer with mask value set to -1.0. However, when I collect the output of  after the BiLSTM layer, I see the forward states are non-zero at the masked positions.



Here is the output:



Please make sure that the boxes below are checked before you submit your issue. If your issue is an implementation question, please ask your question on [StackOverflow](http://stackoverflow.com/questions/tagged/keras) or [join the Keras Slack channel](https://keras-slack-autojoin.herokuapp.com/) and ask there instead of filing a GitHub issue.

Thank you!

- [x] Check that you are up-to-date with the master branch of Keras. You can update with:
pip install git+git://github.com/fchollet/keras.git --upgrade --no-deps

- [x] If running on TensorFlow, check that you are up-to-date with the latest version. The installation instructions can be found [here](https://www.tensorflow.org/get_started/os_setup).

- [ ] If running on Theano, check that you are up-to-date with the master branch of Theano. You can update with:
pip install git+git://github.com/Theano/Theano.git --upgrade --no-deps

- [ ] Provide a link to a GitHub Gist of a Python script that can reproduce your issue (or just copy the script here if it is short).
",0,forward states of bidirectional lstm are not masked,"forward states of bidirectional lstm are not masked Hi Folks,

I am using Keras and TF.
I am passing padded sequences as input (pad value=-1) and masking the input with a  layer with mask value set to -1.0. However, when I collect the output of  after the BiLSTM layer, I see the forward states are non-zero at the masked positions.



Here is the output:



Please make sure that the boxes below are checked before you submit your issue. If your issue is an implementation question, please ask your question on [StackOverflow](http://stackoverflow.com/questions/tagged/keras) or [join the Keras Slack channel](https://keras-slack-autojoin.herokuapp.com/) and ask there instead of filing a GitHub issue.

Thank you!

- [x] Check that you are up-to-date with the master branch of Keras. You can update with:
pip install git+git://github.com/fchollet/keras.git --upgrade --no-deps

- [x] If running on TensorFlow, check that you are up-to-date with the latest version. The installation instructions can be found [here](https://www.tensorflow.org/get_started/os_setup).

- [ ] If running on Theano, check that you are up-to-date with the master branch of Theano. You can update with:
pip install git+git://github.com/Theano/Theano.git --upgrade --no-deps

- [ ] Provide a link to a GitHub Gist of a Python script that can reproduce your issue (or just copy the script here if it is short).
"
keras,7987,"The image_ocr code that is available in the example section, is it possible to use that code for multiple lines? I mean is this possible to use the code for a single printed document? ",0,Is it possible to use image_ocr example for multiple line image? ,"Is it possible to use image_ocr example for multiple line image?  The image_ocr code that is available in the example section, is it possible to use that code for multiple lines? I mean is this possible to use the code for a single printed document? "
keras,11539,"I'm confused by the behavior when I feed the output of, say, an embedding layer to an lstm layer, but also specify the input shape for the lstm layer.

Example code:





Here the output of the embedding layer has shape (None, None, 10). Then I specify the input shape of lstm to be (None, 32, 1), which does not match with the embedding layer's output. The model still compiles without problem.

What is the behavior here? Will  override the previous input shape, or the opposite?",0,lstm input_shape override input from previous layer?,"lstm input_shape override input from previous layer? I'm confused by the behavior when I feed the output of, say, an embedding layer to an lstm layer, but also specify the input shape for the lstm layer.

Example code:





Here the output of the embedding layer has shape (None, None, 10). Then I specify the input shape of lstm to be (None, 32, 1), which does not match with the embedding layer's output. The model still compiles without problem.

What is the behavior here? Will  override the previous input shape, or the opposite?"
keras,11090,"Is there a way to quickly change out the version of Keras in the ""Dockerfile""? 

Specifically, when using Keras docker, I  and add the command say to specify a version of Cuda. How do I change the version of keras in this?

Also is it possible to make sure that dependencies are the correct version too? For example if I use keras version 1.2 it would be nice for it to automatically pick the right theano, and thus pygpu, etc. for that version.

Thanks
",0,Specify a different version of Keras in Docker Install ,"Specify a different version of Keras in Docker Install  Is there a way to quickly change out the version of Keras in the ""Dockerfile""? 

Specifically, when using Keras docker, I  and add the command say to specify a version of Cuda. How do I change the version of keras in this?

Also is it possible to make sure that dependencies are the correct version too? For example if I use keras version 1.2 it would be nice for it to automatically pick the right theano, and thus pygpu, etc. for that version.

Thanks
"
keras,8367,"Please make sure that the boxes below are checked before you submit your issue. If your issue is an implementation question, please ask your question on [StackOverflow](http://stackoverflow.com/questions/tagged/keras) or [join the Keras Slack channel](https://keras-slack-autojoin.herokuapp.com/) and ask there instead of filing a GitHub issue.

Thank you!

- [x] Check that you are up-to-date with the master branch of Keras. You can update with:
pip install git+git://github.com/fchollet/keras.git --upgrade --no-deps

- [x] If running on TensorFlow, check that you are up-to-date with the latest version. The installation instructions can be found [here](https://www.tensorflow.org/get_started/os_setup).

- [x] If running on Theano, check that you are up-to-date with the master branch of Theano. You can update with:
pip install git+git://github.com/Theano/Theano.git --upgrade --no-deps

- [x] Provide a link to a GitHub Gist of a Python script that can reproduce your issue (or just copy the script here if it is short).
",0,Binary Cross Entropy loss function is not working with python 3.6,"Binary Cross Entropy loss function is not working with python 3.6 Please make sure that the boxes below are checked before you submit your issue. If your issue is an implementation question, please ask your question on [StackOverflow](http://stackoverflow.com/questions/tagged/keras) or [join the Keras Slack channel](https://keras-slack-autojoin.herokuapp.com/) and ask there instead of filing a GitHub issue.

Thank you!

- [x] Check that you are up-to-date with the master branch of Keras. You can update with:
pip install git+git://github.com/fchollet/keras.git --upgrade --no-deps

- [x] If running on TensorFlow, check that you are up-to-date with the latest version. The installation instructions can be found [here](https://www.tensorflow.org/get_started/os_setup).

- [x] If running on Theano, check that you are up-to-date with the master branch of Theano. You can update with:
pip install git+git://github.com/Theano/Theano.git --upgrade --no-deps

- [x] Provide a link to a GitHub Gist of a Python script that can reproduce your issue (or just copy the script here if it is short).
"
keras,12679,"I came across this article: https://www.fast.ai/2018/04/30/dawnbench-fastai/#imagenet and I wanted to try implementing dynamic image sizes using ImageDataGenerator and keras applications models (those with global pooling layer). This is to allow the model to accept any input shape for progressively increasing input shape during training, as well as training a variety of image sizes without having to resize the input. However, there are a few problems I encountered.

First, keras.preprocessing.ImageDataGenerator does not accept None or (None, None) as target size. When target_size=(None, None):

When target_size=None:


Also, while most keras applications models seem to accept (None, None, 3) as input shape, it doesn't seem to work with NASNet and ResNeXt. Here is the error when I get for ResNeXt for example:


Dynamic input shape may also break other models due to some implementation detail depending on input shape. For example, in a keras-contrib's implementation of ResNet, the stride value for Conv2D requires this calculation which will break if input shape is (None, None, 3)


It would really be great if ImageDataGenerator can support None type for target size for those fully convolutional networks and image models with global pooling which do not require a fixed input size.

Edit: I just realised that the problem is much more difficult than I initially thought since a numpy array representing a batch of images needs to have a fixed size",0,Feature Request: Accept None as target size for ImageDataGenerator,"Feature Request: Accept None as target size for ImageDataGenerator I came across this article: https://www.fast.ai/2018/04/30/dawnbench-fastai/#imagenet and I wanted to try implementing dynamic image sizes using ImageDataGenerator and keras applications models (those with global pooling layer). This is to allow the model to accept any input shape for progressively increasing input shape during training, as well as training a variety of image sizes without having to resize the input. However, there are a few problems I encountered.

First, keras.preprocessing.ImageDataGenerator does not accept None or (None, None) as target size. When target_size=(None, None):

When target_size=None:


Also, while most keras applications models seem to accept (None, None, 3) as input shape, it doesn't seem to work with NASNet and ResNeXt. Here is the error when I get for ResNeXt for example:


Dynamic input shape may also break other models due to some implementation detail depending on input shape. For example, in a keras-contrib's implementation of ResNet, the stride value for Conv2D requires this calculation which will break if input shape is (None, None, 3)


It would really be great if ImageDataGenerator can support None type for target size for those fully convolutional networks and image models with global pooling which do not require a fixed input size.

Edit: I just realised that the problem is much more difficult than I initially thought since a numpy array representing a batch of images needs to have a fixed size"
keras,12240," with beam search warns about deprecated TF function:

tf.sparse.SparseTensortf.sparse.to_dense

The solution is to change  to . It will not work in some earlier versions of TensorFlow (but I assume they're not supported).

- [x] Check that you are up-to-date with the master branch of Keras. You can update with:
 ()

- [x] Check that your version of TensorFlow is up-to-date. The installation instructions can be found [here](https://www.tensorflow.org/get_started/os_setup). (1.12.0)

- [x] Provide a link to a GitHub Gist of a Python script that can reproduce your issue (or just copy the script here if it is short).

",0,K.ctc_decode() beam search: tf.sparse_to_dense is deprecated,"K.ctc_decode() beam search: tf.sparse_to_dense is deprecated  with beam search warns about deprecated TF function:

tf.sparse.SparseTensortf.sparse.to_dense

The solution is to change  to . It will not work in some earlier versions of TensorFlow (but I assume they're not supported).

- [x] Check that you are up-to-date with the master branch of Keras. You can update with:
 ()

- [x] Check that your version of TensorFlow is up-to-date. The installation instructions can be found [here](https://www.tensorflow.org/get_started/os_setup). (1.12.0)

- [x] Provide a link to a GitHub Gist of a Python script that can reproduce your issue (or just copy the script here if it is short).

"
keras,11024,"Hi,

I'm running a seq2seq model using GRU's on 8 GPUs, using fit_generator and this is the error I'm having:
InvalidArgumentError: Incompatible shapes: [128,100] vs. [1024,100]
	 [[Node: replica_0/model_1/gru_1/while/add = Add[T=DT_FLOAT, _class=[""loc:@train.../Reshape_1""], _device=""/job:localhost/replica:0/task:0/device:GPU:0""](replica_0/model_1/gru_1/while/BiasAdd, replica_0/model_1/gru_1/while/MatMul_3)]]
	 [[Node: replica_0/model_1/gru_2/while/Identity/_967 = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/device:CPU:0"", send_device=""/job:localhost/replica:0/task:0/device:GPU:0"", send_device_incarnation=1, tensor_name=""edge_7766_replica_0/model_1/gru_2/while/Identity"", tensor_type=DT_INT32, _device=""/job:localhost/replica:0/task:0/device:CPU:0""](^_cloopreplica_0/model_1/gru_2/while/TensorArrayReadV3_1/_133)]]

keras was installed from master directly and updated to latest tensorflow-gpu from pip.

the generator is designed to yield a batch of 1024 so each GPU would work on a batch of 128.
if I reduce for example the number of GPUs to 4, I would have the following error:
InvalidArgumentError: Incompatible shapes: [256,100] vs. [1024,100]

for going over similar issues I've found the following issue #9449 and the merge request #10845 saying the issue is fixed. I've installed keras from source and I still have the issue.

Is it possible the issue is not fixed for GRUs?

this is my model definition:
word_input = Input(shape=(word_dim,))
decoder_inputs = Input(shape=(None,))
decoder_embed = Embedding(input_dim=num_tokens, output_dim=word_dim)
decoder_gru = GRU(word_dim, return_sequences=True, return_state=True)
decoder_dense = Dense(num_tokens, activation='softmax')

embedded = decoder_embed(decoder_inputs)
gru_output, state_h = decoder_gru(embedded, initial_state=word_input)
decoder_outputs = decoder_dense(gru_output)

model = Model([word_input, decoder_inputs], decoder_outputs)

rmsprop = optimizers.RMSprop(lr=0.001)

parallel_model = multi_gpu_model(model, gpus=8)
parallel_model.compile(loss='categorical_crossentropy',optimizer=rmsprop,metrics=['acc'])

filename = 'model.h5'
checkpoint = ModelCheckpoint(filename, monitor='loss', verbose=1, save_best_only=True, mode='min')

parallel_model.fit_generator(data_generator.generate(), training_size//batch_size*num_steps, num_epochs, callbacks=[checkpoint], verbose=1)

Thanks!


Please make sure that the boxes below are checked before you submit your issue. If your issue is an implementation question, please ask your question on [StackOverflow](http://stackoverflow.com/questions/tagged/keras) or [join the Keras Slack channel](https://keras-slack-autojoin.herokuapp.com/) and ask there instead of filing a GitHub issue.

Thank you!

- [x] Check that you are up-to-date with the master branch of Keras. You can update with:
pip install git+git://github.com/keras-team/keras.git --upgrade --no-deps

- [x] If running on TensorFlow, check that you are up-to-date with the latest version. The installation instructions can be found [here](https://www.tensorflow.org/get_started/os_setup).

- [x] If running on Theano, check that you are up-to-date with the master branch of Theano. You can update with:
pip install git+git://github.com/Theano/Theano.git --upgrade --no-deps

- [x] Provide a link to a GitHub Gist of a Python script that can reproduce your issue (or just copy the script here if it is short).",0,multi_gpu_model InvalidArgumentError on seq2seq model,"multi_gpu_model InvalidArgumentError on seq2seq model Hi,

I'm running a seq2seq model using GRU's on 8 GPUs, using fit_generator and this is the error I'm having:
InvalidArgumentError: Incompatible shapes: [128,100] vs. [1024,100]
	 [[Node: replica_0/model_1/gru_1/while/add = Add[T=DT_FLOAT, _class=[""loc:@train.../Reshape_1""], _device=""/job:localhost/replica:0/task:0/device:GPU:0""](replica_0/model_1/gru_1/while/BiasAdd, replica_0/model_1/gru_1/while/MatMul_3)]]
	 [[Node: replica_0/model_1/gru_2/while/Identity/_967 = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/device:CPU:0"", send_device=""/job:localhost/replica:0/task:0/device:GPU:0"", send_device_incarnation=1, tensor_name=""edge_7766_replica_0/model_1/gru_2/while/Identity"", tensor_type=DT_INT32, _device=""/job:localhost/replica:0/task:0/device:CPU:0""](^_cloopreplica_0/model_1/gru_2/while/TensorArrayReadV3_1/_133)]]

keras was installed from master directly and updated to latest tensorflow-gpu from pip.

the generator is designed to yield a batch of 1024 so each GPU would work on a batch of 128.
if I reduce for example the number of GPUs to 4, I would have the following error:
InvalidArgumentError: Incompatible shapes: [256,100] vs. [1024,100]

for going over similar issues I've found the following issue #9449 and the merge request #10845 saying the issue is fixed. I've installed keras from source and I still have the issue.

Is it possible the issue is not fixed for GRUs?

this is my model definition:
word_input = Input(shape=(word_dim,))
decoder_inputs = Input(shape=(None,))
decoder_embed = Embedding(input_dim=num_tokens, output_dim=word_dim)
decoder_gru = GRU(word_dim, return_sequences=True, return_state=True)
decoder_dense = Dense(num_tokens, activation='softmax')

embedded = decoder_embed(decoder_inputs)
gru_output, state_h = decoder_gru(embedded, initial_state=word_input)
decoder_outputs = decoder_dense(gru_output)

model = Model([word_input, decoder_inputs], decoder_outputs)

rmsprop = optimizers.RMSprop(lr=0.001)

parallel_model = multi_gpu_model(model, gpus=8)
parallel_model.compile(loss='categorical_crossentropy',optimizer=rmsprop,metrics=['acc'])

filename = 'model.h5'
checkpoint = ModelCheckpoint(filename, monitor='loss', verbose=1, save_best_only=True, mode='min')

parallel_model.fit_generator(data_generator.generate(), training_size//batch_size*num_steps, num_epochs, callbacks=[checkpoint], verbose=1)

Thanks!


Please make sure that the boxes below are checked before you submit your issue. If your issue is an implementation question, please ask your question on [StackOverflow](http://stackoverflow.com/questions/tagged/keras) or [join the Keras Slack channel](https://keras-slack-autojoin.herokuapp.com/) and ask there instead of filing a GitHub issue.

Thank you!

- [x] Check that you are up-to-date with the master branch of Keras. You can update with:
pip install git+git://github.com/keras-team/keras.git --upgrade --no-deps

- [x] If running on TensorFlow, check that you are up-to-date with the latest version. The installation instructions can be found [here](https://www.tensorflow.org/get_started/os_setup).

- [x] If running on Theano, check that you are up-to-date with the master branch of Theano. You can update with:
pip install git+git://github.com/Theano/Theano.git --upgrade --no-deps

- [x] Provide a link to a GitHub Gist of a Python script that can reproduce your issue (or just copy the script here if it is short)."
keras,10786,"Hi!

Is there a way I can use pre-trained weights from a model that takes input_shape of (batch_size, 50, 50, 4) to be initialized for another model that takes input_shape of  (batch_size, 50, 50, 1)? Output_shape has size of (50, 50, 1).

AND

Is there a way I can train a model that takes input_shape of (batch_size, 50, 50, 4) and test on data that has input_shape of  (batch_size, 50, 50, 1)? Output_shape has size of (50, 50, 1).

Thank you!",0,Train and test data that has different channel size,"Train and test data that has different channel size Hi!

Is there a way I can use pre-trained weights from a model that takes input_shape of (batch_size, 50, 50, 4) to be initialized for another model that takes input_shape of  (batch_size, 50, 50, 1)? Output_shape has size of (50, 50, 1).

AND

Is there a way I can train a model that takes input_shape of (batch_size, 50, 50, 4) and test on data that has input_shape of  (batch_size, 50, 50, 1)? Output_shape has size of (50, 50, 1).

Thank you!"
keras,8072,"I confuse with Lstm in Keras. I try to map Keras code to LSTM equation. I found this weights parameter in Keras code. 

![image](https://user-images.githubusercontent.com/10016227/31258172-999520a6-aa67-11e7-9fc4-ffaa6d646994.png)

I think that It's a weight from current weight of cell and recurrent weight of cell.

In the call function in LSTM Keras code, I found the statement for calculate input ,forget ,cell and output term.

![image](https://user-images.githubusercontent.com/10016227/31258223-e7627d42-aa67-11e7-8810-723bd7a05391.png)


In input term of LSTM equation have W_ci * C_t-1 but I can't find in Keras code.

![image](https://user-images.githubusercontent.com/10016227/31258220-e31a4d32-aa67-11e7-8139-ae91968239ec.png)


Could please anyone explains the LSTM equation and Keras code.",0,LSTM equation in Keras,"LSTM equation in Keras I confuse with Lstm in Keras. I try to map Keras code to LSTM equation. I found this weights parameter in Keras code. 

![image](https://user-images.githubusercontent.com/10016227/31258172-999520a6-aa67-11e7-9fc4-ffaa6d646994.png)

I think that It's a weight from current weight of cell and recurrent weight of cell.

In the call function in LSTM Keras code, I found the statement for calculate input ,forget ,cell and output term.

![image](https://user-images.githubusercontent.com/10016227/31258223-e7627d42-aa67-11e7-8810-723bd7a05391.png)


In input term of LSTM equation have W_ci * C_t-1 but I can't find in Keras code.

![image](https://user-images.githubusercontent.com/10016227/31258220-e31a4d32-aa67-11e7-8139-ae91968239ec.png)


Could please anyone explains the LSTM equation and Keras code."
keras,11471,"I am using a custom generator to get the data from my paths and it seems to be working fine. However, one issue I am facing with using a custom generator is that, unlike the default generators of Keras, I cannot really use attributes. What I am trying here for example is getting a report and confusion matrix on my testing set, however, I cannot find a way to input the classes from my generator. Here is the script:



This works well with a default generator, but in this case my testgenerator is defined by a custom generator and inside the custom generator I am using 2 default generators to read 2 streams of data. So obviously I get the error  So my question is, is there a way to define the attributes for my custom generator or is there another way to feed my labels to the confusion matrix without using attribute? Or can new feature be added to deal with this kind of situation ?

I tried: this instead:

> cm = confusion_matrix(np.argmax(testgenerator[1], axis=1), y_pred)

But it gives me  while it is supposed to yield a tuple of ([1,9,1024], [1,9,4])",0,"Access or Define ""classes"" attribute for custom generator","Access or Define ""classes"" attribute for custom generator I am using a custom generator to get the data from my paths and it seems to be working fine. However, one issue I am facing with using a custom generator is that, unlike the default generators of Keras, I cannot really use attributes. What I am trying here for example is getting a report and confusion matrix on my testing set, however, I cannot find a way to input the classes from my generator. Here is the script:



This works well with a default generator, but in this case my testgenerator is defined by a custom generator and inside the custom generator I am using 2 default generators to read 2 streams of data. So obviously I get the error  So my question is, is there a way to define the attributes for my custom generator or is there another way to feed my labels to the confusion matrix without using attribute? Or can new feature be added to deal with this kind of situation ?

I tried: this instead:

> cm = confusion_matrix(np.argmax(testgenerator[1], axis=1), y_pred)

But it gives me  while it is supposed to yield a tuple of ([1,9,1024], [1,9,4])"
keras,11618,"- [x] Check that you are up-to-date with the master branch of Keras. You can update with:


- [x] Check that your version of TensorFlow is up-to-date. The installation instructions can be found [here](https://www.tensorflow.org/get_started/os_setup).

- [x] Provide a link to a GitHub Gist of a Python script that can reproduce your issue (or just copy the script here if it is short).

https://gist.github.com/DSamuylov/1f3d42478be4e277f776783f215816cf

The code is not actually that long:

modelmodel_with_2_inputsmodel_finalmodel_final

I also added a print statement inside  method to print  and , and here there is the output:

modelmodel_with_2_inputsmodel_final
Am I wrong somewhere or there is a bug?",0,Failure in saving weights of a model that has a sub-model that shares weights of another model,"Failure in saving weights of a model that has a sub-model that shares weights of another model - [x] Check that you are up-to-date with the master branch of Keras. You can update with:


- [x] Check that your version of TensorFlow is up-to-date. The installation instructions can be found [here](https://www.tensorflow.org/get_started/os_setup).

- [x] Provide a link to a GitHub Gist of a Python script that can reproduce your issue (or just copy the script here if it is short).

https://gist.github.com/DSamuylov/1f3d42478be4e277f776783f215816cf

The code is not actually that long:

modelmodel_with_2_inputsmodel_finalmodel_final

I also added a print statement inside  method to print  and , and here there is the output:

modelmodel_with_2_inputsmodel_final
Am I wrong somewhere or there is a bug?"
keras,8331,"I have recently started getting  errors when running my training  (it's a bit confusing because because this wasn't happening before and I'm not sure what code introduced the issue).

I believe it's a thread related issue (I'm using the model that I am training from inside my training data generator). I was wondering if there's a way to use the model I'm training from inside my generator while avoiding this issue.

Full stack trace:



Related: https://github.com/fchollet/keras/issues/6462 https://github.com/fchollet/keras/issues/5511 https://github.com/fchollet/keras/issues/2397",0,StopIteration: Tensor [...] is not an element of this graph.,"StopIteration: Tensor [...] is not an element of this graph. I have recently started getting  errors when running my training  (it's a bit confusing because because this wasn't happening before and I'm not sure what code introduced the issue).

I believe it's a thread related issue (I'm using the model that I am training from inside my training data generator). I was wondering if there's a way to use the model I'm training from inside my generator while avoiding this issue.

Full stack trace:



Related: https://github.com/fchollet/keras/issues/6462 https://github.com/fchollet/keras/issues/5511 https://github.com/fchollet/keras/issues/2397"
keras,10284,"[all boxes checked, latest version of Keras etc.]

I want to run keras in a production system where files are stored in a distributed filesystem, and it's quite limiting that you can only pass a filename (string) to ModelCheckpoint and Model.save.

Can we add an option to use a file object instead of passing a filename? This would be uesful for cases where you want to save checkpoints to some cloud storage, or want to handle them in memory (pass a StringIO object).",0,Support file objects in ModelCheckpoint and Model.save,"Support file objects in ModelCheckpoint and Model.save [all boxes checked, latest version of Keras etc.]

I want to run keras in a production system where files are stored in a distributed filesystem, and it's quite limiting that you can only pass a filename (string) to ModelCheckpoint and Model.save.

Can we add an option to use a file object instead of passing a filename? This would be uesful for cases where you want to save checkpoints to some cloud storage, or want to handle them in memory (pass a StringIO object)."
keras,13335,"**System information**  
- Have I written custom code that you can find here :




- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Windows 10
- TensorFlow backend (yes / no):  yes
- TensorFlow version:  1.8.0
- Keras version:  2.1.5
- Python version:  3.6

When I use sess.run to obtain the output of a model (here the first layer of a pretrained VGG19), I obtain a different output that the one obtained by using the predict function of the model. I am looking at an inference output. There is no training at all. All the weights are freeze and not trainable.

Moreover each time, I run tf.global_variables_initializer, I will obtain a different result whereas there is no stochastic element in the code.

You can find a minimal code above.

",0,Side effect of tf.global_variables_initializer on evaluation of output model,"Side effect of tf.global_variables_initializer on evaluation of output model **System information**  
- Have I written custom code that you can find here :




- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Windows 10
- TensorFlow backend (yes / no):  yes
- TensorFlow version:  1.8.0
- Keras version:  2.1.5
- Python version:  3.6

When I use sess.run to obtain the output of a model (here the first layer of a pretrained VGG19), I obtain a different output that the one obtained by using the predict function of the model. I am looking at an inference output. There is no training at all. All the weights are freeze and not trainable.

Moreover each time, I run tf.global_variables_initializer, I will obtain a different result whereas there is no stochastic element in the code.

You can find a minimal code above.

"
keras,13562,"I want to limit the cpu usage of keras training program. I found this kaggle document(https://www.kaggle.com/c/porto-seguro-safe-driver-prediction/discussion/43383):



But with tensorflow 2.0, the ConfigProto and Session are deprecated. ",0,How to set cpu core number in tensorflow 2.0,"How to set cpu core number in tensorflow 2.0 I want to limit the cpu usage of keras training program. I found this kaggle document(https://www.kaggle.com/c/porto-seguro-safe-driver-prediction/discussion/43383):



But with tensorflow 2.0, the ConfigProto and Session are deprecated. "
keras,9407,"I am building my own objective function:

> def my_loss(y_true, y_pred):
>     y_true = tf.reshape(y_true,[-1,256*256,14])     #shape: (num_images, pixels, channels)
>     sum = K.sum(y_true,axis=1)
>     indices = K.tf.where(K.equal(sum, 0))   #Get all the indices where sum of y_true is 0 for each image/channel
>     y_true[ind] = y_pred[ind]     
>     return keras.losses.binary_crossentropy(y_true, y_pred)

What I am not able to figure out how to assign those  of y_true with the values of the same  of y_pred. Basically I cannot do this:  

Any help would be very much appreciated.",0,Replacing tensor values on specific indices ,"Replacing tensor values on specific indices  I am building my own objective function:

> def my_loss(y_true, y_pred):
>     y_true = tf.reshape(y_true,[-1,256*256,14])     #shape: (num_images, pixels, channels)
>     sum = K.sum(y_true,axis=1)
>     indices = K.tf.where(K.equal(sum, 0))   #Get all the indices where sum of y_true is 0 for each image/channel
>     y_true[ind] = y_pred[ind]     
>     return keras.losses.binary_crossentropy(y_true, y_pred)

What I am not able to figure out how to assign those  of y_true with the values of the same  of y_pred. Basically I cannot do this:  

Any help would be very much appreciated."
keras,10538,"hi guys, i'm using a autoencoder, and i batch with 128 samples. could be possible send more data to gpu and execute batch faster? i think i~m with some bottleneck at copy, i see dedicated gpu memory changing from 8GB to 0GB at windows, many many times, and gpu% (smi) stay at ~30% and memory copy ~5%

the idea is something like 

multi_gpu_model.fit_generator(
        dae_generator(),
        steps_per_epoch=ceil(len(df)/(batch_size*2)),
        cache_batchs=100, # for example send 100x data
        epochs=1000, 
        verbose=2)


i'm using tensorflow backend",0,fit_generator with gpu cache,"fit_generator with gpu cache hi guys, i'm using a autoencoder, and i batch with 128 samples. could be possible send more data to gpu and execute batch faster? i think i~m with some bottleneck at copy, i see dedicated gpu memory changing from 8GB to 0GB at windows, many many times, and gpu% (smi) stay at ~30% and memory copy ~5%

the idea is something like 

multi_gpu_model.fit_generator(
        dae_generator(),
        steps_per_epoch=ceil(len(df)/(batch_size*2)),
        cache_batchs=100, # for example send 100x data
        epochs=1000, 
        verbose=2)


i'm using tensorflow backend"
keras,9566,"Hi, dear all,

I know 2.0.8 is a little bit old, however, I observe an issue with 2.0.8 compared with 2.0.4. I don't if it is fixed in the latest version, if not, hope it will give you some information for next updates.

When I run the code with 2.0.8, it will pop out the error message like below:


Traceback (most recent call last):
  File ""/home/fxt120230/Study/MSP/Research/VisVAD/Samsung_VAD/Fusion_Part/ETE_VAD/vadete_run.py"", line 316, in <module>
    run_ete_exp(para_dict)
  File ""/home/fxt120230/Study/MSP/Research/VisVAD/Samsung_VAD/Fusion_Part/ETE_VAD/vadete_run.py"", line 112, in run_ete_exp
    validation_data=([aud_valid,vid_valid],valid_truth_mem,valid_weight),sample_weight=train_weight,verbose=verbose)
  File ""build/bdist.linux-x86_64/egg/keras/engine/training.py"", line 1601, in fit
  File ""build/bdist.linux-x86_64/egg/keras/engine/training.py"", line 1183, in _fit_loop
  File ""build/bdist.linux-x86_64/egg/keras/backend/theano_backend.py"", line 1223, in __call__
  File ""/usr/local/lib/python2.7/dist-packages/theano/compile/function_module.py"", line 917, in __call__
    storage_map=getattr(self.fn, 'storage_map', None))
  File ""/usr/local/lib/python2.7/dist-packages/theano/gof/link.py"", line 325, in raise_with_op
    reraise(exc_type, exc_value, exc_trace)
  File ""/usr/local/lib/python2.7/dist-packages/theano/compile/function_module.py"", line 903, in __call__
    self.fn() if output_subset is None else\
  File ""pygpu/gpuarray.pyx"", line 693, in pygpu.gpuarray.pygpu_empty (pygpu/gpuarray.c:9893)
  File ""pygpu/gpuarray.pyx"", line 301, in pygpu.gpuarray.array_empty (pygpu/gpuarray.c:5694)
pygpu.gpuarray.GpuArrayException: cuMemAlloc: CUDA_ERROR_OUT_OF_MEMORY: out of memory
Apply node that caused the error: GpuAllocEmpty{dtype='int32', context_name=None}(Elemwise{Composite{(Switch(LT(maximum(i0, i1), i2), (maximum(i0, i1) + i3), (maximum(i0, i1) - i4)) + i5)}}.0, Shape_i{0}.0, Shape_i{1}.0)
Toposort index: 1656
Inputs types: [TensorType(int64, scalar), TensorType(int64, scalar), TensorType(int64, scalar)]
Inputs shapes: [(), (), ()]
Inputs strides: [(), (), ()]
Inputs values: [array(2722), array(15360), array(6)]
Outputs clients: [[GpuIncSubtensor{InplaceSet;:int64:}(GpuAllocEmpty{dtype='int32', context_name=None}.0, Rebroadcast{0}.0, Constant{1})]]

HINT: Re-running with most Theano optimization disabled could give you a back-trace of when this node was created. This can be done with by setting the Theano flag 'optimizer=fast_compile'. If that does not work, Theano optimizations can be disabled with 'optimizer=None'.
HINT: Use the Theano flag 'exception_verbosity=high' for a debugprint and storage map footprint of this apply node.


The nework parameters are listed in the following table:


__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            (None, 2721, 5)      0                                            
__________________________________________________________________________________________________
input_2 (InputLayer)            (None, 2721, 23)     0                                            
__________________________________________________________________________________________________
masking_1 (Masking)             (None, 2721, 5)      0           input_1[0][0]                    
__________________________________________________________________________________________________
masking_2 (Masking)             (None, 2721, 23)     0           input_2[0][0]                    
__________________________________________________________________________________________________
time_distributed_1 (TimeDistrib (None, 2721, 16)     288         masking_1[0][0]                  
__________________________________________________________________________________________________
time_distributed_3 (TimeDistrib (None, 2721, 64)     4608        masking_2[0][0]                  
__________________________________________________________________________________________________
dropout_1 (Dropout)             (None, 2721, 16)     0           time_distributed_1[0][0]         
__________________________________________________________________________________________________
dropout_4 (Dropout)             (None, 2721, 64)     0           time_distributed_3[0][0]         
__________________________________________________________________________________________________
time_distributed_2 (TimeDistrib (None, 2721, 16)     816         dropout_1[0][0]                  
__________________________________________________________________________________________________
time_distributed_4 (TimeDistrib (None, 2721, 64)     12480       dropout_4[0][0]                  
__________________________________________________________________________________________________
dropout_2 (Dropout)             (None, 2721, 16)     0           time_distributed_2[0][0]         
__________________________________________________________________________________________________
dropout_5 (Dropout)             (None, 2721, 64)     0           time_distributed_4[0][0]         
__________________________________________________________________________________________________
lstm_1 (LSTM)                   (None, 2721, 16)     2112        dropout_2[0][0]                  
__________________________________________________________________________________________________
lstm_3 (LSTM)                   (None, 2721, 64)     33024       dropout_5[0][0]                  
__________________________________________________________________________________________________
dropout_3 (Dropout)             (None, 2721, 16)     0           lstm_1[0][0]                     
__________________________________________________________________________________________________
dropout_6 (Dropout)             (None, 2721, 64)     0           lstm_3[0][0]                     
__________________________________________________________________________________________________
bidirectional_1 (Bidirectional) (None, 2721, 32)     4224        dropout_3[0][0]                  
__________________________________________________________________________________________________
bidirectional_2 (Bidirectional) (None, 2721, 128)    66048       dropout_6[0][0]                  
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 2721, 160)    0           bidirectional_1[0][0]            
                                                                 bidirectional_2[0][0]            
__________________________________________________________________________________________________
dropout_7 (Dropout)             (None, 2721, 160)    0           concatenate_1[0][0]              
__________________________________________________________________________________________________
bidirectional_3 (Bidirectional) (None, 2721, 256)    295936      dropout_7[0][0]                  
__________________________________________________________________________________________________
dropout_8 (Dropout)             (None, 2721, 256)    0           bidirectional_3[0][0]            
__________________________________________________________________________________________________
bidirectional_4 (Bidirectional) (None, 2721, 256)    394240      dropout_8[0][0]                  
__________________________________________________________________________________________________
dropout_9 (Dropout)             (None, 2721, 256)    0           bidirectional_4[0][0]            
__________________________________________________________________________________________________
time_distributed_5 (TimeDistrib (None, 2721, 128)    98688       dropout_9[0][0]                  
__________________________________________________________________________________________________
dropout_10 (Dropout)            (None, 2721, 128)    0           time_distributed_5[0][0]         
__________________________________________________________________________________________________
time_distributed_6 (TimeDistrib (None, 2721, 2)      258         dropout_10[0][0]                 
__________________________________________________________________________________________________
activation_1 (Activation)       (None, 2721, 2)      0           time_distributed_6[0][0]         
==================================================================================================
Total params: 912,722
Trainable params: 912,722
Non-trainable params: 0
__________________________________________________________________________________________________


I am using theano 1.0.1. The GPU setting is printed below:
Using Theano backend.
Using cuDNN version 5110 on context None
Mapped name None to device cuda: GeForce GTX 1070 (0000:04:00.0)

When I switch it back to 2.0.4 without other change, the program can run successfully. Could anyone tell the reason and how to fix it?

Thank you very much!",0,Keras 2.0.8 model.fit() out of memory,"Keras 2.0.8 model.fit() out of memory Hi, dear all,

I know 2.0.8 is a little bit old, however, I observe an issue with 2.0.8 compared with 2.0.4. I don't if it is fixed in the latest version, if not, hope it will give you some information for next updates.

When I run the code with 2.0.8, it will pop out the error message like below:


Traceback (most recent call last):
  File ""/home/fxt120230/Study/MSP/Research/VisVAD/Samsung_VAD/Fusion_Part/ETE_VAD/vadete_run.py"", line 316, in <module>
    run_ete_exp(para_dict)
  File ""/home/fxt120230/Study/MSP/Research/VisVAD/Samsung_VAD/Fusion_Part/ETE_VAD/vadete_run.py"", line 112, in run_ete_exp
    validation_data=([aud_valid,vid_valid],valid_truth_mem,valid_weight),sample_weight=train_weight,verbose=verbose)
  File ""build/bdist.linux-x86_64/egg/keras/engine/training.py"", line 1601, in fit
  File ""build/bdist.linux-x86_64/egg/keras/engine/training.py"", line 1183, in _fit_loop
  File ""build/bdist.linux-x86_64/egg/keras/backend/theano_backend.py"", line 1223, in __call__
  File ""/usr/local/lib/python2.7/dist-packages/theano/compile/function_module.py"", line 917, in __call__
    storage_map=getattr(self.fn, 'storage_map', None))
  File ""/usr/local/lib/python2.7/dist-packages/theano/gof/link.py"", line 325, in raise_with_op
    reraise(exc_type, exc_value, exc_trace)
  File ""/usr/local/lib/python2.7/dist-packages/theano/compile/function_module.py"", line 903, in __call__
    self.fn() if output_subset is None else\
  File ""pygpu/gpuarray.pyx"", line 693, in pygpu.gpuarray.pygpu_empty (pygpu/gpuarray.c:9893)
  File ""pygpu/gpuarray.pyx"", line 301, in pygpu.gpuarray.array_empty (pygpu/gpuarray.c:5694)
pygpu.gpuarray.GpuArrayException: cuMemAlloc: CUDA_ERROR_OUT_OF_MEMORY: out of memory
Apply node that caused the error: GpuAllocEmpty{dtype='int32', context_name=None}(Elemwise{Composite{(Switch(LT(maximum(i0, i1), i2), (maximum(i0, i1) + i3), (maximum(i0, i1) - i4)) + i5)}}.0, Shape_i{0}.0, Shape_i{1}.0)
Toposort index: 1656
Inputs types: [TensorType(int64, scalar), TensorType(int64, scalar), TensorType(int64, scalar)]
Inputs shapes: [(), (), ()]
Inputs strides: [(), (), ()]
Inputs values: [array(2722), array(15360), array(6)]
Outputs clients: [[GpuIncSubtensor{InplaceSet;:int64:}(GpuAllocEmpty{dtype='int32', context_name=None}.0, Rebroadcast{0}.0, Constant{1})]]

HINT: Re-running with most Theano optimization disabled could give you a back-trace of when this node was created. This can be done with by setting the Theano flag 'optimizer=fast_compile'. If that does not work, Theano optimizations can be disabled with 'optimizer=None'.
HINT: Use the Theano flag 'exception_verbosity=high' for a debugprint and storage map footprint of this apply node.


The nework parameters are listed in the following table:


__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            (None, 2721, 5)      0                                            
__________________________________________________________________________________________________
input_2 (InputLayer)            (None, 2721, 23)     0                                            
__________________________________________________________________________________________________
masking_1 (Masking)             (None, 2721, 5)      0           input_1[0][0]                    
__________________________________________________________________________________________________
masking_2 (Masking)             (None, 2721, 23)     0           input_2[0][0]                    
__________________________________________________________________________________________________
time_distributed_1 (TimeDistrib (None, 2721, 16)     288         masking_1[0][0]                  
__________________________________________________________________________________________________
time_distributed_3 (TimeDistrib (None, 2721, 64)     4608        masking_2[0][0]                  
__________________________________________________________________________________________________
dropout_1 (Dropout)             (None, 2721, 16)     0           time_distributed_1[0][0]         
__________________________________________________________________________________________________
dropout_4 (Dropout)             (None, 2721, 64)     0           time_distributed_3[0][0]         
__________________________________________________________________________________________________
time_distributed_2 (TimeDistrib (None, 2721, 16)     816         dropout_1[0][0]                  
__________________________________________________________________________________________________
time_distributed_4 (TimeDistrib (None, 2721, 64)     12480       dropout_4[0][0]                  
__________________________________________________________________________________________________
dropout_2 (Dropout)             (None, 2721, 16)     0           time_distributed_2[0][0]         
__________________________________________________________________________________________________
dropout_5 (Dropout)             (None, 2721, 64)     0           time_distributed_4[0][0]         
__________________________________________________________________________________________________
lstm_1 (LSTM)                   (None, 2721, 16)     2112        dropout_2[0][0]                  
__________________________________________________________________________________________________
lstm_3 (LSTM)                   (None, 2721, 64)     33024       dropout_5[0][0]                  
__________________________________________________________________________________________________
dropout_3 (Dropout)             (None, 2721, 16)     0           lstm_1[0][0]                     
__________________________________________________________________________________________________
dropout_6 (Dropout)             (None, 2721, 64)     0           lstm_3[0][0]                     
__________________________________________________________________________________________________
bidirectional_1 (Bidirectional) (None, 2721, 32)     4224        dropout_3[0][0]                  
__________________________________________________________________________________________________
bidirectional_2 (Bidirectional) (None, 2721, 128)    66048       dropout_6[0][0]                  
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 2721, 160)    0           bidirectional_1[0][0]            
                                                                 bidirectional_2[0][0]            
__________________________________________________________________________________________________
dropout_7 (Dropout)             (None, 2721, 160)    0           concatenate_1[0][0]              
__________________________________________________________________________________________________
bidirectional_3 (Bidirectional) (None, 2721, 256)    295936      dropout_7[0][0]                  
__________________________________________________________________________________________________
dropout_8 (Dropout)             (None, 2721, 256)    0           bidirectional_3[0][0]            
__________________________________________________________________________________________________
bidirectional_4 (Bidirectional) (None, 2721, 256)    394240      dropout_8[0][0]                  
__________________________________________________________________________________________________
dropout_9 (Dropout)             (None, 2721, 256)    0           bidirectional_4[0][0]            
__________________________________________________________________________________________________
time_distributed_5 (TimeDistrib (None, 2721, 128)    98688       dropout_9[0][0]                  
__________________________________________________________________________________________________
dropout_10 (Dropout)            (None, 2721, 128)    0           time_distributed_5[0][0]         
__________________________________________________________________________________________________
time_distributed_6 (TimeDistrib (None, 2721, 2)      258         dropout_10[0][0]                 
__________________________________________________________________________________________________
activation_1 (Activation)       (None, 2721, 2)      0           time_distributed_6[0][0]         
==================================================================================================
Total params: 912,722
Trainable params: 912,722
Non-trainable params: 0
__________________________________________________________________________________________________


I am using theano 1.0.1. The GPU setting is printed below:
Using Theano backend.
Using cuDNN version 5110 on context None
Mapped name None to device cuda: GeForce GTX 1070 (0000:04:00.0)

When I switch it back to 2.0.4 without other change, the program can run successfully. Could anyone tell the reason and how to fix it?

Thank you very much!"
keras,8546,"I am not sure how to fix this error. Can you please guide? found the code from @flyyufelix on 
https://github.com/flyyufelix/cnn_finetune/blob/master/resnet_50.py




I get this error:
Conv2DConv2D(512, (3, 3), name=""res5b_branch2b"", padding=""same"")Conv2DConv2D(2048, (1, 1), name=""res5b_branch2c"")Conv2DConv2D(512, (1, 1), name=""res5c_branch2a"")Conv2DConv2D(512, (3, 3), name=""res5c_branch2b"", padding=""same"")Conv2DConv2D(2048, (1, 1), name=""res5c_branch2c"")Conv2DConv2D(64, (7, 7), name=""conv1"", strides=(2, 2))Conv2DConv2D(64, (1, 1), name=""res2a_branch2a"", strides=(1, 1))Conv2DConv2D(64, (3, 3), name=""res2a_branch2b"", padding=""same"")Conv2DConv2D(256, (1, 1), name=""res2a_branch2c"")Conv2DConv2D(256, (1, 1), name=""res2a_branch1"", strides=(1, 1))mergekeras.layers.mergeaddconcatenateMergekeras.layers.mergeaddconcatenateConv2DConv2D(64, (1, 1), name=""res2b_branch2a"")Conv2DConv2D(64, (3, 3), name=""res2b_branch2b"", padding=""same"")Conv2DConv2D(256, (1, 1), name=""res2b_branch2c"")mergekeras.layers.mergeaddconcatenateConv2DConv2D(64, (1, 1), name=""res2c_branch2a"")Conv2DConv2D(64, (3, 3), name=""res2c_branch2b"", padding=""same"")Conv2DConv2D(256, (1, 1), name=""res2c_branch2c"")Conv2DConv2D(128, (1, 1), name=""res3a_branch2a"", strides=(2, 2))Conv2DConv2D(128, (3, 3), name=""res3a_branch2b"", padding=""same"")Conv2DConv2D(512, (1, 1), name=""res3a_branch2c"")Conv2DConv2D(512, (1, 1), name=""res3a_branch1"", strides=(2, 2))Conv2DConv2D(128, (1, 1), name=""res3b_branch2a"")Conv2DConv2D(128, (3, 3), name=""res3b_branch2b"", padding=""same"")Conv2DConv2D(512, (1, 1), name=""res3b_branch2c"")Conv2DConv2D(128, (1, 1), name=""res3c_branch2a"")Conv2DConv2D(128, (3, 3), name=""res3c_branch2b"", padding=""same"")Conv2DConv2D(512, (1, 1), name=""res3c_branch2c"")Conv2DConv2D(128, (1, 1), name=""res3d_branch2a"")Conv2DConv2D(128, (3, 3), name=""res3d_branch2b"", padding=""same"")Conv2DConv2D(512, (1, 1), name=""res3d_branch2c"")Conv2DConv2D(256, (1, 1), name=""res4a_branch2a"", strides=(2, 2))Conv2DConv2D(256, (3, 3), name=""res4a_branch2b"", padding=""same"")Conv2DConv2D(1024, (1, 1), name=""res4a_branch2c"")Conv2DConv2D(1024, (1, 1), name=""res4a_branch1"", strides=(2, 2))Conv2DConv2D(256, (1, 1), name=""res4b_branch2a"")Conv2DConv2D(256, (3, 3), name=""res4b_branch2b"", padding=""same"")Conv2DConv2D(1024, (1, 1), name=""res4b_branch2c"")Conv2DConv2D(256, (1, 1), name=""res4c_branch2a"")Conv2DConv2D(256, (3, 3), name=""res4c_branch2b"", padding=""same"")Conv2DConv2D(1024, (1, 1), name=""res4c_branch2c"")Conv2DConv2D(256, (1, 1), name=""res4d_branch2a"")Conv2DConv2D(256, (3, 3), name=""res4d_branch2b"", padding=""same"")Conv2DConv2D(1024, (1, 1), name=""res4d_branch2c"")Conv2DConv2D(256, (1, 1), name=""res4e_branch2a"")Conv2DConv2D(256, (3, 3), name=""res4e_branch2b"", padding=""same"")Conv2DConv2D(1024, (1, 1), name=""res4e_branch2c"")Conv2DConv2D(256, (1, 1), name=""res4f_branch2a"")Conv2DConv2D(256, (3, 3), name=""res4f_branch2b"", padding=""same"")Conv2DConv2D(1024, (1, 1), name=""res4f_branch2c"")Conv2DConv2D(512, (1, 1), name=""res5a_branch2a"", strides=(2, 2))Conv2DConv2D(512, (3, 3), name=""res5a_branch2b"", padding=""same"")Conv2DConv2D(2048, (1, 1), name=""res5a_branch2c"")Conv2DConv2D(2048, (1, 1), name=""res5a_branch1"", strides=(2, 2))Conv2DConv2D(512, (1, 1), name=""res5b_branch2a"")Conv2DConv2D(512, (3, 3), name=""res5b_branch2b"", padding=""same"")Conv2DConv2D(2048, (1, 1), name=""res5b_branch2c"")Conv2DConv2D(512, (1, 1), name=""res5c_branch2a"")Conv2DConv2D(512, (3, 3), name=""res5c_branch2b"", padding=""same"")Conv2DConv2D(2048, (1, 1), name=""res5c_branch2c"")

",0,"OSError: Unable to open file (truncated file: eof = 98304, sblock->base_addr = 0, stored_eoa = 102853048) removing the model from keras/model didn't solve","OSError: Unable to open file (truncated file: eof = 98304, sblock->base_addr = 0, stored_eoa = 102853048) removing the model from keras/model didn't solve I am not sure how to fix this error. Can you please guide? found the code from @flyyufelix on 
https://github.com/flyyufelix/cnn_finetune/blob/master/resnet_50.py




I get this error:
Conv2DConv2D(512, (3, 3), name=""res5b_branch2b"", padding=""same"")Conv2DConv2D(2048, (1, 1), name=""res5b_branch2c"")Conv2DConv2D(512, (1, 1), name=""res5c_branch2a"")Conv2DConv2D(512, (3, 3), name=""res5c_branch2b"", padding=""same"")Conv2DConv2D(2048, (1, 1), name=""res5c_branch2c"")Conv2DConv2D(64, (7, 7), name=""conv1"", strides=(2, 2))Conv2DConv2D(64, (1, 1), name=""res2a_branch2a"", strides=(1, 1))Conv2DConv2D(64, (3, 3), name=""res2a_branch2b"", padding=""same"")Conv2DConv2D(256, (1, 1), name=""res2a_branch2c"")Conv2DConv2D(256, (1, 1), name=""res2a_branch1"", strides=(1, 1))mergekeras.layers.mergeaddconcatenateMergekeras.layers.mergeaddconcatenateConv2DConv2D(64, (1, 1), name=""res2b_branch2a"")Conv2DConv2D(64, (3, 3), name=""res2b_branch2b"", padding=""same"")Conv2DConv2D(256, (1, 1), name=""res2b_branch2c"")mergekeras.layers.mergeaddconcatenateConv2DConv2D(64, (1, 1), name=""res2c_branch2a"")Conv2DConv2D(64, (3, 3), name=""res2c_branch2b"", padding=""same"")Conv2DConv2D(256, (1, 1), name=""res2c_branch2c"")Conv2DConv2D(128, (1, 1), name=""res3a_branch2a"", strides=(2, 2))Conv2DConv2D(128, (3, 3), name=""res3a_branch2b"", padding=""same"")Conv2DConv2D(512, (1, 1), name=""res3a_branch2c"")Conv2DConv2D(512, (1, 1), name=""res3a_branch1"", strides=(2, 2))Conv2DConv2D(128, (1, 1), name=""res3b_branch2a"")Conv2DConv2D(128, (3, 3), name=""res3b_branch2b"", padding=""same"")Conv2DConv2D(512, (1, 1), name=""res3b_branch2c"")Conv2DConv2D(128, (1, 1), name=""res3c_branch2a"")Conv2DConv2D(128, (3, 3), name=""res3c_branch2b"", padding=""same"")Conv2DConv2D(512, (1, 1), name=""res3c_branch2c"")Conv2DConv2D(128, (1, 1), name=""res3d_branch2a"")Conv2DConv2D(128, (3, 3), name=""res3d_branch2b"", padding=""same"")Conv2DConv2D(512, (1, 1), name=""res3d_branch2c"")Conv2DConv2D(256, (1, 1), name=""res4a_branch2a"", strides=(2, 2))Conv2DConv2D(256, (3, 3), name=""res4a_branch2b"", padding=""same"")Conv2DConv2D(1024, (1, 1), name=""res4a_branch2c"")Conv2DConv2D(1024, (1, 1), name=""res4a_branch1"", strides=(2, 2))Conv2DConv2D(256, (1, 1), name=""res4b_branch2a"")Conv2DConv2D(256, (3, 3), name=""res4b_branch2b"", padding=""same"")Conv2DConv2D(1024, (1, 1), name=""res4b_branch2c"")Conv2DConv2D(256, (1, 1), name=""res4c_branch2a"")Conv2DConv2D(256, (3, 3), name=""res4c_branch2b"", padding=""same"")Conv2DConv2D(1024, (1, 1), name=""res4c_branch2c"")Conv2DConv2D(256, (1, 1), name=""res4d_branch2a"")Conv2DConv2D(256, (3, 3), name=""res4d_branch2b"", padding=""same"")Conv2DConv2D(1024, (1, 1), name=""res4d_branch2c"")Conv2DConv2D(256, (1, 1), name=""res4e_branch2a"")Conv2DConv2D(256, (3, 3), name=""res4e_branch2b"", padding=""same"")Conv2DConv2D(1024, (1, 1), name=""res4e_branch2c"")Conv2DConv2D(256, (1, 1), name=""res4f_branch2a"")Conv2DConv2D(256, (3, 3), name=""res4f_branch2b"", padding=""same"")Conv2DConv2D(1024, (1, 1), name=""res4f_branch2c"")Conv2DConv2D(512, (1, 1), name=""res5a_branch2a"", strides=(2, 2))Conv2DConv2D(512, (3, 3), name=""res5a_branch2b"", padding=""same"")Conv2DConv2D(2048, (1, 1), name=""res5a_branch2c"")Conv2DConv2D(2048, (1, 1), name=""res5a_branch1"", strides=(2, 2))Conv2DConv2D(512, (1, 1), name=""res5b_branch2a"")Conv2DConv2D(512, (3, 3), name=""res5b_branch2b"", padding=""same"")Conv2DConv2D(2048, (1, 1), name=""res5b_branch2c"")Conv2DConv2D(512, (1, 1), name=""res5c_branch2a"")Conv2DConv2D(512, (3, 3), name=""res5c_branch2b"", padding=""same"")Conv2DConv2D(2048, (1, 1), name=""res5c_branch2c"")

"
keras,8450,"I built a project using (Merge, merge) layer once, and use share layer in another. The problem I can't find any documentation that explains how merge or share layer works like convolution or max-pooling layers.
I will be grateful if anyone can direct me or suggest some paper to me that helps me to understand how these layers work ???",0,merge layer documentation and architecture  ,"merge layer documentation and architecture   I built a project using (Merge, merge) layer once, and use share layer in another. The problem I can't find any documentation that explains how merge or share layer works like convolution or max-pooling layers.
I will be grateful if anyone can direct me or suggest some paper to me that helps me to understand how these layers work ???"
keras,10821,"I've been thinking about the extra robustness one gets during training by randomizing what data from the training set goes into each mini-batches, but leaving some percentage of them out during each fit. I believe this may be possible at present by setting  and manually entering  to be less than the actual number that will be used. 

However, I think this is a useful enough feature that it could deserve its own option. Perhaps we could call it  which automatically sets , checks to be sure that mini-batches are being used, and either correctly sets , or ignores thae last  of the mini-batches after each reshuffle by adding a couple lines of extra code to the fitting routine.
",0,Could we add an option to model.fit() to keep out N mini-batches per epoch for more robust training?,"Could we add an option to model.fit() to keep out N mini-batches per epoch for more robust training? I've been thinking about the extra robustness one gets during training by randomizing what data from the training set goes into each mini-batches, but leaving some percentage of them out during each fit. I believe this may be possible at present by setting  and manually entering  to be less than the actual number that will be used. 

However, I think this is a useful enough feature that it could deserve its own option. Perhaps we could call it  which automatically sets , checks to be sure that mini-batches are being used, and either correctly sets , or ignores thae last  of the mini-batches after each reshuffle by adding a couple lines of extra code to the fitting routine.
"
keras,6368,"I'm trying to adopt a drop based learning rate decay strategy to my categorical data classification task.
Everything goes fine until I pass the callback to the KerasClassifier wrapper. 

# Drop-Based Learning Rate Decay
import numpy
from pandas import read_csv
from keras.models import Sequential
from keras.layers import Dense
from keras.wrappers.scikit_learn import KerasClassifier
from sklearn.model_selection import cross_val_score
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import StratifiedKFold
from sklearn.preprocessing import MinMaxScaler
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline
from keras.utils import np_utils
from pandas import read_csv
import numpy
import math
from sklearn.preprocessing import LabelEncoder
from keras.callbacks import LearningRateScheduler

# learning rate schedule
def step_decay(epoch):
    initial_lrate = 0.2
    drop = 0.5
    epochs_drop = 10.0
    lrate = initial_lrate * math.pow(drop, math.floor((1+epoch)/epochs_drop))
    return lrate

# fix random seed for reproducibility
seed = 7
numpy.random.seed(seed)

dataframe = read_csv(""Book1.csv"", header=None)
dataset = dataframe.values
X = dataset[:,0:15].astype(float)
Y = dataset[:,15]
#One hot encoding or creating dummy variables from a categorical variable (class)
# encode class values as integers
encoder = LabelEncoder()
encoder.fit(Y)
encoded_Y = encoder.transform(Y)
# convert integers to dummy variables (i.e. one hot encoded)
dummy_y = np_utils.to_categorical(encoded_Y)

# create model
def baseline_model():
    # create model
    model = Sequential()
    model.add(Dense(50, input_dim=15, kernel_initializer='normal', activation='relu'))
    model.add(Dense(3, kernel_initializer='normal', activation='sigmoid'))
    sgd = SGD(lr=0.0, momentum=0.9, decay=0, nesterov=False)
    model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])
    return model
    
#learning schedule callback
lrate = LearningRateScheduler(step_decay)
callbacks_list = [lrate]
estimators = []
estimators.append(('MinMaxScale', MinMaxScaler()))
estimators.append(('standardize', StandardScaler()))
estimators.append(('mlp', KerasClassifier(build_fn=baseline_model, epochs=100,
batch_size=5, callbacks=[lrate], verbose=1)))
pipeline = Pipeline(estimators)
kfold = StratifiedKFold(n_splits=2, shuffle=True, random_state=seed)
results = cross_val_score(pipeline, X, encoded_Y, cv=kfold)
print(""Standardized: %.2f%% (%.2f%%)"" % (results.mean()*100, results.std()*100))
When I run the code, exactly after passing the callback to the kerasclassifier wrapper (at estimators.append(('mlp', KerasClassifier(build_fn=baseline_model, epochs=100,
batch_size=5, callbacks=[lrate], verbose=1))), I get the error:
""Cannot clone object <keras.wrappers.scikit_learn.KerasClassifier object at 0x0000000042582AC8>, as the constructor does not seem to set parameter callbacks""
I would like to get this code corrected with the right way of passing callbacks to the wrapper. thanks in advance.
 ",0,Issue with passing a callback to keras classifier wrapper,"Issue with passing a callback to keras classifier wrapper I'm trying to adopt a drop based learning rate decay strategy to my categorical data classification task.
Everything goes fine until I pass the callback to the KerasClassifier wrapper. 

# Drop-Based Learning Rate Decay
import numpy
from pandas import read_csv
from keras.models import Sequential
from keras.layers import Dense
from keras.wrappers.scikit_learn import KerasClassifier
from sklearn.model_selection import cross_val_score
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import StratifiedKFold
from sklearn.preprocessing import MinMaxScaler
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline
from keras.utils import np_utils
from pandas import read_csv
import numpy
import math
from sklearn.preprocessing import LabelEncoder
from keras.callbacks import LearningRateScheduler

# learning rate schedule
def step_decay(epoch):
    initial_lrate = 0.2
    drop = 0.5
    epochs_drop = 10.0
    lrate = initial_lrate * math.pow(drop, math.floor((1+epoch)/epochs_drop))
    return lrate

# fix random seed for reproducibility
seed = 7
numpy.random.seed(seed)

dataframe = read_csv(""Book1.csv"", header=None)
dataset = dataframe.values
X = dataset[:,0:15].astype(float)
Y = dataset[:,15]
#One hot encoding or creating dummy variables from a categorical variable (class)
# encode class values as integers
encoder = LabelEncoder()
encoder.fit(Y)
encoded_Y = encoder.transform(Y)
# convert integers to dummy variables (i.e. one hot encoded)
dummy_y = np_utils.to_categorical(encoded_Y)

# create model
def baseline_model():
    # create model
    model = Sequential()
    model.add(Dense(50, input_dim=15, kernel_initializer='normal', activation='relu'))
    model.add(Dense(3, kernel_initializer='normal', activation='sigmoid'))
    sgd = SGD(lr=0.0, momentum=0.9, decay=0, nesterov=False)
    model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])
    return model
    
#learning schedule callback
lrate = LearningRateScheduler(step_decay)
callbacks_list = [lrate]
estimators = []
estimators.append(('MinMaxScale', MinMaxScaler()))
estimators.append(('standardize', StandardScaler()))
estimators.append(('mlp', KerasClassifier(build_fn=baseline_model, epochs=100,
batch_size=5, callbacks=[lrate], verbose=1)))
pipeline = Pipeline(estimators)
kfold = StratifiedKFold(n_splits=2, shuffle=True, random_state=seed)
results = cross_val_score(pipeline, X, encoded_Y, cv=kfold)
print(""Standardized: %.2f%% (%.2f%%)"" % (results.mean()*100, results.std()*100))
When I run the code, exactly after passing the callback to the kerasclassifier wrapper (at estimators.append(('mlp', KerasClassifier(build_fn=baseline_model, epochs=100,
batch_size=5, callbacks=[lrate], verbose=1))), I get the error:
""Cannot clone object <keras.wrappers.scikit_learn.KerasClassifier object at 0x0000000042582AC8>, as the constructor does not seem to set parameter callbacks""
I would like to get this code corrected with the right way of passing callbacks to the wrapper. thanks in advance.
 "
keras,12585,"Please make sure that the boxes below are checked before you submit your issue.
If your issue is an **implementation question**, please ask your question on [StackOverflow](http://stackoverflow.com/questions/tagged/keras) or [on the Keras Slack channel](https://keras-slack-autojoin.herokuapp.com/) instead of opening a GitHub issue.

Thank you!

- [x] Check that you are up-to-date with the master branch of Keras. You can update with:


- [x] Check that your version of TensorFlow is up-to-date. The installation instructions can be found [here](https://www.tensorflow.org/get_started/os_setup).

- [ ] Provide a link to a GitHub Gist of a Python script that can reproduce your issue (or just copy the script here if it is short).
",0,Tensorboard AttributeError: 'Model' object has no attribute '_eval_function',"Tensorboard AttributeError: 'Model' object has no attribute '_eval_function' Please make sure that the boxes below are checked before you submit your issue.
If your issue is an **implementation question**, please ask your question on [StackOverflow](http://stackoverflow.com/questions/tagged/keras) or [on the Keras Slack channel](https://keras-slack-autojoin.herokuapp.com/) instead of opening a GitHub issue.

Thank you!

- [x] Check that you are up-to-date with the master branch of Keras. You can update with:


- [x] Check that your version of TensorFlow is up-to-date. The installation instructions can be found [here](https://www.tensorflow.org/get_started/os_setup).

- [ ] Provide a link to a GitHub Gist of a Python script that can reproduce your issue (or just copy the script here if it is short).
"
keras,13522,"I want to perform Hyperparameter Optimization on my Keras Model. The problem is the dataset is quite big, normally in training I use fit_generator to load the data in batch from disk, but the common package like SKlearn Gridsearch, etc. only support fit method. How to use GridSearchCV to implement fit_generator() method? Is it possible to use Keras's scikit-learn API together with fit_generator() method? Thank you.",0,How to use KerasClassifier with fit_generator method?,"How to use KerasClassifier with fit_generator method? I want to perform Hyperparameter Optimization on my Keras Model. The problem is the dataset is quite big, normally in training I use fit_generator to load the data in batch from disk, but the common package like SKlearn Gridsearch, etc. only support fit method. How to use GridSearchCV to implement fit_generator() method? Is it possible to use Keras's scikit-learn API together with fit_generator() method? Thank you."
keras,11416,"Hi, 
I'm working on a heteroscedactic neural network. 
What I want is to get two neural networks and the loss function of each NN depends on the another NN. 
Here a short example : 



Is there a way to do so? Either by define in a correct way my loss functions or by define in another way my NN? 

Thank's for your help
Charles",0,Two models with cross loss function,"Two models with cross loss function Hi, 
I'm working on a heteroscedactic neural network. 
What I want is to get two neural networks and the loss function of each NN depends on the another NN. 
Here a short example : 



Is there a way to do so? Either by define in a correct way my loss functions or by define in another way my NN? 

Thank's for your help
Charles"
keras,9758,"I noticed that ImageDataGenerator, when setting the parameter zoom_range, uses a different random transformation for the horizontal and vertical axis, which produces images with a random aspect ratio.

I checked the source code ([here](https://github.com/keras-team/keras/blob/master/keras/preprocessing/image.py#L853)) and the zoom transformation is indeed random for the horizontal and vertical axis. I think this is counterintuitive because I'm expecting a ""zoom"" transformation to simply magnify (or unmagnify) the input image. Instead, the output of zoom_range are images which are randomly stretched horizontally or vertically.

I suggest introducing an additional parameter like a boolean ""zoom_keep_ar"" (aspect ratio) which if True, keeps the aspect ratio constant (simple magnification, zx=zy=random), if False it also randomly change the aspect ratio (zx=rand, zy=rand).",0,zoom_range in ImageDataGenerator uses different scales for x and y,"zoom_range in ImageDataGenerator uses different scales for x and y I noticed that ImageDataGenerator, when setting the parameter zoom_range, uses a different random transformation for the horizontal and vertical axis, which produces images with a random aspect ratio.

I checked the source code ([here](https://github.com/keras-team/keras/blob/master/keras/preprocessing/image.py#L853)) and the zoom transformation is indeed random for the horizontal and vertical axis. I think this is counterintuitive because I'm expecting a ""zoom"" transformation to simply magnify (or unmagnify) the input image. Instead, the output of zoom_range are images which are randomly stretched horizontally or vertically.

I suggest introducing an additional parameter like a boolean ""zoom_keep_ar"" (aspect ratio) which if True, keeps the aspect ratio constant (simple magnification, zx=zy=random), if False it also randomly change the aspect ratio (zx=rand, zy=rand)."
keras,10890,"I propose that Dice Score/Loss (also known as F1-score or Sorensen score) is added as a metric and loss function as these are very commonly used in image segmentation or bounding box problems. Within the medical community, this is an incredibly important function, although I have seen it in other areas like [astronomy](https://github.com/jakeret/tf_unet).

Additionally, the Intersection Over Union (IoU) (also known as Jaccard Index) is another important metric/loss for these same classes of problem. While the Dice and IoU are very similar functions, the Dice Score weights true positives (the intersection) more heavily than false positives and false negatives than IoU (which gives a more even weighting to TP, FP, & FN). In cases where false positives and false negatives can be very detrimental, the IoU will produce a better result than Dice. Andrew Ng (Stanford Prof, Google Brain co-founder, Coursera founder) even devotes an [entire video](https://www.coursera.org/lecture/convolutional-neural-networks/intersection-over-union-p9gxz) to IoU in his convolutional neural networks (CNN's) course as the metric to use to determine if your bounding box predictions are working.

According to Pull #7032 which sought to add Dice, the end result was the request was closed and that it will only be added if the community continues to bring it up and express interest in adding Dice ([See Comment](https://github.com/keras-team/keras/pull/7032#issuecomment-311459497) by @fchollet ).

As time has passed and interest in CNN's has [skyrocketed](https://trends.google.com/trends/explore?cat=174&date=all&geo=US&q=convolutional%20neural%20network), I suggest that we reconsider adding Dice and IoU as they are becoming more and more common place. Dice has been proposed or mentioned in issues and pull requests multiple times (#292, #369, #2115, #2994, #3442, #3457, #3611, #3653, #3977, #5916, #6933, #7032, #8961, #9154, #9275, #9395, #9444, #9671, #10783). Additionally, IoU has also been mentioned a number of times in this repository (#2016, #2185, #6467, #6538, #8225, #8643, #8669, #9367, #10104, #10602, #10783). Keep in mind that those references are only with in _this_ repository... there are plenty of other Github repositories that use Dice and/or IoU as a loss function.

Furthermore, the research community has used Dice or IoU in numerous papers that make use of CNN's. Here are a few that each have over 100 citations according to Google Scholar, though many more exist... [1](https://ieeexplore.ieee.org/abstract/document/7785132/), [2](https://ieeexplore.ieee.org/abstract/document/7444155/), [3](http://papers.nips.cc/paper/5207-deep-neural-networks-for-object-detection), [4](https://www.cv-foundation.org/openaccess/content_cvpr_2014/papers/Erhan_Scalable_Object_Detection_2014_CVPR_paper.pdf), [5](https://www.cv-foundation.org/openaccess/content_cvpr_workshops_2015/W01/html/Brebisson_Deep_Neural_Networks_2015_CVPR_paper.html), [6](https://www.sciencedirect.com/science/article/pii/S1053811914010660), [7](https://link.springer.com/chapter/10.1007/978-3-319-46723-8_48), [8](https://www.computer.org/csdl/proceedings/icip/1994/6952/02/00413580.pdf), [9](https://link.springer.com/article/10.1007/s11548-016-1501-5), [10](https://ieeexplore.ieee.org/abstract/document/7482843/) (sorry ahead of time if they are behind a paywall).

I personally have been working in medical research with a U-Net for image segmentation and have found that training the model with binary cross entropy as my loss function at first and then switching to dice loss for additional training has significantly improved my performance over using only binary cross entropy. I have been using a custom loss to use Dice loss, however, it would be great to see an official version of this supported by Keras.

Given that over a year has past since PR #7032, would the Keras team reconsider implementing an official version of Dice and IoU loss functions?",0,Add Dice Loss (and Intersection Over Union),"Add Dice Loss (and Intersection Over Union) I propose that Dice Score/Loss (also known as F1-score or Sorensen score) is added as a metric and loss function as these are very commonly used in image segmentation or bounding box problems. Within the medical community, this is an incredibly important function, although I have seen it in other areas like [astronomy](https://github.com/jakeret/tf_unet).

Additionally, the Intersection Over Union (IoU) (also known as Jaccard Index) is another important metric/loss for these same classes of problem. While the Dice and IoU are very similar functions, the Dice Score weights true positives (the intersection) more heavily than false positives and false negatives than IoU (which gives a more even weighting to TP, FP, & FN). In cases where false positives and false negatives can be very detrimental, the IoU will produce a better result than Dice. Andrew Ng (Stanford Prof, Google Brain co-founder, Coursera founder) even devotes an [entire video](https://www.coursera.org/lecture/convolutional-neural-networks/intersection-over-union-p9gxz) to IoU in his convolutional neural networks (CNN's) course as the metric to use to determine if your bounding box predictions are working.

According to Pull #7032 which sought to add Dice, the end result was the request was closed and that it will only be added if the community continues to bring it up and express interest in adding Dice ([See Comment](https://github.com/keras-team/keras/pull/7032#issuecomment-311459497) by @fchollet ).

As time has passed and interest in CNN's has [skyrocketed](https://trends.google.com/trends/explore?cat=174&date=all&geo=US&q=convolutional%20neural%20network), I suggest that we reconsider adding Dice and IoU as they are becoming more and more common place. Dice has been proposed or mentioned in issues and pull requests multiple times (#292, #369, #2115, #2994, #3442, #3457, #3611, #3653, #3977, #5916, #6933, #7032, #8961, #9154, #9275, #9395, #9444, #9671, #10783). Additionally, IoU has also been mentioned a number of times in this repository (#2016, #2185, #6467, #6538, #8225, #8643, #8669, #9367, #10104, #10602, #10783). Keep in mind that those references are only with in _this_ repository... there are plenty of other Github repositories that use Dice and/or IoU as a loss function.

Furthermore, the research community has used Dice or IoU in numerous papers that make use of CNN's. Here are a few that each have over 100 citations according to Google Scholar, though many more exist... [1](https://ieeexplore.ieee.org/abstract/document/7785132/), [2](https://ieeexplore.ieee.org/abstract/document/7444155/), [3](http://papers.nips.cc/paper/5207-deep-neural-networks-for-object-detection), [4](https://www.cv-foundation.org/openaccess/content_cvpr_2014/papers/Erhan_Scalable_Object_Detection_2014_CVPR_paper.pdf), [5](https://www.cv-foundation.org/openaccess/content_cvpr_workshops_2015/W01/html/Brebisson_Deep_Neural_Networks_2015_CVPR_paper.html), [6](https://www.sciencedirect.com/science/article/pii/S1053811914010660), [7](https://link.springer.com/chapter/10.1007/978-3-319-46723-8_48), [8](https://www.computer.org/csdl/proceedings/icip/1994/6952/02/00413580.pdf), [9](https://link.springer.com/article/10.1007/s11548-016-1501-5), [10](https://ieeexplore.ieee.org/abstract/document/7482843/) (sorry ahead of time if they are behind a paywall).

I personally have been working in medical research with a U-Net for image segmentation and have found that training the model with binary cross entropy as my loss function at first and then switching to dice loss for additional training has significantly improved my performance over using only binary cross entropy. I have been using a custom loss to use Dice loss, however, it would be great to see an official version of this supported by Keras.

Given that over a year has past since PR #7032, would the Keras team reconsider implementing an official version of Dice and IoU loss functions?"
keras,6501,"Hi,

if I want to reuse stateful LSTM layers in a separate model like in the code below (very simplified version)

I get the following error when calling otherModel.predict(...)


whereas if I set stateful to False, everything works.
ubuntu 16.04, python 3.5.2, tensorflow 1.1.0",0,Share stateful LSTM layers,"Share stateful LSTM layers Hi,

if I want to reuse stateful LSTM layers in a separate model like in the code below (very simplified version)

I get the following error when calling otherModel.predict(...)


whereas if I set stateful to False, everything works.
ubuntu 16.04, python 3.5.2, tensorflow 1.1.0"
keras,11750,"Hi guys, 
Excuse me for this question without much details, because I'm bound with time in order to post my results on a challenge platform, anyway I try to sum up as much as I can. I have a multivariate time-series, that I trained using an RNN, there are periods and repeating time indexes, from 2013-01 to 2016-09, steps are months, by repeating, I mean various subsets ordered from January to December, many times for the same year, for hundreds of times, and I am predicting the next year knowing other features. I trained using LSTM, on 3 years, and trying to predict also repeating time-series for the year 2017. I used fixed batch size, and one last layer for binary target value so I used such a basic neural network:



The batch size is 12 ( I chose) for 12 months, the target in train is very unbalanced with



the problem is that predicting on test like



is very frustrating before posting my results as all values of probability are above 0.5 and near 1 that means no probability for any entry to be zero.

what could be wrong !!?

Many thanks !! 
Hope to hear from you as soon as possible in order to tweak my model and get more coherent outputs ! :) :)",0,Predicting on real test set gives only very high probability for 1 for a very unbalanced data,"Predicting on real test set gives only very high probability for 1 for a very unbalanced data Hi guys, 
Excuse me for this question without much details, because I'm bound with time in order to post my results on a challenge platform, anyway I try to sum up as much as I can. I have a multivariate time-series, that I trained using an RNN, there are periods and repeating time indexes, from 2013-01 to 2016-09, steps are months, by repeating, I mean various subsets ordered from January to December, many times for the same year, for hundreds of times, and I am predicting the next year knowing other features. I trained using LSTM, on 3 years, and trying to predict also repeating time-series for the year 2017. I used fixed batch size, and one last layer for binary target value so I used such a basic neural network:



The batch size is 12 ( I chose) for 12 months, the target in train is very unbalanced with



the problem is that predicting on test like



is very frustrating before posting my results as all values of probability are above 0.5 and near 1 that means no probability for any entry to be zero.

what could be wrong !!?

Many thanks !! 
Hope to hear from you as soon as possible in order to tweak my model and get more coherent outputs ! :) :)"
keras,12871,"I am trying similar things, but I am getting stuck with the input of the model.

 


I first trained 3 different models:

- model 1 = 3 labels (8,9,10)
- model 2 = 2 labels (30,31)
- model 3 = 4 labels (80,81,82,83)

I would like to combine these 3 models into a final model with 1 output containing 9 labels (8,9,10,30,31,80,81,82,83). The final input needs to get 1 input image instead of 3 images. But I still get stuck. I am building a kind of hierarchy here to improve the accuracy.",0,Keras Issue concatenate single input instead of multiple,"Keras Issue concatenate single input instead of multiple I am trying similar things, but I am getting stuck with the input of the model.

 


I first trained 3 different models:

- model 1 = 3 labels (8,9,10)
- model 2 = 2 labels (30,31)
- model 3 = 4 labels (80,81,82,83)

I would like to combine these 3 models into a final model with 1 output containing 9 labels (8,9,10,30,31,80,81,82,83). The final input needs to get 1 input image instead of 3 images. But I still get stuck. I am building a kind of hierarchy here to improve the accuracy."
keras,13218,"**System information**  
- Have I written custom code (as opposed to using example directory):  yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Linux centos 7
- TensorFlow backend (yes / no):  yes
- TensorFlow version:  v1.12.1-8795-ga675686 1.15.0-dev20190814 (tf-nightly dist)
- Keras version:  2.2.4
- Python version:  3.6
- CUDA/cuDNN version:  10.2
- GPU model and memory:  1080ti _ 11GB

**Describe the current behavior**

Save model returns a ""Not Jason serialisable"" error similar, but not exactly the same, as some of the other issues posted here, which are supposedly resolved by a tf-nightly install. I cannot provide the one-hot-encoded input data.

**Describe the expected behavior** 

Save the best model's weights to a hdf5 file.

**Code to reproduce the issue**  



**Other info / logs**  

TypeError                                 Traceback (most recent call last)
<ipython-input-22-4d0435c2448d> in <module>()
      1 get_ipython().run_line_magic('time', '')
----> 2 CNN_1D_model = keras_1D_CNN()
      3 CNN_history_dict = CNN_1D_model[1].history
      4 CNN_history_dict.keys()
      5 print(""\n%s: %.2f%%"" % (CNN_1D_model[0].metrics_names[1], CNN_1D_model[2][1]*100))

<ipython-input-21-2bf565bb8ec7> in keras_1D_CNN()
    100               epochs=1000,
    101               batch_size=2048,
--> 102               validation_split=0.2, shuffle=True, callbacks=[tensorboard, earlystopper, checkpoint], verbose=0)
    103 #     CNN_history = model.fit(partial_seq_train,
    104 #               partial_label_train,

/d/harpy1/s/python/v3-5.1.0/lib/python3.6/site-packages/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)
   1037                                         initial_epoch=initial_epoch,
   1038                                         steps_per_epoch=steps_per_epoch,
-> 1039                                         validation_steps=validation_steps)
   1040 
   1041     def evaluate(self, x=None, y=None,

/d/harpy1/s/python/v3-5.1.0/lib/python3.6/site-packages/keras/engine/training_arrays.py in fit_loop(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)
    215                         for l, o in zip(out_labels, val_outs):
    216                             epoch_logs['val_' + l] = o
--> 217         callbacks.on_epoch_end(epoch, epoch_logs)
    218         if callback_model.stop_training:
    219             break

/d/harpy1/s/python/v3-5.1.0/lib/python3.6/site-packages/keras/callbacks.py in on_epoch_end(self, epoch, logs)
     77         logs = logs or {}
     78         for callback in self.callbacks:
---> 79             callback.on_epoch_end(epoch, logs)
     80 
     81     def on_batch_begin(self, batch, logs=None):

/d/harpy1/s/python/v3-5.1.0/lib/python3.6/site-packages/keras/callbacks.py in on_epoch_end(self, epoch, logs)
    444                             self.model.save_weights(filepath, overwrite=True)
    445                         else:
--> 446                             self.model.save(filepath, overwrite=True)
    447                     else:
    448                         if self.verbose > 0:

/d/harpy1/s/python/v3-5.1.0/lib/python3.6/site-packages/keras/engine/network.py in save(self, filepath, overwrite, include_optimizer)
   1088             raise NotImplementedError
   1089         from ..models import save_model
-> 1090         save_model(self, filepath, overwrite, include_optimizer)
   1091 
   1092     def save_weights(self, filepath, overwrite=True):

/d/harpy1/s/python/v3-5.1.0/lib/python3.6/site-packages/keras/engine/saving.py in save_model(model, filepath, overwrite, include_optimizer)
    380 
    381     try:
--> 382         _serialize_model(model, f, include_optimizer)
    383     finally:
    384         if opened_new_file:

/d/harpy1/s/python/v3-5.1.0/lib/python3.6/site-packages/keras/engine/saving.py in _serialize_model(model, f, include_optimizer)
     82     model_config['class_name'] = model.__class__.__name__
     83     model_config['config'] = model.get_config()
---> 84     model_config = json.dumps(model_config, default=get_json_type)
     85     model_config = model_config.encode('utf-8')
     86     f['model_config'] = model_config

/d/harpy1/s/python/v3-5.1.0/lib/python3.6/json/__init__.py in dumps(obj, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, default, sort_keys, **kw)
    236         check_circular=check_circular, allow_nan=allow_nan, indent=indent,
    237         separators=separators, default=default, sort_keys=sort_keys,
--> 238         **kw).encode(obj)
    239 
    240 

/d/harpy1/s/python/v3-5.1.0/lib/python3.6/json/encoder.py in encode(self, o)
    197         # exceptions aren't as detailed.  The list call should be roughly
    198         # equivalent to the PySequence_Fast that ''.join() would do.
--> 199         chunks = self.iterencode(o, _one_shot=True)
    200         if not isinstance(chunks, (list, tuple)):
    201             chunks = list(chunks)

/d/harpy1/s/python/v3-5.1.0/lib/python3.6/json/encoder.py in iterencode(self, o, _one_shot)
    255                 self.key_separator, self.item_separator, self.sort_keys,
    256                 self.skipkeys, _one_shot)
--> 257         return _iterencode(o, 0)
    258 
    259 def _make_iterencode(markers, _default, _encoder, _indent, _floatstr,

/d/harpy1/s/python/v3-5.1.0/lib/python3.6/site-packages/keras/engine/saving.py in get_json_type(obj)
     72             return obj.__name__
     73 
---> 74         raise TypeError('Not JSON Serializable: %s' % (obj,))
     75 
     76     from .. import __version__ as keras_version

TypeError: Not JSON Serializable: <module 'tensorflow' from '/d/harpy1/s/python/v3-5.1.0/lib/python3.6/site-packages/tensorflow/__init__.py'>
",0,TypeError: Not JSON Serializable: <module 'tensorflow' from '/path/python/v3-5.1.0/lib/python3.6/site-packages/tensorflow/__init__.py'>,"TypeError: Not JSON Serializable: <module 'tensorflow' from '/path/python/v3-5.1.0/lib/python3.6/site-packages/tensorflow/__init__.py'> **System information**  
- Have I written custom code (as opposed to using example directory):  yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Linux centos 7
- TensorFlow backend (yes / no):  yes
- TensorFlow version:  v1.12.1-8795-ga675686 1.15.0-dev20190814 (tf-nightly dist)
- Keras version:  2.2.4
- Python version:  3.6
- CUDA/cuDNN version:  10.2
- GPU model and memory:  1080ti _ 11GB

**Describe the current behavior**

Save model returns a ""Not Jason serialisable"" error similar, but not exactly the same, as some of the other issues posted here, which are supposedly resolved by a tf-nightly install. I cannot provide the one-hot-encoded input data.

**Describe the expected behavior** 

Save the best model's weights to a hdf5 file.

**Code to reproduce the issue**  



**Other info / logs**  

TypeError                                 Traceback (most recent call last)
<ipython-input-22-4d0435c2448d> in <module>()
      1 get_ipython().run_line_magic('time', '')
----> 2 CNN_1D_model = keras_1D_CNN()
      3 CNN_history_dict = CNN_1D_model[1].history
      4 CNN_history_dict.keys()
      5 print(""\n%s: %.2f%%"" % (CNN_1D_model[0].metrics_names[1], CNN_1D_model[2][1]*100))

<ipython-input-21-2bf565bb8ec7> in keras_1D_CNN()
    100               epochs=1000,
    101               batch_size=2048,
--> 102               validation_split=0.2, shuffle=True, callbacks=[tensorboard, earlystopper, checkpoint], verbose=0)
    103 #     CNN_history = model.fit(partial_seq_train,
    104 #               partial_label_train,

/d/harpy1/s/python/v3-5.1.0/lib/python3.6/site-packages/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)
   1037                                         initial_epoch=initial_epoch,
   1038                                         steps_per_epoch=steps_per_epoch,
-> 1039                                         validation_steps=validation_steps)
   1040 
   1041     def evaluate(self, x=None, y=None,

/d/harpy1/s/python/v3-5.1.0/lib/python3.6/site-packages/keras/engine/training_arrays.py in fit_loop(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)
    215                         for l, o in zip(out_labels, val_outs):
    216                             epoch_logs['val_' + l] = o
--> 217         callbacks.on_epoch_end(epoch, epoch_logs)
    218         if callback_model.stop_training:
    219             break

/d/harpy1/s/python/v3-5.1.0/lib/python3.6/site-packages/keras/callbacks.py in on_epoch_end(self, epoch, logs)
     77         logs = logs or {}
     78         for callback in self.callbacks:
---> 79             callback.on_epoch_end(epoch, logs)
     80 
     81     def on_batch_begin(self, batch, logs=None):

/d/harpy1/s/python/v3-5.1.0/lib/python3.6/site-packages/keras/callbacks.py in on_epoch_end(self, epoch, logs)
    444                             self.model.save_weights(filepath, overwrite=True)
    445                         else:
--> 446                             self.model.save(filepath, overwrite=True)
    447                     else:
    448                         if self.verbose > 0:

/d/harpy1/s/python/v3-5.1.0/lib/python3.6/site-packages/keras/engine/network.py in save(self, filepath, overwrite, include_optimizer)
   1088             raise NotImplementedError
   1089         from ..models import save_model
-> 1090         save_model(self, filepath, overwrite, include_optimizer)
   1091 
   1092     def save_weights(self, filepath, overwrite=True):

/d/harpy1/s/python/v3-5.1.0/lib/python3.6/site-packages/keras/engine/saving.py in save_model(model, filepath, overwrite, include_optimizer)
    380 
    381     try:
--> 382         _serialize_model(model, f, include_optimizer)
    383     finally:
    384         if opened_new_file:

/d/harpy1/s/python/v3-5.1.0/lib/python3.6/site-packages/keras/engine/saving.py in _serialize_model(model, f, include_optimizer)
     82     model_config['class_name'] = model.__class__.__name__
     83     model_config['config'] = model.get_config()
---> 84     model_config = json.dumps(model_config, default=get_json_type)
     85     model_config = model_config.encode('utf-8')
     86     f['model_config'] = model_config

/d/harpy1/s/python/v3-5.1.0/lib/python3.6/json/__init__.py in dumps(obj, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, default, sort_keys, **kw)
    236         check_circular=check_circular, allow_nan=allow_nan, indent=indent,
    237         separators=separators, default=default, sort_keys=sort_keys,
--> 238         **kw).encode(obj)
    239 
    240 

/d/harpy1/s/python/v3-5.1.0/lib/python3.6/json/encoder.py in encode(self, o)
    197         # exceptions aren't as detailed.  The list call should be roughly
    198         # equivalent to the PySequence_Fast that ''.join() would do.
--> 199         chunks = self.iterencode(o, _one_shot=True)
    200         if not isinstance(chunks, (list, tuple)):
    201             chunks = list(chunks)

/d/harpy1/s/python/v3-5.1.0/lib/python3.6/json/encoder.py in iterencode(self, o, _one_shot)
    255                 self.key_separator, self.item_separator, self.sort_keys,
    256                 self.skipkeys, _one_shot)
--> 257         return _iterencode(o, 0)
    258 
    259 def _make_iterencode(markers, _default, _encoder, _indent, _floatstr,

/d/harpy1/s/python/v3-5.1.0/lib/python3.6/site-packages/keras/engine/saving.py in get_json_type(obj)
     72             return obj.__name__
     73 
---> 74         raise TypeError('Not JSON Serializable: %s' % (obj,))
     75 
     76     from .. import __version__ as keras_version

TypeError: Not JSON Serializable: <module 'tensorflow' from '/d/harpy1/s/python/v3-5.1.0/lib/python3.6/site-packages/tensorflow/__init__.py'>
"
keras,7783,"Please make sure that the boxes below are checked before you submit your issue. If your issue is an implementation question, please ask your question on [StackOverflow](http://stackoverflow.com/questions/tagged/keras) or [join the Keras Slack channel](https://keras-slack-autojoin.herokuapp.com/) and ask there instead of filing a GitHub issue.

Thank you!

- [x] Check that you are up-to-date with the master branch of Keras. You can update with:
pip install git+git://github.com/fchollet/keras.git --upgrade --no-deps

- [x] If running on TensorFlow, check that you are up-to-date with the latest version. The installation instructions can be found [here](https://www.tensorflow.org/get_started/os_setup).

- [ ] If running on Theano, check that you are up-to-date with the master branch of Theano. You can update with:
pip install git+git://github.com/Theano/Theano.git --upgrade --no-deps

- [ ] Provide a link to a GitHub Gist of a Python script that can reproduce your issue (or just copy the script here if it is short).

running this:
	resnet = resnet.get_layer('activation_49').output
	flatten = Flatten()(resnet)
	seed = Input(shape=(7,))

	merged = concatenate([flatten, seed], axis=-1)

results in : zero-dimensional arrays cannot be concatenated

However, running this:
	resnet = resnet.get_layer('activation_49').output
	flatten = Flatten()(resnet)
	seed = Input(shape=(7,))

	merged = merge([flatten, seed], mode='concat', concat_axis=-1)

compiles well without errors! Bug?",0,bug in concatenation layer,"bug in concatenation layer Please make sure that the boxes below are checked before you submit your issue. If your issue is an implementation question, please ask your question on [StackOverflow](http://stackoverflow.com/questions/tagged/keras) or [join the Keras Slack channel](https://keras-slack-autojoin.herokuapp.com/) and ask there instead of filing a GitHub issue.

Thank you!

- [x] Check that you are up-to-date with the master branch of Keras. You can update with:
pip install git+git://github.com/fchollet/keras.git --upgrade --no-deps

- [x] If running on TensorFlow, check that you are up-to-date with the latest version. The installation instructions can be found [here](https://www.tensorflow.org/get_started/os_setup).

- [ ] If running on Theano, check that you are up-to-date with the master branch of Theano. You can update with:
pip install git+git://github.com/Theano/Theano.git --upgrade --no-deps

- [ ] Provide a link to a GitHub Gist of a Python script that can reproduce your issue (or just copy the script here if it is short).

running this:
	resnet = resnet.get_layer('activation_49').output
	flatten = Flatten()(resnet)
	seed = Input(shape=(7,))

	merged = concatenate([flatten, seed], axis=-1)

results in : zero-dimensional arrays cannot be concatenated

However, running this:
	resnet = resnet.get_layer('activation_49').output
	flatten = Flatten()(resnet)
	seed = Input(shape=(7,))

	merged = merge([flatten, seed], mode='concat', concat_axis=-1)

compiles well without errors! Bug?"
keras,10661,"When running the following script



using TensorFlow backend:


using CNTK backend:


with CNTK Keras outputs incorrectl tensor for state.",0,Incorrect state dimensions for GRU when using CNTK backend,"Incorrect state dimensions for GRU when using CNTK backend When running the following script



using TensorFlow backend:


using CNTK backend:


with CNTK Keras outputs incorrectl tensor for state."
keras,7462,"If generator is used for the parameter validation_data when calling fit generator, how do you calculate custom metrics at the end of each epoch? Just wanted to try and understand how others are going about this problem.

Here's the code I use:

model.fit_generator(generate_data_from_file('data/0.1-percent/training-data.tsv', binarizer, batch_size),
        validation_data=generate_data_from_file('data/0.1-percent/validation-data.tsv', binarizer, batch_size),
        steps_per_epoch=math.ceil(1.0 * 109527 / batch_size),
        validation_steps=math.ceil(1.0 * 13692 / batch_size),
        epochs=10,
        verbose=1,
        class_weight=class_weights,
        max_queue_size=2)",0,metrics calculation with callbacks when using fit_generator with generators for both training and validation,"metrics calculation with callbacks when using fit_generator with generators for both training and validation If generator is used for the parameter validation_data when calling fit generator, how do you calculate custom metrics at the end of each epoch? Just wanted to try and understand how others are going about this problem.

Here's the code I use:

model.fit_generator(generate_data_from_file('data/0.1-percent/training-data.tsv', binarizer, batch_size),
        validation_data=generate_data_from_file('data/0.1-percent/validation-data.tsv', binarizer, batch_size),
        steps_per_epoch=math.ceil(1.0 * 109527 / batch_size),
        validation_steps=math.ceil(1.0 * 13692 / batch_size),
        epochs=10,
        verbose=1,
        class_weight=class_weights,
        max_queue_size=2)"
keras,12798,"Please make sure that the boxes below are checked before you submit your issue. If your issue is an implementation question, please ask your question on [StackOverflow](http://stackoverflow.com/questions/tagged/keras) or [join the Keras Slack channel](https://keras-slack-autojoin.herokuapp.com/) and ask there instead of filing a GitHub issue.

Thank you!

- [x] Check that you are up-to-date with the master branch of Keras. You can update with:
pip install git+git://github.com/keras-team/keras.git --upgrade --no-deps

- [x] If running on TensorFlow, check that you are up-to-date with the latest version. The installation instructions can be found [here](https://www.tensorflow.org/get_started/os_setup).

- [x] If running on Theano, check that you are up-to-date with the master branch of Theano. You can update with:
pip install git+git://github.com/Theano/Theano.git --upgrade --no-deps

- [x] Provide a link to a GitHub Gist of a Python script that can reproduce your issue (or just copy the script here if it is short).

I want to the Slack Github integration to notify my team of activity in , but I received an error:
 


To reproduce:

1. Add GitHub app to your Slack workspace
2. In a channel, run 
3. See error 
",0,Enable Slack Github Integration and Notifications,"Enable Slack Github Integration and Notifications Please make sure that the boxes below are checked before you submit your issue. If your issue is an implementation question, please ask your question on [StackOverflow](http://stackoverflow.com/questions/tagged/keras) or [join the Keras Slack channel](https://keras-slack-autojoin.herokuapp.com/) and ask there instead of filing a GitHub issue.

Thank you!

- [x] Check that you are up-to-date with the master branch of Keras. You can update with:
pip install git+git://github.com/keras-team/keras.git --upgrade --no-deps

- [x] If running on TensorFlow, check that you are up-to-date with the latest version. The installation instructions can be found [here](https://www.tensorflow.org/get_started/os_setup).

- [x] If running on Theano, check that you are up-to-date with the master branch of Theano. You can update with:
pip install git+git://github.com/Theano/Theano.git --upgrade --no-deps

- [x] Provide a link to a GitHub Gist of a Python script that can reproduce your issue (or just copy the script here if it is short).

I want to the Slack Github integration to notify my team of activity in , but I received an error:
 


To reproduce:

1. Add GitHub app to your Slack workspace
2. In a channel, run 
3. See error 
"
keras,13031,"Hi
I have used Keras wrappers for Scikit to implement Adaboost for single input Keras models. This solution works fine for Dense layers, but not for 3D layers. The moment I try to use any 3D layers such as LSTM or SimpleRNN, the first model training runs ok but immediately thereafter I get the following error message.
ValueError: Input 0 of layer simple_rnn_1 is incompatible with the layer: expected ndim=3, found ndim=2. Full shape received: [None, 3]
(To recreate this error, please install scikit-nightly build 0.22.dev0. The stable 0.21 release does not allow the Adaboost class fit method to take 3D arrays required by LSTM.)



**System information**  
- Have I written custom code (as opposed to using example directory):  This is custom code
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Ubuntu 18.04
- TensorFlow backend (yes / no):  yes
- TensorFlow version:  1.14.0
- Keras version:  2.2.4-tf
- Python version:  3.7
- CUDA/cuDNN version:  None
- GPU model and memory:  NA

I have tried to use a 2D input layer with Scikit stable version 0.21 and then using the Keras Reshape layer to convert to 3D input. Unfortunately, after the initial round of training, the following error message is shown. So the 2 error messages occur at the same point in the training. First all epochs finish successfully, then just when you expect the second round of training to start, this error message pops up.
  File ""<string>"", line 3, in raise_from
tensorflow.python.framework.errors_impl.InvalidArgumentError: Input to reshape is a tensor with 150 values, but the requested shape has 2200
         [[{{node reshape_1/Reshape}}]] [Op:__inference_keras_scratch_graph_4029]

Logs are attached for both errors.
[tensorflow_log_RESHAPE.txt](https://github.com/keras-team/keras/files/3342360/tensorflow_log_RESHAPE.txt)
[tensorflow_log_3D_input.txt](https://github.com/keras-team/keras/files/3342361/tensorflow_log_3D_input.txt)

Any help would be highly appreciated.

Thanks

Best Regards,

Adeel
",0,Error using 3D inputs with Keras to Scikit-Learn Wrapper in Adaboost,"Error using 3D inputs with Keras to Scikit-Learn Wrapper in Adaboost Hi
I have used Keras wrappers for Scikit to implement Adaboost for single input Keras models. This solution works fine for Dense layers, but not for 3D layers. The moment I try to use any 3D layers such as LSTM or SimpleRNN, the first model training runs ok but immediately thereafter I get the following error message.
ValueError: Input 0 of layer simple_rnn_1 is incompatible with the layer: expected ndim=3, found ndim=2. Full shape received: [None, 3]
(To recreate this error, please install scikit-nightly build 0.22.dev0. The stable 0.21 release does not allow the Adaboost class fit method to take 3D arrays required by LSTM.)



**System information**  
- Have I written custom code (as opposed to using example directory):  This is custom code
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Ubuntu 18.04
- TensorFlow backend (yes / no):  yes
- TensorFlow version:  1.14.0
- Keras version:  2.2.4-tf
- Python version:  3.7
- CUDA/cuDNN version:  None
- GPU model and memory:  NA

I have tried to use a 2D input layer with Scikit stable version 0.21 and then using the Keras Reshape layer to convert to 3D input. Unfortunately, after the initial round of training, the following error message is shown. So the 2 error messages occur at the same point in the training. First all epochs finish successfully, then just when you expect the second round of training to start, this error message pops up.
  File ""<string>"", line 3, in raise_from
tensorflow.python.framework.errors_impl.InvalidArgumentError: Input to reshape is a tensor with 150 values, but the requested shape has 2200
         [[{{node reshape_1/Reshape}}]] [Op:__inference_keras_scratch_graph_4029]

Logs are attached for both errors.
[tensorflow_log_RESHAPE.txt](https://github.com/keras-team/keras/files/3342360/tensorflow_log_RESHAPE.txt)
[tensorflow_log_3D_input.txt](https://github.com/keras-team/keras/files/3342361/tensorflow_log_3D_input.txt)

Any help would be highly appreciated.

Thanks

Best Regards,

Adeel
"
keras,11833,"hello , 
i am using keras and tensorflow to implement CNN nets for edge detection , when i tried to run the code ,  this error have occurred . so I would like that you help me to solve it. thanks in advance.
here is the code : 


[from keras.models import Sequential
from keras.layers import (Activation, Dropout, Flatten, Dense, Conv2D, MaxPooling2D)
from keras.utils import np_utils
from keras import backend as K
K.set_image_dim_ordering('th')
import json, pylab
import cv2
import numpy as np


np.set_printoptions(threshold=np.nan)



# some model and data processing constants
batch_size = 128
nb_classes = 2
nb_epoch = 7

# input image dimensions
img_rows, img_cols = 48, 72

# number of convolutional filters to use
nb_filters = 32
# size of pooling area for max pooling
nb_pool = 5
# convolution kernel size
nb_conv = 3

# architecture
model = Sequential()
model.add(Conv2D(nb_filters, (nb_conv, nb_conv),
                        padding='valid',
                        input_shape=(1, img_rows, img_cols)))
model.add(Activation('relu'))
model.add(Conv2D(nb_filters, (nb_conv, nb_conv)))
model.add(Activation('relu'))
model.add(MaxPooling2D(pool_size=(nb_pool, nb_pool)))
model.add(Dropout(0.25))

model.add(Flatten())
model.add(Dense(128))
model.add(Activation('relu'))
model.add(Dropout(0.5))
model.add(Dense(nb_classes))
model.add(Activation('softmax'))
model.compile(loss='sparse_categorical_crossentropy',
              optimizer='adadelta',
              metrics=['accuracy'])




train_imgs=(cv2.imread('1.jpg',0))
ground_truth_train=(cv2.imread('1edges.jpg',0))


test_imgs=(cv2.imread('2.jpg'),0)
ground_truth_test=(cv2.imread('2edges.jpg',0))





print('Preparing images...')

X_train = np.array(train_imgs)
X_test = np.array(ground_truth_train)
y_train = np.array(test_imgs)
y_test = np.array(ground_truth_test)
# prepare the data

X_train = X_train.reshape(X_train.shape[0], 1, img_rows, img_cols)
X_test = X_test.reshape(X_test.shape[0], 1, img_rows, img_cols)



X_train = X_train.astype('float32')
X_test = X_test.astype('float32')
X_train /= 255
X_test /= 255
print('X_train shape:', X_train.shape)
print(X_train.shape, 'train samples')
print(X_test.shape, 'test samples')

# convert class vectors to binary class matrices
# convert class vectors to binary class matrices

from keras.utils import to_categorical 
y_train = np.divide(y_train,255)
y_test = np.divide(y_test,255)
Y_train = to_categorical(y_train,nb_classes)
y_test = to_categorical(y_test,nb_classes)



# train it plz
print('Training model...')


model.fit(X_train, Y_train ,
          batch_size=batch_size,
          nb_epoch=nb_epoch, verbose=1,
          validation_data=(X_test,y_test) )

# let's dump the model
print('Saving model...')
saved_model = model.to_json()
with open('CNN_architecture.json', 'w') as outfile:
    json.dump(saved_model, outfile)
model.save_weights('CNN_weights.h5')
](url)",0,ValueError: setting an array element with a sequence.,"ValueError: setting an array element with a sequence. hello , 
i am using keras and tensorflow to implement CNN nets for edge detection , when i tried to run the code ,  this error have occurred . so I would like that you help me to solve it. thanks in advance.
here is the code : 


[from keras.models import Sequential
from keras.layers import (Activation, Dropout, Flatten, Dense, Conv2D, MaxPooling2D)
from keras.utils import np_utils
from keras import backend as K
K.set_image_dim_ordering('th')
import json, pylab
import cv2
import numpy as np


np.set_printoptions(threshold=np.nan)



# some model and data processing constants
batch_size = 128
nb_classes = 2
nb_epoch = 7

# input image dimensions
img_rows, img_cols = 48, 72

# number of convolutional filters to use
nb_filters = 32
# size of pooling area for max pooling
nb_pool = 5
# convolution kernel size
nb_conv = 3

# architecture
model = Sequential()
model.add(Conv2D(nb_filters, (nb_conv, nb_conv),
                        padding='valid',
                        input_shape=(1, img_rows, img_cols)))
model.add(Activation('relu'))
model.add(Conv2D(nb_filters, (nb_conv, nb_conv)))
model.add(Activation('relu'))
model.add(MaxPooling2D(pool_size=(nb_pool, nb_pool)))
model.add(Dropout(0.25))

model.add(Flatten())
model.add(Dense(128))
model.add(Activation('relu'))
model.add(Dropout(0.5))
model.add(Dense(nb_classes))
model.add(Activation('softmax'))
model.compile(loss='sparse_categorical_crossentropy',
              optimizer='adadelta',
              metrics=['accuracy'])




train_imgs=(cv2.imread('1.jpg',0))
ground_truth_train=(cv2.imread('1edges.jpg',0))


test_imgs=(cv2.imread('2.jpg'),0)
ground_truth_test=(cv2.imread('2edges.jpg',0))





print('Preparing images...')

X_train = np.array(train_imgs)
X_test = np.array(ground_truth_train)
y_train = np.array(test_imgs)
y_test = np.array(ground_truth_test)
# prepare the data

X_train = X_train.reshape(X_train.shape[0], 1, img_rows, img_cols)
X_test = X_test.reshape(X_test.shape[0], 1, img_rows, img_cols)



X_train = X_train.astype('float32')
X_test = X_test.astype('float32')
X_train /= 255
X_test /= 255
print('X_train shape:', X_train.shape)
print(X_train.shape, 'train samples')
print(X_test.shape, 'test samples')

# convert class vectors to binary class matrices
# convert class vectors to binary class matrices

from keras.utils import to_categorical 
y_train = np.divide(y_train,255)
y_test = np.divide(y_test,255)
Y_train = to_categorical(y_train,nb_classes)
y_test = to_categorical(y_test,nb_classes)



# train it plz
print('Training model...')


model.fit(X_train, Y_train ,
          batch_size=batch_size,
          nb_epoch=nb_epoch, verbose=1,
          validation_data=(X_test,y_test) )

# let's dump the model
print('Saving model...')
saved_model = model.to_json()
with open('CNN_architecture.json', 'w') as outfile:
    json.dump(saved_model, outfile)
model.save_weights('CNN_weights.h5')
](url)"
keras,11172,"This issue is opened to host a discussion about the recurrent attention API for keras. 
Related issues:

#11142.
#8296.
#7633.
",0,Recurrent Attention API for keras,"Recurrent Attention API for keras This issue is opened to host a discussion about the recurrent attention API for keras. 
Related issues:

#11142.
#8296.
#7633.
"
keras,12838,"<em>Please make sure that this is a Bug or a Feature Request and provide all applicable information asked by the template.
If your issue is an **implementation question**, please ask your question on [StackOverflow](http://stackoverflow.com/questions/tagged/keras) or [on the Keras Slack channel](https://keras-slack-autojoin.herokuapp.com/) instead of opening a GitHub issue.</em>  

**System information**  
- Have I written custom code (as opposed to using example directory):  
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  
- TensorFlow backend (yes / no):  yes
- TensorFlow version:  latest update
- Keras version:  up to date
- Python version:  up to date
- CUDA/cuDNN version:  -
- GPU model and memory: - 

The keras.io has no documentation on the Generative Adversial Network and No examples too, I guess there should be an section for this too",0,Documentation Required GAN,"Documentation Required GAN <em>Please make sure that this is a Bug or a Feature Request and provide all applicable information asked by the template.
If your issue is an **implementation question**, please ask your question on [StackOverflow](http://stackoverflow.com/questions/tagged/keras) or [on the Keras Slack channel](https://keras-slack-autojoin.herokuapp.com/) instead of opening a GitHub issue.</em>  

**System information**  
- Have I written custom code (as opposed to using example directory):  
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  
- TensorFlow backend (yes / no):  yes
- TensorFlow version:  latest update
- Keras version:  up to date
- Python version:  up to date
- CUDA/cuDNN version:  -
- GPU model and memory: - 

The keras.io has no documentation on the Generative Adversial Network and No examples too, I guess there should be an section for this too"
keras,12944,"Relating to #12249

I am training an autoencoder. The feature vectors extracted from the hidden layers in the middle have to be stored in uint8 as well and utilised to infer the decoder later. Furthermore,  my output from the autoencoder, that is, the reconstructed input, has to compare with the original input in a uint8 precision only. But other values (weights, in/outputs in other layers etc.) can be float32 or float16. 

So has Keras supported such a mixed-precision training? If yes, may I know where the doc is? If not, can it be added? That is, allowing programmers to specify precision for each layer?

Thanks",0,Request for Mixed-precision Training,"Request for Mixed-precision Training Relating to #12249

I am training an autoencoder. The feature vectors extracted from the hidden layers in the middle have to be stored in uint8 as well and utilised to infer the decoder later. Furthermore,  my output from the autoencoder, that is, the reconstructed input, has to compare with the original input in a uint8 precision only. But other values (weights, in/outputs in other layers etc.) can be float32 or float16. 

So has Keras supported such a mixed-precision training? If yes, may I know where the doc is? If not, can it be added? That is, allowing programmers to specify precision for each layer?

Thanks"
keras,10424,"I am trying to train a model that uses cosine distance with negative samples. That is, the loss function is based on multiple values instead of a single y_true to y_pred comparison.

I noticed two things. One, there is no cosine distance with negative samples function for loss, as can be found in [CNTK (cosine_distance_with_negative_samples)](https://docs.microsoft.com/en-us/python/api/cntk.losses?view=cntk-py-2.5.1). Two, a loss function, including custom functions, must conform to the signature of LOSS(y_true, y_pred). I do not see any reason for this, as other libraries (such as Theano, CNTK and tensorflow) are able to use a loss function with multiple arguments.

I was wondering what was holding the implementation of these features back. If it is just because it is not a priority, I may go ahead and try to implement it myself.",0,Feature: cosine distance with negative sampling,"Feature: cosine distance with negative sampling I am trying to train a model that uses cosine distance with negative samples. That is, the loss function is based on multiple values instead of a single y_true to y_pred comparison.

I noticed two things. One, there is no cosine distance with negative samples function for loss, as can be found in [CNTK (cosine_distance_with_negative_samples)](https://docs.microsoft.com/en-us/python/api/cntk.losses?view=cntk-py-2.5.1). Two, a loss function, including custom functions, must conform to the signature of LOSS(y_true, y_pred). I do not see any reason for this, as other libraries (such as Theano, CNTK and tensorflow) are able to use a loss function with multiple arguments.

I was wondering what was holding the implementation of these features back. If it is just because it is not a priority, I may go ahead and try to implement it myself."
keras,7857,"I have a model which runs some 2D convolutions on an image, and then turns that image into a sequence, and runs some GRU layers from left to right on vertical slices of the image. I can train the model, but when I try and load it from C++, using CNTK, it fails, because the user-defined function reshape_with_batch is defined in Keras python code.

I'm using this for OCR, and here is the model:

",0,ReshapeBatch (aka reshape_with_batch) makes CNTK model unable to load from C++,"ReshapeBatch (aka reshape_with_batch) makes CNTK model unable to load from C++ I have a model which runs some 2D convolutions on an image, and then turns that image into a sequence, and runs some GRU layers from left to right on vertical slices of the image. I can train the model, but when I try and load it from C++, using CNTK, it fails, because the user-defined function reshape_with_batch is defined in Keras python code.

I'm using this for OCR, and here is the model:

"
keras,10330,"Hi, I had some problems：
       The following error occurs when I call the model，But not when you're training.

def exponent_neg_manhattan_distance(left, right):
    """"""Helper function for the similarity estimate of the LSTMs outputs""""""
    return K.exp(-K.sum(K.abs(left - right), axis=1, keepdims=True))
--------------------------------------------------------------------------------------------------
 # Calculates the distance as defined by the MaLSTM model
    if distanceFunc == 'exponent_neg_manhattan_distance':
        malstm_distance = Merge(mode=lambda x: exponent_neg_manhattan_distance(x[0], x[1]),
                               output_shape=lambda x: (x[0][0], 1))([left_output, right_output])
--------------------------------------------------------------------------------------------------
ERROR：
/Users/bai/anaconda3/lib/python3.6/site-packages/keras/engine/topology.py:1269: UserWarning: The  layer is deprecated and will be removed after 08/2017. Use instead layers from , e.g. , , etc.
  return cls(**config)
Traceback (most recent call last):
  File ""/Users/bai/pyproject/cikmAnalytiCup/cikm/train_model.py"", line 264, in <module>
    test()
  File ""/Users/bai/pyproject/cikmAnalytiCup/cikm/train_model.py"", line 249, in test
    model = model_from_json(json_string)
  File ""/Users/bai/anaconda3/lib/python3.6/site-packages/keras/models.py"", line 349, in model_from_json
    return layer_module.deserialize(config, custom_objects=custom_objects)
  File ""/Users/bai/anaconda3/lib/python3.6/site-packages/keras/layers/__init__.py"", line 55, in deserialize
    printable_module_name='layer')
  File ""/Users/bai/anaconda3/lib/python3.6/site-packages/keras/utils/generic_utils.py"", line 143, in deserialize_keras_object
    list(custom_objects.items())))
  File ""/Users/bai/anaconda3/lib/python3.6/site-packages/keras/engine/topology.py"", line 2517, in from_config
    process_node(layer, node_data)
  File ""/Users/bai/anaconda3/lib/python3.6/site-packages/keras/engine/topology.py"", line 2476, in process_node
    layer(input_tensors, **kwargs)
  File ""/Users/bai/anaconda3/lib/python3.6/site-packages/keras/engine/topology.py"", line 617, in __call__
    output = self.call(inputs, **kwargs)
  File ""/Users/bai/anaconda3/lib/python3.6/site-packages/keras/legacy/layers.py"", line 208, in call
    return self.mode(inputs, **arguments)
  File ""/Users/bai/pyproject/cikmAnalytiCup/cikm/train_model.py"", line 156, in <lambda>
    malstm_distance = Merge(mode=lambda x: exponent_neg_manhattan_distance(x[0], x[1]),
NameError: name 'exponent_neg_manhattan_distance' is not defined

I hope to get some help",0,Call model error,"Call model error Hi, I had some problems：
       The following error occurs when I call the model，But not when you're training.

def exponent_neg_manhattan_distance(left, right):
    """"""Helper function for the similarity estimate of the LSTMs outputs""""""
    return K.exp(-K.sum(K.abs(left - right), axis=1, keepdims=True))
--------------------------------------------------------------------------------------------------
 # Calculates the distance as defined by the MaLSTM model
    if distanceFunc == 'exponent_neg_manhattan_distance':
        malstm_distance = Merge(mode=lambda x: exponent_neg_manhattan_distance(x[0], x[1]),
                               output_shape=lambda x: (x[0][0], 1))([left_output, right_output])
--------------------------------------------------------------------------------------------------
ERROR：
/Users/bai/anaconda3/lib/python3.6/site-packages/keras/engine/topology.py:1269: UserWarning: The  layer is deprecated and will be removed after 08/2017. Use instead layers from , e.g. , , etc.
  return cls(**config)
Traceback (most recent call last):
  File ""/Users/bai/pyproject/cikmAnalytiCup/cikm/train_model.py"", line 264, in <module>
    test()
  File ""/Users/bai/pyproject/cikmAnalytiCup/cikm/train_model.py"", line 249, in test
    model = model_from_json(json_string)
  File ""/Users/bai/anaconda3/lib/python3.6/site-packages/keras/models.py"", line 349, in model_from_json
    return layer_module.deserialize(config, custom_objects=custom_objects)
  File ""/Users/bai/anaconda3/lib/python3.6/site-packages/keras/layers/__init__.py"", line 55, in deserialize
    printable_module_name='layer')
  File ""/Users/bai/anaconda3/lib/python3.6/site-packages/keras/utils/generic_utils.py"", line 143, in deserialize_keras_object
    list(custom_objects.items())))
  File ""/Users/bai/anaconda3/lib/python3.6/site-packages/keras/engine/topology.py"", line 2517, in from_config
    process_node(layer, node_data)
  File ""/Users/bai/anaconda3/lib/python3.6/site-packages/keras/engine/topology.py"", line 2476, in process_node
    layer(input_tensors, **kwargs)
  File ""/Users/bai/anaconda3/lib/python3.6/site-packages/keras/engine/topology.py"", line 617, in __call__
    output = self.call(inputs, **kwargs)
  File ""/Users/bai/anaconda3/lib/python3.6/site-packages/keras/legacy/layers.py"", line 208, in call
    return self.mode(inputs, **arguments)
  File ""/Users/bai/pyproject/cikmAnalytiCup/cikm/train_model.py"", line 156, in <lambda>
    malstm_distance = Merge(mode=lambda x: exponent_neg_manhattan_distance(x[0], x[1]),
NameError: name 'exponent_neg_manhattan_distance' is not defined

I hope to get some help"
keras,12945,"When evaluating same data with different parameters, it came with different results.


I guess this is related to the warning
use_multiprocessing=Truekeras.utils.Sequence class.
test_data_flowflow_from_directory` which I believe most people would use.

So is it more reasonable to raise an Error than give a warning to protect users from getting wrong results?",0,model.evaluate_generator may give wrong results when using multiprocessing.,"model.evaluate_generator may give wrong results when using multiprocessing. When evaluating same data with different parameters, it came with different results.


I guess this is related to the warning
use_multiprocessing=Truekeras.utils.Sequence class.
test_data_flowflow_from_directory` which I believe most people would use.

So is it more reasonable to raise an Error than give a warning to protect users from getting wrong results?"
keras,13088,"I want to initialize the initial state of an LSTM layer with the final hidden state of another LSTM layer. Basically, I want to implement the _conditional encoding_ as explained in this paper https://www.aclweb.org/anthology/D16-1084 

![image](https://user-images.githubusercontent.com/32245327/60956724-fec33e00-a320-11e9-8ccd-6c8564c626ac.png)

How do I access the final hidden state of one LSTM layer and use it as the initial state of another LSTM layer?
Thanks",0,Initialize LSTM initial state manually,"Initialize LSTM initial state manually I want to initialize the initial state of an LSTM layer with the final hidden state of another LSTM layer. Basically, I want to implement the _conditional encoding_ as explained in this paper https://www.aclweb.org/anthology/D16-1084 

![image](https://user-images.githubusercontent.com/32245327/60956724-fec33e00-a320-11e9-8ccd-6c8564c626ac.png)

How do I access the final hidden state of one LSTM layer and use it as the initial state of another LSTM layer?
Thanks"
keras,12981,"**System information**  
- Have I written custom code (as opposed to using example directory):  No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Linux Ubuntu 16.04
- TensorFlow backend (yes / no):  yes
- TensorFlow version:  1.9.0
- Keras version:  2.2.4
- Python version:  3.5.2
- CUDA/cuDNN version: 9.0  
- GPU model and memory:  NVIDIA P6000

**Describe the current behavior**  

I'm trying to use dilated 3D convolutions. However, when I declare one, like

I run out of memory. It says it cannot allocate a tensor of .

**Describe the expected behavior**  

Given that the batch size is set to 64, where does that 512 come from?
",0,Conv3D dilation increases batch size?,"Conv3D dilation increases batch size? **System information**  
- Have I written custom code (as opposed to using example directory):  No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Linux Ubuntu 16.04
- TensorFlow backend (yes / no):  yes
- TensorFlow version:  1.9.0
- Keras version:  2.2.4
- Python version:  3.5.2
- CUDA/cuDNN version: 9.0  
- GPU model and memory:  NVIDIA P6000

**Describe the current behavior**  

I'm trying to use dilated 3D convolutions. However, when I declare one, like

I run out of memory. It says it cannot allocate a tensor of .

**Describe the expected behavior**  

Given that the batch size is set to 64, where does that 512 come from?
"
keras,9552,"Hi All:

An earlier submission of similar topic disappeared!
So i am doing this twice.
When I looked at code for section 
""6.3-advanced-usage-of-recurrent-neural-networks""
of the book, input arguments like input_shape for 
layers.GRU() is missing from keras documentation: 

https://keras.io/layers/recurrent/#gru
https://keras.io/layers/recurrent/#lstm

i also looked a recurrent,py and it does 
not have any local variable for input_shape 
for classes recurrant and GRU.

What did I missed?
All this missing documentation makes learning very difficult.

Where can I locate all the input arguments 
for a particular class?



",0,"Why are arguments missing from Keras documentation, e.g. GRU() and LSTM()?","Why are arguments missing from Keras documentation, e.g. GRU() and LSTM()? Hi All:

An earlier submission of similar topic disappeared!
So i am doing this twice.
When I looked at code for section 
""6.3-advanced-usage-of-recurrent-neural-networks""
of the book, input arguments like input_shape for 
layers.GRU() is missing from keras documentation: 

https://keras.io/layers/recurrent/#gru
https://keras.io/layers/recurrent/#lstm

i also looked a recurrent,py and it does 
not have any local variable for input_shape 
for classes recurrant and GRU.

What did I missed?
All this missing documentation makes learning very difficult.

Where can I locate all the input arguments 
for a particular class?



"
keras,12690,"Short version: Custom regularizer object seems to be reconstructed with each batch.

Full version:
I'm trying to create a custom Keras regularizer that uses the distance of the layer's weights from it's original weights, but what I used doesn't work and I get a zero difference at all times.

This is the regularizer code:



(I'm using tensorflow as the backend)

After playing with this class a bit, I noted something strange: It's as if the regularizer object is being created over and over again in the training with each batch, which does explain why I'm getting zeros.
I got to this conclusion by changing the class to -



And seeing that the loss does in fact suffer the penalty that follows from _ugly_check being 1 throughout the training.

I would expect the regularizer object to remain the same one throughout training. Is this a bug or am I not understanding the usage of custom regularizers correctly?",0,Custom regularizer does not remain the same object,"Custom regularizer does not remain the same object Short version: Custom regularizer object seems to be reconstructed with each batch.

Full version:
I'm trying to create a custom Keras regularizer that uses the distance of the layer's weights from it's original weights, but what I used doesn't work and I get a zero difference at all times.

This is the regularizer code:



(I'm using tensorflow as the backend)

After playing with this class a bit, I noted something strange: It's as if the regularizer object is being created over and over again in the training with each batch, which does explain why I'm getting zeros.
I got to this conclusion by changing the class to -



And seeing that the loss does in fact suffer the penalty that follows from _ugly_check being 1 throughout the training.

I would expect the regularizer object to remain the same one throughout training. Is this a bug or am I not understanding the usage of custom regularizers correctly?"
keras,12830,"Quick question regarding use of advanced activation functions. Currently working with some RNN's on some regression problem. Let's say for the sake of question that my model looks something like the following:

model.add(LSTM())
model.add(ELU())
...

Now my question refers to that use of ELU activation function, by default the LSTM has 'tanh' set as an activation function, does adding ELU on top of that makes the data flow through LSTM -> Tanh -> ELU?

Regards, ",0,Using advanced activation functions,"Using advanced activation functions Quick question regarding use of advanced activation functions. Currently working with some RNN's on some regression problem. Let's say for the sake of question that my model looks something like the following:

model.add(LSTM())
model.add(ELU())
...

Now my question refers to that use of ELU activation function, by default the LSTM has 'tanh' set as an activation function, does adding ELU on top of that makes the data flow through LSTM -> Tanh -> ELU?

Regards, "
keras,9076,"Greetings all,

The problem that I am working is a binary classification. I have around 150 sequences, where each sequence has 130000 timesteps, where each timestep has 2 features, shape=(1,130000,2). Each of these sequences is labelled as ""1"" or ""0"".  I want to train an lstm network, where at the end, giving it a sequence of 130000 timesteps with 2 features each, will predict ""1"" or ""0"". Hence the problem as I have it in my mind is ""many to one"" right? 

1)If I keep for training 120 of these sequences and the rest 30 for validation, the number of parameters of the model, should be smth like 120x130000x2=31,200,000 parameters?
For example an LSTM layer with ~4000 units?

2)The network should be stateful? or not

The model as I am thinking it is smth like:



I tried for one epoch to fit it, but the memory gets quite high! Any recommendations for the whole problem approach?",0,Rough estimate of number of parameters and LSTM architecture..,"Rough estimate of number of parameters and LSTM architecture.. Greetings all,

The problem that I am working is a binary classification. I have around 150 sequences, where each sequence has 130000 timesteps, where each timestep has 2 features, shape=(1,130000,2). Each of these sequences is labelled as ""1"" or ""0"".  I want to train an lstm network, where at the end, giving it a sequence of 130000 timesteps with 2 features each, will predict ""1"" or ""0"". Hence the problem as I have it in my mind is ""many to one"" right? 

1)If I keep for training 120 of these sequences and the rest 30 for validation, the number of parameters of the model, should be smth like 120x130000x2=31,200,000 parameters?
For example an LSTM layer with ~4000 units?

2)The network should be stateful? or not

The model as I am thinking it is smth like:



I tried for one epoch to fit it, but the memory gets quite high! Any recommendations for the whole problem approach?"
keras,9526,"In running the variational_autoencoder.py file in keras/examples/, the following error is encountered - 
TypeError: compile() missing 1 required positional argument: 'loss'

On adding the vae_loss as the loss parameter in compile (i.e., vae.compile(optimizer='rmsprop'), loss=vae_loss) instead of vae.add_loss(vae_loss)), the following error is encountered - 

TypeError: Using a  as a Python  is not allowed. Use  instead of  to test if a tensor is defined, and use TensorFlow ops such as tf.cond to execute subgraphs conditioned on the value of a tensor.

I think the problem is that we need to define a custom loss function for Keras in terms of a Python function that acts on y_pred and y_true, and this is not being done here.",0,Variational Autoencoder Custom Loss function,"Variational Autoencoder Custom Loss function In running the variational_autoencoder.py file in keras/examples/, the following error is encountered - 
TypeError: compile() missing 1 required positional argument: 'loss'

On adding the vae_loss as the loss parameter in compile (i.e., vae.compile(optimizer='rmsprop'), loss=vae_loss) instead of vae.add_loss(vae_loss)), the following error is encountered - 

TypeError: Using a  as a Python  is not allowed. Use  instead of  to test if a tensor is defined, and use TensorFlow ops such as tf.cond to execute subgraphs conditioned on the value of a tensor.

I think the problem is that we need to define a custom loss function for Keras in terms of a Python function that acts on y_pred and y_true, and this is not being done here."
keras,10591,"I just had an interesting situation where I built the same network architecture using the Sequential and the functional api, however if I train the one and transfer the weights to the other like so:
model2.set_weight(model1.get_weight())
and I don't get the same outputs.
I used the following code to construct the models:

# Using Sequential
model1 = Sequential()
model1.add(GRU(15, activation=""relu"", input_shape=(1052,12), return_sequences=True))
model1.add(Dense(1))

# Using the functional API
inlayer= Input(shape=(1052,12))
hidden= GRU(15, activation=""relu"", return_sequences=True)(inlayer)
outlayer = Dense(1)(hidden) 
model2 = Model(inputs=inlayer, outputs=outlayer)

It is easy to get around this by sticking to the functional API, but why would this situation occur, and is it as intended?",0,Differences in weight data structures for the Sequential and functional API?,"Differences in weight data structures for the Sequential and functional API? I just had an interesting situation where I built the same network architecture using the Sequential and the functional api, however if I train the one and transfer the weights to the other like so:
model2.set_weight(model1.get_weight())
and I don't get the same outputs.
I used the following code to construct the models:

# Using Sequential
model1 = Sequential()
model1.add(GRU(15, activation=""relu"", input_shape=(1052,12), return_sequences=True))
model1.add(Dense(1))

# Using the functional API
inlayer= Input(shape=(1052,12))
hidden= GRU(15, activation=""relu"", return_sequences=True)(inlayer)
outlayer = Dense(1)(hidden) 
model2 = Model(inputs=inlayer, outputs=outlayer)

It is easy to get around this by sticking to the functional API, but why would this situation occur, and is it as intended?"
keras,7244,"Hi,

I'm trying to implement a nonlinear TimeSeries Prediction similar to this publication: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5336098/
Though I'm not interested in using a CNN as the first layer. I have a dataset of this shape:
, which I padded with 0s to have this shape: . This works just fine with normal LSTMs.

For Bidirectional LSTM I implemented my model like this:

Unfortunately I'm getting weird results with this model which look like the model is partly ignoring the masking.

The problem with BiDirectional LSTMs and masking seems to be that by reading that data backwards, masking is once in the front (where it should be) and once in the back (where it shouldn't). Therefore I'm getting weird results. I found this old [blog post](http://dirko.github.io/Bidirectional-LSTMs-with-Keras/) which handles a similar problem though it using the old API. 

Do I need to provide distinct datasets with the padding in the correct place for each layer of the Bi-LSTM? And how would I feed that data into the model? Or should I leave the padding out completely and create batches of same size? Or even use ? 

- [x] Check that you are up-to-date with the master branch of Keras. You can update with:
pip install git+git://github.com/fchollet/keras.git --upgrade --no-deps

- [x] If running on TensorFlow, check that you are up-to-date with the latest version. The installation instructions can be found [here](https://www.tensorflow.org/get_started/os_setup).

Cheers,

hfjn
",0,Masking with BiDirectional LSTMs not working?,"Masking with BiDirectional LSTMs not working? Hi,

I'm trying to implement a nonlinear TimeSeries Prediction similar to this publication: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5336098/
Though I'm not interested in using a CNN as the first layer. I have a dataset of this shape:
, which I padded with 0s to have this shape: . This works just fine with normal LSTMs.

For Bidirectional LSTM I implemented my model like this:

Unfortunately I'm getting weird results with this model which look like the model is partly ignoring the masking.

The problem with BiDirectional LSTMs and masking seems to be that by reading that data backwards, masking is once in the front (where it should be) and once in the back (where it shouldn't). Therefore I'm getting weird results. I found this old [blog post](http://dirko.github.io/Bidirectional-LSTMs-with-Keras/) which handles a similar problem though it using the old API. 

Do I need to provide distinct datasets with the padding in the correct place for each layer of the Bi-LSTM? And how would I feed that data into the model? Or should I leave the padding out completely and create batches of same size? Or even use ? 

- [x] Check that you are up-to-date with the master branch of Keras. You can update with:
pip install git+git://github.com/fchollet/keras.git --upgrade --no-deps

- [x] If running on TensorFlow, check that you are up-to-date with the latest version. The installation instructions can be found [here](https://www.tensorflow.org/get_started/os_setup).

Cheers,

hfjn
"
keras,8365,"It would be great if someone can suggest me how I can fix this stop iteration problem. Based on the property of the generator whenever we call this function, once we’ve exhausted all of the yields within index_generator() running next() again results in a StopIteration error and in my case once reaches to the last batch of the directory iterator, it will stop working. This issue is not allowing me to compile my model.








Please make sure that the boxes below are checked before you submit your issue. If your issue is an implementation question, please ask your question on [StackOverflow](http://stackoverflow.com/questions/tagged/keras) or [join the Keras Slack channel](https://keras-slack-autojoin.herokuapp.com/) and ask there instead of filing a GitHub issue.

Thank you!

- [ ] Check that you are up-to-date with the master branch of Keras. You can update with:
pip install git+git://github.com/fchollet/keras.git --upgrade --no-deps

- [ ] If running on TensorFlow, check that you are up-to-date with the latest version. The installation instructions can be found [here](https://www.tensorflow.org/get_started/os_setup).

- [ ] If running on Theano, check that you are up-to-date with the master branch of Theano. You can update with:
pip install git+git://github.com/Theano/Theano.git --upgrade --no-deps

- [ ] Provide a link to a GitHub Gist of a Python script that can reproduce your issue (or just copy the script here if it is short).
### 

",0,Issue with Generator: Trying to call index_generator from next function but getting the error :  Traceback (most recent call last): generator_ouput=next(output_generator) StopIteration.,"Issue with Generator: Trying to call index_generator from next function but getting the error :  Traceback (most recent call last): generator_ouput=next(output_generator) StopIteration. It would be great if someone can suggest me how I can fix this stop iteration problem. Based on the property of the generator whenever we call this function, once we’ve exhausted all of the yields within index_generator() running next() again results in a StopIteration error and in my case once reaches to the last batch of the directory iterator, it will stop working. This issue is not allowing me to compile my model.








Please make sure that the boxes below are checked before you submit your issue. If your issue is an implementation question, please ask your question on [StackOverflow](http://stackoverflow.com/questions/tagged/keras) or [join the Keras Slack channel](https://keras-slack-autojoin.herokuapp.com/) and ask there instead of filing a GitHub issue.

Thank you!

- [ ] Check that you are up-to-date with the master branch of Keras. You can update with:
pip install git+git://github.com/fchollet/keras.git --upgrade --no-deps

- [ ] If running on TensorFlow, check that you are up-to-date with the latest version. The installation instructions can be found [here](https://www.tensorflow.org/get_started/os_setup).

- [ ] If running on Theano, check that you are up-to-date with the master branch of Theano. You can update with:
pip install git+git://github.com/Theano/Theano.git --upgrade --no-deps

- [ ] Provide a link to a GitHub Gist of a Python script that can reproduce your issue (or just copy the script here if it is short).
### 

"
keras,8407,"my code is:

The error shown is: 

Removing dropout or initial_state argument seems to solve the problem but you can't use them both
Any help on this issue ?",0,Error when using dropout and initial_state in gru layer,"Error when using dropout and initial_state in gru layer my code is:

The error shown is: 

Removing dropout or initial_state argument seems to solve the problem but you can't use them both
Any help on this issue ?"
keras,7217," does a good job to pre-load data batches, avoid the problem of memory insufficiency, and fit for continuously available data. I was wondering, when passing a generator with finite number of outputs, is it possible to add an option to  so that it can behavior like the  function which declares all the data as an epoch instead of using ? Because I have no idea if the whole data are seen total epochs or something like 5.6 epochs when I use , and if I use train_on_batch, I need a thread to pre-load the batches because the preprocessing costs time.

Thanks.",0,Can fit_generator() declare an epoch by the whole data in a generator (finite number of data) instead of steps_per_epoch?,"Can fit_generator() declare an epoch by the whole data in a generator (finite number of data) instead of steps_per_epoch?  does a good job to pre-load data batches, avoid the problem of memory insufficiency, and fit for continuously available data. I was wondering, when passing a generator with finite number of outputs, is it possible to add an option to  so that it can behavior like the  function which declares all the data as an epoch instead of using ? Because I have no idea if the whole data are seen total epochs or something like 5.6 epochs when I use , and if I use train_on_batch, I need a thread to pre-load the batches because the preprocessing costs time.

Thanks."
keras,11335,"I have seen several discussion on this topic in various forums. However, none of them seem to address the central issue I have. Here goes:

**Setup:**
- Working on a LSTM model. 
- It is a sentiment analysis application.
- I save / load the model per this FAQ (https://keras.io/getting-started/faq/#how-can-i-save-a-keras-model)
- It works if I do it from the *same context*. That is same program / .py file: Train / Save / Delete model / Load / Predict works.

**Problem**: 
The following does not work.
- Program 1: Train the model. Save the model.
- Program 2: Load the model. Do prediction.

The predictions are completely off when compared to a single context run.
- There is a similar issue - that had some discussion and then it was marked as stale. https://github.com/keras-team/keras/issues/4904
- Another work around seems here is here: https://github.com/keras-team/keras/issues/7632

**Question**:
- Do we have to store internal states in a some other pickle format - for LSTM cases?
- Is there another recommended way of what I am trying to attempt?

**Output of **:




",0,Keras / LSTM model saving and loading produces inconsistent results,"Keras / LSTM model saving and loading produces inconsistent results I have seen several discussion on this topic in various forums. However, none of them seem to address the central issue I have. Here goes:

**Setup:**
- Working on a LSTM model. 
- It is a sentiment analysis application.
- I save / load the model per this FAQ (https://keras.io/getting-started/faq/#how-can-i-save-a-keras-model)
- It works if I do it from the *same context*. That is same program / .py file: Train / Save / Delete model / Load / Predict works.

**Problem**: 
The following does not work.
- Program 1: Train the model. Save the model.
- Program 2: Load the model. Do prediction.

The predictions are completely off when compared to a single context run.
- There is a similar issue - that had some discussion and then it was marked as stale. https://github.com/keras-team/keras/issues/4904
- Another work around seems here is here: https://github.com/keras-team/keras/issues/7632

**Question**:
- Do we have to store internal states in a some other pickle format - for LSTM cases?
- Is there another recommended way of what I am trying to attempt?

**Output of **:




"
keras,9224,"Hello, When i use **pandas HDFStore** with keras, Python program collapsed.



The code is OK if i comment the line .

-----------------------
System: Win10 x64
Pandas: 0.20.3
keras: 2.1.3
",0,pandas HDFStore conflicts with keras?!,"pandas HDFStore conflicts with keras?! Hello, When i use **pandas HDFStore** with keras, Python program collapsed.



The code is OK if i comment the line .

-----------------------
System: Win10 x64
Pandas: 0.20.3
keras: 2.1.3
"
keras,13658,"EarlyStopping with restore_best_weights=True makes TypeError: 'NoneType' object is not subscriptable.

When restore_best_weights=False no exception.

**System information**  
 
- Keras version:  2.3.1
- Python version:  3.6

Stack trace:
",0,EarlyStopping with restore_best_weights=True makes TypeError: 'NoneType' object is not subscriptable,"EarlyStopping with restore_best_weights=True makes TypeError: 'NoneType' object is not subscriptable EarlyStopping with restore_best_weights=True makes TypeError: 'NoneType' object is not subscriptable.

When restore_best_weights=False no exception.

**System information**  
 
- Keras version:  2.3.1
- Python version:  3.6

Stack trace:
"
keras,9468,"First off, these examples are wonderful.  That being said, I have a question/improvement to the [char-level text generation example](https://github.com/keras-team/keras/blob/master/examples/lstm_text_generation.py).

I was trying this example out with a corpus of my own and kept running to MemoryError when allocating the training set and labels.  I noticed that both use a 1-hot encoding to indicate the activated character. 

**My question is**:  Why not bake this embedding into the model with an identity matrix before the first LSTM (basically just a character embedding layer that returns the 1-hot) and using a sparse categorical loss evaluation?  

This reduces the total in-memory usage by a factor of the number of unique characters.  Seemed to work for me when I tried it out.  

**Note**:  I was trying much longer input sequences (>100) which was why I was using a lot of memory.

Open to thoughts, issues, etc.",0,Reducing Memory Usage of Char-Level Text Generation by LSTMs,"Reducing Memory Usage of Char-Level Text Generation by LSTMs First off, these examples are wonderful.  That being said, I have a question/improvement to the [char-level text generation example](https://github.com/keras-team/keras/blob/master/examples/lstm_text_generation.py).

I was trying this example out with a corpus of my own and kept running to MemoryError when allocating the training set and labels.  I noticed that both use a 1-hot encoding to indicate the activated character. 

**My question is**:  Why not bake this embedding into the model with an identity matrix before the first LSTM (basically just a character embedding layer that returns the 1-hot) and using a sparse categorical loss evaluation?  

This reduces the total in-memory usage by a factor of the number of unique characters.  Seemed to work for me when I tried it out.  

**Note**:  I was trying much longer input sequences (>100) which was why I was using a lot of memory.

Open to thoughts, issues, etc."
keras,13344,"import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.utils import shuffle
from keras.layers import Dense,Flatten, Conv2D
from keras.layers import MaxPooling2D, Dropout
from keras.utils import np_utils, print_summary
import tensorflow as tf
from keras.models import Sequential
from keras.callbacks import ModelCheckpoint
import pickle
from keras.callbacks import TensorBoard

def keras_model(image_x, image_y):
    num_of_classes = 15
    model = Sequential()
    model.add(Conv2D(32, (5, 5), input_shape=(image_x,image_y,1), activation='relu'))
    model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding='same'))
    model.add(Conv2D(64, (5, 5), activation='relu'))
    model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding='same'))

    model.add(Flatten())
    model.add(Dense(512, activation='relu'))
    model.add(Dropout(0.6))
    model.add(Dense(128, activation='relu'))
    model.add(Dropout(0.6))
    model.add(Dense(num_of_classes, activation='softmax'))

    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
    filepath = ""QuickDraw.h5""
    checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')
    callbacks_list = [checkpoint]

    return model, callbacks_list


def loadFromPickle():
    with open(""features"", ""rb"") as f:
        features = np.array(pickle.load(f))
    with open(""labels"", ""rb"") as f:
        labels = np.array(pickle.load(f))

    return features, labels


def augmentData(features, labels):
    features = np.append(features, features[:, :, ::-1], axis=0)
    labels = np.append(labels, -labels, axis=0)
    return features, labels


def prepress_labels(labels):
    labels = np_utils.to_categorical(labels)
    return labels


def main():
    features, labels = loadFromPickle()
    # features, labels = augmentData(features, labels)
    features, labels = shuffle(features, labels)
    labels=prepress_labels(labels)
    train_x, test_x, train_y, test_y = train_test_split(features, labels, random_state=0,
                                                        test_size=0.1)
    train_x = train_x.reshape(train_x.shape[0], 28, 28, 1)
    test_x = test_x.reshape(test_x.shape[0], 28, 28, 1)
    model, callbacks_list = keras_model(28,28)
    print_summary(model)
    model.fit(train_x, train_y, validation_data=(test_x, test_y), epochs=3, batch_size=64,
              callbacks=[TensorBoard(log_dir=""QuickDraw"")])
    model.save('QuickDraw.h5')


main()
",0,"ValueError: Error when checking target: expected dense_3 to have shape (15,) but got array with shape (1,)","ValueError: Error when checking target: expected dense_3 to have shape (15,) but got array with shape (1,) import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.utils import shuffle
from keras.layers import Dense,Flatten, Conv2D
from keras.layers import MaxPooling2D, Dropout
from keras.utils import np_utils, print_summary
import tensorflow as tf
from keras.models import Sequential
from keras.callbacks import ModelCheckpoint
import pickle
from keras.callbacks import TensorBoard

def keras_model(image_x, image_y):
    num_of_classes = 15
    model = Sequential()
    model.add(Conv2D(32, (5, 5), input_shape=(image_x,image_y,1), activation='relu'))
    model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding='same'))
    model.add(Conv2D(64, (5, 5), activation='relu'))
    model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding='same'))

    model.add(Flatten())
    model.add(Dense(512, activation='relu'))
    model.add(Dropout(0.6))
    model.add(Dense(128, activation='relu'))
    model.add(Dropout(0.6))
    model.add(Dense(num_of_classes, activation='softmax'))

    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
    filepath = ""QuickDraw.h5""
    checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')
    callbacks_list = [checkpoint]

    return model, callbacks_list


def loadFromPickle():
    with open(""features"", ""rb"") as f:
        features = np.array(pickle.load(f))
    with open(""labels"", ""rb"") as f:
        labels = np.array(pickle.load(f))

    return features, labels


def augmentData(features, labels):
    features = np.append(features, features[:, :, ::-1], axis=0)
    labels = np.append(labels, -labels, axis=0)
    return features, labels


def prepress_labels(labels):
    labels = np_utils.to_categorical(labels)
    return labels


def main():
    features, labels = loadFromPickle()
    # features, labels = augmentData(features, labels)
    features, labels = shuffle(features, labels)
    labels=prepress_labels(labels)
    train_x, test_x, train_y, test_y = train_test_split(features, labels, random_state=0,
                                                        test_size=0.1)
    train_x = train_x.reshape(train_x.shape[0], 28, 28, 1)
    test_x = test_x.reshape(test_x.shape[0], 28, 28, 1)
    model, callbacks_list = keras_model(28,28)
    print_summary(model)
    model.fit(train_x, train_y, validation_data=(test_x, test_y), epochs=3, batch_size=64,
              callbacks=[TensorBoard(log_dir=""QuickDraw"")])
    model.save('QuickDraw.h5')


main()
"
keras,11195,"code bellow from keras Doucumentation Example:

from keras.layers import Conv2D, MaxPooling2D, Input

input_img = Input(shape=(256, 256, 3))

tower_1 = Conv2D(64, (1, 1), padding='same', activation='relu')(input_img)
tower_1 = Conv2D(64, (3, 3), padding='same', activation='relu')(tower_1)

tower_2 = Conv2D(64, (1, 1), padding='same', activation='relu')(input_img)
tower_2 = Conv2D(64, (5, 5), padding='same', activation='relu')(tower_2)

tower_3 = MaxPooling2D((3, 3), strides=(1, 1), padding='same')(input_img)
tower_3 = Conv2D(64, (1, 1), padding='same', activation='relu')(tower_3)

output = keras.layers.concatenate([tower_1, tower_2, tower_3], axis=1)      #axis = 1, output shape is [?, 768, 256, 64]? I think axis should be -1, and output shape is [None, 256 ,256, 192].",0,Inception module Example in kears Doucumentation dont understand,"Inception module Example in kears Doucumentation dont understand code bellow from keras Doucumentation Example:

from keras.layers import Conv2D, MaxPooling2D, Input

input_img = Input(shape=(256, 256, 3))

tower_1 = Conv2D(64, (1, 1), padding='same', activation='relu')(input_img)
tower_1 = Conv2D(64, (3, 3), padding='same', activation='relu')(tower_1)

tower_2 = Conv2D(64, (1, 1), padding='same', activation='relu')(input_img)
tower_2 = Conv2D(64, (5, 5), padding='same', activation='relu')(tower_2)

tower_3 = MaxPooling2D((3, 3), strides=(1, 1), padding='same')(input_img)
tower_3 = Conv2D(64, (1, 1), padding='same', activation='relu')(tower_3)

output = keras.layers.concatenate([tower_1, tower_2, tower_3], axis=1)      #axis = 1, output shape is [?, 768, 256, 64]? I think axis should be -1, and output shape is [None, 256 ,256, 192]."
keras,11288,"Hi, 

I've implemented my own generator as sequence object. If I'm using it without multi processing, it works fine. Most of the times it also does the job when using multiprocessing=True (fit_generator).

However,  SOMETIMES (unfortunately not reproducible) when using multiprocessing=True, the program does not exit after the training finished. It reaches the last line (print out the time needed for training) but then does not exit. I've to kill it with ctrl + c. Most of the times, some processes are still running and tensorflow still occupies my GPU memory. I've to kill the processes then manually by kill -s 9 id .

I think it might be a problem with my generator:
https://gist.github.com/thorstenwagner/8033f43b99d1d3a1a6a31b054d91e7fc

However, I cannot nail it down to a specific line. I did my best to make it multi threading save.

I observed this problem with Keras 2.2.0, 2.2.2 and 2.2.3. I've tested tensorflow-gpu 1.8.0 and tensorflow-gpu 1.10.1 as backend. I had this problem with python 2 and with python 3.6.

Any ideas what might be problem?

Best,
Thorsten
",0,Python sometimes does not exit when using multiprocessing=True in fit_generator,"Python sometimes does not exit when using multiprocessing=True in fit_generator Hi, 

I've implemented my own generator as sequence object. If I'm using it without multi processing, it works fine. Most of the times it also does the job when using multiprocessing=True (fit_generator).

However,  SOMETIMES (unfortunately not reproducible) when using multiprocessing=True, the program does not exit after the training finished. It reaches the last line (print out the time needed for training) but then does not exit. I've to kill it with ctrl + c. Most of the times, some processes are still running and tensorflow still occupies my GPU memory. I've to kill the processes then manually by kill -s 9 id .

I think it might be a problem with my generator:
https://gist.github.com/thorstenwagner/8033f43b99d1d3a1a6a31b054d91e7fc

However, I cannot nail it down to a specific line. I did my best to make it multi threading save.

I observed this problem with Keras 2.2.0, 2.2.2 and 2.2.3. I've tested tensorflow-gpu 1.8.0 and tensorflow-gpu 1.10.1 as backend. I had this problem with python 2 and with python 3.6.

Any ideas what might be problem?

Best,
Thorsten
"
keras,12483,"First of all, huge thanks for your effort.

I have 2 submodels (, ) out of which I form my full  using  by stacking them logically in ""series"". By this I mean that  accepts the output of plus an extra input tensor and the output of is the output of my full . The full is created **successfully** and I am also able to use .  

However, I want to parallelize the training of by running it on 2 GPUs, thus I use  which fails with the error:

> AssertionError: Could not compute output Tensor(""model_2/Dense_Decoder/truediv:0"", shape=(?, 33, 22), dtype=float32)

I have tried parallelizing the two submodels individually using  and , yet **both succeed**. The problem appears **only** with the full model. 

I am using **Tensorflow 1.12.0** and **Keras 2.2.4**. A snippet that demonstrates the problem (at least on my machine) is:



I believe this problem might be similar to #9599 but I can be mistaken. ",0,AssertionError: Could not compute output Tensor when using multi_gpu_model(),"AssertionError: Could not compute output Tensor when using multi_gpu_model() First of all, huge thanks for your effort.

I have 2 submodels (, ) out of which I form my full  using  by stacking them logically in ""series"". By this I mean that  accepts the output of plus an extra input tensor and the output of is the output of my full . The full is created **successfully** and I am also able to use .  

However, I want to parallelize the training of by running it on 2 GPUs, thus I use  which fails with the error:

> AssertionError: Could not compute output Tensor(""model_2/Dense_Decoder/truediv:0"", shape=(?, 33, 22), dtype=float32)

I have tried parallelizing the two submodels individually using  and , yet **both succeed**. The problem appears **only** with the full model. 

I am using **Tensorflow 1.12.0** and **Keras 2.2.4**. A snippet that demonstrates the problem (at least on my machine) is:



I believe this problem might be similar to #9599 but I can be mistaken. "
keras,13115,"I'm requesting a built-in Keras layer for resizing tensors.

Today:
I have to implement this layer either as a Lambda or a custom layer, both of which simply wrap . This causes huge headaches for exporting to CoreML and TF-Lite. Existing layers like  and  do not meet my needs, since I am downsampling to a fixed output size, so scaling by some factor does not work.

CoreML has a built-in  layer, but it is difficult to connect my custom Keras layer with that CoreML built-in.

Ideal solution:
There is a built-in Keras layer, I ask the CoreML team to use it, they implement a new layer conversion mapping, and I convert my model without digging into the guts of both Keras and CoreML.

",0,Feature Request: built-in ResizeBilinear Layer,"Feature Request: built-in ResizeBilinear Layer I'm requesting a built-in Keras layer for resizing tensors.

Today:
I have to implement this layer either as a Lambda or a custom layer, both of which simply wrap . This causes huge headaches for exporting to CoreML and TF-Lite. Existing layers like  and  do not meet my needs, since I am downsampling to a fixed output size, so scaling by some factor does not work.

CoreML has a built-in  layer, but it is difficult to connect my custom Keras layer with that CoreML built-in.

Ideal solution:
There is a built-in Keras layer, I ask the CoreML team to use it, they implement a new layer conversion mapping, and I convert my model without digging into the guts of both Keras and CoreML.

"
keras,12596,"    
    from keras.models import load_model
    #model = load_model('atrs')
     (notSanta, santa) = model.predict(r)[0]

i got shape of ry as (1, 1, 224, 224, 3) inspite of adding one dimension it gave me two dimensions in case you wanted to know shape of r it is (400, 650, 3)


",0," Error when checking input: expected conv2d_21_input to have 4 dimensions, but got array with shape (1, 1, 224, 224, 3)"," Error when checking input: expected conv2d_21_input to have 4 dimensions, but got array with shape (1, 1, 224, 224, 3)     
    from keras.models import load_model
    #model = load_model('atrs')
     (notSanta, santa) = model.predict(r)[0]

i got shape of ry as (1, 1, 224, 224, 3) inspite of adding one dimension it gave me two dimensions in case you wanted to know shape of r it is (400, 650, 3)


"
keras,13375,"I am trying to classify some video into 3 different classes. Each video has different length of frame. The training data has the shape of (104, None, 528) where: 

 - 104 = Number of videos
 - None = number of frames for each video which are different
 - 528 = Number of features for each frame

As the sequence of frames for each video is long I am using ""stateful LSTM"" to manage the length of sequences. I have defined my model same as below:

    def LSTM_Model():
    
    model = Sequential()
    model.add(LSTM(units = 256, input_shape=(None, 528),\
                           return_sequences=False, stateful=True, batch_size = 1))
    model.add(Dropout(0.4))
    model.add(Dense(3, activation='softmax'))
    opt = keras.optimizers.SGD(lr=0.00005, decay = 1e-6, momentum=0.9, nesterov=True)
    model.compile(loss = 'categorical_crossentropy', optimizer = opt, metrics = ['accuracy'])
    model.summary()
    
    return model

 Then I trained and tested the model:

   

    def train_model(X, y, X_test, y_test, model):
        np.random.seed(200)
        epochs = 100
        maxlen = 500
    
        
        for epoch in range(epochs):
            
            mean_tr_loss, mean_tr_acc =[],[]
            print('Epoch: ', epoch + 1 )
            
            for sbj in range(X.shape[0]):
                
                video = X[sbj]
                y_sbj = y[sbj,:]
                y_new = y_sbj
                nb_frame = video.shape[0]
    
                for count in range(nb_frame // maxlen +1):
                    
                    if count == nb_frame // maxlen :
                        seq = video[count*maxlen + count:, :]
    
                    else:
                        seq = video[count*maxlen+count : (count+1)*maxlen+count, :]
                        seq = np.expand_dims(seq, axis=0)
                        
          #   ''' Using train_on_batch '''
                        
                        tr_loss, tr_acc = model.train_on_batch(seq, np.array([y_new])) 
                        mean_tr_loss.append(tr_loss)
                        mean_tr_acc.append(tr_acc)
   
    
                print('Training on subject', sbj+1, 'done' )
                model.reset_states() 
    

            print('accuracy training = {}'.format(np.mean(mean_tr_acc)))
            print('loss training = {}'.format(np.mean(mean_tr_loss)))
            print('___________________________________')
            

        
            print('Testing....')
            

            mean_te_loss, mean_te_acc =[],[]
        
            for sbj_test in range(X_test.shape[0]):
                
                video_test = X_test[sbj_test]
                y_new_test = y_test[sbj_test]
                nb_frame_test = video_test.shape[0]
                
                for i in range(nb_frame_test // maxlen + 1):
                    
                    if i == nb_frame_test // maxlen :
                        seq_test = video_test[i*maxlen + i:, :]
                    else:
                        seq_test = video_test[i*maxlen+i : (i+1)*maxlen+i, :]
                        seq_test = np.expand_dims(seq_test, axis=0)
                        te_loss, te_acc = model.test_on_batch(seq_test, np.array([y_new_test])) 
                        mean_te_loss.append(te_loss)
                        mean_te_acc.append(te_acc)                
                print('Testing on subject', sbj_test+1, 'done' )
                model.reset_states()   
                 
    

            print('accuracy testing = {}'.format(np.mean(mean_te_acc)))
            print('loss testing = {}'.format(np.mean(mean_te_loss)))

 
        

In the above code I considered each video separately and then each video was divided to different frame sequences with length 500 (except last sequence frame for each video because the length of frames are not divisible by 500). The training accuracy and test accuracy are same as below.

    Epoch1 : accuracy training = 0.3694     accuracy testing = 0.3927
             loss training = 1.146          loss testing = 1.109
    Epoch2 : accuracy training = 0.4423     accuracy testing = 0.4048
             loss training = 1.053          loss testing = 1.109
    Epoch3 : accuracy training = 0.5017     accuracy testing = 0.4236
             loss training = 0.994          loss testing = 1.115
    Epoch4 : accuracy training = 0.5491     accuracy testing = 0.4099
             loss training = 0.94           loss testing = 1.124
    Epoch5: accuracy training = 0.5612      accuracy testing = 0.4013
            loss training = 0.924           loss testing = 1.128
    Epoch6 : accuracy training = 0.6142     accuracy testing = 0.4113
             loss training = 0.859          loss testing = 1.137
    Epoch7 : accuracy training = 0.6263     accuracy testing = 0.4116
             loss training = 0.824          loss testing = 1.142
    Epoch8 : accuracy training = 0.6659     accuracy testing = 0.415
             loss training = 0.775          loss testing = 1.152

 

After 100 epochs training accuracy increases while testing accuracy doesn't improve. If the case is ""overfitting"" adding dropout layer should help which didn't. So, I am confused about the cause. 

Any idea or suggestion would be appreciated.  
  
",0,"Video classification using stateful LSTM , validation accuracy doesn't improve","Video classification using stateful LSTM , validation accuracy doesn't improve I am trying to classify some video into 3 different classes. Each video has different length of frame. The training data has the shape of (104, None, 528) where: 

 - 104 = Number of videos
 - None = number of frames for each video which are different
 - 528 = Number of features for each frame

As the sequence of frames for each video is long I am using ""stateful LSTM"" to manage the length of sequences. I have defined my model same as below:

    def LSTM_Model():
    
    model = Sequential()
    model.add(LSTM(units = 256, input_shape=(None, 528),\
                           return_sequences=False, stateful=True, batch_size = 1))
    model.add(Dropout(0.4))
    model.add(Dense(3, activation='softmax'))
    opt = keras.optimizers.SGD(lr=0.00005, decay = 1e-6, momentum=0.9, nesterov=True)
    model.compile(loss = 'categorical_crossentropy', optimizer = opt, metrics = ['accuracy'])
    model.summary()
    
    return model

 Then I trained and tested the model:

   

    def train_model(X, y, X_test, y_test, model):
        np.random.seed(200)
        epochs = 100
        maxlen = 500
    
        
        for epoch in range(epochs):
            
            mean_tr_loss, mean_tr_acc =[],[]
            print('Epoch: ', epoch + 1 )
            
            for sbj in range(X.shape[0]):
                
                video = X[sbj]
                y_sbj = y[sbj,:]
                y_new = y_sbj
                nb_frame = video.shape[0]
    
                for count in range(nb_frame // maxlen +1):
                    
                    if count == nb_frame // maxlen :
                        seq = video[count*maxlen + count:, :]
    
                    else:
                        seq = video[count*maxlen+count : (count+1)*maxlen+count, :]
                        seq = np.expand_dims(seq, axis=0)
                        
          #   ''' Using train_on_batch '''
                        
                        tr_loss, tr_acc = model.train_on_batch(seq, np.array([y_new])) 
                        mean_tr_loss.append(tr_loss)
                        mean_tr_acc.append(tr_acc)
   
    
                print('Training on subject', sbj+1, 'done' )
                model.reset_states() 
    

            print('accuracy training = {}'.format(np.mean(mean_tr_acc)))
            print('loss training = {}'.format(np.mean(mean_tr_loss)))
            print('___________________________________')
            

        
            print('Testing....')
            

            mean_te_loss, mean_te_acc =[],[]
        
            for sbj_test in range(X_test.shape[0]):
                
                video_test = X_test[sbj_test]
                y_new_test = y_test[sbj_test]
                nb_frame_test = video_test.shape[0]
                
                for i in range(nb_frame_test // maxlen + 1):
                    
                    if i == nb_frame_test // maxlen :
                        seq_test = video_test[i*maxlen + i:, :]
                    else:
                        seq_test = video_test[i*maxlen+i : (i+1)*maxlen+i, :]
                        seq_test = np.expand_dims(seq_test, axis=0)
                        te_loss, te_acc = model.test_on_batch(seq_test, np.array([y_new_test])) 
                        mean_te_loss.append(te_loss)
                        mean_te_acc.append(te_acc)                
                print('Testing on subject', sbj_test+1, 'done' )
                model.reset_states()   
                 
    

            print('accuracy testing = {}'.format(np.mean(mean_te_acc)))
            print('loss testing = {}'.format(np.mean(mean_te_loss)))

 
        

In the above code I considered each video separately and then each video was divided to different frame sequences with length 500 (except last sequence frame for each video because the length of frames are not divisible by 500). The training accuracy and test accuracy are same as below.

    Epoch1 : accuracy training = 0.3694     accuracy testing = 0.3927
             loss training = 1.146          loss testing = 1.109
    Epoch2 : accuracy training = 0.4423     accuracy testing = 0.4048
             loss training = 1.053          loss testing = 1.109
    Epoch3 : accuracy training = 0.5017     accuracy testing = 0.4236
             loss training = 0.994          loss testing = 1.115
    Epoch4 : accuracy training = 0.5491     accuracy testing = 0.4099
             loss training = 0.94           loss testing = 1.124
    Epoch5: accuracy training = 0.5612      accuracy testing = 0.4013
            loss training = 0.924           loss testing = 1.128
    Epoch6 : accuracy training = 0.6142     accuracy testing = 0.4113
             loss training = 0.859          loss testing = 1.137
    Epoch7 : accuracy training = 0.6263     accuracy testing = 0.4116
             loss training = 0.824          loss testing = 1.142
    Epoch8 : accuracy training = 0.6659     accuracy testing = 0.415
             loss training = 0.775          loss testing = 1.152

 

After 100 epochs training accuracy increases while testing accuracy doesn't improve. If the case is ""overfitting"" adding dropout layer should help which didn't. So, I am confused about the cause. 

Any idea or suggestion would be appreciated.  
  
"
keras,10903,"Hi,
I am trying to pass a RGB image from a simulator into my custom neural network. At the source of the RGB generation (simulator), the dimension of RGB image is (3,144,256). 

This is how i construct the neural network:




Now, my rbg_shape is (1, 3, 144, 256).

This is the error that i get:
    _rgb_model.add(Conv2D(96, (11, 11), strides=(3, 3), padding='valid', activation='relu', input_shape=rgb_kshape, data_format = ""channels_first""))
  File ""/usr/local/lib/python2.7/dist-packages/keras/engine/sequential.py"", line 166, in add
    layer(x)
  File ""/usr/local/lib/python2.7/dist-packages/keras/engine/base_layer.py"", line 414, in __call__
    self.assert_input_compatibility(inputs)
  File ""/usr/local/lib/python2.7/dist-packages/keras/engine/base_layer.py"", line 311, in assert_input_compatibility
    str(K.ndim(x)))
ValueError: Input 0 is incompatible with layer conv2d_1: expected ndim=4, found ndim=5_

Why is keras complaining that the expected dimension is 5 when my actual dimension is 4?

Thanks",0,"ValueError: Input 0 is incompatible with layer conv2d_1: expeced ndim=4, found ndim=5","ValueError: Input 0 is incompatible with layer conv2d_1: expeced ndim=4, found ndim=5 Hi,
I am trying to pass a RGB image from a simulator into my custom neural network. At the source of the RGB generation (simulator), the dimension of RGB image is (3,144,256). 

This is how i construct the neural network:




Now, my rbg_shape is (1, 3, 144, 256).

This is the error that i get:
    _rgb_model.add(Conv2D(96, (11, 11), strides=(3, 3), padding='valid', activation='relu', input_shape=rgb_kshape, data_format = ""channels_first""))
  File ""/usr/local/lib/python2.7/dist-packages/keras/engine/sequential.py"", line 166, in add
    layer(x)
  File ""/usr/local/lib/python2.7/dist-packages/keras/engine/base_layer.py"", line 414, in __call__
    self.assert_input_compatibility(inputs)
  File ""/usr/local/lib/python2.7/dist-packages/keras/engine/base_layer.py"", line 311, in assert_input_compatibility
    str(K.ndim(x)))
ValueError: Input 0 is incompatible with layer conv2d_1: expected ndim=4, found ndim=5_

Why is keras complaining that the expected dimension is 5 when my actual dimension is 4?

Thanks"
keras,9845,"Hello,

I am using a trained model form keras. this model doesn't fit in to the GPU memory so I want to divide it into CPU and GPU and maybe GPU on different machines. 
SO if I want to get all tf.variable on my CPU and the calculation on GPU how can I do this because there is some docs that show we can place the tf.variable on CPU and the calculation on a GPU. (I am taking about a pre-trained keras model).

If I want to separate the layers of the pre-trained model  into different GPU on different machines. is it correct to use with  and then add the layers to my sequence model like this:



Thank you very much.
",0,keras trained model using different tf device,"keras trained model using different tf device Hello,

I am using a trained model form keras. this model doesn't fit in to the GPU memory so I want to divide it into CPU and GPU and maybe GPU on different machines. 
SO if I want to get all tf.variable on my CPU and the calculation on GPU how can I do this because there is some docs that show we can place the tf.variable on CPU and the calculation on a GPU. (I am taking about a pre-trained keras model).

If I want to separate the layers of the pre-trained model  into different GPU on different machines. is it correct to use with  and then add the layers to my sequence model like this:



Thank you very much.
"
keras,10577,"Traceback (most recent call last):
  File ""C:\Users\xion\Anaconda2\envs\tensorflow-gpu\lib\site-packages\tensorfl
ow\python\client\session.py"", line 1322, in _do_call
    return fn(*args)
  File ""C:\Users\xion\Anaconda2\envs\tensorflow-gpu\lib\site-packages\tensorfl
ow\python\client\session.py"", line 1305, in _run_fn
    self._extend_graph()
  File ""C:\Users\xion\Anaconda2\envs\tensorflow-gpu\lib\site-packages\tensorfl
ow\python\client\session.py"", line 1340, in _extend_graph
    tf_session.ExtendSession(self._session)
tensorflow.python.framework.errors_impl.InvalidArgumentError: No OpKernel was re
gistered to support Op 'CudnnRNN' with these attrs.  Registered devices: [CPU],
Registered kernels:
  device='GPU'; T in [DT_HALF]
  device='GPU'; T in [DT_FLOAT]
  device='GPU'; T in [DT_DOUBLE]

         [[Node: bidirectional_1/CudnnRNN_1 = CudnnRNN[T=DT_FLOAT, direction=""un
idirectional"", dropout=0, input_mode=""linear_input"", is_training=true, rnn_mode=
""gru"", seed=87654321, seed2=0](bidirectional_1/transpose_2, bidirectional_1/Expa
ndDims_3, bidirectional_1/Const_1, bidirectional_1/concat_1)]]

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""Z:\trader_connect.py"", line 157, in <module>
    tick()
  File ""Z:\trader_connect.py"", line 74, in tick
    model1 = keras.models.load_model('Z:\\Productionmodel.h5')
  File ""C:\Users\xion\Anaconda2\envs\tensorflow-gpu\lib\site-packages\keras\en
gine\saving.py"", line 264, in load_model
    load_weights_from_hdf5_group(f['model_weights'], model.layers)
  File ""C:\Users\xion\Anaconda2\envs\tensorflow-gpu\lib\site-packages\keras\en
gine\saving.py"", line 929, in load_weights_from_hdf5_group
    K.batch_set_value(weight_value_tuples)
  File ""C:\Users\xion\Anaconda2\envs\tensorflow-gpu\lib\site-packages\keras\ba
ckend\tensorflow_backend.py"", line 2435, in batch_set_value
    get_session().run(assign_ops, feed_dict=feed_dict)
  File ""C:\Users\xion\Anaconda2\envs\tensorflow-gpu\lib\site-packages\keras\ba
ckend\tensorflow_backend.py"", line 196, in get_session
    [tf.is_variable_initialized(v) for v in candidate_vars])
  File ""C:\Users\xion\Anaconda2\envs\tensorflow-gpu\lib\site-packages\tensorfl
ow\python\client\session.py"", line 900, in run
    run_metadata_ptr)
  File ""C:\Users\xion\Anaconda2\envs\tensorflow-gpu\lib\site-packages\tensorfl
ow\python\client\session.py"", line 1135, in _run
    feed_dict_tensor, options, run_metadata)
  File ""C:\Users\xion\Anaconda2\envs\tensorflow-gpu\lib\site-packages\tensorfl
ow\python\client\session.py"", line 1316, in _do_run
    run_metadata)
  File ""C:\Users\xion\Anaconda2\envs\tensorflow-gpu\lib\site-packages\tensorfl
ow\python\client\session.py"", line 1335, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.InvalidArgumentError: No OpKernel was re
gistered to support Op 'CudnnRNN' with these attrs.  Registered devices: [CPU],
Registered kernels:
  device='GPU'; T in [DT_HALF]
  device='GPU'; T in [DT_FLOAT]
  device='GPU'; T in [DT_DOUBLE]

         [[Node: bidirectional_1/CudnnRNN_1 = CudnnRNN[T=DT_FLOAT, direction=""un
idirectional"", dropout=0, input_mode=""linear_input"", is_training=true, rnn_mode=
""gru"", seed=87654321, seed2=0](bidirectional_1/transpose_2, bidirectional_1/Expa
ndDims_3, bidirectional_1/Const_1, bidirectional_1/concat_1)]]

Caused by op 'bidirectional_1/CudnnRNN_1', defined at:
  File ""Z:\trader_connect.py"", line 157, in <module>
    tick()
  File ""Z:\trader_connect.py"", line 74, in tick
    model1 = keras.models.load_model('Z:\\Productionmodel.h5')
  File ""C:\Users\xion\Anaconda2\envs\tensorflow-gpu\lib\site-packages\keras\en
gine\saving.py"", line 261, in load_model
    model = model_from_config(model_config, custom_objects=custom_objects)
  File ""C:\Users\xion\Anaconda2\envs\tensorflow-gpu\lib\site-packages\keras\en
gine\saving.py"", line 335, in model_from_config
    return deserialize(config, custom_objects=custom_objects)
  File ""C:\Users\xion\Anaconda2\envs\tensorflow-gpu\lib\site-packages\keras\la
yers\__init__.py"", line 55, in deserialize
    printable_module_name='layer')
  File ""C:\Users\xion\Anaconda2\envs\tensorflow-gpu\lib\site-packages\keras\ut
ils\generic_utils.py"", line 145, in deserialize_keras_object
    list(custom_objects.items())))
  File ""C:\Users\xion\Anaconda2\envs\tensorflow-gpu\lib\site-packages\keras\en
gine\sequential.py"", line 293, in from_config
    model.add(layer)
  File ""C:\Users\xion\Anaconda2\envs\tensorflow-gpu\lib\site-packages\keras\en
gine\sequential.py"", line 166, in add
    layer(x)
  File ""C:\Users\xion\Anaconda2\envs\tensorflow-gpu\lib\site-packages\keras\la
yers\wrappers.py"", line 426, in __call__
    return super(Bidirectional, self).__call__(inputs, **kwargs)
  File ""C:\Users\xion\Anaconda2\envs\tensorflow-gpu\lib\site-packages\keras\en
gine\base_layer.py"", line 460, in __call__
    output = self.call(inputs, **kwargs)
  File ""C:\Users\xion\Anaconda2\envs\tensorflow-gpu\lib\site-packages\keras\la
yers\wrappers.py"", line 505, in call
    y_rev = self.backward_layer.call(inputs, **kwargs)
  File ""C:\Users\xion\Anaconda2\envs\tensorflow-gpu\lib\site-packages\keras\la
yers\cudnn_recurrent.py"", line 90, in call
    output, states = self._process_batch(inputs, initial_state)
  File ""C:\Users\xion\Anaconda2\envs\tensorflow-gpu\lib\site-packages\keras\la
yers\cudnn_recurrent.py"", line 297, in _process_batch
    is_training=True)
  File ""C:\Users\xion\Anaconda2\envs\tensorflow-gpu\lib\site-packages\tensorfl
ow\contrib\cudnn_rnn\python\ops\cudnn_rnn_ops.py"", line 1623, in __call__
    seed=self._seed)
  File ""C:\Users\xion\Anaconda2\envs\tensorflow-gpu\lib\site-packages\tensorfl
ow\contrib\cudnn_rnn\python\ops\cudnn_rnn_ops.py"", line 1012, in _cudnn_rnn_no_i
nput_c
    direction, dropout, seed, name)
  File ""C:\Users\xion\Anaconda2\envs\tensorflow-gpu\lib\site-packages\tensorfl
ow\contrib\cudnn_rnn\python\ops\cudnn_rnn_ops.py"", line 926, in _cudnn_rnn
    name=name)
  File ""C:\Users\xion\Anaconda2\envs\tensorflow-gpu\lib\site-packages\tensorfl
ow\python\ops\gen_cudnn_rnn_ops.py"", line 143, in cudnn_rnn
    is_training=is_training, name=name)
  File ""C:\Users\xion\Anaconda2\envs\tensorflow-gpu\lib\site-packages\tensorfl
ow\python\framework\op_def_library.py"", line 787, in _apply_op_helper
    op_def=op_def)
  File ""C:\Users\xion\Anaconda2\envs\tensorflow-gpu\lib\site-packages\tensorfl
ow\python\framework\ops.py"", line 3392, in create_op
    op_def=op_def)
  File ""C:\Users\xion\Anaconda2\envs\tensorflow-gpu\lib\site-packages\tensorfl
ow\python\framework\ops.py"", line 1718, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-
access

InvalidArgumentError (see above for traceback): No OpKernel was registered to su
pport Op 'CudnnRNN' with these attrs.  Registered devices: [CPU], Registered ker
nels:
  device='GPU'; T in [DT_HALF]
  device='GPU'; T in [DT_FLOAT]
  device='GPU'; T in [DT_DOUBLE]

         [[Node: bidirectional_1/CudnnRNN_1 = CudnnRNN[T=DT_FLOAT, direction=""un
idirectional"", dropout=0, input_mode=""linear_input"", is_training=true, rnn_mode=
""gru"", seed=87654321, seed2=0](bidirectional_1/transpose_2, bidirectional_1/Expa
ndDims_3, bidirectional_1/Const_1, bidirectional_1/concat_1)]]


(tensorflow-gpu) C:\Users\xion>python Z:\trader_connect.py --csv Y:\EURUSD,5.c
sv
Using TensorFlow backend.
2018-07-01 20:58:02.203507: I T:\src\github\tensorflow\tensorflow\core\common_ru
ntime\gpu\gpu_device.cc:1356] Found device 0 with properties:
name: GeForce GT 530 major: 2 minor: 1 memoryClockRate(GHz): 1.399
pciBusID: 0000:01:00.0
totalMemory: 2.00GiB freeMemory: 1.87GiB
2018-07-01 20:58:02.204507: I T:\src\github\tensorflow\tensorflow\core\common_ru
ntime\gpu\gpu_device.cc:1406] Ignoring visible gpu device (device: 0, name: GeFo
rce GT 530, pci bus id: 0000:01:00.0, compute capability: 2.1) with Cuda compute
 capability 2.1. The minimum required Cuda capability is 3.0.
2018-07-01 20:58:02.204507: I T:\src\github\tensorflow\tensorflow\core\common_ru
ntime\gpu\gpu_device.cc:923] Device interconnect StreamExecutor with strength 1
edge matrix:
2018-07-01 20:58:02.204507: I T:\src\github\tensorflow\tensorflow\core\common_ru
ntime\gpu\gpu_device.cc:929]      0
2018-07-01 20:58:02.204507: I T:\src\github\tensorflow\tensorflow\core\common_ru
ntime\gpu\gpu_device.cc:942] 0:   N
Traceback (most recent call last):
  File ""C:\Users\xion\Anaconda2\envs\tensorflow-gpu\lib\site-packages\tensorfl
ow\python\client\session.py"", line 1322, in _do_call
    return fn(*args)
  File ""C:\Users\xion\Anaconda2\envs\tensorflow-gpu\lib\site-packages\tensorfl
ow\python\client\session.py"", line 1305, in _run_fn
    self._extend_graph()
  File ""C:\Users\xion\Anaconda2\envs\tensorflow-gpu\lib\site-packages\tensorfl
ow\python\client\session.py"", line 1340, in _extend_graph
    tf_session.ExtendSession(self._session)
tensorflow.python.framework.errors_impl.InvalidArgumentError: No OpKernel was re
gistered to support Op 'CudnnRNN' with these attrs.  Registered devices: [CPU],
Registered kernels:
  device='GPU'; T in [DT_HALF]
  device='GPU'; T in [DT_FLOAT]
  device='GPU'; T in [DT_DOUBLE]

         [[Node: bidirectional_1/CudnnRNN_1 = CudnnRNN[T=DT_DOUBLE, direction=""u
nidirectional"", dropout=0, input_mode=""linear_input"", is_training=true, rnn_mode
=""gru"", seed=87654321, seed2=0](bidirectional_1/transpose_2, bidirectional_1/Exp
andDims_3, bidirectional_1/Const_1, bidirectional_1/concat_1)]]

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""Z:\trader_connect.py"", line 157, in <module>
    tick()
  File ""Z:\trader_connect.py"", line 74, in tick
    model1 = keras.models.load_model('Z:\\Productionmodel.h5')
  File ""C:\Users\xion\Anaconda2\envs\tensorflow-gpu\lib\site-packages\keras\en
gine\saving.py"", line 264, in load_model
    load_weights_from_hdf5_group(f['model_weights'], model.layers)
  File ""C:\Users\xion\Anaconda2\envs\tensorflow-gpu\lib\site-packages\keras\en
gine\saving.py"", line 929, in load_weights_from_hdf5_group
    K.batch_set_value(weight_value_tuples)
  File ""C:\Users\xion\Anaconda2\envs\tensorflow-gpu\lib\site-packages\keras\ba
ckend\tensorflow_backend.py"", line 2435, in batch_set_value
    get_session().run(assign_ops, feed_dict=feed_dict)
  File ""C:\Users\xion\Anaconda2\envs\tensorflow-gpu\lib\site-packages\keras\ba
ckend\tensorflow_backend.py"", line 196, in get_session
    [tf.is_variable_initialized(v) for v in candidate_vars])
  File ""C:\Users\xion\Anaconda2\envs\tensorflow-gpu\lib\site-packages\tensorfl
ow\python\client\session.py"", line 900, in run
    run_metadata_ptr)
  File ""C:\Users\xion\Anaconda2\envs\tensorflow-gpu\lib\site-packages\tensorfl
ow\python\client\session.py"", line 1135, in _run
    feed_dict_tensor, options, run_metadata)
  File ""C:\Users\xion\Anaconda2\envs\tensorflow-gpu\lib\site-packages\tensorfl
ow\python\client\session.py"", line 1316, in _do_run
    run_metadata)
  File ""C:\Users\xion\Anaconda2\envs\tensorflow-gpu\lib\site-packages\tensorfl
ow\python\client\session.py"", line 1335, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.InvalidArgumentError: No OpKernel was re
gistered to support Op 'CudnnRNN' with these attrs.  Registered devices: [CPU],
Registered kernels:
  device='GPU'; T in [DT_HALF]
  device='GPU'; T in [DT_FLOAT]
  device='GPU'; T in [DT_DOUBLE]

         [[Node: bidirectional_1/CudnnRNN_1 = CudnnRNN[T=DT_DOUBLE, direction=""u
nidirectional"", dropout=0, input_mode=""linear_input"", is_training=true, rnn_mode
=""gru"", seed=87654321, seed2=0](bidirectional_1/transpose_2, bidirectional_1/Exp
andDims_3, bidirectional_1/Const_1, bidirectional_1/concat_1)]]

Caused by op 'bidirectional_1/CudnnRNN_1', defined at:
  File ""Z:\trader_connect.py"", line 157, in <module>
    tick()
  File ""Z:\trader_connect.py"", line 74, in tick
    model1 = keras.models.load_model('Z:\\Productionmodel.h5')
  File ""C:\Users\xion\Anaconda2\envs\tensorflow-gpu\lib\site-packages\keras\en
gine\saving.py"", line 261, in load_model
    model = model_from_config(model_config, custom_objects=custom_objects)
  File ""C:\Users\xion\Anaconda2\envs\tensorflow-gpu\lib\site-packages\keras\en
gine\saving.py"", line 335, in model_from_config
    return deserialize(config, custom_objects=custom_objects)
  File ""C:\Users\xion\Anaconda2\envs\tensorflow-gpu\lib\site-packages\keras\la
yers\__init__.py"", line 55, in deserialize
    printable_module_name='layer')
  File ""C:\Users\xion\Anaconda2\envs\tensorflow-gpu\lib\site-packages\keras\ut
ils\generic_utils.py"", line 145, in deserialize_keras_object
    list(custom_objects.items())))
  File ""C:\Users\xion\Anaconda2\envs\tensorflow-gpu\lib\site-packages\keras\en
gine\sequential.py"", line 293, in from_config
    model.add(layer)
  File ""C:\Users\xion\Anaconda2\envs\tensorflow-gpu\lib\site-packages\keras\en
gine\sequential.py"", line 166, in add
    layer(x)
  File ""C:\Users\xion\Anaconda2\envs\tensorflow-gpu\lib\site-packages\keras\la
yers\wrappers.py"", line 426, in __call__
    return super(Bidirectional, self).__call__(inputs, **kwargs)
  File ""C:\Users\xion\Anaconda2\envs\tensorflow-gpu\lib\site-packages\keras\en
gine\base_layer.py"", line 460, in __call__
    output = self.call(inputs, **kwargs)
  File ""C:\Users\xion\Anaconda2\envs\tensorflow-gpu\lib\site-packages\keras\la
yers\wrappers.py"", line 505, in call
    y_rev = self.backward_layer.call(inputs, **kwargs)
  File ""C:\Users\xion\Anaconda2\envs\tensorflow-gpu\lib\site-packages\keras\la
yers\cudnn_recurrent.py"", line 90, in call
    output, states = self._process_batch(inputs, initial_state)
  File ""C:\Users\xion\Anaconda2\envs\tensorflow-gpu\lib\site-packages\keras\la
yers\cudnn_recurrent.py"", line 297, in _process_batch
    is_training=True)
  File ""C:\Users\xion\Anaconda2\envs\tensorflow-gpu\lib\site-packages\tensorfl
ow\contrib\cudnn_rnn\python\ops\cudnn_rnn_ops.py"", line 1623, in __call__
    seed=self._seed)
  File ""C:\Users\xion\Anaconda2\envs\tensorflow-gpu\lib\site-packages\tensorfl
ow\contrib\cudnn_rnn\python\ops\cudnn_rnn_ops.py"", line 1012, in _cudnn_rnn_no_i
nput_c
    direction, dropout, seed, name)
  File ""C:\Users\xion\Anaconda2\envs\tensorflow-gpu\lib\site-packages\tensorfl
ow\contrib\cudnn_rnn\python\ops\cudnn_rnn_ops.py"", line 926, in _cudnn_rnn
    name=name)
  File ""C:\Users\xion\Anaconda2\envs\tensorflow-gpu\lib\site-packages\tensorfl
ow\python\ops\gen_cudnn_rnn_ops.py"", line 143, in cudnn_rnn
    is_training=is_training, name=name)
  File ""C:\Users\xion\Anaconda2\envs\tensorflow-gpu\lib\site-packages\tensorfl
ow\python\framework\op_def_library.py"", line 787, in _apply_op_helper
    op_def=op_def)
  File ""C:\Users\xion\Anaconda2\envs\tensorflow-gpu\lib\site-packages\tensorfl
ow\python\framework\ops.py"", line 3392, in create_op
    op_def=op_def)
  File ""C:\Users\xion\Anaconda2\envs\tensorflow-gpu\lib\site-packages\tensorfl
ow\python\framework\ops.py"", line 1718, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-
access

InvalidArgumentError (see above for traceback): No OpKernel was registered to su
pport Op 'CudnnRNN' with these attrs.  Registered devices: [CPU], Registered ker
nels:
  device='GPU'; T in [DT_HALF]
  device='GPU'; T in [DT_FLOAT]
  device='GPU'; T in [DT_DOUBLE]

         [[Node: bidirectional_1/CudnnRNN_1 = CudnnRNN[T=DT_DOUBLE, direction=""u
nidirectional"", dropout=0, input_mode=""linear_input"", is_training=true, rnn_mode
=""gru"", seed=87654321, seed2=0](bidirectional_1/transpose_2, bidirectional_1/Exp
andDims_3, bidirectional_1/Const_1, bidirectional_1/concat_1)]]








I've tried--fresh reinstall, change float

Any fixes? No idea why Im getting this. P.S: I trained the model on a titan v and I am trying to now open it on a computer with a GeForce GT 530 Gpu.",0,Failing to load h5 model using tf-gpu as backend for keras?,"Failing to load h5 model using tf-gpu as backend for keras? Traceback (most recent call last):
  File ""C:\Users\xion\Anaconda2\envs\tensorflow-gpu\lib\site-packages\tensorfl
ow\python\client\session.py"", line 1322, in _do_call
    return fn(*args)
  File ""C:\Users\xion\Anaconda2\envs\tensorflow-gpu\lib\site-packages\tensorfl
ow\python\client\session.py"", line 1305, in _run_fn
    self._extend_graph()
  File ""C:\Users\xion\Anaconda2\envs\tensorflow-gpu\lib\site-packages\tensorfl
ow\python\client\session.py"", line 1340, in _extend_graph
    tf_session.ExtendSession(self._session)
tensorflow.python.framework.errors_impl.InvalidArgumentError: No OpKernel was re
gistered to support Op 'CudnnRNN' with these attrs.  Registered devices: [CPU],
Registered kernels:
  device='GPU'; T in [DT_HALF]
  device='GPU'; T in [DT_FLOAT]
  device='GPU'; T in [DT_DOUBLE]

         [[Node: bidirectional_1/CudnnRNN_1 = CudnnRNN[T=DT_FLOAT, direction=""un
idirectional"", dropout=0, input_mode=""linear_input"", is_training=true, rnn_mode=
""gru"", seed=87654321, seed2=0](bidirectional_1/transpose_2, bidirectional_1/Expa
ndDims_3, bidirectional_1/Const_1, bidirectional_1/concat_1)]]

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""Z:\trader_connect.py"", line 157, in <module>
    tick()
  File ""Z:\trader_connect.py"", line 74, in tick
    model1 = keras.models.load_model('Z:\\Productionmodel.h5')
  File ""C:\Users\xion\Anaconda2\envs\tensorflow-gpu\lib\site-packages\keras\en
gine\saving.py"", line 264, in load_model
    load_weights_from_hdf5_group(f['model_weights'], model.layers)
  File ""C:\Users\xion\Anaconda2\envs\tensorflow-gpu\lib\site-packages\keras\en
gine\saving.py"", line 929, in load_weights_from_hdf5_group
    K.batch_set_value(weight_value_tuples)
  File ""C:\Users\xion\Anaconda2\envs\tensorflow-gpu\lib\site-packages\keras\ba
ckend\tensorflow_backend.py"", line 2435, in batch_set_value
    get_session().run(assign_ops, feed_dict=feed_dict)
  File ""C:\Users\xion\Anaconda2\envs\tensorflow-gpu\lib\site-packages\keras\ba
ckend\tensorflow_backend.py"", line 196, in get_session
    [tf.is_variable_initialized(v) for v in candidate_vars])
  File ""C:\Users\xion\Anaconda2\envs\tensorflow-gpu\lib\site-packages\tensorfl
ow\python\client\session.py"", line 900, in run
    run_metadata_ptr)
  File ""C:\Users\xion\Anaconda2\envs\tensorflow-gpu\lib\site-packages\tensorfl
ow\python\client\session.py"", line 1135, in _run
    feed_dict_tensor, options, run_metadata)
  File ""C:\Users\xion\Anaconda2\envs\tensorflow-gpu\lib\site-packages\tensorfl
ow\python\client\session.py"", line 1316, in _do_run
    run_metadata)
  File ""C:\Users\xion\Anaconda2\envs\tensorflow-gpu\lib\site-packages\tensorfl
ow\python\client\session.py"", line 1335, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.InvalidArgumentError: No OpKernel was re
gistered to support Op 'CudnnRNN' with these attrs.  Registered devices: [CPU],
Registered kernels:
  device='GPU'; T in [DT_HALF]
  device='GPU'; T in [DT_FLOAT]
  device='GPU'; T in [DT_DOUBLE]

         [[Node: bidirectional_1/CudnnRNN_1 = CudnnRNN[T=DT_FLOAT, direction=""un
idirectional"", dropout=0, input_mode=""linear_input"", is_training=true, rnn_mode=
""gru"", seed=87654321, seed2=0](bidirectional_1/transpose_2, bidirectional_1/Expa
ndDims_3, bidirectional_1/Const_1, bidirectional_1/concat_1)]]

Caused by op 'bidirectional_1/CudnnRNN_1', defined at:
  File ""Z:\trader_connect.py"", line 157, in <module>
    tick()
  File ""Z:\trader_connect.py"", line 74, in tick
    model1 = keras.models.load_model('Z:\\Productionmodel.h5')
  File ""C:\Users\xion\Anaconda2\envs\tensorflow-gpu\lib\site-packages\keras\en
gine\saving.py"", line 261, in load_model
    model = model_from_config(model_config, custom_objects=custom_objects)
  File ""C:\Users\xion\Anaconda2\envs\tensorflow-gpu\lib\site-packages\keras\en
gine\saving.py"", line 335, in model_from_config
    return deserialize(config, custom_objects=custom_objects)
  File ""C:\Users\xion\Anaconda2\envs\tensorflow-gpu\lib\site-packages\keras\la
yers\__init__.py"", line 55, in deserialize
    printable_module_name='layer')
  File ""C:\Users\xion\Anaconda2\envs\tensorflow-gpu\lib\site-packages\keras\ut
ils\generic_utils.py"", line 145, in deserialize_keras_object
    list(custom_objects.items())))
  File ""C:\Users\xion\Anaconda2\envs\tensorflow-gpu\lib\site-packages\keras\en
gine\sequential.py"", line 293, in from_config
    model.add(layer)
  File ""C:\Users\xion\Anaconda2\envs\tensorflow-gpu\lib\site-packages\keras\en
gine\sequential.py"", line 166, in add
    layer(x)
  File ""C:\Users\xion\Anaconda2\envs\tensorflow-gpu\lib\site-packages\keras\la
yers\wrappers.py"", line 426, in __call__
    return super(Bidirectional, self).__call__(inputs, **kwargs)
  File ""C:\Users\xion\Anaconda2\envs\tensorflow-gpu\lib\site-packages\keras\en
gine\base_layer.py"", line 460, in __call__
    output = self.call(inputs, **kwargs)
  File ""C:\Users\xion\Anaconda2\envs\tensorflow-gpu\lib\site-packages\keras\la
yers\wrappers.py"", line 505, in call
    y_rev = self.backward_layer.call(inputs, **kwargs)
  File ""C:\Users\xion\Anaconda2\envs\tensorflow-gpu\lib\site-packages\keras\la
yers\cudnn_recurrent.py"", line 90, in call
    output, states = self._process_batch(inputs, initial_state)
  File ""C:\Users\xion\Anaconda2\envs\tensorflow-gpu\lib\site-packages\keras\la
yers\cudnn_recurrent.py"", line 297, in _process_batch
    is_training=True)
  File ""C:\Users\xion\Anaconda2\envs\tensorflow-gpu\lib\site-packages\tensorfl
ow\contrib\cudnn_rnn\python\ops\cudnn_rnn_ops.py"", line 1623, in __call__
    seed=self._seed)
  File ""C:\Users\xion\Anaconda2\envs\tensorflow-gpu\lib\site-packages\tensorfl
ow\contrib\cudnn_rnn\python\ops\cudnn_rnn_ops.py"", line 1012, in _cudnn_rnn_no_i
nput_c
    direction, dropout, seed, name)
  File ""C:\Users\xion\Anaconda2\envs\tensorflow-gpu\lib\site-packages\tensorfl
ow\contrib\cudnn_rnn\python\ops\cudnn_rnn_ops.py"", line 926, in _cudnn_rnn
    name=name)
  File ""C:\Users\xion\Anaconda2\envs\tensorflow-gpu\lib\site-packages\tensorfl
ow\python\ops\gen_cudnn_rnn_ops.py"", line 143, in cudnn_rnn
    is_training=is_training, name=name)
  File ""C:\Users\xion\Anaconda2\envs\tensorflow-gpu\lib\site-packages\tensorfl
ow\python\framework\op_def_library.py"", line 787, in _apply_op_helper
    op_def=op_def)
  File ""C:\Users\xion\Anaconda2\envs\tensorflow-gpu\lib\site-packages\tensorfl
ow\python\framework\ops.py"", line 3392, in create_op
    op_def=op_def)
  File ""C:\Users\xion\Anaconda2\envs\tensorflow-gpu\lib\site-packages\tensorfl
ow\python\framework\ops.py"", line 1718, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-
access

InvalidArgumentError (see above for traceback): No OpKernel was registered to su
pport Op 'CudnnRNN' with these attrs.  Registered devices: [CPU], Registered ker
nels:
  device='GPU'; T in [DT_HALF]
  device='GPU'; T in [DT_FLOAT]
  device='GPU'; T in [DT_DOUBLE]

         [[Node: bidirectional_1/CudnnRNN_1 = CudnnRNN[T=DT_FLOAT, direction=""un
idirectional"", dropout=0, input_mode=""linear_input"", is_training=true, rnn_mode=
""gru"", seed=87654321, seed2=0](bidirectional_1/transpose_2, bidirectional_1/Expa
ndDims_3, bidirectional_1/Const_1, bidirectional_1/concat_1)]]


(tensorflow-gpu) C:\Users\xion>python Z:\trader_connect.py --csv Y:\EURUSD,5.c
sv
Using TensorFlow backend.
2018-07-01 20:58:02.203507: I T:\src\github\tensorflow\tensorflow\core\common_ru
ntime\gpu\gpu_device.cc:1356] Found device 0 with properties:
name: GeForce GT 530 major: 2 minor: 1 memoryClockRate(GHz): 1.399
pciBusID: 0000:01:00.0
totalMemory: 2.00GiB freeMemory: 1.87GiB
2018-07-01 20:58:02.204507: I T:\src\github\tensorflow\tensorflow\core\common_ru
ntime\gpu\gpu_device.cc:1406] Ignoring visible gpu device (device: 0, name: GeFo
rce GT 530, pci bus id: 0000:01:00.0, compute capability: 2.1) with Cuda compute
 capability 2.1. The minimum required Cuda capability is 3.0.
2018-07-01 20:58:02.204507: I T:\src\github\tensorflow\tensorflow\core\common_ru
ntime\gpu\gpu_device.cc:923] Device interconnect StreamExecutor with strength 1
edge matrix:
2018-07-01 20:58:02.204507: I T:\src\github\tensorflow\tensorflow\core\common_ru
ntime\gpu\gpu_device.cc:929]      0
2018-07-01 20:58:02.204507: I T:\src\github\tensorflow\tensorflow\core\common_ru
ntime\gpu\gpu_device.cc:942] 0:   N
Traceback (most recent call last):
  File ""C:\Users\xion\Anaconda2\envs\tensorflow-gpu\lib\site-packages\tensorfl
ow\python\client\session.py"", line 1322, in _do_call
    return fn(*args)
  File ""C:\Users\xion\Anaconda2\envs\tensorflow-gpu\lib\site-packages\tensorfl
ow\python\client\session.py"", line 1305, in _run_fn
    self._extend_graph()
  File ""C:\Users\xion\Anaconda2\envs\tensorflow-gpu\lib\site-packages\tensorfl
ow\python\client\session.py"", line 1340, in _extend_graph
    tf_session.ExtendSession(self._session)
tensorflow.python.framework.errors_impl.InvalidArgumentError: No OpKernel was re
gistered to support Op 'CudnnRNN' with these attrs.  Registered devices: [CPU],
Registered kernels:
  device='GPU'; T in [DT_HALF]
  device='GPU'; T in [DT_FLOAT]
  device='GPU'; T in [DT_DOUBLE]

         [[Node: bidirectional_1/CudnnRNN_1 = CudnnRNN[T=DT_DOUBLE, direction=""u
nidirectional"", dropout=0, input_mode=""linear_input"", is_training=true, rnn_mode
=""gru"", seed=87654321, seed2=0](bidirectional_1/transpose_2, bidirectional_1/Exp
andDims_3, bidirectional_1/Const_1, bidirectional_1/concat_1)]]

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""Z:\trader_connect.py"", line 157, in <module>
    tick()
  File ""Z:\trader_connect.py"", line 74, in tick
    model1 = keras.models.load_model('Z:\\Productionmodel.h5')
  File ""C:\Users\xion\Anaconda2\envs\tensorflow-gpu\lib\site-packages\keras\en
gine\saving.py"", line 264, in load_model
    load_weights_from_hdf5_group(f['model_weights'], model.layers)
  File ""C:\Users\xion\Anaconda2\envs\tensorflow-gpu\lib\site-packages\keras\en
gine\saving.py"", line 929, in load_weights_from_hdf5_group
    K.batch_set_value(weight_value_tuples)
  File ""C:\Users\xion\Anaconda2\envs\tensorflow-gpu\lib\site-packages\keras\ba
ckend\tensorflow_backend.py"", line 2435, in batch_set_value
    get_session().run(assign_ops, feed_dict=feed_dict)
  File ""C:\Users\xion\Anaconda2\envs\tensorflow-gpu\lib\site-packages\keras\ba
ckend\tensorflow_backend.py"", line 196, in get_session
    [tf.is_variable_initialized(v) for v in candidate_vars])
  File ""C:\Users\xion\Anaconda2\envs\tensorflow-gpu\lib\site-packages\tensorfl
ow\python\client\session.py"", line 900, in run
    run_metadata_ptr)
  File ""C:\Users\xion\Anaconda2\envs\tensorflow-gpu\lib\site-packages\tensorfl
ow\python\client\session.py"", line 1135, in _run
    feed_dict_tensor, options, run_metadata)
  File ""C:\Users\xion\Anaconda2\envs\tensorflow-gpu\lib\site-packages\tensorfl
ow\python\client\session.py"", line 1316, in _do_run
    run_metadata)
  File ""C:\Users\xion\Anaconda2\envs\tensorflow-gpu\lib\site-packages\tensorfl
ow\python\client\session.py"", line 1335, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.InvalidArgumentError: No OpKernel was re
gistered to support Op 'CudnnRNN' with these attrs.  Registered devices: [CPU],
Registered kernels:
  device='GPU'; T in [DT_HALF]
  device='GPU'; T in [DT_FLOAT]
  device='GPU'; T in [DT_DOUBLE]

         [[Node: bidirectional_1/CudnnRNN_1 = CudnnRNN[T=DT_DOUBLE, direction=""u
nidirectional"", dropout=0, input_mode=""linear_input"", is_training=true, rnn_mode
=""gru"", seed=87654321, seed2=0](bidirectional_1/transpose_2, bidirectional_1/Exp
andDims_3, bidirectional_1/Const_1, bidirectional_1/concat_1)]]

Caused by op 'bidirectional_1/CudnnRNN_1', defined at:
  File ""Z:\trader_connect.py"", line 157, in <module>
    tick()
  File ""Z:\trader_connect.py"", line 74, in tick
    model1 = keras.models.load_model('Z:\\Productionmodel.h5')
  File ""C:\Users\xion\Anaconda2\envs\tensorflow-gpu\lib\site-packages\keras\en
gine\saving.py"", line 261, in load_model
    model = model_from_config(model_config, custom_objects=custom_objects)
  File ""C:\Users\xion\Anaconda2\envs\tensorflow-gpu\lib\site-packages\keras\en
gine\saving.py"", line 335, in model_from_config
    return deserialize(config, custom_objects=custom_objects)
  File ""C:\Users\xion\Anaconda2\envs\tensorflow-gpu\lib\site-packages\keras\la
yers\__init__.py"", line 55, in deserialize
    printable_module_name='layer')
  File ""C:\Users\xion\Anaconda2\envs\tensorflow-gpu\lib\site-packages\keras\ut
ils\generic_utils.py"", line 145, in deserialize_keras_object
    list(custom_objects.items())))
  File ""C:\Users\xion\Anaconda2\envs\tensorflow-gpu\lib\site-packages\keras\en
gine\sequential.py"", line 293, in from_config
    model.add(layer)
  File ""C:\Users\xion\Anaconda2\envs\tensorflow-gpu\lib\site-packages\keras\en
gine\sequential.py"", line 166, in add
    layer(x)
  File ""C:\Users\xion\Anaconda2\envs\tensorflow-gpu\lib\site-packages\keras\la
yers\wrappers.py"", line 426, in __call__
    return super(Bidirectional, self).__call__(inputs, **kwargs)
  File ""C:\Users\xion\Anaconda2\envs\tensorflow-gpu\lib\site-packages\keras\en
gine\base_layer.py"", line 460, in __call__
    output = self.call(inputs, **kwargs)
  File ""C:\Users\xion\Anaconda2\envs\tensorflow-gpu\lib\site-packages\keras\la
yers\wrappers.py"", line 505, in call
    y_rev = self.backward_layer.call(inputs, **kwargs)
  File ""C:\Users\xion\Anaconda2\envs\tensorflow-gpu\lib\site-packages\keras\la
yers\cudnn_recurrent.py"", line 90, in call
    output, states = self._process_batch(inputs, initial_state)
  File ""C:\Users\xion\Anaconda2\envs\tensorflow-gpu\lib\site-packages\keras\la
yers\cudnn_recurrent.py"", line 297, in _process_batch
    is_training=True)
  File ""C:\Users\xion\Anaconda2\envs\tensorflow-gpu\lib\site-packages\tensorfl
ow\contrib\cudnn_rnn\python\ops\cudnn_rnn_ops.py"", line 1623, in __call__
    seed=self._seed)
  File ""C:\Users\xion\Anaconda2\envs\tensorflow-gpu\lib\site-packages\tensorfl
ow\contrib\cudnn_rnn\python\ops\cudnn_rnn_ops.py"", line 1012, in _cudnn_rnn_no_i
nput_c
    direction, dropout, seed, name)
  File ""C:\Users\xion\Anaconda2\envs\tensorflow-gpu\lib\site-packages\tensorfl
ow\contrib\cudnn_rnn\python\ops\cudnn_rnn_ops.py"", line 926, in _cudnn_rnn
    name=name)
  File ""C:\Users\xion\Anaconda2\envs\tensorflow-gpu\lib\site-packages\tensorfl
ow\python\ops\gen_cudnn_rnn_ops.py"", line 143, in cudnn_rnn
    is_training=is_training, name=name)
  File ""C:\Users\xion\Anaconda2\envs\tensorflow-gpu\lib\site-packages\tensorfl
ow\python\framework\op_def_library.py"", line 787, in _apply_op_helper
    op_def=op_def)
  File ""C:\Users\xion\Anaconda2\envs\tensorflow-gpu\lib\site-packages\tensorfl
ow\python\framework\ops.py"", line 3392, in create_op
    op_def=op_def)
  File ""C:\Users\xion\Anaconda2\envs\tensorflow-gpu\lib\site-packages\tensorfl
ow\python\framework\ops.py"", line 1718, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-
access

InvalidArgumentError (see above for traceback): No OpKernel was registered to su
pport Op 'CudnnRNN' with these attrs.  Registered devices: [CPU], Registered ker
nels:
  device='GPU'; T in [DT_HALF]
  device='GPU'; T in [DT_FLOAT]
  device='GPU'; T in [DT_DOUBLE]

         [[Node: bidirectional_1/CudnnRNN_1 = CudnnRNN[T=DT_DOUBLE, direction=""u
nidirectional"", dropout=0, input_mode=""linear_input"", is_training=true, rnn_mode
=""gru"", seed=87654321, seed2=0](bidirectional_1/transpose_2, bidirectional_1/Exp
andDims_3, bidirectional_1/Const_1, bidirectional_1/concat_1)]]








I've tried--fresh reinstall, change float

Any fixes? No idea why Im getting this. P.S: I trained the model on a titan v and I am trying to now open it on a computer with a GeForce GT 530 Gpu."
keras,12504,"Various blog posts, stackoverflow answers and tweets say like it's very easy to convert a Keras model to Tensorflow Keras model which can be used in Tensorflow TPU wrapper to get the model support TPU. But that's not very easy..

What about custom layers? How can I convert custom layers to get trained on TPUs? Or what about when I used Keras backend functions? Surely things are messing up and I am getting nothing but errors.

Do you have any plans on releasing Keras with TPU support? If yes, then can you give an estimated date till which users should wait?",0,Any way for Keras officially supporting Colab TPUs?,"Any way for Keras officially supporting Colab TPUs? Various blog posts, stackoverflow answers and tweets say like it's very easy to convert a Keras model to Tensorflow Keras model which can be used in Tensorflow TPU wrapper to get the model support TPU. But that's not very easy..

What about custom layers? How can I convert custom layers to get trained on TPUs? Or what about when I used Keras backend functions? Surely things are messing up and I am getting nothing but errors.

Do you have any plans on releasing Keras with TPU support? If yes, then can you give an estimated date till which users should wait?"
keras,12370,"Hi! Glad to see that Keras supported stateful metrics. I have been using multi-class average recall in most of my research projects in the past. I used to calculate them by callbacks, now I can finally add a metric.

I have posted my implementation at this [gist](https://gist.github.com/HawkinsZhao/766305acfb0141b70370e5dcd9415eb6). Also, I have found that Keras is going to update the basic metrics APIs #12149. How do you think about my work? And should I submit it as a PR at this time?

The multi-class average recall is a widely used metric in affective computing and many research problems. Besides, there is a lot of discussion about it #9393. I think it could certainly benefit lots of people.",0,Feature request: Multi-class average recall metric,"Feature request: Multi-class average recall metric Hi! Glad to see that Keras supported stateful metrics. I have been using multi-class average recall in most of my research projects in the past. I used to calculate them by callbacks, now I can finally add a metric.

I have posted my implementation at this [gist](https://gist.github.com/HawkinsZhao/766305acfb0141b70370e5dcd9415eb6). Also, I have found that Keras is going to update the basic metrics APIs #12149. How do you think about my work? And should I submit it as a PR at this time?

The multi-class average recall is a widely used metric in affective computing and many research problems. Besides, there is a lot of discussion about it #9393. I think it could certainly benefit lots of people."
keras,10074,"Please make sure that the boxes below are checked before you submit your issue. If your issue is an implementation question, please ask your question on [StackOverflow](http://stackoverflow.com/questions/tagged/keras) or [join the Keras Slack channel](https://keras-slack-autojoin.herokuapp.com/) and ask there instead of filing a GitHub issue.

Thank you!

- [ ] Check that you are up-to-date with the master branch of Keras. You can update with:
pip install git+git://github.com/keras-team/keras.git --upgrade --no-deps

- [X] If running on TensorFlow, check that you are up-to-date with the latest version. The installation instructions can be found [here](https://www.tensorflow.org/get_started/os_setup).

- [ ] If running on Theano, check that you are up-to-date with the master branch of Theano. You can update with:
pip install git+git://github.com/Theano/Theano.git --upgrade --no-deps

- [ ] Provide a link to a GitHub Gist of a Python script that can reproduce your issue (or just copy the script here if it is short).

Trying to train a model that is composed of submodels, and using the Tesnorboard callback.
If histogram_freq is set, there's an error about missing input to the submodel

using tensorflow 1.8 (same error also in older versions)

sample code that reproduces the error:



error:



The reason of using submodels is the need to also run them separately ",0,'You must feed a value for placeholder tensor' when using Tensorboard callback with submodels,"'You must feed a value for placeholder tensor' when using Tensorboard callback with submodels Please make sure that the boxes below are checked before you submit your issue. If your issue is an implementation question, please ask your question on [StackOverflow](http://stackoverflow.com/questions/tagged/keras) or [join the Keras Slack channel](https://keras-slack-autojoin.herokuapp.com/) and ask there instead of filing a GitHub issue.

Thank you!

- [ ] Check that you are up-to-date with the master branch of Keras. You can update with:
pip install git+git://github.com/keras-team/keras.git --upgrade --no-deps

- [X] If running on TensorFlow, check that you are up-to-date with the latest version. The installation instructions can be found [here](https://www.tensorflow.org/get_started/os_setup).

- [ ] If running on Theano, check that you are up-to-date with the master branch of Theano. You can update with:
pip install git+git://github.com/Theano/Theano.git --upgrade --no-deps

- [ ] Provide a link to a GitHub Gist of a Python script that can reproduce your issue (or just copy the script here if it is short).

Trying to train a model that is composed of submodels, and using the Tesnorboard callback.
If histogram_freq is set, there's an error about missing input to the submodel

using tensorflow 1.8 (same error also in older versions)

sample code that reproduces the error:



error:



The reason of using submodels is the need to also run them separately "
keras,9455,"I'm not completely confident in the issue title, but I find it plausible and would love it if someone from the keras community helped verify/fix the issue.

Here's the steps to reproduce:
1. Build any model with one or more batch norm layers.  In my case it was a functional API model, although I doubt that is necessary to reproduce.
2. run model.predict() (wrap it in a timer of some sort)
3. At normalization.py:174 (as of master) add 
4. run model.predict()
5. observe that the model runs significantly faster.  For my model I get about a 3X speed increase.

",0,BatchNormalization calculates batch mean and variance even in inference mode,"BatchNormalization calculates batch mean and variance even in inference mode I'm not completely confident in the issue title, but I find it plausible and would love it if someone from the keras community helped verify/fix the issue.

Here's the steps to reproduce:
1. Build any model with one or more batch norm layers.  In my case it was a functional API model, although I doubt that is necessary to reproduce.
2. run model.predict() (wrap it in a timer of some sort)
3. At normalization.py:174 (as of master) add 
4. run model.predict()
5. observe that the model runs significantly faster.  For my model I get about a 3X speed increase.

"
keras,11703,"There seems to be an issue with model.fit in relation to loading previous weights.
The problems basically as seen in the logging  :
 
Epoch 00001: val_acc improved from -inf to 0.95089, saving model to \Chicken-weights.best.hdf5

Every fit starts with ""-inf"",  negative infinitive thus ignoring the last loaded weights results from previous runs. And so that will overwrite any best trainings from previous runs. if epoch 1 is worse it would still overwrite past saved results. due to ""-inf"", fit cannot improve from past runs.








",0,train.fit epoch1 uses -inf,"train.fit epoch1 uses -inf There seems to be an issue with model.fit in relation to loading previous weights.
The problems basically as seen in the logging  :
 
Epoch 00001: val_acc improved from -inf to 0.95089, saving model to \Chicken-weights.best.hdf5

Every fit starts with ""-inf"",  negative infinitive thus ignoring the last loaded weights results from previous runs. And so that will overwrite any best trainings from previous runs. if epoch 1 is worse it would still overwrite past saved results. due to ""-inf"", fit cannot improve from past runs.








"
keras,12956,"### Configuration

| Library                        | Version  |
| ------------------------------ | -------- |
| pyhton                         | 3.6.8    |
| GCC                            | 7.3.0    |
| tensorflow-base/tensorflow-gpu | 1.13.1   |
| keras-gpu/keras-base           | 2.2.4    |
| theano                         | 1.0.4    |
| cudnn                          | 7.6.0    |
| cudatoolkit                    | 10.0.130 |

Machine Configuration：

Ubuntu 18.04.2 LTS

Intel(R) Xeon(R) CPU E5-2640 v4 @ 2.40GHz 128GB RAM

CUDA Version: 10.2 

Double GeForce GTX 1080ti 

---

### Description

I use  to clone the resnet50 model and use   to set the weights for each layer. But I find that the time consumption for each iteration becomes much larger as time goes by. At the first iteration, the clone stage takes about 4 seconds and the setting-weights stage takes about 3 seconds but those turn to be 34 seconds and 4 minutes at the 50th iteration. 

Is there a problem with my usage? Or would there be a memory leak in keras?

I hope to hear from you soon. Thanks in advance.

The code is shown as bellow:



Here is partial result:

![image](https://user-images.githubusercontent.com/17698785/59478187-019a5280-8e8b-11e9-8901-3e01a0b9daa4.png)

![image](https://user-images.githubusercontent.com/17698785/59478193-08c16080-8e8b-11e9-9e83-a595c0fe2e35.png)
",0,Memory leak in Keras: Functions Clone_model() and Set_Weights() become slower and slower when used in loop,"Memory leak in Keras: Functions Clone_model() and Set_Weights() become slower and slower when used in loop ### Configuration

| Library                        | Version  |
| ------------------------------ | -------- |
| pyhton                         | 3.6.8    |
| GCC                            | 7.3.0    |
| tensorflow-base/tensorflow-gpu | 1.13.1   |
| keras-gpu/keras-base           | 2.2.4    |
| theano                         | 1.0.4    |
| cudnn                          | 7.6.0    |
| cudatoolkit                    | 10.0.130 |

Machine Configuration：

Ubuntu 18.04.2 LTS

Intel(R) Xeon(R) CPU E5-2640 v4 @ 2.40GHz 128GB RAM

CUDA Version: 10.2 

Double GeForce GTX 1080ti 

---

### Description

I use  to clone the resnet50 model and use   to set the weights for each layer. But I find that the time consumption for each iteration becomes much larger as time goes by. At the first iteration, the clone stage takes about 4 seconds and the setting-weights stage takes about 3 seconds but those turn to be 34 seconds and 4 minutes at the 50th iteration. 

Is there a problem with my usage? Or would there be a memory leak in keras?

I hope to hear from you soon. Thanks in advance.

The code is shown as bellow:



Here is partial result:

![image](https://user-images.githubusercontent.com/17698785/59478187-019a5280-8e8b-11e9-8901-3e01a0b9daa4.png)

![image](https://user-images.githubusercontent.com/17698785/59478193-08c16080-8e8b-11e9-9e83-a595c0fe2e35.png)
"
keras,10882,"
this is my code


package versions : 

- Keras==2.2.0
- Keras-Applications==1.0.2
- Keras-Preprocessing==1.0.1
- tensorflow-gpu==1.9.0",0,"""UnboundLocalError: local variable 'x' referenced before assignment"" while using load_model","""UnboundLocalError: local variable 'x' referenced before assignment"" while using load_model 
this is my code


package versions : 

- Keras==2.2.0
- Keras-Applications==1.0.2
- Keras-Preprocessing==1.0.1
- tensorflow-gpu==1.9.0"
keras,5014,"There is my model:


 I fitted the model.However when I load it from a .h5 file,i got an error.


I guess it may caused by the complicacy of my model?",0,Error loading a saved model?,"Error loading a saved model? There is my model:


 I fitted the model.However when I load it from a .h5 file,i got an error.


I guess it may caused by the complicacy of my model?"
keras,8423,"Here is the summary of the model in imdb_cnn sample, can anyone tell me what is the purpose of dense_6  layer?

embedding_6 (Embedding)      (None, 400, 50)           250000    
dropout_5 (Dropout)          (None, 400, 50)           0         
conv1d_4 (Conv1D)            (None, 398, 250)          37750     
global_max_pooling1d_5 (Glob (None, 250)               0         
**dense_6 (Dense)              (None, 250)               62750**     
dropout_6 (Dropout)          (None, 250)               0         
activation_5 (Activation)    (None, 250)               0         
dense_7 (Dense)              (None, 1)                 251       
activation_6 (Activation)    (None, 1)                 0         

Total params: 350,751
Trainable params: 350,751
Non-trainable params: 0
",0,Use of Dense layer in imdb_cnn sample,"Use of Dense layer in imdb_cnn sample Here is the summary of the model in imdb_cnn sample, can anyone tell me what is the purpose of dense_6  layer?

embedding_6 (Embedding)      (None, 400, 50)           250000    
dropout_5 (Dropout)          (None, 400, 50)           0         
conv1d_4 (Conv1D)            (None, 398, 250)          37750     
global_max_pooling1d_5 (Glob (None, 250)               0         
**dense_6 (Dense)              (None, 250)               62750**     
dropout_6 (Dropout)          (None, 250)               0         
activation_5 (Activation)    (None, 250)               0         
dense_7 (Dense)              (None, 1)                 251       
activation_6 (Activation)    (None, 1)                 0         

Total params: 350,751
Trainable params: 350,751
Non-trainable params: 0
"
keras,11493,"Hi,

This is a feature request that I think could be interesting for several users and I wish to share it, as I am not able to implement it.
In the field of scientific computing (especially Computational Fluid Dynamic), there are more and more attempts to use neural network to speed up expensive iterative computations. It can be done in two ways:

- by improving the quality of the initialisation the computation with a neural network

- for computation based on Jacobian evaluation (such as Newton-Raphson), by estimating the Jacobian with a neural network.

In such cases, it is quite common to have access to the derivatives of the function one want to learn, therefore another kind of training can be used : Sobolev training : https://arxiv.org/abs/1706.04859

In such a training, the cost function is not only dependant of the output values of the neural network and the ground truth, but a term depending of the derivatives of the outputs with regards to the inputs is added. This requires to provide also the ground truth derivatives to the learning process.

It would be great to have to possibility to provide the derivatives values in Keras with a call like :



 being the matrices of the derivatives of the outputs with regards to the inputs.

The quality of the training is improved, as in addition of the knowledge of the output value, one use also the available knowledge of the derivative of the outputs with regards to the inputs. This would require to compute also for the learning process the derivative of the output of the network with regards to its inputs, in order to be able to do the gradient based update.

Once the learning process is achieved, both outputs values and derivatives values would have been learn in a single neural networks and it accessing to the values of the derivatives would be adding value to the information gathered by such a training, that means being able to call:



Best regards,",0,Introduction of Sobolev training capability in Keras,"Introduction of Sobolev training capability in Keras Hi,

This is a feature request that I think could be interesting for several users and I wish to share it, as I am not able to implement it.
In the field of scientific computing (especially Computational Fluid Dynamic), there are more and more attempts to use neural network to speed up expensive iterative computations. It can be done in two ways:

- by improving the quality of the initialisation the computation with a neural network

- for computation based on Jacobian evaluation (such as Newton-Raphson), by estimating the Jacobian with a neural network.

In such cases, it is quite common to have access to the derivatives of the function one want to learn, therefore another kind of training can be used : Sobolev training : https://arxiv.org/abs/1706.04859

In such a training, the cost function is not only dependant of the output values of the neural network and the ground truth, but a term depending of the derivatives of the outputs with regards to the inputs is added. This requires to provide also the ground truth derivatives to the learning process.

It would be great to have to possibility to provide the derivatives values in Keras with a call like :



 being the matrices of the derivatives of the outputs with regards to the inputs.

The quality of the training is improved, as in addition of the knowledge of the output value, one use also the available knowledge of the derivative of the outputs with regards to the inputs. This would require to compute also for the learning process the derivative of the output of the network with regards to its inputs, in order to be able to do the gradient based update.

Once the learning process is achieved, both outputs values and derivatives values would have been learn in a single neural networks and it accessing to the values of the derivatives would be adding value to the information gathered by such a training, that means being able to call:



Best regards,"
keras,13098,"1. Problem description

When I design input layers via assigning tensors including constants, variables and etc. to avoid direct input, in not reloading the model, training and predicting are valid. However, after saving the model and reloading it, input layers via assigning tensors are loaded as direct input layers without constant and variable tensors. I want to load an originally designed model with designing loss.

2. Code example

input1 = Input(shape=(10, 1))
input2 = Input(tensor=K.random_normal_variable((10, 1), 0, 1))
input3 = Input(tensor=K.random_normal((10, 1)))

x = Lambda(lambda x: x[0] + x[1] + x[2])([input1, input2, input3])
model = Model(inputs=[input1, input2, input3], outputs=[x])

model.inputs
>> [<tf.Tensor 'input_45:0' shape=(?, 10, 1) dtype=float32>,
 <tf.Variable 'Variable_40:0' shape=(10, 1) dtype=float32_ref>,
 <tf.Tensor 'random_normal_22:0' shape=(10, 1) dtype=float32>]

model.save('example_model.h5')
r_model = load_model('example_model.h5')
r_model.inputs
>>[<tf.Tensor 'input_45_1:0' shape=(?, 10, 1) dtype=float32>,
 <tf.Tensor 'input_46:0' shape=(10, 1) dtype=float32>,
 <tf.Tensor 'input_47:0' shape=(10, 1) dtype=float32>]",0,About the reloading problem of input layers assigned by tensor,"About the reloading problem of input layers assigned by tensor 1. Problem description

When I design input layers via assigning tensors including constants, variables and etc. to avoid direct input, in not reloading the model, training and predicting are valid. However, after saving the model and reloading it, input layers via assigning tensors are loaded as direct input layers without constant and variable tensors. I want to load an originally designed model with designing loss.

2. Code example

input1 = Input(shape=(10, 1))
input2 = Input(tensor=K.random_normal_variable((10, 1), 0, 1))
input3 = Input(tensor=K.random_normal((10, 1)))

x = Lambda(lambda x: x[0] + x[1] + x[2])([input1, input2, input3])
model = Model(inputs=[input1, input2, input3], outputs=[x])

model.inputs
>> [<tf.Tensor 'input_45:0' shape=(?, 10, 1) dtype=float32>,
 <tf.Variable 'Variable_40:0' shape=(10, 1) dtype=float32_ref>,
 <tf.Tensor 'random_normal_22:0' shape=(10, 1) dtype=float32>]

model.save('example_model.h5')
r_model = load_model('example_model.h5')
r_model.inputs
>>[<tf.Tensor 'input_45_1:0' shape=(?, 10, 1) dtype=float32>,
 <tf.Tensor 'input_46:0' shape=(10, 1) dtype=float32>,
 <tf.Tensor 'input_47:0' shape=(10, 1) dtype=float32>]"
keras,10387,"Please make sure that the boxes below are checked before you submit your issue. If your issue is an implementation question, please ask your question on [StackOverflow](http://stackoverflow.com/questions/tagged/keras) or [join the Keras Slack channel](https://keras-slack-autojoin.herokuapp.com/) and ask there instead of filing a GitHub issue.

Thank you!

- [x] Check that you are up-to-date with the master branch of Keras. You can update with:
pip install git+git://github.com/keras-team/keras.git --upgrade --no-deps

- [x] If running on TensorFlow, check that you are up-to-date with the latest version. The installation instructions can be found [here](https://www.tensorflow.org/get_started/os_setup).

- [ ] If running on Theano, check that you are up-to-date with the master branch of Theano. You can update with:
pip install git+git://github.com/Theano/Theano.git --upgrade --no-deps

- [x] Provide a link to a GitHub Gist of a Python script that can reproduce your issue (or just copy the script here if it is short).

Hello. I have a problem with memory error.

I wrote next script



After that, if I set  any value for steps_per_epoch and validation_steps, I get messages next type:

> 2018-06-09 23:59:16.234583: W tensorflow/core/framework/allocator.cc:101] Allocation of 1507688448 exceeds 10% of system memory.

I have not found any information about this problem. Please, help me.

",0,validation st,"validation st Please make sure that the boxes below are checked before you submit your issue. If your issue is an implementation question, please ask your question on [StackOverflow](http://stackoverflow.com/questions/tagged/keras) or [join the Keras Slack channel](https://keras-slack-autojoin.herokuapp.com/) and ask there instead of filing a GitHub issue.

Thank you!

- [x] Check that you are up-to-date with the master branch of Keras. You can update with:
pip install git+git://github.com/keras-team/keras.git --upgrade --no-deps

- [x] If running on TensorFlow, check that you are up-to-date with the latest version. The installation instructions can be found [here](https://www.tensorflow.org/get_started/os_setup).

- [ ] If running on Theano, check that you are up-to-date with the master branch of Theano. You can update with:
pip install git+git://github.com/Theano/Theano.git --upgrade --no-deps

- [x] Provide a link to a GitHub Gist of a Python script that can reproduce your issue (or just copy the script here if it is short).

Hello. I have a problem with memory error.

I wrote next script



After that, if I set  any value for steps_per_epoch and validation_steps, I get messages next type:

> 2018-06-09 23:59:16.234583: W tensorflow/core/framework/allocator.cc:101] Allocation of 1507688448 exceeds 10% of system memory.

I have not found any information about this problem. Please, help me.

"
keras,12269,"Tensorflow version = 1.5
keras version = 2.2.4
Everything is OK when I train the model on GPU machine, but something happens when training or predicting model on CPU machine.
It seems that Embedding Layser cannot recognize the masking value -1.
Here are my codes:

And I preprocess the list of word intergers with padding and filling with -1.


Here is the error:


Once again, everything is fine on my GPU machine. So what's wrong with the same code running on the CPU machine ? ",0,Masking layer conflict with Embedding layer,"Masking layer conflict with Embedding layer Tensorflow version = 1.5
keras version = 2.2.4
Everything is OK when I train the model on GPU machine, but something happens when training or predicting model on CPU machine.
It seems that Embedding Layser cannot recognize the masking value -1.
Here are my codes:

And I preprocess the list of word intergers with padding and filling with -1.


Here is the error:


Once again, everything is fine on my GPU machine. So what's wrong with the same code running on the CPU machine ? "
keras,12524,"Hello, 

I would like to create either a) a deep dense neural net that has n outputs with no activation function (for example, 3 numbers output) or b) an RNN that internally performs the same based on observed effects. So, I want to say, feed it stationary values of X=1 Y=1 but reconsider what the loss value is for the three inputs, in order to train to optimise my function

My issue is that I need to transform the output before the loss is given, eg:


Where: 

- Loss is the _true loss_ of the network
- my_function accepts the three outputs of the network at each step (a, b and c), rounded to integers
- my_function returns a float between 0.0 and 100.0 where 0.0 is the worst and 100.0 is the best possible value

Is it possible to get the network to learn from the above my_function as true loss? I've been trying for days to implement it. I can return the value of my_function() from my code, but when I input this code inside a custom loss function it just executes once at the start (TF Graphs).

Is there any way I can replace my loss with the above function using a Lambda layer, custom loss function or even a callback? I'm not sure how to do this, or if it's even possible (?)

This question seems to be aiming for a somewhat similar goal: https://github.com/keras-team/keras/issues/2691
but unfortunately I'm not quite good enough at Keras (yet!) to apply it to my problem

Help would be much appreciated,
Thanks

PS: I know a genetic or PSO would probably be better but my project is to show the effects of each method with results on multiple problems",0,Neural Networks with no input (dummy) to optimise transformed output?,"Neural Networks with no input (dummy) to optimise transformed output? Hello, 

I would like to create either a) a deep dense neural net that has n outputs with no activation function (for example, 3 numbers output) or b) an RNN that internally performs the same based on observed effects. So, I want to say, feed it stationary values of X=1 Y=1 but reconsider what the loss value is for the three inputs, in order to train to optimise my function

My issue is that I need to transform the output before the loss is given, eg:


Where: 

- Loss is the _true loss_ of the network
- my_function accepts the three outputs of the network at each step (a, b and c), rounded to integers
- my_function returns a float between 0.0 and 100.0 where 0.0 is the worst and 100.0 is the best possible value

Is it possible to get the network to learn from the above my_function as true loss? I've been trying for days to implement it. I can return the value of my_function() from my code, but when I input this code inside a custom loss function it just executes once at the start (TF Graphs).

Is there any way I can replace my loss with the above function using a Lambda layer, custom loss function or even a callback? I'm not sure how to do this, or if it's even possible (?)

This question seems to be aiming for a somewhat similar goal: https://github.com/keras-team/keras/issues/2691
but unfortunately I'm not quite good enough at Keras (yet!) to apply it to my problem

Help would be much appreciated,
Thanks

PS: I know a genetic or PSO would probably be better but my project is to show the effects of each method with results on multiple problems"
keras,13637,"the  parameter works on  but not ?

testing code:


And below is my result on Keras==2.3.1 and Keras-Preprocessing==1.1.0:
![image](https://user-images.githubusercontent.com/16003088/70897626-f8eb2a80-202d-11ea-9898-1320dcad52a1.png)



",0,"""classes"" not working on flow_from_dataframe?","""classes"" not working on flow_from_dataframe? the  parameter works on  but not ?

testing code:


And below is my result on Keras==2.3.1 and Keras-Preprocessing==1.1.0:
![image](https://user-images.githubusercontent.com/16003088/70897626-f8eb2a80-202d-11ea-9898-1320dcad52a1.png)



"
keras,8315,"Hey there,

i converted my MLP Network to an LSTM Network to check if i can get a better accuracy taking
the time into consideration. I have sound signals that are split into multiple frames (87 Frames each sound signal) and 39 Features generated from those Frames (Mel Frequency Cepstrum Coefficients).
In my MLP Network i had a input dimension of 39 Feautres and in the LSTM Network i had to reshape
this vector to 3D Tensor and decided to have those 87 frames as timesteps. (It might make more sense to put this value higher).

So from  i reshaped the vector to  .
my output vector  also had to be reshaped from  to 
and i had to add the  Wrapper around my  output layer because
of the many-to-many (categories) nature of my classification problem.

here is the split and reshape part:

code
the creation of the model:


 depending
on the timesteps the dimensionalty of the 3D Tensor changes.

the keras function to categorical changes my 3D tensor back to 2D Tensor, so i have to reshape again.
Is there another way how tho accomplish this?

Greetings
",0,Categorical time sequence LSTM,"Categorical time sequence LSTM Hey there,

i converted my MLP Network to an LSTM Network to check if i can get a better accuracy taking
the time into consideration. I have sound signals that are split into multiple frames (87 Frames each sound signal) and 39 Features generated from those Frames (Mel Frequency Cepstrum Coefficients).
In my MLP Network i had a input dimension of 39 Feautres and in the LSTM Network i had to reshape
this vector to 3D Tensor and decided to have those 87 frames as timesteps. (It might make more sense to put this value higher).

So from  i reshaped the vector to  .
my output vector  also had to be reshaped from  to 
and i had to add the  Wrapper around my  output layer because
of the many-to-many (categories) nature of my classification problem.

here is the split and reshape part:

code
the creation of the model:


 depending
on the timesteps the dimensionalty of the 3D Tensor changes.

the keras function to categorical changes my 3D tensor back to 2D Tensor, so i have to reshape again.
Is there another way how tho accomplish this?

Greetings
"
keras,9622,"The official document of resnet50 describes that it needs the size of picture must be 197*197 at least. However, every row of my data talks about a sample and the feature of it is 737. Then, how can I use the model of resnet50 in my object? Thank you ~",0,questions about resnet50,"questions about resnet50 The official document of resnet50 describes that it needs the size of picture must be 197*197 at least. However, every row of my data talks about a sample and the feature of it is 737. Then, how can I use the model of resnet50 in my object? Thank you ~"
keras,11097,"Hi all,

in the last weeks I was successful in setting up a Convolutional LSTM using the layers provided by keras (https://github.com/keras-team/keras/blob/master/keras/layers/convolutional_recurrent.py).

For another task in the past I did some experiments with Phased LSTM (https://arxiv.org/abs/1610.09513), which also has been implemented in keras and Tensorflow.

Now, I have these things on my mind:

1. The Phased LSTM model seems really useful for many purposes. Yet, it didn't get really popular over the last years, and I'm wondering about potential reasons. Do I miss any strong counter-arguments, or have there been successful alternative approaches to tackle the same issues (like input from different sources and irregular sampling intervals)?

2. In practice, these two methods should be compatible, right? A ""time gate"" could also be included in an LSTM-CNN setting, analogous to its application in a classical LSTM. Yet, since I couldn't find any implementation, I wonder if there might be any hurdles I'm not aware of.

3. If the first points do not disqualify the idea already, feel free to get in contact with me if you would like to cooperate on its implementation.

Thanks!
",0,Usefulness of Phased LSTM-CNN,"Usefulness of Phased LSTM-CNN Hi all,

in the last weeks I was successful in setting up a Convolutional LSTM using the layers provided by keras (https://github.com/keras-team/keras/blob/master/keras/layers/convolutional_recurrent.py).

For another task in the past I did some experiments with Phased LSTM (https://arxiv.org/abs/1610.09513), which also has been implemented in keras and Tensorflow.

Now, I have these things on my mind:

1. The Phased LSTM model seems really useful for many purposes. Yet, it didn't get really popular over the last years, and I'm wondering about potential reasons. Do I miss any strong counter-arguments, or have there been successful alternative approaches to tackle the same issues (like input from different sources and irregular sampling intervals)?

2. In practice, these two methods should be compatible, right? A ""time gate"" could also be included in an LSTM-CNN setting, analogous to its application in a classical LSTM. Yet, since I couldn't find any implementation, I wonder if there might be any hurdles I'm not aware of.

3. If the first points do not disqualify the idea already, feel free to get in contact with me if you would like to cooperate on its implementation.

Thanks!
"
keras,10496,"I have anaconda 2 with and a created  environment. The python2.7 works with  and , while the  works with  and . They both share the sameJason file. Everytime when I use  with either  or  backend, I needs to modify the Jason file accordingly. 

I am wondering if there are simpler ways to set up  file (e.g., create two different jason files) to deal with such the situation?",0,How to set the keras jason file for theano and tensorflow backends respectively,"How to set the keras jason file for theano and tensorflow backends respectively I have anaconda 2 with and a created  environment. The python2.7 works with  and , while the  works with  and . They both share the sameJason file. Everytime when I use  with either  or  backend, I needs to modify the Jason file accordingly. 

I am wondering if there are simpler ways to set up  file (e.g., create two different jason files) to deal with such the situation?"
keras,8224,"following  https://github.com/fchollet/keras/issues/6142, I defined a model with initial_state as a parameter. The model can be trained successfully.
But when I try to evaluate the model and load model definition form json file, the following error has appeared.


code snippet of defining model:


code snippet of loading model definiation from json file:



Thanks in advance for your kind help.
",0,A bug in loading model when using GRU with initial_state?,"A bug in loading model when using GRU with initial_state? following  https://github.com/fchollet/keras/issues/6142, I defined a model with initial_state as a parameter. The model can be trained successfully.
But when I try to evaluate the model and load model definition form json file, the following error has appeared.


code snippet of defining model:


code snippet of loading model definiation from json file:



Thanks in advance for your kind help.
"
keras,12785,"I'm currently working on an LSTM based research problem. However, while using the RNNs in keras, as I will demonstrate below, I'm running into the above-mentioned error.

I use TF version 1.12.0 and keras 2.2.4.

This seems to work with cells like LSTMCell but fails to work with UGRNNCell.. No idea how to fix this issue.


And this is my model:


When run this is the error obtained:



This results in the above error but works seamlessly and without any error when the UGRNNCell is replaced by an LSTMCell.
Any help would be appreciated.
",0,"ValueError: Variable kernel already exists, disallowed. Did you mean to set reuse=True or reuse=tf.AUTO_REUSE in VarScope?","ValueError: Variable kernel already exists, disallowed. Did you mean to set reuse=True or reuse=tf.AUTO_REUSE in VarScope? I'm currently working on an LSTM based research problem. However, while using the RNNs in keras, as I will demonstrate below, I'm running into the above-mentioned error.

I use TF version 1.12.0 and keras 2.2.4.

This seems to work with cells like LSTMCell but fails to work with UGRNNCell.. No idea how to fix this issue.


And this is my model:


When run this is the error obtained:



This results in the above error but works seamlessly and without any error when the UGRNNCell is replaced by an LSTMCell.
Any help would be appreciated.
"
keras,8562,"So just a set up of the problem I am trying to solve. I have around 200k 64x64x3 RGB images of patches of terrain that a robot drove over. Each patch has a corresponding label of what the roughness of that image patch is. The roughness values range from 0-160. The data was collected with the robot driving at varying speeds, hence the range of the roughness values. My aim is to be able to predict the roughness of a patch. I am using the VGG-16 network, with the last layer modified to do regression. My batch size is 1024, the loss is mean sqaured error, the optimize is rmsprop. The network is shown below. My problem is that after training, the network predicts the exact same value for each test image. Another point to note is that the training loss is always higher than the validation loss which is odd. Lastly I tried with other optimizers such as SGD and Adam, as well as varying batch sizes. Right now I am trying to train the network from scratch but it does not seem too promising. I am not sure what is going wrong here, and I would really appreciate any help I can get. Thanks




    if input_tensor is None:
        img_input = Input(shape=input_shape)
    else:
        if not K.is_keras_tensor(input_tensor):
            img_input = Input(tensor=input_tensor, shape=input_shape)
        else:
            img_input = input_tensor
    # Block 1
    x = Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv1')(img_input)
    x = Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv2')(x)
    x = MaxPooling2D((2, 2), strides=(2, 2), name='block1_pool')(x)

    # Block 2
    x = Conv2D(128, (3, 3), activation='relu', padding='same', name='block2_conv1')(x)
    x = Conv2D(128, (3, 3), activation='relu', padding='same', name='block2_conv2')(x)
    x = MaxPooling2D((2, 2), strides=(2, 2), name='block2_pool')(x)

    # Block 3
    x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv1')(x)
    x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv2')(x)
    x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv3')(x)
    x = MaxPooling2D((2, 2), strides=(2, 2), name='block3_pool')(x)

    # Block 4
    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv1')(x)
    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv2')(x)
    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv3')(x)
    x = MaxPooling2D((2, 2), strides=(2, 2), name='block4_pool')(x)

    # Block 5
    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv1')(x)
    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv2')(x)
    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv3')(x)
    x = MaxPooling2D((2, 2), strides=(2, 2), name='block5_pool')(x)

    x = Flatten(name='flatten')(x)
    x = Dense(4096, activation='relu', name='fc1')(x)
    x = Dense(4096, activation='relu', name='fc2')(x)
    x = Dense(1,name='regression_dense')(x)  ` ",0,CNN Regression with VGG-16 predicts the same value regardless of the input image. ,"CNN Regression with VGG-16 predicts the same value regardless of the input image.  So just a set up of the problem I am trying to solve. I have around 200k 64x64x3 RGB images of patches of terrain that a robot drove over. Each patch has a corresponding label of what the roughness of that image patch is. The roughness values range from 0-160. The data was collected with the robot driving at varying speeds, hence the range of the roughness values. My aim is to be able to predict the roughness of a patch. I am using the VGG-16 network, with the last layer modified to do regression. My batch size is 1024, the loss is mean sqaured error, the optimize is rmsprop. The network is shown below. My problem is that after training, the network predicts the exact same value for each test image. Another point to note is that the training loss is always higher than the validation loss which is odd. Lastly I tried with other optimizers such as SGD and Adam, as well as varying batch sizes. Right now I am trying to train the network from scratch but it does not seem too promising. I am not sure what is going wrong here, and I would really appreciate any help I can get. Thanks




    if input_tensor is None:
        img_input = Input(shape=input_shape)
    else:
        if not K.is_keras_tensor(input_tensor):
            img_input = Input(tensor=input_tensor, shape=input_shape)
        else:
            img_input = input_tensor
    # Block 1
    x = Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv1')(img_input)
    x = Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv2')(x)
    x = MaxPooling2D((2, 2), strides=(2, 2), name='block1_pool')(x)

    # Block 2
    x = Conv2D(128, (3, 3), activation='relu', padding='same', name='block2_conv1')(x)
    x = Conv2D(128, (3, 3), activation='relu', padding='same', name='block2_conv2')(x)
    x = MaxPooling2D((2, 2), strides=(2, 2), name='block2_pool')(x)

    # Block 3
    x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv1')(x)
    x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv2')(x)
    x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv3')(x)
    x = MaxPooling2D((2, 2), strides=(2, 2), name='block3_pool')(x)

    # Block 4
    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv1')(x)
    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv2')(x)
    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv3')(x)
    x = MaxPooling2D((2, 2), strides=(2, 2), name='block4_pool')(x)

    # Block 5
    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv1')(x)
    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv2')(x)
    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv3')(x)
    x = MaxPooling2D((2, 2), strides=(2, 2), name='block5_pool')(x)

    x = Flatten(name='flatten')(x)
    x = Dense(4096, activation='relu', name='fc1')(x)
    x = Dense(4096, activation='relu', name='fc2')(x)
    x = Dense(1,name='regression_dense')(x)  ` "
keras,13492,"**System information**  
- Have I written custom code (as opposed to using example directory):  no
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Windows 10
- TensorFlow backend (yes / no):  yes
- TensorFlow version:  2.0.0
- Keras version:  2.2.4-tf
- Python version: 3.6
- CUDA/cuDNN version:  10.1
- GPU model and memory:  Titan GTX 12Gb

**Describe the current behavior** 
//layer_type is in this case BatchNorm+ELU+Conv2D
 def dens_block(layer_num, layer_type,**kwargs):
	conc_layer = []
	def block(x):
		conc_layer.append(x)
		for i in range(layer_num):
		     x = layer_type(32, 2, strides=(1, 1), padding='same', **kwargs)(x)
		     conc_layer.append(x)
                     x = tf.keras.layers.Concatenate(axis=axis)(**list**(conc_layer))
		return x
	return block

**Describe the expected behavior**  
without the **list** in  tf.keras.layers.Concatenate the code not work

**Code to reproduce the issue**  
import tensorflow as tf
import netron


def bn_elu_conv(filters, kernel_size, strides=(1, 1), padding='valid', **kwargs):
	def layer(input_tensor):
		with tf.keras.backend.name_scope('bn_elu_conv'):
			x = tf.keras.layers.BatchNormalization()(input_tensor)
			x = tf.keras.layers.ELU()(x)
			x = tf.keras.layers.Conv2D(filters, kernel_size, strides=strides, padding=padding, **kwargs)(x)
		return x

	return layer


def conv_bn_elu(filters, kernel_size, strides=(1, 1), padding='valid', **kwargs):
	def layer(input_tensor):
		with tf.keras.backend.name_scope('conv_bn_elu'):
			x = tf.keras.layers.Conv2D(filters, kernel_size, strides=strides, padding=padding, **kwargs)(input_tensor)
			x = tf.keras.layers.BatchNormalization()(x)
			x = tf.keras.layers.ELU()(x)
		return x

	return layer


def dens_block(layer_num, filters, kernels_sizes, strides=(1, 1), padding='same', layer_type='bn_elu_conv', axis=-1, **kwargs):
	layer_type = _get_layerTyp(layer_type)
	filters = _check_list(filters, layer_num)
	kernels_sizes = _check_list(kernels_sizes, layer_num)
	conc_layer = []

	def block(x):
		conc_layer.append(x)
		for i in range(layer_num):
			x = layer_type(filters[i], kernels_sizes[i], strides=strides, padding=padding, **kwargs)(x)
			conc_layer.append(x)
			x = tf.keras.layers.Concatenate(axis=axis)(conc_layer)
			# x = tf.keras.layers.Concatenate(axis=axis)(list(conc_layer) # is working

		return x

	return block


def res_block(layer_num, filters, kernels_sizes, strides=(1, 1), padding='same', layer_type='bn_elu_conv', conv_block=False, **kwargs):
	layer_type = _get_layerTyp(layer_type)
	filters = _check_list(filters, layer_num)
	kernels_sizes = _check_list(kernels_sizes, layer_num)

	def block(x):
		input_tensor = x
		for i in range(layer_num):
			x = layer_type(filters[i], kernels_sizes[i], strides=strides, padding=padding, **kwargs)(x)
		if conv_block:
			input_tensor = layer_type(filters[-1], kernels_sizes[-1], strides=strides, padding=padding, )(input_tensor)
		x = tf.keras.layers.Add()([x, input_tensor])
		return x

	return block


def _get_layerTyp(layer_typ):
	if layer_typ is 'bn_elu_conv':
		return bn_elu_conv
	if layer_typ is 'conv_bn_elu':
		return conv_bn_elu
	else:
		raise ValueError('layer_typ have to be \'bn_elu_conv\' or \'conv_bn_elu\'')


def _check_list(x, list_len):
	if isinstance(x, list):
		if len(x) is list_len:
			return x
		else:
			raise TypeError('x have to be the same length as list_len')
	else:
		return _to_list(x, list_len)


def _to_list(x, list_len):
	new_x = []
	for _ in range(list_len):
		new_x.append(x)
	return new_x


if __name__ == '__main__':
	input_tensor = tf.keras.layers.Input(shape=(256, 256, 3))
	x = bn_elu_conv(filters=3, kernel_size=2)(input_tensor)
	for _ in range(4):
		x = dens_block(layer_num=4, filters=32, kernels_sizes=2)(x)
	model = tf.keras.models.Model(input_tensor, x)
	# model.compile(loss='binary_crossentropy', metrics=['accuracy'])
	model.summary()
	model.save('test.h5')
	netron.start('test.h5')


**Other info / logs**  
ValueError: Graph disconnected: cannot obtain value for tensor Tensor(""conv2d_2/Identity:0"", shape=(None, 255, 255, 32), dtype=float32) at layer ""concatenate"". The following previous layers were accessed without issue: ['input_1', 'batch_normalization', 'elu', 'conv2d', 'batch_normalization_1', 'elu_1', 'conv2d_1']
",0,tf.keras.layers.Concatenate(axis=axis)(conc_layer) not working,"tf.keras.layers.Concatenate(axis=axis)(conc_layer) not working **System information**  
- Have I written custom code (as opposed to using example directory):  no
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Windows 10
- TensorFlow backend (yes / no):  yes
- TensorFlow version:  2.0.0
- Keras version:  2.2.4-tf
- Python version: 3.6
- CUDA/cuDNN version:  10.1
- GPU model and memory:  Titan GTX 12Gb

**Describe the current behavior** 
//layer_type is in this case BatchNorm+ELU+Conv2D
 def dens_block(layer_num, layer_type,**kwargs):
	conc_layer = []
	def block(x):
		conc_layer.append(x)
		for i in range(layer_num):
		     x = layer_type(32, 2, strides=(1, 1), padding='same', **kwargs)(x)
		     conc_layer.append(x)
                     x = tf.keras.layers.Concatenate(axis=axis)(**list**(conc_layer))
		return x
	return block

**Describe the expected behavior**  
without the **list** in  tf.keras.layers.Concatenate the code not work

**Code to reproduce the issue**  
import tensorflow as tf
import netron


def bn_elu_conv(filters, kernel_size, strides=(1, 1), padding='valid', **kwargs):
	def layer(input_tensor):
		with tf.keras.backend.name_scope('bn_elu_conv'):
			x = tf.keras.layers.BatchNormalization()(input_tensor)
			x = tf.keras.layers.ELU()(x)
			x = tf.keras.layers.Conv2D(filters, kernel_size, strides=strides, padding=padding, **kwargs)(x)
		return x

	return layer


def conv_bn_elu(filters, kernel_size, strides=(1, 1), padding='valid', **kwargs):
	def layer(input_tensor):
		with tf.keras.backend.name_scope('conv_bn_elu'):
			x = tf.keras.layers.Conv2D(filters, kernel_size, strides=strides, padding=padding, **kwargs)(input_tensor)
			x = tf.keras.layers.BatchNormalization()(x)
			x = tf.keras.layers.ELU()(x)
		return x

	return layer


def dens_block(layer_num, filters, kernels_sizes, strides=(1, 1), padding='same', layer_type='bn_elu_conv', axis=-1, **kwargs):
	layer_type = _get_layerTyp(layer_type)
	filters = _check_list(filters, layer_num)
	kernels_sizes = _check_list(kernels_sizes, layer_num)
	conc_layer = []

	def block(x):
		conc_layer.append(x)
		for i in range(layer_num):
			x = layer_type(filters[i], kernels_sizes[i], strides=strides, padding=padding, **kwargs)(x)
			conc_layer.append(x)
			x = tf.keras.layers.Concatenate(axis=axis)(conc_layer)
			# x = tf.keras.layers.Concatenate(axis=axis)(list(conc_layer) # is working

		return x

	return block


def res_block(layer_num, filters, kernels_sizes, strides=(1, 1), padding='same', layer_type='bn_elu_conv', conv_block=False, **kwargs):
	layer_type = _get_layerTyp(layer_type)
	filters = _check_list(filters, layer_num)
	kernels_sizes = _check_list(kernels_sizes, layer_num)

	def block(x):
		input_tensor = x
		for i in range(layer_num):
			x = layer_type(filters[i], kernels_sizes[i], strides=strides, padding=padding, **kwargs)(x)
		if conv_block:
			input_tensor = layer_type(filters[-1], kernels_sizes[-1], strides=strides, padding=padding, )(input_tensor)
		x = tf.keras.layers.Add()([x, input_tensor])
		return x

	return block


def _get_layerTyp(layer_typ):
	if layer_typ is 'bn_elu_conv':
		return bn_elu_conv
	if layer_typ is 'conv_bn_elu':
		return conv_bn_elu
	else:
		raise ValueError('layer_typ have to be \'bn_elu_conv\' or \'conv_bn_elu\'')


def _check_list(x, list_len):
	if isinstance(x, list):
		if len(x) is list_len:
			return x
		else:
			raise TypeError('x have to be the same length as list_len')
	else:
		return _to_list(x, list_len)


def _to_list(x, list_len):
	new_x = []
	for _ in range(list_len):
		new_x.append(x)
	return new_x


if __name__ == '__main__':
	input_tensor = tf.keras.layers.Input(shape=(256, 256, 3))
	x = bn_elu_conv(filters=3, kernel_size=2)(input_tensor)
	for _ in range(4):
		x = dens_block(layer_num=4, filters=32, kernels_sizes=2)(x)
	model = tf.keras.models.Model(input_tensor, x)
	# model.compile(loss='binary_crossentropy', metrics=['accuracy'])
	model.summary()
	model.save('test.h5')
	netron.start('test.h5')


**Other info / logs**  
ValueError: Graph disconnected: cannot obtain value for tensor Tensor(""conv2d_2/Identity:0"", shape=(None, 255, 255, 32), dtype=float32) at layer ""concatenate"". The following previous layers were accessed without issue: ['input_1', 'batch_normalization', 'elu', 'conv2d', 'batch_normalization_1', 'elu_1', 'conv2d_1']
"
keras,8661,"I am trying to save best weights of Convoluted NN using Keras, and want to evaluate model based on best weights. But the file for saving best weight does not create.  I have given absolute path, changed the name of file but nothing worked. Although the file is not created in the working directory, error occurs at line ""model.load_weights(""apnamodel.best.hdf5"")""

Here is the code

model = Sequential() 
model.add(Conv2D(32, 2, strides=1, activation='relu', input_shape = input_shape)) 
model.add(MaxPooling2D(pool_size=2))
model.add(Flatten())
model.add(Dense(50, activation='relu'))
model.add(Dense(10, activation= ""softmax""))
nadam  =  optimizers.Nadam(lr=0.002, beta_1=0.9, beta_2=0.999, epsilon=1e-08, schedule_decay=0.004)
model.compile(loss='categorical_crossentropy', optimizer=nadam, metrics=['accuracy'])

# checkpoint

filepath=""apnamodel.best.hdf5""
checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=0, save_best_only=True, mode='max')
callbacks_list = [checkpoint]

earlystop = EarlyStopping(monitor='val_loss', min_delta=0.0001, patience=3, verbose=0, mode='auto')
callbacks_list = [earlystop]

history=model.fit(train_data, label_train_tranformed, epochs=5, batch_size=1000, validation_data=(val_data, label_val_tranformed), callbacks=callbacks_list)
score = model.evaluate(test_data, label_test_tranformed, batch_size=100)
print(""score1 :""+ str(score[0]))
print(""score2 :""+ str(score[0]))

model = Sequential() 
model.add(Conv2D(32, 2, strides=1, activation='relu', input_shape = input_shape)) 
model.add(MaxPooling2D(pool_size=2))
model.add(Flatten())
model.add(Dense(50, activation='relu'))
model.add(Dense(10, activation= ""softmax""))
model.load_weights(""apnamodel.best.hdf5"")
nadam  =  optimizers.Nadam(lr=0.002, beta_1=0.9, beta_2=0.999, epsilon=1e-08, schedule_decay=0.004)
model.compile(loss='categorical_crossentropy', optimizer=nadam, metrics=['accuracy'])
scores = model.evaluate(val_data, label_val_tranformed)
print (""Scores ""+str(scores))",0,"Unable to open file (Unable to open file: name = 'apnamodel.best.hdf5', errno = 2, error message = 'no such file or directory', flags = 0, o_flags = 0)","Unable to open file (Unable to open file: name = 'apnamodel.best.hdf5', errno = 2, error message = 'no such file or directory', flags = 0, o_flags = 0) I am trying to save best weights of Convoluted NN using Keras, and want to evaluate model based on best weights. But the file for saving best weight does not create.  I have given absolute path, changed the name of file but nothing worked. Although the file is not created in the working directory, error occurs at line ""model.load_weights(""apnamodel.best.hdf5"")""

Here is the code

model = Sequential() 
model.add(Conv2D(32, 2, strides=1, activation='relu', input_shape = input_shape)) 
model.add(MaxPooling2D(pool_size=2))
model.add(Flatten())
model.add(Dense(50, activation='relu'))
model.add(Dense(10, activation= ""softmax""))
nadam  =  optimizers.Nadam(lr=0.002, beta_1=0.9, beta_2=0.999, epsilon=1e-08, schedule_decay=0.004)
model.compile(loss='categorical_crossentropy', optimizer=nadam, metrics=['accuracy'])

# checkpoint

filepath=""apnamodel.best.hdf5""
checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=0, save_best_only=True, mode='max')
callbacks_list = [checkpoint]

earlystop = EarlyStopping(monitor='val_loss', min_delta=0.0001, patience=3, verbose=0, mode='auto')
callbacks_list = [earlystop]

history=model.fit(train_data, label_train_tranformed, epochs=5, batch_size=1000, validation_data=(val_data, label_val_tranformed), callbacks=callbacks_list)
score = model.evaluate(test_data, label_test_tranformed, batch_size=100)
print(""score1 :""+ str(score[0]))
print(""score2 :""+ str(score[0]))

model = Sequential() 
model.add(Conv2D(32, 2, strides=1, activation='relu', input_shape = input_shape)) 
model.add(MaxPooling2D(pool_size=2))
model.add(Flatten())
model.add(Dense(50, activation='relu'))
model.add(Dense(10, activation= ""softmax""))
model.load_weights(""apnamodel.best.hdf5"")
nadam  =  optimizers.Nadam(lr=0.002, beta_1=0.9, beta_2=0.999, epsilon=1e-08, schedule_decay=0.004)
model.compile(loss='categorical_crossentropy', optimizer=nadam, metrics=['accuracy'])
scores = model.evaluate(val_data, label_val_tranformed)
print (""Scores ""+str(scores))"
keras,8555,"Running the script with  checks the output result prefix only at the end. It'd be appropriate to do a simple check for the validity of the paths in the beginning because otherwise, any error would only come up after all the processing is done.

<img width=""928"" alt=""screen shot 2017-11-22 at 10 38 20 am"" src=""https://user-images.githubusercontent.com/20726434/33110940-4fa3615e-cf71-11e7-8c21-9df69e2c82c3.png"">
",0,File path check for /examples/deep_dream.py,"File path check for /examples/deep_dream.py Running the script with  checks the output result prefix only at the end. It'd be appropriate to do a simple check for the validity of the paths in the beginning because otherwise, any error would only come up after all the processing is done.

<img width=""928"" alt=""screen shot 2017-11-22 at 10 38 20 am"" src=""https://user-images.githubusercontent.com/20726434/33110940-4fa3615e-cf71-11e7-8c21-9df69e2c82c3.png"">
"
keras,13573,"I am working on an LSTM project. 

Let's I have 100 input data. My input layer uses a slinding windows equal to 10.

Here's my LSTM model : 

    inputs = tf.keras.Input(shape=(10,100))
    LSTM_1 = layers.LSTM(100, stateful=False, return_sequences=True)(inputs)
    FC_1 = layers.Dense(100)(LSTM_1)
    LSTM_2 = layers.LSTM(100, stateful=False, return_sequences=False)(FC_1)
    label = layers.Dense(5, activation='softmax')(LSTM_2)

Here's the model summary.

    Layer (type)                 Output Shape              Param #   
    =================================================================
    input (InputLayer)        (None, 10, 100)           0         
    _________________________________________________________________
    lstm_1 (LSTM)             (None, 10, 100)           80400     
    _________________________________________________________________
    dense (Dense)             (None, 10, 100)           10100     
    _________________________________________________________________
    lstm_2 (LSTM)             (None, 100)               80400     
    _________________________________________________________________
    label (Dense)             (None, 5)                1111      
    =================================================================
    Total params: 172,011
    Trainable params: 172,011
    Non-trainable params: 0
    _________________________________________________________________
    None

Below is the code to load data.

    h5_x = 'X.h5'
    h5_Y = 'Y.h5' 
    
    x_data = HDF5Matrix(h5_x, start=0, end=data_length)
    x_data = np.reshape(x_data,(data_length, 5))       
    
    y_label = HDF5Matrix(h5_Y, start=0, end=data_length) 
    y_label = np.reshape(y_label,(data_length, 5))

And let's say I have a function that splits data to sequences of 10 and feed them to the model.

My question is when I set ***return_sequences*** to ***True***, my code works fine feeding the model with sequences of 10 slices but the output layer is of a shape **(None, 10, 5)** which I don't want it this way. 

So when I set it to **False**, the output layer is of a shape **(None, 5)**, but I get the error message : Error when checking target: expected to have 2 dimensions, but got array with shape (x, x, x). 

I know it's just a shape problem but how can I solve this ? What do I need to reshape ?
",0,"LSTM Error when checking target: expected to have 2 dimensions, but got array with 3D shape","LSTM Error when checking target: expected to have 2 dimensions, but got array with 3D shape I am working on an LSTM project. 

Let's I have 100 input data. My input layer uses a slinding windows equal to 10.

Here's my LSTM model : 

    inputs = tf.keras.Input(shape=(10,100))
    LSTM_1 = layers.LSTM(100, stateful=False, return_sequences=True)(inputs)
    FC_1 = layers.Dense(100)(LSTM_1)
    LSTM_2 = layers.LSTM(100, stateful=False, return_sequences=False)(FC_1)
    label = layers.Dense(5, activation='softmax')(LSTM_2)

Here's the model summary.

    Layer (type)                 Output Shape              Param #   
    =================================================================
    input (InputLayer)        (None, 10, 100)           0         
    _________________________________________________________________
    lstm_1 (LSTM)             (None, 10, 100)           80400     
    _________________________________________________________________
    dense (Dense)             (None, 10, 100)           10100     
    _________________________________________________________________
    lstm_2 (LSTM)             (None, 100)               80400     
    _________________________________________________________________
    label (Dense)             (None, 5)                1111      
    =================================================================
    Total params: 172,011
    Trainable params: 172,011
    Non-trainable params: 0
    _________________________________________________________________
    None

Below is the code to load data.

    h5_x = 'X.h5'
    h5_Y = 'Y.h5' 
    
    x_data = HDF5Matrix(h5_x, start=0, end=data_length)
    x_data = np.reshape(x_data,(data_length, 5))       
    
    y_label = HDF5Matrix(h5_Y, start=0, end=data_length) 
    y_label = np.reshape(y_label,(data_length, 5))

And let's say I have a function that splits data to sequences of 10 and feed them to the model.

My question is when I set ***return_sequences*** to ***True***, my code works fine feeding the model with sequences of 10 slices but the output layer is of a shape **(None, 10, 5)** which I don't want it this way. 

So when I set it to **False**, the output layer is of a shape **(None, 5)**, but I get the error message : Error when checking target: expected to have 2 dimensions, but got array with shape (x, x, x). 

I know it's just a shape problem but how can I solve this ? What do I need to reshape ?
"
keras,4278,"I want to use  and  callbacks with the  scikit_learn wrapper. Normally, when not using scikit_learn wrappers, I pass the callbacks to the fit function [as outlined in the documentation](https://keras.io/callbacks/#usage-of-callbacks). However, when using scikit_learn wrappers, this function is a method of . The documentation mentions that [ can contain arguments to the the fit method](https://keras.io/scikit-learn-api/#wrappers-for-the-scikit-learn-api) (among others) but I am unable to figure out how to use  to pass callbacks to the fit function inside the  class.

My code looks like this (excluding code for loading data into  and  for brevity):



Does anyone know how I should use  and  with this setup?

",0,How to pass callbacks to scikit_learn wrappers (e.g. KerasClassifier),"How to pass callbacks to scikit_learn wrappers (e.g. KerasClassifier) I want to use  and  callbacks with the  scikit_learn wrapper. Normally, when not using scikit_learn wrappers, I pass the callbacks to the fit function [as outlined in the documentation](https://keras.io/callbacks/#usage-of-callbacks). However, when using scikit_learn wrappers, this function is a method of . The documentation mentions that [ can contain arguments to the the fit method](https://keras.io/scikit-learn-api/#wrappers-for-the-scikit-learn-api) (among others) but I am unable to figure out how to use  to pass callbacks to the fit function inside the  class.

My code looks like this (excluding code for loading data into  and  for brevity):



Does anyone know how I should use  and  with this setup?

"
keras,12777,"I have a different set of ROI boxes for each of my input images. My batch_size=1 and I defined my **input images as (none, none, 3)** and **input boxes as (none,4)** to handle variations, but for ROI pooling layer I need to know how many ROI boxes should get extracted. **I want to know the first dimension in (none,4) in run time to pass it to my ROI pooling layer**. K.shape and K.int_shape are not working as one of them get the tensor and the other None.

![model1](https://user-images.githubusercontent.com/30056321/57056497-f1c70600-6c70-11e9-92ac-84e971a45063.png)
",0,"How to get the ""None"" dimension of a tensor in the run time from a keras model?","How to get the ""None"" dimension of a tensor in the run time from a keras model? I have a different set of ROI boxes for each of my input images. My batch_size=1 and I defined my **input images as (none, none, 3)** and **input boxes as (none,4)** to handle variations, but for ROI pooling layer I need to know how many ROI boxes should get extracted. **I want to know the first dimension in (none,4) in run time to pass it to my ROI pooling layer**. K.shape and K.int_shape are not working as one of them get the tensor and the other None.

![model1](https://user-images.githubusercontent.com/30056321/57056497-f1c70600-6c70-11e9-92ac-84e971a45063.png)
"
keras,9786,"Is there any easy interface where we can use Dense, Conv2d etc in our custom layer classes along with other trainable variables. This is something that can be easily achieved in pytorch.
",0,Can we use existing layers in our custom keras layers,"Can we use existing layers in our custom keras layers Is there any easy interface where we can use Dense, Conv2d etc in our custom layer classes along with other trainable variables. This is something that can be easily achieved in pytorch.
"
keras,8130,"Hello, I'm trying to use a model with paired input images through (in their own similar directory trees), augmented through **ImageDataGenerator** using also **flow_from_directory** (so the method infers the labels by the folder structure). I'm getting an error because keras can't handle it in this way.

How can I combine the generators (using **flow_from_directory**) to be accepted by fit_generator?

Here is a sample code


Model definition ***






The error I get is the following:

TypeError: Error when checking model input: data should be a Numpy array, or list/dict of Numpy arrays. Found: <keras.preprocessing.image.DirectoryIterator object at 0x7f824c5080f0>...


",0,how to use fit_generator with multiple image inputs ,"how to use fit_generator with multiple image inputs  Hello, I'm trying to use a model with paired input images through (in their own similar directory trees), augmented through **ImageDataGenerator** using also **flow_from_directory** (so the method infers the labels by the folder structure). I'm getting an error because keras can't handle it in this way.

How can I combine the generators (using **flow_from_directory**) to be accepted by fit_generator?

Here is a sample code


Model definition ***






The error I get is the following:

TypeError: Error when checking model input: data should be a Numpy array, or list/dict of Numpy arrays. Found: <keras.preprocessing.image.DirectoryIterator object at 0x7f824c5080f0>...


"
keras,7702,"I have created two ImageDataGenerator objects to process two images at the same time (one image and his mask) to train an autoencoder/u-net. To randomize the dataset I set . I take these images from two folders (one for the image and other for the mask), so I use two  with the same seed. However, in practice that doesn't work and the mask extracted doesn't correspond to the extracted image. Both images (image and mask) have the same file name.

The problem is that the filenames from the data directories are read in a different order. To solve that I have added the next line to the DirectoryIterator at the end of his __init__ function.


The problem seems to be solved.


- [x] Check that you are up-to-date with the master branch of Keras. You can update with:
pip install git+git://github.com/fchollet/keras.git --upgrade --no-deps

- [x] If running on TensorFlow, check that you are up-to-date with the latest version. The installation instructions can be found [here](https://www.tensorflow.org/get_started/os_setup).

- [x] If running on Theano, check that you are up-to-date with the master branch of Theano. You can update with:
pip install git+git://github.com/Theano/Theano.git --upgrade --no-deps

- [ ] Provide a link to a GitHub Gist of a Python script that can reproduce your issue (or just copy the script here if it is short).
",0,Keras ImageDataGenerator doesn't work properly with shuffle when transforming image and mask together,"Keras ImageDataGenerator doesn't work properly with shuffle when transforming image and mask together I have created two ImageDataGenerator objects to process two images at the same time (one image and his mask) to train an autoencoder/u-net. To randomize the dataset I set . I take these images from two folders (one for the image and other for the mask), so I use two  with the same seed. However, in practice that doesn't work and the mask extracted doesn't correspond to the extracted image. Both images (image and mask) have the same file name.

The problem is that the filenames from the data directories are read in a different order. To solve that I have added the next line to the DirectoryIterator at the end of his __init__ function.


The problem seems to be solved.


- [x] Check that you are up-to-date with the master branch of Keras. You can update with:
pip install git+git://github.com/fchollet/keras.git --upgrade --no-deps

- [x] If running on TensorFlow, check that you are up-to-date with the latest version. The installation instructions can be found [here](https://www.tensorflow.org/get_started/os_setup).

- [x] If running on Theano, check that you are up-to-date with the master branch of Theano. You can update with:
pip install git+git://github.com/Theano/Theano.git --upgrade --no-deps

- [ ] Provide a link to a GitHub Gist of a Python script that can reproduce your issue (or just copy the script here if it is short).
"
keras,9966,"My model is compiled with , I train it with  returning tuples <X, y, sample_weight>, and evaluating it on the full devset passed as . My model has 2 inputs so  is a list of 2 numpy arrays.

Now, in [my custom callback](https://stackoverflow.com/a/49832560/932818) at , I check  and see that it's 5..

OK, the first extra element is  flattened from a sublist into 2 elements of the new validation data (although I have no idea why it happens).

But the second extra element is actually just a scalar  appended to ! 
Where does it even come from???",0,Unknown elements appear in validation_data,"Unknown elements appear in validation_data My model is compiled with , I train it with  returning tuples <X, y, sample_weight>, and evaluating it on the full devset passed as . My model has 2 inputs so  is a list of 2 numpy arrays.

Now, in [my custom callback](https://stackoverflow.com/a/49832560/932818) at , I check  and see that it's 5..

OK, the first extra element is  flattened from a sublist into 2 elements of the new validation data (although I have no idea why it happens).

But the second extra element is actually just a scalar  appended to ! 
Where does it even come from???"
keras,7341,Having an Attention layer in the API like the Bidirectional Layer would be great. It is very difficult to create custom layers in Keras.,0,Attention Layer,Attention Layer Having an Attention layer in the API like the Bidirectional Layer would be great. It is very difficult to create custom layers in Keras.
keras,13571,"## Environment settings

Nvidia Driver Version: 430.50

CUDA Version: 10.1

GeForce RTX 2080 Ti

Ubuntu 16.04.6 LTS

| Library                        | Version  |       |
| ------------------------------ | -------- | ----- |
|libgpuarray               |0.7.6                |h14c3975_0|
|pygpu                     |0.7.6            |py36h035aef0_0|
|tensorflow-gpu            |1.14.0                    |\<pip>|
|tensorflow-estimator      |1.14.0                    |\<pip>|
|Theano                    |1.0.4                     |\<pip>|
|cntk-gpu                  |2.7                       |\<pip>|
|numpy                     |1.14.6           |py36h3b04361_4|
|numpy-base                |1.14.6           |py36h81de0dd_4|
|keras-applications        |1.0.8                      |py_0|
|keras-preprocessing       |1.1.0                      |py_1|

---

## Description

mobilenet.1.00.224-imagenet_origin0-NAI1-LS1-GF1-GF2.h5 is a model for image classification. We find something strange when we run this model for some inputs. When we load this model in Keras, this model works well on the backend of CNTK, but when we configure Tensorflow or Theano as the backend of Keras, the output of this model is NaN, which seems quite strange.

We attach the .h5 file, input image and the code below. The phenomenon I described above can be reproduced using this command:



[20191114-nan.zip](https://github.com/keras-team/keras/files/3877869/20191114-nan.zip)
",0,NaN output on Tensorflow and Theano backend while CNTK output normally,"NaN output on Tensorflow and Theano backend while CNTK output normally ## Environment settings

Nvidia Driver Version: 430.50

CUDA Version: 10.1

GeForce RTX 2080 Ti

Ubuntu 16.04.6 LTS

| Library                        | Version  |       |
| ------------------------------ | -------- | ----- |
|libgpuarray               |0.7.6                |h14c3975_0|
|pygpu                     |0.7.6            |py36h035aef0_0|
|tensorflow-gpu            |1.14.0                    |\<pip>|
|tensorflow-estimator      |1.14.0                    |\<pip>|
|Theano                    |1.0.4                     |\<pip>|
|cntk-gpu                  |2.7                       |\<pip>|
|numpy                     |1.14.6           |py36h3b04361_4|
|numpy-base                |1.14.6           |py36h81de0dd_4|
|keras-applications        |1.0.8                      |py_0|
|keras-preprocessing       |1.1.0                      |py_1|

---

## Description

mobilenet.1.00.224-imagenet_origin0-NAI1-LS1-GF1-GF2.h5 is a model for image classification. We find something strange when we run this model for some inputs. When we load this model in Keras, this model works well on the backend of CNTK, but when we configure Tensorflow or Theano as the backend of Keras, the output of this model is NaN, which seems quite strange.

We attach the .h5 file, input image and the code below. The phenomenon I described above can be reproduced using this command:



[20191114-nan.zip](https://github.com/keras-team/keras/files/3877869/20191114-nan.zip)
"
keras,10089,"tensorflow version==1.3 , keras ==2.0.8


code:

![image](https://user-images.githubusercontent.com/11004307/39503488-d1151ff6-4df8-11e8-97f3-ce8b9e3fb719.png)
",0,keras using tensorflow backend: InvalidArgumentError: <exception str() failed>,"keras using tensorflow backend: InvalidArgumentError: <exception str() failed> tensorflow version==1.3 , keras ==2.0.8


code:

![image](https://user-images.githubusercontent.com/11004307/39503488-d1151ff6-4df8-11e8-97f3-ce8b9e3fb719.png)
"
keras,12637,"![스크린샷, 2019-04-08 23-21-12](https://user-images.githubusercontent.com/33189954/55731397-1c98b280-5a55-11e9-993d-9af62380c639.png)
I want to represent this 28x1x1 output layer 
but for keras all I see is one single number dense layer ",0,How to represent 28x1x1 output dense layer?,"How to represent 28x1x1 output dense layer? ![스크린샷, 2019-04-08 23-21-12](https://user-images.githubusercontent.com/33189954/55731397-1c98b280-5a55-11e9-993d-9af62380c639.png)
I want to represent this 28x1x1 output layer 
but for keras all I see is one single number dense layer "
keras,8500,"I am training and persisting the model found at this [link](https://github.com/Hironsan/anago/blob/master/anago/models.py). It trains and loads on the same machine without a problem. However, if the persisted model is loaded on a different machine than the one it was trained on, then it shows the error message bellow.



The files needed to reproduce the error can be found [here](https://drive.google.com/open?id=1XxeZV3YLcP45AqfEVF8Pll3KGn9quYpK).",0,Saved Model Loading Inconsistent Behavior,"Saved Model Loading Inconsistent Behavior I am training and persisting the model found at this [link](https://github.com/Hironsan/anago/blob/master/anago/models.py). It trains and loads on the same machine without a problem. However, if the persisted model is loaded on a different machine than the one it was trained on, then it shows the error message bellow.



The files needed to reproduce the error can be found [here](https://drive.google.com/open?id=1XxeZV3YLcP45AqfEVF8Pll3KGn9quYpK)."
keras,6499,"Hello, I run a slightly modified version of the [keras fine tuning examples](https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html) which only fine tunes the top layers (with Keras 2.0.3/Tensorflow on Ubuntu with GPU). This looks like the following:

With this, I get unreliable validation accuracy results. For example, predict_generator predicts 640 out of 800 (80%) classes correctly whereas evaluate_generator produces an accuracy score of 95%. Someone in #3477 suggests to remove the  parameter from the validation generator, then I get results of 365/800=45% and 89% from evaluate_generator.

Is there something wrong with my evaluation or is this due to a bug? There are many similar issues (e.g. #3849, #6245) where the stated accuracy (during training and afterwards) doesn't match the actual predictions. Could someone experienced maybe shine some light onto this problem? Thanks
",0,Evaluate_generator produces wrong accuracy scores?,"Evaluate_generator produces wrong accuracy scores? Hello, I run a slightly modified version of the [keras fine tuning examples](https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html) which only fine tunes the top layers (with Keras 2.0.3/Tensorflow on Ubuntu with GPU). This looks like the following:

With this, I get unreliable validation accuracy results. For example, predict_generator predicts 640 out of 800 (80%) classes correctly whereas evaluate_generator produces an accuracy score of 95%. Someone in #3477 suggests to remove the  parameter from the validation generator, then I get results of 365/800=45% and 89% from evaluate_generator.

Is there something wrong with my evaluation or is this due to a bug? There are many similar issues (e.g. #3849, #6245) where the stated accuracy (during training and afterwards) doesn't match the actual predictions. Could someone experienced maybe shine some light onto this problem? Thanks
"
keras,12335,"Reproducible script

throws


---

- [x] Check that you are up-to-date with the master branch of Keras. You can update with:


- [x] Check that your version of CNTK is up-to-date.

- [x] Provide a link to a GitHub Gist of a Python script that can reproduce your issue (or just copy the script here if it is short).
",0,K.in_top_k on CNTK backend is broken,"K.in_top_k on CNTK backend is broken Reproducible script

throws


---

- [x] Check that you are up-to-date with the master branch of Keras. You can update with:


- [x] Check that your version of CNTK is up-to-date.

- [x] Provide a link to a GitHub Gist of a Python script that can reproduce your issue (or just copy the script here if it is short).
"
keras,9495,"Hi all, I'm trying to turn current VAE example into beta-VAE (https://openreview.net/pdf?id=Sy2fzU9gl). Having hard time, asking community with the suggestion or ideally a code snippet.

Here a TF implementation (https://github.com/miyosuda/disentangled_vae), but I would prefer a Keras one.

Thanks in advance!",0,How to turn variational_autoencoder example into beta-VAE?,"How to turn variational_autoencoder example into beta-VAE? Hi all, I'm trying to turn current VAE example into beta-VAE (https://openreview.net/pdf?id=Sy2fzU9gl). Having hard time, asking community with the suggestion or ideally a code snippet.

Here a TF implementation (https://github.com/miyosuda/disentangled_vae), but I would prefer a Keras one.

Thanks in advance!"
keras,9727,"By default, model training will use  and  as the default argument. However, whatever  is True or False, unless workers = 0, the engine will always create a thread pool to handle data reading, but it seems there is a high-frequent dead lock on application exit/quit when training huge model like Inception_v3 and Resnet50, and I have to press many-time of  to kill the hanging thread created by the thread pool.

So I suggest to set default argument of  to be  which will not create any extra threads.",0,Dead Lock on huge model when using `workers > 0`,"Dead Lock on huge model when using `workers > 0` By default, model training will use  and  as the default argument. However, whatever  is True or False, unless workers = 0, the engine will always create a thread pool to handle data reading, but it seems there is a high-frequent dead lock on application exit/quit when training huge model like Inception_v3 and Resnet50, and I have to press many-time of  to kill the hanging thread created by the thread pool.

So I suggest to set default argument of  to be  which will not create any extra threads."
keras,11099,"TensorBoard in Keras crashes deep in the callbacks for the end-of-epoch. Here's a simplified version of the code:

# The Code
    import pandas as pd
    import keras
    from keras.models import Sequential
    from keras.layers import Dense

    class ToyNet ():
        def __init__(self, run=1, layer_1_nodes=50, layer_2_nodes=100, layer_3_nodes=50):
            self.run_seq = run
            self.layer_1_nodes = layer_1_nodes
            self.layer_2_nodes = layer_2_nodes
            self.layer_3_nodes = layer_3_nodes

            # Define the model
            self.model = Sequential()
            self.model.add(Dense(self.layer_1_nodes, input_dim=9, activation='relu', name='layer_1'))
            self.model.add(Dense(self.layer_2_nodes, activation='relu', name='layer_2'))
            self.model.add(Dense(self.layer_3_nodes, activation='relu', name='layer_3'))
            self.model.add(Dense(1, activation='linear', name='output_layer'))
            self.model.compile(loss='mean_squared_error', optimizer='adam')

            # Create a TensorBoard logger
            log_dir = ""logs_{}"".format(self.run_seq)
            self.logger = keras.callbacks.TensorBoard(
                log_dir=log_dir,
                histogram_freq=5
            )

        def train(self, X, Y, epochs=50):
            # Train the model
            self.model.fit(
                X,
                Y,
                epochs=epochs,
                shuffle=True,
                verbose=2,
                validation_split=0.05,
                callbacks=[self.logger]
            )

        def test(self, Xt, Yt):
            test_error_rate = self.model.evaluate(Xt, Yt, verbose=0)
            print(""The mean squared error (MSE) for the test data set is: {}"".format(test_error_rate))

    if __name__ == '__main__':

        training_data_df = pd.read_csv(""sales_data_training_scaled.csv"")

        X = training_data_df.drop('total_earnings', axis=1).values
        Y = training_data_df[['total_earnings']].values

        # Load the test data set
        test_data_df = pd.read_csv(""sales_data_test_scaled.csv"")

        X_test = test_data_df.drop('total_earnings', axis=1).values
        Y_test = test_data_df[['total_earnings']].values

        print(""Run #1"")
        toy1 = ToyNet(1, 50, 100, 50)
        toy1.train(X, Y)
        toy1.test(X_test, Y_test)

        print(""Run #2"")
        toy2 = ToyNet(2, 5, 100, 50)
        toy2.train(X,Y)              ### <--- Crashes here at the end of first epoch while doing callbacks
        toy2.test(X_test, Y_test)

# The Output

/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from  to  is deprecated. In future, it will be treated as .
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
Run #1
Train on 950 samples, validate on 50 samples
Epoch 1/50
 - 1s - loss: 0.0314 - val_loss: 0.0043
Epoch 2/50
 - 0s - loss: 0.0048 - val_loss: 0.0011

> ... output snipped for brevity

Epoch 50/50
 - 0s - loss: 2.4237e-05 - val_loss: 5.0361e-05
The mean squared error (MSE) for the test data set is: 7.575784547952935e-05
Run #2
Train on 950 samples, validate on 50 samples
Epoch 1/50
 - 1s - loss: 0.0333 - val_loss: 0.0172
---------------------------------------------------------------------------
InvalidArgumentError                      Traceback (most recent call last)
/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py in _do_call(self, fn, *args)
   1321     try:
-> 1322       return fn(*args)
   1323     except errors.OpError as e:

/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py in _run_fn(feed_dict, fetch_list, target_list, options, run_metadata)
   1306       return self._call_tf_sessionrun(
-> 1307           options, feed_dict, fetch_list, target_list, run_metadata)
   1308 

/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py in _call_tf_sessionrun(self, options, feed_dict, fetch_list, target_list, run_metadata)
   1408           self._session, options, feed_dict, fetch_list, target_list,
-> 1409           run_metadata)
   1410     else:

InvalidArgumentError: You must feed a value for placeholder tensor 'layer_1_input' with dtype float and shape [?,9]
	 [[Node: layer_1_input = Placeholder[dtype=DT_FLOAT, shape=[?,9], _device=""/job:localhost/replica:0/task:0/device:CPU:0""]()]]

During handling of the above exception, another exception occurred:

InvalidArgumentError                      Traceback (most recent call last)
<ipython-input-1-bb0bb05e17b3> in <module>()
     64 
     65     toy2 = ToyNet(2, 5, 100, 50)
---> 66     toy2.train(X,Y)              ### <--- Crashes here at the end of first epoch while doing callbacks
     67     toy2.test(X_test, Y_test)

<ipython-input-1-bb0bb05e17b3> in train(self, X, Y, epochs)
     39             verbose=2,
     40             validation_split=0.05,
---> 41             callbacks=[self.logger]
     42         )
     43 

/anaconda3/lib/python3.6/site-packages/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)
   1043                                         initial_epoch=initial_epoch,
   1044                                         steps_per_epoch=steps_per_epoch,
-> 1045                                         validation_steps=validation_steps)
   1046 
   1047     def evaluate(self, x=None, y=None,

/anaconda3/lib/python3.6/site-packages/keras/engine/training_arrays.py in fit_loop(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)
    215                         for l, o in zip(out_labels, val_outs):
    216                             epoch_logs['val_' + l] = o
--> 217         callbacks.on_epoch_end(epoch, epoch_logs)
    218         if callback_model.stop_training:
    219             break

/anaconda3/lib/python3.6/site-packages/keras/callbacks.py in on_epoch_end(self, epoch, logs)
     75         logs = logs or {}
     76         for callback in self.callbacks:
---> 77             callback.on_epoch_end(epoch, logs)
     78 
     79     def on_batch_begin(self, batch, logs=None):

/anaconda3/lib/python3.6/site-packages/keras/callbacks.py in on_epoch_end(self, epoch, logs)
    915                     assert len(batch_val) == len(tensors)
    916                     feed_dict = dict(zip(tensors, batch_val))
--> 917                     result = self.sess.run([self.merged], feed_dict=feed_dict)
    918                     summary_str = result[0]
    919                     self.writer.add_summary(summary_str, epoch)

/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py in run(self, fetches, feed_dict, options, run_metadata)
    898     try:
    899       result = self._run(None, fetches, feed_dict, options_ptr,
--> 900                          run_metadata_ptr)
    901       if run_metadata:
    902         proto_data = tf_session.TF_GetBuffer(run_metadata_ptr)

/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py in _run(self, handle, fetches, feed_dict, options, run_metadata)
   1133     if final_fetches or final_targets or (handle and feed_dict_tensor):
   1134       results = self._do_run(handle, final_targets, final_fetches,
-> 1135                              feed_dict_tensor, options, run_metadata)
   1136     else:
   1137       results = []

/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py in _do_run(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)
   1314     if handle is None:
   1315       return self._do_call(_run_fn, feeds, fetches, targets, options,
-> 1316                            run_metadata)
   1317     else:
   1318       return self._do_call(_prun_fn, handle, feeds, fetches)

/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py in _do_call(self, fn, *args)
   1333         except KeyError:
   1334           pass
-> 1335       raise type(e)(node_def, op, message)
   1336 
   1337   def _extend_graph(self):

InvalidArgumentError: You must feed a value for placeholder tensor 'layer_1_input' with dtype float and shape [?,9]
	 [[Node: layer_1_input = Placeholder[dtype=DT_FLOAT, shape=[?,9], _device=""/job:localhost/replica:0/task:0/device:CPU:0""]()]]

Caused by op 'layer_1_input', defined at:
  File ""/anaconda3/lib/python3.6/runpy.py"", line 193, in _run_module_as_main
    ""__main__"", mod_spec)
  File ""/anaconda3/lib/python3.6/runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py"", line 16, in <module>
    app.launch_new_instance()
  File ""/anaconda3/lib/python3.6/site-packages/traitlets/config/application.py"", line 658, in launch_instance
    app.start()
  File ""/anaconda3/lib/python3.6/site-packages/ipykernel/kernelapp.py"", line 486, in start
    self.io_loop.start()
  File ""/anaconda3/lib/python3.6/site-packages/tornado/platform/asyncio.py"", line 127, in start
    self.asyncio_loop.run_forever()
  File ""/anaconda3/lib/python3.6/asyncio/base_events.py"", line 422, in run_forever
    self._run_once()
  File ""/anaconda3/lib/python3.6/asyncio/base_events.py"", line 1432, in _run_once
    handle._run()
  File ""/anaconda3/lib/python3.6/asyncio/events.py"", line 145, in _run
    self._callback(*self._args)
  File ""/anaconda3/lib/python3.6/site-packages/tornado/platform/asyncio.py"", line 117, in _handle_events
    handler_func(fileobj, events)
  File ""/anaconda3/lib/python3.6/site-packages/tornado/stack_context.py"", line 276, in null_wrapper
    return fn(*args, **kwargs)
  File ""/anaconda3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py"", line 450, in _handle_events
    self._handle_recv()
  File ""/anaconda3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py"", line 480, in _handle_recv
    self._run_callback(callback, msg)
  File ""/anaconda3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py"", line 432, in _run_callback
    callback(*args, **kwargs)
  File ""/anaconda3/lib/python3.6/site-packages/tornado/stack_context.py"", line 276, in null_wrapper
    return fn(*args, **kwargs)
  File ""/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py"", line 283, in dispatcher
    return self.dispatch_shell(stream, msg)
  File ""/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py"", line 233, in dispatch_shell
    handler(stream, idents, msg)
  File ""/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py"", line 399, in execute_request
    user_expressions, allow_stdin)
  File ""/anaconda3/lib/python3.6/site-packages/ipykernel/ipkernel.py"", line 208, in do_execute
    res = shell.run_cell(code, store_history=store_history, silent=silent)
  File ""/anaconda3/lib/python3.6/site-packages/ipykernel/zmqshell.py"", line 537, in run_cell
    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)
  File ""/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py"", line 2662, in run_cell
    raw_cell, store_history, silent, shell_futures)
  File ""/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py"", line 2785, in _run_cell
    interactivity=interactivity, compiler=compiler, result=result)
  File ""/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py"", line 2903, in run_ast_nodes
    if self.run_code(code, result):
  File ""/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py"", line 2963, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  File ""<ipython-input-1-bb0bb05e17b3>"", line 61, in <module>
    toy1 = ToyNet(1, 50, 100, 50)
  File ""<ipython-input-1-bb0bb05e17b3>"", line 19, in __init__
    self.model.add(Dense(self.layer_1_nodes, input_dim=9, activation='relu', name='layer_1'))
  File ""/anaconda3/lib/python3.6/site-packages/keras/engine/sequential.py"", line 160, in add
    name=layer.name + '_input')
  File ""/anaconda3/lib/python3.6/site-packages/keras/engine/input_layer.py"", line 178, in Input
    input_tensor=tensor)
  File ""/anaconda3/lib/python3.6/site-packages/keras/legacy/interfaces.py"", line 91, in wrapper
    return func(*args, **kwargs)
  File ""/anaconda3/lib/python3.6/site-packages/keras/engine/input_layer.py"", line 87, in __init__
    name=self.name)
  File ""/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py"", line 517, in placeholder
    x = tf.placeholder(dtype, shape=shape, name=name)
  File ""/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py"", line 1734, in placeholder
    return gen_array_ops.placeholder(dtype=dtype, shape=shape, name=name)
  File ""/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gen_array_ops.py"", line 4924, in placeholder
    ""Placeholder"", dtype=dtype, shape=shape, name=name)
  File ""/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py"", line 787, in _apply_op_helper
    op_def=op_def)
  File ""/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 3414, in create_op
    op_def=op_def)
  File ""/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 1740, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

InvalidArgumentError (see above for traceback): You must feed a value for placeholder tensor 'layer_1_input' with dtype float and shape [?,9]
	 [[Node: layer_1_input = Placeholder[dtype=DT_FLOAT, shape=[?,9], _device=""/job:localhost/replica:0/task:0/device:CPU:0""]()]]

# The Data

For sanity's sake I've only included a short sample of each training and testing data below.

## Training data (""sales_data_training_scaled.csv"")
    critic_rating,is_action,is_exclusive_to_us,is_portable,is_role_playing,is_sequel,is_sports,suitable_for_kids,total_earnings,unit_price
    0.4999999999999999,1.0,1.0,1.0,0.0,1.0,0.0,1.0,0.7991793127668619,1.0
    0.16666666666666663,0.0,0.0,0.0,1.0,1.0,0.0,0.0,0.15750170976506905,1.0
    0.4999999999999999,0.0,0.0,0.0,0.0,1.0,1.0,0.0,0.18970444169239015,1.0
    0.6666666666666666,1.0,1.0,0.0,0.0,1.0,0.0,0.0,0.39223304559989647,0.0
    0.0,1.0,0.0,1.0,0.0,1.0,0.0,0.0,0.21546366980277626,1.0
    0.4999999999999999,1.0,0.0,1.0,0.0,0.0,0.0,1.0,0.2675699155283636,1.0
    0.6666666666666666,1.0,0.0,0.0,0.0,0.0,0.0,1.0,0.2418106874179775,1.0
    0.4999999999999999,1.0,1.0,1.0,0.0,1.0,0.0,0.0,0.7090183175911721,1.0
    0.6666666666666666,1.0,1.0,0.0,0.0,0.0,0.0,0.0,0.4385279384854254,1.0
    0.16666666666666663,0.0,1.0,0.0,1.0,1.0,0.0,1.0,0.3957893569434946,1.0
    0.9999999999999999,1.0,0.0,0.0,0.0,1.0,0.0,0.0,0.24197334614886973,0.0
    0.6666666666666666,1.0,0.0,0.0,0.0,1.0,0.0,0.0,0.33607142197001905,1.0
    0.4999999999999999,0.0,1.0,0.0,0.0,0.0,1.0,0.0,0.26054601578529046,1.0
    0.6666666666666666,0.0,0.0,1.0,1.0,1.0,0.0,0.0,0.3501229182455038,1.0
    0.8333333333333334,0.0,1.0,1.0,0.0,1.0,1.0,0.0,0.39457681004047984,0.0
    0.6666666666666666,0.0,0.0,0.0,0.0,1.0,1.0,0.0,0.2248313339864328,1.0
    0.4999999999999999,0.0,0.0,1.0,0.0,1.0,1.0,1.0,0.28864900833625995,1.0
    0.4999999999999999,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.05268664165172547,0.0
    0.9999999999999999,1.0,0.0,0.0,0.0,1.0,0.0,0.0,0.24431711058945305,0.0
    0.16666666666666663,1.0,0.0,1.0,0.0,1.0,0.0,0.0,0.2740097225559601,1.0
    0.33333333333333337,0.0,0.0,0.0,1.0,1.0,0.0,0.0,0.20141217352729157,1.0
    0.4999999999999999,0.0,1.0,0.0,1.0,1.0,0.0,0.0,0.2802240254339107,0.0
    0.16666666666666663,0.0,1.0,0.0,1.0,1.0,0.0,1.0,0.4069129960629193,1.0
    0.6666666666666666,0.0,0.0,1.0,1.0,0.0,0.0,0.0,0.21487957708729966,1.0
    0.4999999999999999,1.0,0.0,1.0,0.0,1.0,0.0,1.0,0.4250642317147557,1.0
    0.4999999999999999,0.0,0.0,1.0,0.0,1.0,1.0,1.0,0.28513336167538494,1.0
    0.33333333333333337,1.0,1.0,0.0,0.0,1.0,0.0,0.0,0.40075044823570727,0.5
    0.4999999999999999,1.0,0.0,0.0,0.0,1.0,0.0,1.0,0.18811482227685256,0.0
    0.33333333333333337,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.1428661207741077,1.0
    0.8333333333333334,1.0,0.0,1.0,0.0,0.0,0.0,1.0,0.27292286649045305,0.5
    0.16666666666666663,0.0,0.0,0.0,1.0,1.0,0.0,0.0,0.16160144914142066,1.0
    0.4999999999999999,1.0,1.0,0.0,0.0,1.0,0.0,0.0,0.5831426406166245,1.0
    0.6666666666666666,1.0,0.0,0.0,0.0,1.0,0.0,0.0,0.18850668194672926,0.0
    0.8333333333333334,0.0,1.0,0.0,1.0,1.0,0.0,0.0,0.6118297258830706,1.0
    0.9999999999999999,0.0,0.0,0.0,1.0,1.0,0.0,0.0,0.18928670449714424,0.0
    0.4999999999999999,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.13759449917746436,1.0
    0.9999999999999999,0.0,1.0,0.0,0.0,1.0,1.0,0.0,0.34539842147095245,0.0
    0.16666666666666663,0.0,0.0,0.0,1.0,1.0,0.0,1.0,0.13583113066301916,0.5
    0.9999999999999999,0.0,1.0,1.0,0.0,1.0,1.0,1.0,0.48629415352766125,0.0
    0.9999999999999999,0.0,1.0,0.0,1.0,1.0,0.0,1.0,0.7500009241973347,1.0
    0.0,0.0,0.0,0.0,1.0,1.0,0.0,0.0,0.04253895491765401,0.0
    0.4999999999999999,0.0,0.0,0.0,1.0,1.0,0.0,0.0,0.11630468937727585,0.0
    0.16666666666666663,1.0,1.0,0.0,0.0,0.0,0.0,0.0,0.2775253692168352,1.0
    0.4999999999999999,0.0,0.0,1.0,0.0,0.0,1.0,0.0,0.10216816694700652,0.5
    0.4999999999999999,0.0,1.0,0.0,0.0,1.0,1.0,0.0,0.23650949150662648,0.0
    0.33333333333333337,1.0,0.0,1.0,0.0,1.0,0.0,0.0,0.18031089998336447,0.0
    0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.05621337868061589,1.0
    0.9999999999999999,1.0,1.0,1.0,0.0,1.0,0.0,0.0,0.7949538825530027,0.5
    0.4999999999999999,1.0,0.0,0.0,0.0,1.0,0.0,1.0,0.18538289495573096,0.0
    0.8333333333333334,0.0,0.0,0.0,0.0,1.0,1.0,0.0,0.2587900408495222,1.0
    0.16666666666666663,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.08373227851610877,1.0
    0.9999999999999999,1.0,1.0,1.0,0.0,0.0,0.0,1.0,0.7447329993900298,1.0
    0.16666666666666663,1.0,0.0,0.0,0.0,1.0,0.0,0.0,0.20902386277517976,1.0
    0.6666666666666666,1.0,0.0,0.0,0.0,1.0,0.0,0.0,0.18850668194672926,0.0
    0.16666666666666663,0.0,0.0,1.0,0.0,1.0,1.0,0.0,0.12509565442413267,0.5
    0.6666666666666666,1.0,0.0,0.0,0.0,1.0,0.0,0.0,0.3313875898781908,1.0
    0.33333333333333337,1.0,0.0,1.0,0.0,1.0,0.0,1.0,0.3717861037688767,1.0
    0.0,0.0,0.0,0.0,1.0,1.0,0.0,1.0,0.1434502134895843,1.0
    0.33333333333333337,1.0,1.0,1.0,0.0,1.0,0.0,0.0,0.6329051219016284,1.0
    0.8333333333333334,1.0,1.0,0.0,0.0,1.0,0.0,0.0,0.727753645958485,1.0
    0.6666666666666666,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.15144267203933381,0.5
    0.8333333333333334,1.0,1.0,1.0,0.0,1.0,0.0,0.0,0.8846601726400621,1.0
    0.8333333333333334,0.0,0.0,0.0,1.0,1.0,0.0,0.0,0.30738433670357296,1.0

## Testing data (""sales_data_test_scaled.csv"")
    critic_rating,is_action,is_exclusive_to_us,is_portable,is_role_playing,is_sequel,is_sports,suitable_for_kids,total_earnings,unit_price
    0.4999999999999999,1.0,0.0,1.0,0.0,1.0,0.0,0.0,0.3747139609249367,1.0
    0.8333333333333334,0.0,0.0,0.0,0.0,1.0,1.0,0.0,0.19242527864549636,0.5
    0.33333333333333337,0.0,0.0,0.0,0.0,1.0,1.0,0.0,0.11485185116726125,0.5
    0.8333333333333334,1.0,0.0,0.0,0.0,0.0,0.0,1.0,0.14245208036820023,0.0
    0.6666666666666666,1.0,0.0,1.0,0.0,1.0,0.0,1.0,0.4806824273118796,1.0
    0.6666666666666666,0.0,0.0,0.0,1.0,1.0,0.0,0.0,0.13972015304707863,0.0
    0.0,1.0,0.0,0.0,0.0,1.0,0.0,0.0,0.11338792258923126,0.5
    0.8333333333333334,0.0,0.0,1.0,1.0,1.0,0.0,1.0,0.44906748488937354,1.0
    0.4999999999999999,0.0,0.0,1.0,0.0,0.0,1.0,0.0,0.06127428328496702,0.0
    0.16666666666666663,1.0,0.0,0.0,0.0,1.0,0.0,0.0,0.20668009833459638,1.0
    0.8333333333333334,1.0,0.0,1.0,0.0,1.0,0.0,0.0,0.4777545701558197,1.0
    0.4999999999999999,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.13232657437015954,1.0
    0.4999999999999999,0.0,0.0,0.0,1.0,1.0,0.0,0.0,0.17925361823256503,0.5
    0.6666666666666666,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.16335742407718895,1.0
    0.4999999999999999,0.0,0.0,0.0,1.0,1.0,0.0,0.0,0.23946692297739414,1.0
    0.6666666666666666,0.0,0.0,0.0,1.0,1.0,0.0,1.0,0.31206816879540117,1.0
    0.4999999999999999,1.0,0.0,0.0,0.0,1.0,0.0,0.0,0.2285281233248923,0.5
    0.33333333333333337,1.0,0.0,0.0,0.0,1.0,0.0,1.0,0.29274505092327313,1.0
    0.16666666666666663,1.0,1.0,0.0,0.0,1.0,0.0,0.0,0.3480564130053049,0.5
    0.9999999999999999,1.0,0.0,0.0,0.0,1.0,0.0,0.0,0.42799208887081575,1.0
    0.6666666666666666,0.0,0.0,0.0,0.0,1.0,1.0,0.0,0.2248313339864328,1.0
    0.8333333333333334,0.0,0.0,0.0,0.0,0.0,1.0,1.0,0.1280235115801926,0.5
    0.16666666666666663,0.0,1.0,0.0,1.0,1.0,0.0,1.0,0.40164507125561455,1.0
    0.6666666666666666,1.0,0.0,0.0,0.0,1.0,0.0,1.0,0.2963420269495943,0.5
    0.8333333333333334,1.0,0.0,0.0,0.0,1.0,0.0,1.0,0.2486090830114046,0.0
    0.8333333333333334,0.0,0.0,0.0,0.0,1.0,1.0,1.0,0.28923310105173655,1.0
    0.0,0.0,0.0,0.0,1.0,1.0,0.0,1.0,0.06283432838579693,0.0
    0.4999999999999999,1.0,0.0,1.0,0.0,1.0,0.0,0.0,0.29536607456424097,0.5
    0.4999999999999999,0.0,0.0,0.0,1.0,1.0,0.0,1.0,0.2693258904641319,1.0
    0.0,1.0,0.0,1.0,0.0,0.0,0.0,1.0,0.10460804791038984,0.5
    0.4999999999999999,1.0,1.0,0.0,0.0,1.0,0.0,0.0,0.4651485185116726,0.5
",0,TensorBoard crashes on second model run,"TensorBoard crashes on second model run TensorBoard in Keras crashes deep in the callbacks for the end-of-epoch. Here's a simplified version of the code:

# The Code
    import pandas as pd
    import keras
    from keras.models import Sequential
    from keras.layers import Dense

    class ToyNet ():
        def __init__(self, run=1, layer_1_nodes=50, layer_2_nodes=100, layer_3_nodes=50):
            self.run_seq = run
            self.layer_1_nodes = layer_1_nodes
            self.layer_2_nodes = layer_2_nodes
            self.layer_3_nodes = layer_3_nodes

            # Define the model
            self.model = Sequential()
            self.model.add(Dense(self.layer_1_nodes, input_dim=9, activation='relu', name='layer_1'))
            self.model.add(Dense(self.layer_2_nodes, activation='relu', name='layer_2'))
            self.model.add(Dense(self.layer_3_nodes, activation='relu', name='layer_3'))
            self.model.add(Dense(1, activation='linear', name='output_layer'))
            self.model.compile(loss='mean_squared_error', optimizer='adam')

            # Create a TensorBoard logger
            log_dir = ""logs_{}"".format(self.run_seq)
            self.logger = keras.callbacks.TensorBoard(
                log_dir=log_dir,
                histogram_freq=5
            )

        def train(self, X, Y, epochs=50):
            # Train the model
            self.model.fit(
                X,
                Y,
                epochs=epochs,
                shuffle=True,
                verbose=2,
                validation_split=0.05,
                callbacks=[self.logger]
            )

        def test(self, Xt, Yt):
            test_error_rate = self.model.evaluate(Xt, Yt, verbose=0)
            print(""The mean squared error (MSE) for the test data set is: {}"".format(test_error_rate))

    if __name__ == '__main__':

        training_data_df = pd.read_csv(""sales_data_training_scaled.csv"")

        X = training_data_df.drop('total_earnings', axis=1).values
        Y = training_data_df[['total_earnings']].values

        # Load the test data set
        test_data_df = pd.read_csv(""sales_data_test_scaled.csv"")

        X_test = test_data_df.drop('total_earnings', axis=1).values
        Y_test = test_data_df[['total_earnings']].values

        print(""Run #1"")
        toy1 = ToyNet(1, 50, 100, 50)
        toy1.train(X, Y)
        toy1.test(X_test, Y_test)

        print(""Run #2"")
        toy2 = ToyNet(2, 5, 100, 50)
        toy2.train(X,Y)              ### <--- Crashes here at the end of first epoch while doing callbacks
        toy2.test(X_test, Y_test)

# The Output

/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from  to  is deprecated. In future, it will be treated as .
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
Run #1
Train on 950 samples, validate on 50 samples
Epoch 1/50
 - 1s - loss: 0.0314 - val_loss: 0.0043
Epoch 2/50
 - 0s - loss: 0.0048 - val_loss: 0.0011

> ... output snipped for brevity

Epoch 50/50
 - 0s - loss: 2.4237e-05 - val_loss: 5.0361e-05
The mean squared error (MSE) for the test data set is: 7.575784547952935e-05
Run #2
Train on 950 samples, validate on 50 samples
Epoch 1/50
 - 1s - loss: 0.0333 - val_loss: 0.0172
---------------------------------------------------------------------------
InvalidArgumentError                      Traceback (most recent call last)
/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py in _do_call(self, fn, *args)
   1321     try:
-> 1322       return fn(*args)
   1323     except errors.OpError as e:

/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py in _run_fn(feed_dict, fetch_list, target_list, options, run_metadata)
   1306       return self._call_tf_sessionrun(
-> 1307           options, feed_dict, fetch_list, target_list, run_metadata)
   1308 

/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py in _call_tf_sessionrun(self, options, feed_dict, fetch_list, target_list, run_metadata)
   1408           self._session, options, feed_dict, fetch_list, target_list,
-> 1409           run_metadata)
   1410     else:

InvalidArgumentError: You must feed a value for placeholder tensor 'layer_1_input' with dtype float and shape [?,9]
	 [[Node: layer_1_input = Placeholder[dtype=DT_FLOAT, shape=[?,9], _device=""/job:localhost/replica:0/task:0/device:CPU:0""]()]]

During handling of the above exception, another exception occurred:

InvalidArgumentError                      Traceback (most recent call last)
<ipython-input-1-bb0bb05e17b3> in <module>()
     64 
     65     toy2 = ToyNet(2, 5, 100, 50)
---> 66     toy2.train(X,Y)              ### <--- Crashes here at the end of first epoch while doing callbacks
     67     toy2.test(X_test, Y_test)

<ipython-input-1-bb0bb05e17b3> in train(self, X, Y, epochs)
     39             verbose=2,
     40             validation_split=0.05,
---> 41             callbacks=[self.logger]
     42         )
     43 

/anaconda3/lib/python3.6/site-packages/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)
   1043                                         initial_epoch=initial_epoch,
   1044                                         steps_per_epoch=steps_per_epoch,
-> 1045                                         validation_steps=validation_steps)
   1046 
   1047     def evaluate(self, x=None, y=None,

/anaconda3/lib/python3.6/site-packages/keras/engine/training_arrays.py in fit_loop(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)
    215                         for l, o in zip(out_labels, val_outs):
    216                             epoch_logs['val_' + l] = o
--> 217         callbacks.on_epoch_end(epoch, epoch_logs)
    218         if callback_model.stop_training:
    219             break

/anaconda3/lib/python3.6/site-packages/keras/callbacks.py in on_epoch_end(self, epoch, logs)
     75         logs = logs or {}
     76         for callback in self.callbacks:
---> 77             callback.on_epoch_end(epoch, logs)
     78 
     79     def on_batch_begin(self, batch, logs=None):

/anaconda3/lib/python3.6/site-packages/keras/callbacks.py in on_epoch_end(self, epoch, logs)
    915                     assert len(batch_val) == len(tensors)
    916                     feed_dict = dict(zip(tensors, batch_val))
--> 917                     result = self.sess.run([self.merged], feed_dict=feed_dict)
    918                     summary_str = result[0]
    919                     self.writer.add_summary(summary_str, epoch)

/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py in run(self, fetches, feed_dict, options, run_metadata)
    898     try:
    899       result = self._run(None, fetches, feed_dict, options_ptr,
--> 900                          run_metadata_ptr)
    901       if run_metadata:
    902         proto_data = tf_session.TF_GetBuffer(run_metadata_ptr)

/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py in _run(self, handle, fetches, feed_dict, options, run_metadata)
   1133     if final_fetches or final_targets or (handle and feed_dict_tensor):
   1134       results = self._do_run(handle, final_targets, final_fetches,
-> 1135                              feed_dict_tensor, options, run_metadata)
   1136     else:
   1137       results = []

/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py in _do_run(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)
   1314     if handle is None:
   1315       return self._do_call(_run_fn, feeds, fetches, targets, options,
-> 1316                            run_metadata)
   1317     else:
   1318       return self._do_call(_prun_fn, handle, feeds, fetches)

/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py in _do_call(self, fn, *args)
   1333         except KeyError:
   1334           pass
-> 1335       raise type(e)(node_def, op, message)
   1336 
   1337   def _extend_graph(self):

InvalidArgumentError: You must feed a value for placeholder tensor 'layer_1_input' with dtype float and shape [?,9]
	 [[Node: layer_1_input = Placeholder[dtype=DT_FLOAT, shape=[?,9], _device=""/job:localhost/replica:0/task:0/device:CPU:0""]()]]

Caused by op 'layer_1_input', defined at:
  File ""/anaconda3/lib/python3.6/runpy.py"", line 193, in _run_module_as_main
    ""__main__"", mod_spec)
  File ""/anaconda3/lib/python3.6/runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py"", line 16, in <module>
    app.launch_new_instance()
  File ""/anaconda3/lib/python3.6/site-packages/traitlets/config/application.py"", line 658, in launch_instance
    app.start()
  File ""/anaconda3/lib/python3.6/site-packages/ipykernel/kernelapp.py"", line 486, in start
    self.io_loop.start()
  File ""/anaconda3/lib/python3.6/site-packages/tornado/platform/asyncio.py"", line 127, in start
    self.asyncio_loop.run_forever()
  File ""/anaconda3/lib/python3.6/asyncio/base_events.py"", line 422, in run_forever
    self._run_once()
  File ""/anaconda3/lib/python3.6/asyncio/base_events.py"", line 1432, in _run_once
    handle._run()
  File ""/anaconda3/lib/python3.6/asyncio/events.py"", line 145, in _run
    self._callback(*self._args)
  File ""/anaconda3/lib/python3.6/site-packages/tornado/platform/asyncio.py"", line 117, in _handle_events
    handler_func(fileobj, events)
  File ""/anaconda3/lib/python3.6/site-packages/tornado/stack_context.py"", line 276, in null_wrapper
    return fn(*args, **kwargs)
  File ""/anaconda3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py"", line 450, in _handle_events
    self._handle_recv()
  File ""/anaconda3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py"", line 480, in _handle_recv
    self._run_callback(callback, msg)
  File ""/anaconda3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py"", line 432, in _run_callback
    callback(*args, **kwargs)
  File ""/anaconda3/lib/python3.6/site-packages/tornado/stack_context.py"", line 276, in null_wrapper
    return fn(*args, **kwargs)
  File ""/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py"", line 283, in dispatcher
    return self.dispatch_shell(stream, msg)
  File ""/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py"", line 233, in dispatch_shell
    handler(stream, idents, msg)
  File ""/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py"", line 399, in execute_request
    user_expressions, allow_stdin)
  File ""/anaconda3/lib/python3.6/site-packages/ipykernel/ipkernel.py"", line 208, in do_execute
    res = shell.run_cell(code, store_history=store_history, silent=silent)
  File ""/anaconda3/lib/python3.6/site-packages/ipykernel/zmqshell.py"", line 537, in run_cell
    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)
  File ""/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py"", line 2662, in run_cell
    raw_cell, store_history, silent, shell_futures)
  File ""/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py"", line 2785, in _run_cell
    interactivity=interactivity, compiler=compiler, result=result)
  File ""/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py"", line 2903, in run_ast_nodes
    if self.run_code(code, result):
  File ""/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py"", line 2963, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  File ""<ipython-input-1-bb0bb05e17b3>"", line 61, in <module>
    toy1 = ToyNet(1, 50, 100, 50)
  File ""<ipython-input-1-bb0bb05e17b3>"", line 19, in __init__
    self.model.add(Dense(self.layer_1_nodes, input_dim=9, activation='relu', name='layer_1'))
  File ""/anaconda3/lib/python3.6/site-packages/keras/engine/sequential.py"", line 160, in add
    name=layer.name + '_input')
  File ""/anaconda3/lib/python3.6/site-packages/keras/engine/input_layer.py"", line 178, in Input
    input_tensor=tensor)
  File ""/anaconda3/lib/python3.6/site-packages/keras/legacy/interfaces.py"", line 91, in wrapper
    return func(*args, **kwargs)
  File ""/anaconda3/lib/python3.6/site-packages/keras/engine/input_layer.py"", line 87, in __init__
    name=self.name)
  File ""/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py"", line 517, in placeholder
    x = tf.placeholder(dtype, shape=shape, name=name)
  File ""/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py"", line 1734, in placeholder
    return gen_array_ops.placeholder(dtype=dtype, shape=shape, name=name)
  File ""/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gen_array_ops.py"", line 4924, in placeholder
    ""Placeholder"", dtype=dtype, shape=shape, name=name)
  File ""/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py"", line 787, in _apply_op_helper
    op_def=op_def)
  File ""/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 3414, in create_op
    op_def=op_def)
  File ""/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 1740, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

InvalidArgumentError (see above for traceback): You must feed a value for placeholder tensor 'layer_1_input' with dtype float and shape [?,9]
	 [[Node: layer_1_input = Placeholder[dtype=DT_FLOAT, shape=[?,9], _device=""/job:localhost/replica:0/task:0/device:CPU:0""]()]]

# The Data

For sanity's sake I've only included a short sample of each training and testing data below.

## Training data (""sales_data_training_scaled.csv"")
    critic_rating,is_action,is_exclusive_to_us,is_portable,is_role_playing,is_sequel,is_sports,suitable_for_kids,total_earnings,unit_price
    0.4999999999999999,1.0,1.0,1.0,0.0,1.0,0.0,1.0,0.7991793127668619,1.0
    0.16666666666666663,0.0,0.0,0.0,1.0,1.0,0.0,0.0,0.15750170976506905,1.0
    0.4999999999999999,0.0,0.0,0.0,0.0,1.0,1.0,0.0,0.18970444169239015,1.0
    0.6666666666666666,1.0,1.0,0.0,0.0,1.0,0.0,0.0,0.39223304559989647,0.0
    0.0,1.0,0.0,1.0,0.0,1.0,0.0,0.0,0.21546366980277626,1.0
    0.4999999999999999,1.0,0.0,1.0,0.0,0.0,0.0,1.0,0.2675699155283636,1.0
    0.6666666666666666,1.0,0.0,0.0,0.0,0.0,0.0,1.0,0.2418106874179775,1.0
    0.4999999999999999,1.0,1.0,1.0,0.0,1.0,0.0,0.0,0.7090183175911721,1.0
    0.6666666666666666,1.0,1.0,0.0,0.0,0.0,0.0,0.0,0.4385279384854254,1.0
    0.16666666666666663,0.0,1.0,0.0,1.0,1.0,0.0,1.0,0.3957893569434946,1.0
    0.9999999999999999,1.0,0.0,0.0,0.0,1.0,0.0,0.0,0.24197334614886973,0.0
    0.6666666666666666,1.0,0.0,0.0,0.0,1.0,0.0,0.0,0.33607142197001905,1.0
    0.4999999999999999,0.0,1.0,0.0,0.0,0.0,1.0,0.0,0.26054601578529046,1.0
    0.6666666666666666,0.0,0.0,1.0,1.0,1.0,0.0,0.0,0.3501229182455038,1.0
    0.8333333333333334,0.0,1.0,1.0,0.0,1.0,1.0,0.0,0.39457681004047984,0.0
    0.6666666666666666,0.0,0.0,0.0,0.0,1.0,1.0,0.0,0.2248313339864328,1.0
    0.4999999999999999,0.0,0.0,1.0,0.0,1.0,1.0,1.0,0.28864900833625995,1.0
    0.4999999999999999,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.05268664165172547,0.0
    0.9999999999999999,1.0,0.0,0.0,0.0,1.0,0.0,0.0,0.24431711058945305,0.0
    0.16666666666666663,1.0,0.0,1.0,0.0,1.0,0.0,0.0,0.2740097225559601,1.0
    0.33333333333333337,0.0,0.0,0.0,1.0,1.0,0.0,0.0,0.20141217352729157,1.0
    0.4999999999999999,0.0,1.0,0.0,1.0,1.0,0.0,0.0,0.2802240254339107,0.0
    0.16666666666666663,0.0,1.0,0.0,1.0,1.0,0.0,1.0,0.4069129960629193,1.0
    0.6666666666666666,0.0,0.0,1.0,1.0,0.0,0.0,0.0,0.21487957708729966,1.0
    0.4999999999999999,1.0,0.0,1.0,0.0,1.0,0.0,1.0,0.4250642317147557,1.0
    0.4999999999999999,0.0,0.0,1.0,0.0,1.0,1.0,1.0,0.28513336167538494,1.0
    0.33333333333333337,1.0,1.0,0.0,0.0,1.0,0.0,0.0,0.40075044823570727,0.5
    0.4999999999999999,1.0,0.0,0.0,0.0,1.0,0.0,1.0,0.18811482227685256,0.0
    0.33333333333333337,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.1428661207741077,1.0
    0.8333333333333334,1.0,0.0,1.0,0.0,0.0,0.0,1.0,0.27292286649045305,0.5
    0.16666666666666663,0.0,0.0,0.0,1.0,1.0,0.0,0.0,0.16160144914142066,1.0
    0.4999999999999999,1.0,1.0,0.0,0.0,1.0,0.0,0.0,0.5831426406166245,1.0
    0.6666666666666666,1.0,0.0,0.0,0.0,1.0,0.0,0.0,0.18850668194672926,0.0
    0.8333333333333334,0.0,1.0,0.0,1.0,1.0,0.0,0.0,0.6118297258830706,1.0
    0.9999999999999999,0.0,0.0,0.0,1.0,1.0,0.0,0.0,0.18928670449714424,0.0
    0.4999999999999999,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.13759449917746436,1.0
    0.9999999999999999,0.0,1.0,0.0,0.0,1.0,1.0,0.0,0.34539842147095245,0.0
    0.16666666666666663,0.0,0.0,0.0,1.0,1.0,0.0,1.0,0.13583113066301916,0.5
    0.9999999999999999,0.0,1.0,1.0,0.0,1.0,1.0,1.0,0.48629415352766125,0.0
    0.9999999999999999,0.0,1.0,0.0,1.0,1.0,0.0,1.0,0.7500009241973347,1.0
    0.0,0.0,0.0,0.0,1.0,1.0,0.0,0.0,0.04253895491765401,0.0
    0.4999999999999999,0.0,0.0,0.0,1.0,1.0,0.0,0.0,0.11630468937727585,0.0
    0.16666666666666663,1.0,1.0,0.0,0.0,0.0,0.0,0.0,0.2775253692168352,1.0
    0.4999999999999999,0.0,0.0,1.0,0.0,0.0,1.0,0.0,0.10216816694700652,0.5
    0.4999999999999999,0.0,1.0,0.0,0.0,1.0,1.0,0.0,0.23650949150662648,0.0
    0.33333333333333337,1.0,0.0,1.0,0.0,1.0,0.0,0.0,0.18031089998336447,0.0
    0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.05621337868061589,1.0
    0.9999999999999999,1.0,1.0,1.0,0.0,1.0,0.0,0.0,0.7949538825530027,0.5
    0.4999999999999999,1.0,0.0,0.0,0.0,1.0,0.0,1.0,0.18538289495573096,0.0
    0.8333333333333334,0.0,0.0,0.0,0.0,1.0,1.0,0.0,0.2587900408495222,1.0
    0.16666666666666663,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.08373227851610877,1.0
    0.9999999999999999,1.0,1.0,1.0,0.0,0.0,0.0,1.0,0.7447329993900298,1.0
    0.16666666666666663,1.0,0.0,0.0,0.0,1.0,0.0,0.0,0.20902386277517976,1.0
    0.6666666666666666,1.0,0.0,0.0,0.0,1.0,0.0,0.0,0.18850668194672926,0.0
    0.16666666666666663,0.0,0.0,1.0,0.0,1.0,1.0,0.0,0.12509565442413267,0.5
    0.6666666666666666,1.0,0.0,0.0,0.0,1.0,0.0,0.0,0.3313875898781908,1.0
    0.33333333333333337,1.0,0.0,1.0,0.0,1.0,0.0,1.0,0.3717861037688767,1.0
    0.0,0.0,0.0,0.0,1.0,1.0,0.0,1.0,0.1434502134895843,1.0
    0.33333333333333337,1.0,1.0,1.0,0.0,1.0,0.0,0.0,0.6329051219016284,1.0
    0.8333333333333334,1.0,1.0,0.0,0.0,1.0,0.0,0.0,0.727753645958485,1.0
    0.6666666666666666,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.15144267203933381,0.5
    0.8333333333333334,1.0,1.0,1.0,0.0,1.0,0.0,0.0,0.8846601726400621,1.0
    0.8333333333333334,0.0,0.0,0.0,1.0,1.0,0.0,0.0,0.30738433670357296,1.0

## Testing data (""sales_data_test_scaled.csv"")
    critic_rating,is_action,is_exclusive_to_us,is_portable,is_role_playing,is_sequel,is_sports,suitable_for_kids,total_earnings,unit_price
    0.4999999999999999,1.0,0.0,1.0,0.0,1.0,0.0,0.0,0.3747139609249367,1.0
    0.8333333333333334,0.0,0.0,0.0,0.0,1.0,1.0,0.0,0.19242527864549636,0.5
    0.33333333333333337,0.0,0.0,0.0,0.0,1.0,1.0,0.0,0.11485185116726125,0.5
    0.8333333333333334,1.0,0.0,0.0,0.0,0.0,0.0,1.0,0.14245208036820023,0.0
    0.6666666666666666,1.0,0.0,1.0,0.0,1.0,0.0,1.0,0.4806824273118796,1.0
    0.6666666666666666,0.0,0.0,0.0,1.0,1.0,0.0,0.0,0.13972015304707863,0.0
    0.0,1.0,0.0,0.0,0.0,1.0,0.0,0.0,0.11338792258923126,0.5
    0.8333333333333334,0.0,0.0,1.0,1.0,1.0,0.0,1.0,0.44906748488937354,1.0
    0.4999999999999999,0.0,0.0,1.0,0.0,0.0,1.0,0.0,0.06127428328496702,0.0
    0.16666666666666663,1.0,0.0,0.0,0.0,1.0,0.0,0.0,0.20668009833459638,1.0
    0.8333333333333334,1.0,0.0,1.0,0.0,1.0,0.0,0.0,0.4777545701558197,1.0
    0.4999999999999999,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.13232657437015954,1.0
    0.4999999999999999,0.0,0.0,0.0,1.0,1.0,0.0,0.0,0.17925361823256503,0.5
    0.6666666666666666,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.16335742407718895,1.0
    0.4999999999999999,0.0,0.0,0.0,1.0,1.0,0.0,0.0,0.23946692297739414,1.0
    0.6666666666666666,0.0,0.0,0.0,1.0,1.0,0.0,1.0,0.31206816879540117,1.0
    0.4999999999999999,1.0,0.0,0.0,0.0,1.0,0.0,0.0,0.2285281233248923,0.5
    0.33333333333333337,1.0,0.0,0.0,0.0,1.0,0.0,1.0,0.29274505092327313,1.0
    0.16666666666666663,1.0,1.0,0.0,0.0,1.0,0.0,0.0,0.3480564130053049,0.5
    0.9999999999999999,1.0,0.0,0.0,0.0,1.0,0.0,0.0,0.42799208887081575,1.0
    0.6666666666666666,0.0,0.0,0.0,0.0,1.0,1.0,0.0,0.2248313339864328,1.0
    0.8333333333333334,0.0,0.0,0.0,0.0,0.0,1.0,1.0,0.1280235115801926,0.5
    0.16666666666666663,0.0,1.0,0.0,1.0,1.0,0.0,1.0,0.40164507125561455,1.0
    0.6666666666666666,1.0,0.0,0.0,0.0,1.0,0.0,1.0,0.2963420269495943,0.5
    0.8333333333333334,1.0,0.0,0.0,0.0,1.0,0.0,1.0,0.2486090830114046,0.0
    0.8333333333333334,0.0,0.0,0.0,0.0,1.0,1.0,1.0,0.28923310105173655,1.0
    0.0,0.0,0.0,0.0,1.0,1.0,0.0,1.0,0.06283432838579693,0.0
    0.4999999999999999,1.0,0.0,1.0,0.0,1.0,0.0,0.0,0.29536607456424097,0.5
    0.4999999999999999,0.0,0.0,0.0,1.0,1.0,0.0,1.0,0.2693258904641319,1.0
    0.0,1.0,0.0,1.0,0.0,0.0,0.0,1.0,0.10460804791038984,0.5
    0.4999999999999999,1.0,1.0,0.0,0.0,1.0,0.0,0.0,0.4651485185116726,0.5
"
keras,7755,"I use the following code to export keras model to tensorflow.



Im getting the following error while using the generated pb file.

> InvalidArgumentError (see above for traceback): You must feed a value for placeholder tensor 'batch_normalization_1/keras_learning_phase' with dtype bool
> 	 [[Node: batch_normalization_1/keras_learning_phase = Placeholder[dtype=DT_BOOL, shape=<unknown>, _device=""/job:localhost/replica:0/task:0/cpu:0""]()]]
",0,InvalidArgumentError ,"InvalidArgumentError  I use the following code to export keras model to tensorflow.



Im getting the following error while using the generated pb file.

> InvalidArgumentError (see above for traceback): You must feed a value for placeholder tensor 'batch_normalization_1/keras_learning_phase' with dtype bool
> 	 [[Node: batch_normalization_1/keras_learning_phase = Placeholder[dtype=DT_BOOL, shape=<unknown>, _device=""/job:localhost/replica:0/task:0/cpu:0""]()]]
"
keras,10965,"If  is being used with an RNN (e.g. LSTM), if you pass in an initial_state (either with  kwarg or as a list of tensors in the argument when calling the layer (), the initial state overrides the stored states in the stateful RNN.",0,RNN stateful is incompatible with initial_state,"RNN stateful is incompatible with initial_state If  is being used with an RNN (e.g. LSTM), if you pass in an initial_state (either with  kwarg or as a list of tensors in the argument when calling the layer (), the initial state overrides the stored states in the stateful RNN."
keras,13391,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- TensorFlow installed from (source or binary): built from source
- TensorFlow version (use command below): 2.0.0 (i.e. the release)
- Keras version: 2.3.0
- Python version: 3.7 conda
- Bazel version (if compiling Tensorflow from source): 0.26.1
- GCC/Compiler version (if compiling Tensorflow from source): 7.4.0
- CUDA/cuDNN version: 10 / 7.6.4
- GPU model and memory: RTX 2080 Ti and Tesla V100 (tried on both. error occurs on both)

**Describe the current behavior**
I am going through Francois Chollet's book ""Deep Learning with Python"" and running the code in his Jupyter Notebooks with Tensorflow 2.0.0 as a backend to Keras 2.3.0.  Notebook 6.3, (under the heading ""1.6 Using recurrent dropout to fight overfitting"") has a model with a tensorflow.keras.layers.GRU(32, dropout=0.2, recurrent_dropout=0.2, input_shape=(None, float_data.shape[-1])).  The data is read earlier in the notebook from jena_climate_2009_2016.csv. I get a loss of 699013271268870062080.0000 after the first epoch and similar figures after subsequent epochs.  This figure is simply wrong (see below).  The original notebook (from Francois Chollet) is here: [link to github](https://github.com/fchollet/deep-learning-with-python-notebooks/blob/master/6.3-advanced-usage-of-recurrent-neural-networks.ipynb) and includes the correct output.

**Describe the expected behavior**
The loss after 1 or 2 epochs is supposed to be around 0.3 

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
Download the data as follows:
cd ~
mkdir Datasets
cd ~/Datasets
mkdir jena_climate
cd jena_climate
wget https://s3.amazonaws.com/keras-datasets/jena_climate_2009_2016.csv.zip
unzip jena_climate_2009_2016.csv.zip

Run jupyter notebook and load the notebook in a Python 3.7 environment with tensorflow 2.0.0 as the backend and keras 2.30.
Run each cell from the beginning of the notebook so you load the data and create the generators before you get to the example under heading 1.6.  Then try to run the example.  You will find that the loss is terribly wrong.

The code under heading ""1.7 Stacking recurrent layers"" also runs incorrectly.  The loss produced is ""nan"" and val_loss is ""nan"" (both should be around 0.3).  I think it is the same problem with layers.GRU

I have reproduced this problem running tensorflow.keras in tensorflow 2.0.0.
The problem also occurs running Keras 2.3.0 with a tensorflow 1.1.4 backend.
The problem does **not** occur with tensorflow.keras in tensorflow 1.1.4.",0,[tf2.0.0 keras2.3.0] keras.layers.GRU incorrect output of model.fit_generator trying to run Francois Chollet's notebook #32987,"[tf2.0.0 keras2.3.0] keras.layers.GRU incorrect output of model.fit_generator trying to run Francois Chollet's notebook #32987 **System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- TensorFlow installed from (source or binary): built from source
- TensorFlow version (use command below): 2.0.0 (i.e. the release)
- Keras version: 2.3.0
- Python version: 3.7 conda
- Bazel version (if compiling Tensorflow from source): 0.26.1
- GCC/Compiler version (if compiling Tensorflow from source): 7.4.0
- CUDA/cuDNN version: 10 / 7.6.4
- GPU model and memory: RTX 2080 Ti and Tesla V100 (tried on both. error occurs on both)

**Describe the current behavior**
I am going through Francois Chollet's book ""Deep Learning with Python"" and running the code in his Jupyter Notebooks with Tensorflow 2.0.0 as a backend to Keras 2.3.0.  Notebook 6.3, (under the heading ""1.6 Using recurrent dropout to fight overfitting"") has a model with a tensorflow.keras.layers.GRU(32, dropout=0.2, recurrent_dropout=0.2, input_shape=(None, float_data.shape[-1])).  The data is read earlier in the notebook from jena_climate_2009_2016.csv. I get a loss of 699013271268870062080.0000 after the first epoch and similar figures after subsequent epochs.  This figure is simply wrong (see below).  The original notebook (from Francois Chollet) is here: [link to github](https://github.com/fchollet/deep-learning-with-python-notebooks/blob/master/6.3-advanced-usage-of-recurrent-neural-networks.ipynb) and includes the correct output.

**Describe the expected behavior**
The loss after 1 or 2 epochs is supposed to be around 0.3 

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
Download the data as follows:
cd ~
mkdir Datasets
cd ~/Datasets
mkdir jena_climate
cd jena_climate
wget https://s3.amazonaws.com/keras-datasets/jena_climate_2009_2016.csv.zip
unzip jena_climate_2009_2016.csv.zip

Run jupyter notebook and load the notebook in a Python 3.7 environment with tensorflow 2.0.0 as the backend and keras 2.30.
Run each cell from the beginning of the notebook so you load the data and create the generators before you get to the example under heading 1.6.  Then try to run the example.  You will find that the loss is terribly wrong.

The code under heading ""1.7 Stacking recurrent layers"" also runs incorrectly.  The loss produced is ""nan"" and val_loss is ""nan"" (both should be around 0.3).  I think it is the same problem with layers.GRU

I have reproduced this problem running tensorflow.keras in tensorflow 2.0.0.
The problem also occurs running Keras 2.3.0 with a tensorflow 1.1.4 backend.
The problem does **not** occur with tensorflow.keras in tensorflow 1.1.4."
keras,10784,"Loading of nested models including BatchNormalisation and 1D convolutions fail with the following traceback:



I have been able to trim it down to a minimum example:

https://gist.github.com/Dapid/cd1be6e9d9d8d61f7f225544f0967e35

Inspecting the shapes, it seems like it is trying to load the bias vector of a conv1d on both the bias and kernel, and of course the shapes don't match. I have been unable to trace it back to the original place.
The model was trained on Tensorflow.

Possibly related:
https://github.com/keras-team/keras/issues/10777
Also, inspecting the shapes of the loading weights, it is possible that some batch normalisation is not being loaded, which would explain https://github.com/keras-team/keras/issues/10780 among others.",0,Wrong loading of weights on nested models,"Wrong loading of weights on nested models Loading of nested models including BatchNormalisation and 1D convolutions fail with the following traceback:



I have been able to trim it down to a minimum example:

https://gist.github.com/Dapid/cd1be6e9d9d8d61f7f225544f0967e35

Inspecting the shapes, it seems like it is trying to load the bias vector of a conv1d on both the bias and kernel, and of course the shapes don't match. I have been unable to trace it back to the original place.
The model was trained on Tensorflow.

Possibly related:
https://github.com/keras-team/keras/issues/10777
Also, inspecting the shapes of the loading weights, it is possible that some batch normalisation is not being loaded, which would explain https://github.com/keras-team/keras/issues/10780 among others."
keras,10938,"I'm trying to build a seq2seq model that generates sentences and classifies them. To do the latter, I want to feed the decoder's softmax output to a pre-trained classifier. This classifier however uses a Keras embedding layer so passing the raw softmax into the classifier isn't an option. I thought I could use a gumbel softmax to get a one-hot encoding then use the OneHotEmbedding layer I found here (https://github.com/keras-team/keras/issues/2505) to solve this.

Eric Jang provided this TensorFlow code for the gumbel softmax and I wanted to know how to turn this into a Keras layer. In particular, I am interested in the  property that ensures the vector on the forward pass is strictly categorical but on the backward pass, the gradient is the gumbel softmax output. Can anyone help with this please? 

Thanks.",0,gumbel softmax in keras,"gumbel softmax in keras I'm trying to build a seq2seq model that generates sentences and classifies them. To do the latter, I want to feed the decoder's softmax output to a pre-trained classifier. This classifier however uses a Keras embedding layer so passing the raw softmax into the classifier isn't an option. I thought I could use a gumbel softmax to get a one-hot encoding then use the OneHotEmbedding layer I found here (https://github.com/keras-team/keras/issues/2505) to solve this.

Eric Jang provided this TensorFlow code for the gumbel softmax and I wanted to know how to turn this into a Keras layer. In particular, I am interested in the  property that ensures the vector on the forward pass is strictly categorical but on the backward pass, the gradient is the gumbel softmax output. Can anyone help with this please? 

Thanks."
keras,12383,"Please make sure that the boxes below are checked before you submit your issue.
If your issue is an **implementation question**, please ask your question on [StackOverflow](http://stackoverflow.com/questions/tagged/keras) or [on the Keras Slack channel](https://keras-slack-autojoin.herokuapp.com/) instead of opening a GitHub issue.

Thank you!

- [x] Check that you are up-to-date with the master branch of Keras. You can update with:


- [x] Check that your version of TensorFlow is up-to-date. The installation instructions can be found [here](https://www.tensorflow.org/get_started/os_setup).

- [x] Provide a link to a GitHub Gist of a Python script that can reproduce your issue (or just copy the script here if it is short).
",0,keras : with session.graph.as_default(): AttributeError: 'NoneType' object has no attribute 'graph',"keras : with session.graph.as_default(): AttributeError: 'NoneType' object has no attribute 'graph' Please make sure that the boxes below are checked before you submit your issue.
If your issue is an **implementation question**, please ask your question on [StackOverflow](http://stackoverflow.com/questions/tagged/keras) or [on the Keras Slack channel](https://keras-slack-autojoin.herokuapp.com/) instead of opening a GitHub issue.

Thank you!

- [x] Check that you are up-to-date with the master branch of Keras. You can update with:


- [x] Check that your version of TensorFlow is up-to-date. The installation instructions can be found [here](https://www.tensorflow.org/get_started/os_setup).

- [x] Provide a link to a GitHub Gist of a Python script that can reproduce your issue (or just copy the script here if it is short).
"
keras,13504,"Hi,

I work on the [Waymo Open Dataset](https://waymo.com/open/). We'd like to make it as easy as possible for users to get up and running with it.

How might we go about adding it to keras.datasets?

Thank you!",0,Adding the Waymo Open Dataset to keras.datasets,"Adding the Waymo Open Dataset to keras.datasets Hi,

I work on the [Waymo Open Dataset](https://waymo.com/open/). We'd like to make it as easy as possible for users to get up and running with it.

How might we go about adding it to keras.datasets?

Thank you!"
keras,8065,"- [x] Check that you are up-to-date with the master branch of Keras. You can update with:
pip install git+git://github.com/fchollet/keras.git --upgrade --no-deps

- [X] If running on TensorFlow, check that you are up-to-date with the latest version. The installation instructions can be found [here](https://www.tensorflow.org/get_started/os_setup).

- [X] Provide a link to a GitHub Gist of a Python script that can reproduce your issue (or just copy the script here if it is short).

I'm excluding the code where I load and preprocess (imagenetutils.preprocess_input). My data is the Kaggle cats-and-dogs data.



This never gets beyond 50-60% accuracy. VGG16 quickly gets to 98%+. If I remove ""tensorflow.contrib.keras.api."" from the start of all imports, this also gets to 98%+. So something is definitely different between tf and base Keras. I'm just not sure what.",0,"ResNet50 from tf.keras will not fine-tune, but the one from base keras will","ResNet50 from tf.keras will not fine-tune, but the one from base keras will - [x] Check that you are up-to-date with the master branch of Keras. You can update with:
pip install git+git://github.com/fchollet/keras.git --upgrade --no-deps

- [X] If running on TensorFlow, check that you are up-to-date with the latest version. The installation instructions can be found [here](https://www.tensorflow.org/get_started/os_setup).

- [X] Provide a link to a GitHub Gist of a Python script that can reproduce your issue (or just copy the script here if it is short).

I'm excluding the code where I load and preprocess (imagenetutils.preprocess_input). My data is the Kaggle cats-and-dogs data.



This never gets beyond 50-60% accuracy. VGG16 quickly gets to 98%+. If I remove ""tensorflow.contrib.keras.api."" from the start of all imports, this also gets to 98%+. So something is definitely different between tf and base Keras. I'm just not sure what."
keras,10058,"In current Keras code with Tensorflow as it’s backend, the non-training batch norm operator/layer will finally run into Tensorflow’s old non-fused batch norm API (tf.nn.batch_normalization), which result in bad performance in both CPU and GPU.  
Tensorflow’s tf.nn.batch_normalization is non-fused batch norm, does computations using several individual Ops, While Tensorflow’s tf.nn.fused_batch_norm is fused batch norm which is just a new implementation that comprise several ops into one, has much better performance.

Following code shows the place where calling to Tensorflow's non fused batch norm happen, and suggest to call Tensorflow's fused Batch Norm API.
output = (x - mean) / (sqrt(var) + epsilon) * gamma + beta 

Thanks.",0,Non training Batch Norm operator has bad performance for it running into tensorflow's non fused batch norm API,"Non training Batch Norm operator has bad performance for it running into tensorflow's non fused batch norm API In current Keras code with Tensorflow as it’s backend, the non-training batch norm operator/layer will finally run into Tensorflow’s old non-fused batch norm API (tf.nn.batch_normalization), which result in bad performance in both CPU and GPU.  
Tensorflow’s tf.nn.batch_normalization is non-fused batch norm, does computations using several individual Ops, While Tensorflow’s tf.nn.fused_batch_norm is fused batch norm which is just a new implementation that comprise several ops into one, has much better performance.

Following code shows the place where calling to Tensorflow's non fused batch norm happen, and suggest to call Tensorflow's fused Batch Norm API.
output = (x - mean) / (sqrt(var) + epsilon) * gamma + beta 

Thanks."
keras,7681,"I am using keras with tensorflow backend. I am creating a simple 1 layer LSTM model. I need my code to give same val_loss every time I train on the same data. I am running my system on CPU only. I tried:


from numpy.random import seed
seed(1337)
from tensorflow import set_random_seed
set_random_seed(1337)

on the top of my code.
I also initialized all the kernels and recurrent as ones:

model.add(LSTM(150,input_shape=(None,124), W_regularizer=l2(0.001),kernel_initializer='ones', recurrent_initializer='ones', bias_initializer='ones'))
model.add(Dense(2,kernel_initializer='ones', bias_initializer='ones'))
model.add(Activation(""softmax""))

I also set shuffle=False in model.fit(). I am using rms for optimizing.

I also set PYTHONHASHSEED to 0. But still I am getting different train loss as well as val loss on each epoch when rum multiple times.

I  am running keras on a server with 56 cpu cores and CentOS.

Pls help soon as I have tried everything everyone has suggested in other threads !!!!!",0,Getting different result while training an mlp with LSTM layer on Keras,"Getting different result while training an mlp with LSTM layer on Keras I am using keras with tensorflow backend. I am creating a simple 1 layer LSTM model. I need my code to give same val_loss every time I train on the same data. I am running my system on CPU only. I tried:


from numpy.random import seed
seed(1337)
from tensorflow import set_random_seed
set_random_seed(1337)

on the top of my code.
I also initialized all the kernels and recurrent as ones:

model.add(LSTM(150,input_shape=(None,124), W_regularizer=l2(0.001),kernel_initializer='ones', recurrent_initializer='ones', bias_initializer='ones'))
model.add(Dense(2,kernel_initializer='ones', bias_initializer='ones'))
model.add(Activation(""softmax""))

I also set shuffle=False in model.fit(). I am using rms for optimizing.

I also set PYTHONHASHSEED to 0. But still I am getting different train loss as well as val loss on each epoch when rum multiple times.

I  am running keras on a server with 56 cpu cores and CentOS.

Pls help soon as I have tried everything everyone has suggested in other threads !!!!!"
keras,12602,"When I use the model with  and target values of shape  when running  the method runs fine. But when switching to  with target values of shape  the method raises this error:
    
    Error when checking target: expected activation_1 to have 3 dimensions, but got array with shape (20, 10)

Here is the model that I use:

    inp = Input(batch_shape=(batch_size, seq_size))
    mat = Embedding(num_classes, num_lstms)(inp)
    mat = LSTM(num_lstms, return_sequences=True, stateful=True)(mat)
    mat = LSTM(num_lstms, return_sequences=True, stateful=True)(mat)
    mat = Dropout(0.5)(mat)
    mat = TimeDistributed(Dense(vocab_size))(mat)
    out_1 = Activation('softmax')(mat)
    model = Model(inputs=[inp], outputs=[out_1])",0,Error when checking target: expected activation_1 to have 3 dimensions,"Error when checking target: expected activation_1 to have 3 dimensions When I use the model with  and target values of shape  when running  the method runs fine. But when switching to  with target values of shape  the method raises this error:
    
    Error when checking target: expected activation_1 to have 3 dimensions, but got array with shape (20, 10)

Here is the model that I use:

    inp = Input(batch_shape=(batch_size, seq_size))
    mat = Embedding(num_classes, num_lstms)(inp)
    mat = LSTM(num_lstms, return_sequences=True, stateful=True)(mat)
    mat = LSTM(num_lstms, return_sequences=True, stateful=True)(mat)
    mat = Dropout(0.5)(mat)
    mat = TimeDistributed(Dense(vocab_size))(mat)
    out_1 = Activation('softmax')(mat)
    model = Model(inputs=[inp], outputs=[out_1])"
keras,13343,"I have a classification network with last two layers as Dense and activation respectively. I popped the layers from the model as :

    activationlayer=classifier.layers.pop()
    denselayer=classifier.layers.pop()

Then I pushed the layers back into the model:

    outt1=denselayer(classifier.layers[-1].output)
    outt=activationlayer(outt1)
    classifier=Model(classifier.input,outt)

Now while indexing the output of dense layer I get error:

    classifier.layers[-2].output

    Error: Layer out2 has multiple inbound nodes, hence the notion of ""layer output"" is ill-defined.",0,Unable to extract layer output after pushing layers back to model,"Unable to extract layer output after pushing layers back to model I have a classification network with last two layers as Dense and activation respectively. I popped the layers from the model as :

    activationlayer=classifier.layers.pop()
    denselayer=classifier.layers.pop()

Then I pushed the layers back into the model:

    outt1=denselayer(classifier.layers[-1].output)
    outt=activationlayer(outt1)
    classifier=Model(classifier.input,outt)

Now while indexing the output of dense layer I get error:

    classifier.layers[-2].output

    Error: Layer out2 has multiple inbound nodes, hence the notion of ""layer output"" is ill-defined."
keras,7728,"Hi all:

Below is my naive way to implement a group deconvolution3d layer in Keras 2.0+theano. Can you please give me suggestion on it? (e.g. better solution)

1. In theano,  is used to compute the default deconvolution. It actually can take the ""number of groups"" as an argument
[https://github.com/Theano/Theano/blob/master/theano/tensor/nnet/abstract_conv.py#L1163](url)

2. in keras2.0, 

- update  : add  based on  
- update : add  based on , call   and fix the output dimension calculation

Thanks
Donglai",0,"Suggestion on implementing ""group deconvolution3D"" layer","Suggestion on implementing ""group deconvolution3D"" layer Hi all:

Below is my naive way to implement a group deconvolution3d layer in Keras 2.0+theano. Can you please give me suggestion on it? (e.g. better solution)

1. In theano,  is used to compute the default deconvolution. It actually can take the ""number of groups"" as an argument
[https://github.com/Theano/Theano/blob/master/theano/tensor/nnet/abstract_conv.py#L1163](url)

2. in keras2.0, 

- update  : add  based on  
- update : add  based on , call   and fix the output dimension calculation

Thanks
Donglai"
keras,10611,"Adam was recently demonstrated to be implemented incorrectly in several packages including Keras.  Propose to fix the optimizer using method described here:

https://arxiv.org/pdf/1711.05101.pdf",0,Fix Adam Optimizer to Implement Paper Correctly,"Fix Adam Optimizer to Implement Paper Correctly Adam was recently demonstrated to be implemented incorrectly in several packages including Keras.  Propose to fix the optimizer using method described here:

https://arxiv.org/pdf/1711.05101.pdf"
keras,2370,"could we support or do we plan to support more backends?
Theano can only support multiple GPU using OpenCL but not CUDA, tensorflow seems slower than other framework using GPU for now.

Could we support Caffe, Leaf, MXNET, CNTK or Torch7?
![image](https://cloud.githubusercontent.com/assets/11470826/14587597/322ccb2e-04e8-11e6-8f82-b4759b5577fe.png)
![image](https://cloud.githubusercontent.com/assets/11470826/14587599/39d0fb2a-04e8-11e6-8300-be794b350bf2.png)
",0,feature request: more backends,"feature request: more backends could we support or do we plan to support more backends?
Theano can only support multiple GPU using OpenCL but not CUDA, tensorflow seems slower than other framework using GPU for now.

Could we support Caffe, Leaf, MXNET, CNTK or Torch7?
![image](https://cloud.githubusercontent.com/assets/11470826/14587597/322ccb2e-04e8-11e6-8f82-b4759b5577fe.png)
![image](https://cloud.githubusercontent.com/assets/11470826/14587599/39d0fb2a-04e8-11e6-8300-be794b350bf2.png)
"
keras,10923,"Please make sure that the boxes below are checked before you submit your issue. If your issue is an implementation question, please ask your question on [StackOverflow](http://stackoverflow.com/questions/tagged/keras) or [join the Keras Slack channel](https://keras-slack-autojoin.herokuapp.com/) and ask there instead of filing a GitHub issue.

Thank you!

- [x ] Check that you are up-to-date with the master branch of Keras. You can update with:
pip install git+git://github.com/keras-team/keras.git --upgrade --no-deps

- [x ] If running on TensorFlow, check that you are up-to-date with the latest version. The installation instructions can be found [here](https://www.tensorflow.org/get_started/os_setup).

- [ ] If running on Theano, check that you are up-to-date with the master branch of Theano. You can update with:
pip install git+git://github.com/Theano/Theano.git --upgrade --no-deps

- [x ] Provide a link to a GitHub Gist of a Python script that can reproduce your issue (or just copy the script here if it is short).

Hello,
I collected several models in the .json format which I import using model_from_json function.
These models have different inputs and outputs (tensors) in the respective first and final layers. (Some of these models are not Sequential). In my API the user can select the input_shape and data_format coming from the flow generator. So, I need to modify the architecture shape, in order to take data in the format the user specified. However, after spending the two days looking for alternative ways I could not find an effective way of changing the architecture input layer. Keras supposed to be intuitive. xP Hereby, I'll give an example of a small model architecture generated by me and some ideas I've tried. Honestly, I really appreciate some help with this.

Model:


Goal:
Make it receive batch data in the shape (None,32,32,3), 'channels_last'
Make it output batch data in the shape (None,6)

Attempted:
1. Using set shape
 
2. Using config

3. Changing tensor directly
def changeInputsShape(model, inputs):
   assert len(model.inputs) == len(inputs), ""Model inputs and assigned inputs mismatch.""

   for i in range(len(inputs)):
     model.inputs[i] = keras.layers.Input(shape=inputs[i])

Reason:
1. set_shape only update non defined dimensions of the previously defined tensor shape.
2. setting configuration does not update input tensors themselves, compiles not run.
3. with model.build() prior to compiling: 
3. with compiling: 

Note: This is note a weights/ knowledge tranfer problem since the model is not trained so it should be relatively simpler. I guess not.

Any help would be appreciated,
Andre
",0,Cannot modified established architecture shape.,"Cannot modified established architecture shape. Please make sure that the boxes below are checked before you submit your issue. If your issue is an implementation question, please ask your question on [StackOverflow](http://stackoverflow.com/questions/tagged/keras) or [join the Keras Slack channel](https://keras-slack-autojoin.herokuapp.com/) and ask there instead of filing a GitHub issue.

Thank you!

- [x ] Check that you are up-to-date with the master branch of Keras. You can update with:
pip install git+git://github.com/keras-team/keras.git --upgrade --no-deps

- [x ] If running on TensorFlow, check that you are up-to-date with the latest version. The installation instructions can be found [here](https://www.tensorflow.org/get_started/os_setup).

- [ ] If running on Theano, check that you are up-to-date with the master branch of Theano. You can update with:
pip install git+git://github.com/Theano/Theano.git --upgrade --no-deps

- [x ] Provide a link to a GitHub Gist of a Python script that can reproduce your issue (or just copy the script here if it is short).

Hello,
I collected several models in the .json format which I import using model_from_json function.
These models have different inputs and outputs (tensors) in the respective first and final layers. (Some of these models are not Sequential). In my API the user can select the input_shape and data_format coming from the flow generator. So, I need to modify the architecture shape, in order to take data in the format the user specified. However, after spending the two days looking for alternative ways I could not find an effective way of changing the architecture input layer. Keras supposed to be intuitive. xP Hereby, I'll give an example of a small model architecture generated by me and some ideas I've tried. Honestly, I really appreciate some help with this.

Model:


Goal:
Make it receive batch data in the shape (None,32,32,3), 'channels_last'
Make it output batch data in the shape (None,6)

Attempted:
1. Using set shape
 
2. Using config

3. Changing tensor directly
def changeInputsShape(model, inputs):
   assert len(model.inputs) == len(inputs), ""Model inputs and assigned inputs mismatch.""

   for i in range(len(inputs)):
     model.inputs[i] = keras.layers.Input(shape=inputs[i])

Reason:
1. set_shape only update non defined dimensions of the previously defined tensor shape.
2. setting configuration does not update input tensors themselves, compiles not run.
3. with model.build() prior to compiling: 
3. with compiling: 

Note: This is note a weights/ knowledge tranfer problem since the model is not trained so it should be relatively simpler. I guess not.

Any help would be appreciated,
Andre
"
keras,10388,"I use Horovod Keras from my multi gpus job, and I found that when I use multi gpus and each process will count the image num by use flow_from_directory, but the image num will be different:
![](https://user-images.githubusercontent.com/3112825/41193236-f8df388a-6c3b-11e8-8ee7-2b7ac54a2c34.png)
here is the code:



Could anyone help me with it ?  It's so strange, and maybe caused by multiprocessing ?",0,flow_from_directory error when use multi gpus,"flow_from_directory error when use multi gpus I use Horovod Keras from my multi gpus job, and I found that when I use multi gpus and each process will count the image num by use flow_from_directory, but the image num will be different:
![](https://user-images.githubusercontent.com/3112825/41193236-f8df388a-6c3b-11e8-8ee7-2b7ac54a2c34.png)
here is the code:



Could anyone help me with it ?  It's so strange, and maybe caused by multiprocessing ?"
keras,10689,"Sometimes ModelCheckpoint doesn't save any model at all! Why?

",0,ModelCheckpoint doesn't work sometimes.,"ModelCheckpoint doesn't work sometimes. Sometimes ModelCheckpoint doesn't save any model at all! Why?

"
keras,11957,"Tensorflow GPU with Keras.

When I run Tensorboard with  where , it uses the CPU at the end of each epoch when Tensorboard writes to logs. 

Usually the GPU is used for the entirety of training and writing to logs and this is also true when . 

This [issue] (https://github.com/keras-team/keras/issues/3358) has been mentioned in previous questions but it is dissimilar in that their issue occurs when validation data is passed via a data generator and that histograms are not created - no reference to CPU or GPU. I do not use a data generator for validation and histograms are created on my end.

Notes: 

Training carried out by Keras .

Training dataset passed by .

Validation dataset is passed without data generator.

**Edit**

Switching Keras  out for  crashes when attempting to write to Tensorboard logs when  but runs fine when . The crash log is below. Notice it switches from GPU to CPU. Perhaps this could be something to work with?

     InvalidArgumentError: You must feed a value for placeholder tensor 'conv2d_1_input' with dtype float and shape [?,48,48,1]
	 [[{{node conv2d_1_input}} = Placeholder[dtype=DT_FLOAT, shape=[?,48,48,1], _device=""/job:localhost/replica:0/task:0/device:GPU:0""]()]]
	 [[{{node dense_2/bias/read/_407}} = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/device:CPU:0"", send_device=""/job:localhost/replica:0/task:0/device:GPU:0"", send_device_incarnation=1, tensor_name=""edge_335_dense_2/bias/read"", tensor_type=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/device:CPU:0""]()]]InvalidArgumentError: You must feed a value for placeholder tensor 'conv2d_1_input' with dtype float and shape [?,48,48,1]
	 [[{{node conv2d_1_input}} = Placeholder[dtype=DT_FLOAT, shape=[?,48,48,1], _device=""/job:localhost/replica:0/task:0/device:GPU:0""]()]]
	 [[{{node dense_2/bias/read/_407}} = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/device:CPU:0"", send_device=""/job:localhost/replica:0/task:0/device:GPU:0"", send_device_incarnation=1, tensor_name=""edge_335_dense_2/bias/read"", tensor_type=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/device:CPU:0""]()]]

Is anyone aware of a fix to prevent CPU being used when  is enabled? I ask because the writing to logs on CPU can make the training process up to five times longer.",0,Tensorboard histogram_freq switching from GPU to CPU to write logs,"Tensorboard histogram_freq switching from GPU to CPU to write logs Tensorflow GPU with Keras.

When I run Tensorboard with  where , it uses the CPU at the end of each epoch when Tensorboard writes to logs. 

Usually the GPU is used for the entirety of training and writing to logs and this is also true when . 

This [issue] (https://github.com/keras-team/keras/issues/3358) has been mentioned in previous questions but it is dissimilar in that their issue occurs when validation data is passed via a data generator and that histograms are not created - no reference to CPU or GPU. I do not use a data generator for validation and histograms are created on my end.

Notes: 

Training carried out by Keras .

Training dataset passed by .

Validation dataset is passed without data generator.

**Edit**

Switching Keras  out for  crashes when attempting to write to Tensorboard logs when  but runs fine when . The crash log is below. Notice it switches from GPU to CPU. Perhaps this could be something to work with?

     InvalidArgumentError: You must feed a value for placeholder tensor 'conv2d_1_input' with dtype float and shape [?,48,48,1]
	 [[{{node conv2d_1_input}} = Placeholder[dtype=DT_FLOAT, shape=[?,48,48,1], _device=""/job:localhost/replica:0/task:0/device:GPU:0""]()]]
	 [[{{node dense_2/bias/read/_407}} = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/device:CPU:0"", send_device=""/job:localhost/replica:0/task:0/device:GPU:0"", send_device_incarnation=1, tensor_name=""edge_335_dense_2/bias/read"", tensor_type=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/device:CPU:0""]()]]InvalidArgumentError: You must feed a value for placeholder tensor 'conv2d_1_input' with dtype float and shape [?,48,48,1]
	 [[{{node conv2d_1_input}} = Placeholder[dtype=DT_FLOAT, shape=[?,48,48,1], _device=""/job:localhost/replica:0/task:0/device:GPU:0""]()]]
	 [[{{node dense_2/bias/read/_407}} = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/device:CPU:0"", send_device=""/job:localhost/replica:0/task:0/device:GPU:0"", send_device_incarnation=1, tensor_name=""edge_335_dense_2/bias/read"", tensor_type=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/device:CPU:0""]()]]

Is anyone aware of a fix to prevent CPU being used when  is enabled? I ask because the writing to logs on CPU can make the training process up to five times longer."
keras,8151,"Please make sure that the boxes below are checked before you submit your issue. If your issue is an implementation question, please ask your question on [StackOverflow](http://stackoverflow.com/questions/tagged/keras) or [join the Keras Slack channel](https://keras-slack-autojoin.herokuapp.com/) and ask there instead of filing a GitHub issue.

Thank you!

- [ ] Check that you are up-to-date with the master branch of Keras. You can update with:
pip install git+git://github.com/fchollet/keras.git --upgrade --no-deps

- [ ] If running on TensorFlow, check that you are up-to-date with the latest version. The installation instructions can be found [here](https://www.tensorflow.org/get_started/os_setup).

- [ ] If running on Theano, check that you are up-to-date with the master branch of Theano. You can update with:
pip install git+git://github.com/Theano/Theano.git --upgrade --no-deps

- [ ] Provide a link to a GitHub Gist of a Python script that can reproduce your issue (or just copy the script here if it is short).
",0,Extract the weights of a layer,"Extract the weights of a layer Please make sure that the boxes below are checked before you submit your issue. If your issue is an implementation question, please ask your question on [StackOverflow](http://stackoverflow.com/questions/tagged/keras) or [join the Keras Slack channel](https://keras-slack-autojoin.herokuapp.com/) and ask there instead of filing a GitHub issue.

Thank you!

- [ ] Check that you are up-to-date with the master branch of Keras. You can update with:
pip install git+git://github.com/fchollet/keras.git --upgrade --no-deps

- [ ] If running on TensorFlow, check that you are up-to-date with the latest version. The installation instructions can be found [here](https://www.tensorflow.org/get_started/os_setup).

- [ ] If running on Theano, check that you are up-to-date with the master branch of Theano. You can update with:
pip install git+git://github.com/Theano/Theano.git --upgrade --no-deps

- [ ] Provide a link to a GitHub Gist of a Python script that can reproduce your issue (or just copy the script here if it is short).
"
keras,10239,"I m getting this error::   
Here is my code

#Create The model
def baseline_model():
    model = Sequential()
    model.add(Dense(4, activation='relu', input_shape=(4,)))
    model.add(Dense(3, activation='sigmoid'))
    #compile the model
    model.compile(optimizer='adam', loss='categorical_crossentropy',metrics = ['accuracy'] )
    return model
#in order to fit the model
estimator = KerasClassifier(build_fn=baseline_model, nb_epoch=200, verbose=0, batch_size=5)
kfold = KFold(n=len(X), n_folds=10, shuffle=True, random_state=seed)
results = cross_val_score(estimator, X, dummy_y, cv=kfold)
print(""Accuracy: %.2f%% (%.2f%%)"" % (results.mean()*100, results.std()*100))",0,"ValueError: Error when checking target: expected dense_20 to have shape (3,) but got array with shape (22,)","ValueError: Error when checking target: expected dense_20 to have shape (3,) but got array with shape (22,) I m getting this error::   
Here is my code

#Create The model
def baseline_model():
    model = Sequential()
    model.add(Dense(4, activation='relu', input_shape=(4,)))
    model.add(Dense(3, activation='sigmoid'))
    #compile the model
    model.compile(optimizer='adam', loss='categorical_crossentropy',metrics = ['accuracy'] )
    return model
#in order to fit the model
estimator = KerasClassifier(build_fn=baseline_model, nb_epoch=200, verbose=0, batch_size=5)
kfold = KFold(n=len(X), n_folds=10, shuffle=True, random_state=seed)
results = cross_val_score(estimator, X, dummy_y, cv=kfold)
print(""Accuracy: %.2f%% (%.2f%%)"" % (results.mean()*100, results.std()*100))"
keras,11465,"Dear keras users,

Hello everyone. I need some help about dealing with activity regularizer in keras.

As I know, the activity regularizer is the one that adding a 'norm of output' into a loss.

It is really similar to Tikhonov regularization, but is it really working well in the deep learning?

I doubt it because when differentiating the term of output norm loss with weight(w),  it becomes zero.

Actually I am not majored in Computer science, I know that I do not know that much.

So I want to ask some help know the basic principle of it.

Or, can you recommend me some related papers?

Thank you so much.  
",0,About principle of activity regularizer in Keras,"About principle of activity regularizer in Keras Dear keras users,

Hello everyone. I need some help about dealing with activity regularizer in keras.

As I know, the activity regularizer is the one that adding a 'norm of output' into a loss.

It is really similar to Tikhonov regularization, but is it really working well in the deep learning?

I doubt it because when differentiating the term of output norm loss with weight(w),  it becomes zero.

Actually I am not majored in Computer science, I know that I do not know that much.

So I want to ask some help know the basic principle of it.

Or, can you recommend me some related papers?

Thank you so much.  
"
keras,10703,"Please make sure that the boxes below are checked before you submit your issue. If your issue is an implementation question, please ask your question on [StackOverflow](http://stackoverflow.com/questions/tagged/keras) or [join the Keras Slack channel](https://keras-slack-autojoin.herokuapp.com/) and ask there instead of filing a GitHub issue.

Thank you!

- [x] Check that you are up-to-date with the master branch of Keras. You can update with:
pip install git+git://github.com/keras-team/keras.git --upgrade --no-deps

- [x] If running on TensorFlow, check that you are up-to-date with the latest version. The installation instructions can be found [here](https://www.tensorflow.org/get_started/os_setup).

- [x] If running on Theano, check that you are up-to-date with the master branch of Theano. You can update with:
pip install git+git://github.com/Theano/Theano.git --upgrade --no-deps

- [x] Provide a link to a GitHub Gist of a Python script that can reproduce your issue (or just copy the script here if it is short).
",0,Is there any way to set different learning rates for different different layers of a CNN model in Keras?,"Is there any way to set different learning rates for different different layers of a CNN model in Keras? Please make sure that the boxes below are checked before you submit your issue. If your issue is an implementation question, please ask your question on [StackOverflow](http://stackoverflow.com/questions/tagged/keras) or [join the Keras Slack channel](https://keras-slack-autojoin.herokuapp.com/) and ask there instead of filing a GitHub issue.

Thank you!

- [x] Check that you are up-to-date with the master branch of Keras. You can update with:
pip install git+git://github.com/keras-team/keras.git --upgrade --no-deps

- [x] If running on TensorFlow, check that you are up-to-date with the latest version. The installation instructions can be found [here](https://www.tensorflow.org/get_started/os_setup).

- [x] If running on Theano, check that you are up-to-date with the master branch of Theano. You can update with:
pip install git+git://github.com/Theano/Theano.git --upgrade --no-deps

- [x] Provide a link to a GitHub Gist of a Python script that can reproduce your issue (or just copy the script here if it is short).
"
keras,13150,"<em>Please make sure that this is a Bug or a Feature Request and provide all applicable information asked by the template.
If your issue is an **implementation question**, please ask your question on [StackOverflow](http://stackoverflow.com/questions/tagged/keras) or [on the Keras Slack channel](https://keras-slack-autojoin.herokuapp.com/) instead of opening a GitHub issue.</em>  

**System information**  
- Have I written custom code (as opposed to using example directory):  
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Linux
- TensorFlow backend (yes / no):  yes
- TensorFlow version:  1.12.0
- Keras version:  2.2.4
- Python version:  3.5.4
- CUDA/cuDNN version:  8.0, V8.0.61
- GPU model and memory:   NVIDIA Pascal P100

**Describe the current behavior**  
Getting error


tensorflow.python.framework.errors_impl.InvalidArgumentError: You must feed a value for placeholder tensor 'input_1' with dtype float and shape [?,1,512,20]
	 [[{{node input_1}} = Placeholder [dtype=DT_FLOAT, shape=[?,1,512,20], _device=""/job:localhost/replica:0/task:0/device:GPU:0""]()]]

**Code to reproduce the issue**  
Provide a reproducible test case that is the bare minimum necessary to generate the problem.  
base_network


**Other info / logs**  

File ""testKerasClean.py"", line 329, in <module>
    history=model.fit([input_a,input_b],output_trg,batch_size=100,shuffle=True,epochs=100,class_weight=class_weight_val,validation_split=0.2,verbose=1,callbacks=[TensorBoard(log_dir='./Graph',write_graph=True,write_grads=True,histogram_freq=3)])
  File ""/project/6000341/saby2k13/DPPI2/venv/tensorflow/lib/python3.5/site-packages/tensorflow/python/keras/engine/training.py"", line 1639, in fit
    validation_steps=validation_steps)
  File ""/project/6000341/saby2k13/DPPI2/venv/tensorflow/lib/python3.5/site-packages/tensorflow/python/keras/engine/training_arrays.py"", line 233, in fit_loop
    verbose=0)
  File ""/project/6000341/saby2k13/DPPI2/venv/tensorflow/lib/python3.5/site-packages/tensorflow/python/keras/engine/training_arrays.py"", line 439, in test_loop
    batch_outs = f(ins_batch)
  File ""/project/6000341/saby2k13/DPPI2/venv/tensorflow/lib/python3.5/site-packages/tensorflow/python/keras/backend.py"", line 2986, in __call__
    run_metadata=self.run_metadata)
  File ""/project/6000341/saby2k13/DPPI2/venv/tensorflow/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 1439, in __call__
    run_metadata_ptr)
  File ""/project/6000341/saby2k13/DPPI2/venv/tensorflow/lib/python3.5/site-packages/tensorflow/python/framework/errors_impl.py"", line 528, in __exit__
    c_api.TF_GetCode(self.status.status))
tensorflow.python.framework.errors_impl.InvalidArgumentError: You must feed a value for placeholder tensor 'input_1' with 
dtype float and shape [?,1,512,20]
	 [[{{node input_1}} = Placeholder \[dtype=DT_FLOAT, shape=[?,1,512,20], _device=""/job:localhost/replica:0/task:0/device:GPU:0""]()]]",0,Tensorboard error with histogram_freq > 0,"Tensorboard error with histogram_freq > 0 <em>Please make sure that this is a Bug or a Feature Request and provide all applicable information asked by the template.
If your issue is an **implementation question**, please ask your question on [StackOverflow](http://stackoverflow.com/questions/tagged/keras) or [on the Keras Slack channel](https://keras-slack-autojoin.herokuapp.com/) instead of opening a GitHub issue.</em>  

**System information**  
- Have I written custom code (as opposed to using example directory):  
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Linux
- TensorFlow backend (yes / no):  yes
- TensorFlow version:  1.12.0
- Keras version:  2.2.4
- Python version:  3.5.4
- CUDA/cuDNN version:  8.0, V8.0.61
- GPU model and memory:   NVIDIA Pascal P100

**Describe the current behavior**  
Getting error


tensorflow.python.framework.errors_impl.InvalidArgumentError: You must feed a value for placeholder tensor 'input_1' with dtype float and shape [?,1,512,20]
	 [[{{node input_1}} = Placeholder [dtype=DT_FLOAT, shape=[?,1,512,20], _device=""/job:localhost/replica:0/task:0/device:GPU:0""]()]]

**Code to reproduce the issue**  
Provide a reproducible test case that is the bare minimum necessary to generate the problem.  
base_network


**Other info / logs**  

File ""testKerasClean.py"", line 329, in <module>
    history=model.fit([input_a,input_b],output_trg,batch_size=100,shuffle=True,epochs=100,class_weight=class_weight_val,validation_split=0.2,verbose=1,callbacks=[TensorBoard(log_dir='./Graph',write_graph=True,write_grads=True,histogram_freq=3)])
  File ""/project/6000341/saby2k13/DPPI2/venv/tensorflow/lib/python3.5/site-packages/tensorflow/python/keras/engine/training.py"", line 1639, in fit
    validation_steps=validation_steps)
  File ""/project/6000341/saby2k13/DPPI2/venv/tensorflow/lib/python3.5/site-packages/tensorflow/python/keras/engine/training_arrays.py"", line 233, in fit_loop
    verbose=0)
  File ""/project/6000341/saby2k13/DPPI2/venv/tensorflow/lib/python3.5/site-packages/tensorflow/python/keras/engine/training_arrays.py"", line 439, in test_loop
    batch_outs = f(ins_batch)
  File ""/project/6000341/saby2k13/DPPI2/venv/tensorflow/lib/python3.5/site-packages/tensorflow/python/keras/backend.py"", line 2986, in __call__
    run_metadata=self.run_metadata)
  File ""/project/6000341/saby2k13/DPPI2/venv/tensorflow/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 1439, in __call__
    run_metadata_ptr)
  File ""/project/6000341/saby2k13/DPPI2/venv/tensorflow/lib/python3.5/site-packages/tensorflow/python/framework/errors_impl.py"", line 528, in __exit__
    c_api.TF_GetCode(self.status.status))
tensorflow.python.framework.errors_impl.InvalidArgumentError: You must feed a value for placeholder tensor 'input_1' with 
dtype float and shape [?,1,512,20]
	 [[{{node input_1}} = Placeholder \[dtype=DT_FLOAT, shape=[?,1,512,20], _device=""/job:localhost/replica:0/task:0/device:GPU:0""]()]]"
keras,13597,"**System information**  
- Have I written custom code (as opposed to using example directory):  

Yes

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  
Windows 10

- TensorFlow backend (yes / no):

Yes
  
- TensorFlow version:  

2.0

- Keras version:  

2.3.1

- Python version:  

3.7

- CUDA/cuDNN version:  

None - CPU only

**Describe the current behavior**  

I am attempting to generate confidence intervals using dropout as per logic in [here](https://medium.com/hal24k-techblog/how-to-generate-neural-network-confidence-intervals-with-keras-e4c0b78ebbdf) 

unfortunately I keep receiving (now) the error above?


**Describe the expected behavior**  

Expect a new model to be initialised from the saved config.

**Code to reproduce the issue**  
Provide a reproducible test case that is the bare minimum necessary to generate the problem.  



Adding a note in to mention that my train/test/forecast data is generated via tf.data.Dataset:

`


Thank you for your help!
",0,"ValueError: Arguments and signature arguments do not match. got: 13, expected: 14 ","ValueError: Arguments and signature arguments do not match. got: 13, expected: 14  **System information**  
- Have I written custom code (as opposed to using example directory):  

Yes

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  
Windows 10

- TensorFlow backend (yes / no):

Yes
  
- TensorFlow version:  

2.0

- Keras version:  

2.3.1

- Python version:  

3.7

- CUDA/cuDNN version:  

None - CPU only

**Describe the current behavior**  

I am attempting to generate confidence intervals using dropout as per logic in [here](https://medium.com/hal24k-techblog/how-to-generate-neural-network-confidence-intervals-with-keras-e4c0b78ebbdf) 

unfortunately I keep receiving (now) the error above?


**Describe the expected behavior**  

Expect a new model to be initialised from the saved config.

**Code to reproduce the issue**  
Provide a reproducible test case that is the bare minimum necessary to generate the problem.  



Adding a note in to mention that my train/test/forecast data is generated via tf.data.Dataset:

`


Thank you for your help!
"
keras,10370,"In , if the validation data is a generator, it will not be passed to the callback as a parameter which makes it impossible to access the validation data for use in custom callbacks.

https://github.com/keras-team/keras/blob/632560d91286bf278228de72e7ce64f6c5aa530c/keras/engine/training_generator.py#L92-L98",0,validation_data not passed as param to callback.set_params() in fit_generator,"validation_data not passed as param to callback.set_params() in fit_generator In , if the validation data is a generator, it will not be passed to the callback as a parameter which makes it impossible to access the validation data for use in custom callbacks.

https://github.com/keras-team/keras/blob/632560d91286bf278228de72e7ce64f6c5aa530c/keras/engine/training_generator.py#L92-L98"
keras,9684,"Dear everyone,
                     I found the roc_auc_score function in  https://github.com/tflearn/tflearn/blob/master/tflearn/objectives.py. Now I want to write this function by keras. But failed. The following is the code:
TensorTensorTensorTensorTensorTensorTensorTensor
**The problem is the following :**+1: 
ValueError                                Traceback (most recent call last)
<ipython-input-3-cc3e34fe5d20> in <module>()
    149         epochs=epochs,
    150         validation_data=validation_generator,
--> 151         validation_steps=validation_samples// batch_size)
    152 
    153 model.save_weights('models/basic_cnn_30_epochs.h5')

/usr/local/lib/python2.7/dist-packages/keras/legacy/interfaces.pyc in wrapper(*args, **kwargs)
     85                 warnings.warn('Update your  call to the Keras 2 API: ' + signature, stacklevel=2)
---> 87             return func(*args, **kwargs)
     88         wrapper._original_function = func
     89         return wrapper

/usr/local/lib/python2.7/dist-packages/keras/models.pyc in fit_generator(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, initial_epoch)
   1119                                         workers=workers,
   1120                                         use_multiprocessing=use_multiprocessing,
-> 1121                                         initial_epoch=initial_epoch)
   1122 
   1123     @interfaces.legacy_generator_methods_support

/usr/local/lib/python2.7/dist-packages/keras/legacy/interfaces.pyc in wrapper(*args, **kwargs)
     85                 warnings.warn('Update your  call to the Keras 2 API: ' + signature, stacklevel=2)
---> 87             return func(*args, **kwargs)
     88         wrapper._original_function = func
     89         return wrapper

/usr/local/lib/python2.7/dist-packages/keras/engine/training.pyc in fit_generator(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)
   1924 
   1925         do_validation = bool(validation_data)
-> 1926         self._make_train_function()
   1927         if do_validation:
   1928             self._make_test_function()

/usr/local/lib/python2.7/dist-packages/keras/engine/training.pyc in _make_train_function(self)
    958                     training_updates = self.optimizer.get_updates(
    959                         params=self._collected_trainable_weights,
--> 960                         loss=self.total_loss)
    961                 updates = self.updates + training_updates
    962                 # Gets loss and metrics. Updates weights at each call.

/usr/local/lib/python2.7/dist-packages/keras/legacy/interfaces.pyc in wrapper(*args, **kwargs)
     85                 warnings.warn('Update your  call to the Keras 2 API: ' + signature, stacklevel=2)
---> 87             return func(*args, **kwargs)
     88         wrapper._original_function = func
     89         return wrapper

/usr/local/lib/python2.7/dist-packages/keras/optimizers.pyc in get_updates(self, loss, params)
    235         for p, g, a in zip(params, grads, accumulators):
    236             # update accumulator
--> 237             new_a = self.rho * a + (1. - self.rho) * K.square(g)
    238             self.updates.append(K.update(a, new_a))
    239             new_p = p - lr * g / (K.sqrt(new_a) + self.epsilon)

/usr/local/lib/python2.7/dist-packages/keras/backend/tensorflow_backend.pyc in square(x)
   1356         A tensor.
   1357     """"""
-> 1358     return tf.square(x)
   1359 
   1360 

/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/math_ops.pyc in square(x, name)
    447           indices=x.indices, values=x_square, dense_shape=x.dense_shape)
    448     else:
--> 449       return gen_math_ops.square(x, name=name)
    450 
    451 

/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_math_ops.pyc in square(x, name)
   4565   if _ctx.in_graph_mode():
   4566     _, _, _op = _op_def_lib._apply_op_helper(
-> 4567         ""Square"", x=x, name=name)
   4568     _result = _op.outputs[:]
   4569     _inputs_flat = _op.inputs

/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.pyc in _apply_op_helper(self, op_type_name, name, **keywords)
    526               raise ValueError(
    527                   ""Tried to convert '%s' to a tensor and failed. Error: %s"" %
--> 528                   (input_name, err))
    529             prefix = (""Input '%s' of '%s' Op has type %s that does not match"" %
    530                       (input_name, op_type_name, observed))

ValueError: Tried to convert 'x' to a tensor and failed. Error: None values not supported.

Finally, Anyone can check this problem. Looking forward to reply. Thanks advanced!
",0,how to write the objectives function roc_auc_score in tflearn by keras,"how to write the objectives function roc_auc_score in tflearn by keras Dear everyone,
                     I found the roc_auc_score function in  https://github.com/tflearn/tflearn/blob/master/tflearn/objectives.py. Now I want to write this function by keras. But failed. The following is the code:
TensorTensorTensorTensorTensorTensorTensorTensor
**The problem is the following :**+1: 
ValueError                                Traceback (most recent call last)
<ipython-input-3-cc3e34fe5d20> in <module>()
    149         epochs=epochs,
    150         validation_data=validation_generator,
--> 151         validation_steps=validation_samples// batch_size)
    152 
    153 model.save_weights('models/basic_cnn_30_epochs.h5')

/usr/local/lib/python2.7/dist-packages/keras/legacy/interfaces.pyc in wrapper(*args, **kwargs)
     85                 warnings.warn('Update your  call to the Keras 2 API: ' + signature, stacklevel=2)
---> 87             return func(*args, **kwargs)
     88         wrapper._original_function = func
     89         return wrapper

/usr/local/lib/python2.7/dist-packages/keras/models.pyc in fit_generator(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, initial_epoch)
   1119                                         workers=workers,
   1120                                         use_multiprocessing=use_multiprocessing,
-> 1121                                         initial_epoch=initial_epoch)
   1122 
   1123     @interfaces.legacy_generator_methods_support

/usr/local/lib/python2.7/dist-packages/keras/legacy/interfaces.pyc in wrapper(*args, **kwargs)
     85                 warnings.warn('Update your  call to the Keras 2 API: ' + signature, stacklevel=2)
---> 87             return func(*args, **kwargs)
     88         wrapper._original_function = func
     89         return wrapper

/usr/local/lib/python2.7/dist-packages/keras/engine/training.pyc in fit_generator(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)
   1924 
   1925         do_validation = bool(validation_data)
-> 1926         self._make_train_function()
   1927         if do_validation:
   1928             self._make_test_function()

/usr/local/lib/python2.7/dist-packages/keras/engine/training.pyc in _make_train_function(self)
    958                     training_updates = self.optimizer.get_updates(
    959                         params=self._collected_trainable_weights,
--> 960                         loss=self.total_loss)
    961                 updates = self.updates + training_updates
    962                 # Gets loss and metrics. Updates weights at each call.

/usr/local/lib/python2.7/dist-packages/keras/legacy/interfaces.pyc in wrapper(*args, **kwargs)
     85                 warnings.warn('Update your  call to the Keras 2 API: ' + signature, stacklevel=2)
---> 87             return func(*args, **kwargs)
     88         wrapper._original_function = func
     89         return wrapper

/usr/local/lib/python2.7/dist-packages/keras/optimizers.pyc in get_updates(self, loss, params)
    235         for p, g, a in zip(params, grads, accumulators):
    236             # update accumulator
--> 237             new_a = self.rho * a + (1. - self.rho) * K.square(g)
    238             self.updates.append(K.update(a, new_a))
    239             new_p = p - lr * g / (K.sqrt(new_a) + self.epsilon)

/usr/local/lib/python2.7/dist-packages/keras/backend/tensorflow_backend.pyc in square(x)
   1356         A tensor.
   1357     """"""
-> 1358     return tf.square(x)
   1359 
   1360 

/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/math_ops.pyc in square(x, name)
    447           indices=x.indices, values=x_square, dense_shape=x.dense_shape)
    448     else:
--> 449       return gen_math_ops.square(x, name=name)
    450 
    451 

/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_math_ops.pyc in square(x, name)
   4565   if _ctx.in_graph_mode():
   4566     _, _, _op = _op_def_lib._apply_op_helper(
-> 4567         ""Square"", x=x, name=name)
   4568     _result = _op.outputs[:]
   4569     _inputs_flat = _op.inputs

/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.pyc in _apply_op_helper(self, op_type_name, name, **keywords)
    526               raise ValueError(
    527                   ""Tried to convert '%s' to a tensor and failed. Error: %s"" %
--> 528                   (input_name, err))
    529             prefix = (""Input '%s' of '%s' Op has type %s that does not match"" %
    530                       (input_name, op_type_name, observed))

ValueError: Tried to convert 'x' to a tensor and failed. Error: None values not supported.

Finally, Anyone can check this problem. Looking forward to reply. Thanks advanced!
"
keras,12393,"I have a multi-GPU machine and am trying to train different models on different GPUs. I'm trying to set the GPU programmatically rather than use env vars, but am running into issues. Here's my code:



However, when I try to run the code in two separate jupyter notebooks, using ids 0 and 1, I get the following error:



It works fine if I set  at the top of the script, but I'd rather not have to manage this. Any sense of what I may be doing wrong? I'm using
- keras 2.2.4
- tensorflow 1.13.1
- ubuntu 18.04
- jupyter 5.2.4
- jupyter lab 0.35.4
",0,Error selecting GPU programmatically from jupyter,"Error selecting GPU programmatically from jupyter I have a multi-GPU machine and am trying to train different models on different GPUs. I'm trying to set the GPU programmatically rather than use env vars, but am running into issues. Here's my code:



However, when I try to run the code in two separate jupyter notebooks, using ids 0 and 1, I get the following error:



It works fine if I set  at the top of the script, but I'd rather not have to manage this. Any sense of what I may be doing wrong? I'm using
- keras 2.2.4
- tensorflow 1.13.1
- ubuntu 18.04
- jupyter 5.2.4
- jupyter lab 0.35.4
"
keras,6810,package pydot can't be installed on a conda python 3.6 platform so  doesn't work,0,issue with conda 3.6 and pydot and plot_model,issue with conda 3.6 and pydot and plot_model package pydot can't be installed on a conda python 3.6 platform so  doesn't work
keras,11929,"Hello!

For a multi-GPU model built using , the function  outputs incorrect information. More precisely, for the following code:


The output is:


However, calling  on the original model (e.g. ) outputs the correct architecture:



Is this the expected behaviour? Is it related to the same issue as calling  on the parallel model?
",0,model.summary() output problem for multi GPU model ,"model.summary() output problem for multi GPU model  Hello!

For a multi-GPU model built using , the function  outputs incorrect information. More precisely, for the following code:


The output is:


However, calling  on the original model (e.g. ) outputs the correct architecture:



Is this the expected behaviour? Is it related to the same issue as calling  on the parallel model?
"
keras,9336,"Hello,

I am creating the following network:

When using CNTK as backend, I am getting error:
'RNN dropout is not supported with the CNTK backend when using dynamic RNNs (i.e. non-unrolled).  You can either set , set  and  to 0, 'or use a different backend.'

However, I cannot unroll, because train_x_lstm.shape[1] = 1",0,RNN with dropout / recurrent dropout and 1 timestep shows warning with CNTK,"RNN with dropout / recurrent dropout and 1 timestep shows warning with CNTK Hello,

I am creating the following network:

When using CNTK as backend, I am getting error:
'RNN dropout is not supported with the CNTK backend when using dynamic RNNs (i.e. non-unrolled).  You can either set , set  and  to 0, 'or use a different backend.'

However, I cannot unroll, because train_x_lstm.shape[1] = 1"
keras,10356,"Making issue from comment https://github.com/keras-team/keras/issues/10080#issuecomment-394640409 at #10080.

When converting weights between CuDNNGRU and GRU which is wrapped in TimeDistributed the conversion is skipped by mistake. Similar to Bidirectional (#8908) and Model/Sequential (#10080).

Example of failure:



Fails with:



Without TimeDistributed it works ok:


The same for the other direction (plain -> CuDNN).

Do we know of any other wrapper layers that need this conversion?",0,CuDNN RNN layers nested in TimeDistributed are not converted when loading,"CuDNN RNN layers nested in TimeDistributed are not converted when loading Making issue from comment https://github.com/keras-team/keras/issues/10080#issuecomment-394640409 at #10080.

When converting weights between CuDNNGRU and GRU which is wrapped in TimeDistributed the conversion is skipped by mistake. Similar to Bidirectional (#8908) and Model/Sequential (#10080).

Example of failure:



Fails with:



Without TimeDistributed it works ok:


The same for the other direction (plain -> CuDNN).

Do we know of any other wrapper layers that need this conversion?"
keras,3424,"[Some research papers](http://www.clsp.jhu.edu/~samuel/pdfs/annealed_dropout.pdf) showed the benefit to adjust dropout rate over epochs.
Specifically, they used this dropout rate:



where  is the current epoch and N is a fixed parameter.

How can I achieve this using Keras? I think I can re-write the call function in the dropout layer but the problem is that how can I get the epoch variable.
",0,Adjust dropout rate over epochs,"Adjust dropout rate over epochs [Some research papers](http://www.clsp.jhu.edu/~samuel/pdfs/annealed_dropout.pdf) showed the benefit to adjust dropout rate over epochs.
Specifically, they used this dropout rate:



where  is the current epoch and N is a fixed parameter.

How can I achieve this using Keras? I think I can re-write the call function in the dropout layer but the problem is that how can I get the epoch variable.
"
keras,3721,"Hi all,

I am currently running some test with simple Autoencoders. I just copied and pasted the code from this keras blog entry: https://blog.keras.io/building-autoencoders-in-keras.html

However, when I was testing different architectures, I just found out that even an Autoencoder with zero nodes in the (well, technically not even existing) first layer appears to be learning something. The loss I get is comparable with the loss I get with bigger architectures.

Could this be a bug in the Keras autoencoder code or might this be a problem of my dataset (which is quite noisy). My intuition was that I shouldn't learn anything when using a layer with zero nodes.

Any suggestions would be very helpful!
Thanks a lot! 
",0,Autoencoder with 0 nodes learns something,"Autoencoder with 0 nodes learns something Hi all,

I am currently running some test with simple Autoencoders. I just copied and pasted the code from this keras blog entry: https://blog.keras.io/building-autoencoders-in-keras.html

However, when I was testing different architectures, I just found out that even an Autoencoder with zero nodes in the (well, technically not even existing) first layer appears to be learning something. The loss I get is comparable with the loss I get with bigger architectures.

Could this be a bug in the Keras autoencoder code or might this be a problem of my dataset (which is quite noisy). My intuition was that I shouldn't learn anything when using a layer with zero nodes.

Any suggestions would be very helpful!
Thanks a lot! 
"
keras,7235,"(x, y, sample_weight)(x, y)  ",0,ImageDataGenerator object is not an iterator,"ImageDataGenerator object is not an iterator (x, y, sample_weight)(x, y)  "
keras,109,"Hi, I am reading [a tutorial with nolearn/lasagne](http://danielnouri.org/notes/2014/12/17/using-convolutional-neural-nets-to-detect-facial-keypoints-tutorial/), where the author point out it is possible to [update learning rate and momentum over time](http://danielnouri.org/notes/2014/12/17/using-convolutional-neural-nets-to-detect-facial-keypoints-tutorial/#changing-learning-rate-and-momentum-over-time).

Is it possible to do it in keras? The coverage speed is low, so I am wondering whether it's possible to use the technique to improve the speed. 
",0,[help wanted] any way to update learning rate and momentum,"[help wanted] any way to update learning rate and momentum Hi, I am reading [a tutorial with nolearn/lasagne](http://danielnouri.org/notes/2014/12/17/using-convolutional-neural-nets-to-detect-facial-keypoints-tutorial/), where the author point out it is possible to [update learning rate and momentum over time](http://danielnouri.org/notes/2014/12/17/using-convolutional-neural-nets-to-detect-facial-keypoints-tutorial/#changing-learning-rate-and-momentum-over-time).

Is it possible to do it in keras? The coverage speed is low, so I am wondering whether it's possible to use the technique to improve the speed. 
"
keras,703,"Mentioned in the ""mega-issue"" #100, though it would be worth having a separate one.

The simplest model zoo is a github wiki page, another option it to have it as a page in the documentation and receive updates via push requests.

I think model zoo matters most for models that take ""more 6 hours to train"" (i.e. a large fraction of 24h). 
",0,Models zoo,"Models zoo Mentioned in the ""mega-issue"" #100, though it would be worth having a separate one.

The simplest model zoo is a github wiki page, another option it to have it as a page in the documentation and receive updates via push requests.

I think model zoo matters most for models that take ""more 6 hours to train"" (i.e. a large fraction of 24h). 
"
keras,7814,"My model looks something like this :

now the problem is in the y_ture.shape as whenever i try to access y_trrue to do needed calculation inside the loss function i get y_true.shape as 

y_true (?, ?, ?, ?)
y_true Tensor(""decoder/FinalConv_target:0"", shape=(?, ?, ?, ?), dtype=float32)

Thank you!

- [-] Check that you are up-to-date with the master branch of Keras. You can update with:
pip install git+git://github.com/fchollet/keras.git --upgrade --no-deps

- [-] If running on TensorFlow, check that you are up-to-date with the latest version. The installation instructions can be found [here](https://www.tensorflow.org/get_started/os_setup).

- [-] If running on Theano, check that you are up-to-date with the master branch of Theano. You can update with:
pip install git+git://github.com/Theano/Theano.git --upgrade --no-deps

- [-] Provide a link to a GitHub Gist of a Python script that can reproduce your issue (or just copy the script here if it is short).
",0,"y_ture.shape is expected to be (N,3) yet (?,?,?,?) is received","y_ture.shape is expected to be (N,3) yet (?,?,?,?) is received My model looks something like this :

now the problem is in the y_ture.shape as whenever i try to access y_trrue to do needed calculation inside the loss function i get y_true.shape as 

y_true (?, ?, ?, ?)
y_true Tensor(""decoder/FinalConv_target:0"", shape=(?, ?, ?, ?), dtype=float32)

Thank you!

- [-] Check that you are up-to-date with the master branch of Keras. You can update with:
pip install git+git://github.com/fchollet/keras.git --upgrade --no-deps

- [-] If running on TensorFlow, check that you are up-to-date with the latest version. The installation instructions can be found [here](https://www.tensorflow.org/get_started/os_setup).

- [-] If running on Theano, check that you are up-to-date with the master branch of Theano. You can update with:
pip install git+git://github.com/Theano/Theano.git --upgrade --no-deps

- [-] Provide a link to a GitHub Gist of a Python script that can reproduce your issue (or just copy the script here if it is short).
"
keras,7055,"When given a variable length input, CNTK fixes it to the length of the first example.

The following example runs under Theano and Tensorflow.


CNTK fails, since once it has seen the first batch, it assumes the second dimension is fixed to 100:

",0,CNTK cannot take variable length inputs,"CNTK cannot take variable length inputs When given a variable length input, CNTK fixes it to the length of the first example.

The following example runs under Theano and Tensorflow.


CNTK fails, since once it has seen the first batch, it assumes the second dimension is fixed to 100:

"
keras,5083,"I believe there is a bug on the save/load mechanism of version 1.2.0. The problem did not exist on version 1.1.2.

To reproduce the bug I'll be using the following code snippet taken from the Keras documentation: [Fine-tune InceptionV3 on a new set of classes](https://keras.io/applications/)


Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/usr/local/lib/python2.7/dist-packages/keras/models.py"", line 143, in load_model
    model.load_weights_from_hdf5_group(f['model_weights'])
  File ""/usr/local/lib/python2.7/dist-packages/keras/engine/topology.py"", line 2753, in load_weights_from_hdf5_group
    str(len(flattened_layers)) + ' layers.')
ValueError: You are trying to load a weight file containing 190 layers into a model with 2 layers.",0,Bug on the save/load mechanism of version 1.2.0,"Bug on the save/load mechanism of version 1.2.0 I believe there is a bug on the save/load mechanism of version 1.2.0. The problem did not exist on version 1.1.2.

To reproduce the bug I'll be using the following code snippet taken from the Keras documentation: [Fine-tune InceptionV3 on a new set of classes](https://keras.io/applications/)


Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/usr/local/lib/python2.7/dist-packages/keras/models.py"", line 143, in load_model
    model.load_weights_from_hdf5_group(f['model_weights'])
  File ""/usr/local/lib/python2.7/dist-packages/keras/engine/topology.py"", line 2753, in load_weights_from_hdf5_group
    str(len(flattened_layers)) + ' layers.')
ValueError: You are trying to load a weight file containing 190 layers into a model with 2 layers."
keras,5088,"The following code produces this error: .

",0,"Saving a model that does not import the backend as ""K"" breaks model loading.","Saving a model that does not import the backend as ""K"" breaks model loading. The following code produces this error: .

"
keras,1722,"I would like to train a graph on task, add a layer, then resume training.  What is the best way to do this?  I won't always be adding a layer to the end of the graph.

I am assuming I will perform the first step with one graph, save the weights, and attempt to initialize a new graph with the weights learned in the first step.  The current code seems to rely upon the order of the params list when naming the weights in the hdf file.  If I want to be able to make changes to the graph that would change that order, I need a method for knowing which params to set when reloading the weights.

My current idea is to generate a unique name for each param based on the layer name, and store the params in the hdf file according to their unique names.  Then when loading the weights into the modified graph I would know which params to set.

Is this a sensible approach?  Would it break things?  Would it be compatible with all existing functionality?

thanks for your excellent work
",0,How to load weights learned in one network into another?,"How to load weights learned in one network into another? I would like to train a graph on task, add a layer, then resume training.  What is the best way to do this?  I won't always be adding a layer to the end of the graph.

I am assuming I will perform the first step with one graph, save the weights, and attempt to initialize a new graph with the weights learned in the first step.  The current code seems to rely upon the order of the params list when naming the weights in the hdf file.  If I want to be able to make changes to the graph that would change that order, I need a method for knowing which params to set when reloading the weights.

My current idea is to generate a unique name for each param based on the layer name, and store the params in the hdf file according to their unique names.  Then when loading the weights into the modified graph I would know which params to set.

Is this a sensible approach?  Would it break things?  Would it be compatible with all existing functionality?

thanks for your excellent work
"
keras,3580,"Newbie to Keras alert!
1. I need to compute a recursive network's error just on the last time step of a network and have that BPTT to correct the network, can this be done by changing the objective function?, is there some better way?
2. I need a recursive layer that doesn't have all recursive connections in the sense that each neuron only knows it's last output and not the outputs of all the neurons in its layer, can this be done by creating a model for each neuron and then merging the outputs? Could I force some of the weights to remain zero?
",0,I need to do some custom changes to a network's behavior.,"I need to do some custom changes to a network's behavior. Newbie to Keras alert!
1. I need to compute a recursive network's error just on the last time step of a network and have that BPTT to correct the network, can this be done by changing the objective function?, is there some better way?
2. I need a recursive layer that doesn't have all recursive connections in the sense that each neuron only knows it's last output and not the outputs of all the neurons in its layer, can this be done by creating a model for each neuron and then merging the outputs? Could I force some of the weights to remain zero?
"
keras,1023,"[1] show that combining filter lengths around the 'best' filter length achieves better sentence classification results than just using the 'best' filter length. Is there a possibility to specify a combination of filter lengths for 1D convolution? If not, is this a feature that should be implemented?

Zhang, Y., & Wallace, B. (2015). A Sensitivity Analysis of (and Practitioners’ Guide to) Convolutional Neural Networks for Sentence Classification, (1). Retrieved from http://arxiv.org/abs/1510.03820
",0,Combining different filter lengths in 1D convolutional layers,"Combining different filter lengths in 1D convolutional layers [1] show that combining filter lengths around the 'best' filter length achieves better sentence classification results than just using the 'best' filter length. Is there a possibility to specify a combination of filter lengths for 1D convolution? If not, is this a feature that should be implemented?

Zhang, Y., & Wallace, B. (2015). A Sensitivity Analysis of (and Practitioners’ Guide to) Convolutional Neural Networks for Sentence Classification, (1). Retrieved from http://arxiv.org/abs/1510.03820
"
keras,3295,"@fchollet and other kerasors,
We all know that if we are dealing with large scale of data such as ImageNet, we could write a customized generator which produces batch data (often as numpy.array) from disk. Then we could train our model with . But, if we want to use  to do the online data augmentation at the same time, what is the simplest way to implement? Note that I would like to use its  method instead of  method.
",0,How to use data (image) augmentation with fit_generator()?,"How to use data (image) augmentation with fit_generator()? @fchollet and other kerasors,
We all know that if we are dealing with large scale of data such as ImageNet, we could write a customized generator which produces batch data (often as numpy.array) from disk. Then we could train our model with . But, if we want to use  to do the online data augmentation at the same time, what is the simplest way to implement? Note that I would like to use its  method instead of  method.
"
keras,1989,"In the CNN example for the minst dataset:
https://github.com/fchollet/keras/blob/master/examples/mnist_cnn.py

they tell you how to make a good CNN network to recognise hand written digits.

The issue is that it doesn't tell you how to predict new digits.

For example give an image, if I do this:



instead of telling me what digits it think it is, it instead gives me a list of 10 numbers (presumably probabilities)

Please help!
",0,How do you predict values for categories?,"How do you predict values for categories? In the CNN example for the minst dataset:
https://github.com/fchollet/keras/blob/master/examples/mnist_cnn.py

they tell you how to make a good CNN network to recognise hand written digits.

The issue is that it doesn't tell you how to predict new digits.

For example give an image, if I do this:



instead of telling me what digits it think it is, it instead gives me a list of 10 numbers (presumably probabilities)

Please help!
"
keras,1917,"Hi,
I jut ran a CNN built with Keras on a big training set, and I has weird loss values at each epoch (see below):
66496/511502 [==>...........................] - ETA: 63s - loss: 8.2800
66528/511502 [==>...........................] - ETA: 63s - loss: -204433556137039776.0000

345664/511502 [===================>..........] - ETA: 23s - loss: 8.3174
345696/511502 [===================>..........] - ETA: 23s - loss: -39342531075525840.0000

214080/511502 [===========>..................] - ETA: 41s - loss: 8.3406
214112/511502 [===========>..................] - ETA: 41s - loss: -63520753730220536.0000

How is that possible? The loss becomes suddenly to big and the value gets bigger than the double encoding?
Is there a way to avoid it?

Regards,
",0,The loss becomes negative,"The loss becomes negative Hi,
I jut ran a CNN built with Keras on a big training set, and I has weird loss values at each epoch (see below):
66496/511502 [==>...........................] - ETA: 63s - loss: 8.2800
66528/511502 [==>...........................] - ETA: 63s - loss: -204433556137039776.0000

345664/511502 [===================>..........] - ETA: 23s - loss: 8.3174
345696/511502 [===================>..........] - ETA: 23s - loss: -39342531075525840.0000

214080/511502 [===========>..................] - ETA: 41s - loss: 8.3406
214112/511502 [===========>..................] - ETA: 41s - loss: -63520753730220536.0000

How is that possible? The loss becomes suddenly to big and the value gets bigger than the double encoding?
Is there a way to avoid it?

Regards,
"
keras,5861,"get_file has some serious limitations, even with untar=true it can't unzip  it assumes it is a  file.

What about changing the parameter from untar to uncompress, and doing something similar to what's discussed in this [stackoverflow link for identifying compressed files and uncomrpressing them](http://stackoverflow.com/questions/13044562/python-mechanism-to-identify-compressed-file-type-and-uncompress)?

The parameter could be  and have options , , ,  etc...

Also, the md5 check should be a sha2 check since md5 is known to be insecure.

Relevant datasets:

    # original PASCAL VOC 2012
    # wget http://host.robots.ox.ac.uk/pascal/VOC/voc2012/VOCtrainval_11-May-2012.tar # 2 GB

    # berkeley augmented Pascal VOC
    # wget http://www.eecs.berkeley.edu/Research/Projects/CS/vision/grouping/semantic_contours/benchmark.tgz # 1.3 GB",0,get_file() - reducing limitations,"get_file() - reducing limitations get_file has some serious limitations, even with untar=true it can't unzip  it assumes it is a  file.

What about changing the parameter from untar to uncompress, and doing something similar to what's discussed in this [stackoverflow link for identifying compressed files and uncomrpressing them](http://stackoverflow.com/questions/13044562/python-mechanism-to-identify-compressed-file-type-and-uncompress)?

The parameter could be  and have options , , ,  etc...

Also, the md5 check should be a sha2 check since md5 is known to be insecure.

Relevant datasets:

    # original PASCAL VOC 2012
    # wget http://host.robots.ox.ac.uk/pascal/VOC/voc2012/VOCtrainval_11-May-2012.tar # 2 GB

    # berkeley augmented Pascal VOC
    # wget http://www.eecs.berkeley.edu/Research/Projects/CS/vision/grouping/semantic_contours/benchmark.tgz # 1.3 GB"
keras,13459,"model.test_on_batch returns total loss instead of average - Keras 2.2.4 

Tensorflow is the issue and it can be fixed using this",0,model.test_on_batch returns total loss instead of average - Keras 2.2.4 ,"model.test_on_batch returns total loss instead of average - Keras 2.2.4  model.test_on_batch returns total loss instead of average - Keras 2.2.4 

Tensorflow is the issue and it can be fixed using this"
keras,3975,"I workings on multiple gpu and 'device' state does't save and nothing to load. Set default device for all after loading. I tried save mode full and save via YAML. Result the same.

Here simple test for understanding:



And output without device state.


",0,"After load model device (gpu0, gpu1, ...) don't loaded","After load model device (gpu0, gpu1, ...) don't loaded I workings on multiple gpu and 'device' state does't save and nothing to load. Set default device for all after loading. I tried save mode full and save via YAML. Result the same.

Here simple test for understanding:



And output without device state.


"
keras,2232,"

I want to get the the output of lstm layer in model0. I know how to get the internal output in the simple model without merging, like:



However, the inputs of my merging model have two styles of input. Please give me some suggestions!
",0,How can I get the internal output of left or right model?,"How can I get the internal output of left or right model? 

I want to get the the output of lstm layer in model0. I know how to get the internal output in the simple model without merging, like:



However, the inputs of my merging model have two styles of input. Please give me some suggestions!
"
keras,10221,"When using sample weights its important to use  instead of  to get the correct accuracy. However, the current implementation of load/save_model does not take these metrics in to account. 

Current implementations:

-  only saves metrics, see https://github.com/keras-team/keras/blob/master/keras/engine/saving.py#L144
-  only sets metrics, see https://github.com/keras-team/keras/blob/master/keras/engine/saving.py#L286

This problem only occurs after a loading a model and then trying to continue training (or using one of the weighted metrics). Ideally, the weighted metrics should also be saved and subsequently loaded.",0,"Weighted metrics are not loaded using load_model, only normal metrics are set","Weighted metrics are not loaded using load_model, only normal metrics are set When using sample weights its important to use  instead of  to get the correct accuracy. However, the current implementation of load/save_model does not take these metrics in to account. 

Current implementations:

-  only saves metrics, see https://github.com/keras-team/keras/blob/master/keras/engine/saving.py#L144
-  only sets metrics, see https://github.com/keras-team/keras/blob/master/keras/engine/saving.py#L286

This problem only occurs after a loading a model and then trying to continue training (or using one of the weighted metrics). Ideally, the weighted metrics should also be saved and subsequently loaded."
keras,11204,"**Test:** wrap the combined generator and discriminator in the multiple_gpu_model API
**Syntax:** 
**Reproduce issue:** https://gist.github.com/emilwallner/f2c411b83c499c1834fd1be6646f7389
**Error:** ValueError: The name ""model_1"" is used 2 times in the model. All layer names should be unique.",0,Can't use multiple GPUs with GANs,"Can't use multiple GPUs with GANs **Test:** wrap the combined generator and discriminator in the multiple_gpu_model API
**Syntax:** 
**Reproduce issue:** https://gist.github.com/emilwallner/f2c411b83c499c1834fd1be6646f7389
**Error:** ValueError: The name ""model_1"" is used 2 times in the model. All layer names should be unique."
keras,10685,"Can VGG16 be trained fresh with Hyperspectral Dataset with input shape like (5, 5, 30)? I have not used the functions. Write the full code for VGG16. ",0,Input_shape Problem with VGG16,"Input_shape Problem with VGG16 Can VGG16 be trained fresh with Hyperspectral Dataset with input shape like (5, 5, 30)? I have not used the functions. Write the full code for VGG16. "
keras,4238,"hi,all~
i wonder if there any body have ever use decovolution3D,  or unpooling3D ?
the inverse option to convolution3D or maxpooling option.

thank you in advance!

Please make sure that the boxes below are checked before you submit your issue. Thank you!

- [1 ] Check that you are up-to-date with the master branch of Keras. You can update with:
pip install git+git://github.com/fchollet/keras.git --upgrade --no-deps

- [1 ] If running on Theano, check that you are up-to-date with the master branch of Theano. You can update with:
pip install git+git://github.com/Theano/Theano.git --upgrade --no-deps

- [ 1] Provide a link to a GitHub Gist of a Python script that can reproduce your issue (or just copy the script here if it is short).",0,how to implemente deconv3D,"how to implemente deconv3D hi,all~
i wonder if there any body have ever use decovolution3D,  or unpooling3D ?
the inverse option to convolution3D or maxpooling option.

thank you in advance!

Please make sure that the boxes below are checked before you submit your issue. Thank you!

- [1 ] Check that you are up-to-date with the master branch of Keras. You can update with:
pip install git+git://github.com/fchollet/keras.git --upgrade --no-deps

- [1 ] If running on Theano, check that you are up-to-date with the master branch of Theano. You can update with:
pip install git+git://github.com/Theano/Theano.git --upgrade --no-deps

- [ 1] Provide a link to a GitHub Gist of a Python script that can reproduce your issue (or just copy the script here if it is short)."
keras,2603,"Hi,
I just finished to train my first RNN with keras, but I have two small technical problems. I have several Warning Messages because apparently something is slowing down my training phase.
The first problem is 'TimeDistributedDense()'. Apparently is deprecated and I am supposed to use 'TimeDistributed(Dense(...))' instead, as suggested by the warning msg itself. When I surf into keras documentation this method is cited in section 'Core', but the code line:

from keras.layers.core import TimeDistributed

gives me an error. How am I suposed to import TimeDistributed?

The second problem is 'on_batch_end()'. The original warning msg is the following:

Warning (from warnings module):
  File ""/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/keras/callbacks.py"", line 66
    % delta_t_median)
UserWarning: Method on_batch_end() is slow compared to the batch update (0.122654). Check your callbacks

I found another post here about this problem but people were saying that is due to heavy custom callbacks, but I didn't define any callback by my own. So what could be the problem?
",0,Slow training due to 'on_batch_end()' and 'TimeDistributedDense()',"Slow training due to 'on_batch_end()' and 'TimeDistributedDense()' Hi,
I just finished to train my first RNN with keras, but I have two small technical problems. I have several Warning Messages because apparently something is slowing down my training phase.
The first problem is 'TimeDistributedDense()'. Apparently is deprecated and I am supposed to use 'TimeDistributed(Dense(...))' instead, as suggested by the warning msg itself. When I surf into keras documentation this method is cited in section 'Core', but the code line:

from keras.layers.core import TimeDistributed

gives me an error. How am I suposed to import TimeDistributed?

The second problem is 'on_batch_end()'. The original warning msg is the following:

Warning (from warnings module):
  File ""/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/keras/callbacks.py"", line 66
    % delta_t_median)
UserWarning: Method on_batch_end() is slow compared to the batch update (0.122654). Check your callbacks

I found another post here about this problem but people were saying that is due to heavy custom callbacks, but I didn't define any callback by my own. So what could be the problem?
"
keras,5481,"Suppose I want to train a stateful RNN and I have two samples. For the first, I have 100 observations - 100 X values and 100 Y values to train on. For the second, I have 200 observations - 100 X values and 100 Y values to train on.

I set up a stateful RNN with a batch size of 2. I start training on batches of 2. All is well and good for the first 100 batches. But what happens on the 101st? My batch size really needs to be 1 at this point, but I don't believe I can just change the batch size mid-training.

Is masking intelligent enough to handle this situation? So, suppose I pad my first observation with 100 sets of zeros at the beginning of my X series. I also pad its Y series with 100 zeros. I use a masking layer to mask zeros from the input data. I know masking will prevent information from the X series from being used, but will it also prevent the model from doing anything with the dummy Y values?

I suppose it would also be possible to use a batch size of 1 throughout and simply reset the model state when I'm ready to switch samples. Is there any downside to doing so (other than a little more coding overhead)?

Is there another best practice for this?

I hope I have explained this well.",0,Stateful RNN with different lengths for each sample,"Stateful RNN with different lengths for each sample Suppose I want to train a stateful RNN and I have two samples. For the first, I have 100 observations - 100 X values and 100 Y values to train on. For the second, I have 200 observations - 100 X values and 100 Y values to train on.

I set up a stateful RNN with a batch size of 2. I start training on batches of 2. All is well and good for the first 100 batches. But what happens on the 101st? My batch size really needs to be 1 at this point, but I don't believe I can just change the batch size mid-training.

Is masking intelligent enough to handle this situation? So, suppose I pad my first observation with 100 sets of zeros at the beginning of my X series. I also pad its Y series with 100 zeros. I use a masking layer to mask zeros from the input data. I know masking will prevent information from the X series from being used, but will it also prevent the model from doing anything with the dummy Y values?

I suppose it would also be possible to use a batch size of 1 throughout and simply reset the model state when I'm ready to switch samples. Is there any downside to doing so (other than a little more coding overhead)?

Is there another best practice for this?

I hope I have explained this well."
keras,2486,"Hi,
I've built a layer called dissimilarity which returns :

but the following error raised on buildinf the Model:


The test code is:

    crop_right_bound = Boundary(1)(A)
    crop_left = Crop_Side(3,0)(B)

    dis = Dissimilarity([crop_left, crop_right_bound])

    patch_compare = Model([part_A_input, part_B_input], dis)

Do i need to configure anything else in my implementation of the layer?
",0,Unable to output custom layer,"Unable to output custom layer Hi,
I've built a layer called dissimilarity which returns :

but the following error raised on buildinf the Model:


The test code is:

    crop_right_bound = Boundary(1)(A)
    crop_left = Crop_Side(3,0)(B)

    dis = Dissimilarity([crop_left, crop_right_bound])

    patch_compare = Model([part_A_input, part_B_input], dis)

Do i need to configure anything else in my implementation of the layer?
"
keras,2882,"I try to train a network like [this](http://arxiv.org/pdf/1506.02640v5.pdf). But I've no idea about the objective function. how to implement the function that return 1 in some cases depend on the output and return 0 otherwise. ?
",0,custom objective function,"custom objective function I try to train a network like [this](http://arxiv.org/pdf/1506.02640v5.pdf). But I've no idea about the objective function. how to implement the function that return 1 in some cases depend on the output and return 0 otherwise. ?
"
keras,519,"@wxs , @fchollet : I'm trying to transplant CTC objective into Keras. When reading the codes, I noticed in  function of  class,  there are lines as



Since the weighted_objectivemasked_y_truemasked_y_predy_truey_pred`, does this mean the objective function should expect the shapes of masked variables instead of unmasked ones?
",0,Why the loss is weighted in compile() ?,"Why the loss is weighted in compile() ? @wxs , @fchollet : I'm trying to transplant CTC objective into Keras. When reading the codes, I noticed in  function of  class,  there are lines as



Since the weighted_objectivemasked_y_truemasked_y_predy_truey_pred`, does this mean the objective function should expect the shapes of masked variables instead of unmasked ones?
"
keras,4237,"Please make sure that the boxes below are checked before you submit your issue. Thank you!

- [x] Check that you are up-to-date with the master branch of Keras. You can update with:
pip install git+git://github.com/fchollet/keras.git --upgrade --no-deps

- [x] If running on Theano, check that you are up-to-date with the master branch of Theano. You can update with:
pip install git+git://github.com/Theano/Theano.git --upgrade --no-deps

- [x] Provide a link to a GitHub Gist of a Python script that can reproduce your issue (or just copy the script here if it is short).",0,"in keras, how to import the dataset made by myself, and whats the requirements of the data in the dataset?","in keras, how to import the dataset made by myself, and whats the requirements of the data in the dataset? Please make sure that the boxes below are checked before you submit your issue. Thank you!

- [x] Check that you are up-to-date with the master branch of Keras. You can update with:
pip install git+git://github.com/fchollet/keras.git --upgrade --no-deps

- [x] If running on Theano, check that you are up-to-date with the master branch of Theano. You can update with:
pip install git+git://github.com/Theano/Theano.git --upgrade --no-deps

- [x] Provide a link to a GitHub Gist of a Python script that can reproduce your issue (or just copy the script here if it is short)."
keras,11094,"Time-decay or Drop-decay maybe a good strategy to train a model, but I think a better way is to reduce the learning rate when valid loss increasing or valid acc decreasing.  But I can only change learning rate by passing a  parameter into LearningRateSchedule. How can I do it?",0,How to reduce learning rate based last epoch valid acc or valid loss?,"How to reduce learning rate based last epoch valid acc or valid loss? Time-decay or Drop-decay maybe a good strategy to train a model, but I think a better way is to reduce the learning rate when valid loss increasing or valid acc decreasing.  But I can only change learning rate by passing a  parameter into LearningRateSchedule. How can I do it?"
keras,1597,"I use LSTM to do a sequence labeling task, but I got the same acc and cal_acc for each epoch.
here is my code:

def moduleRnn(self):
        model = Sequential()
        model.add(LSTM(output_dim=64,input_length=self.seq_len,batch_input_shape=(16,1,200),input_dim=self.embed_length,return_sequences=True,stateful=False ))
        #model.add(LSTM(output_dim=16,return_sequences=True,stateful=False ))
        model.add(Dropout(0.2))
        model.add(TimeDistributedDense(output_dim=self.labs_len))
        model.add(Activation('softmax'))
        model.compile(loss=""categorical_crossentropy"" , optimizer='rmsprop' , class_mode='categorical')
        #model.fit(self.train,self.train_lab,batch_size=16,nb_epoch=3,verbose=1, validation_split=0.1,show_accuracy=True)
        model.fit(self.X_train,self.Y_train,batch_size=16,nb_epoch=15,verbose=1,show_accuracy=True,validation_split=0.2)
        score = model.evaluate(self.X_test,self.Y_test,batch_size=16)
        print score

Anyone meets the same problem?  please help me
",0, acc and val_acc don't change?," acc and val_acc don't change? I use LSTM to do a sequence labeling task, but I got the same acc and cal_acc for each epoch.
here is my code:

def moduleRnn(self):
        model = Sequential()
        model.add(LSTM(output_dim=64,input_length=self.seq_len,batch_input_shape=(16,1,200),input_dim=self.embed_length,return_sequences=True,stateful=False ))
        #model.add(LSTM(output_dim=16,return_sequences=True,stateful=False ))
        model.add(Dropout(0.2))
        model.add(TimeDistributedDense(output_dim=self.labs_len))
        model.add(Activation('softmax'))
        model.compile(loss=""categorical_crossentropy"" , optimizer='rmsprop' , class_mode='categorical')
        #model.fit(self.train,self.train_lab,batch_size=16,nb_epoch=3,verbose=1, validation_split=0.1,show_accuracy=True)
        model.fit(self.X_train,self.Y_train,batch_size=16,nb_epoch=15,verbose=1,show_accuracy=True,validation_split=0.2)
        score = model.evaluate(self.X_test,self.Y_test,batch_size=16)
        print score

Anyone meets the same problem?  please help me
"
keras,6296,"Hello everyone, I was confused by this problem for several days...

My question is that why the training time has such massive difference between that I set the batch_size to be ""1"" and ""20"" for my generator.

If I set the **_batch_size_** to be **1**, the **training time** of _**1 epoch**_ is approximately **180 ~ 200 sec**.
If I set the **_batch_size_** to be **20**, the **training time** of **1 epoch** is approximately **3000 ~ 3200 sec**. 

However, this horrible difference between these training times seems to be abnormal..., since it should be the reversed result:
batch_size = 1, training time -> 3000 ~ 3200 sec.
batch_size = 20, training time -> 180 ~ 200 sec.

The input to my generator is not the file path, but the numpy arrays which are already loaded into the
memory via calling ""np.load()"".
So I think the I/O trade-off issue doesn't exist.

I'm using Keras-2.0.3 and my backend is tensorflow-gpu 1.0.1

I have seen the update of this merged [PR](https://github.com/fchollet/keras/pull/5879/files),
but it seems that this change won't affect anything at all. (the usage is just the same with original one)

The [link](https://gist.github.com/HappyStorm/cb6c22ffec18a8fbb4912e9c79b6d87c) here is the gist of my self-defined generator and the part of my fit_generator.

Could somebody help me explain this problem...???
Thank you so much...Orz",0,"What's the difference between ""samples_per_epoch"" and ""steps_per_epoch""","What's the difference between ""samples_per_epoch"" and ""steps_per_epoch"" Hello everyone, I was confused by this problem for several days...

My question is that why the training time has such massive difference between that I set the batch_size to be ""1"" and ""20"" for my generator.

If I set the **_batch_size_** to be **1**, the **training time** of _**1 epoch**_ is approximately **180 ~ 200 sec**.
If I set the **_batch_size_** to be **20**, the **training time** of **1 epoch** is approximately **3000 ~ 3200 sec**. 

However, this horrible difference between these training times seems to be abnormal..., since it should be the reversed result:
batch_size = 1, training time -> 3000 ~ 3200 sec.
batch_size = 20, training time -> 180 ~ 200 sec.

The input to my generator is not the file path, but the numpy arrays which are already loaded into the
memory via calling ""np.load()"".
So I think the I/O trade-off issue doesn't exist.

I'm using Keras-2.0.3 and my backend is tensorflow-gpu 1.0.1

I have seen the update of this merged [PR](https://github.com/fchollet/keras/pull/5879/files),
but it seems that this change won't affect anything at all. (the usage is just the same with original one)

The [link](https://gist.github.com/HappyStorm/cb6c22ffec18a8fbb4912e9c79b6d87c) here is the gist of my self-defined generator and the part of my fit_generator.

Could somebody help me explain this problem...???
Thank you so much...Orz"
keras,9783,"I noticed a weird difference in the Model API.  the  class inherits , but changes the signature of the  function.   Is there a reason for this? Seems like those two APIs should be consistent. 

https://github.com/keras-team/keras/blob/eb97bc385599dec8182963fe263bd958b9ab0057/keras/models.py#L1355-L1363
vs.
https://github.com/keras-team/keras/blob/eb97bc385599dec8182963fe263bd958b9ab0057/keras/engine/topology.py#L2326
",0,"get_config on keras.models.Sequential model returns list, but get_config on keras.engine.training.Model returns dict","get_config on keras.models.Sequential model returns list, but get_config on keras.engine.training.Model returns dict I noticed a weird difference in the Model API.  the  class inherits , but changes the signature of the  function.   Is there a reason for this? Seems like those two APIs should be consistent. 

https://github.com/keras-team/keras/blob/eb97bc385599dec8182963fe263bd958b9ab0057/keras/models.py#L1355-L1363
vs.
https://github.com/keras-team/keras/blob/eb97bc385599dec8182963fe263bd958b9ab0057/keras/engine/topology.py#L2326
"
keras,13024,"**System information**  
- Have I written custom code (as opposed to using example directory):  
- Linux Ubuntu 16.04  
- TensorFlow backend yes
- TensorFlow version:  ''v1.13.1-0-g6612da8951' 1.13.1
- Keras version:  2.2.4
- Python version: 3.6.8
- CUDA/cuDNN version:  CUDA 10.0
- GPU model and memory:  4 GB

i was tried continues training in keras.
because i was build keras multiclass classification model, after i have new labels, and values. so i want to build new model without retraining. that is why i tried continuous train in keras. 


after completing save the model , i want to do continues training. so i tried,


i tried this. but it was thrown error.like previously 10 classes. 

but now we add new class means error occurred.

so what is my question is, **is it possible for continues training in keras for multiclass classification for new class?**


Thank you :)",0,is it possible for continues training in keras for multiclass classification for new class?,"is it possible for continues training in keras for multiclass classification for new class? **System information**  
- Have I written custom code (as opposed to using example directory):  
- Linux Ubuntu 16.04  
- TensorFlow backend yes
- TensorFlow version:  ''v1.13.1-0-g6612da8951' 1.13.1
- Keras version:  2.2.4
- Python version: 3.6.8
- CUDA/cuDNN version:  CUDA 10.0
- GPU model and memory:  4 GB

i was tried continues training in keras.
because i was build keras multiclass classification model, after i have new labels, and values. so i want to build new model without retraining. that is why i tried continuous train in keras. 


after completing save the model , i want to do continues training. so i tried,


i tried this. but it was thrown error.like previously 10 classes. 

but now we add new class means error occurred.

so what is my question is, **is it possible for continues training in keras for multiclass classification for new class?**


Thank you :)"
keras,3926,"I have a network that predicts images Y. How can I output to file all the predicted images in each batch, so that I can monitor the progress during training? 

My current understanding is to write a callback that calls  in the  routine, but I am not sure where the actual batch data X is going to be? (In my case, I have a custom generator creating X, and the model is fit using . 

Does anyone know how to do this, or are there any examples? Thanks! 
",0,Output batch images,"Output batch images I have a network that predicts images Y. How can I output to file all the predicted images in each batch, so that I can monitor the progress during training? 

My current understanding is to write a callback that calls  in the  routine, but I am not sure where the actual batch data X is going to be? (In my case, I have a custom generator creating X, and the model is fit using . 

Does anyone know how to do this, or are there any examples? Thanks! 
"
keras,6171,"**Environment:**
TypeErrorTypeError: Value passed to parameter 'shape' has DataType float32 not in list of allowed values: int32, int64`.

The Flatten layer's output is float32. However, it seems like the Dense layer only accepts int32, int64.



**Error Output:**


All help are appreciated. Thank you!
",0,"[Dense Layer Issue] TypeError: Value passed to parameter 'shape' has DataType float32 not in list of allowed values: int32, int64","[Dense Layer Issue] TypeError: Value passed to parameter 'shape' has DataType float32 not in list of allowed values: int32, int64 **Environment:**
TypeErrorTypeError: Value passed to parameter 'shape' has DataType float32 not in list of allowed values: int32, int64`.

The Flatten layer's output is float32. However, it seems like the Dense layer only accepts int32, int64.



**Error Output:**


All help are appreciated. Thank you!
"
keras,7432,"I was using the standard code, that I found in the internet.
This code is about using scikit-learn to select the parameter by using Keras.
I am using the code here
http://machinelearningmastery.com/grid-search-hyperparameters-deep-learning-models-python-keras/
I am not sure if this problem is because of keras or scikit-learn.

Then I got some errors.


I have no idea what happens.",0,forrtl: error (200): program aborting due to control-C event,"forrtl: error (200): program aborting due to control-C event I was using the standard code, that I found in the internet.
This code is about using scikit-learn to select the parameter by using Keras.
I am using the code here
http://machinelearningmastery.com/grid-search-hyperparameters-deep-learning-models-python-keras/
I am not sure if this problem is because of keras or scikit-learn.

Then I got some errors.


I have no idea what happens."
keras,4613,"## Problem configuration
- using tensorflow as backed. 
- computer with 1GPU card and 12 CPUs
- not distributed learning over cluster
- with only one session, use GPU or use CPUs. Not using both of them at any time.

## Way to force keras calling tensorflow in GPU or CPUs
###  run keras in CPU


### run keras in GPU
I don't write anything, let everything be as default. So since I install tensorflow in GPU version. It should assume default using GPU.


This undocumented trick works for me so far. However, since keras is a blackbox to me, while tensorflow is more structured and clear, I feel there should be an improvement for keras to better control the CPU/GPU device **with in keras**. I know we could just use keras as simplified tensorflow layer constructor. Thus it is possible to run everything under framework of tensorflow rather than living in the world of keras.

## Comment:
If you have alternative ways to force keras be used in CPU or GPU, please comment below and let everyone else know.

Best,
Shaowu
",0,Can keras model run on specific device?,"Can keras model run on specific device? ## Problem configuration
- using tensorflow as backed. 
- computer with 1GPU card and 12 CPUs
- not distributed learning over cluster
- with only one session, use GPU or use CPUs. Not using both of them at any time.

## Way to force keras calling tensorflow in GPU or CPUs
###  run keras in CPU


### run keras in GPU
I don't write anything, let everything be as default. So since I install tensorflow in GPU version. It should assume default using GPU.


This undocumented trick works for me so far. However, since keras is a blackbox to me, while tensorflow is more structured and clear, I feel there should be an improvement for keras to better control the CPU/GPU device **with in keras**. I know we could just use keras as simplified tensorflow layer constructor. Thus it is possible to run everything under framework of tensorflow rather than living in the world of keras.

## Comment:
If you have alternative ways to force keras be used in CPU or GPU, please comment below and let everyone else know.

Best,
Shaowu
"
keras,3420,"if axis == -1,  then line 507 will be 
      broadcastable = x.broadcastable[:-1] + x.broadcastable[0:]
and will not remove   @@correctly?

504 def squeeze(x, axis):
505    '''Remove a 1-dimension from the tensor at index ""axis"".
506    '''
507    broadcastable = x.broadcastable[:axis] + x.broadcastable[axis+1:]
508    x = T.patternbroadcast(x, [i == axis for i in range(x.type.ndim)])
509    x = T.squeeze(x)
510    x = T.patternbroadcast(x, broadcastable)
511    return x
",0,"maybe a bug in "" keras/backend/theano_backend.py: line 507 in function squeeze""??","maybe a bug in "" keras/backend/theano_backend.py: line 507 in function squeeze""?? if axis == -1,  then line 507 will be 
      broadcastable = x.broadcastable[:-1] + x.broadcastable[0:]
and will not remove   @@correctly?

504 def squeeze(x, axis):
505    '''Remove a 1-dimension from the tensor at index ""axis"".
506    '''
507    broadcastable = x.broadcastable[:axis] + x.broadcastable[axis+1:]
508    x = T.patternbroadcast(x, [i == axis for i in range(x.type.ndim)])
509    x = T.squeeze(x)
510    x = T.patternbroadcast(x, broadcastable)
511    return x
"
keras,7890,"> Exception in thread Thread-8:
> Traceback (most recent call last):
>   File ""/users/gpu/rohitg1/miniconda2/envs/tensorflow/lib/python2.7/threading.py"", line 801, in __bootstrap_inner
>     self.run()
>   File ""/users/gpu/rohitg1/miniconda2/envs/tensorflow/lib/python2.7/threading.py"", line 754, in run
>     self.__target(*self.__args, **self.__kwargs)
>   File ""/users/gpu/rohitg1/.local/lib/python2.7/site-packages/keras/utils/data_utils.py"", line 492, in _run
>     self.sequence.on_epoch_end()
>   File ""/users/gpu/rohitg1/.local/lib/python2.7/site-packages/keras/utils/data_utils.py"", line 358, in on_epoch_end
>     raise NotImplementedError
> NotImplementedError

I am trying to train a Video classification model. Gist for portion of code is available here:

https://gist.github.com/rohit-gupta/7668b79389e29598ace41813fc1a50d2


Do I need to implement a ""on_epoch_end"" function in my Sequence object to get this o work ?",0,"Getting NotImplementedError for ""on_epoch_end"" while training using fit_generator on a Sequence object","Getting NotImplementedError for ""on_epoch_end"" while training using fit_generator on a Sequence object > Exception in thread Thread-8:
> Traceback (most recent call last):
>   File ""/users/gpu/rohitg1/miniconda2/envs/tensorflow/lib/python2.7/threading.py"", line 801, in __bootstrap_inner
>     self.run()
>   File ""/users/gpu/rohitg1/miniconda2/envs/tensorflow/lib/python2.7/threading.py"", line 754, in run
>     self.__target(*self.__args, **self.__kwargs)
>   File ""/users/gpu/rohitg1/.local/lib/python2.7/site-packages/keras/utils/data_utils.py"", line 492, in _run
>     self.sequence.on_epoch_end()
>   File ""/users/gpu/rohitg1/.local/lib/python2.7/site-packages/keras/utils/data_utils.py"", line 358, in on_epoch_end
>     raise NotImplementedError
> NotImplementedError

I am trying to train a Video classification model. Gist for portion of code is available here:

https://gist.github.com/rohit-gupta/7668b79389e29598ace41813fc1a50d2


Do I need to implement a ""on_epoch_end"" function in my Sequence object to get this o work ?"
keras,2808,"I'm confused about the  parameter in  function of keras, how does it work concretely, for example, I use a learning rate of , and set the  parameter to be , anyone can help me? Thanks in advance!
",0,How does the parameter of decay in keras work concretely?,"How does the parameter of decay in keras work concretely? I'm confused about the  parameter in  function of keras, how does it work concretely, for example, I use a learning rate of , and set the  parameter to be , anyone can help me? Thanks in advance!
"
keras,4161,"I'm using Keras with TensorFlow to train a large number of tiny networks (~4 layers, less than 30 nodes in each layer). Currently TF allocates all GPU memory to a single process and therefore prevents me from opening more learning processes in parallel. I found on TF document that I can use 



to do this. However, I wasn't able to integrate that into keras. Does someone know the way to initialize a tf session on keras? Thank you very much!
",0,Using allow_growth on keras with tensorflow,"Using allow_growth on keras with tensorflow I'm using Keras with TensorFlow to train a large number of tiny networks (~4 layers, less than 30 nodes in each layer). Currently TF allocates all GPU memory to a single process and therefore prevents me from opening more learning processes in parallel. I found on TF document that I can use 



to do this. However, I wasn't able to integrate that into keras. Does someone know the way to initialize a tf session on keras? Thank you very much!
"
keras,10426,"I think  it uses functions like binary_entropy , binary_accuracy etc.   Is any smoothing used ?   Some description in the documentation will be good ( pointers to the exact code will also help). ",0,How does model.fit () calculate loss and acc ? Documentation will  be helpful. ,"How does model.fit () calculate loss and acc ? Documentation will  be helpful.  I think  it uses functions like binary_entropy , binary_accuracy etc.   Is any smoothing used ?   Some description in the documentation will be good ( pointers to the exact code will also help). "
keras,2776,"Is it possible to have a scalar Input() instance?

I can use  to get output from  correctly but cannot use the input defined by  as it is not a Keras Tensor.

Is there a way to generate a scalar Input(), either directly or by converting the backend Tensor object?
",0,Scalar Input(),"Scalar Input() Is it possible to have a scalar Input() instance?

I can use  to get output from  correctly but cannot use the input defined by  as it is not a Keras Tensor.

Is there a way to generate a scalar Input(), either directly or by converting the backend Tensor object?
"
keras,7945,"
Traceback (most recent call last):
  File ""seq2seq.py"", line 90, in <module>
    Seq2seq.train_seq2seq()
  File ""seq2seq.py"", line 74, in train_seq2seq
    model = s2s.seq2seq_plain()
  File ""/home/david/Downloads/Seq2Seq-master/seq2seq/model.py"", line 28, in seq2seq_plain
    model.add(RNN(self.hidden_dim, return_sequences=True))#, input_shape=(100, 128)))
  File ""/usr/local/lib/python2.7/dist-packages/keras/models.py"", line 327, in add
    output_tensor = layer(self.outputs[0])
  File ""/usr/local/lib/python2.7/dist-packages/keras/engine/topology.py"", line 543, in __call__
    self.build(input_shapes[0])
  File ""/usr/local/lib/python2.7/dist-packages/keras/layers/recurrent.py"", line 763, in build
    self.W = K.concatenate([self.W_i, self.W_f, self.W_c, self.W_o])
  File ""/usr/local/lib/python2.7/dist-packages/keras/backend/tensorflow_backend.py"", line 1222, in concatenate
    return tf.concat(axis, [to_dense(x) for x in tensors])
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/array_ops.py"", line 1061, in concat
    dtype=dtypes.int32).get_shape(
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 611, in convert_to_tensor
    as_ref=False)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 676, in internal_convert_to_tensor
    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/constant_op.py"", line 121, in _constant_tensor_conversion_function
    return constant(v, dtype=dtype, name=name)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/constant_op.py"", line 102, in constant
    tensor_util.make_tensor_proto(value, dtype=dtype, shape=shape, verify_shape=verify_shape))
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/tensor_util.py"", line 376, in make_tensor_proto
    _AssertCompatible(values, dtype)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/tensor_util.py"", line 302, in _AssertCompatible
    (dtype.name, repr(mismatch), type(mismatch).__name__))
TypeError: Expected int32, got <tf.Variable 'lstm_1_W_i:0' shape=(100, 200) dtype=float32_ref> of type 'Variable' instead.
`",0,Error while running Seq2seq model in Tensorflow version 1.3 and keras version 1.2.0,"Error while running Seq2seq model in Tensorflow version 1.3 and keras version 1.2.0 
Traceback (most recent call last):
  File ""seq2seq.py"", line 90, in <module>
    Seq2seq.train_seq2seq()
  File ""seq2seq.py"", line 74, in train_seq2seq
    model = s2s.seq2seq_plain()
  File ""/home/david/Downloads/Seq2Seq-master/seq2seq/model.py"", line 28, in seq2seq_plain
    model.add(RNN(self.hidden_dim, return_sequences=True))#, input_shape=(100, 128)))
  File ""/usr/local/lib/python2.7/dist-packages/keras/models.py"", line 327, in add
    output_tensor = layer(self.outputs[0])
  File ""/usr/local/lib/python2.7/dist-packages/keras/engine/topology.py"", line 543, in __call__
    self.build(input_shapes[0])
  File ""/usr/local/lib/python2.7/dist-packages/keras/layers/recurrent.py"", line 763, in build
    self.W = K.concatenate([self.W_i, self.W_f, self.W_c, self.W_o])
  File ""/usr/local/lib/python2.7/dist-packages/keras/backend/tensorflow_backend.py"", line 1222, in concatenate
    return tf.concat(axis, [to_dense(x) for x in tensors])
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/array_ops.py"", line 1061, in concat
    dtype=dtypes.int32).get_shape(
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 611, in convert_to_tensor
    as_ref=False)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 676, in internal_convert_to_tensor
    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/constant_op.py"", line 121, in _constant_tensor_conversion_function
    return constant(v, dtype=dtype, name=name)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/constant_op.py"", line 102, in constant
    tensor_util.make_tensor_proto(value, dtype=dtype, shape=shape, verify_shape=verify_shape))
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/tensor_util.py"", line 376, in make_tensor_proto
    _AssertCompatible(values, dtype)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/tensor_util.py"", line 302, in _AssertCompatible
    (dtype.name, repr(mismatch), type(mismatch).__name__))
TypeError: Expected int32, got <tf.Variable 'lstm_1_W_i:0' shape=(100, 200) dtype=float32_ref> of type 'Variable' instead.
`"
keras,3041,"If I have a model defined like this, no issue 



But if the  is a component of another sub model, like so:



I get the following error:

BatchNormalizationmode=2BatchNormalization

If I remove the  object from the sub model it works just fine. 

Obviously the intended case for this, the sub model is more complicated. 
- [X] Check that you are up-to-date with the master branch of Keras. You can update with:
  pip install git+git://github.com/fchollet/keras.git --upgrade --no-deps
- [X] If running on Theano, check that you are up-to-date with the master branch of Theano. You can update with:
  pip install git+git://github.com/Theano/Theano.git --upgrade --no-deps
- [X] Provide a link to a GitHub Gist of a Python script that can reproduce your issue (or just copy the script here if it is short).
",0,BatchNormalization can't be used across TimeDistributed when component of sub model,"BatchNormalization can't be used across TimeDistributed when component of sub model If I have a model defined like this, no issue 



But if the  is a component of another sub model, like so:



I get the following error:

BatchNormalizationmode=2BatchNormalization

If I remove the  object from the sub model it works just fine. 

Obviously the intended case for this, the sub model is more complicated. 
- [X] Check that you are up-to-date with the master branch of Keras. You can update with:
  pip install git+git://github.com/fchollet/keras.git --upgrade --no-deps
- [X] If running on Theano, check that you are up-to-date with the master branch of Theano. You can update with:
  pip install git+git://github.com/Theano/Theano.git --upgrade --no-deps
- [X] Provide a link to a GitHub Gist of a Python script that can reproduce your issue (or just copy the script here if it is short).
"
keras,5448,I am trying to use a pooling layer (both 1D and 2D) with border_mode = 'same' but it seems to change the dimensions anyway,0,border_mode = 'same' in pooling layers still changes the dimentions ,border_mode = 'same' in pooling layers still changes the dimentions  I am trying to use a pooling layer (both 1D and 2D) with border_mode = 'same' but it seems to change the dimensions anyway
keras,1628,"I'm trying to train a multi-task regression model, but my outputs are not complete (in fact I only have on average <1% of the values per training instance).  I expected mean squared error for the non-null outputs to be a reasonable objective, however, obviously using keras' mean squared error objective, the cost comes out as , as the nans will propagate.  

Is there any plans for supporting this sort of thing (or is it already supported somehow and I missed it?)

If not, anyone have an idea of a hack? I tried writing a new cost function, like:



(sorry for mixing keras and theano!)

This evaluates correctly on 1D vectors with nans in the y_true, but this doesn't work with keras, even with batch size set to 1.  My next plan was to set the NaNs in y_true to equal y_pred, but I'm not experienced with theano.
",0,Using outputs with missing values,"Using outputs with missing values I'm trying to train a multi-task regression model, but my outputs are not complete (in fact I only have on average <1% of the values per training instance).  I expected mean squared error for the non-null outputs to be a reasonable objective, however, obviously using keras' mean squared error objective, the cost comes out as , as the nans will propagate.  

Is there any plans for supporting this sort of thing (or is it already supported somehow and I missed it?)

If not, anyone have an idea of a hack? I tried writing a new cost function, like:



(sorry for mixing keras and theano!)

This evaluates correctly on 1D vectors with nans in the y_true, but this doesn't work with keras, even with batch size set to 1.  My next plan was to set the NaNs in y_true to equal y_pred, but I'm not experienced with theano.
"
keras,9634,"Let's say I have a dataset of shape **(30, 2)**, which means I have **30 samples**, and **2 features**, which are sales and promotion. I want to build a model to perform 1 day ahead forecasting of sales, based on **sales of the last 5 days** and **promotion of the last 5 days** and **the current day**. By doing some shifting and removing NA's, I could get a new dataframe with 25 rows, and 12 features, which are, **promotion(t-5), sales(t-5), promotion(t-4), sales(t-4), ... , promotion(t-1), sales(t-1), promotion(t), sales(t)**. The last column will be treated as response **y**, and the first 11 columns will be treated as **X**, the question is how to shape X, should I add a new column with constant and shape it into (25, 6, 2)?  the promotion(t) column has to be included in X",0,LSTM - how to reshape the input dataset if prediction is made based on both past and current features,"LSTM - how to reshape the input dataset if prediction is made based on both past and current features Let's say I have a dataset of shape **(30, 2)**, which means I have **30 samples**, and **2 features**, which are sales and promotion. I want to build a model to perform 1 day ahead forecasting of sales, based on **sales of the last 5 days** and **promotion of the last 5 days** and **the current day**. By doing some shifting and removing NA's, I could get a new dataframe with 25 rows, and 12 features, which are, **promotion(t-5), sales(t-5), promotion(t-4), sales(t-4), ... , promotion(t-1), sales(t-1), promotion(t), sales(t)**. The last column will be treated as response **y**, and the first 11 columns will be treated as **X**, the question is how to shape X, should I add a new column with constant and shape it into (25, 6, 2)?  the promotion(t) column has to be included in X"
keras,7262,"Hi, I am new to Keras and RNN
I have a dataset that has 1000 videos, each video has 2000 frames and each frame has 5 features. I would like to train a RNN that can classify each video frame to 1 of the 3 categories. 

Example
Video1
  Frame1 [fc11,fc12,fc13,fc14,fc15] -> output1
  Frame2 [fc21,fc22,fc23,fc24,fc25] -> output2
  Frame3 [fc31,fc32,fc33,fc34,fc35] -> output3
  ...
Video2
...


x_shape = [1000, 2000, 5]
y_shape = [1000, 2000, 3] (Since each frame has it's own y value)
I am a little confused how to construct a Model that will fit this shape
",0,Shape of output of RNN,"Shape of output of RNN Hi, I am new to Keras and RNN
I have a dataset that has 1000 videos, each video has 2000 frames and each frame has 5 features. I would like to train a RNN that can classify each video frame to 1 of the 3 categories. 

Example
Video1
  Frame1 [fc11,fc12,fc13,fc14,fc15] -> output1
  Frame2 [fc21,fc22,fc23,fc24,fc25] -> output2
  Frame3 [fc31,fc32,fc33,fc34,fc35] -> output3
  ...
Video2
...


x_shape = [1000, 2000, 5]
y_shape = [1000, 2000, 3] (Since each frame has it's own y value)
I am a little confused how to construct a Model that will fit this shape
"
keras,2586,"Hello,

I've got the latest pulls of keras and theano.

I am trying to reproduce the code from [this blog post](http://cbonnett.github.io/MDN.html).

What I have so far is in a gist [here](https://gist.github.com/sergeyf/cf20b2759a7d38035f30384769bed9df).

The issue is that after a few batch updates (on the order of 10), the loss becomes nan.

Here is a repaste of the most likely culprit: the negative log-likelihood loss function:



I've tried to alter various parts of this function, and can't spot any specific issues.

I've added the log-sum-exp trick to prevent underflow in there, and when I check out values of each intermediate variable within the loss function during any of the batches before the nan loss, they all seem very well behaved.

Any ideas?

Thanks for your help, and the fabulous package.
",0,Mixture Density Network quickly gets to nan loss,"Mixture Density Network quickly gets to nan loss Hello,

I've got the latest pulls of keras and theano.

I am trying to reproduce the code from [this blog post](http://cbonnett.github.io/MDN.html).

What I have so far is in a gist [here](https://gist.github.com/sergeyf/cf20b2759a7d38035f30384769bed9df).

The issue is that after a few batch updates (on the order of 10), the loss becomes nan.

Here is a repaste of the most likely culprit: the negative log-likelihood loss function:



I've tried to alter various parts of this function, and can't spot any specific issues.

I've added the log-sum-exp trick to prevent underflow in there, and when I check out values of each intermediate variable within the loss function during any of the batches before the nan loss, they all seem very well behaved.

Any ideas?

Thanks for your help, and the fabulous package.
"
keras,2361,"Hi, I got an error while fitting a model on Keras 1.0 on Theano on Windows 10 x64. Previously, I can fit Keras models on Keras 0.6 with Theano 0.8-dev just fine. Here a simple Perceptron code to demonstrate the error while fitting:





Is this a code regression in Keras or just another installation error? If so, any tips to debug it?

Thanks.

**Additional Info:**
- [x] Up-to-date with the master branch of Keras
- [x] Up-to-date with the master branch of Theano
- OS: Windows 10 x64
- Running on CPU
- Backend: Theano
- can import theano
",0,ImportError while fitting a Keras model,"ImportError while fitting a Keras model Hi, I got an error while fitting a model on Keras 1.0 on Theano on Windows 10 x64. Previously, I can fit Keras models on Keras 0.6 with Theano 0.8-dev just fine. Here a simple Perceptron code to demonstrate the error while fitting:





Is this a code regression in Keras or just another installation error? If so, any tips to debug it?

Thanks.

**Additional Info:**
- [x] Up-to-date with the master branch of Keras
- [x] Up-to-date with the master branch of Theano
- OS: Windows 10 x64
- Running on CPU
- Backend: Theano
- can import theano
"
keras,6838,"I have searched high and low but found no satisfying results...
So, here is the problem:
The neural network outputs a (4,) array, and the predicted labels should be those greater than 0.5.
The problem is, top_k_categorical_accuracy won't work.(of course)
And, when I tried to implement one by my own,
I noticed that:

the parameter **y_true and y_pred** are tensors. And what I tried to do is in-place assignment, making those values greater than 0.5 become 1 and others 0.
Any ideas?",0,Customize metrics: binary_accuracy of outputs > 0.5,"Customize metrics: binary_accuracy of outputs > 0.5 I have searched high and low but found no satisfying results...
So, here is the problem:
The neural network outputs a (4,) array, and the predicted labels should be those greater than 0.5.
The problem is, top_k_categorical_accuracy won't work.(of course)
And, when I tried to implement one by my own,
I noticed that:

the parameter **y_true and y_pred** are tensors. And what I tried to do is in-place assignment, making those values greater than 0.5 become 1 and others 0.
Any ideas?"
keras,7288,"I've managed to finish experiments using attention mechanism adopted from [@cbaziotis implementation](https://gist.github.com/cbaziotis/7ef97ccf71cbc14366835198c09809d2) and now i'm confused with the visualization. I don't really understand the heatmap as well. If you guys can explain these to me, it means a lot :

1. What is heatmap? how to read them?
2. What to visualize in an attention mechanism? The weight?
3. The code to visualize an attention mechanism, in conjunction to [@cbaziotis' implementation](https://gist.github.com/cbaziotis/7ef97ccf71cbc14366835198c09809d2)

Thanks in advance!",0,How to visualize an attention mechanism in a classification task?,"How to visualize an attention mechanism in a classification task? I've managed to finish experiments using attention mechanism adopted from [@cbaziotis implementation](https://gist.github.com/cbaziotis/7ef97ccf71cbc14366835198c09809d2) and now i'm confused with the visualization. I don't really understand the heatmap as well. If you guys can explain these to me, it means a lot :

1. What is heatmap? how to read them?
2. What to visualize in an attention mechanism? The weight?
3. The code to visualize an attention mechanism, in conjunction to [@cbaziotis' implementation](https://gist.github.com/cbaziotis/7ef97ccf71cbc14366835198c09809d2)

Thanks in advance!"
keras,7956,"Please make sure that the boxes below are checked before you submit your issue. If your issue is an implementation question, please ask your question on [StackOverflow](http://stackoverflow.com/questions/tagged/keras) or [join the Keras Slack channel](https://keras-slack-autojoin.herokuapp.com/) and ask there instead of filing a GitHub issue.

Thank you!

- [ Y] Check that you are up-to-date with the master branch of Keras. You can update with:
pip install git+git://github.com/fchollet/keras.git --upgrade --no-deps

- [ ] If running on TensorFlow, check that you are up-to-date with the latest version. The installation instructions can be found [here](https://www.tensorflow.org/get_started/os_setup).

- [Y] If running on Theano, check that you are up-to-date with the master branch of Theano. You can update with:
pip install git+git://github.com/Theano/Theano.git --upgrade --no-deps

- [ ] Provide a link to a GitHub Gist of a Python script that can reproduce your issue (or just copy the script here if it is short).
",0,How can we know the final value of alpha that is learned in PReLU? ,"How can we know the final value of alpha that is learned in PReLU?  Please make sure that the boxes below are checked before you submit your issue. If your issue is an implementation question, please ask your question on [StackOverflow](http://stackoverflow.com/questions/tagged/keras) or [join the Keras Slack channel](https://keras-slack-autojoin.herokuapp.com/) and ask there instead of filing a GitHub issue.

Thank you!

- [ Y] Check that you are up-to-date with the master branch of Keras. You can update with:
pip install git+git://github.com/fchollet/keras.git --upgrade --no-deps

- [ ] If running on TensorFlow, check that you are up-to-date with the latest version. The installation instructions can be found [here](https://www.tensorflow.org/get_started/os_setup).

- [Y] If running on Theano, check that you are up-to-date with the master branch of Theano. You can update with:
pip install git+git://github.com/Theano/Theano.git --upgrade --no-deps

- [ ] Provide a link to a GitHub Gist of a Python script that can reproduce your issue (or just copy the script here if it is short).
"
keras,5912,"The IMDB dataset in imdb.npz is broken or at least incompatible with the provided word index--converting reviews to a string gives nonsense (either the wrong words or mixed up word order--not sure). Example:



Output:


- [x] Check that you are up-to-date with the master branch of Keras. You can update with:
pip install git+git://github.com/fchollet/keras.git --upgrade --no-deps

- [x] If running on TensorFlow, check that you are up-to-date with the latest version. The installation instructions can be found [here](https://www.tensorflow.org/get_started/os_setup).

- [x ] Provide a link to a GitHub Gist of a Python script that can reproduce your issue (or just copy the script here if it is short).",0,New version of imdb dataset is broken (word order?),"New version of imdb dataset is broken (word order?) The IMDB dataset in imdb.npz is broken or at least incompatible with the provided word index--converting reviews to a string gives nonsense (either the wrong words or mixed up word order--not sure). Example:



Output:


- [x] Check that you are up-to-date with the master branch of Keras. You can update with:
pip install git+git://github.com/fchollet/keras.git --upgrade --no-deps

- [x] If running on TensorFlow, check that you are up-to-date with the latest version. The installation instructions can be found [here](https://www.tensorflow.org/get_started/os_setup).

- [x ] Provide a link to a GitHub Gist of a Python script that can reproduce your issue (or just copy the script here if it is short)."
keras,8121,"Consider the following scenario



I would expect that, since  has been compiled, the output of the two  calls would be the same. However, after running the above code the actual output is



Is this the expected behavior? 

In addition, from other experiments I get the feeling that although the number of trainable weights reported by  differs in the two cases, the actual number of trainable weights is the expected one, that is, in the example above, even after setting , training will really update only 2,112 parameters and not 5,344. So, maybe this is just a reporting issue.",0,[BUG] Number of trainable weights seem to change after model compilation,"[BUG] Number of trainable weights seem to change after model compilation Consider the following scenario



I would expect that, since  has been compiled, the output of the two  calls would be the same. However, after running the above code the actual output is



Is this the expected behavior? 

In addition, from other experiments I get the feeling that although the number of trainable weights reported by  differs in the two cases, the actual number of trainable weights is the expected one, that is, in the example above, even after setting , training will really update only 2,112 parameters and not 5,344. So, maybe this is just a reporting issue."
keras,3241,"I have a vector  of size  and  of size . I can compute the dot product to get a a vector of size . However, the model summary shape does not agree with the expected shape. Code below should reproduce the error.


- [x] Check that you are up-to-date with the master branch of Keras. You can update with:
  pip install git+git://github.com/fchollet/keras.git --upgrade --no-deps
- [x] If running on Theano, check that you are up-to-date with the master branch of Theano. You can update with:
  pip install git+git://github.com/Theano/Theano.git --upgrade --no-deps
- [x] Provide a link to a GitHub Gist of a Python script that can reproduce your issue (or just copy the script here if it is short).
",0,Model Summary shape not correct.,"Model Summary shape not correct. I have a vector  of size  and  of size . I can compute the dot product to get a a vector of size . However, the model summary shape does not agree with the expected shape. Code below should reproduce the error.


- [x] Check that you are up-to-date with the master branch of Keras. You can update with:
  pip install git+git://github.com/fchollet/keras.git --upgrade --no-deps
- [x] If running on Theano, check that you are up-to-date with the master branch of Theano. You can update with:
  pip install git+git://github.com/Theano/Theano.git --upgrade --no-deps
- [x] Provide a link to a GitHub Gist of a Python script that can reproduce your issue (or just copy the script here if it is short).
"
keras,1533,"If you use  function and pass it a parameter  which is higher than 1 and at the same time provide a non threadsafe generator as a first argument, a confusing situation occurrs.

The generator worker fails:
https://github.com/fchollet/keras/blob/master/keras/models.py#L953
because of an error - in my case it was , but it can be any other error. It fails silently because of the  statement and only sets the  event, so it's difficult to get what's wrong.

I have an older version of the repo, but in my version the line:
https://github.com/fchollet/keras/blob/master/keras/models.py#L966
doesn't exist and therefore, the program breaks in a weird way on the line:
https://github.com/fchollet/keras/blob/master/keras/models.py#L974
with a . I see that it has now been fixed, but I still had some troubles with figuring out that the problem is with the generator.

Wouldn't it be a good idea to remove this  block altogether or at least remove the _catch 'em all_ statement? 
",0,fit_generator workers fail silently,"fit_generator workers fail silently If you use  function and pass it a parameter  which is higher than 1 and at the same time provide a non threadsafe generator as a first argument, a confusing situation occurrs.

The generator worker fails:
https://github.com/fchollet/keras/blob/master/keras/models.py#L953
because of an error - in my case it was , but it can be any other error. It fails silently because of the  statement and only sets the  event, so it's difficult to get what's wrong.

I have an older version of the repo, but in my version the line:
https://github.com/fchollet/keras/blob/master/keras/models.py#L966
doesn't exist and therefore, the program breaks in a weird way on the line:
https://github.com/fchollet/keras/blob/master/keras/models.py#L974
with a . I see that it has now been fixed, but I still had some troubles with figuring out that the problem is with the generator.

Wouldn't it be a good idea to remove this  block altogether or at least remove the _catch 'em all_ statement? 
"
keras,12188,"Hi all!

I have an error when I am trying to use **ImageDataGenerator** with **flow_from_directory** function for transfer learning of NasNet model from **keras.applications**.

OS: ArchLinux
Tensorflow version: 1.12.0
Keras version: 2.2.4 (updated from master)
GPUs: GeForce GTX 1080 Ti
CUDA version: 9.0.176-4
CUDNN version: 7.0.5-2

My code:



Output I get:



I tried to debug the code and seems like in this line https://github.com/keras-team/keras/blob/e59570ae26670f788d6c649191031e4a8824f955/keras/engine/training_generator.py#L110 if statement is false due **val_gen == False**. In the code above variable **val_gen** could be initialized as False because generator has no **\_\_next\_\_** or **next** functions.

Is it normal behavior?

",0,TypeError: object of type 'ImageDataGenerator' has no len(),"TypeError: object of type 'ImageDataGenerator' has no len() Hi all!

I have an error when I am trying to use **ImageDataGenerator** with **flow_from_directory** function for transfer learning of NasNet model from **keras.applications**.

OS: ArchLinux
Tensorflow version: 1.12.0
Keras version: 2.2.4 (updated from master)
GPUs: GeForce GTX 1080 Ti
CUDA version: 9.0.176-4
CUDNN version: 7.0.5-2

My code:



Output I get:



I tried to debug the code and seems like in this line https://github.com/keras-team/keras/blob/e59570ae26670f788d6c649191031e4a8824f955/keras/engine/training_generator.py#L110 if statement is false due **val_gen == False**. In the code above variable **val_gen** could be initialized as False because generator has no **\_\_next\_\_** or **next** functions.

Is it normal behavior?

"
keras,1384,"Hi!

Now I implement DAG-CNNs
（http://arxiv.org/pdf/1505.05232.pdf）

It models cannot be compiled for error. but I cannot found node don't connect.
Visualize graph is expected graph.

Why errors happen this model?

Traceback (most recent call last):
  File ""/Users/Tereka/Programing/MachineLearningCombinator/mlc/model/keras_recipe.py"", line 315, in <module>
    model.compile('sgd', {'output':'categorical_crossentropy'})
  File ""build/bdist.macosx-10.9-x86_64/egg/keras/models.py"", line 1047, in compile
  File ""build/bdist.macosx-10.9-x86_64/egg/keras/optimizers.py"", line 79, in get_updates
  File ""build/bdist.macosx-10.9-x86_64/egg/keras/optimizers.py"", line 47, in get_gradients
  File ""build/bdist.macosx-10.9-x86_64/egg/keras/backend/theano_backend.py"", line 373, in gradients
  File ""/Users/Tereka/.pyenv/versions/2.7.8/lib/python2.7/site-packages/theano/gradient.py"", line 545, in grad
    handle_disconnected(elem)
  File ""/Users/Tereka/.pyenv/versions/2.7.8/lib/python2.7/site-packages/theano/gradient.py"", line 532, in handle_disconnected
    raise DisconnectedInputError(message)
theano.gradient.DisconnectedInputError: grad method was asked to compute the gradient with respect to a variable that is not part of the computational graph of the cost, or is used only by a non-differentiable operator: <TensorType(float32, 4D)>
Backtrace when the node is created:
  File ""build/bdist.macosx-10.9-x86_64/egg/keras/backend/theano_backend.py"", line 34, in variable
    return theano.shared(value=value, name=name, strict=False)


",0,Why this model cannot be compiled,"Why this model cannot be compiled Hi!

Now I implement DAG-CNNs
（http://arxiv.org/pdf/1505.05232.pdf）

It models cannot be compiled for error. but I cannot found node don't connect.
Visualize graph is expected graph.

Why errors happen this model?

Traceback (most recent call last):
  File ""/Users/Tereka/Programing/MachineLearningCombinator/mlc/model/keras_recipe.py"", line 315, in <module>
    model.compile('sgd', {'output':'categorical_crossentropy'})
  File ""build/bdist.macosx-10.9-x86_64/egg/keras/models.py"", line 1047, in compile
  File ""build/bdist.macosx-10.9-x86_64/egg/keras/optimizers.py"", line 79, in get_updates
  File ""build/bdist.macosx-10.9-x86_64/egg/keras/optimizers.py"", line 47, in get_gradients
  File ""build/bdist.macosx-10.9-x86_64/egg/keras/backend/theano_backend.py"", line 373, in gradients
  File ""/Users/Tereka/.pyenv/versions/2.7.8/lib/python2.7/site-packages/theano/gradient.py"", line 545, in grad
    handle_disconnected(elem)
  File ""/Users/Tereka/.pyenv/versions/2.7.8/lib/python2.7/site-packages/theano/gradient.py"", line 532, in handle_disconnected
    raise DisconnectedInputError(message)
theano.gradient.DisconnectedInputError: grad method was asked to compute the gradient with respect to a variable that is not part of the computational graph of the cost, or is used only by a non-differentiable operator: <TensorType(float32, 4D)>
Backtrace when the node is created:
  File ""build/bdist.macosx-10.9-x86_64/egg/keras/backend/theano_backend.py"", line 34, in variable
    return theano.shared(value=value, name=name, strict=False)


"
keras,8924,"Please make sure that the boxes below are checked before you submit your issue. If your issue is an implementation question, please ask your question on [StackOverflow](http://stackoverflow.com/questions/tagged/keras) or [join the Keras Slack channel](https://keras-slack-autojoin.herokuapp.com/) and ask there instead of filing a GitHub issue.

Thank you!

- [ ] Check that you are up-to-date with the master branch of Keras. You can update with:
pip install git+git://github.com/keras-team/keras.git --upgrade --no-deps

- [ ] If running on TensorFlow, check that you are up-to-date with the latest version. The installation instructions can be found [here](https://www.tensorflow.org/get_started/os_setup).

- [ ] If running on Theano, check that you are up-to-date with the master branch of Theano. You can update with:
pip install git+git://github.com/Theano/Theano.git --upgrade --no-deps

- [ ] Provide a link to a GitHub Gist of a Python script that can reproduce your issue (or just copy the script here if it is short).
",0,How to build correct data with a LSTM NN?,"How to build correct data with a LSTM NN? Please make sure that the boxes below are checked before you submit your issue. If your issue is an implementation question, please ask your question on [StackOverflow](http://stackoverflow.com/questions/tagged/keras) or [join the Keras Slack channel](https://keras-slack-autojoin.herokuapp.com/) and ask there instead of filing a GitHub issue.

Thank you!

- [ ] Check that you are up-to-date with the master branch of Keras. You can update with:
pip install git+git://github.com/keras-team/keras.git --upgrade --no-deps

- [ ] If running on TensorFlow, check that you are up-to-date with the latest version. The installation instructions can be found [here](https://www.tensorflow.org/get_started/os_setup).

- [ ] If running on Theano, check that you are up-to-date with the master branch of Theano. You can update with:
pip install git+git://github.com/Theano/Theano.git --upgrade --no-deps

- [ ] Provide a link to a GitHub Gist of a Python script that can reproduce your issue (or just copy the script here if it is short).
"
keras,2155,"The docs for  method of  a  say

> y: labels, as a numpy array.

It's unclear, however, wheter i should be a vector of ints or one-hot matrix.
Perhaps one should mention that there exists ?

Also, the following code will fail on Theano but not on Tensorflow backend.


",0,Inconsistent behavior between backends + missing docs,"Inconsistent behavior between backends + missing docs The docs for  method of  a  say

> y: labels, as a numpy array.

It's unclear, however, wheter i should be a vector of ints or one-hot matrix.
Perhaps one should mention that there exists ?

Also, the following code will fail on Theano but not on Tensorflow backend.


"
keras,4822,"Hi Keras,

Thank you for this project!

It seems that you can't make the hidden unit and the output units of a SimpleRNN layer have différent values.

The doc says in the argument part :

""output_dim: dimension of the internal projections and final output.""

It seems unclear to me what are ""internal projections"". Anyway there's only one argument of this type so it's clear you can't set the hidden state dimension as you want.

Is it wanted? What if I want a big hidden state yo remember a lot of information but a unidimensionnal output?

Merry Christmas :)",0,SimpleRNN : Hidden and output units of different dimensions,"SimpleRNN : Hidden and output units of different dimensions Hi Keras,

Thank you for this project!

It seems that you can't make the hidden unit and the output units of a SimpleRNN layer have différent values.

The doc says in the argument part :

""output_dim: dimension of the internal projections and final output.""

It seems unclear to me what are ""internal projections"". Anyway there's only one argument of this type so it's clear you can't set the hidden state dimension as you want.

Is it wanted? What if I want a big hidden state yo remember a lot of information but a unidimensionnal output?

Merry Christmas :)"
keras,1685,"So I've been trying to work with the fit_generator method for the graph model.
All works well if I set nb_worker=1, but any value higher than that and it just crashes.

It seems thread.start automatically triggers _stop.is_set() after the first thread. This is in python3. Haven't tested python2.7

I lack the skills for any more informative debugging >.>
",0,fit_generator,"fit_generator So I've been trying to work with the fit_generator method for the graph model.
All works well if I set nb_worker=1, but any value higher than that and it just crashes.

It seems thread.start automatically triggers _stop.is_set() after the first thread. This is in python3. Haven't tested python2.7

I lack the skills for any more informative debugging >.>
"
keras,4079,"I had to restart my ipython interpreter so I saved the model with model.save() but loading isnt working

batch_size is of course set. 50.
The model is a deeper deconv vae from the examples, nothing fancy. vae_loss is the same with the example.



model_fom_configSequential.from_config(config)
",0,load_model not working - NameError: global name 'batch_size' is not defined????,"load_model not working - NameError: global name 'batch_size' is not defined???? I had to restart my ipython interpreter so I saved the model with model.save() but loading isnt working

batch_size is of course set. 50.
The model is a deeper deconv vae from the examples, nothing fancy. vae_loss is the same with the example.



model_fom_configSequential.from_config(config)
"
keras,1761,"Hi,
I'm working on a deep neural network (Sequential Dense), but i don't understand why the training accuracy is lower than the validation accuracy (the two sets being separated and having roughly the same distribution).
I started working on machine learning not so long ago and i was told that a way(the only one i know) to check if my network is not overfitting is to compare validation and train accuracy. If validation accuracy start dropping while the training accuracy continue to increase that's when i should be concerned.
Problem is validation accuracy is higher than training accuracy which doesn't make any sense for me...

I'm sure i missed something somewhere (maybe the training accuracy displayed by keras is not the one i think of) or the way i put my validation data is not the good one... Or maybe am I just wrong all along.
So if any one could give me an advice, or knows if there is a way to plot or visualize the data training within Keras to prevent overfit that would help.

I'm talking about that kind of thing (i'm copy pasting a random epoch but all are roughly the same):
50000/50000 [==============================] - 32s - loss: 1.7436 - acc.: 0.5749 - val. loss: 1.5925 - val. acc.: 0.6434

My network parameters are the following : a sequential dense network of that kind 20000>1000>1000>1000>1000 
trained on ReLU for all except the last Softmax. The Loss is categorical crossentropy
",0,validation accuracy superior to training accuracy,"validation accuracy superior to training accuracy Hi,
I'm working on a deep neural network (Sequential Dense), but i don't understand why the training accuracy is lower than the validation accuracy (the two sets being separated and having roughly the same distribution).
I started working on machine learning not so long ago and i was told that a way(the only one i know) to check if my network is not overfitting is to compare validation and train accuracy. If validation accuracy start dropping while the training accuracy continue to increase that's when i should be concerned.
Problem is validation accuracy is higher than training accuracy which doesn't make any sense for me...

I'm sure i missed something somewhere (maybe the training accuracy displayed by keras is not the one i think of) or the way i put my validation data is not the good one... Or maybe am I just wrong all along.
So if any one could give me an advice, or knows if there is a way to plot or visualize the data training within Keras to prevent overfit that would help.

I'm talking about that kind of thing (i'm copy pasting a random epoch but all are roughly the same):
50000/50000 [==============================] - 32s - loss: 1.7436 - acc.: 0.5749 - val. loss: 1.5925 - val. acc.: 0.6434

My network parameters are the following : a sequential dense network of that kind 20000>1000>1000>1000>1000 
trained on ReLU for all except the last Softmax. The Loss is categorical crossentropy
"
keras,12218,"Can anyone guide me how can I use reshape function?
encoded (?, 4)
encod (?, 2)
normalize (?, 2)
Complex_symbols (?,)
decode(?,)
concat (?, 2)
deco (?, 4)
decod (?, 4)
I have size (None,) in 2 layers decode and concat where I need the size to be (None,1)
how should I use a Reshape function in this case? Or is there any other option




Please make sure that the boxes below are checked before you submit your issue.
If your issue is an **implementation question**, please ask your question on [StackOverflow](http://stackoverflow.com/questions/tagged/keras) or [on the Keras Slack channel](https://keras-slack-autojoin.herokuapp.com/) instead of opening a GitHub issue.

Thank you!

- [ ] Check that you are up-to-date with the master branch of Keras. You can update with:


- [ ] Check that your version of TensorFlow is up-to-date. The installation instructions can be found [here](https://www.tensorflow.org/get_started/os_setup).

- [ ] Provide a link to a GitHub Gist of a Python script that can reproduce your issue (or just copy the script here if it is short).
",0,Reshape function,"Reshape function Can anyone guide me how can I use reshape function?
encoded (?, 4)
encod (?, 2)
normalize (?, 2)
Complex_symbols (?,)
decode(?,)
concat (?, 2)
deco (?, 4)
decod (?, 4)
I have size (None,) in 2 layers decode and concat where I need the size to be (None,1)
how should I use a Reshape function in this case? Or is there any other option




Please make sure that the boxes below are checked before you submit your issue.
If your issue is an **implementation question**, please ask your question on [StackOverflow](http://stackoverflow.com/questions/tagged/keras) or [on the Keras Slack channel](https://keras-slack-autojoin.herokuapp.com/) instead of opening a GitHub issue.

Thank you!

- [ ] Check that you are up-to-date with the master branch of Keras. You can update with:


- [ ] Check that your version of TensorFlow is up-to-date. The installation instructions can be found [here](https://www.tensorflow.org/get_started/os_setup).

- [ ] Provide a link to a GitHub Gist of a Python script that can reproduce your issue (or just copy the script here if it is short).
"
keras,5362,"I have a Keras model with 2 losses. For all even observations I want to make use of both losses, whereas for all odd observations I want to use only the 1st loss. The  parameter in  seems to only allow me to multiply the loss contribution across all losses with a single scalar for each observation. How do I make all the odd observations not make use of the 2nd loss?",0,Sample loss weights with multiple losses,"Sample loss weights with multiple losses I have a Keras model with 2 losses. For all even observations I want to make use of both losses, whereas for all odd observations I want to use only the 1st loss. The  parameter in  seems to only allow me to multiply the loss contribution across all losses with a single scalar for each observation. How do I make all the odd observations not make use of the 2nd loss?"
keras,3879,"I saved a model's weight in an HDF5 file and I am trying to load it to initialize a different model (with a different output layer) using this command:
model.load_weights('model_weights.h5', by_name=True)

But I got this error:
TypeError: load_weights() got an unexpected keyword argument 'by_name'

Could anyone help?
",0,TypeError: load_weights() got an unexpected keyword argument 'by_name',"TypeError: load_weights() got an unexpected keyword argument 'by_name' I saved a model's weight in an HDF5 file and I am trying to load it to initialize a different model (with a different output layer) using this command:
model.load_weights('model_weights.h5', by_name=True)

But I got this error:
TypeError: load_weights() got an unexpected keyword argument 'by_name'

Could anyone help?
"
keras,7003,"model.fit support multi input/output network, but if data-set is large enough and one have to use model.fit_generator, its complicated to generate tuple for such case, is there any plan to make it more simpler like model.fit.

I have network that take one input and produce two outputs,  i created  generator for each, but i am not able to run network on this. my input should be of form x, [y1, y2].

i think i need to extend generator for such case ?

  

Please make sure that the boxes below are checked before you submit your issue. If your issue is an implementation question, please ask your question on [StackOverflow](http://stackoverflow.com/questions/tagged/keras) or [join the Keras Slack channel](https://keras-slack-autojoin.herokuapp.com/) and ask there instead of filing a GitHub issue.

Thank you!

- [x] Check that you are up-to-date with the master branch of Keras. You can update with:
pip install git+git://github.com/fchollet/keras.git --upgrade --no-deps

- [x] If running on TensorFlow, check that you are up-to-date with the latest version. The installation instructions can be found [here](https://www.tensorflow.org/get_started/os_setup).

- [x] If running on Theano, check that you are up-to-date with the master branch of Theano. You can update with:
pip install git+git://github.com/Theano/Theano.git --upgrade --no-deps

- [x] Provide a link to a GitHub Gist of a Python script that can reproduce your issue (or just copy the script here if it is short).
",0,Multi output network generator,"Multi output network generator model.fit support multi input/output network, but if data-set is large enough and one have to use model.fit_generator, its complicated to generate tuple for such case, is there any plan to make it more simpler like model.fit.

I have network that take one input and produce two outputs,  i created  generator for each, but i am not able to run network on this. my input should be of form x, [y1, y2].

i think i need to extend generator for such case ?

  

Please make sure that the boxes below are checked before you submit your issue. If your issue is an implementation question, please ask your question on [StackOverflow](http://stackoverflow.com/questions/tagged/keras) or [join the Keras Slack channel](https://keras-slack-autojoin.herokuapp.com/) and ask there instead of filing a GitHub issue.

Thank you!

- [x] Check that you are up-to-date with the master branch of Keras. You can update with:
pip install git+git://github.com/fchollet/keras.git --upgrade --no-deps

- [x] If running on TensorFlow, check that you are up-to-date with the latest version. The installation instructions can be found [here](https://www.tensorflow.org/get_started/os_setup).

- [x] If running on Theano, check that you are up-to-date with the master branch of Theano. You can update with:
pip install git+git://github.com/Theano/Theano.git --upgrade --no-deps

- [x] Provide a link to a GitHub Gist of a Python script that can reproduce your issue (or just copy the script here if it is short).
"
keras,12559,"Please make sure that the boxes below are checked before you submit your issue. If your issue is an implementation question, please ask your question on [StackOverflow](http://stackoverflow.com/questions/tagged/keras) or [join the Keras Slack channel](https://keras-slack-autojoin.herokuapp.com/) and ask there instead of filing a GitHub issue.

Thank you!

- [ ] Check that you are up-to-date with the master branch of Keras. You can update with:
pip install git+git://github.com/keras-team/keras.git --upgrade --no-deps

- [ ] If running on TensorFlow, check that you are up-to-date with the latest version. The installation instructions can be found [here](https://www.tensorflow.org/get_started/os_setup).

- [ ] If running on Theano, check that you are up-to-date with the master branch of Theano. You can update with:
pip install git+git://github.com/Theano/Theano.git --upgrade --no-deps

- [ ] Provide a link to a GitHub Gist of a Python script that can reproduce your issue (or just copy the script here if it is short).
",0,"model = Sequential() model.add(Embedding(10000, embedding_size, input_length=max_words)) model.add(Conv1D(64, 3, padding='same')) model.add(Conv1D(32, 3, padding='same')) model.add(Flatten()) model.add(Dropout(0.2)) model.add(Dense(512,activation='relu')) model.add(Dropout(0.2)) model.add(Dense(1,activation='softmax’))","model = Sequential() model.add(Embedding(10000, embedding_size, input_length=max_words)) model.add(Conv1D(64, 3, padding='same')) model.add(Conv1D(32, 3, padding='same')) model.add(Flatten()) model.add(Dropout(0.2)) model.add(Dense(512,activation='relu')) model.add(Dropout(0.2)) model.add(Dense(1,activation='softmax’)) Please make sure that the boxes below are checked before you submit your issue. If your issue is an implementation question, please ask your question on [StackOverflow](http://stackoverflow.com/questions/tagged/keras) or [join the Keras Slack channel](https://keras-slack-autojoin.herokuapp.com/) and ask there instead of filing a GitHub issue.

Thank you!

- [ ] Check that you are up-to-date with the master branch of Keras. You can update with:
pip install git+git://github.com/keras-team/keras.git --upgrade --no-deps

- [ ] If running on TensorFlow, check that you are up-to-date with the latest version. The installation instructions can be found [here](https://www.tensorflow.org/get_started/os_setup).

- [ ] If running on Theano, check that you are up-to-date with the master branch of Theano. You can update with:
pip install git+git://github.com/Theano/Theano.git --upgrade --no-deps

- [ ] Provide a link to a GitHub Gist of a Python script that can reproduce your issue (or just copy the script here if it is short).
"
keras,6061,"Why i am getting this error ?


===============================                                                                                                                                                            
Problem occurred during compilation with the command line below:                                                                                                                           
""C:\Users\om\Anaconda3\envs\tensorflow-gpu\Library\mingw-w64\bin\g++.exe"" -shared -g -march=broadwell -mmmx -mno-3dnow -msse -msse2 -msse3 -mssse3 -mno-sse4a -mcx16 -msahf -mmovbe -maes -
mno-sha -mpclmul -mpopcnt -mabm -mno-lwp -mfma -mno-fma4 -mno-xop -mbmi -mbmi2 -mno-tbm -mavx -mavx2 -msse4.2 -msse4.1 -mlzcnt -mrtm -mhle -mrdrnd -mf16c -mfsgsbase -mrdseed -mprfchw -mad
x -mfxsr -mxsave -mxsaveopt -mno-avx512f -mno-avx512er -mno-avx512cd -mno-avx512pf -mno-prefetchwt1 -mclflushopt -mxsavec -mxsaves -mno-avx512dq -mno-avx512bw -mno-avx512vl -mno-avx512ifm
a -mno-avx512vbmi -mno-clwb -mno-pcommit -mno-mwaitx --param l1-cache-size=32 --param l1-cache-line-size=64 --param l2-cache-size=6144 -mtune=generic -DNPY_NO_DEPRECATED_API=NPY_1_7_API_V
ERSION -m64 -DMS_WIN64 -I""C:\Users\om\Anaconda3\envs\tensorflow-gpu\lib\site-packages\numpy\core\include"" -I""C:\Users\om\Anaconda3\envs\tensorflow-gpu\include"" -I""C:\Users\om\Anaconda3\en
vs\tensorflow-gpu\lib\site-packages\theano\gof"" -L""C:\Users\om\Anaconda3\envs\tensorflow-gpu\libs"" -L""C:\Users\om\Anaconda3\envs\tensorflow-gpu"" -o C:\Users\om\AppData\Local\Theano\compil
edir_Windows-10-10.0.14393-SP0-Intel64_Family_6_Model_94_Stepping_3_GenuineIntel-3.5.2-64\lazylinker_ext\lazylinker_ext.pyd C:\Users\om\AppData\Local\Theano\compiledir_Windows-10-10.0.143
93-SP0-Intel64_Family_6_Model_94_Stepping_3_GenuineIntel-3.5.2-64\lazylinker_ext\mod.cpp -lpython35                                                                                        
C:\Users\om\AppData\Local\Temp\ccUzk0H7.o: In function __imp_PyExc_ImportError'                             
C:/Users/om/Anaconda3/envs/tensorflow-gpu/lib/site-packages/numpy/core/include/numpy/__multiarray_api.h:1461: undefined reference to __imp_PyCapsule_Type'                                
C:/Users/om/Anaconda3/envs/tensorflow-gpu/lib/site-packages/numpy/core/include/numpy/__multiarray_api.h:1467: undefined reference to __imp_PyExc_RuntimeError'                            
C:/Users/om/Anaconda3/envs/tensorflow-gpu/lib/site-packages/numpy/core/include/numpy/__multiarray_api.h:1490: undefined reference to __imp_PyExc_RuntimeError'                            
C:/Users/om/Anaconda3/envs/tensorflow-gpu/lib/site-packages/numpy/core/include/numpy/__multiarray_api.h:1506: undefined reference to __imp
_PyExc_RuntimeError' follow                                                                                                                                                                
C:\Users\om\AppData\Local\Temp\ccUzk0H7.o: In function __imp_PyCapsule_Type'                                     
C:\Users\om\AppData\Local\Temp\ccUzk0H7.o: In function __imp_P
yExc_TypeError'                                                                                                                                                                            
C:/Users/om/AppData/Local/Theano/compiledir_Windows-10-10.0.14393-SP0-Intel64_Family_6_Model_94_Stepping_3_GenuineIntel-3.5.2-64/lazylinker_ext/mod.cpp:58: undefined reference to CLazyLinker_init':                                                                                                                 
C:/Users/om/AppData/Local/Theano/compiledir_Windows-10-10.0.14393-SP0-Intel64_Family_6_Model_94_Stepping_3_GenuineIntel-3.5.2-64/lazylinker_ext/mod.cpp:352: undefined reference to __imp_
PyExc_IndexError'                                                                                                                                                                          
C:/Users/om/AppData/Local/Theano/compiledir_Windows-10-10.0.14393-SP0-Intel64_Family_6_Model_94_Stepping_3_GenuineIntel-3.5.2-64/lazylinker_ext/mod.cpp:385: undefined reference to __imp_
PyExc_IndexError'                                                                                                                                                                          
C:\Users\om\AppData\Local\Temp\ccUzk0H7.o:C:/Users/om/AppData/Local/Theano/compiledir_Windows-10-10.0.14393-SP0-Intel64_Family_6_Model_94_Stepping_3_GenuineIntel-3.5.2-64/lazylinker_ext/m
od.cpp:393: more undefined references to CLazyLinker_init':                                                                                                                 
C:/Users/om/AppData/Local/Theano/compiledir_Windows-10-10.0.14393-SP0-Intel64_Family_6_Model_94_Stepping_3_GenuineIntel-3.5.2-64/lazylinker_ext/mod.cpp:405: undefined reference to __imp_
_Py_NoneStruct'                                                                                                                                                                            
C:/Users/om/AppData/Local/Theano/compiledir_Windows-10-10.0.14393-SP0-Intel64_Family_6_Model_94_Stepping_3_GenuineIntel-3.5.2-64/lazylinker_ext/mod.cpp:426: undefined reference to __imp_
PyExc_TypeError'                                                                                                                                                                           
C:/Users/om/AppData/Local/Theano/compiledir_Windows-10-10.0.14393-SP0-Intel64_Family_6_Model_94_Stepping_3_GenuineIntel-3.5.2-64/lazylinker_ext/mod.cpp:444: undefined reference to c_call':                                                                                                                           
C:/Users/om/AppData/Local/Theano/compiledir_Windows-10-10.0.14393-SP0-Intel64_Family_6_Model_94_Stepping_3_GenuineIntel-3.5.2-64/lazylinker_ext/mod.cpp:545: undefined reference to __imp_
_Py_NoneStruct'                                                                                                                                                                            
C:/Users/om/AppData/Local/Theano/compiledir_Windows-10-10.0.14393-SP0-Intel64_Family_6_Model_94_Stepping_3_GenuineIntel-3.5.2-64/lazylinker_ext/mod.cpp:545: undefined reference to __imp_
_Py_NoneStruct'                                                                                                                                                                            
C:\Users\om\AppData\Local\Temp\ccUzk0H7.o:C:/Users/om/AppData/Local/Theano/compiledir_Windows-10-10.0.14393-SP0-Intel64_Family_6_Model_94_Stepping_3_GenuineIntel-3.5.2-64/lazylinker_ext/m
od.cpp:546: more undefined references to lazy_rec_eval':                                                                                                                    
C:/Users/om/AppData/Local/Theano/compiledir_Windows-10-10.0.14393-SP0-Intel64_Family_6_Model_94_Stepping_3_GenuineIntel-3.5.2-64/lazylinker_ext/mod.cpp:618: undefined reference to __imp_
PyExc_TypeError'                                                                                                                                                                           
C:/Users/om/AppData/Local/Theano/compiledir_Windows-10-10.0.14393-SP0-Intel64_Family_6_Model_94_Stepping_3_GenuineIntel-3.5.2-64/lazylinker_ext/mod.cpp:649: undefined reference to __imp_
PyExc_IndexError'                                                                                                                                                                          
C:/Users/om/AppData/Local/Theano/compiledir_Windows-10-10.0.14393-SP0-Intel64_Family_6_Model_94_Stepping_3_GenuineIntel-3.5.2-64/lazylinker_ext/mod.cpp:708: undefined reference to __imp_
PyExc_TypeError'                                                                                                                                                                           
C:/Users/om/AppData/Local/Theano/compiledir_Windows-10-10.0.14393-SP0-Intel64_Family_6_Model_94_Stepping_3_GenuineIntel-3.5.2-64/lazylinker_ext/mod.cpp:721: undefined reference to __imp_
_Py_NoneStruct'                                                                                                                                                                            
C:/Users/om/AppData/Local/Theano/compiledir_Windows-10-10.0.14393-SP0-Intel64_Family_6_Model_94_Stepping_3_GenuineIntel-3.5.2-64/lazylinker_ext/mod.cpp:771: undefined reference to __imp_
_Py_NoneStruct'                                                                                                                                                                            
C:\Users\om\AppData\Local\Temp\ccUzk0H7.o: In function __imp_
PyExc_RuntimeError'                                                                                                                                                                        
C:/Users/om/AppData/Local/Theano/compiledir_Windows-10-10.0.14393-SP0-Intel64_Family_6_Model_94_Stepping_3_GenuineIntel-3.5.2-64/lazylinker_ext/mod.cpp:826: undefined reference to __imp_
_Py_NoneStruct'                                                                                                                                                                            
C:/Users/om/AppData/Local/Theano/compiledir_Windows-10-10.0.14393-SP0-Intel64_Family_6_Model_94_Stepping_3_GenuineIntel-3.5.2-64/lazylinker_ext/mod.cpp:839: undefined reference to __imp_
_Py_NoneStruct'                                                                                                                                                                            
C:/Users/om/AppData/Local/Theano/compiledir_Windows-10-10.0.14393-SP0-Intel64_Family_6_Model_94_Stepping_3_GenuineIntel-3.5.2-64/lazylinker_ext/mod.cpp:849: undefined reference to __imp_
_Py_NoneStruct'                                                                                                                                                                            
C:\Users\om\AppData\Local\Temp\ccUzk0H7.o:C:/Users/om/AppData/Local/Theano/compiledir_Windows-10-10.0.14393-SP0-Intel64_Family_6_Model_94_Stepping_3_GenuineIntel-3.5.2-64/lazylinker_ext/m
od.cpp:850: more undefined references to CLazyLinker_call':                                                                                                                 
C:/Users/om/AppData/Local/Theano/compiledir_Windows-10-10.0.14393-SP0-Intel64_Family_6_Model_94_Stepping_3_GenuineIntel-3.5.2-64/lazylinker_ext/mod.cpp:894: undefined reference to __imp_
_Py_NoneStruct'                                                                                                                                                                            
C:/Users/om/AppData/Local/Theano/compiledir_Windows-10-10.0.14393-SP0-Intel64_Family_6_Model_94_Stepping_3_GenuineIntel-3.5.2-64/lazylinker_ext/mod.cpp:937: undefined reference to __imp_
_Py_NoneStruct'                                                                                                                                                                            
C:\Users\om\AppData\Local\Temp\ccUzk0H7.o: In function __imp_
PyBool_Type'                                                                                                                                                                               
C:/Users/om/AppData/Local/Theano/compiledir_Windows-10-10.0.14393-SP0-Intel64_Family_6_Model_94_Stepping_3_GenuineIntel-3.5.2-64/lazylinker_ext/mod.cpp:976: undefined reference to __i
. C:/Users/om/AppData/Local/Theano/compiledir_Windows-10-10.0.14393-SP0-Intel64_Family_6_Model_94_Stepping_3_GenuineIntel-3.5.2-64/lazylinker_ext/mod.cpp:48: undefined reference to __imp
. C:/Users/om/AppData/Local/Theano/compiledir_Windows-10-10.0.14393-SP0-Intel64_Family_6_Model_94_Stepping_3_GenuineIntel-3.5.2-64/lazylinker_ext/mod.cpp:352: undefined reference to __im
. C:/Users/om/AppData/Local/Theano/compiledir_Windows-10-10.0.14393-SP0-Intel64_Family_6_Model_94_Stepping_3_GenuineIntel-3.5.2-64/lazylinker_ext/mod.cpp:385: undefined reference to __im
. C:\Users\om\AppData\Local\Temp\ccUzk0H7.o:C:/Users/om/AppData/Local/Theano/compiledir_Windows-10-10.0.14393-SP0-Intel64_Family_6_Model_94_Stepping_3_GenuineIntel-3.5.2-64/lazylinker_ext
. C:/Users/om/AppData/Local/Theano/compiledir_Windows-10-10.0.14393-SP0-Intel64_Family_6_Model_94_Stepping_3_GenuineIntel-3.5.2-64/lazylinker_ext/mod.cpp:405: undefined reference to __im
. C:/Users/om/AppData/Local/Theano/compiledir_Windows-10-10.0.14393-SP0-Intel64_Family_6_Model_94_Stepping_3_GenuineIntel-3.5.2-64/lazylinker_ext/mod.cpp:426: undefined reference to __im
. C:/Users/om/AppData/Local/Theano/compiledir_Windows-10-10.0.14393-SP0-Intel64_Family_6_Model_94_Stepping_3_GenuineIntel-3.5.2-64/lazylinker_ext/mod.cpp:444: undefined reference to __im
. C:/Users/om/AppData/Local/Theano/compiledir_Windows-10-10.0.14393-SP0-Intel64_Family_6_Model_94_Stepping_3_GenuineIntel-3.5.2-64/lazylinker_ext/mod.cpp:545: undefined reference to __im
. C:/Users/om/AppData/Local/Theano/compiledir_Windows-10-10.0.14393-SP0-Intel64_Family_6_Model_94_Stepping_3_GenuineIntel-3.5.2-64/lazylinker_ext/mod.cpp:546: undefined reference to __im
. C:/Users/om/AppData/Local/Theano/compiledir_Windows-10-10.0.14393-SP0-Intel64_Family_6_Model_94_Stepping_3_GenuineIntel-3.5.2-64/lazylinker_ext/mod.cpp:641: undefined reference to __im
. C:/Users/om/AppData/Local/Theano/compiledir_Windows-10-10.0.14393-SP0-Intel64_Family_6_Model_94_Stepping_3_GenuineIntel-3.5.2-64/lazylinker_ext/mod.cpp:657: undefined reference to __im
. C:/Users/om/AppData/Local/Theano/compiledir_Windows-10-10.0.14393-SP0-Intel64_Family_6_Model_94_Stepping_3_GenuineIntel-3.5.2-64/lazylinker_ext/mod.cpp:715: undefined reference to __im
. C:/Users/om/AppData/Local/Theano/compiledir_Windows-10-10.0.14393-SP0-Intel64_Family_6_Model_94_Stepping_3_GenuineIntel-3.5.2-64/lazylinker_ext/mod.cpp:771: undefined reference to __im
. C:/Users/om/AppData/Local/Theano/compiledir_Windows-10-10.0.14393-SP0-Intel64_Family_6_Model_94_Stepping_3_GenuineIntel-3.5.2-64/lazylinker_ext/mod.cpp:772: undefined reference to __im
. C:/Users/om/AppData/Local/Theano/compiledir_Windows-10-10.0.14393-SP0-Intel64_Family_6_Model_94_Stepping_3_GenuineIntel-3.5.2-64/lazylinker_ext/mod.cpp:826: undefined reference to __im
. C:/Users/om/AppData/Local/Theano/compiledir_Windows-10-10.0.14393-SP0-Intel64_Family_6_Model_94_Stepping_3_GenuineIntel-3.5.2-64/lazylinker_ext/mod.cpp:839: undefined reference to __im
. C:/Users/om/AppData/Local/Theano/compiledir_Windows-10-10.0.14393-SP0-Intel64_Family_6_Model_94_Stepping_3_GenuineIntel-3.5.2-64/lazylinker_ext/mod.cpp:849: undefined reference to __im
. C:\Users\om\AppData\Local\Temp\ccUzk0H7.o:C:/Users/om/AppData/Local/Theano/compiledir_Windows-10-10.0.14393-SP0-Intel64_Family_6_Model_94_Stepping_3_GenuineIntel-3.5.2-64/lazylinker_ext
. C:/Users/om/AppData/Local/Theano/compiledir_Windows-10-10.0.14393-SP0-Intel64_Family_6_Model_94_Stepping_3_GenuineIntel-3.5.2-64/lazylinker_ext/mod.cpp:894: undefined reference to __im
. C:/Users/om/AppData/Local/Theano/compiledir_Windows-10-10.0.14393-SP0-Intel64_Family_6_Model_94_Stepping_3_GenuineIntel-3.5.2-64/lazylinker_ext/mod.cpp:937: undefined reference to __im
. C:/Users/om/AppData/Local/Theano/compiledir_Windows-10-10.0.14393-SP0-Intel64_Family_6_Model_94_Stepping_3_GenuineIntel-3.5.2-64/lazylinker_ext/mod.cpp:973: undefined reference to __im
. collect2.exe: error: ld returned 1 exit status                                                                                                                                           ",0,"Big Error , why i am getting this ?","Big Error , why i am getting this ? Why i am getting this error ?


===============================                                                                                                                                                            
Problem occurred during compilation with the command line below:                                                                                                                           
""C:\Users\om\Anaconda3\envs\tensorflow-gpu\Library\mingw-w64\bin\g++.exe"" -shared -g -march=broadwell -mmmx -mno-3dnow -msse -msse2 -msse3 -mssse3 -mno-sse4a -mcx16 -msahf -mmovbe -maes -
mno-sha -mpclmul -mpopcnt -mabm -mno-lwp -mfma -mno-fma4 -mno-xop -mbmi -mbmi2 -mno-tbm -mavx -mavx2 -msse4.2 -msse4.1 -mlzcnt -mrtm -mhle -mrdrnd -mf16c -mfsgsbase -mrdseed -mprfchw -mad
x -mfxsr -mxsave -mxsaveopt -mno-avx512f -mno-avx512er -mno-avx512cd -mno-avx512pf -mno-prefetchwt1 -mclflushopt -mxsavec -mxsaves -mno-avx512dq -mno-avx512bw -mno-avx512vl -mno-avx512ifm
a -mno-avx512vbmi -mno-clwb -mno-pcommit -mno-mwaitx --param l1-cache-size=32 --param l1-cache-line-size=64 --param l2-cache-size=6144 -mtune=generic -DNPY_NO_DEPRECATED_API=NPY_1_7_API_V
ERSION -m64 -DMS_WIN64 -I""C:\Users\om\Anaconda3\envs\tensorflow-gpu\lib\site-packages\numpy\core\include"" -I""C:\Users\om\Anaconda3\envs\tensorflow-gpu\include"" -I""C:\Users\om\Anaconda3\en
vs\tensorflow-gpu\lib\site-packages\theano\gof"" -L""C:\Users\om\Anaconda3\envs\tensorflow-gpu\libs"" -L""C:\Users\om\Anaconda3\envs\tensorflow-gpu"" -o C:\Users\om\AppData\Local\Theano\compil
edir_Windows-10-10.0.14393-SP0-Intel64_Family_6_Model_94_Stepping_3_GenuineIntel-3.5.2-64\lazylinker_ext\lazylinker_ext.pyd C:\Users\om\AppData\Local\Theano\compiledir_Windows-10-10.0.143
93-SP0-Intel64_Family_6_Model_94_Stepping_3_GenuineIntel-3.5.2-64\lazylinker_ext\mod.cpp -lpython35                                                                                        
C:\Users\om\AppData\Local\Temp\ccUzk0H7.o: In function __imp_PyExc_ImportError'                             
C:/Users/om/Anaconda3/envs/tensorflow-gpu/lib/site-packages/numpy/core/include/numpy/__multiarray_api.h:1461: undefined reference to __imp_PyCapsule_Type'                                
C:/Users/om/Anaconda3/envs/tensorflow-gpu/lib/site-packages/numpy/core/include/numpy/__multiarray_api.h:1467: undefined reference to __imp_PyExc_RuntimeError'                            
C:/Users/om/Anaconda3/envs/tensorflow-gpu/lib/site-packages/numpy/core/include/numpy/__multiarray_api.h:1490: undefined reference to __imp_PyExc_RuntimeError'                            
C:/Users/om/Anaconda3/envs/tensorflow-gpu/lib/site-packages/numpy/core/include/numpy/__multiarray_api.h:1506: undefined reference to __imp
_PyExc_RuntimeError' follow                                                                                                                                                                
C:\Users\om\AppData\Local\Temp\ccUzk0H7.o: In function __imp_PyCapsule_Type'                                     
C:\Users\om\AppData\Local\Temp\ccUzk0H7.o: In function __imp_P
yExc_TypeError'                                                                                                                                                                            
C:/Users/om/AppData/Local/Theano/compiledir_Windows-10-10.0.14393-SP0-Intel64_Family_6_Model_94_Stepping_3_GenuineIntel-3.5.2-64/lazylinker_ext/mod.cpp:58: undefined reference to CLazyLinker_init':                                                                                                                 
C:/Users/om/AppData/Local/Theano/compiledir_Windows-10-10.0.14393-SP0-Intel64_Family_6_Model_94_Stepping_3_GenuineIntel-3.5.2-64/lazylinker_ext/mod.cpp:352: undefined reference to __imp_
PyExc_IndexError'                                                                                                                                                                          
C:/Users/om/AppData/Local/Theano/compiledir_Windows-10-10.0.14393-SP0-Intel64_Family_6_Model_94_Stepping_3_GenuineIntel-3.5.2-64/lazylinker_ext/mod.cpp:385: undefined reference to __imp_
PyExc_IndexError'                                                                                                                                                                          
C:\Users\om\AppData\Local\Temp\ccUzk0H7.o:C:/Users/om/AppData/Local/Theano/compiledir_Windows-10-10.0.14393-SP0-Intel64_Family_6_Model_94_Stepping_3_GenuineIntel-3.5.2-64/lazylinker_ext/m
od.cpp:393: more undefined references to CLazyLinker_init':                                                                                                                 
C:/Users/om/AppData/Local/Theano/compiledir_Windows-10-10.0.14393-SP0-Intel64_Family_6_Model_94_Stepping_3_GenuineIntel-3.5.2-64/lazylinker_ext/mod.cpp:405: undefined reference to __imp_
_Py_NoneStruct'                                                                                                                                                                            
C:/Users/om/AppData/Local/Theano/compiledir_Windows-10-10.0.14393-SP0-Intel64_Family_6_Model_94_Stepping_3_GenuineIntel-3.5.2-64/lazylinker_ext/mod.cpp:426: undefined reference to __imp_
PyExc_TypeError'                                                                                                                                                                           
C:/Users/om/AppData/Local/Theano/compiledir_Windows-10-10.0.14393-SP0-Intel64_Family_6_Model_94_Stepping_3_GenuineIntel-3.5.2-64/lazylinker_ext/mod.cpp:444: undefined reference to c_call':                                                                                                                           
C:/Users/om/AppData/Local/Theano/compiledir_Windows-10-10.0.14393-SP0-Intel64_Family_6_Model_94_Stepping_3_GenuineIntel-3.5.2-64/lazylinker_ext/mod.cpp:545: undefined reference to __imp_
_Py_NoneStruct'                                                                                                                                                                            
C:/Users/om/AppData/Local/Theano/compiledir_Windows-10-10.0.14393-SP0-Intel64_Family_6_Model_94_Stepping_3_GenuineIntel-3.5.2-64/lazylinker_ext/mod.cpp:545: undefined reference to __imp_
_Py_NoneStruct'                                                                                                                                                                            
C:\Users\om\AppData\Local\Temp\ccUzk0H7.o:C:/Users/om/AppData/Local/Theano/compiledir_Windows-10-10.0.14393-SP0-Intel64_Family_6_Model_94_Stepping_3_GenuineIntel-3.5.2-64/lazylinker_ext/m
od.cpp:546: more undefined references to lazy_rec_eval':                                                                                                                    
C:/Users/om/AppData/Local/Theano/compiledir_Windows-10-10.0.14393-SP0-Intel64_Family_6_Model_94_Stepping_3_GenuineIntel-3.5.2-64/lazylinker_ext/mod.cpp:618: undefined reference to __imp_
PyExc_TypeError'                                                                                                                                                                           
C:/Users/om/AppData/Local/Theano/compiledir_Windows-10-10.0.14393-SP0-Intel64_Family_6_Model_94_Stepping_3_GenuineIntel-3.5.2-64/lazylinker_ext/mod.cpp:649: undefined reference to __imp_
PyExc_IndexError'                                                                                                                                                                          
C:/Users/om/AppData/Local/Theano/compiledir_Windows-10-10.0.14393-SP0-Intel64_Family_6_Model_94_Stepping_3_GenuineIntel-3.5.2-64/lazylinker_ext/mod.cpp:708: undefined reference to __imp_
PyExc_TypeError'                                                                                                                                                                           
C:/Users/om/AppData/Local/Theano/compiledir_Windows-10-10.0.14393-SP0-Intel64_Family_6_Model_94_Stepping_3_GenuineIntel-3.5.2-64/lazylinker_ext/mod.cpp:721: undefined reference to __imp_
_Py_NoneStruct'                                                                                                                                                                            
C:/Users/om/AppData/Local/Theano/compiledir_Windows-10-10.0.14393-SP0-Intel64_Family_6_Model_94_Stepping_3_GenuineIntel-3.5.2-64/lazylinker_ext/mod.cpp:771: undefined reference to __imp_
_Py_NoneStruct'                                                                                                                                                                            
C:\Users\om\AppData\Local\Temp\ccUzk0H7.o: In function __imp_
PyExc_RuntimeError'                                                                                                                                                                        
C:/Users/om/AppData/Local/Theano/compiledir_Windows-10-10.0.14393-SP0-Intel64_Family_6_Model_94_Stepping_3_GenuineIntel-3.5.2-64/lazylinker_ext/mod.cpp:826: undefined reference to __imp_
_Py_NoneStruct'                                                                                                                                                                            
C:/Users/om/AppData/Local/Theano/compiledir_Windows-10-10.0.14393-SP0-Intel64_Family_6_Model_94_Stepping_3_GenuineIntel-3.5.2-64/lazylinker_ext/mod.cpp:839: undefined reference to __imp_
_Py_NoneStruct'                                                                                                                                                                            
C:/Users/om/AppData/Local/Theano/compiledir_Windows-10-10.0.14393-SP0-Intel64_Family_6_Model_94_Stepping_3_GenuineIntel-3.5.2-64/lazylinker_ext/mod.cpp:849: undefined reference to __imp_
_Py_NoneStruct'                                                                                                                                                                            
C:\Users\om\AppData\Local\Temp\ccUzk0H7.o:C:/Users/om/AppData/Local/Theano/compiledir_Windows-10-10.0.14393-SP0-Intel64_Family_6_Model_94_Stepping_3_GenuineIntel-3.5.2-64/lazylinker_ext/m
od.cpp:850: more undefined references to CLazyLinker_call':                                                                                                                 
C:/Users/om/AppData/Local/Theano/compiledir_Windows-10-10.0.14393-SP0-Intel64_Family_6_Model_94_Stepping_3_GenuineIntel-3.5.2-64/lazylinker_ext/mod.cpp:894: undefined reference to __imp_
_Py_NoneStruct'                                                                                                                                                                            
C:/Users/om/AppData/Local/Theano/compiledir_Windows-10-10.0.14393-SP0-Intel64_Family_6_Model_94_Stepping_3_GenuineIntel-3.5.2-64/lazylinker_ext/mod.cpp:937: undefined reference to __imp_
_Py_NoneStruct'                                                                                                                                                                            
C:\Users\om\AppData\Local\Temp\ccUzk0H7.o: In function __imp_
PyBool_Type'                                                                                                                                                                               
C:/Users/om/AppData/Local/Theano/compiledir_Windows-10-10.0.14393-SP0-Intel64_Family_6_Model_94_Stepping_3_GenuineIntel-3.5.2-64/lazylinker_ext/mod.cpp:976: undefined reference to __i
. C:/Users/om/AppData/Local/Theano/compiledir_Windows-10-10.0.14393-SP0-Intel64_Family_6_Model_94_Stepping_3_GenuineIntel-3.5.2-64/lazylinker_ext/mod.cpp:48: undefined reference to __imp
. C:/Users/om/AppData/Local/Theano/compiledir_Windows-10-10.0.14393-SP0-Intel64_Family_6_Model_94_Stepping_3_GenuineIntel-3.5.2-64/lazylinker_ext/mod.cpp:352: undefined reference to __im
. C:/Users/om/AppData/Local/Theano/compiledir_Windows-10-10.0.14393-SP0-Intel64_Family_6_Model_94_Stepping_3_GenuineIntel-3.5.2-64/lazylinker_ext/mod.cpp:385: undefined reference to __im
. C:\Users\om\AppData\Local\Temp\ccUzk0H7.o:C:/Users/om/AppData/Local/Theano/compiledir_Windows-10-10.0.14393-SP0-Intel64_Family_6_Model_94_Stepping_3_GenuineIntel-3.5.2-64/lazylinker_ext
. C:/Users/om/AppData/Local/Theano/compiledir_Windows-10-10.0.14393-SP0-Intel64_Family_6_Model_94_Stepping_3_GenuineIntel-3.5.2-64/lazylinker_ext/mod.cpp:405: undefined reference to __im
. C:/Users/om/AppData/Local/Theano/compiledir_Windows-10-10.0.14393-SP0-Intel64_Family_6_Model_94_Stepping_3_GenuineIntel-3.5.2-64/lazylinker_ext/mod.cpp:426: undefined reference to __im
. C:/Users/om/AppData/Local/Theano/compiledir_Windows-10-10.0.14393-SP0-Intel64_Family_6_Model_94_Stepping_3_GenuineIntel-3.5.2-64/lazylinker_ext/mod.cpp:444: undefined reference to __im
. C:/Users/om/AppData/Local/Theano/compiledir_Windows-10-10.0.14393-SP0-Intel64_Family_6_Model_94_Stepping_3_GenuineIntel-3.5.2-64/lazylinker_ext/mod.cpp:545: undefined reference to __im
. C:/Users/om/AppData/Local/Theano/compiledir_Windows-10-10.0.14393-SP0-Intel64_Family_6_Model_94_Stepping_3_GenuineIntel-3.5.2-64/lazylinker_ext/mod.cpp:546: undefined reference to __im
. C:/Users/om/AppData/Local/Theano/compiledir_Windows-10-10.0.14393-SP0-Intel64_Family_6_Model_94_Stepping_3_GenuineIntel-3.5.2-64/lazylinker_ext/mod.cpp:641: undefined reference to __im
. C:/Users/om/AppData/Local/Theano/compiledir_Windows-10-10.0.14393-SP0-Intel64_Family_6_Model_94_Stepping_3_GenuineIntel-3.5.2-64/lazylinker_ext/mod.cpp:657: undefined reference to __im
. C:/Users/om/AppData/Local/Theano/compiledir_Windows-10-10.0.14393-SP0-Intel64_Family_6_Model_94_Stepping_3_GenuineIntel-3.5.2-64/lazylinker_ext/mod.cpp:715: undefined reference to __im
. C:/Users/om/AppData/Local/Theano/compiledir_Windows-10-10.0.14393-SP0-Intel64_Family_6_Model_94_Stepping_3_GenuineIntel-3.5.2-64/lazylinker_ext/mod.cpp:771: undefined reference to __im
. C:/Users/om/AppData/Local/Theano/compiledir_Windows-10-10.0.14393-SP0-Intel64_Family_6_Model_94_Stepping_3_GenuineIntel-3.5.2-64/lazylinker_ext/mod.cpp:772: undefined reference to __im
. C:/Users/om/AppData/Local/Theano/compiledir_Windows-10-10.0.14393-SP0-Intel64_Family_6_Model_94_Stepping_3_GenuineIntel-3.5.2-64/lazylinker_ext/mod.cpp:826: undefined reference to __im
. C:/Users/om/AppData/Local/Theano/compiledir_Windows-10-10.0.14393-SP0-Intel64_Family_6_Model_94_Stepping_3_GenuineIntel-3.5.2-64/lazylinker_ext/mod.cpp:839: undefined reference to __im
. C:/Users/om/AppData/Local/Theano/compiledir_Windows-10-10.0.14393-SP0-Intel64_Family_6_Model_94_Stepping_3_GenuineIntel-3.5.2-64/lazylinker_ext/mod.cpp:849: undefined reference to __im
. C:\Users\om\AppData\Local\Temp\ccUzk0H7.o:C:/Users/om/AppData/Local/Theano/compiledir_Windows-10-10.0.14393-SP0-Intel64_Family_6_Model_94_Stepping_3_GenuineIntel-3.5.2-64/lazylinker_ext
. C:/Users/om/AppData/Local/Theano/compiledir_Windows-10-10.0.14393-SP0-Intel64_Family_6_Model_94_Stepping_3_GenuineIntel-3.5.2-64/lazylinker_ext/mod.cpp:894: undefined reference to __im
. C:/Users/om/AppData/Local/Theano/compiledir_Windows-10-10.0.14393-SP0-Intel64_Family_6_Model_94_Stepping_3_GenuineIntel-3.5.2-64/lazylinker_ext/mod.cpp:937: undefined reference to __im
. C:/Users/om/AppData/Local/Theano/compiledir_Windows-10-10.0.14393-SP0-Intel64_Family_6_Model_94_Stepping_3_GenuineIntel-3.5.2-64/lazylinker_ext/mod.cpp:973: undefined reference to __im
. collect2.exe: error: ld returned 1 exit status                                                                                                                                           "
keras,10353,"Please make sure that the boxes below are checked before you submit your issue. If your issue is an implementation question, please ask your question on [StackOverflow](http://stackoverflow.com/questions/tagged/keras) or [join the Keras Slack channel](https://keras-slack-autojoin.herokuapp.com/) and ask there instead of filing a GitHub issue.

Thank you!

- [ ] Check that you are up-to-date with the master branch of Keras. You can update with:
pip install git+git://github.com/keras-team/keras.git --upgrade --no-deps

- [ ] If running on TensorFlow, check that you are up-to-date with the latest version. The installation instructions can be found [here](https://www.tensorflow.org/get_started/os_setup).

- [ ] If running on Theano, check that you are up-to-date with the master branch of Theano. You can update with:
pip install git+git://github.com/Theano/Theano.git --upgrade --no-deps

- [ ] Provide a link to a GitHub Gist of a Python script that can reproduce your issue (or just copy the script here if it is short).
",0,callbacks,"callbacks Please make sure that the boxes below are checked before you submit your issue. If your issue is an implementation question, please ask your question on [StackOverflow](http://stackoverflow.com/questions/tagged/keras) or [join the Keras Slack channel](https://keras-slack-autojoin.herokuapp.com/) and ask there instead of filing a GitHub issue.

Thank you!

- [ ] Check that you are up-to-date with the master branch of Keras. You can update with:
pip install git+git://github.com/keras-team/keras.git --upgrade --no-deps

- [ ] If running on TensorFlow, check that you are up-to-date with the latest version. The installation instructions can be found [here](https://www.tensorflow.org/get_started/os_setup).

- [ ] If running on Theano, check that you are up-to-date with the master branch of Theano. You can update with:
pip install git+git://github.com/Theano/Theano.git --upgrade --no-deps

- [ ] Provide a link to a GitHub Gist of a Python script that can reproduce your issue (or just copy the script here if it is short).
"
keras,3037,"I'm implementing a custom layer. When I try to call it using the following code it gives me an error. Is this the right way to implement a layer with multiple inputs? Also the code in the call function of the layer is using some theano.Tensor functions instead of the functions given in keras backend. Does this mean I won't be able to use this with a functional api anymore?



Traceback (most recent call last):
  File ""/Users/Aditya/Documents/Qbit Logic/TBCNN/Tree.py"", line 338, in <module>
    fitlog = model.fit([trees, connections, n_leaves_mat], y, batch_size=1, nb_epoch=50, verbose=1)
  File ""/Users/Aditya/anaconda/envs/acads/lib/python3.5/site-packages/keras/models.py"", line 409, in fit
    sample_weight=sample_weight)
  File ""/Users/Aditya/anaconda/envs/acads/lib/python3.5/site-packages/keras/engine/training.py"", line 1037, in fit
    self._make_train_function()
  File ""/Users/Aditya/anaconda/envs/acads/lib/python3.5/site-packages/keras/engine/training.py"", line 663, in _make_train_function
    training_updates = self.optimizer.get_updates(trainable_weights, self.constraints, self.total_loss)
  File ""/Users/Aditya/anaconda/envs/acads/lib/python3.5/site-packages/keras/optimizers.py"", line 321, in get_updates
    grads = self.get_gradients(loss, params)
  File ""/Users/Aditya/anaconda/envs/acads/lib/python3.5/site-packages/keras/optimizers.py"", line 53, in get_gradients
    grads = K.gradients(loss, params)
  File ""/Users/Aditya/anaconda/envs/acads/lib/python3.5/site-packages/keras/backend/theano_backend.py"", line 532, in gradients
    return T.grad(loss, variables)
  File ""/Users/Aditya/anaconda/envs/acads/lib/python3.5/site-packages/theano/gradient.py"", line 545, in grad
    handle_disconnected(elem)
  File ""/Users/Aditya/anaconda/envs/acads/lib/python3.5/site-packages/theano/gradient.py"", line 532, in handle_disconnected
    raise DisconnectedInputError(message)
theano.gradient.DisconnectedInputError: grad method was asked to compute the gradient with respect to a variable that is not part of the computational graph of the cost, or is used only by a non-differentiable operator: <TensorType(float32, matrix)>
Backtrace when the node is created:
  File ""/Users/Aditya/Documents/Qbit Logic/TBCNN/Tree.py"", line 336, in <module>
    model.compile('adam', 'mse')
  File ""/Users/Aditya/anaconda/envs/acads/lib/python3.5/site-packages/keras/models.py"", line 339, in compile
    **kwargs)
  File ""/Users/Aditya/anaconda/envs/acads/lib/python3.5/site-packages/keras/engine/training.py"", line 510, in compile
    masks = self.compute_mask(self.inputs, mask=None)
  File ""/Users/Aditya/anaconda/envs/acads/lib/python3.5/site-packages/keras/engine/topology.py"", line 1914, in compute_mask
    output_tensors, output_masks, output_shapes = self.run_internal_graph(inputs, masks)
  File ""/Users/Aditya/anaconda/envs/acads/lib/python3.5/site-packages/keras/engine/topology.py"", line 2049, in run_internal_graph
    output_tensors = to_list(layer.call(computed_tensors, computed_masks))
  File ""/Users/Aditya/Documents/Qbit Logic/TBCNN/Tree.py"", line 178, in call
    self.build([x.shape for x in inputs])
  File ""/Users/Aditya/Documents/Qbit Logic/TBCNN/Tree.py"", line 168, in build
    self.VCi, self.Wl, self.Wr = K.variable(vci), K.variable(wl), K.variable(wr)
  File ""/Users/Aditya/anaconda/envs/acads/lib/python3.5/site-packages/keras/backend/theano_backend.py"", line 31, in variable
    return theano.shared(value=value, name=name, strict=False)



Please make sure that the boxes below are checked before you submit your issue. Thank you!
- [x] Check that you are up-to-date with the master branch of Keras. You can update with:
  pip install git+git://github.com/fchollet/keras.git --upgrade --no-deps
- [x] If running on Theano, check that you are up-to-date with the master branch of Theano. You can update with:
  pip install git+git://github.com/Theano/Theano.git --upgrade --no-deps
- [x] Provide a link to a GitHub Gist of a Python script that can reproduce your issue (or just copy the script here if it is short).
",0,Implement custom layer with multiple inputs which is input layer and has trainable weights,"Implement custom layer with multiple inputs which is input layer and has trainable weights I'm implementing a custom layer. When I try to call it using the following code it gives me an error. Is this the right way to implement a layer with multiple inputs? Also the code in the call function of the layer is using some theano.Tensor functions instead of the functions given in keras backend. Does this mean I won't be able to use this with a functional api anymore?



Traceback (most recent call last):
  File ""/Users/Aditya/Documents/Qbit Logic/TBCNN/Tree.py"", line 338, in <module>
    fitlog = model.fit([trees, connections, n_leaves_mat], y, batch_size=1, nb_epoch=50, verbose=1)
  File ""/Users/Aditya/anaconda/envs/acads/lib/python3.5/site-packages/keras/models.py"", line 409, in fit
    sample_weight=sample_weight)
  File ""/Users/Aditya/anaconda/envs/acads/lib/python3.5/site-packages/keras/engine/training.py"", line 1037, in fit
    self._make_train_function()
  File ""/Users/Aditya/anaconda/envs/acads/lib/python3.5/site-packages/keras/engine/training.py"", line 663, in _make_train_function
    training_updates = self.optimizer.get_updates(trainable_weights, self.constraints, self.total_loss)
  File ""/Users/Aditya/anaconda/envs/acads/lib/python3.5/site-packages/keras/optimizers.py"", line 321, in get_updates
    grads = self.get_gradients(loss, params)
  File ""/Users/Aditya/anaconda/envs/acads/lib/python3.5/site-packages/keras/optimizers.py"", line 53, in get_gradients
    grads = K.gradients(loss, params)
  File ""/Users/Aditya/anaconda/envs/acads/lib/python3.5/site-packages/keras/backend/theano_backend.py"", line 532, in gradients
    return T.grad(loss, variables)
  File ""/Users/Aditya/anaconda/envs/acads/lib/python3.5/site-packages/theano/gradient.py"", line 545, in grad
    handle_disconnected(elem)
  File ""/Users/Aditya/anaconda/envs/acads/lib/python3.5/site-packages/theano/gradient.py"", line 532, in handle_disconnected
    raise DisconnectedInputError(message)
theano.gradient.DisconnectedInputError: grad method was asked to compute the gradient with respect to a variable that is not part of the computational graph of the cost, or is used only by a non-differentiable operator: <TensorType(float32, matrix)>
Backtrace when the node is created:
  File ""/Users/Aditya/Documents/Qbit Logic/TBCNN/Tree.py"", line 336, in <module>
    model.compile('adam', 'mse')
  File ""/Users/Aditya/anaconda/envs/acads/lib/python3.5/site-packages/keras/models.py"", line 339, in compile
    **kwargs)
  File ""/Users/Aditya/anaconda/envs/acads/lib/python3.5/site-packages/keras/engine/training.py"", line 510, in compile
    masks = self.compute_mask(self.inputs, mask=None)
  File ""/Users/Aditya/anaconda/envs/acads/lib/python3.5/site-packages/keras/engine/topology.py"", line 1914, in compute_mask
    output_tensors, output_masks, output_shapes = self.run_internal_graph(inputs, masks)
  File ""/Users/Aditya/anaconda/envs/acads/lib/python3.5/site-packages/keras/engine/topology.py"", line 2049, in run_internal_graph
    output_tensors = to_list(layer.call(computed_tensors, computed_masks))
  File ""/Users/Aditya/Documents/Qbit Logic/TBCNN/Tree.py"", line 178, in call
    self.build([x.shape for x in inputs])
  File ""/Users/Aditya/Documents/Qbit Logic/TBCNN/Tree.py"", line 168, in build
    self.VCi, self.Wl, self.Wr = K.variable(vci), K.variable(wl), K.variable(wr)
  File ""/Users/Aditya/anaconda/envs/acads/lib/python3.5/site-packages/keras/backend/theano_backend.py"", line 31, in variable
    return theano.shared(value=value, name=name, strict=False)



Please make sure that the boxes below are checked before you submit your issue. Thank you!
- [x] Check that you are up-to-date with the master branch of Keras. You can update with:
  pip install git+git://github.com/fchollet/keras.git --upgrade --no-deps
- [x] If running on Theano, check that you are up-to-date with the master branch of Theano. You can update with:
  pip install git+git://github.com/Theano/Theano.git --upgrade --no-deps
- [x] Provide a link to a GitHub Gist of a Python script that can reproduce your issue (or just copy the script here if it is short).
"
keras,7424,"Hi,

I'm not pointing a problem, I'm just wondering how I can manage to do what I want. I explain myself : 
For an image X, the Y that I want to predict is a couple of numpy array [Y1, Y2]. Y1 is (38, 60, 18) shaped and Y2 is (38, 60, 72) shaped. As I am working with a big dataset, I would like to use the train_on_batch function. However, this function should take a numpy array of X  values (which is not a problem) and a numpy array of Y values. Because Y1 and Y2 are not of the same shape, I can't stack them into a single array, and using a list raise an exception because train_on_batch calls the Y shape. Does anyone has an idea about how I can trick that ?

Thanks for reading",0,train_on_batch for multiple predictions,"train_on_batch for multiple predictions Hi,

I'm not pointing a problem, I'm just wondering how I can manage to do what I want. I explain myself : 
For an image X, the Y that I want to predict is a couple of numpy array [Y1, Y2]. Y1 is (38, 60, 18) shaped and Y2 is (38, 60, 72) shaped. As I am working with a big dataset, I would like to use the train_on_batch function. However, this function should take a numpy array of X  values (which is not a problem) and a numpy array of Y values. Because Y1 and Y2 are not of the same shape, I can't stack them into a single array, and using a list raise an exception because train_on_batch calls the Y shape. Does anyone has an idea about how I can trick that ?

Thanks for reading"
keras,4891,"Hello everyone!

Could anyone tell me, please, whether a built-in leave-one-out validation exists in Keras? I mean, let's assume I have only a training data set consisting of 100 records and want to build the model 100 times based on 99 records and validating it on the remaining one. Would like to avoid implementing this on my own with loops, if possible. :)
Of course, during this process I'd like to accumulate the error of predictions (basically, my target variable is continuous, so speeking here about a regression task) from model to model.

Thank you very much 4 your responses in advance!

Almost forgot: Happy New Year! :)",0,Built-in leave-one-out validation in Keras,"Built-in leave-one-out validation in Keras Hello everyone!

Could anyone tell me, please, whether a built-in leave-one-out validation exists in Keras? I mean, let's assume I have only a training data set consisting of 100 records and want to build the model 100 times based on 99 records and validating it on the remaining one. Would like to avoid implementing this on my own with loops, if possible. :)
Of course, during this process I'd like to accumulate the error of predictions (basically, my target variable is continuous, so speeking here about a regression task) from model to model.

Thank you very much 4 your responses in advance!

Almost forgot: Happy New Year! :)"
keras,9576,"https://keras.io/activations/

It appears the last line is redundant here:

> 

Otherwise this contradicts to previous paragraphs in that page:

> 
> 
> This is equivalent to:
> ",0,docs correction: https://keras.io/activations/,"docs correction: https://keras.io/activations/ https://keras.io/activations/

It appears the last line is redundant here:

> 

Otherwise this contradicts to previous paragraphs in that page:

> 
> 
> This is equivalent to:
> "
keras,4045,"Hi, 
I am using my pretrained embedding layer on top of a stateful LSTM. My word2vec embedding is trained on a larger corpus(word2vec corpus) than the lstm training corpus(model corpus). i'm mapping word vectors to embedding weights using the  word2vec model. Truncated model corpus to be divided into uniform sequences
For preparing the labels of train data  i'm using the LSTM model vocab(not word2vec vocab) with one-hot encoding and no 0 masking.
While training the model, I'm getting the following error- 



Model-



Am I missing something? Thanks.
",0,Index value out of bound while training using embedding + stateful LSTM,"Index value out of bound while training using embedding + stateful LSTM Hi, 
I am using my pretrained embedding layer on top of a stateful LSTM. My word2vec embedding is trained on a larger corpus(word2vec corpus) than the lstm training corpus(model corpus). i'm mapping word vectors to embedding weights using the  word2vec model. Truncated model corpus to be divided into uniform sequences
For preparing the labels of train data  i'm using the LSTM model vocab(not word2vec vocab) with one-hot encoding and no 0 masking.
While training the model, I'm getting the following error- 



Model-



Am I missing something? Thanks.
"
keras,7539,"Please make sure that the boxes below are checked before you submit your issue. If your issue is an implementation question, please ask your question on [StackOverflow](http://stackoverflow.com/questions/tagged/keras) or [join the Keras Slack channel](https://keras-slack-autojoin.herokuapp.com/) and ask there instead of filing a GitHub issue.

Thank you!

- [x] Check that you are up-to-date with the master branch of Keras. You can update with:
pip install git+git://github.com/fchollet/keras.git --upgrade --no-deps

- [x] If running on TensorFlow, check that you are up-to-date with the latest version. The installation instructions can be found [here](https://www.tensorflow.org/get_started/os_setup).

- [ ] If running on Theano, check that you are up-to-date with the master branch of Theano. You can update with:
pip install git+git://github.com/Theano/Theano.git --upgrade --no-deps

- [x] Provide a link to a GitHub Gist of a Python script that can reproduce your issue (or just copy the script here if it is short).

I have asked this question on stackoverflow, buy I haven't received any answers to it.

When I try to fit a model with the theano backend I run into the following error



Here's the full error stack


I am required to use the theano backend due to compatibility issues of the tensorflow backend with my university cluster. I have tested this code on CPU's on my personal device with the tensorflow backend and it runs error free.

Load Script

Train script

Packages
",0,UnicodeDecode error in keras fit function with theano backend,"UnicodeDecode error in keras fit function with theano backend Please make sure that the boxes below are checked before you submit your issue. If your issue is an implementation question, please ask your question on [StackOverflow](http://stackoverflow.com/questions/tagged/keras) or [join the Keras Slack channel](https://keras-slack-autojoin.herokuapp.com/) and ask there instead of filing a GitHub issue.

Thank you!

- [x] Check that you are up-to-date with the master branch of Keras. You can update with:
pip install git+git://github.com/fchollet/keras.git --upgrade --no-deps

- [x] If running on TensorFlow, check that you are up-to-date with the latest version. The installation instructions can be found [here](https://www.tensorflow.org/get_started/os_setup).

- [ ] If running on Theano, check that you are up-to-date with the master branch of Theano. You can update with:
pip install git+git://github.com/Theano/Theano.git --upgrade --no-deps

- [x] Provide a link to a GitHub Gist of a Python script that can reproduce your issue (or just copy the script here if it is short).

I have asked this question on stackoverflow, buy I haven't received any answers to it.

When I try to fit a model with the theano backend I run into the following error



Here's the full error stack


I am required to use the theano backend due to compatibility issues of the tensorflow backend with my university cluster. I have tested this code on CPU's on my personal device with the tensorflow backend and it runs error free.

Load Script

Train script

Packages
"
keras,6842,"I'm unable to save the model trained using KerasRegressor wrapper...


Here's my import:





Any ideas?",0, 'KerasRegressor' object has no attribute 'to_json'," 'KerasRegressor' object has no attribute 'to_json' I'm unable to save the model trained using KerasRegressor wrapper...


Here's my import:





Any ideas?"
keras,2607,"Most examples are use metrics=['accuracy'], but accuracy is not always suitable for every task. 
1. So are there any metrics such as precision, recall and so on?
2. If there are, what should I write in metrics list in order to use them?
3. If I just have one output, can I use multiple metrics to evaluate it from different aspect?
",0,what metrics can be used in keras,"what metrics can be used in keras Most examples are use metrics=['accuracy'], but accuracy is not always suitable for every task. 
1. So are there any metrics such as precision, recall and so on?
2. If there are, what should I write in metrics list in order to use them?
3. If I just have one output, can I use multiple metrics to evaluate it from different aspect?
"
keras,1415,"Interesting.. not really an issue but  in Malay language is ''Hard""


",0,Keras in Malay,"Keras in Malay Interesting.. not really an issue but  in Malay language is ''Hard""


"
keras,3543,"I am trying to implement the layer normalization in a standard fully connected neural network with keras, by writing a new layer. I copy nearly all the code of Dense layer and add a function of layer normalization and corresponding parameters. My code is as below:



But during fit, it got an TypeError: unorderable types: NoneType() < NoneType(). According to the log message, seems that the reason is the trainable_weights:



Here is the code building the model and fit it:



Could you please tell me what have I done wrong and how should I fix it? Thank you in advance!
",0,How to set trainable_weights properly?,"How to set trainable_weights properly? I am trying to implement the layer normalization in a standard fully connected neural network with keras, by writing a new layer. I copy nearly all the code of Dense layer and add a function of layer normalization and corresponding parameters. My code is as below:



But during fit, it got an TypeError: unorderable types: NoneType() < NoneType(). According to the log message, seems that the reason is the trainable_weights:



Here is the code building the model and fit it:



Could you please tell me what have I done wrong and how should I fix it? Thank you in advance!
"
keras,3651,"

I am using  as shown above only to operate on higher order inputs, and propagate the shape information accordingly. However, when I run the code above, I see that the first print statement shows a (symbolic) mask () and the second one just gives a . I'm not sure why  does not propagate the mask.
- [x] Check that you are up-to-date with the master branch of Keras. You can update with:
  pip install git+git://github.com/fchollet/keras.git --upgrade --no-deps
- [x] If running on Theano, check that you are up-to-date with the master branch of Theano. You can update with:
  pip install git+git://github.com/Theano/Theano.git --upgrade --no-deps
- [x] Provide a link to a GitHub Gist of a Python script that can reproduce your issue (or just copy the script here if it is short).
",0,Mask propagation in TimeDistributed doesn't seem to work after Embedding,"Mask propagation in TimeDistributed doesn't seem to work after Embedding 

I am using  as shown above only to operate on higher order inputs, and propagate the shape information accordingly. However, when I run the code above, I see that the first print statement shows a (symbolic) mask () and the second one just gives a . I'm not sure why  does not propagate the mask.
- [x] Check that you are up-to-date with the master branch of Keras. You can update with:
  pip install git+git://github.com/fchollet/keras.git --upgrade --no-deps
- [x] If running on Theano, check that you are up-to-date with the master branch of Theano. You can update with:
  pip install git+git://github.com/Theano/Theano.git --upgrade --no-deps
- [x] Provide a link to a GitHub Gist of a Python script that can reproduce your issue (or just copy the script here if it is short).
"
keras,3008,"It doesn't seem possible to save and load models to/from JSON or YAML if there's a Merge layer with a lambda.
",0,JSON/YAML loading not working with Lambda merge layers,"JSON/YAML loading not working with Lambda merge layers It doesn't seem possible to save and load models to/from JSON or YAML if there's a Merge layer with a lambda.
"
keras,544,"Hi guys,

I just updated my keras package and now I can't import Sequential properly. It will give me the following erros:

  File ""/usr/lib/python2.6/site-packages/Keras-0.1.2-py2.6.egg/keras/models.py"", line 118
    if model_name not in {'Graph', 'Sequential'}:
                                 ^
SyntaxError: invalid syntax

Could anyone help me?
",0,Can't import sequential,"Can't import sequential Hi guys,

I just updated my keras package and now I can't import Sequential properly. It will give me the following erros:

  File ""/usr/lib/python2.6/site-packages/Keras-0.1.2-py2.6.egg/keras/models.py"", line 118
    if model_name not in {'Graph', 'Sequential'}:
                                 ^
SyntaxError: invalid syntax

Could anyone help me?
"
keras,4767,"My problem is, I'm writing a layer with input shape (None, 10), and I want to append the same value to the end of every row to make it (None, 11). How can I do this? 

Right now I'm thinking of creating another (None, 1), then concatenating them.

Thanks in advance.",0,"How can I create a variable with shape (None, 1) ?","How can I create a variable with shape (None, 1) ? My problem is, I'm writing a layer with input shape (None, 10), and I want to append the same value to the end of every row to make it (None, 11). How can I do this? 

Right now I'm thinking of creating another (None, 1), then concatenating them.

Thanks in advance."
keras,4776,"Hi, I am doing a sequence labelling task(like PosTagging) using Keras + theano. The model I use is BiLSTM. For training, I did not do padding. Instead, I just used the batch size of 1, and did not specify the input_length parameter. It works well for training and evaluation(for both I use the generator version to deal with the memory problem). However, when I want to do prediction, I have the following errors:



I think the reason is the first sentence have a length of 42 words, while the second sentence have a length of 10 words. 

My question is, why it works well for training and evaluation, but does not work for prediction?

The code I think relevant is here

",0,predict_generator function does not support samples with different lengths,"predict_generator function does not support samples with different lengths Hi, I am doing a sequence labelling task(like PosTagging) using Keras + theano. The model I use is BiLSTM. For training, I did not do padding. Instead, I just used the batch size of 1, and did not specify the input_length parameter. It works well for training and evaluation(for both I use the generator version to deal with the memory problem). However, when I want to do prediction, I have the following errors:



I think the reason is the first sentence have a length of 42 words, while the second sentence have a length of 10 words. 

My question is, why it works well for training and evaluation, but does not work for prediction?

The code I think relevant is here

"
keras,8070,"Hi all,

I'm trying to split one keras layer using Lambda function. Follow is my code snippet:

This model compiles well.
But when I feed real training data to this model, it gives me the error messege:
  [generated_images,f_id] = generator.predict([image_batch, c_, z])
  File ""build/bdist.linux-x86_64/egg/keras/engine/training.py"", line 1653, in predict
  File ""build/bdist.linux-x86_64/egg/keras/engine/training.py"", line 1246, in _predict_loop
  File ""build/bdist.linux-x86_64/egg/keras/backend/tensorflow_backend.py"", line 2255, in __call__
  File ""/home/vivo/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 767, in run
    run_metadata_ptr)
  File ""/home/vivo/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 965, in _run
    feed_dict_string, options, run_metadata)
  File ""/home/vivo/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1015, in _do_run
    target_list, options, run_metadata)
  File ""/home/vivo/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1035, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.InvalidArgumentError: Input to reshape is a tensor with 8768 values, but the requested shape requires a multiple of 146
	 [[Node: dense_4/Reshape = Reshape[T=DT_FLOAT, Tshape=DT_INT32, _device=""/job:localhost/replica:0/task:0/gpu:0""](concatenate_1/concat, dense_4/Reshape/shape)]]
	 [[Node: conv2d_transpose_3/Tanh/_53 = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/gpu:0"", send_device_incarnation=1, tensor_name=""edge_1_conv2d_transpose_3/Tanh"", tensor_type=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/cpu:0""]()]]

Caused by op u'dense_4/Reshape', defined at:
  File ""GAN_UTD_2nd_model.py"", line 428, in <module>
    generator = model_generator(latent_dim = latent_dim, input_shape = input_shape, units = units)
  File ""GAN_UTD_2nd_model.py"", line 240, in model_generator
    h_dense = Dense(15 * 20 * 64, activation = 'relu')(h)
  File ""build/bdist.linux-x86_64/egg/keras/engine/topology.py"", line 602, in __call__
    output = self.call(inputs, **kwargs)
  File ""build/bdist.linux-x86_64/egg/keras/layers/core.py"", line 841, in call
    output = K.dot(inputs, self.kernel)
  File ""build/bdist.linux-x86_64/egg/keras/backend/tensorflow_backend.py"", line 973, in dot
    xt = tf.reshape(x, [-1, x_shape[-1]])
  File ""/home/vivo/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/ops/gen_array_ops.py"", line 2630, in reshape
    name=name)
  File ""/home/vivo/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py"", line 763, in apply_op
    op_def=op_def)
  File ""/home/vivo/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 2327, in create_op
    original_op=self._default_original_op, op_def=op_def)
  File ""/home/vivo/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 1226, in __init__
    self._traceback = _extract_stack()

InvalidArgumentError (see above for traceback): Input to reshape is a tensor with 8768 values, but the requested shape requires a multiple of 146
	 [[Node: dense_4/Reshape = Reshape[T=DT_FLOAT, Tshape=DT_INT32, _device=""/job:localhost/replica:0/task:0/gpu:0""](concatenate_1/concat, dense_4/Reshape/shape)]]
	 [[Node: conv2d_transpose_3/Tanh/_53 = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/gpu:0"", send_device_incarnation=1, tensor_name=""edge_1_conv2d_transpose_3/Tanh"", tensor_type=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/cpu:0""]()]]


I don't understand especially this error message""tensorflow.python.framework.errors_impl.InvalidArgumentError: Input to reshape is a tensor with 8768 values, but the requested shape requires a multiple of 146"".
The output of Dense layer is 19200 units, so it should be right for the following Reshape layer...

Any idea??
Best
",0,Layer split use Lambda layer gives error,"Layer split use Lambda layer gives error Hi all,

I'm trying to split one keras layer using Lambda function. Follow is my code snippet:

This model compiles well.
But when I feed real training data to this model, it gives me the error messege:
  [generated_images,f_id] = generator.predict([image_batch, c_, z])
  File ""build/bdist.linux-x86_64/egg/keras/engine/training.py"", line 1653, in predict
  File ""build/bdist.linux-x86_64/egg/keras/engine/training.py"", line 1246, in _predict_loop
  File ""build/bdist.linux-x86_64/egg/keras/backend/tensorflow_backend.py"", line 2255, in __call__
  File ""/home/vivo/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 767, in run
    run_metadata_ptr)
  File ""/home/vivo/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 965, in _run
    feed_dict_string, options, run_metadata)
  File ""/home/vivo/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1015, in _do_run
    target_list, options, run_metadata)
  File ""/home/vivo/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1035, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.InvalidArgumentError: Input to reshape is a tensor with 8768 values, but the requested shape requires a multiple of 146
	 [[Node: dense_4/Reshape = Reshape[T=DT_FLOAT, Tshape=DT_INT32, _device=""/job:localhost/replica:0/task:0/gpu:0""](concatenate_1/concat, dense_4/Reshape/shape)]]
	 [[Node: conv2d_transpose_3/Tanh/_53 = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/gpu:0"", send_device_incarnation=1, tensor_name=""edge_1_conv2d_transpose_3/Tanh"", tensor_type=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/cpu:0""]()]]

Caused by op u'dense_4/Reshape', defined at:
  File ""GAN_UTD_2nd_model.py"", line 428, in <module>
    generator = model_generator(latent_dim = latent_dim, input_shape = input_shape, units = units)
  File ""GAN_UTD_2nd_model.py"", line 240, in model_generator
    h_dense = Dense(15 * 20 * 64, activation = 'relu')(h)
  File ""build/bdist.linux-x86_64/egg/keras/engine/topology.py"", line 602, in __call__
    output = self.call(inputs, **kwargs)
  File ""build/bdist.linux-x86_64/egg/keras/layers/core.py"", line 841, in call
    output = K.dot(inputs, self.kernel)
  File ""build/bdist.linux-x86_64/egg/keras/backend/tensorflow_backend.py"", line 973, in dot
    xt = tf.reshape(x, [-1, x_shape[-1]])
  File ""/home/vivo/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/ops/gen_array_ops.py"", line 2630, in reshape
    name=name)
  File ""/home/vivo/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py"", line 763, in apply_op
    op_def=op_def)
  File ""/home/vivo/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 2327, in create_op
    original_op=self._default_original_op, op_def=op_def)
  File ""/home/vivo/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 1226, in __init__
    self._traceback = _extract_stack()

InvalidArgumentError (see above for traceback): Input to reshape is a tensor with 8768 values, but the requested shape requires a multiple of 146
	 [[Node: dense_4/Reshape = Reshape[T=DT_FLOAT, Tshape=DT_INT32, _device=""/job:localhost/replica:0/task:0/gpu:0""](concatenate_1/concat, dense_4/Reshape/shape)]]
	 [[Node: conv2d_transpose_3/Tanh/_53 = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/gpu:0"", send_device_incarnation=1, tensor_name=""edge_1_conv2d_transpose_3/Tanh"", tensor_type=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/cpu:0""]()]]


I don't understand especially this error message""tensorflow.python.framework.errors_impl.InvalidArgumentError: Input to reshape is a tensor with 8768 values, but the requested shape requires a multiple of 146"".
The output of Dense layer is 19200 units, so it should be right for the following Reshape layer...

Any idea??
Best
"
keras,1198,,0,How to display RGB image (e.g. cifar) in Keras?,
keras,2545,"I have 2 questions:
1. I discussed about sequence to sequence learning where the length of input and output are different in problem #2403 and I concluded that I have to use encoder and decoder architecture. I have a fundamental question, what is the reason that keras doesn't support this case and encoder and decoder is needed.  Is it a fundamental problem of recurrent neural networks (if it is, can anyone explain this to me?), or is it the Keras problem that doesn't support this case? 

2.When I have the following model:



consider batch_size =1, then which of the following statement is true:
A. All _n_prev_ inputs are fed to all  _hidden_neurons_ in the first LSTM layer?
B. At each time only one of _in_prev_ inputs are fed  to the first LSTM layer and it will propagate to all other _hidden_neurons_ in this layer?
",0,Sequence to sequence training and predicting (Decoder Encoder),"Sequence to sequence training and predicting (Decoder Encoder) I have 2 questions:
1. I discussed about sequence to sequence learning where the length of input and output are different in problem #2403 and I concluded that I have to use encoder and decoder architecture. I have a fundamental question, what is the reason that keras doesn't support this case and encoder and decoder is needed.  Is it a fundamental problem of recurrent neural networks (if it is, can anyone explain this to me?), or is it the Keras problem that doesn't support this case? 

2.When I have the following model:



consider batch_size =1, then which of the following statement is true:
A. All _n_prev_ inputs are fed to all  _hidden_neurons_ in the first LSTM layer?
B. At each time only one of _in_prev_ inputs are fed  to the first LSTM layer and it will propagate to all other _hidden_neurons_ in this layer?
"
keras,13268,"<em>Please make sure that this is a Bug or a Feature Request and provide all applicable information asked by the template.
If your issue is an **implementation question**, please ask your question on [StackOverflow](http://stackoverflow.com/questions/tagged/keras) or [on the Keras Slack channel](https://keras-slack-autojoin.herokuapp.com/) instead of opening a GitHub issue.</em>  

**System information**  
- Have I written custom code (as opposed to using example directory):  
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Linux CentOS 7.0
- TensorFlow backend (yes / no):  yes
- TensorFlow version:  1.14
- Keras version:  2.2.0
- Python version:  2.7
- CUDA/cuDNN version:  8.0
- GPU model and memory:  single GPU with 16GB memory

You can obtain the TensorFlow version with:  
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""  
You can obtain the Keras version with:  
python -c 'import keras as k; print(k.__version__)'  

**Describe the current behavior**  

**Describe the expected behavior**  

**Code to reproduce the issue**  
Provide a reproducible test case that is the bare minimum necessary to generate the problem.  

**Other info / logs**  
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.  

Hi ,I found I run the same code and same weight with same images but got different predictions each time ,my code and images are on github [binary_categorical](https://github.com/yasohasakii/binary_categorical) and you can run the  script on colab, and weight  file has been upload to [google dirver](https://drive.google.com/open?id=1sCIAgoQ7Og18iBmhwaOUmVSbRziAX5fo). Below codes are my result for twice times.  
  


",0,predictions different with same weight,"predictions different with same weight <em>Please make sure that this is a Bug or a Feature Request and provide all applicable information asked by the template.
If your issue is an **implementation question**, please ask your question on [StackOverflow](http://stackoverflow.com/questions/tagged/keras) or [on the Keras Slack channel](https://keras-slack-autojoin.herokuapp.com/) instead of opening a GitHub issue.</em>  

**System information**  
- Have I written custom code (as opposed to using example directory):  
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Linux CentOS 7.0
- TensorFlow backend (yes / no):  yes
- TensorFlow version:  1.14
- Keras version:  2.2.0
- Python version:  2.7
- CUDA/cuDNN version:  8.0
- GPU model and memory:  single GPU with 16GB memory

You can obtain the TensorFlow version with:  
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""  
You can obtain the Keras version with:  
python -c 'import keras as k; print(k.__version__)'  

**Describe the current behavior**  

**Describe the expected behavior**  

**Code to reproduce the issue**  
Provide a reproducible test case that is the bare minimum necessary to generate the problem.  

**Other info / logs**  
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.  

Hi ,I found I run the same code and same weight with same images but got different predictions each time ,my code and images are on github [binary_categorical](https://github.com/yasohasakii/binary_categorical) and you can run the  script on colab, and weight  file has been upload to [google dirver](https://drive.google.com/open?id=1sCIAgoQ7Og18iBmhwaOUmVSbRziAX5fo). Below codes are my result for twice times.  
  


"
keras,5044,"[Here's the script][1] to run to see the exception. One can run it like . After a quick glance at the source, it seems like  is not used in the  constructor to prevent  from being set to , which ultimately becomes  in the base .

Some reporters of a previous bug (#4699) have reported that this ""works"" when you create an additional directory under the directory passed to . However, I don't think this is ideal, for semantically, there _is_ no class/label when dealing with test images.

My proposal to fix this is to set self.nb_sample = number of valid image files in the input directory when , in . The second clause in the condition will help prevent breakage of code that has hacked its way around this issue by creating an additional directory (or we don't have to be afraid of breaking, it's up to you). Please let me know if you are okay with this, and if you have any ideas (including calling this issue a non-issue :)), and I can accordingly prepare a quick PR (or not).

[1]: https://gist.github.com/yati-sagade/ff309678a6d6ec849c488b1f9a5fa6b3",0,`ZeroDivisionError` when using `class_mode=None` with `ImageDataGenerator.flow_from_directory()`,"`ZeroDivisionError` when using `class_mode=None` with `ImageDataGenerator.flow_from_directory()` [Here's the script][1] to run to see the exception. One can run it like . After a quick glance at the source, it seems like  is not used in the  constructor to prevent  from being set to , which ultimately becomes  in the base .

Some reporters of a previous bug (#4699) have reported that this ""works"" when you create an additional directory under the directory passed to . However, I don't think this is ideal, for semantically, there _is_ no class/label when dealing with test images.

My proposal to fix this is to set self.nb_sample = number of valid image files in the input directory when , in . The second clause in the condition will help prevent breakage of code that has hacked its way around this issue by creating an additional directory (or we don't have to be afraid of breaking, it's up to you). Please let me know if you are okay with this, and if you have any ideas (including calling this issue a non-issue :)), and I can accordingly prepare a quick PR (or not).

[1]: https://gist.github.com/yati-sagade/ff309678a6d6ec849c488b1f9a5fa6b3"
keras,8073,"I've written my own loss function which seems to get stuck in this error which I can't figure out.
Seems it could either be sample_weight or mask, but i don't know what to do with it.

Can anyone help?

![image](https://user-images.githubusercontent.com/5853644/31272735-49a469ce-aa8c-11e7-9d48-80eb1a64b29a.png)


",0,Not understanding error,"Not understanding error I've written my own loss function which seems to get stuck in this error which I can't figure out.
Seems it could either be sample_weight or mask, but i don't know what to do with it.

Can anyone help?

![image](https://user-images.githubusercontent.com/5853644/31272735-49a469ce-aa8c-11e7-9d48-80eb1a64b29a.png)


"
keras,7193,"I use keras 2.02 with theano backend

When I call the  attribute 'ctc_decode' using K.ctc_decode, an error was thrown. I use dir(K) to check its avaliable attribute, but I do not find 'ctc_decode'. 
Where is the problem? Thank you in advance!",0,AttributeError: 'module' object has no attribute 'ctc_decode',"AttributeError: 'module' object has no attribute 'ctc_decode' I use keras 2.02 with theano backend

When I call the  attribute 'ctc_decode' using K.ctc_decode, an error was thrown. I use dir(K) to check its avaliable attribute, but I do not find 'ctc_decode'. 
Where is the problem? Thank you in advance!"
keras,8827,"Hi,
i´m wondering about the correct input_shape and/or target_size when using fit_generator and flow_from_directory. I´m running keras 2.0.8-tf and tensorflow 1.4 as the backend. My images have the following dimensions:

width: 725 Pixel
height: 180 Pixel
channels: 3

I define the Input_shape as: (width, height, channels) --> (725,180,3), as it is suggested when using tensorflow as the backend.

I use the ImageDataGenerator with the flow_from_directory method to get the training data. Here the target_size may be specified. The target_size can be either (width, height) or (height, width). The API is defining it as (height, width). 

And now the Problem occurs. If the Input_shape is (width, height, channels) and the target_size is (heigth, width) - as it should be - a ValueError is raised, saying the shapes don´t match (Error when checking input: expected conv2d_1_input to have shape (None, 725, 180, 3) but got array with shape (50, 180, 725, 3)).

If i set the target_size to (width, height), the training is starting, but if i then plot an Image using the Generator, i see that width and height are exchanged and hence the Image is massively transformed.

How to solve this issue? In fchollets great [example](https://gist.github.com/fchollet/0830affa1f7f19fd47b06d4cf89ed44d) the target_size is also set the other way around as in the API.

Any help is appreciated! Thank you.

Here is my code example:





",0,[help wanted] Input shape / target size flow from directory,"[help wanted] Input shape / target size flow from directory Hi,
i´m wondering about the correct input_shape and/or target_size when using fit_generator and flow_from_directory. I´m running keras 2.0.8-tf and tensorflow 1.4 as the backend. My images have the following dimensions:

width: 725 Pixel
height: 180 Pixel
channels: 3

I define the Input_shape as: (width, height, channels) --> (725,180,3), as it is suggested when using tensorflow as the backend.

I use the ImageDataGenerator with the flow_from_directory method to get the training data. Here the target_size may be specified. The target_size can be either (width, height) or (height, width). The API is defining it as (height, width). 

And now the Problem occurs. If the Input_shape is (width, height, channels) and the target_size is (heigth, width) - as it should be - a ValueError is raised, saying the shapes don´t match (Error when checking input: expected conv2d_1_input to have shape (None, 725, 180, 3) but got array with shape (50, 180, 725, 3)).

If i set the target_size to (width, height), the training is starting, but if i then plot an Image using the Generator, i see that width and height are exchanged and hence the Image is massively transformed.

How to solve this issue? In fchollets great [example](https://gist.github.com/fchollet/0830affa1f7f19fd47b06d4cf89ed44d) the target_size is also set the other way around as in the API.

Any help is appreciated! Thank you.

Here is my code example:





"
keras,4094,"While trying to load an earlier trained model and reproduce the outputs, I observed very different outputs for the same input and same model loaded in different Keras versions (namely 1.0.5 and 1.1.0). The model was trained with Keras 1.0.5. I tried to also verify this with a dummy input data fed to the same model loaded in different versions. I checked that the loaded weights match for two versions. Is there any difference between how the layers are defined for v_1.0.5 and v_1.1.0? I noticed at least the default parameters for batch normalization layer is slightly different. For what it's worth, I get similar accuracy when I train two models with same parameters with these two versions (about 1% change), but the difference when loading the model in the other version is huge. I attached the model parameters and summary output.

[model_summary.txt](https://github.com/fchollet/keras/files/533414/model_summary.txt)
",0,"Same model, different outputs for Keras versions 1.0.5 and 1.1.0","Same model, different outputs for Keras versions 1.0.5 and 1.1.0 While trying to load an earlier trained model and reproduce the outputs, I observed very different outputs for the same input and same model loaded in different Keras versions (namely 1.0.5 and 1.1.0). The model was trained with Keras 1.0.5. I tried to also verify this with a dummy input data fed to the same model loaded in different versions. I checked that the loaded weights match for two versions. Is there any difference between how the layers are defined for v_1.0.5 and v_1.1.0? I noticed at least the default parameters for batch normalization layer is slightly different. For what it's worth, I get similar accuracy when I train two models with same parameters with these two versions (about 1% change), but the difference when loading the model in the other version is huge. I attached the model parameters and summary output.

[model_summary.txt](https://github.com/fchollet/keras/files/533414/model_summary.txt)
"
keras,4412,"Hi,
I am trying to make a model of [this](http://tinyclouds.org/colorize/residual_encoder.png). Here is the relevant code:

Here, I want to train the model on a bunch of images in a directory where the input to the model is rgb2gray(image) and op should be close to rgb2uv(image). How do I do that?
Also, have I defined the models correctly?",0,Use images as both x and y,"Use images as both x and y Hi,
I am trying to make a model of [this](http://tinyclouds.org/colorize/residual_encoder.png). Here is the relevant code:

Here, I want to train the model on a bunch of images in a directory where the input to the model is rgb2gray(image) and op should be close to rgb2uv(image). How do I do that?
Also, have I defined the models correctly?"
keras,4186,"An rnn can work on batches having sequences of different length. in the case of return_sequences=True, we should average the losses over batches taking into account not only the batch size but also the different sequence length for the averaging weights. a batch with longer sequences should have a stronger effect on the loss than a batch of same batch size but with shorter sequences.

A related issue is to also take into account the effective length changes due to masking.
",0,Loss averaging over batches does not take into account sequence lengths and masking when return_sequences=True,"Loss averaging over batches does not take into account sequence lengths and masking when return_sequences=True An rnn can work on batches having sequences of different length. in the case of return_sequences=True, we should average the losses over batches taking into account not only the batch size but also the different sequence length for the averaging weights. a batch with longer sequences should have a stronger effect on the loss than a batch of same batch size but with shorter sequences.

A related issue is to also take into account the effective length changes due to masking.
"
keras,2257,"Please make sure that the boxes below are checked before you submit your issue. Thank you!
- [ ] Check that you are up-to-date with the master branch of Keras. You can update with:
  pip install git+git://github.com/fchollet/keras.git --upgrade --no-deps
- [ ] If running on Theano, check that you are up-to-date with the master branch of Theano. You can update with:
  pip install git+git://github.com/Theano/Theano.git --upgrade --no-deps
- [ ] Provide a link to a GitHub Gist of a Python script that can reproduce your issue (or just copy the script here if it is short).
",0,Can anyone explain how to find the derivative of internal states and include them in the keras code ?,"Can anyone explain how to find the derivative of internal states and include them in the keras code ? Please make sure that the boxes below are checked before you submit your issue. Thank you!
- [ ] Check that you are up-to-date with the master branch of Keras. You can update with:
  pip install git+git://github.com/fchollet/keras.git --upgrade --no-deps
- [ ] If running on Theano, check that you are up-to-date with the master branch of Theano. You can update with:
  pip install git+git://github.com/Theano/Theano.git --upgrade --no-deps
- [ ] Provide a link to a GitHub Gist of a Python script that can reproduce your issue (or just copy the script here if it is short).
"
keras,9042,"I am writing a custom loss function but couldn't find the function do perform element-wise multiplication. Any suggestion?
Thanks
",0,How to multiply 2 matrices of the same size element-wise using Keras backend?,"How to multiply 2 matrices of the same size element-wise using Keras backend? I am writing a custom loss function but couldn't find the function do perform element-wise multiplication. Any suggestion?
Thanks
"
keras,11023,"Thank you!

- [ ] Check that you are up-to-date with the master branch of Keras. You can update with:
pip install git+git://github.com/keras-team/keras.git --upgrade --no-deps

- [x] If running on TensorFlow, check that you are up-to-date with the latest version. The installation instructions can be found [here](https://www.tensorflow.org/get_started/os_setup).

- [ ] If running on Theano, check that you are up-to-date with the master branch of Theano. You can update with:
pip install git+git://github.com/Theano/Theano.git --upgrade --no-deps

- [x] Provide a link to a GitHub Gist of a Python script that can reproduce your issue (or just copy the script here if it is short).

I am using Google Colab to train a CNN and then save the entire model to a  file. The code is available here: [CNN-Colab](https://gist.github.com/abhisheksoni27/184c49ca703eb124e1b17eb8dd8af518)

The model gets saved but when I later try to load it back, I get the following error:



The entire Output log is here: [CNN - Colab - Error](https://gist.github.com/abhisheksoni27/732bec240629d2dd721e80130cb2956b)
",0,Cannot load_model,"Cannot load_model Thank you!

- [ ] Check that you are up-to-date with the master branch of Keras. You can update with:
pip install git+git://github.com/keras-team/keras.git --upgrade --no-deps

- [x] If running on TensorFlow, check that you are up-to-date with the latest version. The installation instructions can be found [here](https://www.tensorflow.org/get_started/os_setup).

- [ ] If running on Theano, check that you are up-to-date with the master branch of Theano. You can update with:
pip install git+git://github.com/Theano/Theano.git --upgrade --no-deps

- [x] Provide a link to a GitHub Gist of a Python script that can reproduce your issue (or just copy the script here if it is short).

I am using Google Colab to train a CNN and then save the entire model to a  file. The code is available here: [CNN-Colab](https://gist.github.com/abhisheksoni27/184c49ca703eb124e1b17eb8dd8af518)

The model gets saved but when I later try to load it back, I get the following error:



The entire Output log is here: [CNN - Colab - Error](https://gist.github.com/abhisheksoni27/732bec240629d2dd721e80130cb2956b)
"
keras,10699,"Using  I run into a 404 error on PyPi because there is no arm7 wheel for pyYAML. 



I've tried to just install Keras from this git repo using


but when I try to import the library I have other dependency issues 


Can somebody suggest a fix to install keras with tensorflow backend?",0,cannot install keras on raspberry pi 3 python=3.5.3,"cannot install keras on raspberry pi 3 python=3.5.3 Using  I run into a 404 error on PyPi because there is no arm7 wheel for pyYAML. 



I've tried to just install Keras from this git repo using


but when I try to import the library I have other dependency issues 


Can somebody suggest a fix to install keras with tensorflow backend?"
keras,3048,"Hi, 

Am trying to solve a classification problem using time series/ Sequential data using CNN.
I have created some features using simple mathematical transformation from same data. 
Following are the things I tried.
1. 1D conv net with features as multiple row
2. 2D conv net with single channel and features as rows( Filter nb_rows =1)
3. 2D conv net with channel = number of features(Filter nb_rows =1)

I want to know what would be the correct approach. 

My understanding is as follows:
1. 1D conv net with multiple rows net treats each row as different information. 
2. 2D with 1 channel and multiple rows with filter nb_rows=1 should behave same as 1D conv net with multiple rows
3. 2D conv net with features as different channels help to learn new abstractions.

Appreciate your comments on which approach fits best theoritically.

Please make sure that the boxes below are checked before you submit your issue. Thank you!
- [x] Check that you are up-to-date with the master branch of Keras. You can update with:
  pip install git+git://github.com/fchollet/keras.git --upgrade --no-deps
- [x] If running on Theano, check that you are up-to-date with the master branch of Theano. You can update with:
  pip install git+git://github.com/Theano/Theano.git --upgrade --no-deps
- [x] Provide a link to a GitHub Gist of a Python script that can reproduce your issue (or just copy the script here if it is short).
",0,Classification using 1D and 2D Conv net for time series data with multiple features,"Classification using 1D and 2D Conv net for time series data with multiple features Hi, 

Am trying to solve a classification problem using time series/ Sequential data using CNN.
I have created some features using simple mathematical transformation from same data. 
Following are the things I tried.
1. 1D conv net with features as multiple row
2. 2D conv net with single channel and features as rows( Filter nb_rows =1)
3. 2D conv net with channel = number of features(Filter nb_rows =1)

I want to know what would be the correct approach. 

My understanding is as follows:
1. 1D conv net with multiple rows net treats each row as different information. 
2. 2D with 1 channel and multiple rows with filter nb_rows=1 should behave same as 1D conv net with multiple rows
3. 2D conv net with features as different channels help to learn new abstractions.

Appreciate your comments on which approach fits best theoritically.

Please make sure that the boxes below are checked before you submit your issue. Thank you!
- [x] Check that you are up-to-date with the master branch of Keras. You can update with:
  pip install git+git://github.com/fchollet/keras.git --upgrade --no-deps
- [x] If running on Theano, check that you are up-to-date with the master branch of Theano. You can update with:
  pip install git+git://github.com/Theano/Theano.git --upgrade --no-deps
- [x] Provide a link to a GitHub Gist of a Python script that can reproduce your issue (or just copy the script here if it is short).
"
keras,7206,"Hi,

when im using




print('Loaded %s word vectors.' % len(embeddings_index))`

I get the following error in  for line in f:

",0,UnicodeDecodeError for GloVe,"UnicodeDecodeError for GloVe Hi,

when im using




print('Loaded %s word vectors.' % len(embeddings_index))`

I get the following error in  for line in f:

"
keras,12041,"Hello guys, i would like to know why i cannot use 'categorical_crossentropy'. 
I have 12 class both in training and validation folder.

the structure of the class like this

> database/validation/P1
> database/validation/...
> database/validation/P12

How to make this model should be able to compile with  'categorical_crossentropy'? Thanks

This is my actual code:


The terminal output:
categorical_crossentropycategorical_crossentropycategorical_crossentropysparse_categorical_crossentropy",0,Cannot use categorical_crossentropy,"Cannot use categorical_crossentropy Hello guys, i would like to know why i cannot use 'categorical_crossentropy'. 
I have 12 class both in training and validation folder.

the structure of the class like this

> database/validation/P1
> database/validation/...
> database/validation/P12

How to make this model should be able to compile with  'categorical_crossentropy'? Thanks

This is my actual code:


The terminal output:
categorical_crossentropycategorical_crossentropycategorical_crossentropysparse_categorical_crossentropy"
keras,7352,"I found one small bug for function 'add_ngram' in the file examples/imdb_fasttext.py. When adding n grams, the grams with size of 1, 2,..., n-1 should be all added according to the original paper. However, this function cannot fully achieve this goal due to these two lines: ""for i in range(len(new_list) - ngram_range + 1):""; ""for ngram_value in range(2, ngram_range + 1):"". I think the positions of the two lines should be swapped so that it looks like this: ""for ngram_value in range(2, ngram_range + 1): for i in range(len(new_list) - ngram_value + 1):"". Please check on this issue to make updates. Thank you!",0,One problem for examples/imdb_fasttext.py,"One problem for examples/imdb_fasttext.py I found one small bug for function 'add_ngram' in the file examples/imdb_fasttext.py. When adding n grams, the grams with size of 1, 2,..., n-1 should be all added according to the original paper. However, this function cannot fully achieve this goal due to these two lines: ""for i in range(len(new_list) - ngram_range + 1):""; ""for ngram_value in range(2, ngram_range + 1):"". I think the positions of the two lines should be swapped so that it looks like this: ""for ngram_value in range(2, ngram_range + 1): for i in range(len(new_list) - ngram_value + 1):"". Please check on this issue to make updates. Thank you!"
keras,61,"I think it would be much more clear and easy to have a ""batch size""  parameter separately for Batch Normalization layer.  We can just directly pass the outputs of our convolution or pooling layers to it. The layers as a whole will be more coherent. 
(As an aside,  did anyone have any luck with batch normalization?  I tried many times,  but actually got worse results most of the time.) 
",0,Adding Batch Size as explicit parameter for Batch Normalization layer,"Adding Batch Size as explicit parameter for Batch Normalization layer I think it would be much more clear and easy to have a ""batch size""  parameter separately for Batch Normalization layer.  We can just directly pass the outputs of our convolution or pooling layers to it. The layers as a whole will be more coherent. 
(As an aside,  did anyone have any luck with batch normalization?  I tried many times,  but actually got worse results most of the time.) 
"
keras,2378,"I work at an institute where it is not allowed to run a workstation overnight, hence I had to split the training process into multiple days. I trained a model for 10 epochs which took approximately 1 day, and saved the model + weights using the methods described in keras documentation like this:



and load the model the next day like this:



but when I restarted the training process it initialized to the same training and validation loss that I had got the earlier day at the 1st epoch! It should have started with an accuracy of 60% which was the last best accuracy I got the earlier day, but it doesn't. 

I have also tried to call model.compile() before and after load_weights, as well as leaving it out altogether, but that doesn't work either. 

Please help me in this regard. Thanks in advance. 
",0,Not able to resume training after loading model + weights,"Not able to resume training after loading model + weights I work at an institute where it is not allowed to run a workstation overnight, hence I had to split the training process into multiple days. I trained a model for 10 epochs which took approximately 1 day, and saved the model + weights using the methods described in keras documentation like this:



and load the model the next day like this:



but when I restarted the training process it initialized to the same training and validation loss that I had got the earlier day at the 1st epoch! It should have started with an accuracy of 60% which was the last best accuracy I got the earlier day, but it doesn't. 

I have also tried to call model.compile() before and after load_weights, as well as leaving it out altogether, but that doesn't work either. 

Please help me in this regard. Thanks in advance. 
"
keras,2356,"Using a batch generator and fit_generator raises an error after the final epoch when running with tensorflow backend + gpu on linux and reading images from disk during next(generator) rather than simply taking entries from a pre-populated numpy array. 

Here is a gist of the issue and the log: https://gist.github.com/aachkar-miovision/89daac9ce598dbcb5a698612a3a42684

In my use case, it is not possible to load all the images into a single numpy array as in the cifar10_cnn fit_generator example, so I have to resort to loading images in batches from within my own ImageDataGenerator. I use cv2 because I perform different transforms involving color conversion, etc., so I load the images for a batch using cv2.imread(). 

With small images, I did not see this issue, but my images are on the order of 480x720 pixels and I do see it with images on that size (related to time it takes to read the image?). 

A possible fix would be to also check for AttributeError at https://github.com/fchollet/keras/blob/master/keras/engine/training.py#L399

Is there a reason that this line only checks for ValueError?

Please make sure that the boxes below are checked before you submit your issue. Thank you!
- [yes] Check that you are up-to-date with the master branch of Keras. You can update with:
  pip install git+git://github.com/fchollet/keras.git --upgrade --no-deps
- [using tf] If running on Theano, check that you are up-to-date with the master branch of Theano. You can update with:
  pip install git+git://github.com/Theano/Theano.git --upgrade --no-deps
- [done] Provide a link to a GitHub Gist of a Python script that can reproduce your issue (or just copy the script here if it is short).
",0,Error after final epoch when using fit_generator,"Error after final epoch when using fit_generator Using a batch generator and fit_generator raises an error after the final epoch when running with tensorflow backend + gpu on linux and reading images from disk during next(generator) rather than simply taking entries from a pre-populated numpy array. 

Here is a gist of the issue and the log: https://gist.github.com/aachkar-miovision/89daac9ce598dbcb5a698612a3a42684

In my use case, it is not possible to load all the images into a single numpy array as in the cifar10_cnn fit_generator example, so I have to resort to loading images in batches from within my own ImageDataGenerator. I use cv2 because I perform different transforms involving color conversion, etc., so I load the images for a batch using cv2.imread(). 

With small images, I did not see this issue, but my images are on the order of 480x720 pixels and I do see it with images on that size (related to time it takes to read the image?). 

A possible fix would be to also check for AttributeError at https://github.com/fchollet/keras/blob/master/keras/engine/training.py#L399

Is there a reason that this line only checks for ValueError?

Please make sure that the boxes below are checked before you submit your issue. Thank you!
- [yes] Check that you are up-to-date with the master branch of Keras. You can update with:
  pip install git+git://github.com/fchollet/keras.git --upgrade --no-deps
- [using tf] If running on Theano, check that you are up-to-date with the master branch of Theano. You can update with:
  pip install git+git://github.com/Theano/Theano.git --upgrade --no-deps
- [done] Provide a link to a GitHub Gist of a Python script that can reproduce your issue (or just copy the script here if it is short).
"
keras,6279,"Activations from  (e.g. ) are , so typically we should do something like this:

    model.add(LeakyReLU())

However, there're numbers of people who struggled  which doesn't work and will not raise a warning.  (e.g. https://github.com/fchollet/keras/issues/3816) 

I think maybe we can add some warnings?",0,[Request] Add warnings when LeakyReLU is passed into Activation,"[Request] Add warnings when LeakyReLU is passed into Activation Activations from  (e.g. ) are , so typically we should do something like this:

    model.add(LeakyReLU())

However, there're numbers of people who struggled  which doesn't work and will not raise a warning.  (e.g. https://github.com/fchollet/keras/issues/3816) 

I think maybe we can add some warnings?"
keras,5343,"I would like to experiment with meta-RL, but the central idea involves adding the previous prediction and its quality to the input vector at each timestep.

This means I'll be writing my own train loop and in the middle I will have something like this.



But there are two problems with that.
1. It runs the prediction step twice, once internally for the training step, and once to give me the prediction
2. It gives me a scalar loss, not a vector representing the individual loss for each prediction.

So basically, I would like to have a single function that does everything in one go.


",0,Meta-RL,"Meta-RL I would like to experiment with meta-RL, but the central idea involves adding the previous prediction and its quality to the input vector at each timestep.

This means I'll be writing my own train loop and in the middle I will have something like this.



But there are two problems with that.
1. It runs the prediction step twice, once internally for the training step, and once to give me the prediction
2. It gives me a scalar loss, not a vector representing the individual loss for each prediction.

So basically, I would like to have a single function that does everything in one go.


"
keras,11415,"NameError                                 Traceback (most recent call last)
<ipython-input-46-095f9679650a> in <module>()
      1 # set up transfer learning on pre-trained ImageNet Inception_V3 model - remove fully connected layer and replace
      2 # with softmax for classifying 10 classes
----> 3 incepV3_model = InceptionV3(weights = 'imagenet', include_top = False, input_shape=(299,299,3))
      4 x = incepV3_model.output
      5 x = Flatten(name='flatten')(x)

~/anaconda3/envs/tensorflow-venv/lib/python3.6/site-packages/keras_applications/inception_v3.py in InceptionV3(include_top, weights, input_tensor, input_shape, pooling, classes)
    362         elif pooling == 'max':
    363             x = layers.GlobalMaxPooling2D()(x)
--> 364         x = Flatten(name='flatten')(x)
    365  # Ensure that the model takes into account
    366  # any potential predecessors of .

NameError: name 'Flatten' is not defined
",0,"Name Error-"" name 'Flatten' is not defined "", when i try to fine tune the pretrained model on InceptionV3","Name Error-"" name 'Flatten' is not defined "", when i try to fine tune the pretrained model on InceptionV3 NameError                                 Traceback (most recent call last)
<ipython-input-46-095f9679650a> in <module>()
      1 # set up transfer learning on pre-trained ImageNet Inception_V3 model - remove fully connected layer and replace
      2 # with softmax for classifying 10 classes
----> 3 incepV3_model = InceptionV3(weights = 'imagenet', include_top = False, input_shape=(299,299,3))
      4 x = incepV3_model.output
      5 x = Flatten(name='flatten')(x)

~/anaconda3/envs/tensorflow-venv/lib/python3.6/site-packages/keras_applications/inception_v3.py in InceptionV3(include_top, weights, input_tensor, input_shape, pooling, classes)
    362         elif pooling == 'max':
    363             x = layers.GlobalMaxPooling2D()(x)
--> 364         x = Flatten(name='flatten')(x)
    365  # Ensure that the model takes into account
    366  # any potential predecessors of .

NameError: name 'Flatten' is not defined
"
keras,6619,"Hi, is it possible to specify the initial state for a stateful recurrent layer? For example, I have the following code, will this work? 


The idea is to have the need_initial_flag turned on in the beginning of a sequence and then turned off.
I am not sure if the logic is correct for Keras implementation. Thanks! @farizrahman4u",0,Specifying initial state for Stateful Recurrent layers,"Specifying initial state for Stateful Recurrent layers Hi, is it possible to specify the initial state for a stateful recurrent layer? For example, I have the following code, will this work? 


The idea is to have the need_initial_flag turned on in the beginning of a sequence and then turned off.
I am not sure if the logic is correct for Keras implementation. Thanks! @farizrahman4u"
keras,11000,"I am trying to implement a lstm model which process 100 texts and then concatenate them together to feed into a dense layer. However there comes an error, which is:



The code shows below:


Can anyone gives some help, thanks!
",0,Keras Error when input Dense layer with a reshaped layer,"Keras Error when input Dense layer with a reshaped layer I am trying to implement a lstm model which process 100 texts and then concatenate them together to feed into a dense layer. However there comes an error, which is:



The code shows below:


Can anyone gives some help, thanks!
"
keras,6011,"I have the following function:

    def transpose_dot(vects):
        x, y = vects
        # <x,x> + <y,y> - 2<x,y>
    
        return K.dot(x, K.transpose(y))

When try to evaluate it with  it works 

    x = K.variable(np.array(np_x))
    y = K.variable(np.array(np_x))
    obj = transpose_dot
    objective_output = obj((x, y))
    print('-----------------')
    print (K.eval(objective_output))

result with:

    [[ 1.  1.  1.  2.]
     [ 1.  2.  2.  4.]
     [ 1.  2.  2.  4.]
     [ 2.  4.  4.  8.]

**But**, when try to use it as function for  layer it does not work. 
    
    np_x = [[1, 0], [1, 1], [1, 1], [2, 2]]
    features = np.array([np_x])
    test_input = Input(shape=np.array(np_x).shape)
    dot_layer= Lambda(transpose_dot, output_shape=(4,4))([test_input, test_input])
    x = Model(inputs=test_input, outputs=dot_layer)
    x.predict(features, batch_size=1)


Result with 

    self.fn() if output_subset is None else\
    ValueError: Shape mismatch: x has 2 cols (and 4 rows) but y has 4 rows (and 2 cols)
    Apply node that caused the error: Dot22(Reshape{2}.0, Reshape{2}.0)
    Toposort index: 11
    Inputs types: [TensorType(float32, matrix), TensorType(float32, matrix)]
    Inputs shapes: [(4, 2), (4, 2)]
    Inputs strides: [(8, 4), (8, 4)]
    Inputs values: ['not shown', 'not shown']
    Outputs clients: [[Reshape{4}(Dot22.0, MakeVector{dtype='int64'}.0)]]
 

Any idea what I'm missing here?

P.s 
[Stackoverflow link provide as well ](http://stackoverflow.com/questions/43049984/keras-lambda-function-dot-product-mistmatch)",0,Lambda function and transpose operation,"Lambda function and transpose operation I have the following function:

    def transpose_dot(vects):
        x, y = vects
        # <x,x> + <y,y> - 2<x,y>
    
        return K.dot(x, K.transpose(y))

When try to evaluate it with  it works 

    x = K.variable(np.array(np_x))
    y = K.variable(np.array(np_x))
    obj = transpose_dot
    objective_output = obj((x, y))
    print('-----------------')
    print (K.eval(objective_output))

result with:

    [[ 1.  1.  1.  2.]
     [ 1.  2.  2.  4.]
     [ 1.  2.  2.  4.]
     [ 2.  4.  4.  8.]

**But**, when try to use it as function for  layer it does not work. 
    
    np_x = [[1, 0], [1, 1], [1, 1], [2, 2]]
    features = np.array([np_x])
    test_input = Input(shape=np.array(np_x).shape)
    dot_layer= Lambda(transpose_dot, output_shape=(4,4))([test_input, test_input])
    x = Model(inputs=test_input, outputs=dot_layer)
    x.predict(features, batch_size=1)


Result with 

    self.fn() if output_subset is None else\
    ValueError: Shape mismatch: x has 2 cols (and 4 rows) but y has 4 rows (and 2 cols)
    Apply node that caused the error: Dot22(Reshape{2}.0, Reshape{2}.0)
    Toposort index: 11
    Inputs types: [TensorType(float32, matrix), TensorType(float32, matrix)]
    Inputs shapes: [(4, 2), (4, 2)]
    Inputs strides: [(8, 4), (8, 4)]
    Inputs values: ['not shown', 'not shown']
    Outputs clients: [[Reshape{4}(Dot22.0, MakeVector{dtype='int64'}.0)]]
 

Any idea what I'm missing here?

P.s 
[Stackoverflow link provide as well ](http://stackoverflow.com/questions/43049984/keras-lambda-function-dot-product-mistmatch)"
keras,2091,"I'm new to Keras and having some trouble with shapes, specially when it comes to RNNs and LSTMs.

**I'm running this code:**



The variable predictor_train is a numpy array with 119 inner arrays, each one having 80 different items.

**I'm having this error:**



So far what I found out is that an RNN receives 3D tensor with shape of (batch_size, timesteps, dimension) and when you set input_shape the batch_size is usually omitted, and you should just provide a tuple of (timesteps, dimension). But how exactly do I write that in my code??
",0,"SimpleRNN - Wrong number of dimensions, expected 3, got 2 with shape (119,80)","SimpleRNN - Wrong number of dimensions, expected 3, got 2 with shape (119,80) I'm new to Keras and having some trouble with shapes, specially when it comes to RNNs and LSTMs.

**I'm running this code:**



The variable predictor_train is a numpy array with 119 inner arrays, each one having 80 different items.

**I'm having this error:**



So far what I found out is that an RNN receives 3D tensor with shape of (batch_size, timesteps, dimension) and when you set input_shape the batch_size is usually omitted, and you should just provide a tuple of (timesteps, dimension). But how exactly do I write that in my code??
"
keras,6761,"Would be nice to have a standard way to reference this library on papers.

TensorFlow have it: https://www.tensorflow.org/versions/r0.11/resources/bib

Best regards",0,Standard bibtex entry for reference in academic documents,"Standard bibtex entry for reference in academic documents Would be nice to have a standard way to reference this library on papers.

TensorFlow have it: https://www.tensorflow.org/versions/r0.11/resources/bib

Best regards"
keras,6938,"I want to run the different model in the same time. 
However, I cannot get the correct answer for each running time.
The following is my code.
I generate the MNIST-like data, and build the 2 CNN network to do the training and testing.

I create individual session and graph object. 
Sometime I got the correct answer as the following:

However, I got the error sometimes...

Is there any wrong about my implementation?
or some thing I should notice while doing multi-thread work?",0,The issue to implement multiple CNN models on the individual thread with keras,"The issue to implement multiple CNN models on the individual thread with keras I want to run the different model in the same time. 
However, I cannot get the correct answer for each running time.
The following is my code.
I generate the MNIST-like data, and build the 2 CNN network to do the training and testing.

I create individual session and graph object. 
Sometime I got the correct answer as the following:

However, I got the error sometimes...

Is there any wrong about my implementation?
or some thing I should notice while doing multi-thread work?"
keras,9232,"I am facing an issue with a project I'm currently working on. I am attempting to build a many-to-many model that takes a series of images and classifies them. That part is relatively straight forward. I have a model built using Keras that uses convolutional layers inside the time distributed wrapper that feed into an LSTM and it works fine. The complexity in my current project comes from the fact that this model needs to be converted to CoreML for deployment. I feel I'm up against a wall with this so any help provided would be a life saver.

Like I said previously, my current model is trainable using the time distributed wrapper, but this doesn't seem to be supported by CoreML. I have seen some examples using an LSTM with CoreML where the LSTM states are passed in and out of the model with each item in the sequence. This essentially creates a recurrent network that takes only a single item from the sequence (as well as the previous predictions LSTM states) as an input, rather than the whole sequence at once. That LSTM state loop (for lack of a better term) seems to be the best option as CoreML doesn't support sequential image inputs. My issue then comes from training. How can I train my network properly on sequential data, then convert it to CoreML?

If I remove the time distribution from the non-LSTM layers, the model won't compile because it's missing the extra time dimension. Essentially, the catch here is I can't remove the time distribution wrappers as the model isn't functional without the inclusion of time steps, and I can't convert to CoreML while they are present.

Does anyone have any ideas on how to do this? I hope this question is understandable. It's quite late and I've been working on this for 20+ hours straight so I'm a bit fried at the moment. Thanks in advance for any input, thoughts, or ideas provided. Cheers!

My model:

    image_input = Input(shape=(max_sequence_length, 224, 224, 3))
    
    convolutional_1 = TimeDistributed(Conv2D(64, (3, 3), activation='relu', data_format = 'channels_last'))(image_input)
    pooling_1 = TimeDistributed(MaxPooling2D((2, 2), strides=(1, 1)(convolutional_1)

    convolutional_2 = TimeDistributed(Conv2D(128, (4,4), activation='relu'))(pooling_1)
    pooling_2 = TimeDistributed(MaxPooling2D((2, 2), strides=(2, 2)))(convolutional_2)

    convolutional_3 = TimeDistributed(Conv2D(256, (4,4), activation='relu'))(pooling_2)
    pooling_3 = TimeDistributed(MaxPooling2D((2, 2), strides=(2, 2)))(convolutional_3)

    flatten_1 = TimeDistributed(Flatten())(pooling_3)
    dropout_1 = TimeDistributed(Dropout(0.5))(flatten_1)

    lstm_1 = LSTM(256, return_sequences=True, return_state=False, stateful=False, dropout=0.5)(dropout_1)

    dense_1 = TimeDistributed(Dense(num_classes, activation='sigmoid'))(lstm_1)

    model = Model(inputs = image_input, outputs = dense_1)


I wanted to add that I have seen some posts where it seems people were using time distribution wrappers with CoreML, however when I try to convert my model it raises this error as soon as it hits the first wrapper:

""AttributeError: The layer has never been called and thus has no defined output shape.""

I have modified the conversion script for Keras -> CoreML to handle a 4D input (although I haven't been able to test it to see if it works as expected as I can't convert my model) for the image sequence, so if I can get it to convert with the time distribution layers in place, it would be functional.

[Link to an Apple article discussing RNN's in CoreML](https://developer.apple.com/documentation/coreml/core_ml_api/making_predictions_with_a_sequence_of_inputs)

[Link to a GitHub repo with an implementation of an LSTM RNN](https://github.com/akimach/GestureAI-CoreML-iOS)",0,Use LSTM for recurrent convolutional network without time distributed wrapper,"Use LSTM for recurrent convolutional network without time distributed wrapper I am facing an issue with a project I'm currently working on. I am attempting to build a many-to-many model that takes a series of images and classifies them. That part is relatively straight forward. I have a model built using Keras that uses convolutional layers inside the time distributed wrapper that feed into an LSTM and it works fine. The complexity in my current project comes from the fact that this model needs to be converted to CoreML for deployment. I feel I'm up against a wall with this so any help provided would be a life saver.

Like I said previously, my current model is trainable using the time distributed wrapper, but this doesn't seem to be supported by CoreML. I have seen some examples using an LSTM with CoreML where the LSTM states are passed in and out of the model with each item in the sequence. This essentially creates a recurrent network that takes only a single item from the sequence (as well as the previous predictions LSTM states) as an input, rather than the whole sequence at once. That LSTM state loop (for lack of a better term) seems to be the best option as CoreML doesn't support sequential image inputs. My issue then comes from training. How can I train my network properly on sequential data, then convert it to CoreML?

If I remove the time distribution from the non-LSTM layers, the model won't compile because it's missing the extra time dimension. Essentially, the catch here is I can't remove the time distribution wrappers as the model isn't functional without the inclusion of time steps, and I can't convert to CoreML while they are present.

Does anyone have any ideas on how to do this? I hope this question is understandable. It's quite late and I've been working on this for 20+ hours straight so I'm a bit fried at the moment. Thanks in advance for any input, thoughts, or ideas provided. Cheers!

My model:

    image_input = Input(shape=(max_sequence_length, 224, 224, 3))
    
    convolutional_1 = TimeDistributed(Conv2D(64, (3, 3), activation='relu', data_format = 'channels_last'))(image_input)
    pooling_1 = TimeDistributed(MaxPooling2D((2, 2), strides=(1, 1)(convolutional_1)

    convolutional_2 = TimeDistributed(Conv2D(128, (4,4), activation='relu'))(pooling_1)
    pooling_2 = TimeDistributed(MaxPooling2D((2, 2), strides=(2, 2)))(convolutional_2)

    convolutional_3 = TimeDistributed(Conv2D(256, (4,4), activation='relu'))(pooling_2)
    pooling_3 = TimeDistributed(MaxPooling2D((2, 2), strides=(2, 2)))(convolutional_3)

    flatten_1 = TimeDistributed(Flatten())(pooling_3)
    dropout_1 = TimeDistributed(Dropout(0.5))(flatten_1)

    lstm_1 = LSTM(256, return_sequences=True, return_state=False, stateful=False, dropout=0.5)(dropout_1)

    dense_1 = TimeDistributed(Dense(num_classes, activation='sigmoid'))(lstm_1)

    model = Model(inputs = image_input, outputs = dense_1)


I wanted to add that I have seen some posts where it seems people were using time distribution wrappers with CoreML, however when I try to convert my model it raises this error as soon as it hits the first wrapper:

""AttributeError: The layer has never been called and thus has no defined output shape.""

I have modified the conversion script for Keras -> CoreML to handle a 4D input (although I haven't been able to test it to see if it works as expected as I can't convert my model) for the image sequence, so if I can get it to convert with the time distribution layers in place, it would be functional.

[Link to an Apple article discussing RNN's in CoreML](https://developer.apple.com/documentation/coreml/core_ml_api/making_predictions_with_a_sequence_of_inputs)

[Link to a GitHub repo with an implementation of an LSTM RNN](https://github.com/akimach/GestureAI-CoreML-iOS)"
keras,9394,"Hi,

I just upgrade my Keras to latest version and getting a error related to keras/engine/training on the code that used to work fine beforehand (my previous version was 2.1.0). I am not sure if it's some kind of deprectation or etc.


It's happening on a brach of [Dropout + BB-alpha for detecting adversarial examples](https://github.com/YingzhenLi/Dropout_BBalpha/). Unfortunately, I coudn't share my own version due to disclusure of the project. Thanks for being understanding. 

I found [Add Stateful (Global) Metrics #9200](https://github.com/keras-team/keras/pull/9200/files/1ba271554f3cebf6268d382090e7097f075e5794), but I couldn't track down changes. Any idea how to solve the issue? 

Thanks,
Shek 

- [checked] Check that you are up-to-date with the master branch of Keras. You can update with:
pip install git+git://github.com/keras-team/keras.git --upgrade --no-deps

- [checked] If running on TensorFlow, check that you are up-to-date with the latest version. The installation instructions can be found [here](https://www.tensorflow.org/get_started/os_setup).

",0,AttributeError: 'Model' object has no attribute 'stateful_metric_names',"AttributeError: 'Model' object has no attribute 'stateful_metric_names' Hi,

I just upgrade my Keras to latest version and getting a error related to keras/engine/training on the code that used to work fine beforehand (my previous version was 2.1.0). I am not sure if it's some kind of deprectation or etc.


It's happening on a brach of [Dropout + BB-alpha for detecting adversarial examples](https://github.com/YingzhenLi/Dropout_BBalpha/). Unfortunately, I coudn't share my own version due to disclusure of the project. Thanks for being understanding. 

I found [Add Stateful (Global) Metrics #9200](https://github.com/keras-team/keras/pull/9200/files/1ba271554f3cebf6268d382090e7097f075e5794), but I couldn't track down changes. Any idea how to solve the issue? 

Thanks,
Shek 

- [checked] Check that you are up-to-date with the master branch of Keras. You can update with:
pip install git+git://github.com/keras-team/keras.git --upgrade --no-deps

- [checked] If running on TensorFlow, check that you are up-to-date with the latest version. The installation instructions can be found [here](https://www.tensorflow.org/get_started/os_setup).

"
keras,3538,"For generating different initial random state for neural network I have been doing this based on 
suggestions in the forum:

import numpy as np ;
import datetime ; 
np.random.seed(datetime.datetime.now().microsecond)
from keras import ....

(setting numpy random seed before importing anything from keras). 

Now, assume that I want to train/evaluate a neural network with different initial random states, 
for 10 times, and take average of f-scores. I had to run the file externally for 10 times (e.i., python myprog.py) to make it work. 

Now assume I want to this inside python:

for i in range(10):
         import numpy as np ;
         import datetime ; 
         np.random.seed(datetime.datetime.now().microsecond)
         from keras import ....
         train()
         evaluate()

but I guess this will not do it, because keras is already imported. 
does **del np, del keras**, inside the function works? 
does **reload (np), reload (keras)** works ? 

**What is the correct way of doing this?** 

Thanks in advance. 
",0,How to reset keras random state many times inside python?,"How to reset keras random state many times inside python? For generating different initial random state for neural network I have been doing this based on 
suggestions in the forum:

import numpy as np ;
import datetime ; 
np.random.seed(datetime.datetime.now().microsecond)
from keras import ....

(setting numpy random seed before importing anything from keras). 

Now, assume that I want to train/evaluate a neural network with different initial random states, 
for 10 times, and take average of f-scores. I had to run the file externally for 10 times (e.i., python myprog.py) to make it work. 

Now assume I want to this inside python:

for i in range(10):
         import numpy as np ;
         import datetime ; 
         np.random.seed(datetime.datetime.now().microsecond)
         from keras import ....
         train()
         evaluate()

but I guess this will not do it, because keras is already imported. 
does **del np, del keras**, inside the function works? 
does **reload (np), reload (keras)** works ? 

**What is the correct way of doing this?** 

Thanks in advance. 
"
keras,6852,"Consider the following example:



This seems to cause confusing bugs downstream:
ConcatenateConcatenate

However if we just examine the size of  all seems fine:

",0,Invalid `output_shape` of a model,"Invalid `output_shape` of a model Consider the following example:



This seems to cause confusing bugs downstream:
ConcatenateConcatenate

However if we just examine the size of  all seems fine:

"
keras,13194,"Please make sure that the boxes below are checked before you submit your issue. If your issue is an implementation question, please ask your question on [StackOverflow](http://stackoverflow.com/questions/tagged/keras) or [join the Keras Slack channel](https://keras-slack-autojoin.herokuapp.com/) and ask there instead of filing a GitHub issue.

Thank you!

- [ ] Check that you are up-to-date with the master branch of Keras. You can update with:
pip install git+git://github.com/keras-team/keras.git --upgrade --no-deps

- [ ] If running on TensorFlow, check that you are up-to-date with the latest version. The installation instructions can be found [here](https://www.tensorflow.org/get_started/os_setup).

- [ ] If running on Theano, check that you are up-to-date with the master branch of Theano. You can update with:
pip install git+git://github.com/Theano/Theano.git --upgrade --no-deps

- [ ] Provide a link to a GitHub Gist of a Python script that can reproduce your issue (or just copy the script here if it is short).
",0,"Hi, ","Hi,  Please make sure that the boxes below are checked before you submit your issue. If your issue is an implementation question, please ask your question on [StackOverflow](http://stackoverflow.com/questions/tagged/keras) or [join the Keras Slack channel](https://keras-slack-autojoin.herokuapp.com/) and ask there instead of filing a GitHub issue.

Thank you!

- [ ] Check that you are up-to-date with the master branch of Keras. You can update with:
pip install git+git://github.com/keras-team/keras.git --upgrade --no-deps

- [ ] If running on TensorFlow, check that you are up-to-date with the latest version. The installation instructions can be found [here](https://www.tensorflow.org/get_started/os_setup).

- [ ] If running on Theano, check that you are up-to-date with the master branch of Theano. You can update with:
pip install git+git://github.com/Theano/Theano.git --upgrade --no-deps

- [ ] Provide a link to a GitHub Gist of a Python script that can reproduce your issue (or just copy the script here if it is short).
"
keras,4738,"This is the error and it seems that the layer shape shown in the example is not the same as the 'vgg16_weights.h5' downloaded. 

**Here is the error message:**
Traceback (most recent call last):
<module>
    model.layers[k].set_weights(weights)
  File ""/usr/local/lib/python3.4/dist-packages/keras/engine/topology.py"", line 889, in set_weights
    'provided weight shape ' + str(w.shape))
Exception: Layer weight shape (3, 3, 128, 64) not compatible with provided weight shape (64, 3, 3, 3)

**System configuration:**
Running vgg16 example on ubuntu 14.04, python 3.4
Using TensorFlow backend",0,Vgg16 example: Layer weight shape not compatible with provided weight shape,"Vgg16 example: Layer weight shape not compatible with provided weight shape This is the error and it seems that the layer shape shown in the example is not the same as the 'vgg16_weights.h5' downloaded. 

**Here is the error message:**
Traceback (most recent call last):
<module>
    model.layers[k].set_weights(weights)
  File ""/usr/local/lib/python3.4/dist-packages/keras/engine/topology.py"", line 889, in set_weights
    'provided weight shape ' + str(w.shape))
Exception: Layer weight shape (3, 3, 128, 64) not compatible with provided weight shape (64, 3, 3, 3)

**System configuration:**
Running vgg16 example on ubuntu 14.04, python 3.4
Using TensorFlow backend"
keras,1153,"In a744b60 this highly useful function disappeared. What is the reason, and is there a chance of bringing it back?
",0,print_layer_shapes is gone,"print_layer_shapes is gone In a744b60 this highly useful function disappeared. What is the reason, and is there a chance of bringing it back?
"
keras,3309,"Please make sure that the boxes below are checked before you submit your issue. Thank you!
- [ ] Check that you are up-to-date with the master branch of Keras. You can update with:
  pip install git+git://github.com/fchollet/keras.git --upgrade --no-deps
- [ ] If running on Theano, check that you are up-to-date with the master branch of Theano. You can update with:
  pip install git+git://github.com/Theano/Theano.git --upgrade --no-deps
- [ ] Provide a link to a GitHub Gist of a Python script that can reproduce your issue (or just copy the script here if it is short).
",0,"Hi,","Hi, Please make sure that the boxes below are checked before you submit your issue. Thank you!
- [ ] Check that you are up-to-date with the master branch of Keras. You can update with:
  pip install git+git://github.com/fchollet/keras.git --upgrade --no-deps
- [ ] If running on Theano, check that you are up-to-date with the master branch of Theano. You can update with:
  pip install git+git://github.com/Theano/Theano.git --upgrade --no-deps
- [ ] Provide a link to a GitHub Gist of a Python script that can reproduce your issue (or just copy the script here if it is short).
"
keras,5989,"Description
-----------------------
I used to transfer learning from Resnet50, basicly retrain it with some different top network above it. It worked just fine. But now with Keras 2 it seems that it takes forever. I'm using  and the code is runs on GPU ( I ran  to verify that GPU is actually being used and it does)

I checked both  and  backend. I'm getting huge number like 1151049s per epoch. Which means around 13 days! Where before the update I got around 4000s per epoch

Any ideas what can be the cause for that? 


Thanks!



Some details
-----------------------

1) Ubuntu 16 LTS
2) I'm using fit_generator
3) Tesla K80
4) tested with keras 2.0.1 and 2.0.2
5) Theano - update from branch master
6) Tensorflow - 1.0.1
7) Running python mnist_cnn.py from  takes aroudn 8s per epoch on  backend



- [Done ] Check that you are up-to-date with the master branch of Keras. You can update with:
pip install git+git://github.com/fchollet/keras.git --upgrade --no-deps

- [Done] If running on TensorFlow, check that you are up-to-date with the latest version. The installation instructions can be found [here](https://www.tensorflow.org/get_started/os_setup).

- [Done] If running on Theano, check that you are up-to-date with the master branch of Theano. You can update with:
pip install git+git://github.com/Theano/Theano.git --upgrade --no-deps

- [ ] Provide a link to a GitHub Gist of a Python script that can reproduce your issue (or just copy the script here if it is short).
",0,Keras 2. - resnet training speed,"Keras 2. - resnet training speed Description
-----------------------
I used to transfer learning from Resnet50, basicly retrain it with some different top network above it. It worked just fine. But now with Keras 2 it seems that it takes forever. I'm using  and the code is runs on GPU ( I ran  to verify that GPU is actually being used and it does)

I checked both  and  backend. I'm getting huge number like 1151049s per epoch. Which means around 13 days! Where before the update I got around 4000s per epoch

Any ideas what can be the cause for that? 


Thanks!



Some details
-----------------------

1) Ubuntu 16 LTS
2) I'm using fit_generator
3) Tesla K80
4) tested with keras 2.0.1 and 2.0.2
5) Theano - update from branch master
6) Tensorflow - 1.0.1
7) Running python mnist_cnn.py from  takes aroudn 8s per epoch on  backend



- [Done ] Check that you are up-to-date with the master branch of Keras. You can update with:
pip install git+git://github.com/fchollet/keras.git --upgrade --no-deps

- [Done] If running on TensorFlow, check that you are up-to-date with the latest version. The installation instructions can be found [here](https://www.tensorflow.org/get_started/os_setup).

- [Done] If running on Theano, check that you are up-to-date with the master branch of Theano. You can update with:
pip install git+git://github.com/Theano/Theano.git --upgrade --no-deps

- [ ] Provide a link to a GitHub Gist of a Python script that can reproduce your issue (or just copy the script here if it is short).
"
keras,2845,"Hi people.
I'm new to both Neural networks and Keras.
I'm working on ipython notebook & Windows 8.1 on CPU.

**I'm getting this error while trying to train the network:
AssertionError: AbstractConv2d Theano optimization failed: there is no implementation available supporting the requested options. Did you exclude both ""conv_dnn"" and ""conv_gemm"" from the optimizer? If on GPU, is cuDNN available and does the GPU support it? If on CPU, do you have a BLAS library installed Theano can link against?**

I've looked at similar issues on Theano/Keras github issues page. 
Almost all the solutions pointed towards updating Theano & Keras. 
I've tried updating both of them, and that didn't resolve the issue. 

Is the issue in how we give the images to the training function(model.fit)?
In what format should the 2D grey scale images be given as the training set?
Could someone please help me out in fixing this.

Please find the link to the code & error [here](https://github.com/Vivek-B/Kaggle/blob/master/Digit%20Recognition/Conv-2D%20--%20Keras.ipynb)
",0,Error: AbstractConv2d Theano optimization failed,"Error: AbstractConv2d Theano optimization failed Hi people.
I'm new to both Neural networks and Keras.
I'm working on ipython notebook & Windows 8.1 on CPU.

**I'm getting this error while trying to train the network:
AssertionError: AbstractConv2d Theano optimization failed: there is no implementation available supporting the requested options. Did you exclude both ""conv_dnn"" and ""conv_gemm"" from the optimizer? If on GPU, is cuDNN available and does the GPU support it? If on CPU, do you have a BLAS library installed Theano can link against?**

I've looked at similar issues on Theano/Keras github issues page. 
Almost all the solutions pointed towards updating Theano & Keras. 
I've tried updating both of them, and that didn't resolve the issue. 

Is the issue in how we give the images to the training function(model.fit)?
In what format should the 2D grey scale images be given as the training set?
Could someone please help me out in fixing this.

Please find the link to the code & error [here](https://github.com/Vivek-B/Kaggle/blob/master/Digit%20Recognition/Conv-2D%20--%20Keras.ipynb)
"
keras,1741,"hi, guys:
   why Merge layer doesn't exist 'similarity matrx' merge-mode? i implement a simiarity matrix layer, but i suspect its effectiveness? is my implementation right?


def __init__(self, dim1, dim2, layers, init='uniform',
             W_regularizer=None, activity_regularizer=None,
             W_constraint=None, **kwargs):

    assert len(layers) == 2
    self.layers = layers
    self.dim1 = dim1
    self.dim2 = dim2

    self.init = initializations.get(init)
    self.W_constraint = constraints.get(W_constraint)
    self.W_regularizer = regularizers.get(W_regularizer)
    self.activity_regularizer = regularizers.get(activity_regularizer)

    self.params = []
    self.regularizers = []
    self.constraints = []
    self.updates = []
    for l in self.layers:
        params, regs, consts, updates = l.get_params()
        self.regularizers += regs
        self.updates += updates
        # params and constraints have the same size
        for p, c in zip(params, consts):
            if p not in self.params:
                self.params.append(p)
                self.constraints.append(c)

    self.W = self.init((self.dim1, self.dim2))
    self.params.append(self.W)

    if self.W_regularizer:
        self.W_regularizer.set_param(self.W)
        self.regularizers.append(self.W_regularizer)

    if self.activity_regularizer:
        self.activity_regularizer.set_layer(self)
        self.regularizers.append(self.activity_regularizer)

    super(SimilarityMatrix, self).__init__(**kwargs)

@property
def output_shape(self):
    return [self.layers[0].output_shape[0], 1]

def get_params(self):
    return self.params, self.regularizers, self.constraints, self.updates

def get_input(self, train=False):
    res = []
    for i in range(len(self.layers)):
        o = self.layers[i].get_input(train)
        if not type(o) == list:
            o = [o]
        for output in o:
            if output not in res:
                res.append(output)
    return res

def get_output(self, train):

    s1 = self.layers[0].get_output(train)
    s2 = self.layers[1].get_output(train)

    sim = T.sum(T.dot(s1, self.W)*s2, axis=1)

    return sim.dimshuffle(0,'x')

@property
def input(self):
    return self.get_input()

def supports_masked_input(self):
    return False

def get_output_mask(self, train=None):
    return None

def get_weights(self):
    weights = []
    for l in self.layers:
        weights += l.get_weights()
    return weights

def set_weights(self, weights):
    for i in range(len(self.layers)):
        nb_param = len(self.layers[i].params)
        self.layers[i].set_weights(weights[:nb_param])
        weights = weights[nb_param:]
",0,does my similairty matrix layer implementation right?,"does my similairty matrix layer implementation right? hi, guys:
   why Merge layer doesn't exist 'similarity matrx' merge-mode? i implement a simiarity matrix layer, but i suspect its effectiveness? is my implementation right?


def __init__(self, dim1, dim2, layers, init='uniform',
             W_regularizer=None, activity_regularizer=None,
             W_constraint=None, **kwargs):

    assert len(layers) == 2
    self.layers = layers
    self.dim1 = dim1
    self.dim2 = dim2

    self.init = initializations.get(init)
    self.W_constraint = constraints.get(W_constraint)
    self.W_regularizer = regularizers.get(W_regularizer)
    self.activity_regularizer = regularizers.get(activity_regularizer)

    self.params = []
    self.regularizers = []
    self.constraints = []
    self.updates = []
    for l in self.layers:
        params, regs, consts, updates = l.get_params()
        self.regularizers += regs
        self.updates += updates
        # params and constraints have the same size
        for p, c in zip(params, consts):
            if p not in self.params:
                self.params.append(p)
                self.constraints.append(c)

    self.W = self.init((self.dim1, self.dim2))
    self.params.append(self.W)

    if self.W_regularizer:
        self.W_regularizer.set_param(self.W)
        self.regularizers.append(self.W_regularizer)

    if self.activity_regularizer:
        self.activity_regularizer.set_layer(self)
        self.regularizers.append(self.activity_regularizer)

    super(SimilarityMatrix, self).__init__(**kwargs)

@property
def output_shape(self):
    return [self.layers[0].output_shape[0], 1]

def get_params(self):
    return self.params, self.regularizers, self.constraints, self.updates

def get_input(self, train=False):
    res = []
    for i in range(len(self.layers)):
        o = self.layers[i].get_input(train)
        if not type(o) == list:
            o = [o]
        for output in o:
            if output not in res:
                res.append(output)
    return res

def get_output(self, train):

    s1 = self.layers[0].get_output(train)
    s2 = self.layers[1].get_output(train)

    sim = T.sum(T.dot(s1, self.W)*s2, axis=1)

    return sim.dimshuffle(0,'x')

@property
def input(self):
    return self.get_input()

def supports_masked_input(self):
    return False

def get_output_mask(self, train=None):
    return None

def get_weights(self):
    weights = []
    for l in self.layers:
        weights += l.get_weights()
    return weights

def set_weights(self, weights):
    for i in range(len(self.layers)):
        nb_param = len(self.layers[i].params)
        self.layers[i].set_weights(weights[:nb_param])
        weights = weights[nb_param:]
"
keras,12652,"python 3.7 tensorflow 2.0

Create a model. K.clear_session(). Create a model.

The layer names are not being reset. Can create example if this does not make sense immediately.",0,K.clear_session() does not reset layer naming conventions,"K.clear_session() does not reset layer naming conventions python 3.7 tensorflow 2.0

Create a model. K.clear_session(). Create a model.

The layer names are not being reset. Can create example if this does not make sense immediately."
keras,895,"So I'm not sure why this is coming up. I included the snippet of code and the error message. This seems to be stemming from NumPy, what I'm really confused on is why. When I run it, I often dip way into my memory before the error comes up. I feel like I'm missing something obvious. 

Thanks in Advance!

Error Message:



Snippet of Code Below:


",0,weird memory error,"weird memory error So I'm not sure why this is coming up. I included the snippet of code and the error message. This seems to be stemming from NumPy, what I'm really confused on is why. When I run it, I often dip way into my memory before the error comes up. I feel like I'm missing something obvious. 

Thanks in Advance!

Error Message:



Snippet of Code Below:


"
keras,11885,"Please make sure that the boxes below are checked before you submit your issue.
If your issue is an **implementation question**, please ask your question on [StackOverflow](http://stackoverflow.com/questions/tagged/keras) or [on the Keras Slack channel](https://keras-slack-autojoin.herokuapp.com/) instead of opening a GitHub issue.

Thank you!

- [x] Check that you are up-to-date with the master branch of Keras. You can update with:


- [x] Check that your version of TensorFlow is up-to-date. The installation instructions can be found [here](https://www.tensorflow.org/get_started/os_setup).

- [x] Provide a link to a GitHub Gist of a Python script that can reproduce your issue (or just copy the script here if it is short).
",0,How can I remove / delete specific weights from a neural network layer (Dense layer or convolution layer) in Keras (or Tensorflow)?,"How can I remove / delete specific weights from a neural network layer (Dense layer or convolution layer) in Keras (or Tensorflow)? Please make sure that the boxes below are checked before you submit your issue.
If your issue is an **implementation question**, please ask your question on [StackOverflow](http://stackoverflow.com/questions/tagged/keras) or [on the Keras Slack channel](https://keras-slack-autojoin.herokuapp.com/) instead of opening a GitHub issue.

Thank you!

- [x] Check that you are up-to-date with the master branch of Keras. You can update with:


- [x] Check that your version of TensorFlow is up-to-date. The installation instructions can be found [here](https://www.tensorflow.org/get_started/os_setup).

- [x] Provide a link to a GitHub Gist of a Python script that can reproduce your issue (or just copy the script here if it is short).
"
keras,699,"On the home page of the documentation, there is a claim that ""...Building ... a Neural Turing Machine ... is just as fast"". This doesn't sound implausible, but I didn't think anyone had successfully reproduced the results from the NTM paper; does anyone know if this is referring to a specific NTM implementation, and if so, how well that implementation worked?

from http://keras.io/
",0,Has an NTM actually been built in keras?,"Has an NTM actually been built in keras? On the home page of the documentation, there is a claim that ""...Building ... a Neural Turing Machine ... is just as fast"". This doesn't sound implausible, but I didn't think anyone had successfully reproduced the results from the NTM paper; does anyone know if this is referring to a specific NTM implementation, and if so, how well that implementation worked?

from http://keras.io/
"
keras,5990,"I am new to Keras so maybe I just don't know it better yet but there are a few things I noticed.

For example  changes its return type from a scalar to a list depending on whether the network has just one or many outputs. If I wanted to create something more generic that handles both cases, I am forced to perform type-checking operation on my end. This is why I wanted to ask whether it was possible to change thing like this and simply _always_ return a list.

Something similar, but way worse, happens when one changes the model inputs from single, to multi channel. I noticed that I have to change because in a single-input network I might hand in a  matrix whereas this changes to a  for each input where each list has to contain the data for each channel etc. Imho it would be easier to simply _always_ demand a list because that way you do not run into the issue where you have to re-write your batch generation logic. 

This is not a complaint. I love Keras and I think it's awesome and please forgive me if I am wrong with what I suggest here but if not it would be great to keep such things in mind.

And since I am on it .. I noticed that _a lot_ of questions are getting asked here on github. A lot of questions that would also fit to stackoverflow. I am not sure why but wouldn't it be better to enforce this as some point by close all new issues and ask the people to move their general questions to stackoverflow? I am not affiliated with so but it's simply much easier to get an overview of existing questions over there. 

What do you guys think?",0,Keras API ,"Keras API  I am new to Keras so maybe I just don't know it better yet but there are a few things I noticed.

For example  changes its return type from a scalar to a list depending on whether the network has just one or many outputs. If I wanted to create something more generic that handles both cases, I am forced to perform type-checking operation on my end. This is why I wanted to ask whether it was possible to change thing like this and simply _always_ return a list.

Something similar, but way worse, happens when one changes the model inputs from single, to multi channel. I noticed that I have to change because in a single-input network I might hand in a  matrix whereas this changes to a  for each input where each list has to contain the data for each channel etc. Imho it would be easier to simply _always_ demand a list because that way you do not run into the issue where you have to re-write your batch generation logic. 

This is not a complaint. I love Keras and I think it's awesome and please forgive me if I am wrong with what I suggest here but if not it would be great to keep such things in mind.

And since I am on it .. I noticed that _a lot_ of questions are getting asked here on github. A lot of questions that would also fit to stackoverflow. I am not sure why but wouldn't it be better to enforce this as some point by close all new issues and ask the people to move their general questions to stackoverflow? I am not affiliated with so but it's simply much easier to get an overview of existing questions over there. 

What do you guys think?"
keras,3752,"Here is my network :

model = Sequential()
model.add(Convolution2D(128, 3, 3, border_mode='valid',input_shape=data.shape[-3:]))
model.add(Activation(act))
model.add(Convolution2D(128, 3, 3, border_mode='valid'))
model.add(Activation(act))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Dropout(0.25))
model.add(Convolution2D(256, 3, 3, border_mode='valid'))
model.add(Activation(act))
model.add(Convolution2D(256, 3, 3, border_mode='valid'))
model.add(Activation(act))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Dropout(0.25))
model.add(Convolution2D(512, 3, 3, border_mode='valid'))
model.add(Activation(act))
model.add(MaxPooling2D(pool_size=(2, 2)))
# 

model.add(Flatten())
model.add(Dense(4096, init='normal'))
model.add(Activation(act))
model.add(Dropout(0.5))
model.add(Dense(4096, init='normal'))
model.add(Activation(act))
model.add(Dropout(0.5))

model.add(Dense(classNumber, init='normal'))
model.add(Activation('softmax'))

First I set The act = 'relu' ,  the loss and accuracy does't change.
Then I changed the act = 'tanh' and traing it again,  the loss and acc were normal。
I that a known issue？  
",0,is there a bug with 'relu'?,"is there a bug with 'relu'? Here is my network :

model = Sequential()
model.add(Convolution2D(128, 3, 3, border_mode='valid',input_shape=data.shape[-3:]))
model.add(Activation(act))
model.add(Convolution2D(128, 3, 3, border_mode='valid'))
model.add(Activation(act))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Dropout(0.25))
model.add(Convolution2D(256, 3, 3, border_mode='valid'))
model.add(Activation(act))
model.add(Convolution2D(256, 3, 3, border_mode='valid'))
model.add(Activation(act))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Dropout(0.25))
model.add(Convolution2D(512, 3, 3, border_mode='valid'))
model.add(Activation(act))
model.add(MaxPooling2D(pool_size=(2, 2)))
# 

model.add(Flatten())
model.add(Dense(4096, init='normal'))
model.add(Activation(act))
model.add(Dropout(0.5))
model.add(Dense(4096, init='normal'))
model.add(Activation(act))
model.add(Dropout(0.5))

model.add(Dense(classNumber, init='normal'))
model.add(Activation('softmax'))

First I set The act = 'relu' ,  the loss and accuracy does't change.
Then I changed the act = 'tanh' and traing it again,  the loss and acc were normal。
I that a known issue？  
"
keras,4605,"Hey! Guys, recently I'm working on a project for training LSTM with variable-length sequences, the length of the sequences is 4-12, I'm using one-hot representation, following the example here: [https://github.com/erikbern/rnn-lang-model/blob/master/train_lstm.py](url) , but the problem is that I have to train the network with variable-length sequences,
**First**, I tried to use making layer, but I don't know if my code is correct:(I pasted major part of the code)

training data and validation data: where and mylist_test is list of sequences, is total vocabulary of the letters of sequences, this is to get one-hot representation of the training data and validation data


network: 

then is training:


**second**, I tried to train the network with one pair of data per-time, which is  method and remove the masking layer, to test if my use of masking layer is correct
**third**, I tried to use the 'pad with neutral data ' method following [https://github.com/fchollet/keras/issues/40](url), I paded the sequences with end symbol 

I have not tried group method yet, the performance of my above try are(evaluated with validation loss): 
batch_size =1 method >masking >padding with neutral data

my question is : am I right on using the masking method?  since its performance is much lower than the batch_size =1 method, I think there is some problem with my using of masking layer, can anyone help me ? thanks in advance!",0,Training keras LSTM model with variable-length-sequence: mask or pading or batch_size = 1 or group ?,"Training keras LSTM model with variable-length-sequence: mask or pading or batch_size = 1 or group ? Hey! Guys, recently I'm working on a project for training LSTM with variable-length sequences, the length of the sequences is 4-12, I'm using one-hot representation, following the example here: [https://github.com/erikbern/rnn-lang-model/blob/master/train_lstm.py](url) , but the problem is that I have to train the network with variable-length sequences,
**First**, I tried to use making layer, but I don't know if my code is correct:(I pasted major part of the code)

training data and validation data: where and mylist_test is list of sequences, is total vocabulary of the letters of sequences, this is to get one-hot representation of the training data and validation data


network: 

then is training:


**second**, I tried to train the network with one pair of data per-time, which is  method and remove the masking layer, to test if my use of masking layer is correct
**third**, I tried to use the 'pad with neutral data ' method following [https://github.com/fchollet/keras/issues/40](url), I paded the sequences with end symbol 

I have not tried group method yet, the performance of my above try are(evaluated with validation loss): 
batch_size =1 method >masking >padding with neutral data

my question is : am I right on using the masking method?  since its performance is much lower than the batch_size =1 method, I think there is some problem with my using of masking layer, can anyone help me ? thanks in advance!"
keras,12083,"I need to use the model.fit_generator method with use_multiprocessin=True and workers>1 because I want to parallelize augmentation.

The Keras ImageDataGenerator would be a perfect match for the model.fit_generator. But several people found out, that this causes problems because of lacking thread safety. 

Is there a simple way to make those two work together? Not everyone working with Keras wants (or is able) to write a data generator from scratch.

It would be greatly appreciated if you would give an example in the documentation.
In my case, the images that need to be augmented are in directories. One for each class,

Thanks! ",0,Multiprocessing with model.fit_generator and data augmentation,"Multiprocessing with model.fit_generator and data augmentation I need to use the model.fit_generator method with use_multiprocessin=True and workers>1 because I want to parallelize augmentation.

The Keras ImageDataGenerator would be a perfect match for the model.fit_generator. But several people found out, that this causes problems because of lacking thread safety. 

Is there a simple way to make those two work together? Not everyone working with Keras wants (or is able) to write a data generator from scratch.

It would be greatly appreciated if you would give an example in the documentation.
In my case, the images that need to be augmented are in directories. One for each class,

Thanks! "
keras,4292,"Please make sure that the boxes below are checked before you submit your issue. Thank you!

- [X] Check that you are up-to-date with the master branch of Keras. You can update with:
pip install git+git://github.com/fchollet/keras.git --upgrade --no-deps

- [X] If running on Theano, check that you are up-to-date with the master branch of Theano. You can update with:
pip install git+git://github.com/Theano/Theano.git --upgrade --no-deps

- [X] Provide a link to a GitHub Gist of a Python script that can reproduce your issue (or just copy the script here if it is short).


Hi guys, 
I am wondering if any of you have implemented the SSIM (**structural similarity index**) to be used as objective. It is often used for measuring the similarity between two images **x** and **y**. Its formulation is as follow:
![image](https://cloud.githubusercontent.com/assets/810340/20015848/3a1ed87c-a2a4-11e6-825b-fac27edcb146.png)

I think this must be easy to implement using generic functions of the backends (Theano os TF), but I am not familiarized with them enough.
",0,SSIM as objective,"SSIM as objective Please make sure that the boxes below are checked before you submit your issue. Thank you!

- [X] Check that you are up-to-date with the master branch of Keras. You can update with:
pip install git+git://github.com/fchollet/keras.git --upgrade --no-deps

- [X] If running on Theano, check that you are up-to-date with the master branch of Theano. You can update with:
pip install git+git://github.com/Theano/Theano.git --upgrade --no-deps

- [X] Provide a link to a GitHub Gist of a Python script that can reproduce your issue (or just copy the script here if it is short).


Hi guys, 
I am wondering if any of you have implemented the SSIM (**structural similarity index**) to be used as objective. It is often used for measuring the similarity between two images **x** and **y**. Its formulation is as follow:
![image](https://cloud.githubusercontent.com/assets/810340/20015848/3a1ed87c-a2a4-11e6-825b-fac27edcb146.png)

I think this must be easy to implement using generic functions of the backends (Theano os TF), but I am not familiarized with them enough.
"
keras,1686,"From Document(FAQ), I can get output given specific input(layer 0).
Then I want to know whether to get outpu given intermedia layer input. For example, we have 7 layers totally, if I give the 3th layer's input, can we get the output of the 7th layers use keras?
Thanks.
",0,How to get output given intermedia layer input ?,"How to get output given intermedia layer input ? From Document(FAQ), I can get output given specific input(layer 0).
Then I want to know whether to get outpu given intermedia layer input. For example, we have 7 layers totally, if I give the 3th layer's input, can we get the output of the 7th layers use keras?
Thanks.
"
keras,4097,"Type checking is looking at class names rather than inheritance (for example  - it is also somewhat old Python style).

This is preventing the use of inheritance-based customization patterns. For example:

model

With the pull request https://github.com/fchollet/keras/pull/4069 , the code above would be working.
",0,Use inheritance rather than class name equality,"Use inheritance rather than class name equality Type checking is looking at class names rather than inheritance (for example  - it is also somewhat old Python style).

This is preventing the use of inheritance-based customization patterns. For example:

model

With the pull request https://github.com/fchollet/keras/pull/4069 , the code above would be working.
"
keras,2790,"When I am using the functional API to create the model and try to save it using the following line, It gives me the following error saying:

  File ""trainer.py"", line 48, in create_training_features
  json_string = model.to_json()
  File ""/usr/local/lib/python2.7/site-packages/keras/engine/topology.py"", line 2368, in to_json
  config = self.get_config()
  File ""/usr/local/lib/python2.7/site-packages/keras/engine/topology.py"", line 2163, in get_config
  new_node_index = node_conversion_map[node_key]
  KeyError: 'input_1_ib-0'

Here is the gist: https://gist.github.com/akmahaja/ef406b2087b5c50befc1a479989b1921
",0,An error when saving model,"An error when saving model When I am using the functional API to create the model and try to save it using the following line, It gives me the following error saying:

  File ""trainer.py"", line 48, in create_training_features
  json_string = model.to_json()
  File ""/usr/local/lib/python2.7/site-packages/keras/engine/topology.py"", line 2368, in to_json
  config = self.get_config()
  File ""/usr/local/lib/python2.7/site-packages/keras/engine/topology.py"", line 2163, in get_config
  new_node_index = node_conversion_map[node_key]
  KeyError: 'input_1_ib-0'

Here is the gist: https://gist.github.com/akmahaja/ef406b2087b5c50befc1a479989b1921
"
keras,12741,"**fit_generator**. 

It does work in:  
Python 3.6.5
Keras 2.1.6

But doesn't in:
Python 3.6.8
Keras 2.2.4

Unless related to version of other modules. ",0,fit_generator not working in Keras 2.2.4 with python 3.6.8,"fit_generator not working in Keras 2.2.4 with python 3.6.8 **fit_generator**. 

It does work in:  
Python 3.6.5
Keras 2.1.6

But doesn't in:
Python 3.6.8
Keras 2.2.4

Unless related to version of other modules. "
keras,337,"You forgot to initialize self.monitor with monitor.
The line appears like this:
self.monitor

and the error
AttributeError: 'ModelCheckpoint' object has no attribute 'monitor'

Would appreciate if you could fix that, thanks
",0,ModelCheckpoint - a bug in line 177,"ModelCheckpoint - a bug in line 177 You forgot to initialize self.monitor with monitor.
The line appears like this:
self.monitor

and the error
AttributeError: 'ModelCheckpoint' object has no attribute 'monitor'

Would appreciate if you could fix that, thanks
"
keras,4178,"(**EDIT:** The following issue is only a minimal example of how to produce the error. My actual goal is to use a more complicated model instead of  here.)

When executing the following script a  occurs:



This is the simplest model that produces the error (In my original architecture, I tried to distribute a more complex model). The same issue occurs when replacing the  layer with e.g. , , but not for e.g. . I think the error boils down to the combination of  and any layer (or model) that uses the learning phase.

Maybe there is a conceptual problem with  and the learning phase input?

These issues seem to be somewhat related: #3834, #2609, #3686, #2391

The full stack trace is this:



---

Please make sure that the boxes below are checked before you submit your issue. Thank you!
- [ x] Check that you are up-to-date with the master branch of Keras. You can update with:
  pip install git+git://github.com/fchollet/keras.git --upgrade --no-deps
- [x ] If running on Theano, check that you are up-to-date with the master branch of Theano. You can update with:
  pip install git+git://github.com/Theano/Theano.git --upgrade --no-deps
- [x ] Provide a link to a GitHub Gist of a Python script that can reproduce your issue (or just copy the script here if it is short).
",0,Problem with TimeDistributed() and Learning Phase,"Problem with TimeDistributed() and Learning Phase (**EDIT:** The following issue is only a minimal example of how to produce the error. My actual goal is to use a more complicated model instead of  here.)

When executing the following script a  occurs:



This is the simplest model that produces the error (In my original architecture, I tried to distribute a more complex model). The same issue occurs when replacing the  layer with e.g. , , but not for e.g. . I think the error boils down to the combination of  and any layer (or model) that uses the learning phase.

Maybe there is a conceptual problem with  and the learning phase input?

These issues seem to be somewhat related: #3834, #2609, #3686, #2391

The full stack trace is this:



---

Please make sure that the boxes below are checked before you submit your issue. Thank you!
- [ x] Check that you are up-to-date with the master branch of Keras. You can update with:
  pip install git+git://github.com/fchollet/keras.git --upgrade --no-deps
- [x ] If running on Theano, check that you are up-to-date with the master branch of Theano. You can update with:
  pip install git+git://github.com/Theano/Theano.git --upgrade --no-deps
- [x ] Provide a link to a GitHub Gist of a Python script that can reproduce your issue (or just copy the script here if it is short).
"
keras,312,"Is it possible to stack Merge layers? The model compiles but the train methods breaks with 



",0,Stacking Merge layers?,"Stacking Merge layers? Is it possible to stack Merge layers? The model compiles but the train methods breaks with 



"
keras,10518,"Please make sure that the boxes below are checked before you submit your issue. If your issue is an implementation question, please ask your question on [StackOverflow](http://stackoverflow.com/questions/tagged/keras) or [join the Keras Slack channel](https://keras-slack-autojoin.herokuapp.com/) and ask there instead of filing a GitHub issue.

Thank you!

- [ ] Check that you are up-to-date with the master branch of Keras. You can update with:
pip install git+git://github.com/keras-team/keras.git --upgrade --no-deps

- [ ] If running on TensorFlow, check that you are up-to-date with the latest version. The installation instructions can be found [here](https://www.tensorflow.org/get_started/os_setup).

- [ ] If running on Theano, check that you are up-to-date with the master branch of Theano. You can update with:
pip install git+git://github.com/Theano/Theano.git --upgrade --no-deps

- [ ] Provide a link to a GitHub Gist of a Python script that can reproduce your issue (or just copy the script here if it is short).
",0,"i am working on a regressor problem with 50,000 rows and 20 coloumns ,i want to implement CNN and find MSE on different architecture,  model = Sequential() model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(100, 100, 3))) model.add(Conv2D(32, (3, 3), activation='relu')) model.add(MaxPooling2D(pool_size=(2, 2))) model.add(Dropout(0.25))  model.add(Conv2D(64, (3, 3), activation='relu')) model.add(Conv2D(64, (3, 3), activation='relu')) model.add(MaxPooling2D(pool_size=(2, 2))) model.add(Dropout(0.25))  model.add(Flatten()) model.add(Dense(256, activation='relu')) model.add(Dropout(0.5)) model.add(Dense(10, activation='tanh'))  sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True) model.compile(loss='mean_squared_error', optimizer=sgd)  model.fit(Bx_train, Fx_train, batch_size=32, epochs=10) score = model.evaluate(Bx_test, Fx_test, batch_size=32), what dimension sholud i used as input ","i am working on a regressor problem with 50,000 rows and 20 coloumns ,i want to implement CNN and find MSE on different architecture,  model = Sequential() model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(100, 100, 3))) model.add(Conv2D(32, (3, 3), activation='relu')) model.add(MaxPooling2D(pool_size=(2, 2))) model.add(Dropout(0.25))  model.add(Conv2D(64, (3, 3), activation='relu')) model.add(Conv2D(64, (3, 3), activation='relu')) model.add(MaxPooling2D(pool_size=(2, 2))) model.add(Dropout(0.25))  model.add(Flatten()) model.add(Dense(256, activation='relu')) model.add(Dropout(0.5)) model.add(Dense(10, activation='tanh'))  sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True) model.compile(loss='mean_squared_error', optimizer=sgd)  model.fit(Bx_train, Fx_train, batch_size=32, epochs=10) score = model.evaluate(Bx_test, Fx_test, batch_size=32), what dimension sholud i used as input  Please make sure that the boxes below are checked before you submit your issue. If your issue is an implementation question, please ask your question on [StackOverflow](http://stackoverflow.com/questions/tagged/keras) or [join the Keras Slack channel](https://keras-slack-autojoin.herokuapp.com/) and ask there instead of filing a GitHub issue.

Thank you!

- [ ] Check that you are up-to-date with the master branch of Keras. You can update with:
pip install git+git://github.com/keras-team/keras.git --upgrade --no-deps

- [ ] If running on TensorFlow, check that you are up-to-date with the latest version. The installation instructions can be found [here](https://www.tensorflow.org/get_started/os_setup).

- [ ] If running on Theano, check that you are up-to-date with the master branch of Theano. You can update with:
pip install git+git://github.com/Theano/Theano.git --upgrade --no-deps

- [ ] Provide a link to a GitHub Gist of a Python script that can reproduce your issue (or just copy the script here if it is short).
"
keras,676,"Loading an Image using the  results in an error.


",0,Python 3 compatibility problem with Image loading,"Python 3 compatibility problem with Image loading Loading an Image using the  results in an error.


"
keras,1584,"Hi,

Shouldn't theano.scan() be used for things like the get_output() in Graph containers and such? Would theano be able to (GPU) optimize such functions if scan() was used instead of normal python dictionary iterations?
",0,theano.scan() unused in keras source?,"theano.scan() unused in keras source? Hi,

Shouldn't theano.scan() be used for things like the get_output() in Graph containers and such? Would theano be able to (GPU) optimize such functions if scan() was used instead of normal python dictionary iterations?
"
keras,2711,"I have a model that I am trying to train where the loss does not go down. I have a custom image set that I am using. These images are 106 x 106 px (black and white) and I have two (2) classes,  Bargraph or Gels. These two classes are very different. I have run the Cifar10 dataset and it did reduce the loss, but I am very confused as to why my model will always predict only one class for everything. 

Xtrain is a numpy array of images (which are numpy arrays), Ytrain is a numpy array of arrays ([0,1] or [1,0]) the shapes look like this: 



Here is my model:



Right now I am just doing very small training sets (I tried doing 1000 examples as well, with similar results).

I have also tried RMS and SDG with large and small learning rates.

What else can I try ?
- [X] Check that you are up-to-date with the master branch of Keras. You can update with:
  pip install git+git://github.com/fchollet/keras.git --upgrade --no-deps
- [X] If running on Theano, check that you are up-to-date with the master branch of Theano. You can update with:
  pip install git+git://github.com/Theano/Theano.git --upgrade --no-deps
",0,Loss not changing when training,"Loss not changing when training I have a model that I am trying to train where the loss does not go down. I have a custom image set that I am using. These images are 106 x 106 px (black and white) and I have two (2) classes,  Bargraph or Gels. These two classes are very different. I have run the Cifar10 dataset and it did reduce the loss, but I am very confused as to why my model will always predict only one class for everything. 

Xtrain is a numpy array of images (which are numpy arrays), Ytrain is a numpy array of arrays ([0,1] or [1,0]) the shapes look like this: 



Here is my model:



Right now I am just doing very small training sets (I tried doing 1000 examples as well, with similar results).

I have also tried RMS and SDG with large and small learning rates.

What else can I try ?
- [X] Check that you are up-to-date with the master branch of Keras. You can update with:
  pip install git+git://github.com/fchollet/keras.git --upgrade --no-deps
- [X] If running on Theano, check that you are up-to-date with the master branch of Theano. You can update with:
  pip install git+git://github.com/Theano/Theano.git --upgrade --no-deps
"
keras,9824,"from numpy import array
from pickle import load
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
from keras.utils import to_categorical
#from keras.utils import plot_model
from keras.models import Model
from keras.layers import Input
from keras.layers import Dense
from keras.layers import LSTM
from keras.layers import Embedding
from keras.layers import Dropout
from keras.layers.merge import add
from keras.callbacks import ModelCheckpoint
import itertools

# load doc into memory
def load_doc(filename):
	# open the file as read only
	file = open(filename, 'r')
	# read all text
	text = file.read()
	# close the file
	file.close()
	return text

# load a pre-defined list of photo identifiers
def load_set(filename):
	doc = load_doc(filename)
	dataset = list()
	# process line by line
	for line in doc.split('\n'):
		# skip empty lines
		if len(line) < 1:
			continue
		# get the image identifier
		identifier = line.split('.')[0]
		dataset.append(identifier)
	return set(dataset)

# load clean descriptions into memory
def load_clean_descriptions(filename, dataset):
	# load document
	doc = load_doc(filename)
	descriptions = dict()
	for line in doc.split('\n'):
		# split line by white space
		tokens = line.split()
		# split id from description
		image_id, image_desc = tokens[0], tokens[1:]
		# skip images not in the set
		if image_id in dataset:
			# create list
			if image_id not in descriptions:
				descriptions[image_id] = list()
			# wrap description in tokens
			desc = 'startseq ' + ' '.join(image_desc) + ' endseq'
			# store
			descriptions[image_id].append(desc)
	return descriptions

# load photo features
def load_photo_features(filename, dataset):
	# load all features
	all_features = load(open(filename, 'rb'))
	# filter features
	features = {k: all_features[k] for k in dataset}
	return features

# covert a dictionary of clean descriptions to a list of descriptions
def to_lines(descriptions):
	all_desc = list()
	for key in descriptions.keys():
		[all_desc.append(d) for d in descriptions[key]]
	return all_desc

# fit a tokenizer given caption descriptions
def create_tokenizer(descriptions):
	lines = to_lines(descriptions)
	tokenizer = Tokenizer()
	tokenizer.fit_on_texts(lines)
	return tokenizer

# calculate the length of the description with the most words
def max_length(descriptions):
	lines = to_lines(descriptions)
	return max(len(d.split()) for d in lines)

# create sequences of images, input sequences and output words for an image
def create_sequences(tokenizer, max_length, descriptions, photos):
	X1, X2, y = list(), list(), list()
	# walk through each image identifier
	for key, desc_list in descriptions.items():
		# walk through each description for the image
		for desc in desc_list:
			# encode the sequence
			seq = tokenizer.texts_to_sequences([desc])[0]
			# split one sequence into multiple X,y pairs
			for i in range(1, len(seq)):
				# split into input and output pair
				in_seq, out_seq = seq[:i], seq[i]
				# pad input sequence
				in_seq = pad_sequences([in_seq], maxlen=max_length)[0]
				# encode output sequence
				out_seq = to_categorical([out_seq], num_classes=vocab_size)
				# store

				if key in photos.items():
					X1.append(photos[key][0])

				X2.append(in_seq)
				y.append(out_seq)
	return array(X1), array(X2), array(y)

# define the captioning model
def define_model(vocab_size, max_length):
	# feature extractor model
	 inputs1 = Input(shape=(4096,))
	 fe1 = Dropout(0.5)(inputs1)
	 fe2 = Dense(256, activation='relu')(fe1)
	# sequence model
	 inputs2 = Input(shape=(max_length,))
	 se1 = Embedding(vocab_size, 256, mask_zero=True)(inputs2)
	 se2 = Dropout(0.5)(se1)
	 se3 = LSTM(256)(se2)
	# decoder model
	 decoder1 = add([fe2, se3])
	 decoder2 = Dense(256, activation='relu')(decoder1)
	 outputs = Dense(vocab_size, activation='softmax')(decoder2)
	# tie it together [image, seq] [word]
	 model = Model(inputs=[inputs1, inputs2], outputs=outputs)
	 model.compile(loss='categorical_crossentropy', optimizer='adam')
	# summarize model
	 print(model.summary())
	# plot_model(model, to_file='model.png', show_shapes=True)
	 return model

# train dataset

# load training dataset (6K)
filename = 'Flickr8k_text/Flickr_8k.trainImages.txt'
train = load_set(filename)
print('Dataset: %d' % len(train))
# descriptions
train_descriptions = load_clean_descriptions('descriptions.txt', train)
print('Descriptions: train=%d' % len(train_descriptions))
# photo features
train_features = load_photo_features('features.pkl', train)
print('Photos: train=%d' % len(train_features))
# prepare tokenizer
tokenizer = create_tokenizer(train_descriptions)
vocab_size = len(tokenizer.word_index) + 1
print('Vocabulary Size: %d' % vocab_size)
# determine the maximum sequence length
max_length = max_length(train_descriptions)
print('Description Length: %d' % max_length)
#print(train_descriptions.type())
#print(train_features.type())
interdesc = dict(itertools.islice(train_descriptions.items(),1000))
interfeatures = dict(itertools.islice(train_features.items(),1000))
#print(train_descriptions)
#print(interfeatures)
# prepare sequences
X1train, X2train, ytrain = create_sequences(tokenizer, max_length, interdesc, interfeatures)

# dev dataset

# load test set
filename = 'Flickr8k_text/Flickr_8k.devImages.txt'
test = load_set(filename)
print('Dataset: %d' % len(test))
# descriptions
test_descriptions = load_clean_descriptions('descriptions.txt', test)
print('Descriptions: test=%d' % len(test_descriptions))
# photo features
test_features = load_photo_features('features.pkl', test)
print('Photos: test=%d' % len(test_features))
# prepare sequences
X1test, X2test, ytest = create_sequences(tokenizer, max_length, test_descriptions, test_features)

# fit model

# define the model
model = define_model(vocab_size, max_length)
# define checkpoint callback
filepath = 'model-ep{epoch:03d}-loss{loss:.3f}-val_loss{val_loss:.3f}.h5'
checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')
# fit model
model.fit([X1train, X2train], ytrain, epochs=20, verbose=2, callbacks=[checkpoint], validation_data=([X1test, X2test], ytest))",0,"ValueError: Error when checking input: expected input_1 to have shape (None, 4096) but got array with shape (0, 1)","ValueError: Error when checking input: expected input_1 to have shape (None, 4096) but got array with shape (0, 1) from numpy import array
from pickle import load
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
from keras.utils import to_categorical
#from keras.utils import plot_model
from keras.models import Model
from keras.layers import Input
from keras.layers import Dense
from keras.layers import LSTM
from keras.layers import Embedding
from keras.layers import Dropout
from keras.layers.merge import add
from keras.callbacks import ModelCheckpoint
import itertools

# load doc into memory
def load_doc(filename):
	# open the file as read only
	file = open(filename, 'r')
	# read all text
	text = file.read()
	# close the file
	file.close()
	return text

# load a pre-defined list of photo identifiers
def load_set(filename):
	doc = load_doc(filename)
	dataset = list()
	# process line by line
	for line in doc.split('\n'):
		# skip empty lines
		if len(line) < 1:
			continue
		# get the image identifier
		identifier = line.split('.')[0]
		dataset.append(identifier)
	return set(dataset)

# load clean descriptions into memory
def load_clean_descriptions(filename, dataset):
	# load document
	doc = load_doc(filename)
	descriptions = dict()
	for line in doc.split('\n'):
		# split line by white space
		tokens = line.split()
		# split id from description
		image_id, image_desc = tokens[0], tokens[1:]
		# skip images not in the set
		if image_id in dataset:
			# create list
			if image_id not in descriptions:
				descriptions[image_id] = list()
			# wrap description in tokens
			desc = 'startseq ' + ' '.join(image_desc) + ' endseq'
			# store
			descriptions[image_id].append(desc)
	return descriptions

# load photo features
def load_photo_features(filename, dataset):
	# load all features
	all_features = load(open(filename, 'rb'))
	# filter features
	features = {k: all_features[k] for k in dataset}
	return features

# covert a dictionary of clean descriptions to a list of descriptions
def to_lines(descriptions):
	all_desc = list()
	for key in descriptions.keys():
		[all_desc.append(d) for d in descriptions[key]]
	return all_desc

# fit a tokenizer given caption descriptions
def create_tokenizer(descriptions):
	lines = to_lines(descriptions)
	tokenizer = Tokenizer()
	tokenizer.fit_on_texts(lines)
	return tokenizer

# calculate the length of the description with the most words
def max_length(descriptions):
	lines = to_lines(descriptions)
	return max(len(d.split()) for d in lines)

# create sequences of images, input sequences and output words for an image
def create_sequences(tokenizer, max_length, descriptions, photos):
	X1, X2, y = list(), list(), list()
	# walk through each image identifier
	for key, desc_list in descriptions.items():
		# walk through each description for the image
		for desc in desc_list:
			# encode the sequence
			seq = tokenizer.texts_to_sequences([desc])[0]
			# split one sequence into multiple X,y pairs
			for i in range(1, len(seq)):
				# split into input and output pair
				in_seq, out_seq = seq[:i], seq[i]
				# pad input sequence
				in_seq = pad_sequences([in_seq], maxlen=max_length)[0]
				# encode output sequence
				out_seq = to_categorical([out_seq], num_classes=vocab_size)
				# store

				if key in photos.items():
					X1.append(photos[key][0])

				X2.append(in_seq)
				y.append(out_seq)
	return array(X1), array(X2), array(y)

# define the captioning model
def define_model(vocab_size, max_length):
	# feature extractor model
	 inputs1 = Input(shape=(4096,))
	 fe1 = Dropout(0.5)(inputs1)
	 fe2 = Dense(256, activation='relu')(fe1)
	# sequence model
	 inputs2 = Input(shape=(max_length,))
	 se1 = Embedding(vocab_size, 256, mask_zero=True)(inputs2)
	 se2 = Dropout(0.5)(se1)
	 se3 = LSTM(256)(se2)
	# decoder model
	 decoder1 = add([fe2, se3])
	 decoder2 = Dense(256, activation='relu')(decoder1)
	 outputs = Dense(vocab_size, activation='softmax')(decoder2)
	# tie it together [image, seq] [word]
	 model = Model(inputs=[inputs1, inputs2], outputs=outputs)
	 model.compile(loss='categorical_crossentropy', optimizer='adam')
	# summarize model
	 print(model.summary())
	# plot_model(model, to_file='model.png', show_shapes=True)
	 return model

# train dataset

# load training dataset (6K)
filename = 'Flickr8k_text/Flickr_8k.trainImages.txt'
train = load_set(filename)
print('Dataset: %d' % len(train))
# descriptions
train_descriptions = load_clean_descriptions('descriptions.txt', train)
print('Descriptions: train=%d' % len(train_descriptions))
# photo features
train_features = load_photo_features('features.pkl', train)
print('Photos: train=%d' % len(train_features))
# prepare tokenizer
tokenizer = create_tokenizer(train_descriptions)
vocab_size = len(tokenizer.word_index) + 1
print('Vocabulary Size: %d' % vocab_size)
# determine the maximum sequence length
max_length = max_length(train_descriptions)
print('Description Length: %d' % max_length)
#print(train_descriptions.type())
#print(train_features.type())
interdesc = dict(itertools.islice(train_descriptions.items(),1000))
interfeatures = dict(itertools.islice(train_features.items(),1000))
#print(train_descriptions)
#print(interfeatures)
# prepare sequences
X1train, X2train, ytrain = create_sequences(tokenizer, max_length, interdesc, interfeatures)

# dev dataset

# load test set
filename = 'Flickr8k_text/Flickr_8k.devImages.txt'
test = load_set(filename)
print('Dataset: %d' % len(test))
# descriptions
test_descriptions = load_clean_descriptions('descriptions.txt', test)
print('Descriptions: test=%d' % len(test_descriptions))
# photo features
test_features = load_photo_features('features.pkl', test)
print('Photos: test=%d' % len(test_features))
# prepare sequences
X1test, X2test, ytest = create_sequences(tokenizer, max_length, test_descriptions, test_features)

# fit model

# define the model
model = define_model(vocab_size, max_length)
# define checkpoint callback
filepath = 'model-ep{epoch:03d}-loss{loss:.3f}-val_loss{val_loss:.3f}.h5'
checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')
# fit model
model.fit([X1train, X2train], ytrain, epochs=20, verbose=2, callbacks=[checkpoint], validation_data=([X1test, X2test], ytest))"
keras,3880,"I am studying Chinese stock market with  binary classification with GRU, the traing samples is balance and the output is fine with class 1s and 0s, however, when i want to predict the real, almost all  return is all 0s.
 It is very strange the i have one mode can predict 2 classes with oneday, the predict output is similar to training data, which is fine.
when this mode is trained with more epoches, I try to use it to predict the same day data, it's predict data is all like 0.2-0.3, which means I get all 0s?
I prepare all real data with same code so it is hard to explain one day is fine and the others are wrong
Any idea?thanks
",0,binary classification get all 0s with GRU,"binary classification get all 0s with GRU I am studying Chinese stock market with  binary classification with GRU, the traing samples is balance and the output is fine with class 1s and 0s, however, when i want to predict the real, almost all  return is all 0s.
 It is very strange the i have one mode can predict 2 classes with oneday, the predict output is similar to training data, which is fine.
when this mode is trained with more epoches, I try to use it to predict the same day data, it's predict data is all like 0.2-0.3, which means I get all 0s?
I prepare all real data with same code so it is hard to explain one day is fine and the others are wrong
Any idea?thanks
"
keras,13190,,0,.,
keras,5963,"The siamese network example for mnist (https://github.com/fchollet/keras/blob/master/examples/mnist_siamese_graph.py) uses the following function to compute accuracy



This seems flawed to me. For example:


Surely the order of the predictions is important? In the example above, all of the predictions are different from the labels, and yet the compute_accuracy returns 100%. 

Am I missing something obvious, or is this a mistake.




- [X] Check that you are up-to-date with the master branch of Keras. You can update with:
pip install git+git://github.com/fchollet/keras.git --upgrade --no-deps

- [x] If running on TensorFlow, check that you are up-to-date with the latest version. The installation instructions can be found [here](https://www.tensorflow.org/get_started/os_setup).

- [X] If running on Theano, check that you are up-to-date with the master branch of Theano. You can update with:
pip install git+git://github.com/Theano/Theano.git --upgrade --no-deps

- [X] Provide a link to a GitHub Gist of a Python script that can reproduce your issue (or just copy the script here if it is short).
",0,Bug in Siamese Example Accuracy Calculation,"Bug in Siamese Example Accuracy Calculation The siamese network example for mnist (https://github.com/fchollet/keras/blob/master/examples/mnist_siamese_graph.py) uses the following function to compute accuracy



This seems flawed to me. For example:


Surely the order of the predictions is important? In the example above, all of the predictions are different from the labels, and yet the compute_accuracy returns 100%. 

Am I missing something obvious, or is this a mistake.




- [X] Check that you are up-to-date with the master branch of Keras. You can update with:
pip install git+git://github.com/fchollet/keras.git --upgrade --no-deps

- [x] If running on TensorFlow, check that you are up-to-date with the latest version. The installation instructions can be found [here](https://www.tensorflow.org/get_started/os_setup).

- [X] If running on Theano, check that you are up-to-date with the master branch of Theano. You can update with:
pip install git+git://github.com/Theano/Theano.git --upgrade --no-deps

- [X] Provide a link to a GitHub Gist of a Python script that can reproduce your issue (or just copy the script here if it is short).
"
keras,6912,"Hi,
I wonder if there is a way to specify different learning rates to different layers of a CNN.
thanks
",0,different learning rates to different layers of a CNN.,"different learning rates to different layers of a CNN. Hi,
I wonder if there is a way to specify different learning rates to different layers of a CNN.
thanks
"
keras,1093,"Hello,

When I try to run the  example, the model fails to build, and it throws the following error:



I'm using the latest Keras from , as well as the latest Theano from github, as was suggested many times. The error obviously occurs even when I try to create a backwards LSTM layer in my own models.

Any clue why this might be?
",0,LSTM `go_backwards`,"LSTM `go_backwards` Hello,

When I try to run the  example, the model fails to build, and it throws the following error:



I'm using the latest Keras from , as well as the latest Theano from github, as was suggested many times. The error obviously occurs even when I try to create a backwards LSTM layer in my own models.

Any clue why this might be?
"
keras,9054,"Question regarding this example file: https://github.com/keras-team/keras/blob/master/examples/mnist_siamese.py

In the contrastive loss layer, the example file is using a margin for setting up the training process.
I would assume that the margin would be the same parameter as the threshold for the accuracy of the model. (here 0.5 in the compute_acc function). What is the reason for using a different threshold?

",0,Siamese network forward pass / test accuracy ,"Siamese network forward pass / test accuracy  Question regarding this example file: https://github.com/keras-team/keras/blob/master/examples/mnist_siamese.py

In the contrastive loss layer, the example file is using a margin for setting up the training process.
I would assume that the margin would be the same parameter as the threshold for the accuracy of the model. (here 0.5 in the compute_acc function). What is the reason for using a different threshold?

"
keras,2946,"

I upgraded the keras to 1.0.3 and since then it has started giving me this error. Earlier was 0.3.3, however it is running fine on another system with keras 1.0.3 version. Unable to figure out the issue. 
",0,Error saving the model keras.__version__=1.0.3,"Error saving the model keras.__version__=1.0.3 

I upgraded the keras to 1.0.3 and since then it has started giving me this error. Earlier was 0.3.3, however it is running fine on another system with keras 1.0.3 version. Unable to figure out the issue. 
"
keras,2160,"I want to train a model with parts of parameters fixed. When I tried to load saved weights from the HDF5 file, it fails. It seems the saveweight function does not save none-trainable weights, while loadweight function requires them. See example code below:

<pre><code>
from keras.models import Sequential
from keras.layers.core import Merge
from keras.layers.convolutional import Convolution2D


model_range_5 = Sequential()
model_range_5.add(Convolution2D(128,5,5,dim_ordering='tf',activation='relu',border_mode='same',input_shape=(256,200,3),trainable=False))

model_range_4 = Sequential()
model_range_4.add(Convolution2D(128,5,5,dim_ordering='tf',activation='relu',border_mode='same',input_shape=(256,200,3)))


test_model = Sequential([Merge([model_range_5,model_range_4])])

test_model.compile(optimizer='SGD',loss='mse')
test_model.save_weights('weight.model',overwrite=True)
test_model.load_weights('weight.model')
</code></pre>
",0,Bugs in save and load weights,"Bugs in save and load weights I want to train a model with parts of parameters fixed. When I tried to load saved weights from the HDF5 file, it fails. It seems the saveweight function does not save none-trainable weights, while loadweight function requires them. See example code below:

<pre><code>
from keras.models import Sequential
from keras.layers.core import Merge
from keras.layers.convolutional import Convolution2D


model_range_5 = Sequential()
model_range_5.add(Convolution2D(128,5,5,dim_ordering='tf',activation='relu',border_mode='same',input_shape=(256,200,3),trainable=False))

model_range_4 = Sequential()
model_range_4.add(Convolution2D(128,5,5,dim_ordering='tf',activation='relu',border_mode='same',input_shape=(256,200,3)))


test_model = Sequential([Merge([model_range_5,model_range_4])])

test_model.compile(optimizer='SGD',loss='mse')
test_model.save_weights('weight.model',overwrite=True)
test_model.load_weights('weight.model')
</code></pre>
"
keras,5176,From the documentation in Keras it seems that the scikit-learn API is (only) supported on Sequential models. Does it then make any sense to use it as well when using the Keras Functional API?,0,Scikit wrapper on Functional API as well?,Scikit wrapper on Functional API as well? From the documentation in Keras it seems that the scikit-learn API is (only) supported on Sequential models. Does it then make any sense to use it as well when using the Keras Functional API?
keras,2905,"Hi, Guys, 

I have come across a problem that the training dataset is very large so I split it into many subset and then load each subset for training. In each subset, I split it into batches for training. Here, after compiling, can I use the function **graph.fit()** many times to update the weights for different subsets? 
",0,Rerun graph.fit many times,"Rerun graph.fit many times Hi, Guys, 

I have come across a problem that the training dataset is very large so I split it into many subset and then load each subset for training. In each subset, I split it into batches for training. Here, after compiling, can I use the function **graph.fit()** many times to update the weights for different subsets? 
"
keras,2521,"A minor issue, admittedly, but is there a builtin way to set the call order of a model's callbacks (,  and so on)? I assume the callbacks follow the input list's ordering, but what about the defaults. Specifically ProgbarLogger, since it seems to be called last, the output gets wonky in case any other callback also prints to stdout during .
",0,"Setting callbacks order, including default callbacks","Setting callbacks order, including default callbacks A minor issue, admittedly, but is there a builtin way to set the call order of a model's callbacks (,  and so on)? I assume the callbacks follow the input list's ordering, but what about the defaults. Specifically ProgbarLogger, since it seems to be called last, the output gets wonky in case any other callback also prints to stdout during .
"
keras,2560,"Users of  like  have a race condition when the generator exits after generating a number of samples equal to or slightly greater than . The function will occasionally fetch  from the queue instead of the final elements.
",0,Race condition in fit_generator() when generator exits,"Race condition in fit_generator() when generator exits Users of  like  have a race condition when the generator exits after generating a number of samples equal to or slightly greater than . The function will occasionally fetch  from the queue instead of the final elements.
"
keras,3425,"Hi! My **recurrent neural network (LSTM, resp. GRU)** behaves in a way I cannot explain. The training starts and it trains well (the results look quite good) when **suddenly accuracy drops (and loss rapidly increases)** - both training and testing metrics. Sometimes the net just _goes crazy_ and returns random outputs and sometimes (as in the last of three given examples) it starts to return _same output to all the inputs_. 

![image](https://cloud.githubusercontent.com/assets/8523511/17508785/f94aaf50-5e16-11e6-92cd-4d842b4c0e9c.png)

Do you have **any explanation for this behavior**? Any opinion is welcome. Please, see the task description and the figures below.

**The task:** From a word predict its word2vec vector
**The input:** We have an own word2vec model (normalized) and we feed the network with a word (letter by letter). We pad the words (see the example below).
**Example:** We have a word _football_ and we want to predict its word2vec vector which is 100 dimensions wide. Then the input is . 

Three examples of the behavior:

**Single layer LSTM**



![image](https://cloud.githubusercontent.com/assets/8523511/17507845/43eecbfe-5e12-11e6-8ae3-179ac10d82b7.png)

**Single layer GRU**



![image](https://cloud.githubusercontent.com/assets/8523511/17507923/8ec75934-5e12-11e6-891b-a2d2f2c10e39.png)

**Double layer LSTM**



![image](https://cloud.githubusercontent.com/assets/8523511/17507947/a29ad68e-5e12-11e6-9355-bde7711df71f.png)

We have also _experienced this kind of behavior in another project before_ which used similar architecture but its objective and data were different. Thus the reason should not be hidden in the data or in the particular objective but rather in the architecture.
",0,Sudden accuracy drop when training LSTM or GRU,"Sudden accuracy drop when training LSTM or GRU Hi! My **recurrent neural network (LSTM, resp. GRU)** behaves in a way I cannot explain. The training starts and it trains well (the results look quite good) when **suddenly accuracy drops (and loss rapidly increases)** - both training and testing metrics. Sometimes the net just _goes crazy_ and returns random outputs and sometimes (as in the last of three given examples) it starts to return _same output to all the inputs_. 

![image](https://cloud.githubusercontent.com/assets/8523511/17508785/f94aaf50-5e16-11e6-92cd-4d842b4c0e9c.png)

Do you have **any explanation for this behavior**? Any opinion is welcome. Please, see the task description and the figures below.

**The task:** From a word predict its word2vec vector
**The input:** We have an own word2vec model (normalized) and we feed the network with a word (letter by letter). We pad the words (see the example below).
**Example:** We have a word _football_ and we want to predict its word2vec vector which is 100 dimensions wide. Then the input is . 

Three examples of the behavior:

**Single layer LSTM**



![image](https://cloud.githubusercontent.com/assets/8523511/17507845/43eecbfe-5e12-11e6-8ae3-179ac10d82b7.png)

**Single layer GRU**



![image](https://cloud.githubusercontent.com/assets/8523511/17507923/8ec75934-5e12-11e6-891b-a2d2f2c10e39.png)

**Double layer LSTM**



![image](https://cloud.githubusercontent.com/assets/8523511/17507947/a29ad68e-5e12-11e6-9355-bde7711df71f.png)

We have also _experienced this kind of behavior in another project before_ which used similar architecture but its objective and data were different. Thus the reason should not be hidden in the data or in the particular objective but rather in the architecture.
"
keras,2758,"Should be a fairly simple question:

I am training a sequence to sequence model, and I would like to have accuracy reported during training. Note that this score should measure exact accuracy, i.e. **all** output symbols need to be correct for one input sequence in order for the prediction to be correct. In other words, the metric should measure zero-one loss over all outputs together, rather than per individual output.

For instance:



Most letters are correct, but the prediction only counts if all of them are.

Currently, my last layers are:



and I compile using 



Is this correct, or will this report per-symbol accuracy? If so, how can I best implement the custom metric?
",0,What accuracy is reported on sequence output?,"What accuracy is reported on sequence output? Should be a fairly simple question:

I am training a sequence to sequence model, and I would like to have accuracy reported during training. Note that this score should measure exact accuracy, i.e. **all** output symbols need to be correct for one input sequence in order for the prediction to be correct. In other words, the metric should measure zero-one loss over all outputs together, rather than per individual output.

For instance:



Most letters are correct, but the prediction only counts if all of them are.

Currently, my last layers are:



and I compile using 



Is this correct, or will this report per-symbol accuracy? If so, how can I best implement the custom metric?
"
keras,7146,"Related to #3921 and maybe more, I think we need a template for merge layer (this page)[https://keras.io/layers/merge/} (probably @fchollet should put some input on it?) and few examples that I can work in after understanding things clear.

1. Why documentation?
- Because we need to inform how to import merge layers. Otherwise,


- Also I think we need to make it clear that it's about dealing with Tensors and not layers which is already done in the docstrings but not really in the documentation -- it's outdated though. 

python
    tensor_a = Input(shape=(32,))
    tensor_b = Input(shape=(32,))
    merged_tensor = merge([tensor_a, tensor_b], mode='concat', concat_axis=1)
    concatdotcoscompute_output_shape

2. Examples
Just to show clearly that how to do it with 
- tensors within a simple functional model
- maybe how to use them with  models? 
",0,Do Merge layers need more documentations and example? ,"Do Merge layers need more documentations and example?  Related to #3921 and maybe more, I think we need a template for merge layer (this page)[https://keras.io/layers/merge/} (probably @fchollet should put some input on it?) and few examples that I can work in after understanding things clear.

1. Why documentation?
- Because we need to inform how to import merge layers. Otherwise,


- Also I think we need to make it clear that it's about dealing with Tensors and not layers which is already done in the docstrings but not really in the documentation -- it's outdated though. 

python
    tensor_a = Input(shape=(32,))
    tensor_b = Input(shape=(32,))
    merged_tensor = merge([tensor_a, tensor_b], mode='concat', concat_axis=1)
    concatdotcoscompute_output_shape

2. Examples
Just to show clearly that how to do it with 
- tensors within a simple functional model
- maybe how to use them with  models? 
"
keras,536,"I'd like to ask if we could make the optimizer's learning rate () and momentum () into shared scalars. This way we could change their values during training with  using custom rules.

I could work on the PR as long as you guys don't have a reason for not to.
",0,Learning rate and momentum as shared scalars,"Learning rate and momentum as shared scalars I'd like to ask if we could make the optimizer's learning rate () and momentum () into shared scalars. This way we could change their values during training with  using custom rules.

I could work on the PR as long as you guys don't have a reason for not to.
"
keras,1524,"Hey there,

I have a binary cross-entropy problem that I would like to solve using one-class classification.

So to do this, I have the following model...



However, this is returning the following error.



Can anyone help?
",0,One class classification,"One class classification Hey there,

I have a binary cross-entropy problem that I would like to solve using one-class classification.

So to do this, I have the following model...



However, this is returning the following error.



Can anyone help?
"
keras,868,"It doesn't work.



It is obviously trying to load the loss function from the relevant keras module, and not finding it.

How about adding some parameters to model.model_from_json? Something like  would be great.

More generally though, maybe we should add the possibility of giving a dictionary of named functions for all the custom functions the model might use: activation, loss, optimizer, ...
",0,Using custom loss function with model_from_json,"Using custom loss function with model_from_json It doesn't work.



It is obviously trying to load the loss function from the relevant keras module, and not finding it.

How about adding some parameters to model.model_from_json? Something like  would be great.

More generally though, maybe we should add the possibility of giving a dictionary of named functions for all the custom functions the model might use: activation, loss, optimizer, ...
"
keras,11757,"- [ yes] Check that you are up-to-date with the master branch of Keras. You can update with:


- [ yes] Check that your version of TensorFlow is up-to-date. The installation instructions can be found [here](https://www.tensorflow.org/get_started/os_setup).

- [https://github.com/keras-team/keras/blob/master/examples/tensorboard_embeddings_mnist.py] Provide a link to a GitHub Gist of a Python script that can reproduce your issue (or just copy the script here if it is short).

I am using Anaconda python(3.6) for running the example code for tensorboard embedding in mnist. After training for an epoch, when the call back is called, it gives the following error. 


Epoch 1/12
60000/60000 [==============================] - 8s 139us/step - loss: 0.2665 - acc: 0.9170 - val_loss: 0.0716 - val_acc: 0.9774
2018-11-29 13:04:24.254467: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at save_restore_v2_ops.cc:137 : Unknown: Failed to rename: ./logs\keras_embedding.ckpt-0.data-00000-of-00001.tempstate7943206387758954579 to: ./logs\keras_embedding.ckpt-0.data-00000-of-00001 : Access is denied.
; Input/output error
Traceback (most recent call last):
  File ""C:\Users\abalu\AppData\Local\Continuum\anaconda3\envs\keras\lib\site-packages\tensorflow\python\client\session.py"", line 1334, in _do_call
    return fn(*args)
  File ""C:\Users\abalu\AppData\Local\Continuum\anaconda3\envs\keras\lib\site-packages\tensorflow\python\client\session.py"", line 1319, in _run_fn
    options, feed_dict, fetch_list, target_list, run_metadata)
  File ""C:\Users\abalu\AppData\Local\Continuum\anaconda3\envs\keras\lib\site-packages\tensorflow\python\client\session.py"", line 1407, in _call_tf_sessionrun
    run_metadata)
tensorflow.python.framework.errors_impl.UnknownError: Failed to rename: ./logs\keras_embedding.ckpt-0.data-00000-of-00001.tempstate7943206387758954579 to: ./logs\keras_embedding.ckpt-0.data-00000-of-00001 : Access is denied.
; Input/output error
         [[{{node save/SaveV2}} = SaveV2[dtypes=[DT_FLOAT], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](_arg_save/Const_0_0, save/SaveV2/tensor_names, save/SaveV2/shape_and_slices, features_embedding/_137)]]

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""embedding.py"", line 88, in <module>
    validation_data=(x_test, y_test))
  File ""C:\Users\abalu\AppData\Local\Continuum\anaconda3\envs\keras\lib\site-packages\keras\engine\training.py"", line 1039, in fit
    validation_steps=validation_steps)
  File ""C:\Users\abalu\AppData\Local\Continuum\anaconda3\envs\keras\lib\site-packages\keras\engine\training_arrays.py"", line 217, in fit_loop
    callbacks.on_epoch_end(epoch, epoch_logs)
  File ""C:\Users\abalu\AppData\Local\Continuum\anaconda3\envs\keras\lib\site-packages\keras\callbacks.py"", line 79, in on_epoch_end
    callback.on_epoch_end(epoch, logs)
  File ""C:\Users\abalu\AppData\Local\Continuum\anaconda3\envs\keras\lib\site-packages\keras\callbacks.py"", line 981, in on_epoch_end
    epoch)
  File ""C:\Users\abalu\AppData\Local\Continuum\anaconda3\envs\keras\lib\site-packages\tensorflow\python\training\saver.py"", line 1441, in save
    {self.saver_def.filename_tensor_name: checkpoint_file})
  File ""C:\Users\abalu\AppData\Local\Continuum\anaconda3\envs\keras\lib\site-packages\tensorflow\python\client\session.py"", line 929, in run
    run_metadata_ptr)
  File ""C:\Users\abalu\AppData\Local\Continuum\anaconda3\envs\keras\lib\site-packages\tensorflow\python\client\session.py"", line 1152, in _run
    feed_dict_tensor, options, run_metadata)
  File ""C:\Users\abalu\AppData\Local\Continuum\anaconda3\envs\keras\lib\site-packages\tensorflow\python\client\session.py"", line 1328, in _do_run
    run_metadata)
  File ""C:\Users\abalu\AppData\Local\Continuum\anaconda3\envs\keras\lib\site-packages\tensorflow\python\client\session.py"", line 1348, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.UnknownError: Failed to rename: ./logs\keras_embedding.ckpt-0.data-00000-of-00001.tempstate7943206387758954579 to: ./logs\keras_embedding.ckpt-0.data-00000-of-00001 : Access is denied.
; Input/output error
         [[node save/SaveV2 (defined at C:\Users\abalu\AppData\Local\Continuum\anaconda3\envs\keras\lib\site-packages\keras\callbacks.py:887)  = SaveV2[dtypes=[DT_FLOAT], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](_arg_save/Const_0_0, save/SaveV2/tensor_names, save/SaveV2/shape_and_slices, features_embedding/_137)]]


Caused by op 'save/SaveV2', defined at:
  File ""embedding.py"", line 88, in <module>
    validation_data=(x_test, y_test))
  File ""C:\Users\abalu\AppData\Local\Continuum\anaconda3\envs\keras\lib\site-packages\keras\engine\training.py"", line 1039, in fit
    validation_steps=validation_steps)
  File ""C:\Users\abalu\AppData\Local\Continuum\anaconda3\envs\keras\lib\site-packages\keras\engine\training_arrays.py"", line 117, in fit_loop
    callbacks.set_model(callback_model)
  File ""C:\Users\abalu\AppData\Local\Continuum\anaconda3\envs\keras\lib\site-packages\keras\callbacks.py"", line 54, in set_model
    callback.set_model(model)
  File ""C:\Users\abalu\AppData\Local\Continuum\anaconda3\envs\keras\lib\site-packages\keras\callbacks.py"", line 887, in set_model
    self.saver = tf.train.Saver(list(embeddings_vars.values()))
  File ""C:\Users\abalu\AppData\Local\Continuum\anaconda3\envs\keras\lib\site-packages\tensorflow\python\training\saver.py"", line 1102, in __init__
    self.build()
  File ""C:\Users\abalu\AppData\Local\Continuum\anaconda3\envs\keras\lib\site-packages\tensorflow\python\training\saver.py"", line 1114, in build
    self._build(self._filename, build_save=True, build_restore=True)
  File ""C:\Users\abalu\AppData\Local\Continuum\anaconda3\envs\keras\lib\site-packages\tensorflow\python\training\saver.py"", line 1151, in _build
    build_save=build_save, build_restore=build_restore)
  File ""C:\Users\abalu\AppData\Local\Continuum\anaconda3\envs\keras\lib\site-packages\tensorflow\python\training\saver.py"", line 792, in _build_internal
    save_tensor = self._AddSaveOps(filename_tensor, saveables)
  File ""C:\Users\abalu\AppData\Local\Continuum\anaconda3\envs\keras\lib\site-packages\tensorflow\python\training\saver.py"", line 284, in _AddSaveOps
    save = self.save_op(filename_tensor, saveables)
  File ""C:\Users\abalu\AppData\Local\Continuum\anaconda3\envs\keras\lib\site-packages\tensorflow\python\training\saver.py"", line 202, in save_op
    tensors)
  File ""C:\Users\abalu\AppData\Local\Continuum\anaconda3\envs\keras\lib\site-packages\tensorflow\python\ops\gen_io_ops.py"", line 1690, in save_v2
    shape_and_slices=shape_and_slices, tensors=tensors, name=name)
  File ""C:\Users\abalu\AppData\Local\Continuum\anaconda3\envs\keras\lib\site-packages\tensorflow\python\framework\op_def_library.py"", line 787, in _apply_op_helper
    op_def=op_def)
  File ""C:\Users\abalu\AppData\Local\Continuum\anaconda3\envs\keras\lib\site-packages\tensorflow\python\util\deprecation.py"", line 488, in new_func
    return func(*args, **kwargs)
  File ""C:\Users\abalu\AppData\Local\Continuum\anaconda3\envs\keras\lib\site-packages\tensorflow\python\framework\ops.py"", line 3274, in create_op
    op_def=op_def)
  File ""C:\Users\abalu\AppData\Local\Continuum\anaconda3\envs\keras\lib\site-packages\tensorflow\python\framework\ops.py"", line 1770, in __init__
    self._traceback = tf_stack.extract_stack()

UnknownError (see above for traceback): Failed to rename: ./logs\keras_embedding.ckpt-0.data-00000-of-00001.tempstate7943206387758954579 to: ./logs\keras_embedding.ckpt-0.data-00000-of-00001 : Access is denied.
; Input/output error
         [[node save/SaveV2 (defined at C:\Users\abalu\AppData\Local\Continuum\anaconda3\envs\keras\lib\site-packages\keras\callbacks.py:887)  = SaveV2[dtypes=[DT_FLOAT], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](_arg_save/Const_0_0, save/SaveV2/tensor_names, save/SaveV2/shape_and_slices, features_embedding/_137)]]


I searched for the possible reasons for this to happen, I found the following few issues/discussion:
https://github.com/balancap/SSD-Tensorflow/issues/72
https://stackoverflow.com/questions/43644893/windows-tensorflow-could-not-restore-checkpoint-access-is-denied

my understanding is that, this could be something to do with the behavior in windows. I dont know what the exact issue is, I would like to contribute if someone could let me know what might be the issue. Thanks

",0,keras/examples/tensorboard_embeddings_mnist.py gives an ,"keras/examples/tensorboard_embeddings_mnist.py gives an  - [ yes] Check that you are up-to-date with the master branch of Keras. You can update with:


- [ yes] Check that your version of TensorFlow is up-to-date. The installation instructions can be found [here](https://www.tensorflow.org/get_started/os_setup).

- [https://github.com/keras-team/keras/blob/master/examples/tensorboard_embeddings_mnist.py] Provide a link to a GitHub Gist of a Python script that can reproduce your issue (or just copy the script here if it is short).

I am using Anaconda python(3.6) for running the example code for tensorboard embedding in mnist. After training for an epoch, when the call back is called, it gives the following error. 


Epoch 1/12
60000/60000 [==============================] - 8s 139us/step - loss: 0.2665 - acc: 0.9170 - val_loss: 0.0716 - val_acc: 0.9774
2018-11-29 13:04:24.254467: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at save_restore_v2_ops.cc:137 : Unknown: Failed to rename: ./logs\keras_embedding.ckpt-0.data-00000-of-00001.tempstate7943206387758954579 to: ./logs\keras_embedding.ckpt-0.data-00000-of-00001 : Access is denied.
; Input/output error
Traceback (most recent call last):
  File ""C:\Users\abalu\AppData\Local\Continuum\anaconda3\envs\keras\lib\site-packages\tensorflow\python\client\session.py"", line 1334, in _do_call
    return fn(*args)
  File ""C:\Users\abalu\AppData\Local\Continuum\anaconda3\envs\keras\lib\site-packages\tensorflow\python\client\session.py"", line 1319, in _run_fn
    options, feed_dict, fetch_list, target_list, run_metadata)
  File ""C:\Users\abalu\AppData\Local\Continuum\anaconda3\envs\keras\lib\site-packages\tensorflow\python\client\session.py"", line 1407, in _call_tf_sessionrun
    run_metadata)
tensorflow.python.framework.errors_impl.UnknownError: Failed to rename: ./logs\keras_embedding.ckpt-0.data-00000-of-00001.tempstate7943206387758954579 to: ./logs\keras_embedding.ckpt-0.data-00000-of-00001 : Access is denied.
; Input/output error
         [[{{node save/SaveV2}} = SaveV2[dtypes=[DT_FLOAT], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](_arg_save/Const_0_0, save/SaveV2/tensor_names, save/SaveV2/shape_and_slices, features_embedding/_137)]]

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""embedding.py"", line 88, in <module>
    validation_data=(x_test, y_test))
  File ""C:\Users\abalu\AppData\Local\Continuum\anaconda3\envs\keras\lib\site-packages\keras\engine\training.py"", line 1039, in fit
    validation_steps=validation_steps)
  File ""C:\Users\abalu\AppData\Local\Continuum\anaconda3\envs\keras\lib\site-packages\keras\engine\training_arrays.py"", line 217, in fit_loop
    callbacks.on_epoch_end(epoch, epoch_logs)
  File ""C:\Users\abalu\AppData\Local\Continuum\anaconda3\envs\keras\lib\site-packages\keras\callbacks.py"", line 79, in on_epoch_end
    callback.on_epoch_end(epoch, logs)
  File ""C:\Users\abalu\AppData\Local\Continuum\anaconda3\envs\keras\lib\site-packages\keras\callbacks.py"", line 981, in on_epoch_end
    epoch)
  File ""C:\Users\abalu\AppData\Local\Continuum\anaconda3\envs\keras\lib\site-packages\tensorflow\python\training\saver.py"", line 1441, in save
    {self.saver_def.filename_tensor_name: checkpoint_file})
  File ""C:\Users\abalu\AppData\Local\Continuum\anaconda3\envs\keras\lib\site-packages\tensorflow\python\client\session.py"", line 929, in run
    run_metadata_ptr)
  File ""C:\Users\abalu\AppData\Local\Continuum\anaconda3\envs\keras\lib\site-packages\tensorflow\python\client\session.py"", line 1152, in _run
    feed_dict_tensor, options, run_metadata)
  File ""C:\Users\abalu\AppData\Local\Continuum\anaconda3\envs\keras\lib\site-packages\tensorflow\python\client\session.py"", line 1328, in _do_run
    run_metadata)
  File ""C:\Users\abalu\AppData\Local\Continuum\anaconda3\envs\keras\lib\site-packages\tensorflow\python\client\session.py"", line 1348, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.UnknownError: Failed to rename: ./logs\keras_embedding.ckpt-0.data-00000-of-00001.tempstate7943206387758954579 to: ./logs\keras_embedding.ckpt-0.data-00000-of-00001 : Access is denied.
; Input/output error
         [[node save/SaveV2 (defined at C:\Users\abalu\AppData\Local\Continuum\anaconda3\envs\keras\lib\site-packages\keras\callbacks.py:887)  = SaveV2[dtypes=[DT_FLOAT], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](_arg_save/Const_0_0, save/SaveV2/tensor_names, save/SaveV2/shape_and_slices, features_embedding/_137)]]


Caused by op 'save/SaveV2', defined at:
  File ""embedding.py"", line 88, in <module>
    validation_data=(x_test, y_test))
  File ""C:\Users\abalu\AppData\Local\Continuum\anaconda3\envs\keras\lib\site-packages\keras\engine\training.py"", line 1039, in fit
    validation_steps=validation_steps)
  File ""C:\Users\abalu\AppData\Local\Continuum\anaconda3\envs\keras\lib\site-packages\keras\engine\training_arrays.py"", line 117, in fit_loop
    callbacks.set_model(callback_model)
  File ""C:\Users\abalu\AppData\Local\Continuum\anaconda3\envs\keras\lib\site-packages\keras\callbacks.py"", line 54, in set_model
    callback.set_model(model)
  File ""C:\Users\abalu\AppData\Local\Continuum\anaconda3\envs\keras\lib\site-packages\keras\callbacks.py"", line 887, in set_model
    self.saver = tf.train.Saver(list(embeddings_vars.values()))
  File ""C:\Users\abalu\AppData\Local\Continuum\anaconda3\envs\keras\lib\site-packages\tensorflow\python\training\saver.py"", line 1102, in __init__
    self.build()
  File ""C:\Users\abalu\AppData\Local\Continuum\anaconda3\envs\keras\lib\site-packages\tensorflow\python\training\saver.py"", line 1114, in build
    self._build(self._filename, build_save=True, build_restore=True)
  File ""C:\Users\abalu\AppData\Local\Continuum\anaconda3\envs\keras\lib\site-packages\tensorflow\python\training\saver.py"", line 1151, in _build
    build_save=build_save, build_restore=build_restore)
  File ""C:\Users\abalu\AppData\Local\Continuum\anaconda3\envs\keras\lib\site-packages\tensorflow\python\training\saver.py"", line 792, in _build_internal
    save_tensor = self._AddSaveOps(filename_tensor, saveables)
  File ""C:\Users\abalu\AppData\Local\Continuum\anaconda3\envs\keras\lib\site-packages\tensorflow\python\training\saver.py"", line 284, in _AddSaveOps
    save = self.save_op(filename_tensor, saveables)
  File ""C:\Users\abalu\AppData\Local\Continuum\anaconda3\envs\keras\lib\site-packages\tensorflow\python\training\saver.py"", line 202, in save_op
    tensors)
  File ""C:\Users\abalu\AppData\Local\Continuum\anaconda3\envs\keras\lib\site-packages\tensorflow\python\ops\gen_io_ops.py"", line 1690, in save_v2
    shape_and_slices=shape_and_slices, tensors=tensors, name=name)
  File ""C:\Users\abalu\AppData\Local\Continuum\anaconda3\envs\keras\lib\site-packages\tensorflow\python\framework\op_def_library.py"", line 787, in _apply_op_helper
    op_def=op_def)
  File ""C:\Users\abalu\AppData\Local\Continuum\anaconda3\envs\keras\lib\site-packages\tensorflow\python\util\deprecation.py"", line 488, in new_func
    return func(*args, **kwargs)
  File ""C:\Users\abalu\AppData\Local\Continuum\anaconda3\envs\keras\lib\site-packages\tensorflow\python\framework\ops.py"", line 3274, in create_op
    op_def=op_def)
  File ""C:\Users\abalu\AppData\Local\Continuum\anaconda3\envs\keras\lib\site-packages\tensorflow\python\framework\ops.py"", line 1770, in __init__
    self._traceback = tf_stack.extract_stack()

UnknownError (see above for traceback): Failed to rename: ./logs\keras_embedding.ckpt-0.data-00000-of-00001.tempstate7943206387758954579 to: ./logs\keras_embedding.ckpt-0.data-00000-of-00001 : Access is denied.
; Input/output error
         [[node save/SaveV2 (defined at C:\Users\abalu\AppData\Local\Continuum\anaconda3\envs\keras\lib\site-packages\keras\callbacks.py:887)  = SaveV2[dtypes=[DT_FLOAT], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](_arg_save/Const_0_0, save/SaveV2/tensor_names, save/SaveV2/shape_and_slices, features_embedding/_137)]]


I searched for the possible reasons for this to happen, I found the following few issues/discussion:
https://github.com/balancap/SSD-Tensorflow/issues/72
https://stackoverflow.com/questions/43644893/windows-tensorflow-could-not-restore-checkpoint-access-is-denied

my understanding is that, this could be something to do with the behavior in windows. I dont know what the exact issue is, I would like to contribute if someone could let me know what might be the issue. Thanks

"
keras,11600,"I use a custom layer for a trainable Swish Activation function : https://arxiv.org/abs/1710.05941. It can be set to non-trainable with any beta value, so it's quite general. I'd like to add it to Keras so that it can be part of the next release. It's quite popular. Or I can add it to keras contrib. 

Where should I add the my code for Swish before making a pull request?",0,Add Trainable Swish Layer to Keras,"Add Trainable Swish Layer to Keras I use a custom layer for a trainable Swish Activation function : https://arxiv.org/abs/1710.05941. It can be set to non-trainable with any beta value, so it's quite general. I'd like to add it to Keras so that it can be part of the next release. It's quite popular. Or I can add it to keras contrib. 

Where should I add the my code for Swish before making a pull request?"
keras,6657,"I using Keras 2.x ‘tf’ seeting.
Why I can’t using
X_batch, y_batch = datagen.flow(train, train, batch_size=32)
For example :

# Code
from keras.datasets import mnist
from keras.preprocessing.image import ImageDataGenerator

(X_train, y_train), (X_test, y_test) = mnist.load_data()
X_train = X_train.reshape(X_train.shape[0], 28, 28,1)
X_test = X_test.reshape(X_test.shape[0], 28, 28,1)
X_train = X_train.astype(‘float32’)
X_test = X_test.astype(‘float32’)
datagen = ImageDataGenerator(featurewise_center=True, featurewise_std_normalization=True)
datagen.fit(X_train)
X_batch, y_batch = datagen.flow(X_train, y_train, batch_size=9)

#Question 
Can anyone tell me why?
Thanks!",0,datagen.flow question,"datagen.flow question I using Keras 2.x ‘tf’ seeting.
Why I can’t using
X_batch, y_batch = datagen.flow(train, train, batch_size=32)
For example :

# Code
from keras.datasets import mnist
from keras.preprocessing.image import ImageDataGenerator

(X_train, y_train), (X_test, y_test) = mnist.load_data()
X_train = X_train.reshape(X_train.shape[0], 28, 28,1)
X_test = X_test.reshape(X_test.shape[0], 28, 28,1)
X_train = X_train.astype(‘float32’)
X_test = X_test.astype(‘float32’)
datagen = ImageDataGenerator(featurewise_center=True, featurewise_std_normalization=True)
datagen.fit(X_train)
X_batch, y_batch = datagen.flow(X_train, y_train, batch_size=9)

#Question 
Can anyone tell me why?
Thanks!"
keras,7510,"When trying to import a model exported with  using , I get the following error:
 
     File ""Q:\Anaconda2\lib\site-packages\keras\models.py"", line 325, in model_from_json
      return layer_module.deserialize(config, custom_objects=custom_objects)
    File ""Q:\Anaconda2\lib\site-packages\keras\layers\__init__.py"", line 46, in deserialize
      printable_module_name='layer')
    File ""Q:\Anaconda2\lib\site-packages\keras\utils\generic_utils.py"", line 140, in deserialize_keras_object
      list(custom_objects.items())))
    File ""Q:\Anaconda2\lib\site-packages\keras\engine\topology.py"", line 2374, in from_config
      process_layer(layer_data)
    File ""Q:\Anaconda2\lib\site-packages\keras\engine\topology.py"", line 2343, in process_layer
      custom_objects=custom_objects)
    File ""Q:\Anaconda2\lib\site-packages\keras\layers\__init__.py"", line 46, in deserialize
      printable_module_name='layer')
    File ""Q:\Anaconda2\lib\site-packages\keras\utils\generic_utils.py"", line 141, in deserialize_keras_object
      return cls.from_config(config['config'])
    File ""Q:\Anaconda2\lib\site-packages\keras\engine\topology.py"", line 1206, in from_config
      return cls(**config)
    File ""Q:\Anaconda2\lib\site-packages\keras\legacy\interfaces.py"", line 88, in wrapper
      return func(*args, **kwargs)
    File ""Q:\Anaconda2\lib\site-packages\keras\layers\recurrent.py"", line 931, in __init__
      super(LSTM, self).__init__(**kwargs)
    File ""Q:\Anaconda2\lib\site-packages\keras\layers\recurrent.py"", line 181, in __init__
      super(Recurrent, self).__init__(**kwargs)
    File ""Q:\Anaconda2\lib\site-packages\keras\engine\topology.py"", line 277, in __init__
      raise TypeError('Keyword argument not understood:', kwarg)
    TypeError: ('Keyword argument not understood:', u'return_state')

The problem occurs when trying to import an LSTM layer: in ,  is called with the keyword args; however,  throws an exception if the keyword is not in the  list. However,  is an allowed kwarg of  (and ) as per the [documentation](https://keras.io/layers/recurrent/). The model was json-serialized using the same version of Keras as when I try to load it. Is this desired behavior? One might expect that when a model can be serialized, it should also be deserializable using the same version of Keras.

The JSON description of the model can be found in [this gist](https://gist.github.com/benjaminalt/04f00629cf52b735414d700e2352930c).

Here is my setup:
OS: 64-bit Windows 10
Python: Python 2.7.13 (Anaconda 4.4.0 64-bit)
Keras: 2.0.6
Theano: 0.9.0
",0,'Keyword argument not understood' error when importing a Keras model from json,"'Keyword argument not understood' error when importing a Keras model from json When trying to import a model exported with  using , I get the following error:
 
     File ""Q:\Anaconda2\lib\site-packages\keras\models.py"", line 325, in model_from_json
      return layer_module.deserialize(config, custom_objects=custom_objects)
    File ""Q:\Anaconda2\lib\site-packages\keras\layers\__init__.py"", line 46, in deserialize
      printable_module_name='layer')
    File ""Q:\Anaconda2\lib\site-packages\keras\utils\generic_utils.py"", line 140, in deserialize_keras_object
      list(custom_objects.items())))
    File ""Q:\Anaconda2\lib\site-packages\keras\engine\topology.py"", line 2374, in from_config
      process_layer(layer_data)
    File ""Q:\Anaconda2\lib\site-packages\keras\engine\topology.py"", line 2343, in process_layer
      custom_objects=custom_objects)
    File ""Q:\Anaconda2\lib\site-packages\keras\layers\__init__.py"", line 46, in deserialize
      printable_module_name='layer')
    File ""Q:\Anaconda2\lib\site-packages\keras\utils\generic_utils.py"", line 141, in deserialize_keras_object
      return cls.from_config(config['config'])
    File ""Q:\Anaconda2\lib\site-packages\keras\engine\topology.py"", line 1206, in from_config
      return cls(**config)
    File ""Q:\Anaconda2\lib\site-packages\keras\legacy\interfaces.py"", line 88, in wrapper
      return func(*args, **kwargs)
    File ""Q:\Anaconda2\lib\site-packages\keras\layers\recurrent.py"", line 931, in __init__
      super(LSTM, self).__init__(**kwargs)
    File ""Q:\Anaconda2\lib\site-packages\keras\layers\recurrent.py"", line 181, in __init__
      super(Recurrent, self).__init__(**kwargs)
    File ""Q:\Anaconda2\lib\site-packages\keras\engine\topology.py"", line 277, in __init__
      raise TypeError('Keyword argument not understood:', kwarg)
    TypeError: ('Keyword argument not understood:', u'return_state')

The problem occurs when trying to import an LSTM layer: in ,  is called with the keyword args; however,  throws an exception if the keyword is not in the  list. However,  is an allowed kwarg of  (and ) as per the [documentation](https://keras.io/layers/recurrent/). The model was json-serialized using the same version of Keras as when I try to load it. Is this desired behavior? One might expect that when a model can be serialized, it should also be deserializable using the same version of Keras.

The JSON description of the model can be found in [this gist](https://gist.github.com/benjaminalt/04f00629cf52b735414d700e2352930c).

Here is my setup:
OS: 64-bit Windows 10
Python: Python 2.7.13 (Anaconda 4.4.0 64-bit)
Keras: 2.0.6
Theano: 0.9.0
"
keras,1490,"Keras and Theano are all newest. I have used sudo pip install git+git://github.com/Theano/Theano.git. How can I solve this problem? Many thanks!
",0,AttributeError: 'module' object has no attribute 'relu',"AttributeError: 'module' object has no attribute 'relu' Keras and Theano are all newest. I have used sudo pip install git+git://github.com/Theano/Theano.git. How can I solve this problem? Many thanks!
"
keras,8298,"Please make sure that the boxes below are checked before you submit your issue. If your issue is an implementation question, please ask your question on [StackOverflow](http://stackoverflow.com/questions/tagged/keras) or [join the Keras Slack channel](https://keras-slack-autojoin.herokuapp.com/) and ask there instead of filing a GitHub issue.

Thank you!

- [ ] Check that you are up-to-date with the master branch of Keras. You can update with:
pip install git+git://github.com/fchollet/keras.git --upgrade --no-deps

- [ ] If running on TensorFlow, check that you are up-to-date with the latest version. The installation instructions can be found [here](https://www.tensorflow.org/get_started/os_setup).

- [ ] If running on Theano, check that you are up-to-date with the master branch of Theano. You can update with:
pip install git+git://github.com/Theano/Theano.git --upgrade --no-deps

- [ ] Provide a link to a GitHub Gist of a Python script that can reproduce your issue (or just copy the script here if it is short).
",0,index_generator:,"index_generator: Please make sure that the boxes below are checked before you submit your issue. If your issue is an implementation question, please ask your question on [StackOverflow](http://stackoverflow.com/questions/tagged/keras) or [join the Keras Slack channel](https://keras-slack-autojoin.herokuapp.com/) and ask there instead of filing a GitHub issue.

Thank you!

- [ ] Check that you are up-to-date with the master branch of Keras. You can update with:
pip install git+git://github.com/fchollet/keras.git --upgrade --no-deps

- [ ] If running on TensorFlow, check that you are up-to-date with the latest version. The installation instructions can be found [here](https://www.tensorflow.org/get_started/os_setup).

- [ ] If running on Theano, check that you are up-to-date with the master branch of Theano. You can update with:
pip install git+git://github.com/Theano/Theano.git --upgrade --no-deps

- [ ] Provide a link to a GitHub Gist of a Python script that can reproduce your issue (or just copy the script here if it is short).
"
keras,2003,"I'm using Keras to build and train a recurrent neural network.



 is an an array of sequences of [latitude, longitude, temperature], all padded to the same length with values of [0,0,0]. When I train this network, the first epoch gives me a loss of about 63, and _increases_ after more epochs.
This is causing a  call later in the code to give values that are completely off of the training values. For example, most of the training values in each sequence is around , but the RNN outputs values consistently around , which causes me to think something is wrong with the masking layer. 

The training X () data looks something like this (only much larger): 



and the training Y () data looks like this:


",0,Loss increasing after each epoch.,"Loss increasing after each epoch. I'm using Keras to build and train a recurrent neural network.



 is an an array of sequences of [latitude, longitude, temperature], all padded to the same length with values of [0,0,0]. When I train this network, the first epoch gives me a loss of about 63, and _increases_ after more epochs.
This is causing a  call later in the code to give values that are completely off of the training values. For example, most of the training values in each sequence is around , but the RNN outputs values consistently around , which causes me to think something is wrong with the masking layer. 

The training X () data looks something like this (only much larger): 



and the training Y () data looks like this:


"
keras,303,"Most of the time when I have a bug it's because I haven't correctly figured out the input dimensionality for some hidden layer that's layered on top of a convolutional or max pooling layer. Can we have the code auto-compute the input dimensionality of non-input layers for ease of use? This would have saved me hours initially, as is how Passage works (you just specify the output dimensionality).
",0,Omit input dimensions (for hidden layers) and auto-calculate if not specified,"Omit input dimensions (for hidden layers) and auto-calculate if not specified Most of the time when I have a bug it's because I haven't correctly figured out the input dimensionality for some hidden layer that's layered on top of a convolutional or max pooling layer. Can we have the code auto-compute the input dimensionality of non-input layers for ease of use? This would have saved me hours initially, as is how Passage works (you just specify the output dimensionality).
"
keras,579,,0,"Can CNN  and LSTM classify muti-categories texts, how to modify the code? Thanks!",
keras,2943,,0,Typo,
keras,197,"@karpathy's [character-based RNN in Torch](https://github.com/karpathy/char-rnn) has gotten a great deal of attention recently after [his blog post](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) entitled ""The Unreasonable Effectiveness of Recurrent Neural Networks"".

It would be nice if there was an example of doing the same thing in Keras.
",0,Add a character-based RNN example.,"Add a character-based RNN example. @karpathy's [character-based RNN in Torch](https://github.com/karpathy/char-rnn) has gotten a great deal of attention recently after [his blog post](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) entitled ""The Unreasonable Effectiveness of Recurrent Neural Networks"".

It would be nice if there was an example of doing the same thing in Keras.
"
keras,1641,"Hey everyone,

I'm trying to use custom data on the LSTM model, but it keeps giving shape errors. After reading some other issues along the same lines, I even tried reshaping the input data to size (nb_inputs, timestamps, 1) which looks approximately like (4200, 60, 1), but that returns an error that says a shape of (None, 4200, 60, 1) is no good. Any thoughts?



Output:

> Using Theano backend.
> Loading data...
> 4130 train sequences
> 1016 test sequences
> X_train shape: (4130L, 60L)
> X_test shape: (1016L, 60L)
> Build model...
> Train...
> Train on 4130 samples, validate on 1016 samples
> Epoch 1/3
> Traceback (most recent call last):
>   File ""main.py"", line 52, in <module>
>     validation_data=(X_test, y_test), show_accuracy=True)
>   File ""C:\Miniconda2\lib\site-packages\keras\models.py"", line 507, in fit
>     shuffle=shuffle, metrics=metrics)
>   File ""C:\Miniconda2\lib\site-packages\keras\models.py"", line 226, in _fit
>     outs = f(ins_batch)
>   File ""C:\Miniconda2\lib\site-packages\keras\backend\theano_backend.py"", line 357, in __call__
>     return self.function(*inputs)
>   File ""C:\Miniconda2\lib\site-packages\theano\compile\function_module.py"", line 513, in **call**
>     allow_downcast=s.allow_downcast)
>   File ""C:\Miniconda2\lib\site-packages\theano\tensor\type.py"", line 169, in filter
>     data.shape))
> TypeError: ('Bad input argument to theano function with name ""C:\Miniconda2\lib\site-packages\keras\backend\theano_backend.py:354""  at index 0(0-based)', 'Wrong number of dimensions: expected 3, got 2 with shape (32L, 60L).')
",0,"Help: 'Wrong number of dimensions: expected 3, got 2 with shape (32L, 60L).' in LSTM model","Help: 'Wrong number of dimensions: expected 3, got 2 with shape (32L, 60L).' in LSTM model Hey everyone,

I'm trying to use custom data on the LSTM model, but it keeps giving shape errors. After reading some other issues along the same lines, I even tried reshaping the input data to size (nb_inputs, timestamps, 1) which looks approximately like (4200, 60, 1), but that returns an error that says a shape of (None, 4200, 60, 1) is no good. Any thoughts?



Output:

> Using Theano backend.
> Loading data...
> 4130 train sequences
> 1016 test sequences
> X_train shape: (4130L, 60L)
> X_test shape: (1016L, 60L)
> Build model...
> Train...
> Train on 4130 samples, validate on 1016 samples
> Epoch 1/3
> Traceback (most recent call last):
>   File ""main.py"", line 52, in <module>
>     validation_data=(X_test, y_test), show_accuracy=True)
>   File ""C:\Miniconda2\lib\site-packages\keras\models.py"", line 507, in fit
>     shuffle=shuffle, metrics=metrics)
>   File ""C:\Miniconda2\lib\site-packages\keras\models.py"", line 226, in _fit
>     outs = f(ins_batch)
>   File ""C:\Miniconda2\lib\site-packages\keras\backend\theano_backend.py"", line 357, in __call__
>     return self.function(*inputs)
>   File ""C:\Miniconda2\lib\site-packages\theano\compile\function_module.py"", line 513, in **call**
>     allow_downcast=s.allow_downcast)
>   File ""C:\Miniconda2\lib\site-packages\theano\tensor\type.py"", line 169, in filter
>     data.shape))
> TypeError: ('Bad input argument to theano function with name ""C:\Miniconda2\lib\site-packages\keras\backend\theano_backend.py:354""  at index 0(0-based)', 'Wrong number of dimensions: expected 3, got 2 with shape (32L, 60L).')
"
keras,6241,"hi, I just got a problem when I run a sequential model. 
I fit the model firstly, and then I want to get the first FullConnect Layer's parameters ,that is  ""dense1 = model.get_layer(index=1).get_weights()"".  But I got the wrong ""AttributeError: 'NoneType' object has no attribute 'get_weights' ""。I have no idea about this. Can anyone help me?

Sincerely",0,AttributeError: 'NoneType' object has no attribute 'get_weights',"AttributeError: 'NoneType' object has no attribute 'get_weights' hi, I just got a problem when I run a sequential model. 
I fit the model firstly, and then I want to get the first FullConnect Layer's parameters ,that is  ""dense1 = model.get_layer(index=1).get_weights()"".  But I got the wrong ""AttributeError: 'NoneType' object has no attribute 'get_weights' ""。I have no idea about this. Can anyone help me?

Sincerely"
keras,2529,"When running  on a up to date Keras repo (c9f7d970e97d5a) following the instructions in  the following error happens:



In addition, there could be a small error in the example in :
 is supposed to be  on line 7 in function .
- [X] Check that you are up-to-date with the master branch of Keras. You can update with:
  pip install git+git://github.com/fchollet/keras.git --upgrade --no-deps
- [X] If running on Theano, check that you are up-to-date with the master branch of Theano. You can update with:
  pip install git+git://github.com/Theano/Theano.git --upgrade --no-deps
- [X] Provide a link to a GitHub Gist of a Python script that can reproduce your issue (or just copy the script here if it is short).
",0,Missing file docs/sources/layers/writing-your-own-keras-layers.md when building documentation,"Missing file docs/sources/layers/writing-your-own-keras-layers.md when building documentation When running  on a up to date Keras repo (c9f7d970e97d5a) following the instructions in  the following error happens:



In addition, there could be a small error in the example in :
 is supposed to be  on line 7 in function .
- [X] Check that you are up-to-date with the master branch of Keras. You can update with:
  pip install git+git://github.com/fchollet/keras.git --upgrade --no-deps
- [X] If running on Theano, check that you are up-to-date with the master branch of Theano. You can update with:
  pip install git+git://github.com/Theano/Theano.git --upgrade --no-deps
- [X] Provide a link to a GitHub Gist of a Python script that can reproduce your issue (or just copy the script here if it is short).
"
keras,3873,"I wanted to be able to show sequential networks in a clean and minimalistic way for didactic purpose. Both  and graph export were not enough - I wanted dimensions, numbers of parameters and activation functions in one place, at the same time without unnecessary overhead.

Bear in mind that I purposefully make no distinction between adding activation function as a keyword argument or as a separate layer (vide [Activations - Keras documentation](https://keras.io/activations/)), unlike in  or .

Code: https://gist.github.com/stared/8411d4e7e457b0f14f39d700afc8511c

Should I clean and generalise it, so that it can be a part of ?

Any comments, remarks and (sub)feature requests ale welcomed! :)
## Examples
### Proof of principle


### VGG16


",0,Feature: ASCII prints for sequential models,"Feature: ASCII prints for sequential models I wanted to be able to show sequential networks in a clean and minimalistic way for didactic purpose. Both  and graph export were not enough - I wanted dimensions, numbers of parameters and activation functions in one place, at the same time without unnecessary overhead.

Bear in mind that I purposefully make no distinction between adding activation function as a keyword argument or as a separate layer (vide [Activations - Keras documentation](https://keras.io/activations/)), unlike in  or .

Code: https://gist.github.com/stared/8411d4e7e457b0f14f39d700afc8511c

Should I clean and generalise it, so that it can be a part of ?

Any comments, remarks and (sub)feature requests ale welcomed! :)
## Examples
### Proof of principle


### VGG16


"
keras,4007,"So for example in this code : 



If I want to see the output of the right branch model given a test input **(before the merge layer)** how to do that ?
Thanks
",0,How to visualize the output of one of two merged layers ? ,"How to visualize the output of one of two merged layers ?  So for example in this code : 



If I want to see the output of the right branch model given a test input **(before the merge layer)** how to do that ?
Thanks
"
keras,6973,"I am trying to use batch normalization, but for some reason, even for the simplest network, when I run model.fit  even for one epoch,the loss is nan and naturally no learning is performed.
For example - I use a simple model like this:
 model = Sequential()
 model.add(Conv2D(32,kernel_size=(3,3),activation='relu',input_shape=(16,16,3)))
 model.add(MaxPool2D(pool_size=(2, 2)))
 model.add(BatchNormalization())
model.add(Flatten())
model.add(Dense(2,activation='softmax'))
 model.compile (loss='binary_crossentropy',  optimizer='adam',  metrics=['accuracy'])

If I remove the batch normalization, everything works great.
I am using keras 2.0.4 and theano 0.9.0, cuda 7. I tried removing cudnn, got the same results.
I tried a diffrent axis (axis=1) when calling BN,  (although this should not be right) and got the same result.
What am I doing wrong ?
Thank YOU!
",0,Problem with batch normalization layer,"Problem with batch normalization layer I am trying to use batch normalization, but for some reason, even for the simplest network, when I run model.fit  even for one epoch,the loss is nan and naturally no learning is performed.
For example - I use a simple model like this:
 model = Sequential()
 model.add(Conv2D(32,kernel_size=(3,3),activation='relu',input_shape=(16,16,3)))
 model.add(MaxPool2D(pool_size=(2, 2)))
 model.add(BatchNormalization())
model.add(Flatten())
model.add(Dense(2,activation='softmax'))
 model.compile (loss='binary_crossentropy',  optimizer='adam',  metrics=['accuracy'])

If I remove the batch normalization, everything works great.
I am using keras 2.0.4 and theano 0.9.0, cuda 7. I tried removing cudnn, got the same results.
I tried a diffrent axis (axis=1) when calling BN,  (although this should not be right) and got the same result.
What am I doing wrong ?
Thank YOU!
"
keras,3988,"Say I have a pertained model like the VGG16. If I want to select only the first few convolutional blocks I can so something like:



Is there a way to use the same API to slice a middle part of a model (including weights) ? I've tried something like:



but got an error:


",0,Slicing layers from a pre-trained Model,"Slicing layers from a pre-trained Model Say I have a pertained model like the VGG16. If I want to select only the first few convolutional blocks I can so something like:



Is there a way to use the same API to slice a middle part of a model (including weights) ? I've tried something like:



but got an error:


"
keras,4156,"Please make sure that the boxes below are checked before you submit your issue. Thank you!
- [x] Check that you are up-to-date with the master branch of Keras. You can update with:
  pip install git+git://github.com/fchollet/keras.git --upgrade --no-deps
- [x] If running on Theano, check that you are up-to-date with the master branch of Theano. You can update with:
  pip install git+git://github.com/Theano/Theano.git --upgrade --no-deps
- [x] Provide a link to a GitHub Gist of a Python script that can reproduce your issue (or just copy the script here if it is short).

Having a callback in the model:



and model compilation:



In the final csv in column 'lr' I get always 0.03 instead of lr calculated by decay.

After looking at callbacks.py I think the problem is in the 'logs' object passed to callback not in the callback itself. 

Any hints? I could try to make a PR - given some guidance
",0,callbacks e.g. CSV Callback receive constant lr when doing a LR decay,"callbacks e.g. CSV Callback receive constant lr when doing a LR decay Please make sure that the boxes below are checked before you submit your issue. Thank you!
- [x] Check that you are up-to-date with the master branch of Keras. You can update with:
  pip install git+git://github.com/fchollet/keras.git --upgrade --no-deps
- [x] If running on Theano, check that you are up-to-date with the master branch of Theano. You can update with:
  pip install git+git://github.com/Theano/Theano.git --upgrade --no-deps
- [x] Provide a link to a GitHub Gist of a Python script that can reproduce your issue (or just copy the script here if it is short).

Having a callback in the model:



and model compilation:



In the final csv in column 'lr' I get always 0.03 instead of lr calculated by decay.

After looking at callbacks.py I think the problem is in the 'logs' object passed to callback not in the callback itself. 

Any hints? I could try to make a PR - given some guidance
"
keras,2619,"Hi,
I design a network which composes of shared layers, I write code using functional API. However, there always be errors.  
I paste my network structure graph , code , and error information below, hope to provide a clue.
Thanks!

Network Structure:

![network structure jpeg](https://raw.githubusercontent.com/ylqfp/attachments/bb396650622b37b2a311d29864795aec67301042/cnn2_2_lstm.jpg)
Share layers:
1. Embedding is shared among 3 input.(blue color). However, input_length is different for input1/input2 and input3. The input_length for input1/input2 is LSTM_MAXLEN, 1000, for input3 is QUERY_MAXLEN, 5, eg. max 5 words for a query.
2. CNN feature learning layer is shared between input1 and input2. (green color)
",0,Why nested shared layers as such network doesn't work?,"Why nested shared layers as such network doesn't work? Hi,
I design a network which composes of shared layers, I write code using functional API. However, there always be errors.  
I paste my network structure graph , code , and error information below, hope to provide a clue.
Thanks!

Network Structure:

![network structure jpeg](https://raw.githubusercontent.com/ylqfp/attachments/bb396650622b37b2a311d29864795aec67301042/cnn2_2_lstm.jpg)
Share layers:
1. Embedding is shared among 3 input.(blue color). However, input_length is different for input1/input2 and input3. The input_length for input1/input2 is LSTM_MAXLEN, 1000, for input3 is QUERY_MAXLEN, 5, eg. max 5 words for a query.
2. CNN feature learning layer is shared between input1 and input2. (green color)
"
keras,5284,"

`

I am precomputing the features of a convolutional layer and training fully connected layers using them. I have to use hdf5 file and a custom generator because the size of dataset is quite large.

Following is the output:
<img width=""544"" alt=""screen shot 2017-02-06 at 11 36 13 am"" src=""https://cloud.githubusercontent.com/assets/6660192/22636432/0c2d05a4-ec61-11e6-9193-1a8bd0ac64df.png"">

**The loss when using fit decreases much more quickly then using fit_generator with a custom generator.**
Am I doing something wrong? To me it looks like some bug in fit_generator. 
",0,Problems using fit_generator with a custom generator,"Problems using fit_generator with a custom generator 

`

I am precomputing the features of a convolutional layer and training fully connected layers using them. I have to use hdf5 file and a custom generator because the size of dataset is quite large.

Following is the output:
<img width=""544"" alt=""screen shot 2017-02-06 at 11 36 13 am"" src=""https://cloud.githubusercontent.com/assets/6660192/22636432/0c2d05a4-ec61-11e6-9193-1a8bd0ac64df.png"">

**The loss when using fit decreases much more quickly then using fit_generator with a custom generator.**
Am I doing something wrong? To me it looks like some bug in fit_generator. 
"
keras,1592,"I am trying to run the specific code on a pc with a Titan X gpu:



and it yields the following error



also the versions are 



When i run the same code on another pc it passes!



It is a machine without gpu and versions 



What could i do in order to use such a good gpu ? 
Should i fall back to keras 3.0 or is it a theano problem ? 

Thank you in advance
",0,Convolutional Neural Net bug,"Convolutional Neural Net bug I am trying to run the specific code on a pc with a Titan X gpu:



and it yields the following error



also the versions are 



When i run the same code on another pc it passes!



It is a machine without gpu and versions 



What could i do in order to use such a good gpu ? 
Should i fall back to keras 3.0 or is it a theano problem ? 

Thank you in advance
"
keras,5167,"Hi, I have a model which uses a Merge() layer. The model runs and saves. I am having trouble loading the model again.



The error I get is:

> arg 5 (closure) must be None or tuple

After researching some previous issues, I saw that other people had potentially similar issues using the _Lambda()_. But I didn't get whether there was a solution to this. 

I am running into this issue with both Python 2.7/3.5


Update:

I changed the merge layer. Lambda now takes only one argument, and the output shape is literally defined:

 where  is a list of output layers from models being merged.

Now, when I try to load the model, the error is:

> The layer has never been called and thus it has no defined output shape

Any help would be appreciated.


Gerti


Please make sure that the boxes below are checked before you submit your issue. If your issue is an implementation question, please ask your question on [StackOverflow](http://stackoverflow.com/questions/tagged/keras) or [join the Keras Slack channel](https://keras-slack-autojoin.herokuapp.com/) and ask there instead of filing a GitHub issue.

Thank you!

- [x ] Check that you are up-to-date with the master branch of Keras. You can update with:
pip install git+git://github.com/fchollet/keras.git --upgrade --no-deps

- [ ] If running on TensorFlow, check that you are up-to-date with the latest version. The installation instructions can be found [here](https://www.tensorflow.org/get_started/os_setup).

- [ x] If running on Theano, check that you are up-to-date with the master branch of Theano. You can update with:
pip install git+git://github.com/Theano/Theano.git --upgrade --no-deps

- [x ] Provide a link to a GitHub Gist of a Python script that can reproduce your issue (or just copy the script here if it is short).
",0,Merge layer with lambda - potential issue with reloading model after save,"Merge layer with lambda - potential issue with reloading model after save Hi, I have a model which uses a Merge() layer. The model runs and saves. I am having trouble loading the model again.



The error I get is:

> arg 5 (closure) must be None or tuple

After researching some previous issues, I saw that other people had potentially similar issues using the _Lambda()_. But I didn't get whether there was a solution to this. 

I am running into this issue with both Python 2.7/3.5


Update:

I changed the merge layer. Lambda now takes only one argument, and the output shape is literally defined:

 where  is a list of output layers from models being merged.

Now, when I try to load the model, the error is:

> The layer has never been called and thus it has no defined output shape

Any help would be appreciated.


Gerti


Please make sure that the boxes below are checked before you submit your issue. If your issue is an implementation question, please ask your question on [StackOverflow](http://stackoverflow.com/questions/tagged/keras) or [join the Keras Slack channel](https://keras-slack-autojoin.herokuapp.com/) and ask there instead of filing a GitHub issue.

Thank you!

- [x ] Check that you are up-to-date with the master branch of Keras. You can update with:
pip install git+git://github.com/fchollet/keras.git --upgrade --no-deps

- [ ] If running on TensorFlow, check that you are up-to-date with the latest version. The installation instructions can be found [here](https://www.tensorflow.org/get_started/os_setup).

- [ x] If running on Theano, check that you are up-to-date with the master branch of Theano. You can update with:
pip install git+git://github.com/Theano/Theano.git --upgrade --no-deps

- [x ] Provide a link to a GitHub Gist of a Python script that can reproduce your issue (or just copy the script here if it is short).
"
keras,8926,,0,load_modelCrashing,
keras,4966,"I'm trying to implement the following architecture with Keras (Theano backend). 

I have a first Sequential network (say S1) which takes an image as input and has 4 linear output, which do correspond to the upper-left and the bottom-right coordinates of a rectangular ""window"" in the input image which is supposed to contain the object I want to identify. 

Once I have those four outputs... I'd like to actually crop the image!

So I thought to take again my image as input in a new Sequential network (say S2) and merge these two networks using a Merge Layer with function mode.

I turned out to be a bit more complicated than I expected and I'm stuck with some Theano errors I can't get rid of.

Here's the relevant part of the code:


    model1 = Sequential()
    model1.add(Convolution2D(64, 3, 3, border_mode='same', input_shape=(1280, 720, 3), activation='relu'))
    # now model.output_shape == (None, 1280, 720, 64)

    # add a 3x3 convolution on top, with 32 output filters:
    model1.add(Convolution2D(32, 3, 3, border_mode='same',activation='relu'))
    # now model.output_shape == (None, 1280, 720, 32)
    model1.add(Flatten())
    model1.add(Dense(4, activation='linear'))

    model2 = Sequential()
    # How can I create a simple ""input"" net?
    model2.add(Reshape((1280, 720, 3), input_shape=(1280, 720, 3)))

    def merger(l):
        image = l[1]
        indexes = l[0]
        index_0 = indexes[:][0]
        index_1 = indexes[:,1]
        index_2 = indexes[:,2]
        index_3 = indexes[:,3]
        cropped_image = image[:,index_0:index_1+1, index_2:index_3+1,:]
        return cropped_image

    merged_model = Sequential()
    merged_model.add(Merge([model1, model2], mode=merger))

And here you have the error:

     <ipython-input-25-3b844142556c> in merger(l)
          11     index_2 = indexes[:,2]
          12     index_3 = indexes[:,3]
     ---> 13     cropped_image = image[:,index_0:index_1+1, index_2:index_3+1,:]

     [...]

    ValueError: ('TensorType could not be cast to have 0 dimensions', TensorType(float32, vector))


What's going on?",0,"How to crop ""dinamically""","How to crop ""dinamically"" I'm trying to implement the following architecture with Keras (Theano backend). 

I have a first Sequential network (say S1) which takes an image as input and has 4 linear output, which do correspond to the upper-left and the bottom-right coordinates of a rectangular ""window"" in the input image which is supposed to contain the object I want to identify. 

Once I have those four outputs... I'd like to actually crop the image!

So I thought to take again my image as input in a new Sequential network (say S2) and merge these two networks using a Merge Layer with function mode.

I turned out to be a bit more complicated than I expected and I'm stuck with some Theano errors I can't get rid of.

Here's the relevant part of the code:


    model1 = Sequential()
    model1.add(Convolution2D(64, 3, 3, border_mode='same', input_shape=(1280, 720, 3), activation='relu'))
    # now model.output_shape == (None, 1280, 720, 64)

    # add a 3x3 convolution on top, with 32 output filters:
    model1.add(Convolution2D(32, 3, 3, border_mode='same',activation='relu'))
    # now model.output_shape == (None, 1280, 720, 32)
    model1.add(Flatten())
    model1.add(Dense(4, activation='linear'))

    model2 = Sequential()
    # How can I create a simple ""input"" net?
    model2.add(Reshape((1280, 720, 3), input_shape=(1280, 720, 3)))

    def merger(l):
        image = l[1]
        indexes = l[0]
        index_0 = indexes[:][0]
        index_1 = indexes[:,1]
        index_2 = indexes[:,2]
        index_3 = indexes[:,3]
        cropped_image = image[:,index_0:index_1+1, index_2:index_3+1,:]
        return cropped_image

    merged_model = Sequential()
    merged_model.add(Merge([model1, model2], mode=merger))

And here you have the error:

     <ipython-input-25-3b844142556c> in merger(l)
          11     index_2 = indexes[:,2]
          12     index_3 = indexes[:,3]
     ---> 13     cropped_image = image[:,index_0:index_1+1, index_2:index_3+1,:]

     [...]

    ValueError: ('TensorType could not be cast to have 0 dimensions', TensorType(float32, vector))


What's going on?"
keras,3776,"I'm using latest version of Keras with theano backend.
Here is a short description of my problem (a code that reproduces the problem is also shown):
I have a convolution network with a 1-channel 2D input.
This input layer is shared by two nodes, each performing some convolution operations.
Then outputs of the two nodes are merged by another node, which produces the final output.
I am expecting to get an output dimension of (batch_size, 2x4x9), which is (15, 72).
However, the error message says I am getting (30, 72), which is (2xbatch_size, 2x4x9).
It seems something is wrong with the merge node by looking at ""2xbatch_size"".

The structure of my model is:
![capture](https://cloud.githubusercontent.com/assets/22214494/18573430/e74a2964-7bf5-11e6-9d28-82239e29d293.PNG)

Here is my model:



The error info is pasted here:



I have tried to make this minimum example as simple as possible.
Any idea of where I was doing wrong?
Thanks a lot for your help!
",0,"fit_generator, unexpected output dimension","fit_generator, unexpected output dimension I'm using latest version of Keras with theano backend.
Here is a short description of my problem (a code that reproduces the problem is also shown):
I have a convolution network with a 1-channel 2D input.
This input layer is shared by two nodes, each performing some convolution operations.
Then outputs of the two nodes are merged by another node, which produces the final output.
I am expecting to get an output dimension of (batch_size, 2x4x9), which is (15, 72).
However, the error message says I am getting (30, 72), which is (2xbatch_size, 2x4x9).
It seems something is wrong with the merge node by looking at ""2xbatch_size"".

The structure of my model is:
![capture](https://cloud.githubusercontent.com/assets/22214494/18573430/e74a2964-7bf5-11e6-9d28-82239e29d293.PNG)

Here is my model:



The error info is pasted here:



I have tried to make this minimum example as simple as possible.
Any idea of where I was doing wrong?
Thanks a lot for your help!
"
keras,11943,"This is very minor, but the second link used as reference in Conv2DTranspose seems to be invalid. 
It can be changed from https://www.matthewzeiler.com/pubs/cvpr2010/cvpr2010.pdf to https://www.matthewzeiler.com/mattzeiler/deconvolutionalnetworks.pdf.",0,The link to deconv paper is broken in Conv2DTranspose comments,"The link to deconv paper is broken in Conv2DTranspose comments This is very minor, but the second link used as reference in Conv2DTranspose seems to be invalid. 
It can be changed from https://www.matthewzeiler.com/pubs/cvpr2010/cvpr2010.pdf to https://www.matthewzeiler.com/mattzeiler/deconvolutionalnetworks.pdf."
keras,5617,"Please make sure that the boxes below are checked before you submit your issue. If your issue is an implementation question, please ask your question on [StackOverflow](http://stackoverflow.com/questions/tagged/keras) or [join the Keras Slack channel](https://keras-slack-autojoin.herokuapp.com/) and ask there instead of filing a GitHub issue.

Thank you!

- [x] Check that you are up-to-date with the master branch of Keras. You can update with:
pip install git+git://github.com/fchollet/keras.git --upgrade --no-deps

- [x] If running on TensorFlow, check that you are up-to-date with the latest version. The installation instructions can be found [here](https://www.tensorflow.org/get_started/os_setup).

- [x] If running on Theano, check that you are up-to-date with the master branch of Theano. You can update with:
pip install git+git://github.com/Theano/Theano.git --upgrade --no-deps

- [x] Provide a link to a GitHub Gist of a Python script that can reproduce your issue (or just copy the script here if it is short).

I need the LSTM to generate words at each timestep which is given as input to the next timestep. Can this be done in keras?
",0,Obtain output at each timestep,"Obtain output at each timestep Please make sure that the boxes below are checked before you submit your issue. If your issue is an implementation question, please ask your question on [StackOverflow](http://stackoverflow.com/questions/tagged/keras) or [join the Keras Slack channel](https://keras-slack-autojoin.herokuapp.com/) and ask there instead of filing a GitHub issue.

Thank you!

- [x] Check that you are up-to-date with the master branch of Keras. You can update with:
pip install git+git://github.com/fchollet/keras.git --upgrade --no-deps

- [x] If running on TensorFlow, check that you are up-to-date with the latest version. The installation instructions can be found [here](https://www.tensorflow.org/get_started/os_setup).

- [x] If running on Theano, check that you are up-to-date with the master branch of Theano. You can update with:
pip install git+git://github.com/Theano/Theano.git --upgrade --no-deps

- [x] Provide a link to a GitHub Gist of a Python script that can reproduce your issue (or just copy the script here if it is short).

I need the LSTM to generate words at each timestep which is given as input to the next timestep. Can this be done in keras?
"
keras,5164,"@fchollet I came across something to make life better. There are a handful of tools out there to automatically close issues if they have been abandoned. Here is one example but there are plenty of options.

https://github.com/twbs/no-carrier

A configuration like this might be reasonable:
* Send a warning at 2, 4 and 6 weeks.
* Close after no activity for 8 weeks (other than the warnings), making sure to mark the resolution as something like ""automatic closure"" so it doesn't get confused with things that are manually closed. It would be easy to browse the automatic closures if you wanted to.
* If something gets closed by accident, someone could always reopen it
* Make some exceptions for any issues you want to pin and keep active.

There are currently 1,945 issues (not including this one) and rising. Does this sound like something reasonable to implement?

Cheers",0,Automatically closing abandoned issues,"Automatically closing abandoned issues @fchollet I came across something to make life better. There are a handful of tools out there to automatically close issues if they have been abandoned. Here is one example but there are plenty of options.

https://github.com/twbs/no-carrier

A configuration like this might be reasonable:
* Send a warning at 2, 4 and 6 weeks.
* Close after no activity for 8 weeks (other than the warnings), making sure to mark the resolution as something like ""automatic closure"" so it doesn't get confused with things that are manually closed. It would be easy to browse the automatic closures if you wanted to.
* If something gets closed by accident, someone could always reopen it
* Make some exceptions for any issues you want to pin and keep active.

There are currently 1,945 issues (not including this one) and rising. Does this sound like something reasonable to implement?

Cheers"
keras,4982,"Would it be possible to support images with more channels than just 3 or 1. I'm currently working on a project where I have images with 8 channels, and currently I have to truncate all but 3 of the channels. Any way around this?",0,Multichannel Images,"Multichannel Images Would it be possible to support images with more channels than just 3 or 1. I'm currently working on a project where I have images with 8 channels, and currently I have to truncate all but 3 of the channels. Any way around this?"
keras,1625,"I've been using this callback for stopping training after a fixed period of time, useful when testing different architectures consecutively. Is it worth contributing?

One thing I noticed was  only stops the model after an epoch, but with some training where the epochs are very long you might want to stop after a batch.


",0,TimeStopping callback,"TimeStopping callback I've been using this callback for stopping training after a fixed period of time, useful when testing different architectures consecutively. Is it worth contributing?

One thing I noticed was  only stops the model after an epoch, but with some training where the epochs are very long you might want to stop after a batch.


"
keras,5896,"I'm using Ubuntu 16.04, Python 3.5.2, Keras 2.0.1 and Tensorflow 1.01. 

Keras/tensorflow crash when using threads. The following code is a simplified version of what I am trying to do but recreates the crash.

https://gist.github.com/eyesonlyhack/c43dea734f872a9c45da8587eefec581

I found a way to get around this issue but think this is probably a bug in Keras. My workaround is:
https://gist.github.com/eyesonlyhack/2f0b20f1e73aaf5e9b83f49415f3601a

",0,Cannot use Keras in threads,"Cannot use Keras in threads I'm using Ubuntu 16.04, Python 3.5.2, Keras 2.0.1 and Tensorflow 1.01. 

Keras/tensorflow crash when using threads. The following code is a simplified version of what I am trying to do but recreates the crash.

https://gist.github.com/eyesonlyhack/c43dea734f872a9c45da8587eefec581

I found a way to get around this issue but think this is probably a bug in Keras. My workaround is:
https://gist.github.com/eyesonlyhack/2f0b20f1e73aaf5e9b83f49415f3601a

"
keras,4740,"I try to run sequential with Keras; I found that the thread running sequential model generated about 13 sub-threads. How to control the number of sub-threads? 

part of my code: 
model = Sequential()
model.add(...)
.....
model.compile(loss='binary_crossentropy', optimizer=params['optimizer'])
model.fit(.....)

Please help me with this problem. Thanks in advance!",0,How to control the number of threads when running sequential with Keras,"How to control the number of threads when running sequential with Keras I try to run sequential with Keras; I found that the thread running sequential model generated about 13 sub-threads. How to control the number of sub-threads? 

part of my code: 
model = Sequential()
model.add(...)
.....
model.compile(loss='binary_crossentropy', optimizer=params['optimizer'])
model.fit(.....)

Please help me with this problem. Thanks in advance!"
keras,3486,"merge, Not Merge

> > > merge([forwards, backwards], mode='max')
> > > Traceback (most recent call last):
> > >   File ""<stdin>"", line 1, in <module>
> > >   File ""/home/job/analyse/env/lib/python2.7/site-packages/keras/engine/topology.py"", line 1490, in merge
> > >     name=name)
> > >   File ""/home/job/analyse/env/lib/python2.7/site-packages/keras/engine/topology.py"", line 1148, in **init**
> > >     self.add_inbound_node(layers, node_indices, tensor_indices)
> > >   File ""/home/job/analyse/env/lib/python2.7/site-packages/keras/engine/topology.py"", line 543, in add_inbound_node
> > >     Node.create_node(self, inbound_layers, node_indices, tensor_indices)
> > >   File ""/home/job/analyse/env/lib/python2.7/site-packages/keras/engine/topology.py"", line 154, in create_node
> > >     output_masks = to_list(outbound_layer.compute_mask(input_tensors, input_masks))
> > >   File ""/home/job/analyse/env/lib/python2.7/site-packages/keras/engine/topology.py"", line 1372, in compute_mask
> > >     raise Exception('Invalid merge mode: {}'.format(self.mode))
> > > Exception: Invalid merge mode: max
",0,max mode can not be used in merge function ? Why ?,"max mode can not be used in merge function ? Why ? merge, Not Merge

> > > merge([forwards, backwards], mode='max')
> > > Traceback (most recent call last):
> > >   File ""<stdin>"", line 1, in <module>
> > >   File ""/home/job/analyse/env/lib/python2.7/site-packages/keras/engine/topology.py"", line 1490, in merge
> > >     name=name)
> > >   File ""/home/job/analyse/env/lib/python2.7/site-packages/keras/engine/topology.py"", line 1148, in **init**
> > >     self.add_inbound_node(layers, node_indices, tensor_indices)
> > >   File ""/home/job/analyse/env/lib/python2.7/site-packages/keras/engine/topology.py"", line 543, in add_inbound_node
> > >     Node.create_node(self, inbound_layers, node_indices, tensor_indices)
> > >   File ""/home/job/analyse/env/lib/python2.7/site-packages/keras/engine/topology.py"", line 154, in create_node
> > >     output_masks = to_list(outbound_layer.compute_mask(input_tensors, input_masks))
> > >   File ""/home/job/analyse/env/lib/python2.7/site-packages/keras/engine/topology.py"", line 1372, in compute_mask
> > >     raise Exception('Invalid merge mode: {}'.format(self.mode))
> > > Exception: Invalid merge mode: max
"
keras,4862,"It looks like commit https://github.com/fchollet/keras/commit/2a3d4722c21d99d882b2cbc2da451108147fe1c4 introduced a bug in (at least) .  

Rolling back the commit fixes this problem, but I'm not sure whether this breaks something somewhere else.",0,`K.int_shape` bug,"`K.int_shape` bug It looks like commit https://github.com/fchollet/keras/commit/2a3d4722c21d99d882b2cbc2da451108147fe1c4 introduced a bug in (at least) .  

Rolling back the commit fixes this problem, but I'm not sure whether this breaks something somewhere else."
keras,6412,"Hello I am still exploring keras in the past few months so I am new here. I like it so much thanks for @fchollet  . I just have 3 problem now.
1) How we can create new loss function?[better with example]
In here for example I have 4 pairs of coordinate  which allocated in 1x8 array as the output of network. I want to create euclidean loos function which compare  and  so I will have 4 distance as the result comparing every point in the array. The problem is I just know that I must use Keras Backend operation. Now the proble is how can I iterate through the tensor? for example I am taking the mean of all 4 distance:









It still have error, I know maybe because of tensor we must use backend operation, but I dont know what to do.
2) How can we create new metrics, for example we define our custom metric of accuracy? like in my case 
3) How we monitor using new metrics and save the best weight using our custom metrics?
Any suggestion is welcomed. Thank you.",0,New Loss function and Metric to monitor best weights,"New Loss function and Metric to monitor best weights Hello I am still exploring keras in the past few months so I am new here. I like it so much thanks for @fchollet  . I just have 3 problem now.
1) How we can create new loss function?[better with example]
In here for example I have 4 pairs of coordinate  which allocated in 1x8 array as the output of network. I want to create euclidean loos function which compare  and  so I will have 4 distance as the result comparing every point in the array. The problem is I just know that I must use Keras Backend operation. Now the proble is how can I iterate through the tensor? for example I am taking the mean of all 4 distance:









It still have error, I know maybe because of tensor we must use backend operation, but I dont know what to do.
2) How can we create new metrics, for example we define our custom metric of accuracy? like in my case 
3) How we monitor using new metrics and save the best weight using our custom metrics?
Any suggestion is welcomed. Thank you."
keras,1872,"I saved the model and weights after each epoch using callbacks.ModelCheckpoint. I want to train it again from the last epoch.
How to set the model.fit() command to start from the previous epoch?
",0,resume training from previous epoch,"resume training from previous epoch I saved the model and weights after each epoch using callbacks.ModelCheckpoint. I want to train it again from the last epoch.
How to set the model.fit() command to start from the previous epoch?
"
keras,4869,"Can you please advice how to to do Object Detection using Keras?
",0,Object Detection using Keras,"Object Detection using Keras Can you please advice how to to do Object Detection using Keras?
"
keras,4479,"
before predicting give 


to different vectors",0,Model gives same answers,"Model gives same answers 
before predicting give 


to different vectors"
keras,1643,"Hello, 

I am actually new to python, 

I'm wondering is there some way to use taps in Keras?

E.g. say, I want to compute,  i_t = T.dot(W_xi, x_t) + T.dot (W_hi1, h_tm1) + T.dot(W_hi2,h_tm2)

h_tm1 :h(t-1)
h_tm2 : h(t-2)

It's quite straightforward in theano, just use output_info, and specify the taps.
",0,"Help: Is there any way to set up taps for scan op in Keras, for LSTM application","Help: Is there any way to set up taps for scan op in Keras, for LSTM application Hello, 

I am actually new to python, 

I'm wondering is there some way to use taps in Keras?

E.g. say, I want to compute,  i_t = T.dot(W_xi, x_t) + T.dot (W_hi1, h_tm1) + T.dot(W_hi2,h_tm2)

h_tm1 :h(t-1)
h_tm2 : h(t-2)

It's quite straightforward in theano, just use output_info, and specify the taps.
"
keras,8642,can we use different sizes of strides in max pooling other than 2x2 . if we use other than that is there any problem or changes happen to the model like accuracy prediction changes?,0,can we use other than 2x2 strides size in maxpooling in model?,can we use other than 2x2 strides size in maxpooling in model? can we use different sizes of strides in max pooling other than 2x2 . if we use other than that is there any problem or changes happen to the model like accuracy prediction changes?
keras,1103,"I can't figure out how to fix the Travis config to install TF and run the tests with the TF backend (Python 2.7 only). Any Travis experts up for the challenge? 

See the  branch: https://github.com/fchollet/keras/blob/backend/.travis.yml
",0,Fixing the Travis config for TensorFlow,"Fixing the Travis config for TensorFlow I can't figure out how to fix the Travis config to install TF and run the tests with the TF backend (Python 2.7 only). Any Travis experts up for the challenge? 

See the  branch: https://github.com/fchollet/keras/blob/backend/.travis.yml
"
keras,4965,"I'm using latest versions of Keras & Theano on a MacBook Pro with discrete AMD graphics. It works fine on CPU, but I'm getting this error when I try to use OpenCL backend:

(the description of the exception is >30000 characters long so I removed it; let me know if it's useful)

The code:
",0,pygpu.gpuarray.GpuArrayException on Theano OpenCL backend,"pygpu.gpuarray.GpuArrayException on Theano OpenCL backend I'm using latest versions of Keras & Theano on a MacBook Pro with discrete AMD graphics. It works fine on CPU, but I'm getting this error when I try to use OpenCL backend:

(the description of the exception is >30000 characters long so I removed it; let me know if it's useful)

The code:
"
keras,3866,"I have developed an ML app on my local machine and now wanted to do the deployment on AWS EC2 (CPU instance only at this time as I talk about a predict flow)

After installation of all packages and specifically Keras and Theano backend, I am able to call under Python (Ubuntu 14:04  - Python version 2.7.6) all packages nicely.

So, as example, when I interactively do call up Python and then import keras, I get the standard print 'Using Theano backend.' as desired and set in the keras.json file

Now, I installed flask server and and apache2 server working with wsgi package.
I test this server out with several types of routes and returns and all is working nicley.

my flask file is looking essentially like



my predict.py file is in the python directory and looks like



so, I start my apache server and look at the logs and the logs tell me 
'Using TensorFlow backend.' which is not what I want.

Can anybody give me a hint why this is happening
many thanks
Peter
",0,Keras on AWS EC2,"Keras on AWS EC2 I have developed an ML app on my local machine and now wanted to do the deployment on AWS EC2 (CPU instance only at this time as I talk about a predict flow)

After installation of all packages and specifically Keras and Theano backend, I am able to call under Python (Ubuntu 14:04  - Python version 2.7.6) all packages nicely.

So, as example, when I interactively do call up Python and then import keras, I get the standard print 'Using Theano backend.' as desired and set in the keras.json file

Now, I installed flask server and and apache2 server working with wsgi package.
I test this server out with several types of routes and returns and all is working nicley.

my flask file is looking essentially like



my predict.py file is in the python directory and looks like



so, I start my apache server and look at the logs and the logs tell me 
'Using TensorFlow backend.' which is not what I want.

Can anybody give me a hint why this is happening
many thanks
Peter
"
keras,4168,"Is it possible to get the training set predictions of the model at the end of each epoch from inside the function on_epoch_end at the end of a callback without having to call self.model.predict() every time, since the model already computed them?
",0,Get current epoch predictions from inside callback (on_epoch_end) ,"Get current epoch predictions from inside callback (on_epoch_end)  Is it possible to get the training set predictions of the model at the end of each epoch from inside the function on_epoch_end at the end of a callback without having to call self.model.predict() every time, since the model already computed them?
"
keras,1556,"e.g. https://travis-ci.org/fchollet/keras/jobs/104864770


",0,Travis CI tests are failing due to connection errors to data (possible aws problems?),"Travis CI tests are failing due to connection errors to data (possible aws problems?) e.g. https://travis-ci.org/fchollet/keras/jobs/104864770


"
keras,649,"Just noticed that there is no tests for Convolution1D, this is a side effect of not having code coverage metrics.
",0,No test coverage for Keras,"No test coverage for Keras Just noticed that there is no tests for Convolution1D, this is a side effect of not having code coverage metrics.
"
keras,5792,"Hello, 

I'm wondering if the line 411-413 is correct ?
https://github.com/fchollet/keras/blob/master/keras/layers/local.py#L412-L413

output = K.reshape(output, (self.output_row, self.output_col, -1, filters))

output is of dimension :  output_row x output_col x batch_size x filters

output = K.permute_dimensions(output, (2, 0, 1, 3))

should this be (1,2,0,3) ?

Thanks !
",0,small bugs in local.py on LocallyConnected2D?,"small bugs in local.py on LocallyConnected2D? Hello, 

I'm wondering if the line 411-413 is correct ?
https://github.com/fchollet/keras/blob/master/keras/layers/local.py#L412-L413

output = K.reshape(output, (self.output_row, self.output_col, -1, filters))

output is of dimension :  output_row x output_col x batch_size x filters

output = K.permute_dimensions(output, (2, 0, 1, 3))

should this be (1,2,0,3) ?

Thanks !
"
keras,131,"

Fix is probably just change  to  in  line 188.
",0,model.fit(shuffle=False) gives TypeError,"model.fit(shuffle=False) gives TypeError 

Fix is probably just change  to  in  line 188.
"
keras,7453,"A deep very deep model (200 layers) is giving me this error:

",0,"Node index for deep model, recursion depth exceeded","Node index for deep model, recursion depth exceeded A deep very deep model (200 layers) is giving me this error:

"
keras,4010,"Hi,
I'm trying to model a stateful LSTM for TTS system. I've added-


However, I'm getting  with the following setup-



Any ideas? Thanks!
",0,Stateful LSTM: dimensions using input_batch_shape?,"Stateful LSTM: dimensions using input_batch_shape? Hi,
I'm trying to model a stateful LSTM for TTS system. I've added-


However, I'm getting  with the following setup-



Any ideas? Thanks!
"
keras,12724,"**System information**  
- Have I written custom code (as opposed to using example directory):  Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Ubuntu 18.04
- TensorFlow backend (yes / no):   yes
- TensorFlow version:  1.13.1
- Keras version:  2.4.4 (bug exists on latest master as well)
- Python version:  3.6.8
- CUDA/cuDNN version:  not using cuda for this
- GPU model and memory:  not using cuda for this

**Describe the current behavior**  
If I set all the random seeds, use only the CPU, disable CPU multiprocessing, and I run the same experiment 10 times, the loss of the second  call comes out as one of two different values each time.

**Describe the expected behavior**  
The loss should be bit-perfect reproducible in this situation.

**Code to reproduce the issue**  

Check the reproducibility with the following command:

    for i in  ; do python3 code_example.py 2>/dev/null | tail -n 1 ; done

Result:


**Other info / logs**


There are a variety of things which affect whether or not the training is bit-perfect reproducible:
 - The loss is always consistent after the first .  Training for three steps exhibits identical behavior to training for two steps (all losses come out to one of two values)
 - Commenting out various layers restores reproducibility (see comments in code example)
 - Eliminating the decay from  optimizer (or using  optimizer with decay) restores reproducibility
 - Metric selection affects reproducibility.  Some metrics break reproducibility when included twice, one metric,  breaks reproducibility when included at all.
 - I noticed no change in behavior when I used the git master branch.
 - Inserting a  into the graph, while using the RMSprop with decay, will restore reproducibility.  Patch:

",0,Keras appears to generate non-deterministic tensorflow graph,"Keras appears to generate non-deterministic tensorflow graph **System information**  
- Have I written custom code (as opposed to using example directory):  Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Ubuntu 18.04
- TensorFlow backend (yes / no):   yes
- TensorFlow version:  1.13.1
- Keras version:  2.4.4 (bug exists on latest master as well)
- Python version:  3.6.8
- CUDA/cuDNN version:  not using cuda for this
- GPU model and memory:  not using cuda for this

**Describe the current behavior**  
If I set all the random seeds, use only the CPU, disable CPU multiprocessing, and I run the same experiment 10 times, the loss of the second  call comes out as one of two different values each time.

**Describe the expected behavior**  
The loss should be bit-perfect reproducible in this situation.

**Code to reproduce the issue**  

Check the reproducibility with the following command:

    for i in  ; do python3 code_example.py 2>/dev/null | tail -n 1 ; done

Result:


**Other info / logs**


There are a variety of things which affect whether or not the training is bit-perfect reproducible:
 - The loss is always consistent after the first .  Training for three steps exhibits identical behavior to training for two steps (all losses come out to one of two values)
 - Commenting out various layers restores reproducibility (see comments in code example)
 - Eliminating the decay from  optimizer (or using  optimizer with decay) restores reproducibility
 - Metric selection affects reproducibility.  Some metrics break reproducibility when included twice, one metric,  breaks reproducibility when included at all.
 - I noticed no change in behavior when I used the git master branch.
 - Inserting a  into the graph, while using the RMSprop with decay, will restore reproducibility.  Patch:

"
keras,5880,"
",0,"Hello, Can I  ask you how to build a 3D Convolution Autoencoder?","Hello, Can I  ask you how to build a 3D Convolution Autoencoder? 
"
keras,11881,"Hi,
I wanted to train a U-net, and everything works fine on my laptop with the tensorflow CPU version. However, on the cluster with a GPU, I can create and compile it, but when I call the  method I get the following error:

    Traceback (most recent call last):
      File ""Cluster.py"", line 46, in <module>
    unet.fit_generator(gen, steps_per_epoch=gen.getStepsPerEpoch(), epochs=101)
      File ""/home/exacloud/tempwork/ChangLab/Guillaume/Python/lib/python3.5/site-packages/keras/legacy/interfaces.py"", line 91, in wrapper
    return func(*args, **kwargs)
      File ""/home/exacloud/tempwork/ChangLab/Guillaume/Python/lib/python3.5/site-packages/keras/engine/training.py"", line 1418, in fit_generator
    initial_epoch=initial_epoch)
      File ""/home/exacloud/tempwork/ChangLab/Guillaume/Python/lib/python3.5/site-packages/keras/engine/training_generator.py"", line 40, in fit_generator
    model._make_train_function()
      File ""/home/exacloud/tempwork/ChangLab/Guillaume/Python/lib/python3.5/site-packages/keras/engine/training.py"", line 509, in _make_train_function
    loss=self.total_loss)
      File ""/home/exacloud/tempwork/ChangLab/Guillaume/Python/lib/python3.5/site-packages/keras/legacy/interfaces.py"", line 91, in wrapper
    return func(*args, **kwargs)
      File ""/home/exacloud/tempwork/ChangLab/Guillaume/Python/lib/python3.5/site-packages/keras/optimizers.py"", line 410, in get_updates
    self.updates.append(K.update(a, new_a))
      File ""/home/exacloud/tempwork/ChangLab/Guillaume/Python/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py"", line 973, in update
    return tf.assign(x, new_x)
      File ""/home/exacloud/tempwork/ChangLab/Guillaume/Conda/DL35/lib/python3.5/site-packages/tensorflow/python/ops/state_ops.py"", line 277, in assign
    return ref.assign(value)
    AttributeError: 'Tensor' object has no attribute 'assign'

So the exact same code runs perfectly on my laptop on the cpu only version of Tensorflow, but not on the GPU one.

Any idea what the issue may be?

Thanks in advance!",0,Code runs fine with CPU but AttributeError with GPU,"Code runs fine with CPU but AttributeError with GPU Hi,
I wanted to train a U-net, and everything works fine on my laptop with the tensorflow CPU version. However, on the cluster with a GPU, I can create and compile it, but when I call the  method I get the following error:

    Traceback (most recent call last):
      File ""Cluster.py"", line 46, in <module>
    unet.fit_generator(gen, steps_per_epoch=gen.getStepsPerEpoch(), epochs=101)
      File ""/home/exacloud/tempwork/ChangLab/Guillaume/Python/lib/python3.5/site-packages/keras/legacy/interfaces.py"", line 91, in wrapper
    return func(*args, **kwargs)
      File ""/home/exacloud/tempwork/ChangLab/Guillaume/Python/lib/python3.5/site-packages/keras/engine/training.py"", line 1418, in fit_generator
    initial_epoch=initial_epoch)
      File ""/home/exacloud/tempwork/ChangLab/Guillaume/Python/lib/python3.5/site-packages/keras/engine/training_generator.py"", line 40, in fit_generator
    model._make_train_function()
      File ""/home/exacloud/tempwork/ChangLab/Guillaume/Python/lib/python3.5/site-packages/keras/engine/training.py"", line 509, in _make_train_function
    loss=self.total_loss)
      File ""/home/exacloud/tempwork/ChangLab/Guillaume/Python/lib/python3.5/site-packages/keras/legacy/interfaces.py"", line 91, in wrapper
    return func(*args, **kwargs)
      File ""/home/exacloud/tempwork/ChangLab/Guillaume/Python/lib/python3.5/site-packages/keras/optimizers.py"", line 410, in get_updates
    self.updates.append(K.update(a, new_a))
      File ""/home/exacloud/tempwork/ChangLab/Guillaume/Python/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py"", line 973, in update
    return tf.assign(x, new_x)
      File ""/home/exacloud/tempwork/ChangLab/Guillaume/Conda/DL35/lib/python3.5/site-packages/tensorflow/python/ops/state_ops.py"", line 277, in assign
    return ref.assign(value)
    AttributeError: 'Tensor' object has no attribute 'assign'

So the exact same code runs perfectly on my laptop on the cpu only version of Tensorflow, but not on the GPU one.

Any idea what the issue may be?

Thanks in advance!"
keras,3518,"Sometimes I need to set a threshold for early stopping. The threshold is the minimum amount of improvement as considered to be new maximum performance. 

Please add a comment if you think you would need it, too!
",0,Survey: do we need a threshold in Early stopping?,"Survey: do we need a threshold in Early stopping? Sometimes I need to set a threshold for early stopping. The threshold is the minimum amount of improvement as considered to be new maximum performance. 

Please add a comment if you think you would need it, too!
"
keras,2801,"I'm doing a lambda layer in which I'd like to split a tensor into two (so the opposite of K.concatenate, essentially) to perform some different operations on the two parts, before concatenating them again. Any thoughts on how to split with the Keras backend?


",0,Split tensor,"Split tensor I'm doing a lambda layer in which I'd like to split a tensor into two (so the opposite of K.concatenate, essentially) to perform some different operations on the two parts, before concatenating them again. Any thoughts on how to split with the Keras backend?


"
keras,9941,"Hello, 
I've created a layer as follows : 
from keras import backend as K
from keras.engine.topology import Layer
import numpy as np
import cv2

class MedianLayer(Layer):

    def __init__(self, output_dim, input_shape, **kwargs):
        self.output_dim = output_dim
        super(MedianLayer, self).__init__(**kwargs)
    def build(self, input_shape):
        # Create a trainable weight variable for this layer.
        self.kernel = self.add_weight(name='kernel', 
                                      shape=(input_shape[1], self.output_dim),
                                      initializer='uniform',
                                      trainable=True)
        super(MedianLayer, self).build(input_shape)  # Be sure to call this somewhere!
    def call(self, x):
        return cv2.medianBlur(x,5) - x

    def compute_output_shape(self, input_shape):
        return (input_shape[0], self.output_dim)
===================================================================
After this, I do the following: 
k = Sequential()
layer = MedianLayer(output_dim=(100,100), input_shape=(100,100))
lc = layer.get_config()
x = np.random.randn(100, 100)
lc[""input_shape""] = x.shape
lc[""output_dim""] = x.shape
print(lc)
This prints : 
{'name': 'median_layer_3', 'trainable': True, 'input_shape': (100, 100), 'output_dim': (100, 100)}
 AS expected. 
==================================================================
Now, when I do : 
# Instantiate a new layer object from old config
layer1 = layer.from_config(lc)
print(layer1.get_config())
print(lc)
I get the following : 
{'name': 'median_layer_3', 'trainable': True}  # New layer config
{'name': 'median_layer_3', 'trainable': True, 'input_shape': (100, 100), 'output_dim': (100, 100)} # old layer config, used as input for re-instantiation.

The re-instantiation of another layer object using an existing layer config is failing - any clues highly appreciated. 
Thank you. 
Kumar",0,New layer instantiation from config failing. ,"New layer instantiation from config failing.  Hello, 
I've created a layer as follows : 
from keras import backend as K
from keras.engine.topology import Layer
import numpy as np
import cv2

class MedianLayer(Layer):

    def __init__(self, output_dim, input_shape, **kwargs):
        self.output_dim = output_dim
        super(MedianLayer, self).__init__(**kwargs)
    def build(self, input_shape):
        # Create a trainable weight variable for this layer.
        self.kernel = self.add_weight(name='kernel', 
                                      shape=(input_shape[1], self.output_dim),
                                      initializer='uniform',
                                      trainable=True)
        super(MedianLayer, self).build(input_shape)  # Be sure to call this somewhere!
    def call(self, x):
        return cv2.medianBlur(x,5) - x

    def compute_output_shape(self, input_shape):
        return (input_shape[0], self.output_dim)
===================================================================
After this, I do the following: 
k = Sequential()
layer = MedianLayer(output_dim=(100,100), input_shape=(100,100))
lc = layer.get_config()
x = np.random.randn(100, 100)
lc[""input_shape""] = x.shape
lc[""output_dim""] = x.shape
print(lc)
This prints : 
{'name': 'median_layer_3', 'trainable': True, 'input_shape': (100, 100), 'output_dim': (100, 100)}
 AS expected. 
==================================================================
Now, when I do : 
# Instantiate a new layer object from old config
layer1 = layer.from_config(lc)
print(layer1.get_config())
print(lc)
I get the following : 
{'name': 'median_layer_3', 'trainable': True}  # New layer config
{'name': 'median_layer_3', 'trainable': True, 'input_shape': (100, 100), 'output_dim': (100, 100)} # old layer config, used as input for re-instantiation.

The re-instantiation of another layer object using an existing layer config is failing - any clues highly appreciated. 
Thank you. 
Kumar"
keras,12986,"Does Keras provide a way to get the output of a nested model? Or is there a way to flatten the nested model? 


For the model architecture below, how can I 'get layer1' or 'layer2'? When I compile m2, I only see 'layer3' and the TimeDistributed layer.

**System information**  
- Have I written custom code (as opposed to using example directory):  No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Mac OS
- TensorFlow backend (yes / no):  yes
- TensorFlow version:  b'v1.13.0-rc2-5-g6612da8951' 1.13.1
- Keras version:  2.2.4
- Python version:  3.6.5
- CUDA/cuDNN version: no  
- GPU model and memory: no ",0,How to Obtain Layer Output of Nested Model in Keras,"How to Obtain Layer Output of Nested Model in Keras Does Keras provide a way to get the output of a nested model? Or is there a way to flatten the nested model? 


For the model architecture below, how can I 'get layer1' or 'layer2'? When I compile m2, I only see 'layer3' and the TimeDistributed layer.

**System information**  
- Have I written custom code (as opposed to using example directory):  No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Mac OS
- TensorFlow backend (yes / no):  yes
- TensorFlow version:  b'v1.13.0-rc2-5-g6612da8951' 1.13.1
- Keras version:  2.2.4
- Python version:  3.6.5
- CUDA/cuDNN version: no  
- GPU model and memory: no "
keras,1309,"Hi,

I added BatchNormalization layers to my model and it suddenly took much more time to train. 

It takes 561 seconds for one epoch with them:
Epoch 1/1
4096/4096 [==============================] - 561s - loss: 0.0946 - acc: 0.9006

This is if I comment out the BatchNorm layers (186 seconds):
Epoch 1/1
4096/4096 [==============================] - 186s - loss: 4.5043 - acc: 0.5933

I wrote on the user group and someone informed me that adding Dropout slows it down further, but even without Dropout this is much slower (above 500 seconds too).

Is it a CPU-related issue? Or is such a slowdown expected? Below is my model.


",0,Slow BatchNormalization layers (CPU tested),"Slow BatchNormalization layers (CPU tested) Hi,

I added BatchNormalization layers to my model and it suddenly took much more time to train. 

It takes 561 seconds for one epoch with them:
Epoch 1/1
4096/4096 [==============================] - 561s - loss: 0.0946 - acc: 0.9006

This is if I comment out the BatchNorm layers (186 seconds):
Epoch 1/1
4096/4096 [==============================] - 186s - loss: 4.5043 - acc: 0.5933

I wrote on the user group and someone informed me that adding Dropout slows it down further, but even without Dropout this is much slower (above 500 seconds too).

Is it a CPU-related issue? Or is such a slowdown expected? Below is my model.


"
keras,6243,"Since upgrading to Keras 2 and latest Theano (last updated ~Jan '17), the train_on_batch() call now takes a ~10 second pause after approximately the first 10 batches for Convolutional layers.  Is there a reason for this pause, and is there a way to get around it?",0,"Inefficient, newly introduced pause in train_on_batch()","Inefficient, newly introduced pause in train_on_batch() Since upgrading to Keras 2 and latest Theano (last updated ~Jan '17), the train_on_batch() call now takes a ~10 second pause after approximately the first 10 batches for Convolutional layers.  Is there a reason for this pause, and is there a way to get around it?"
keras,6849,"I'm trying to use Keras' implementation of VGG16 with grayscale images, channels first.
My training data is shaped as follows:



When I build my model:



it throws this error:

input_shape=' + str(input_shape) + 'input_shape=(1, 128, 128)

This seems odd since I am using  and  but it still seems to expect a 3 channel tensor and is passing my shape through , which seems inflexible in allowing for anything < 3 channels (see ).

Is this desirable?

If I keep  then I get a shape mismatch error at the beginning of training epoch 1.

Keras 2.0.4
Python 3.5",0,Error when checking VGG16 input shape with include_top=False,"Error when checking VGG16 input shape with include_top=False I'm trying to use Keras' implementation of VGG16 with grayscale images, channels first.
My training data is shaped as follows:



When I build my model:



it throws this error:

input_shape=' + str(input_shape) + 'input_shape=(1, 128, 128)

This seems odd since I am using  and  but it still seems to expect a 3 channel tensor and is passing my shape through , which seems inflexible in allowing for anything < 3 channels (see ).

Is this desirable?

If I keep  then I get a shape mismatch error at the beginning of training epoch 1.

Keras 2.0.4
Python 3.5"
keras,6893,"If I run the keras/examples/babi_rnn.py script
using: 


it runs fine, from there are wanted to export the model according to your example code found on https://blog.keras.io/keras-as-a-simplified-interface-to-tensorflow-tutorial.html#exporting-a-model-with-tensorflow-serving : 



If I do that I get the following error. Do you know what's going wrong?
...
",0,Error exporting model from keras/examples/babi_rnn.py,"Error exporting model from keras/examples/babi_rnn.py If I run the keras/examples/babi_rnn.py script
using: 


it runs fine, from there are wanted to export the model according to your example code found on https://blog.keras.io/keras-as-a-simplified-interface-to-tensorflow-tutorial.html#exporting-a-model-with-tensorflow-serving : 



If I do that I get the following error. Do you know what's going wrong?
...
"
keras,1854,"I am getting an error when i am trying to change those argument inside a LSTM function 
Did they always exist or did the typo changed ? Because they are on the keras documentation. Thanks!
",0,LSTM argument,"LSTM argument I am getting an error when i am trying to change those argument inside a LSTM function 
Did they always exist or did the typo changed ? Because they are on the keras documentation. Thanks!
"
keras,11410,"Getting this error when I try to load resnet50:



And when I try to load xception i get this:

input_shape=(48, 48, 3)

Please check the code sample [here](https://gist.github.com/hasibzunair/606c9037abed51836e5dc6059141bfff)",0,ConnectionResetError: [Errno 104] Connection reset by peer,"ConnectionResetError: [Errno 104] Connection reset by peer Getting this error when I try to load resnet50:



And when I try to load xception i get this:

input_shape=(48, 48, 3)

Please check the code sample [here](https://gist.github.com/hasibzunair/606c9037abed51836e5dc6059141bfff)"
keras,2293,"master branch seems not compatible with previous version's model dump file, when using json read the old model file, there's a bug
",0,master branch seems not compatible ,"master branch seems not compatible  master branch seems not compatible with previous version's model dump file, when using json read the old model file, there's a bug
"
keras,11673,"The current [documentation on callbacks](https://keras.io/callbacks/) isn't showing bullet points correctly under the ""Arguments"" section of a few models. Here's the example for :

> filepath: string, path to save the model file. monitor: quantity to monitor. verbose: verbosity mode, 0 or 1. save_best_only: if save_best_only=True, the latest best model according to the quantity monitored will not be overwritten. mode: one of {auto, min, max}. If save_best_only=True, the decision to overwrite the current save file is made based on either the maximization or the minimization of the monitored quantity. For val_acc, this should be max, for val_loss this should be min, etc. In auto mode, the direction is automatically inferred from the name of the monitored quantity. save_weights_only: if True, then only the model's weights will be saved (model.save_weights(filepath)), else the full model is saved (model.save(filepath)). period: Interval (number of epochs) between checkpoints.

Looking at the source code, the docstring seems to be organized correctly:
https://github.com/keras-team/keras/blob/dc9e510192d0a8a6f6943cd46e9554364d4dcdd2/keras/callbacks.py#L371-L390

It is however showing up correctly for other models, e.g. :

> # Arguments
> * __count_mode:__ One of ""steps"" or ""samples"". Whether the progress bar should count samples seen or steps (batches) seen.
> * __stateful_metrics:__ Iterable of string names of metrics that should not be averaged over an epoch. Metrics in this list will be logged as-is. All others will be averaged over time (e.g. loss, etc).",0,Callbacks documentation not showing bullet points correctly,"Callbacks documentation not showing bullet points correctly The current [documentation on callbacks](https://keras.io/callbacks/) isn't showing bullet points correctly under the ""Arguments"" section of a few models. Here's the example for :

> filepath: string, path to save the model file. monitor: quantity to monitor. verbose: verbosity mode, 0 or 1. save_best_only: if save_best_only=True, the latest best model according to the quantity monitored will not be overwritten. mode: one of {auto, min, max}. If save_best_only=True, the decision to overwrite the current save file is made based on either the maximization or the minimization of the monitored quantity. For val_acc, this should be max, for val_loss this should be min, etc. In auto mode, the direction is automatically inferred from the name of the monitored quantity. save_weights_only: if True, then only the model's weights will be saved (model.save_weights(filepath)), else the full model is saved (model.save(filepath)). period: Interval (number of epochs) between checkpoints.

Looking at the source code, the docstring seems to be organized correctly:
https://github.com/keras-team/keras/blob/dc9e510192d0a8a6f6943cd46e9554364d4dcdd2/keras/callbacks.py#L371-L390

It is however showing up correctly for other models, e.g. :

> # Arguments
> * __count_mode:__ One of ""steps"" or ""samples"". Whether the progress bar should count samples seen or steps (batches) seen.
> * __stateful_metrics:__ Iterable of string names of metrics that should not be averaged over an epoch. Metrics in this list will be logged as-is. All others will be averaged over time (e.g. loss, etc)."
keras,3667,"Hi there!
Is there a way I could fix this error? it is caused by the **BatchNormalization()** layer..  



It happens when I run the following code:



My machine configuration:
Operating system: openSuse
Theano version: '0.9.0dev2'
Keras version: 1.0.8
cuDNN version: v5.1
CUDA version: 7.5

Thanks!
",0,ValueError caused by the BatchNormalization() layer,"ValueError caused by the BatchNormalization() layer Hi there!
Is there a way I could fix this error? it is caused by the **BatchNormalization()** layer..  



It happens when I run the following code:



My machine configuration:
Operating system: openSuse
Theano version: '0.9.0dev2'
Keras version: 1.0.8
cuDNN version: v5.1
CUDA version: 7.5

Thanks!
"
keras,8823,"I have a model to work with LSTM layer. It works fine :
 
    encoder_inputs = Input(shape=(None, num_encoder_tokens))
    encoder = LSTM(latent_dim, return_state=True)
    encoder_outputs, state_h, state_c = encoder(encoder_inputs)

But when i add a wrap by a Bidirectional layer : 

    encoder_inputs = Input(shape=(None, num_encoder_tokens))
    encoder = Bidirectional(LSTM(latent_dim, return_state=True),merge_mode=""mul"")
    encoder_outputs, state_h, state_c = encoder(encoder_inputs)

It lead me an error : 

    output = y * y_rev
    TypeError: can't multiply sequence by non-int of type 'list'.
So how can i get both output, state and memory with Bidirectional layer ? When i remove  , it also works",0,Can not get state and memory from LSTM layer when wrap by a Bidirectional layer ?,"Can not get state and memory from LSTM layer when wrap by a Bidirectional layer ? I have a model to work with LSTM layer. It works fine :
 
    encoder_inputs = Input(shape=(None, num_encoder_tokens))
    encoder = LSTM(latent_dim, return_state=True)
    encoder_outputs, state_h, state_c = encoder(encoder_inputs)

But when i add a wrap by a Bidirectional layer : 

    encoder_inputs = Input(shape=(None, num_encoder_tokens))
    encoder = Bidirectional(LSTM(latent_dim, return_state=True),merge_mode=""mul"")
    encoder_outputs, state_h, state_c = encoder(encoder_inputs)

It lead me an error : 

    output = y * y_rev
    TypeError: can't multiply sequence by non-int of type 'list'.
So how can i get both output, state and memory with Bidirectional layer ? When i remove  , it also works"
keras,4630,"In short we only see rescale option in ImageDataGenerator.  How do you do the following for pertained VGG16 network that needs the channels values preprocessed as follows:
    im = cv2.resize(cv2.imread('cat.jpg'), (224, 224)).astype(np.float32)
    im[:,:,0] -= 103.939
    im[:,:,1] -= 116.779
    im[:,:,2] -= 123.68
    im = im.transpose((2,0,1))
    im = np.expand_dims(im, axis=0)
Where do we specify these options in ImageDataGenerator?

Thanks
Dr 
",0,How to allow for preprocessing of image channel values in ImageDataGenerator and flow_from_directory?,"How to allow for preprocessing of image channel values in ImageDataGenerator and flow_from_directory? In short we only see rescale option in ImageDataGenerator.  How do you do the following for pertained VGG16 network that needs the channels values preprocessed as follows:
    im = cv2.resize(cv2.imread('cat.jpg'), (224, 224)).astype(np.float32)
    im[:,:,0] -= 103.939
    im[:,:,1] -= 116.779
    im[:,:,2] -= 123.68
    im = im.transpose((2,0,1))
    im = np.expand_dims(im, axis=0)
Where do we specify these options in ImageDataGenerator?

Thanks
Dr 
"
keras,1852,"The theory of dropout layer in keras is the same as ""Hinton G., N. Srivastava, A. Krizhevsky, I. Sutskever, and R. Salakhutdinov. 2012. Improving Neural Networks by Preventing Co-adaptation of Feature Detectors. ResearchGate.""?
",0,"Does the theory of dropout layer in keras refer to (Hinton et al., 2012)?","Does the theory of dropout layer in keras refer to (Hinton et al., 2012)? The theory of dropout layer in keras is the same as ""Hinton G., N. Srivastava, A. Krizhevsky, I. Sutskever, and R. Salakhutdinov. 2012. Improving Neural Networks by Preventing Co-adaptation of Feature Detectors. ResearchGate.""?
"
keras,12354,"I'm getting a weird error when I try to create a network using an upsampling layer, when I manually set the interpolate keyword to bilinear. If I leave it out, and go with the default of 'nearest neighbour; it works fine. Does anyone know what's up?

TF = 1.12.0
Keras = 2.2.2
OS = Ubuntu 18.04


Code for model. Error is thrown at layer 'up1'

    chnl4_input = Input(shape=(368, 256, 4))
    chnl3_input = Input(shape=(736, 512, 3))

    conv1 = Conv2D(26, self.kernel_size, activation='relu', padding='same')(chnl4_input)
    conv2 = Conv2D(26, self.kernel_size, strides=(2, 2), activation='relu', padding='same')(conv1)

    conv5 = Conv2D(64, self.kernel_size, activation='relu', padding='same')(conv2)
    conv6 = Conv2D(64, self.kernel_size, activation='relu', padding='same')(conv5)

    up1 = concatenate([UpSampling2D(size=(2, 2), interpolation='bilinear')(conv6), conv1], axis=-1)
    conv7 = Conv2D(64, self.kernel_size, activation='relu', padding='same')(up1)

    conv8 = Conv2D(64, self.kernel_size, activation='relu', padding='same')(conv7)
    conv9 = Conv2D(64, self.kernel_size, activation='relu', padding='same')(conv8)

    conv11 = Conv2D(64, self.kernel_size, activation='relu', padding='same')(conv9)
    conv12 = Conv2D(64, self.kernel_size, activation='relu', padding='same')(conv11)

    up3 = concatenate([UpSampling2D(size=(2, 2), interpolation='bilinear')(conv12), chnl3_input], axis=-1)
    conv13 = Conv2D(67, self.kernel_size, activation='relu', padding='same')(up3)

    conv14 = Conv2D(67, self.kernel_size, activation='relu', padding='same')(conv13)
    conv15 = Conv2D(32, self.kernel_size, activation='relu', padding='same')(conv14)
    conv16 = Conv2D(3, self.kernel_size, activation='relu', padding='same')(conv15)

    out = conv16

    self.model = Model(inputs=[chnl4_input, chnl3_input], outputs=[out])

    self.model.compile(optimizer=self.optimizer_func, loss=self.loss_func)
    self.model.name = 'UNET'

    return self.modele here`

Errror = TypeError: ('Keyword argument not understood:', 'interpolation')",0,UpSampling2D throwing error with keyword 'interpolation',"UpSampling2D throwing error with keyword 'interpolation' I'm getting a weird error when I try to create a network using an upsampling layer, when I manually set the interpolate keyword to bilinear. If I leave it out, and go with the default of 'nearest neighbour; it works fine. Does anyone know what's up?

TF = 1.12.0
Keras = 2.2.2
OS = Ubuntu 18.04


Code for model. Error is thrown at layer 'up1'

    chnl4_input = Input(shape=(368, 256, 4))
    chnl3_input = Input(shape=(736, 512, 3))

    conv1 = Conv2D(26, self.kernel_size, activation='relu', padding='same')(chnl4_input)
    conv2 = Conv2D(26, self.kernel_size, strides=(2, 2), activation='relu', padding='same')(conv1)

    conv5 = Conv2D(64, self.kernel_size, activation='relu', padding='same')(conv2)
    conv6 = Conv2D(64, self.kernel_size, activation='relu', padding='same')(conv5)

    up1 = concatenate([UpSampling2D(size=(2, 2), interpolation='bilinear')(conv6), conv1], axis=-1)
    conv7 = Conv2D(64, self.kernel_size, activation='relu', padding='same')(up1)

    conv8 = Conv2D(64, self.kernel_size, activation='relu', padding='same')(conv7)
    conv9 = Conv2D(64, self.kernel_size, activation='relu', padding='same')(conv8)

    conv11 = Conv2D(64, self.kernel_size, activation='relu', padding='same')(conv9)
    conv12 = Conv2D(64, self.kernel_size, activation='relu', padding='same')(conv11)

    up3 = concatenate([UpSampling2D(size=(2, 2), interpolation='bilinear')(conv12), chnl3_input], axis=-1)
    conv13 = Conv2D(67, self.kernel_size, activation='relu', padding='same')(up3)

    conv14 = Conv2D(67, self.kernel_size, activation='relu', padding='same')(conv13)
    conv15 = Conv2D(32, self.kernel_size, activation='relu', padding='same')(conv14)
    conv16 = Conv2D(3, self.kernel_size, activation='relu', padding='same')(conv15)

    out = conv16

    self.model = Model(inputs=[chnl4_input, chnl3_input], outputs=[out])

    self.model.compile(optimizer=self.optimizer_func, loss=self.loss_func)
    self.model.name = 'UNET'

    return self.modele here`

Errror = TypeError: ('Keyword argument not understood:', 'interpolation')"
keras,4262,"I am trying to build my custom function in Keras as shown in:

https://github.com/fchollet/keras/blob/master/keras/objectives.py

However, I get this error:

    $ ipython 
    Python 2.7.12 |Anaconda 4.2.0 (x86_64)| (default, Jul  2 2016, 17:43:17) 
    Type ""copyright"", ""credits"" or ""license"" for more information.
    
    IPython 5.1.0 -- An enhanced Interactive Python.
    ?         -> Introduction and overview of IPython's features.
    %quickref -> Quick reference.
    help      -> Python's own help system.
    object?   -> Details about 'object', use 'object??' for extra details.
    
    In [1]: import keras
    Using TensorFlow backend.
    im
    In [2]: from . import backend as K
    ---------------------------------------------------------------------------
    ValueError                                Traceback (most recent call last)
    <ipython-input-2-b7b972de130c> in <module>()
    ----> 1 from . import backend as K
    
    ValueError: Attempted relative import in non-package

Any help? Thank you very much.",0,cannot import backend from keras,"cannot import backend from keras I am trying to build my custom function in Keras as shown in:

https://github.com/fchollet/keras/blob/master/keras/objectives.py

However, I get this error:

    $ ipython 
    Python 2.7.12 |Anaconda 4.2.0 (x86_64)| (default, Jul  2 2016, 17:43:17) 
    Type ""copyright"", ""credits"" or ""license"" for more information.
    
    IPython 5.1.0 -- An enhanced Interactive Python.
    ?         -> Introduction and overview of IPython's features.
    %quickref -> Quick reference.
    help      -> Python's own help system.
    object?   -> Details about 'object', use 'object??' for extra details.
    
    In [1]: import keras
    Using TensorFlow backend.
    im
    In [2]: from . import backend as K
    ---------------------------------------------------------------------------
    ValueError                                Traceback (most recent call last)
    <ipython-input-2-b7b972de130c> in <module>()
    ----> 1 from . import backend as K
    
    ValueError: Attempted relative import in non-package

Any help? Thank you very much."
keras,10693,"Hello. I'm new to Keras. I'm using TensorFlow as a backend. I'd like to load many models and use those in different threads. For different threads, I have to use different graphs, so I have to use different sessions. (Am I right?)

Here is a trimmed version of my code that can reproduce this error. What am I doing wrong?

I am using Keras-2.2.0 and tensorflow 1.8.0 without GPU on Ubuntu 16.04.4.

    import tensorflow as tf
    import numpy as np

    from keras.models import Sequential
    from keras.layers import Masking

    def predict():
        model = Sequential()
        layer = Masking(batch_input_shape=[None, 1])
        model.add(layer)

        x = np.array([[0]], dtype='int32')

        model.predict(x, batch_size=1)

    if __name__ == '__main__':
        with tf.Session():
            predict()

        with tf.Session():
            predict()
",0,Segmentation fault (core dumped) happened when I use more than one session.,"Segmentation fault (core dumped) happened when I use more than one session. Hello. I'm new to Keras. I'm using TensorFlow as a backend. I'd like to load many models and use those in different threads. For different threads, I have to use different graphs, so I have to use different sessions. (Am I right?)

Here is a trimmed version of my code that can reproduce this error. What am I doing wrong?

I am using Keras-2.2.0 and tensorflow 1.8.0 without GPU on Ubuntu 16.04.4.

    import tensorflow as tf
    import numpy as np

    from keras.models import Sequential
    from keras.layers import Masking

    def predict():
        model = Sequential()
        layer = Masking(batch_input_shape=[None, 1])
        model.add(layer)

        x = np.array([[0]], dtype='int32')

        model.predict(x, batch_size=1)

    if __name__ == '__main__':
        with tf.Session():
            predict()

        with tf.Session():
            predict()
"
keras,13006,"**System information**  
- Have I written custom code (as opposed to using example directory): yes
- OS Platform and Distribution: Linux ubuntu 4.15.0-51-generic #16~18.04.1-Ubuntu SMP
- TensorFlow backend: yes
- TensorFlow version: 1.13.1
- Keras version: 2.2.4
- Python version: 3.6.8

I have used a model (provided [here](https://medium.com/@ksusorokina/image-classification-with-convolutional-neural-networks-496815db12a8)] that trains a model on two categories of pictures and then tries to classify them.
Furthermore, I force the network to use the same seeds when training so as to get comparable results. I also create and close the tf sessions as I have read that this may also cause problems.

**Describe the current behaviour** 
Most of the time the test and validation accuracy converge around 0.5 and the loss stays at exactly the same value for every epoch. I can run the same code, without changing it, several times in a row and get this problem and only rarely do I get a run where the neural network is trained properly and rises to and above 0.9 accuracy for training and validation.


**Describe the expected behaviour**  
For the same seeds the run should produce at least very similar results i.e. it should not be stuck around 0.5 accuracy.

**Code to reproduce the issue**  


[Edit]
I've tried a [different tutorial](https://www.geeksforgeeks.org/python-image-classification-using-keras/) and I get the same result. The only changes I have done to the code is to change the image dimension to the size of my images, changed the image directories and to turn off Image Augmentation, as well as changed the number of images.

About half of my runs produced output such as this

while the other half produced 

(the numbers were not always exactly the same).

I though that maybe I have bad images so I tried use images from other tutorials (like [this](https://github.com/perseus784/BvS)) but I got the same results: about half or more of the runs are useless because they converge at 0.5 accuracy. What's going on?",0,Keras training and validation accuracy always converges to 0.5,"Keras training and validation accuracy always converges to 0.5 **System information**  
- Have I written custom code (as opposed to using example directory): yes
- OS Platform and Distribution: Linux ubuntu 4.15.0-51-generic #16~18.04.1-Ubuntu SMP
- TensorFlow backend: yes
- TensorFlow version: 1.13.1
- Keras version: 2.2.4
- Python version: 3.6.8

I have used a model (provided [here](https://medium.com/@ksusorokina/image-classification-with-convolutional-neural-networks-496815db12a8)] that trains a model on two categories of pictures and then tries to classify them.
Furthermore, I force the network to use the same seeds when training so as to get comparable results. I also create and close the tf sessions as I have read that this may also cause problems.

**Describe the current behaviour** 
Most of the time the test and validation accuracy converge around 0.5 and the loss stays at exactly the same value for every epoch. I can run the same code, without changing it, several times in a row and get this problem and only rarely do I get a run where the neural network is trained properly and rises to and above 0.9 accuracy for training and validation.


**Describe the expected behaviour**  
For the same seeds the run should produce at least very similar results i.e. it should not be stuck around 0.5 accuracy.

**Code to reproduce the issue**  


[Edit]
I've tried a [different tutorial](https://www.geeksforgeeks.org/python-image-classification-using-keras/) and I get the same result. The only changes I have done to the code is to change the image dimension to the size of my images, changed the image directories and to turn off Image Augmentation, as well as changed the number of images.

About half of my runs produced output such as this

while the other half produced 

(the numbers were not always exactly the same).

I though that maybe I have bad images so I tried use images from other tutorials (like [this](https://github.com/perseus784/BvS)) but I got the same results: about half or more of the runs are useless because they converge at 0.5 accuracy. What's going on?"
keras,4696,"Hi, I am new to keras and trying to implements a model a part of that includes encoding sentences using convolution network.

    # import statements
    from keras.layers.embeddings import Embedding
    from keras.layers.wrappers import TimeDistributed
    from keras.layers.convolutional import Convolution1D
    from keras.layers.pooling import MaxPooling1D
    from keras.engine.topology import Merge
    from keras.layers.core import Dropout, RepeatVector
    from keras.layers import LSTM, Input, Dense
    from keras.models import Model, Sequential
    from IPython.display import SVG


    def conv_encoder(nb_filters, filter_len, in_shape):
        """"""model to encode a sentence with a particular filter size
            nb_filters: number of filters, filter_len: size of filter,
            in_shape: input shape (_, SENT_LEN, WORD_LEN)
        """"""
        model = Sequential()
        model.add(Convolution1D(nb_filters, filter_len, border_mode='same', input_shape=in_shape))
        model.add(MaxPooling1D(pool_length=2, stride=None, border_mode='same'))
        return model

     def make_sentence_encoder(nb_filters, filter_lens, in_shape):
         """"""model to encode a sentence with different filter sizes
             basically, learns different types of representations based
             on filter sizes like unigram, bigram, trigram etc.
         """"""
         models = []
         for flt in filter_lens:
             models.append(conv_encoder(nb_filters, flt, in_shape))
         merged_model = Sequential()
         merged_model.add(Merge(models, mode='sum', concat_axis=1))
         return merged_model

    def encode_all_sentences(vocab_size, dense_emb_dim, doc_maxlen, sent_maxlen, nb_filters,   filter_lens, in_shape):
         """"""A model that encodes each sentences using conv nets.
            Returns encoded set of sentences.
            Input: Document,  SHAPE: (_, DOC_LEN, SENT_LEN)
            Output: SHAPE: (_, DOC_LEN, NEW_SENT_LEN)
         """"""
         # setup a sentence encoder
         sent_encoder = make_sentence_encoder(nb_filters, filter_lens, in_shape)

        # embed the input sequence into a sequence of vectors                     
        sentences_encoder = Sequential()
        # initialize embedding layer with wordvec
        sentences_encoder.add(TimeDistributed(Embedding(input_dim=vocab_size,    output_dim=dense_emb_dim, mask_zero=True),
                    input_shape=(doc_maxlen, sent_maxlen), input_dtype='int32'))
        sentences_encoder.add(TimeDistributed(Dropout(0.3)))
   
        sentences_encoder.add(TimeDistributed(sent_encoder))
        return sentences_encoder



Test code:

    inputs = Input(shape=(50, 170))
    sents_enc = encode_all_sentences(1000, 60, 50, 170, 32, [1, 2, 3], (170, 60))
    print sents_enc.output_shape
    out = sents_enc(inputs)
    model = Model(input=inputs, output=out)


The line  in  method is source of problem. I guess it's because  takes input as list of size equal to len(filter_lens) i.e total number of different filters used. I don't know/am confused how to pass it here.

Help will be highly appreciated. Thanks.

Error thrown : 


",0,"AssertionError: Could not compute output Elemwise{add,no_inplace}.0 ","AssertionError: Could not compute output Elemwise{add,no_inplace}.0  Hi, I am new to keras and trying to implements a model a part of that includes encoding sentences using convolution network.

    # import statements
    from keras.layers.embeddings import Embedding
    from keras.layers.wrappers import TimeDistributed
    from keras.layers.convolutional import Convolution1D
    from keras.layers.pooling import MaxPooling1D
    from keras.engine.topology import Merge
    from keras.layers.core import Dropout, RepeatVector
    from keras.layers import LSTM, Input, Dense
    from keras.models import Model, Sequential
    from IPython.display import SVG


    def conv_encoder(nb_filters, filter_len, in_shape):
        """"""model to encode a sentence with a particular filter size
            nb_filters: number of filters, filter_len: size of filter,
            in_shape: input shape (_, SENT_LEN, WORD_LEN)
        """"""
        model = Sequential()
        model.add(Convolution1D(nb_filters, filter_len, border_mode='same', input_shape=in_shape))
        model.add(MaxPooling1D(pool_length=2, stride=None, border_mode='same'))
        return model

     def make_sentence_encoder(nb_filters, filter_lens, in_shape):
         """"""model to encode a sentence with different filter sizes
             basically, learns different types of representations based
             on filter sizes like unigram, bigram, trigram etc.
         """"""
         models = []
         for flt in filter_lens:
             models.append(conv_encoder(nb_filters, flt, in_shape))
         merged_model = Sequential()
         merged_model.add(Merge(models, mode='sum', concat_axis=1))
         return merged_model

    def encode_all_sentences(vocab_size, dense_emb_dim, doc_maxlen, sent_maxlen, nb_filters,   filter_lens, in_shape):
         """"""A model that encodes each sentences using conv nets.
            Returns encoded set of sentences.
            Input: Document,  SHAPE: (_, DOC_LEN, SENT_LEN)
            Output: SHAPE: (_, DOC_LEN, NEW_SENT_LEN)
         """"""
         # setup a sentence encoder
         sent_encoder = make_sentence_encoder(nb_filters, filter_lens, in_shape)

        # embed the input sequence into a sequence of vectors                     
        sentences_encoder = Sequential()
        # initialize embedding layer with wordvec
        sentences_encoder.add(TimeDistributed(Embedding(input_dim=vocab_size,    output_dim=dense_emb_dim, mask_zero=True),
                    input_shape=(doc_maxlen, sent_maxlen), input_dtype='int32'))
        sentences_encoder.add(TimeDistributed(Dropout(0.3)))
   
        sentences_encoder.add(TimeDistributed(sent_encoder))
        return sentences_encoder



Test code:

    inputs = Input(shape=(50, 170))
    sents_enc = encode_all_sentences(1000, 60, 50, 170, 32, [1, 2, 3], (170, 60))
    print sents_enc.output_shape
    out = sents_enc(inputs)
    model = Model(input=inputs, output=out)


The line  in  method is source of problem. I guess it's because  takes input as list of size equal to len(filter_lens) i.e total number of different filters used. I don't know/am confused how to pass it here.

Help will be highly appreciated. Thanks.

Error thrown : 


"
keras,4395,"Hello,
I'm not sure if this is the proper place to ask a question about keras. I apologize in advance if it is not.

 I'm currently trying to reproduce this publication which is using recurrent neural nets to predict chemical activities.

> Goulon, A., T. Picot, A. Duprat, and G. Dreyfus. “Predicting Activities without Computing Descriptors: Graph Machines for QSAR.” SAR and QSAR in Environmental Research 18, no. 1–2 (January 1, 2007): 141–53. doi:10.1080/10629360601054313.

In two words : the autors represent a molecule as a directed acyclic graph. Each node of this graph being an atom and each edge being a chemical bond. Then a neural network is applied on the ""exit node"" (called root node in the publication). This neural network calls itself on the parents of the root node and so on to provide a prediction on the whole molecule.

I think I understand how the training works for this graph machine: for each molecule, a gradient of the cost function is calculated by backpropagation in the molecule. The gradient over a batch of molecules is a weighted average of the gradient calculated for each molecule composing the batch. 
A gradient descent algorithm is then used to minimize the cost-function.

My main trouble is : how can I implement this training procedure in Keras ? Usually, you just provide a list of X and a list of corresponding Y and just use model.compile() and then model.fit(). From what I understand, it is not possible here because each molecule should have its own keras.model

Is keras even the best solution here ?

Best regards,
",0,[Question] Graph Machines in keras,"[Question] Graph Machines in keras Hello,
I'm not sure if this is the proper place to ask a question about keras. I apologize in advance if it is not.

 I'm currently trying to reproduce this publication which is using recurrent neural nets to predict chemical activities.

> Goulon, A., T. Picot, A. Duprat, and G. Dreyfus. “Predicting Activities without Computing Descriptors: Graph Machines for QSAR.” SAR and QSAR in Environmental Research 18, no. 1–2 (January 1, 2007): 141–53. doi:10.1080/10629360601054313.

In two words : the autors represent a molecule as a directed acyclic graph. Each node of this graph being an atom and each edge being a chemical bond. Then a neural network is applied on the ""exit node"" (called root node in the publication). This neural network calls itself on the parents of the root node and so on to provide a prediction on the whole molecule.

I think I understand how the training works for this graph machine: for each molecule, a gradient of the cost function is calculated by backpropagation in the molecule. The gradient over a batch of molecules is a weighted average of the gradient calculated for each molecule composing the batch. 
A gradient descent algorithm is then used to minimize the cost-function.

My main trouble is : how can I implement this training procedure in Keras ? Usually, you just provide a list of X and a list of corresponding Y and just use model.compile() and then model.fit(). From what I understand, it is not possible here because each molecule should have its own keras.model

Is keras even the best solution here ?

Best regards,
"
keras,10179,"    import numpy as np
    import tensorflow as tf
    tf.set_random_seed(49999)

    maxlen = 2
    word_dim = 2
    n_rnn = 3

    x = np.array(range(4), dtype=np.float32).reshape([1, 2, 2])
    x_r = np.flip(x, axis=1)

    inputs_r = tf.constant(x_r, dtype=tf.float32)
    seq_len = tf.constant([2], dtype=tf.int32)

    cell_fw = tf.nn.rnn_cell.LSTMCell(n_rnn, name=""fw"")
    cell_bw = tf.nn.rnn_cell.LSTMCell(n_rnn, name=""bw"")

    # tf forward
    tf_hs_fw, (tf_c_fw, tf_h_fw) = tf.nn.dynamic_rnn(
        cell_fw, inputs_r, seq_len, dtype=tf.float32)

    # tf backward
    tf_hs_bw, (tf_c_bw, tf_h_bw) = tf.nn.dynamic_rnn(
        cell_bw, inputs_r, seq_len, dtype=tf.float32)

    sess = tf.InteractiveSession()
    init = tf.global_variables_initializer()
    sess.run(init)

    print(tf.trainable_variables())

    tf_w = [v.eval() for v in tf.trainable_variables()]

    v_tf_h_fw = tf_h_fw.eval()[0]
    v_tf_h_bw = tf_h_bw.eval()[0]


    def tf2keras(params, reshape_dim=(0, 2, 1, 3), in_dim=2, rnn_dim=3):
        # params: [(5, 12), (12,), (5, 12), (12,)]
        assert in_dim+rnn_dim == params[0].shape[0]
        if len(params) == 2:
            tf_lstm = [params[0][:in_dim, :], params[0][in_dim:, :], params[1]]
        else:  # == 4
            tf_lstm = [params[0][:in_dim, :], params[0][in_dim:, :], params[1],
                       params[2][:in_dim, :], params[2][in_dim:, :], params[3]]

        # tf_lstm: [(2, 12), (3, 12), (12,),
        #          (2, 12), (3, 12), (12,)]

        def dim_recombination(x, reshape_dim):
            # [i, j, f, o]  => [i, f, c, o]  where j is c
            rst = None
            if len(x.shape) == 1:
                # [i, j, f, o]
                tmp = [x[0:rnn_dim * 1],
                       x[rnn_dim * 1:rnn_dim * 2],
                       x[rnn_dim * 2:rnn_dim * 3],
                       x[rnn_dim * 3: rnn_dim * 4]]
                rst = np.hstack([tmp[i] for i in reshape_dim])
            elif len(x.shape) == 2:
                tmp = [x[:, 0:rnn_dim*1],
                       x[:, rnn_dim*1:rnn_dim*2],
                       x[:, rnn_dim*2:rnn_dim*3],
                       x[:, rnn_dim*3:rnn_dim*4]]
                rst = np.hstack([tmp[i] for i in reshape_dim])
            else:
                print(""xxxxx"")

            return rst

        keras_w = [dim_recombination(v, reshape_dim) for v in tf_lstm]
        return keras_w


    k_params = tf2keras(tf_w, (0, 2, 1, 3))


    #  keras
    from keras.layers import Input
    from keras.layers import LSTM
    from keras.models import Model

    keras_input = Input(shape=[maxlen, word_dim], dtype='float32', name='input_layer')


    k_lstm1 = LSTM(n_rnn, recurrent_activation='sigmoid', return_sequences=True, return_state=True,
                   name=""lstm1"")(keras_input, training=False)
    k_lstm2 = LSTM(n_rnn, recurrent_activation='sigmoid', return_sequences=True, return_state=True,
                   name=""lstm2"")(keras_input, training=False)

    m1 = Model(inputs=keras_input, outputs=k_lstm1)
    m1.set_weights(k_params[:3])
    k_hs_fw, k_h_fw, k_c_fw = m1.predict(x_r)
    v_k_h_fw = k_h_fw[0]

    m2 = Model(inputs=keras_input, outputs=k_lstm2)
    m2.set_weights(k_params[3:])

    k_hs_bw, k_h_bw, k_c_bw = m2.predict(x_r)
    v_k_h_bw = k_h_bw[0]

    print(v_k_h_fw - v_tf_h_fw)
    # expect: [0, 0, 0]
    # but get [-0.0370398  -0.02871804  0.00494046]

    print(v_k_h_bw - v_tf_h_bw)
    # expect: [0, 0, 0]
    # but get [-0.03148754 -0.0340828   0.05445436]",0,"feed the lstm weight get by tensorflow to lstm of keras, but I failed to repeat the result! ","feed the lstm weight get by tensorflow to lstm of keras, but I failed to repeat the result!      import numpy as np
    import tensorflow as tf
    tf.set_random_seed(49999)

    maxlen = 2
    word_dim = 2
    n_rnn = 3

    x = np.array(range(4), dtype=np.float32).reshape([1, 2, 2])
    x_r = np.flip(x, axis=1)

    inputs_r = tf.constant(x_r, dtype=tf.float32)
    seq_len = tf.constant([2], dtype=tf.int32)

    cell_fw = tf.nn.rnn_cell.LSTMCell(n_rnn, name=""fw"")
    cell_bw = tf.nn.rnn_cell.LSTMCell(n_rnn, name=""bw"")

    # tf forward
    tf_hs_fw, (tf_c_fw, tf_h_fw) = tf.nn.dynamic_rnn(
        cell_fw, inputs_r, seq_len, dtype=tf.float32)

    # tf backward
    tf_hs_bw, (tf_c_bw, tf_h_bw) = tf.nn.dynamic_rnn(
        cell_bw, inputs_r, seq_len, dtype=tf.float32)

    sess = tf.InteractiveSession()
    init = tf.global_variables_initializer()
    sess.run(init)

    print(tf.trainable_variables())

    tf_w = [v.eval() for v in tf.trainable_variables()]

    v_tf_h_fw = tf_h_fw.eval()[0]
    v_tf_h_bw = tf_h_bw.eval()[0]


    def tf2keras(params, reshape_dim=(0, 2, 1, 3), in_dim=2, rnn_dim=3):
        # params: [(5, 12), (12,), (5, 12), (12,)]
        assert in_dim+rnn_dim == params[0].shape[0]
        if len(params) == 2:
            tf_lstm = [params[0][:in_dim, :], params[0][in_dim:, :], params[1]]
        else:  # == 4
            tf_lstm = [params[0][:in_dim, :], params[0][in_dim:, :], params[1],
                       params[2][:in_dim, :], params[2][in_dim:, :], params[3]]

        # tf_lstm: [(2, 12), (3, 12), (12,),
        #          (2, 12), (3, 12), (12,)]

        def dim_recombination(x, reshape_dim):
            # [i, j, f, o]  => [i, f, c, o]  where j is c
            rst = None
            if len(x.shape) == 1:
                # [i, j, f, o]
                tmp = [x[0:rnn_dim * 1],
                       x[rnn_dim * 1:rnn_dim * 2],
                       x[rnn_dim * 2:rnn_dim * 3],
                       x[rnn_dim * 3: rnn_dim * 4]]
                rst = np.hstack([tmp[i] for i in reshape_dim])
            elif len(x.shape) == 2:
                tmp = [x[:, 0:rnn_dim*1],
                       x[:, rnn_dim*1:rnn_dim*2],
                       x[:, rnn_dim*2:rnn_dim*3],
                       x[:, rnn_dim*3:rnn_dim*4]]
                rst = np.hstack([tmp[i] for i in reshape_dim])
            else:
                print(""xxxxx"")

            return rst

        keras_w = [dim_recombination(v, reshape_dim) for v in tf_lstm]
        return keras_w


    k_params = tf2keras(tf_w, (0, 2, 1, 3))


    #  keras
    from keras.layers import Input
    from keras.layers import LSTM
    from keras.models import Model

    keras_input = Input(shape=[maxlen, word_dim], dtype='float32', name='input_layer')


    k_lstm1 = LSTM(n_rnn, recurrent_activation='sigmoid', return_sequences=True, return_state=True,
                   name=""lstm1"")(keras_input, training=False)
    k_lstm2 = LSTM(n_rnn, recurrent_activation='sigmoid', return_sequences=True, return_state=True,
                   name=""lstm2"")(keras_input, training=False)

    m1 = Model(inputs=keras_input, outputs=k_lstm1)
    m1.set_weights(k_params[:3])
    k_hs_fw, k_h_fw, k_c_fw = m1.predict(x_r)
    v_k_h_fw = k_h_fw[0]

    m2 = Model(inputs=keras_input, outputs=k_lstm2)
    m2.set_weights(k_params[3:])

    k_hs_bw, k_h_bw, k_c_bw = m2.predict(x_r)
    v_k_h_bw = k_h_bw[0]

    print(v_k_h_fw - v_tf_h_fw)
    # expect: [0, 0, 0]
    # but get [-0.0370398  -0.02871804  0.00494046]

    print(v_k_h_bw - v_tf_h_bw)
    # expect: [0, 0, 0]
    # but get [-0.03148754 -0.0340828   0.05445436]"
keras,8139,"Two changes are proposed in this issue to make  more usable for the final testing phase:

### Change 1: ImageDataGenerator can be exhaused

The [documentation of ImageDataGenerator](https://keras.io/preprocessing/image/) says:

> Generate batches of tensor image data with real-time data augmentation. The data will be looped over (in batches) indefinitely.

It seems to me that, this (loops indefinitely) implies that, the image data generator is mainly used for the training process, and not for the final test (because we only need one epoch when testing).

Sure you can achieve the one epoch purpose by using , but this relies on the implementation of  and is not a documented behaviour.

I propose:

1. Add an  keyword argument that controls  whether the iterator yields indefinitely or not.

2. Alternative to 1, we can also make the  mentioned above a documented behaviour, and also document that after the final step, each image is visited exactly once.

### Change 2: Add a new  to yields the filenames of the batch

The current implementation of  allows you to recover the current batch's filenames by using its  and  property, but this relies on the implementation and is not documented.

I propose we add a new  to yields the filenames of the current batch, just like we don't let user slice its  property to get the current batch's class. (The filename is useful at least for making some Kaggle submissions).

Both these changes are backward compatible.",0,[Feature Request] Make ImageDataGenerator suitable for test,"[Feature Request] Make ImageDataGenerator suitable for test Two changes are proposed in this issue to make  more usable for the final testing phase:

### Change 1: ImageDataGenerator can be exhaused

The [documentation of ImageDataGenerator](https://keras.io/preprocessing/image/) says:

> Generate batches of tensor image data with real-time data augmentation. The data will be looped over (in batches) indefinitely.

It seems to me that, this (loops indefinitely) implies that, the image data generator is mainly used for the training process, and not for the final test (because we only need one epoch when testing).

Sure you can achieve the one epoch purpose by using , but this relies on the implementation of  and is not a documented behaviour.

I propose:

1. Add an  keyword argument that controls  whether the iterator yields indefinitely or not.

2. Alternative to 1, we can also make the  mentioned above a documented behaviour, and also document that after the final step, each image is visited exactly once.

### Change 2: Add a new  to yields the filenames of the batch

The current implementation of  allows you to recover the current batch's filenames by using its  and  property, but this relies on the implementation and is not documented.

I propose we add a new  to yields the filenames of the current batch, just like we don't let user slice its  property to get the current batch's class. (The filename is useful at least for making some Kaggle submissions).

Both these changes are backward compatible."
keras,5267,"I'd like to modify categorical accuracy to take into account non-padded values, the code I've come up with is:









But I keep getting the error message from below.  I've tested the function with some made up examples and it seems to work just fine, but when I pass categorical_accuracy as a metric to my model, it doesn't. Any help? 

Thanks so much in advance!











    metric_result = metric_fn(y_true, y_pred)  File ""../codeswitch_pos_tagger/trainer.py"", line 27, in categorical_accuracy
 

",0,Modifying categorical accuracy,"Modifying categorical accuracy I'd like to modify categorical accuracy to take into account non-padded values, the code I've come up with is:









But I keep getting the error message from below.  I've tested the function with some made up examples and it seems to work just fine, but when I pass categorical_accuracy as a metric to my model, it doesn't. Any help? 

Thanks so much in advance!











    metric_result = metric_fn(y_true, y_pred)  File ""../codeswitch_pos_tagger/trainer.py"", line 27, in categorical_accuracy
 

"
keras,10439,Apologies ,0,Accidental Submission ,Accidental Submission  Apologies 
keras,5760,"Keras 1.2.2

  File ""resnet50.py"", line 39, in <module>
    from keras.applications.imagenet_utils import _obtain_input_shape
ImportError: cannot import name '_obtain_input_shape'

",0,deep-learning-models :  ImportError: cannot import name '_obtain_input_shape',"deep-learning-models :  ImportError: cannot import name '_obtain_input_shape' Keras 1.2.2

  File ""resnet50.py"", line 39, in <module>
    from keras.applications.imagenet_utils import _obtain_input_shape
ImportError: cannot import name '_obtain_input_shape'

"
keras,1679,"I am trying to create convolutional autoencoder for temporal sequences using Keras.
Here's the code



I am getting the following error at the 3rd line where decoder is being initialized:



Please help me resolve the issue
",0,Convolutional Autoencoder for temporal sequences,"Convolutional Autoencoder for temporal sequences I am trying to create convolutional autoencoder for temporal sequences using Keras.
Here's the code



I am getting the following error at the 3rd line where decoder is being initialized:



Please help me resolve the issue
"
keras,9245,"@fchollet, I recently wrote an [factorized version](https://github.com/taehoonlee/tensornets/blob/master/tensornets/resnets.py) for all the ResNet variants with pre-trained weights including ResNet, ResNetv2, ResNeXt, and WideResNet (total 11 models). If they are added here Keras, will it be hard to manage and cause more problems? or will it be useful? What do you think about adding the 11 ResNet models?",0,Add all the ResNet variants,"Add all the ResNet variants @fchollet, I recently wrote an [factorized version](https://github.com/taehoonlee/tensornets/blob/master/tensornets/resnets.py) for all the ResNet variants with pre-trained weights including ResNet, ResNetv2, ResNeXt, and WideResNet (total 11 models). If they are added here Keras, will it be hard to manage and cause more problems? or will it be useful? What do you think about adding the 11 ResNet models?"
keras,4142,"Hello All,

I am using fit_generator to train my model, I am trying the batch implementation similar to [test_multiprocessing.py](https://github.com/fchollet/keras/blob/master/tests/keras/test_multiprocessing.py)



And the output is 



It is clear from the above output that multiple process are trying to access the same samples (we can see intersection of (start,end) from different process batches). 

I have also tried implementation suggested in #1638 but no luck, it is also working similarly. 

What should be the batch generator implementation to work correctly(no intersection between the batches as long as possible. In this case always, as samples > samples_per_epoch) with nb_worker > 1 and pickle_safe=True. ?
TIA.
",0,using fit_generator with nb_worker > 1 and pickle_safe=True,"using fit_generator with nb_worker > 1 and pickle_safe=True Hello All,

I am using fit_generator to train my model, I am trying the batch implementation similar to [test_multiprocessing.py](https://github.com/fchollet/keras/blob/master/tests/keras/test_multiprocessing.py)



And the output is 



It is clear from the above output that multiple process are trying to access the same samples (we can see intersection of (start,end) from different process batches). 

I have also tried implementation suggested in #1638 but no luck, it is also working similarly. 

What should be the batch generator implementation to work correctly(no intersection between the batches as long as possible. In this case always, as samples > samples_per_epoch) with nb_worker > 1 and pickle_safe=True. ?
TIA.
"
keras,5700,"Regarding patch-wise training for image classification or segmentation, I need to put multiple patches corresponding to a single image into a single mini-batch during training process. How to do that in Keras? Or how can I ensure multiple training patches in a single mini-batch belong to the same training image?",0,regarding putting multiple patches for a single image into a single mini-batch,"regarding putting multiple patches for a single image into a single mini-batch Regarding patch-wise training for image classification or segmentation, I need to put multiple patches corresponding to a single image into a single mini-batch during training process. How to do that in Keras? Or how can I ensure multiple training patches in a single mini-batch belong to the same training image?"
keras,5418,"I see that the backend has an implementation for batchwise dot product.
Can there be a function to do batchwise outer product?

I tried implementing an outer product Layer of my own, but I am getting stuck (dealing with batches) Would appreciate some pointers or help. 

Thanks in advance.",0,Batchwise outer product?,"Batchwise outer product? I see that the backend has an implementation for batchwise dot product.
Can there be a function to do batchwise outer product?

I tried implementing an outer product Layer of my own, but I am getting stuck (dealing with batches) Would appreciate some pointers or help. 

Thanks in advance."
keras,1821,"Not really an <em>issue</em>, more of a question to ensure for myself (and others) that the output from the  function for a multi-label classification problem is being interpreted correctly.

So here's a toy problem:



, 
                
             
            
             


    
     
    
    d


                  






Using a basic Keras  model:












Which outputs:





So how do I interpret this output? I'm used to using  Classifiers, which return probability output for multi-label problems in the shape of  where  are the number of test instances and  are 2 (0, 1). But I think that the model above is outputting in the shape of  where samples are test instances and classes are the unique number of classes in . 

Could someone clarify this output for me?
",0,How to interprect `.predict_proba()` for multi-label Classification problem?,"How to interprect `.predict_proba()` for multi-label Classification problem? Not really an <em>issue</em>, more of a question to ensure for myself (and others) that the output from the  function for a multi-label classification problem is being interpreted correctly.

So here's a toy problem:



, 
                
             
            
             


    
     
    
    d


                  






Using a basic Keras  model:












Which outputs:





So how do I interpret this output? I'm used to using  Classifiers, which return probability output for multi-label problems in the shape of  where  are the number of test instances and  are 2 (0, 1). But I think that the model above is outputting in the shape of  where samples are test instances and classes are the unique number of classes in . 

Could someone clarify this output for me?
"
keras,4271,"Not really an issue, but I haven't found any documentation on how to access the hidden state in LSTM RNN.

**Context:** I want to capture the semantic meaning of a sentence, by feeding in the Glove word embedding vectors ( [https://github.com/fchollet/keras/blob/master/examples/pretrained_word_embeddings.py](url) ) into the RNN and when the sentence ends, the final hidden state captures the so-called sentence embedding. I want to access this vector obtained from the final hidden state.",0,Semantic Meaning of a sentence,"Semantic Meaning of a sentence Not really an issue, but I haven't found any documentation on how to access the hidden state in LSTM RNN.

**Context:** I want to capture the semantic meaning of a sentence, by feeding in the Glove word embedding vectors ( [https://github.com/fchollet/keras/blob/master/examples/pretrained_word_embeddings.py](url) ) into the RNN and when the sentence ends, the final hidden state captures the so-called sentence embedding. I want to access this vector obtained from the final hidden state."
keras,987,"I have already installed Keras 2.0 successfully. But, it print ""CNMeM is disabled"", who can tell me what's meaning of this note.
",0,CNMeM is disabled,"CNMeM is disabled I have already installed Keras 2.0 successfully. But, it print ""CNMeM is disabled"", who can tell me what's meaning of this note.
"
keras,4733,"Hi guys. I am trying to augment my image data. However, my labels are also images. So i need to do the same augmentation operations on both image and label. I would like to save both as image but somehow, when I use datagen.flow, it does not save the label, only the image. I also insert the code I am  using:
https://github.com/adkoadko/ostruvky/blob/master/Keras_augmentation

Is there a way to save also the label? Or have I missed something?

Thank you",0,ImageDataGenerator does not save label as an image,"ImageDataGenerator does not save label as an image Hi guys. I am trying to augment my image data. However, my labels are also images. So i need to do the same augmentation operations on both image and label. I would like to save both as image but somehow, when I use datagen.flow, it does not save the label, only the image. I also insert the code I am  using:
https://github.com/adkoadko/ostruvky/blob/master/Keras_augmentation

Is there a way to save also the label? Or have I missed something?

Thank you"
keras,4220," If I were to follow the guide and integrate with my TensorFlow workflow (https://blog.keras.io/keras-as-a-simplified-interface-to-tensorflow-tutorial.html) as with others, you cannot access the weight variable because we **won't be building the model** as shown in the guide. We're merely using the layers. There is no need to compile when we use it as a simplified interface to TensorFlow. **How then do we access the weights (variables)?**

Because if we use with TensorFlow like the guide, we **do not call**  or  but merely use the layers to build. 
",0,How to access variables of layers?,"How to access variables of layers?  If I were to follow the guide and integrate with my TensorFlow workflow (https://blog.keras.io/keras-as-a-simplified-interface-to-tensorflow-tutorial.html) as with others, you cannot access the weight variable because we **won't be building the model** as shown in the guide. We're merely using the layers. There is no need to compile when we use it as a simplified interface to TensorFlow. **How then do we access the weights (variables)?**

Because if we use with TensorFlow like the guide, we **do not call**  or  but merely use the layers to build. 
"
keras,4727,"I've built a model with multiple outputs and need to balance the losses against each other.
The theano backend can do this via the parameter in .
Unfortunately I'm using tensorflow.

How does Keras deal with multiple losses:
Do you add them and then take the derivative of the sum ?
Or do you optimize them individually ?

My idea is to just implement the losses I need myself and multiply the weighting factor into the loss manually. But that would only work if you're optimizing the sum of losses.",0,Weigh multiple losses,"Weigh multiple losses I've built a model with multiple outputs and need to balance the losses against each other.
The theano backend can do this via the parameter in .
Unfortunately I'm using tensorflow.

How does Keras deal with multiple losses:
Do you add them and then take the derivative of the sum ?
Or do you optimize them individually ?

My idea is to just implement the losses I need myself and multiply the weighting factor into the loss manually. But that would only work if you're optimizing the sum of losses."
keras,1926,"I wanted to reproduce the issues that I cited in [this post](https://github.com/fchollet/keras/issues/1917).
So I wanted to train a neural network using keras, but the training does not start, and it happens in really weird situations.

Here is a portion of my code:



If I run this code in the terminal using , everything goes fine and the training starts.

But if I run this code in the terminal using , , or if I redirect the outputs in my code , then the program stall for the  method, so the learning does not start. The is displayed, but nothing more, no training.

I took a look and it seems that the program keeps calling this function 

Any ideas why it does not start when I redirect the outputs?
",0,Learning does not start,"Learning does not start I wanted to reproduce the issues that I cited in [this post](https://github.com/fchollet/keras/issues/1917).
So I wanted to train a neural network using keras, but the training does not start, and it happens in really weird situations.

Here is a portion of my code:



If I run this code in the terminal using , everything goes fine and the training starts.

But if I run this code in the terminal using , , or if I redirect the outputs in my code , then the program stall for the  method, so the learning does not start. The is displayed, but nothing more, no training.

I took a look and it seems that the program keeps calling this function 

Any ideas why it does not start when I redirect the outputs?
"
keras,230,"I have added this layer in my local keras (under convolutional layers):

class GlobalPooling(Layer):
    '''
        Apply a global pooling to an output.
    '''
    def **init**(self, pooling_function = T.mean):
        super(GlobalPooling,self).**init**()
        self.pooling_function = pooling_function
    def get_output(self, train):
        X = self.get_input(train)
        return self.pooling_function(X.flatten(3), axis = 2)



It seems to work fine, can you please add this support?

Thanks!
",0,Can i add global pooling layer?,"Can i add global pooling layer? I have added this layer in my local keras (under convolutional layers):

class GlobalPooling(Layer):
    '''
        Apply a global pooling to an output.
    '''
    def **init**(self, pooling_function = T.mean):
        super(GlobalPooling,self).**init**()
        self.pooling_function = pooling_function
    def get_output(self, train):
        X = self.get_input(train)
        return self.pooling_function(X.flatten(3), axis = 2)



It seems to work fine, can you please add this support?

Thanks!
"
keras,9140,"I am trying to understand the code given here:
![conv_lstm](https://github.com/keras-team/keras/blob/master/examples/conv_lstm.py). 

I have implemented the whole code in a jupyter python notebook linked here:
![LSTM](https://github.com/Anirudh257/Audio-files-extraction/blob/master/UNDERSTANDING%20LSTM%20.ipynb)

When I start training the network, my notebook becomes unresponsive, my firefox browser freezes and crashes after sometime. The kernel dies too.

What can be the reason for this?
Are these 2 arrays **noisy_movies** and **shifted_movies** too large for the notebook to process?
Is anybody else experiencing the same issue as well?

I would be glad if someone can help.

",0,Notebook dying too often.,"Notebook dying too often. I am trying to understand the code given here:
![conv_lstm](https://github.com/keras-team/keras/blob/master/examples/conv_lstm.py). 

I have implemented the whole code in a jupyter python notebook linked here:
![LSTM](https://github.com/Anirudh257/Audio-files-extraction/blob/master/UNDERSTANDING%20LSTM%20.ipynb)

When I start training the network, my notebook becomes unresponsive, my firefox browser freezes and crashes after sometime. The kernel dies too.

What can be the reason for this?
Are these 2 arrays **noisy_movies** and **shifted_movies** too large for the notebook to process?
Is anybody else experiencing the same issue as well?

I would be glad if someone can help.

"
keras,3854,"I have saved the model using _model.save('my_model.h5')_ it's working. However, when I try to load the model in a different project I get this error message: ValueError: Tensor(""cond/pred_id:0"", dtype=bool) must be from the same graph as Tensor(""dropout_1/mul_1:0"", shape=(?, 1, 256), dtype=float32).
Could this be a bug? Any idea?
",0,A Convolutional Neural Network Model Could not be loaded,"A Convolutional Neural Network Model Could not be loaded I have saved the model using _model.save('my_model.h5')_ it's working. However, when I try to load the model in a different project I get this error message: ValueError: Tensor(""cond/pred_id:0"", dtype=bool) must be from the same graph as Tensor(""dropout_1/mul_1:0"", shape=(?, 1, 256), dtype=float32).
Could this be a bug? Any idea?
"
keras,2372,"HI, 

Thanks for making such a wonderful tool!

I'm using Keras 1.0. I want to save and load the model both the arch and the parameters. So I use the method in FAQ. Here is the code.



When I load model and use model.predict(), there is a error:
AttributeError: 'NoneType' object has no attribute 'predict'

Don't know why. If I don't load the model from file, just train a model and use it, everything seems ok.

I checked the issues, most people just need to load the parameters. Is it possible when I load the architecture, I overwrite the old model and loose the model.predict()?

Thanks again for making Keras!

Ben
",0,problem of save/load model,"problem of save/load model HI, 

Thanks for making such a wonderful tool!

I'm using Keras 1.0. I want to save and load the model both the arch and the parameters. So I use the method in FAQ. Here is the code.



When I load model and use model.predict(), there is a error:
AttributeError: 'NoneType' object has no attribute 'predict'

Don't know why. If I don't load the model from file, just train a model and use it, everything seems ok.

I checked the issues, most people just need to load the parameters. Is it possible when I load the architecture, I overwrite the old model and loose the model.predict()?

Thanks again for making Keras!

Ben
"
keras,5191,"**Edit**

Docs state the below, but unroll is a parameter used by both backends.

> unroll: with TensorFlow the RNN is always unrolled, but with Theano you can use this boolean flag to unroll the RNN.

For theano backend we use scan but for tf we use while_loop, even through tf also supports scan. Also, does anyone know if using scan instead of while_loop has any performance or other benefits?

Cheers,
Ben

Please make sure that the boxes below are checked before you submit your issue. If your issue is an implementation question, please ask your question on [StackOverflow](http://stackoverflow.com/questions/tagged/keras) or [join the Keras Slack channel](https://keras-slack-autojoin.herokuapp.com/) and ask there instead of filing a GitHub issue.

Thank you!

- [x ] Check that you are up-to-date with the master branch of Keras. You can update with:
pip install git+git://github.com/fchollet/keras.git --upgrade --no-deps

- [x ] If running on TensorFlow, check that you are up-to-date with the latest version. The installation instructions can be found [here](https://www.tensorflow.org/get_started/os_setup).

- [x ] If running on Theano, check that you are up-to-date with the master branch of Theano. You can update with:
pip install git+git://github.com/Theano/Theano.git --upgrade --no-deps

- [ x] Provide a link to a GitHub Gist of a Python script that can reproduce your issue (or just copy the script here if it is short).
",0,Fix K.rnn documentation on unroll parameter,"Fix K.rnn documentation on unroll parameter **Edit**

Docs state the below, but unroll is a parameter used by both backends.

> unroll: with TensorFlow the RNN is always unrolled, but with Theano you can use this boolean flag to unroll the RNN.

For theano backend we use scan but for tf we use while_loop, even through tf also supports scan. Also, does anyone know if using scan instead of while_loop has any performance or other benefits?

Cheers,
Ben

Please make sure that the boxes below are checked before you submit your issue. If your issue is an implementation question, please ask your question on [StackOverflow](http://stackoverflow.com/questions/tagged/keras) or [join the Keras Slack channel](https://keras-slack-autojoin.herokuapp.com/) and ask there instead of filing a GitHub issue.

Thank you!

- [x ] Check that you are up-to-date with the master branch of Keras. You can update with:
pip install git+git://github.com/fchollet/keras.git --upgrade --no-deps

- [x ] If running on TensorFlow, check that you are up-to-date with the latest version. The installation instructions can be found [here](https://www.tensorflow.org/get_started/os_setup).

- [x ] If running on Theano, check that you are up-to-date with the master branch of Theano. You can update with:
pip install git+git://github.com/Theano/Theano.git --upgrade --no-deps

- [ x] Provide a link to a GitHub Gist of a Python script that can reproduce your issue (or just copy the script here if it is short).
"
