Repository,Number,Body,class,Title,Combined_Text
caffe,3075,"Hi,

I tried to run the example here: https://github.com/BVLC/caffe/blob/master/examples/00-classification.ipynb, but find that my result is quite different from the notebook's description,

Here is my result:
**Predicted class is #48281.**

And the example says:
**Predicted class is #281.**

Does anyone know why here it's different?

BTW, first I think it might be category#48 and category#281, but now I'm sure that it's not, since I check the _/data/ilsvrc12/synset_words.txt_ and think they're unrelated. I have done several times with the newest caffe repos and download the models from the _readme_'s url: [http://dl.caffe.berkeleyvision.org/bvlc_reference_caffenet.caffemodel](http://dl.caffe.berkeleyvision.org/bvlc_reference_caffenet.caffemodel), I tried to download(Model Zoo) with **./scripts/download_model_binary.py models/bvlc_reference_caffenet/** but it's too slow so I don't test this model, but I wonder that maybe this model is the same as the previous url.

Any suggestions will be much appreciated, Thanks.
",1,strange result when running blvc_reference_caffenet model,"strange result when running blvc_reference_caffenet model Hi,

I tried to run the example here: https://github.com/BVLC/caffe/blob/master/examples/00-classification.ipynb, but find that my result is quite different from the notebook's description,

Here is my result:
**Predicted class is #48281.**

And the example says:
**Predicted class is #281.**

Does anyone know why here it's different?

BTW, first I think it might be category#48 and category#281, but now I'm sure that it's not, since I check the _/data/ilsvrc12/synset_words.txt_ and think they're unrelated. I have done several times with the newest caffe repos and download the models from the _readme_'s url: [http://dl.caffe.berkeleyvision.org/bvlc_reference_caffenet.caffemodel](http://dl.caffe.berkeleyvision.org/bvlc_reference_caffenet.caffemodel), I tried to download(Model Zoo) with **./scripts/download_model_binary.py models/bvlc_reference_caffenet/** but it's too slow so I don't test this model, but I wonder that maybe this model is the same as the previous url.

Any suggestions will be much appreciated, Thanks.
"
caffe,4102,"I used a deploy.prototxt to initialize a new net, the batch_size is 256, loading from the deploy.prototxt



when I use the following code:



As after I run the forward(),the runing time is about 13 ms.
But when I change the size in deploy.prototxt,the forward() runing time as follows:

prototxt_size: 256  new_batch_size: 16    runing time: 13ms
prototxt_size: 16   new_batch_size: 16     runing time: 1.5ms
prototxt_size: 1024 new_batch_size: 16      runing time: 60ms

the method Reshape() seems did not work.
Thanks~
",1,Change the batch_size by the method Reshape(),"Change the batch_size by the method Reshape() I used a deploy.prototxt to initialize a new net, the batch_size is 256, loading from the deploy.prototxt



when I use the following code:



As after I run the forward(),the runing time is about 13 ms.
But when I change the size in deploy.prototxt,the forward() runing time as follows:

prototxt_size: 256  new_batch_size: 16    runing time: 13ms
prototxt_size: 16   new_batch_size: 16     runing time: 1.5ms
prototxt_size: 1024 new_batch_size: 16      runing time: 60ms

the method Reshape() seems did not work.
Thanks~
"
caffe,5643,"Hi all, I have a question to discuss with anyone who is interested in mobile applications. I have trained a caffemodel and the result is good(~35FPS) when I test it with my desktop, which is equipped with Nvidia Titan XP. But when I transform all these work into Nvidia Tx2, it can work but the detection speed is slow (2FPS). To solve this problem, I get an idea. Since I have got the trained model, I can convert the forward pass into C++/python scripts and get rid of caffe framework. Anyone has ever done same kind of things before and do you think it's workable and can improve detection speed? Any discussion is appreciated.
Thx",1,Convert caffemodel into C++/python scripts to improve detection speed?,"Convert caffemodel into C++/python scripts to improve detection speed? Hi all, I have a question to discuss with anyone who is interested in mobile applications. I have trained a caffemodel and the result is good(~35FPS) when I test it with my desktop, which is equipped with Nvidia Titan XP. But when I transform all these work into Nvidia Tx2, it can work but the detection speed is slow (2FPS). To solve this problem, I get an idea. Since I have got the trained model, I can convert the forward pass into C++/python scripts and get rid of caffe framework. Anyone has ever done same kind of things before and do you think it's workable and can improve detection speed? Any discussion is appreciated.
Thx"
caffe,3086,"I'm working on generating saliency maps though gradient data, as in [this](http://www.robots.ox.ac.uk/~vgg/publications/2014/Simonyan14a/simonyan14a.pdf) famous paper.

It works well when using pycaffe in CPU mode. When in GPU mode, all the gradients computed by the backward pass are zeros.

I'm using Ubuntu 14.04, VGG Net without a Data Layer and I set force_backward : true.

This is the code that works



This works, indeed


",1,PyCaffe backward() produces zero gradients when GPU mode,"PyCaffe backward() produces zero gradients when GPU mode I'm working on generating saliency maps though gradient data, as in [this](http://www.robots.ox.ac.uk/~vgg/publications/2014/Simonyan14a/simonyan14a.pdf) famous paper.

It works well when using pycaffe in CPU mode. When in GPU mode, all the gradients computed by the backward pass are zeros.

I'm using Ubuntu 14.04, VGG Net without a Data Layer and I set force_backward : true.

This is the code that works



This works, indeed


"
caffe,2448,"Hi everyone. When I run Caffe with CPU usage close to 100%,   each iteration needs only about 0.34s. However, after several iterations the CPU usage drops significantly, and each iteration needs about > 5s, and most of the time is for data IO. 

My system is Ubuntu 14.04 and the data layer is LMDB. Does anybody know how to solve the problem? Thanks!

Here is the comparison of running time:
1. High CPU usage:


1. Low CPU usage



Thanks! 
",1,CPU usage at a low level,"CPU usage at a low level Hi everyone. When I run Caffe with CPU usage close to 100%,   each iteration needs only about 0.34s. However, after several iterations the CPU usage drops significantly, and each iteration needs about > 5s, and most of the time is for data IO. 

My system is Ubuntu 14.04 and the data layer is LMDB. Does anybody know how to solve the problem? Thanks!

Here is the comparison of running time:
1. High CPU usage:


1. Low CPU usage



Thanks! 
"
caffe,2322,"I have trained a ConvNet and I am getting a loss of 0.0002. The output of the softmax containing the votes for each class is also very high for one class (0.99 in general, but some are also 0.8) and very low for the others.
Wouldn't that mean that the network is doing very accurate predictions? why am I then getting an accuracy of ""only"" 80%?

Thanks!
",1,"loss very small 0.0002 but accuracy ""not that high""","loss very small 0.0002 but accuracy ""not that high"" I have trained a ConvNet and I am getting a loss of 0.0002. The output of the softmax containing the votes for each class is also very high for one class (0.99 in general, but some are also 0.8) and very low for the others.
Wouldn't that mean that the network is doing very accurate predictions? why am I then getting an accuracy of ""only"" 80%?

Thanks!
"
caffe,6557,"Thank you for watching, I have spent lots of time but cannot find any solution.
## problem
when I was doing fine-tuning train, caffe hang without any error when  
*[layer_factory.hpp:77] Creating layer data*  
* my console shows that caffe initializing net from parameters and then create layers.  
It completed the first time but hang the second time.  *you can see it at the bottom*
**I wonder if there is no more memory to import parameters and create layers?**
* or what I changed to the origin is illeagal, such as
**when I change output num, it means that the num of parameters changed as well, so how origin caffelmodel file do to fit this change?**
* or it's a limit on Windows and imageData doesn't work
**the net uses imageData layer that will do image-lmdb automatically, but it doesn't happend, can it do on Windows?**
## what I changed
what I changed to the origin net is what finetuning should do, I think.
* I want to finetune a googLeNet,so I change output num of 3 Inproduct layer but I don't change their name
* I add propagate_down choice to some layers to avoid backward-evaluation.
* I changed source path,batch_size and some rate parameter.
* I decrease some parameter of solver file because my data set is small.
* I changed solver to CPU mode.
## terminal message
using git bash terminal.
",1,Caffe hang without any error prompt at Creating layer data when using  fine-tuning train,"Caffe hang without any error prompt at Creating layer data when using  fine-tuning train Thank you for watching, I have spent lots of time but cannot find any solution.
## problem
when I was doing fine-tuning train, caffe hang without any error when  
*[layer_factory.hpp:77] Creating layer data*  
* my console shows that caffe initializing net from parameters and then create layers.  
It completed the first time but hang the second time.  *you can see it at the bottom*
**I wonder if there is no more memory to import parameters and create layers?**
* or what I changed to the origin is illeagal, such as
**when I change output num, it means that the num of parameters changed as well, so how origin caffelmodel file do to fit this change?**
* or it's a limit on Windows and imageData doesn't work
**the net uses imageData layer that will do image-lmdb automatically, but it doesn't happend, can it do on Windows?**
## what I changed
what I changed to the origin net is what finetuning should do, I think.
* I want to finetune a googLeNet,so I change output num of 3 Inproduct layer but I don't change their name
* I add propagate_down choice to some layers to avoid backward-evaluation.
* I changed source path,batch_size and some rate parameter.
* I decrease some parameter of solver file because my data set is small.
* I changed solver to CPU mode.
## terminal message
using git bash terminal.
"
caffe,3239,"Running with the same model on the same dataset, the latest master with v3 took 25 minutes to finish 1000 iterations while v2 took 10 minutes. The kernels of the first three convolutional layers are of 9, 7 and 5 pixels. This problem was also [reported in a fork of Caffe](https://github.com/happynear/caffe-windows/issues/24) and [many other sources](https://www.google.co.in/?gws_rd=ssl#newwindow=1&q=cudnn+v3++slower).

Maybe cuDNN v3 only works well on the Maxwell architecture GPUs or just can't choose the most efficient convolution algorithm. The simplest solution is to revert to an older version of Caffe [since v2 was no longer supported](https://github.com/BVLC/caffe/pull/3218) before NVIDIA fix the problem. But we still want to keep Caffe up to date to experiment with some new features such as batch normalization. 

The second way is to specify the convolution algorithm explicitly which is not flexible enough for various model configurations. 

The last resort is to keep backwards compatibility with cuDNN v2 in Caffe.
",1,cuDNN v3 can be much slower than v2,"cuDNN v3 can be much slower than v2 Running with the same model on the same dataset, the latest master with v3 took 25 minutes to finish 1000 iterations while v2 took 10 minutes. The kernels of the first three convolutional layers are of 9, 7 and 5 pixels. This problem was also [reported in a fork of Caffe](https://github.com/happynear/caffe-windows/issues/24) and [many other sources](https://www.google.co.in/?gws_rd=ssl#newwindow=1&q=cudnn+v3++slower).

Maybe cuDNN v3 only works well on the Maxwell architecture GPUs or just can't choose the most efficient convolution algorithm. The simplest solution is to revert to an older version of Caffe [since v2 was no longer supported](https://github.com/BVLC/caffe/pull/3218) before NVIDIA fix the problem. But we still want to keep Caffe up to date to experiment with some new features such as batch normalization. 

The second way is to specify the convolution algorithm explicitly which is not flexible enough for various model configurations. 

The last resort is to keep backwards compatibility with cuDNN v2 in Caffe.
"
caffe,6061,"It seems that the speed bottleneck is not the data... Why the training is too slow? There's two gtx1080 I use.


Here's the network
",1,"very low speed to train network, I have some time test.","very low speed to train network, I have some time test. It seems that the speed bottleneck is not the data... Why the training is too slow? There's two gtx1080 I use.


Here's the network
"
caffe,6065,"### Issue summary
I use model definition(models/bvlc_alexnet/train_val.prototxt) and training protocol(models/bvlc_alexnet/solver.prototxt) provided by caffe to train alexnet on ImageNet.The only modification  I made is replacing mean.binaryproto with mean values computed by Facebook.And I have checked they are very close: 103.894,116.555,122.579 vs 103.94,116.78,123.68 .
But the accuracy on validation set I got is 55.06% which is observably low compared to the one reported by caffe comunity(57.1%,https://github.com/BVLC/caffe/tree/master/models/bvlc_alexnet).
The differences of mean values are tiny so I think there are other reasons making such a large drop in accuracy.
Then I use pre-trained caffemodel provided by caffe(http://dl.caffe.berkeleyvision.org/bvlc_alexnet.caffemodel) and get the accuracy.It still fall behind the reference accuracy, 55.9% vs 57.1%.
The imagenet lmdb dataset are created by script at  examples/imagenet/create_imagenet.sh with RESIZE=true.
Could anyone give some suggestions about this problem?
### Steps to reproduce
//train alexnet using caffe
./build/tools/caffe train -gpu 0 -solver models/bvlc_alexnet/solver.prototxt
//use pre-trained alexnet model
./build/tools/caffe test -gpu 0  -model models/bvlc_alexnet/train_val.prototxt -weights models/bvlc_alexnet.caffemodel
### Your system configuration
Operating system:Ubuntu 16.04
Compiler:nvcc-8,gcc-5.4.0
CUDA version (if applicable):CUDA8
CUDNN version (if applicable):cudnn515
BLAS:atlas ,cublas8.0
",1,unable to reproduce accuracy of bvlc-alexnet,"unable to reproduce accuracy of bvlc-alexnet ### Issue summary
I use model definition(models/bvlc_alexnet/train_val.prototxt) and training protocol(models/bvlc_alexnet/solver.prototxt) provided by caffe to train alexnet on ImageNet.The only modification  I made is replacing mean.binaryproto with mean values computed by Facebook.And I have checked they are very close: 103.894,116.555,122.579 vs 103.94,116.78,123.68 .
But the accuracy on validation set I got is 55.06% which is observably low compared to the one reported by caffe comunity(57.1%,https://github.com/BVLC/caffe/tree/master/models/bvlc_alexnet).
The differences of mean values are tiny so I think there are other reasons making such a large drop in accuracy.
Then I use pre-trained caffemodel provided by caffe(http://dl.caffe.berkeleyvision.org/bvlc_alexnet.caffemodel) and get the accuracy.It still fall behind the reference accuracy, 55.9% vs 57.1%.
The imagenet lmdb dataset are created by script at  examples/imagenet/create_imagenet.sh with RESIZE=true.
Could anyone give some suggestions about this problem?
### Steps to reproduce
//train alexnet using caffe
./build/tools/caffe train -gpu 0 -solver models/bvlc_alexnet/solver.prototxt
//use pre-trained alexnet model
./build/tools/caffe test -gpu 0  -model models/bvlc_alexnet/train_val.prototxt -weights models/bvlc_alexnet.caffemodel
### Your system configuration
Operating system:Ubuntu 16.04
Compiler:nvcc-8,gcc-5.4.0
CUDA version (if applicable):CUDA8
CUDNN version (if applicable):cudnn515
BLAS:atlas ,cublas8.0
"
caffe,6450,"### Issue summary

I have evaluated the speed of OpenCL vs CUDA Caffe on the OpenPose Project. This is the OpenPose model:

https://raw.githubusercontent.com/CMU-Perceptual-Computing-Lab/openpose/master/models/pose/coco/pose_deploy_linevec.prototxt

It seems to be significantly slower (2x)

https://pastebin.com/14xzRi3z

Eg:

For example conv2_1 which takes in a 3x186x186 feature map and applies a convolution takes 2.5 milliseconds on OpenCL and 0.86 milliseconds in CUDA



### Steps to reproduce


### System configuration

* Operating system: 16.04 Ubuntu
* Compiler: GCC 5.4
* CUDA version (if applicable): 9.0 + OPENCL (GTX 1080TI)
* CUDNN version (if applicable): 
* BLAS: 
* Python version (if using pycaffe): 
* MATLAB version (if using matcaffe): 
",1,OpenCL Speed Slower on Large Convolutions,"OpenCL Speed Slower on Large Convolutions ### Issue summary

I have evaluated the speed of OpenCL vs CUDA Caffe on the OpenPose Project. This is the OpenPose model:

https://raw.githubusercontent.com/CMU-Perceptual-Computing-Lab/openpose/master/models/pose/coco/pose_deploy_linevec.prototxt

It seems to be significantly slower (2x)

https://pastebin.com/14xzRi3z

Eg:

For example conv2_1 which takes in a 3x186x186 feature map and applies a convolution takes 2.5 milliseconds on OpenCL and 0.86 milliseconds in CUDA



### Steps to reproduce


### System configuration

* Operating system: 16.04 Ubuntu
* Compiler: GCC 5.4
* CUDA version (if applicable): 9.0 + OPENCL (GTX 1080TI)
* CUDNN version (if applicable): 
* BLAS: 
* Python version (if using pycaffe): 
* MATLAB version (if using matcaffe): 
"
caffe,3250,"Given a dataset with imbalanced class distribution learning a robust NN model is hard. In general, the network leads a bias toward the common classes and ignores the rare ones. For solving this issue, I target to rescale gradient signals regarding the class frequencies. I plan to follow these steps for the implementation;
- Compute class frequency coefficients which are less then or equal to 1 and 1 for the smallest class and  min_class_freq/class_freq for the other classes. These values are written to binary proto file as a list.
- I give an extra proto parameter to SoftmaxWithLoss layer as the file name of this freq. file similar to mean file used by the data layer.
- At the runtime, softmax layer read the file and define a blob including these freq values, at the setup.
- Then for each iteration I scale the gradient with corresponding frequency value indexed by the label value.

Here is the basic change on softmax_loss_layer.cu to perform the scaling.



Basically, I decoupled the GPU operations for a normal backpropagation and the scaled one and I scale the gradient value at SoftmaxLossBackwardGPU1.

Even I spend whole my week, I have a problem that cannot be solved. With any setting of learning rate, with this change, network ends up with exploding gradients after some iterations. However, it does not make sense since the frequency values are less then or equal to 1 then we should observe smaller gradients which might cause diminishing gradient but not explosion.

If someone can help me to point out the problem that would be great. I already asked this at the user forum but I could not get ans response. Sorry for the wrong platform for this question.
",1,Implementing gradient scaling with class frequencies on Softmax Loss Layer.,"Implementing gradient scaling with class frequencies on Softmax Loss Layer. Given a dataset with imbalanced class distribution learning a robust NN model is hard. In general, the network leads a bias toward the common classes and ignores the rare ones. For solving this issue, I target to rescale gradient signals regarding the class frequencies. I plan to follow these steps for the implementation;
- Compute class frequency coefficients which are less then or equal to 1 and 1 for the smallest class and  min_class_freq/class_freq for the other classes. These values are written to binary proto file as a list.
- I give an extra proto parameter to SoftmaxWithLoss layer as the file name of this freq. file similar to mean file used by the data layer.
- At the runtime, softmax layer read the file and define a blob including these freq values, at the setup.
- Then for each iteration I scale the gradient with corresponding frequency value indexed by the label value.

Here is the basic change on softmax_loss_layer.cu to perform the scaling.



Basically, I decoupled the GPU operations for a normal backpropagation and the scaled one and I scale the gradient value at SoftmaxLossBackwardGPU1.

Even I spend whole my week, I have a problem that cannot be solved. With any setting of learning rate, with this change, network ends up with exploding gradients after some iterations. However, it does not make sense since the frequency values are less then or equal to 1 then we should observe smaller gradients which might cause diminishing gradient but not explosion.

If someone can help me to point out the problem that would be great. I already asked this at the user forum but I could not get ans response. Sorry for the wrong platform for this question.
"
caffe,4026,"I posted this on the users group and a responder said it may be useful here.

valgrind will complete if leveldb is used instead of lmdb.

Compiled with GPU and run with GPU
==4315== LEAK SUMMARY:
==4315==    definitely lost: 96 bytes in 4 blocks
==4315==    indirectly lost: 0 bytes in 0 blocks
==4315==      possibly lost: 70,778,113 bytes in 20,095 blocks
==4315==    still reachable: 114,126,748 bytes in 153,749 blocks
==4315==         suppressed: 0 bytes in 0 blocks

Compiled with GPU and run as CPU
==5232== LEAK SUMMARY:
==5232==    definitely lost: 96 bytes in 4 blocks
==5232==    indirectly lost: 0 bytes in 0 blocks
==5232==      possibly lost: 70,775,503 bytes in 20,075 blocks
==5232==    still reachable: 114,116,564 bytes in 153,675 blocks
==5232==         suppressed: 0 bytes in 0 blocks

Compiled as CPU only
==12882== LEAK SUMMARY:
==12882==    definitely lost: 0 bytes in 0 blocks
==12882==    indirectly lost: 0 bytes in 0 blocks
==12882==      possibly lost: 86,909 bytes in 1,906 blocks
==12882==    still reachable: 552,984 bytes in 6,332 blocks
==12882==         suppressed: 0 bytes in 0 blocks

This valgrind page http://valgrind.org/docs/manual/faq.html#faq.deflost gives the following for 'possibly lost'.

""possibly lost"" means your program is leaking memory, unless you're doing unusual things with pointers that could cause them to point into the middle of an allocated block; see the user manual for some possible causes.

The mnist GPU run was repeated 273 times (just under 100 minutes) at which point the computer was freezing up from lack of memory. 'cat /proc/meminfo' was used to collect memory stats every 10 seconds during the sequence. Charts of the 42 stats in /proc/meminfo are combined on to a single page at https://github.com/neilnelson/misc/blob/master/meminfo.png. (Right click, select View Image, click on the image to expand.)

The interesting charts are
  MemFree declines to about zero (175372 bytes) at the 219 run.
  Buffers stays the same until MemFree gets to zero and then declines to zero
  Cached is similar to Buffers
  SwapFree declines (an increase in swap usage) when MemFree gets to zero.

Just before LEAK SUMMARY in the 'compiled with GPU and run as CPU' run instance is the following 'possibly lost' section.

==5232== 22,144,025 bytes in 2,207 blocks are possibly lost in loss record 2,543 of 2,544
==5232==    at 0x4C2AB80: malloc (in /usr/lib/valgrind/vgpreload_memcheck-amd64-linux.so)
==5232==    by 0x28E90F1D: ??? (in /usr/lib/x86_64-linux-gnu/libcuda.so.352.63)
==5232==    by 0x28E4134C: ??? (in /usr/lib/x86_64-linux-gnu/libcuda.so.352.63)
==5232==    by 0x28E5211F: ??? (in /usr/lib/x86_64-linux-gnu/libcuda.so.352.63)
==5232==    by 0x28F3FCCF: ??? (in /usr/lib/x86_64-linux-gnu/libcuda.so.352.63)
==5232==    by 0x28F3FFDF: ??? (in /usr/lib/x86_64-linux-gnu/libcuda.so.352.63)
==5232==    by 0xE43FD2C: ??? (in /usr/local/cuda-7.0/targets/x86_64-linux/lib/libcudnn.so.4)
==5232==    by 0xE432AAF: ??? (in /usr/local/cuda-7.0/targets/x86_64-linux/lib/libcudnn.so.4)
==5232==    by 0xE43EDB6: ??? (in /usr/local/cuda-7.0/targets/x86_64-linux/lib/libcudnn.so.4)
==5232==    by 0xE443570: ??? (in /usr/local/cuda-7.0/targets/x86_64-linux/lib/libcudnn.so.4)
==5232==    by 0xE4371DB: ??? (in /usr/local/cuda-7.0/targets/x86_64-linux/lib/libcudnn.so.4)
==5232==    by 0xE4256A1: ??? (in /usr/local/cuda-7.0/targets/x86_64-linux/lib/libcudnn.so.4)
==5232==    by 0xE458C9E: ??? (in /usr/local/cuda-7.0/targets/x86_64-linux/lib/libcudnn.so.4)
==5232==    by 0xE146C11: cudnnCreate (in /usr/local/cuda-7.0/targets/x86_64-linux/lib/libcudnn.so.4)
==5232==    by 0x51CE276: caffe::CuDNNConvolutionLayer<float>::LayerSetUp(std::vectorcaffe::Blob<float_, std::allocatorcaffe::Blob<float_> > const&, std::vectorcaffe::Blob<float_, std::allocatorcaffe::Blob<float_> > const&) (cudnn_conv_layer.cpp:53)
==5232==    by 0x518EC4B: caffe::Layer<float>::SetUp(std::vectorcaffe::Blob<float_, std::allocatorcaffe::Blob<float_> > const&, std::vectorcaffe::Blob<float_, std::allocatorcaffe::Blob<float_> > const&) (layer.hpp:71)
==5232==    by 0x51946E0: caffe::Net<float>::Init(caffe::NetParameter const&) (net.cpp:139)
==5232==    by 0x5192A76: caffe::Net<float>::Net(caffe::NetParameter const&, caffe::Net<float> const*) (net.cpp:27)
==5232==    by 0x516BB72: caffe::Solver<float>::InitTrainNet() (solver.cpp:105)
==5232==    by 0x516B395: caffe::Solver<float>::Init(caffe::SolverParameter const&) (solver.cpp:57)

This seems a little odd in that the 22,144,025 possibly lost bytes is related to Cuda libs out of caffe::CuDNNConvolutionLayer. That is, if the GPU is not being used in a CPU run, Cuda libs would not expected to be used as shown.

The 'possibly lost' figure of the 'Compiled as CPU only' LEAK SUMMARY is insignificant when compared to the figures from the Cuda compiled code.

The memory decline over the first 192 Cuda mnist runs averages to 59.8 megabytes per run. valgrind shows 67.5 megabytes 'possibly lost' for a single run.
",1,Caffe memory leaks,"Caffe memory leaks I posted this on the users group and a responder said it may be useful here.

valgrind will complete if leveldb is used instead of lmdb.

Compiled with GPU and run with GPU
==4315== LEAK SUMMARY:
==4315==    definitely lost: 96 bytes in 4 blocks
==4315==    indirectly lost: 0 bytes in 0 blocks
==4315==      possibly lost: 70,778,113 bytes in 20,095 blocks
==4315==    still reachable: 114,126,748 bytes in 153,749 blocks
==4315==         suppressed: 0 bytes in 0 blocks

Compiled with GPU and run as CPU
==5232== LEAK SUMMARY:
==5232==    definitely lost: 96 bytes in 4 blocks
==5232==    indirectly lost: 0 bytes in 0 blocks
==5232==      possibly lost: 70,775,503 bytes in 20,075 blocks
==5232==    still reachable: 114,116,564 bytes in 153,675 blocks
==5232==         suppressed: 0 bytes in 0 blocks

Compiled as CPU only
==12882== LEAK SUMMARY:
==12882==    definitely lost: 0 bytes in 0 blocks
==12882==    indirectly lost: 0 bytes in 0 blocks
==12882==      possibly lost: 86,909 bytes in 1,906 blocks
==12882==    still reachable: 552,984 bytes in 6,332 blocks
==12882==         suppressed: 0 bytes in 0 blocks

This valgrind page http://valgrind.org/docs/manual/faq.html#faq.deflost gives the following for 'possibly lost'.

""possibly lost"" means your program is leaking memory, unless you're doing unusual things with pointers that could cause them to point into the middle of an allocated block; see the user manual for some possible causes.

The mnist GPU run was repeated 273 times (just under 100 minutes) at which point the computer was freezing up from lack of memory. 'cat /proc/meminfo' was used to collect memory stats every 10 seconds during the sequence. Charts of the 42 stats in /proc/meminfo are combined on to a single page at https://github.com/neilnelson/misc/blob/master/meminfo.png. (Right click, select View Image, click on the image to expand.)

The interesting charts are
  MemFree declines to about zero (175372 bytes) at the 219 run.
  Buffers stays the same until MemFree gets to zero and then declines to zero
  Cached is similar to Buffers
  SwapFree declines (an increase in swap usage) when MemFree gets to zero.

Just before LEAK SUMMARY in the 'compiled with GPU and run as CPU' run instance is the following 'possibly lost' section.

==5232== 22,144,025 bytes in 2,207 blocks are possibly lost in loss record 2,543 of 2,544
==5232==    at 0x4C2AB80: malloc (in /usr/lib/valgrind/vgpreload_memcheck-amd64-linux.so)
==5232==    by 0x28E90F1D: ??? (in /usr/lib/x86_64-linux-gnu/libcuda.so.352.63)
==5232==    by 0x28E4134C: ??? (in /usr/lib/x86_64-linux-gnu/libcuda.so.352.63)
==5232==    by 0x28E5211F: ??? (in /usr/lib/x86_64-linux-gnu/libcuda.so.352.63)
==5232==    by 0x28F3FCCF: ??? (in /usr/lib/x86_64-linux-gnu/libcuda.so.352.63)
==5232==    by 0x28F3FFDF: ??? (in /usr/lib/x86_64-linux-gnu/libcuda.so.352.63)
==5232==    by 0xE43FD2C: ??? (in /usr/local/cuda-7.0/targets/x86_64-linux/lib/libcudnn.so.4)
==5232==    by 0xE432AAF: ??? (in /usr/local/cuda-7.0/targets/x86_64-linux/lib/libcudnn.so.4)
==5232==    by 0xE43EDB6: ??? (in /usr/local/cuda-7.0/targets/x86_64-linux/lib/libcudnn.so.4)
==5232==    by 0xE443570: ??? (in /usr/local/cuda-7.0/targets/x86_64-linux/lib/libcudnn.so.4)
==5232==    by 0xE4371DB: ??? (in /usr/local/cuda-7.0/targets/x86_64-linux/lib/libcudnn.so.4)
==5232==    by 0xE4256A1: ??? (in /usr/local/cuda-7.0/targets/x86_64-linux/lib/libcudnn.so.4)
==5232==    by 0xE458C9E: ??? (in /usr/local/cuda-7.0/targets/x86_64-linux/lib/libcudnn.so.4)
==5232==    by 0xE146C11: cudnnCreate (in /usr/local/cuda-7.0/targets/x86_64-linux/lib/libcudnn.so.4)
==5232==    by 0x51CE276: caffe::CuDNNConvolutionLayer<float>::LayerSetUp(std::vectorcaffe::Blob<float_, std::allocatorcaffe::Blob<float_> > const&, std::vectorcaffe::Blob<float_, std::allocatorcaffe::Blob<float_> > const&) (cudnn_conv_layer.cpp:53)
==5232==    by 0x518EC4B: caffe::Layer<float>::SetUp(std::vectorcaffe::Blob<float_, std::allocatorcaffe::Blob<float_> > const&, std::vectorcaffe::Blob<float_, std::allocatorcaffe::Blob<float_> > const&) (layer.hpp:71)
==5232==    by 0x51946E0: caffe::Net<float>::Init(caffe::NetParameter const&) (net.cpp:139)
==5232==    by 0x5192A76: caffe::Net<float>::Net(caffe::NetParameter const&, caffe::Net<float> const*) (net.cpp:27)
==5232==    by 0x516BB72: caffe::Solver<float>::InitTrainNet() (solver.cpp:105)
==5232==    by 0x516B395: caffe::Solver<float>::Init(caffe::SolverParameter const&) (solver.cpp:57)

This seems a little odd in that the 22,144,025 possibly lost bytes is related to Cuda libs out of caffe::CuDNNConvolutionLayer. That is, if the GPU is not being used in a CPU run, Cuda libs would not expected to be used as shown.

The 'possibly lost' figure of the 'Compiled as CPU only' LEAK SUMMARY is insignificant when compared to the figures from the Cuda compiled code.

The memory decline over the first 192 Cuda mnist runs averages to 59.8 megabytes per run. valgrind shows 67.5 megabytes 'possibly lost' for a single run.
"
caffe,5051,"### Issue summary

I just installed Caffe and tried to run the MNIST Lenet example on CPU only mode (changed solved_mode to CPU). And the loss is NAN from the first display. 

> I1202 07:38:36.619820 20673 caffe.cpp:251] Starting Optimization
> I1202 07:38:36.619825 20673 solver.cpp:279] Solving LeNet
> I1202 07:38:36.619829 20673 solver.cpp:280] Learning Rate Policy: inv
> I1202 07:38:36.620288 20673 solver.cpp:337] Iteration 0, Testing net (#0)
> I1202 07:38:40.081092 20673 solver.cpp:404]     Test net output #0: accuracy = 0.1034
> I1202 07:38:40.081132 20673 solver.cpp:404]     Test net output #1: loss = 71.5575 (* 1 = 71.5575 loss)
> I1202 07:38:40.133375 20673 solver.cpp:228] Iteration 0, loss = 68.1442
> I1202 07:38:40.133416 20673 solver.cpp:244]     Train net output #0: loss = 68.1442 (* 1 = 68.1442 loss)
> I1202 07:38:40.133440 20673 sgd_solver.cpp:106] Iteration 0, lr = 0.01
> I1202 07:38:45.040522 20673 solver.cpp:228] Iteration 100, loss = -nan
> I1202 07:38:45.040565 20673 solver.cpp:244]     Train net output #0: loss = -nan (* 1 = -nan loss)
> I1202 07:38:45.040580 20673 sgd_solver.cpp:106] Iteration 100, lr = 0.00992565
> I1202 07:38:49.861625 20673 solver.cpp:228] Iteration 200, loss = -nan
> I1202 07:38:49.861666 20673 solver.cpp:244]     Train net output #0: loss = -nan (* 1 = -nan loss)
> I1202 07:38:49.861677 20673 sgd_solver.cpp:106] Iteration 200, lr = 0.00985258

If I change it back to GPU mode, it does not happen.

What can I do to train it in CPU mode ? 
Is there a stable release of Caffe that I should try ? 

Thanks

### Steps to reproduce

(HEAD 24d2f67173db3344141dce24b1008efffbfe1c7d)

git clone https://github.com/BVLC/caffe caffe
cd caffe
make all
./data/mnist/get_mnist.sh
./examples/mnist/create_mnist.sh
./examples/mnist/train_lenet.sh

### Your system configuration
Operating system: Gentoo
Compiler: G++ 4.9.3
CUDA version (if applicable): 7.5.18
BLAS: MKL",1,NaN loss when training MNIST LeNet in CPU mode,"NaN loss when training MNIST LeNet in CPU mode ### Issue summary

I just installed Caffe and tried to run the MNIST Lenet example on CPU only mode (changed solved_mode to CPU). And the loss is NAN from the first display. 

> I1202 07:38:36.619820 20673 caffe.cpp:251] Starting Optimization
> I1202 07:38:36.619825 20673 solver.cpp:279] Solving LeNet
> I1202 07:38:36.619829 20673 solver.cpp:280] Learning Rate Policy: inv
> I1202 07:38:36.620288 20673 solver.cpp:337] Iteration 0, Testing net (#0)
> I1202 07:38:40.081092 20673 solver.cpp:404]     Test net output #0: accuracy = 0.1034
> I1202 07:38:40.081132 20673 solver.cpp:404]     Test net output #1: loss = 71.5575 (* 1 = 71.5575 loss)
> I1202 07:38:40.133375 20673 solver.cpp:228] Iteration 0, loss = 68.1442
> I1202 07:38:40.133416 20673 solver.cpp:244]     Train net output #0: loss = 68.1442 (* 1 = 68.1442 loss)
> I1202 07:38:40.133440 20673 sgd_solver.cpp:106] Iteration 0, lr = 0.01
> I1202 07:38:45.040522 20673 solver.cpp:228] Iteration 100, loss = -nan
> I1202 07:38:45.040565 20673 solver.cpp:244]     Train net output #0: loss = -nan (* 1 = -nan loss)
> I1202 07:38:45.040580 20673 sgd_solver.cpp:106] Iteration 100, lr = 0.00992565
> I1202 07:38:49.861625 20673 solver.cpp:228] Iteration 200, loss = -nan
> I1202 07:38:49.861666 20673 solver.cpp:244]     Train net output #0: loss = -nan (* 1 = -nan loss)
> I1202 07:38:49.861677 20673 sgd_solver.cpp:106] Iteration 200, lr = 0.00985258

If I change it back to GPU mode, it does not happen.

What can I do to train it in CPU mode ? 
Is there a stable release of Caffe that I should try ? 

Thanks

### Steps to reproduce

(HEAD 24d2f67173db3344141dce24b1008efffbfe1c7d)

git clone https://github.com/BVLC/caffe caffe
cd caffe
make all
./data/mnist/get_mnist.sh
./examples/mnist/create_mnist.sh
./examples/mnist/train_lenet.sh

### Your system configuration
Operating system: Gentoo
Compiler: G++ 4.9.3
CUDA version (if applicable): 7.5.18
BLAS: MKL"
caffe,2624,"## CPU and GPU traning should have given the same result, but no!

I am training Cifar10 example with my own architecture shown below. I think it is suppose to be that CPU or GPU would give roughly the same accuracy and loss values.
With all other parameters being the same except CPU or GPU, my two experiment has much different test accuracy. 

GPU: 
I0619 12:18:22.621153 15991 solver.cpp:269] Iteration 5000, Testing net (#0)
I0619 12:18:22.898356 15991 solver.cpp:318]     Test net output #0: accuracy = 0.5712
I0619 12:18:22.898421 15991 solver.cpp:318]     Test net output #1: loss = 1.35011 (\* 1 = 1.35011 loss)

CPU:
I0619 12:28:27.662317 15616 solver.cpp:269] Iteration 5000, Testing net (#0)
I0619 12:28:31.197854 15616 solver.cpp:318]     Test net output #0: accuracy = 0.3764
I0619 12:28:31.197901 15616 solver.cpp:318]     Test net output #1: loss = 1.78204 (\* 1 = 1.78204 loss)
## Here is my solver.prototxt

net: ""examples/cifar10/cifar10_cs231n_train_test.prototxt""
test_iter: 100
test_interval: 500
base_lr: 0.0001
momentum: 0.9
weight_decay: 0.001
lr_policy: ""fixed""
display: 100
max_iter: 5000
snapshot: 500
snapshot_prefix: ""examples/cifar10/cifar10_cs231n""
solver_mode: GPU

---
## Here is my train_test.prototxt

name: ""CIFAR10_cs231n""
layers {
  name: ""cifar""
  type: DATA
  top: ""data""
  top: ""label""
  data_param {
    source: ""examples/cifar10/cifar10_train_lmdb""
    batch_size: 100
    backend: LMDB
  }
  transform_param {
    mean_file: ""examples/cifar10/mean.binaryproto""
  }
  include: { phase: TRAIN }
}
layers {
  name: ""cifar""
  type: DATA
  top: ""data""
  top: ""label""
  data_param {
    source: ""examples/cifar10/cifar10_test_lmdb""
    batch_size: 100
    backend: LMDB
  }
  transform_param {
    mean_file: ""examples/cifar10/mean.binaryproto""
  }
  include: { phase: TEST }
}
layers {
  name: ""conv1""
  type: CONVOLUTION
  bottom: ""data""
  top: ""conv1""
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 32
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: ""constant""
      value: 0.001
    }
    bias_filler {
      type: ""constant""
      value: 0.001
    }
  }
}
layers {
  name: ""relu1""
  type: RELU
  bottom: ""conv1""
  top: ""relu1""
}
layers {
  name: ""pool1""
  type: POOLING
  bottom: ""relu1""
  top: ""pool1""
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  name: ""ip1""
  type: INNER_PRODUCT
  bottom: ""pool1""
  top: ""ip1""
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 10
    weight_filler {
      type: ""constant""
      value: 0.001
    }
    bias_filler {
      type: ""constant""
      value: 0.001
    }
  }
}
layers {
  name: ""accuracy""
  type: ACCURACY
  bottom: ""ip1""
  bottom: ""label""
  top: ""accuracy""
  include: { phase: TEST }
}
layers {
  name: ""loss""
  type: SOFTMAX_LOSS
  bottom: ""ip1""
  bottom: ""label""
  top: ""loss""
## }
",1,20% testing accuracy difference CPU and GPU in CIFAR10 dataset,"20% testing accuracy difference CPU and GPU in CIFAR10 dataset ## CPU and GPU traning should have given the same result, but no!

I am training Cifar10 example with my own architecture shown below. I think it is suppose to be that CPU or GPU would give roughly the same accuracy and loss values.
With all other parameters being the same except CPU or GPU, my two experiment has much different test accuracy. 

GPU: 
I0619 12:18:22.621153 15991 solver.cpp:269] Iteration 5000, Testing net (#0)
I0619 12:18:22.898356 15991 solver.cpp:318]     Test net output #0: accuracy = 0.5712
I0619 12:18:22.898421 15991 solver.cpp:318]     Test net output #1: loss = 1.35011 (\* 1 = 1.35011 loss)

CPU:
I0619 12:28:27.662317 15616 solver.cpp:269] Iteration 5000, Testing net (#0)
I0619 12:28:31.197854 15616 solver.cpp:318]     Test net output #0: accuracy = 0.3764
I0619 12:28:31.197901 15616 solver.cpp:318]     Test net output #1: loss = 1.78204 (\* 1 = 1.78204 loss)
## Here is my solver.prototxt

net: ""examples/cifar10/cifar10_cs231n_train_test.prototxt""
test_iter: 100
test_interval: 500
base_lr: 0.0001
momentum: 0.9
weight_decay: 0.001
lr_policy: ""fixed""
display: 100
max_iter: 5000
snapshot: 500
snapshot_prefix: ""examples/cifar10/cifar10_cs231n""
solver_mode: GPU

---
## Here is my train_test.prototxt

name: ""CIFAR10_cs231n""
layers {
  name: ""cifar""
  type: DATA
  top: ""data""
  top: ""label""
  data_param {
    source: ""examples/cifar10/cifar10_train_lmdb""
    batch_size: 100
    backend: LMDB
  }
  transform_param {
    mean_file: ""examples/cifar10/mean.binaryproto""
  }
  include: { phase: TRAIN }
}
layers {
  name: ""cifar""
  type: DATA
  top: ""data""
  top: ""label""
  data_param {
    source: ""examples/cifar10/cifar10_test_lmdb""
    batch_size: 100
    backend: LMDB
  }
  transform_param {
    mean_file: ""examples/cifar10/mean.binaryproto""
  }
  include: { phase: TEST }
}
layers {
  name: ""conv1""
  type: CONVOLUTION
  bottom: ""data""
  top: ""conv1""
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 32
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: ""constant""
      value: 0.001
    }
    bias_filler {
      type: ""constant""
      value: 0.001
    }
  }
}
layers {
  name: ""relu1""
  type: RELU
  bottom: ""conv1""
  top: ""relu1""
}
layers {
  name: ""pool1""
  type: POOLING
  bottom: ""relu1""
  top: ""pool1""
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layers {
  name: ""ip1""
  type: INNER_PRODUCT
  bottom: ""pool1""
  top: ""ip1""
  blobs_lr: 1
  blobs_lr: 2
  inner_product_param {
    num_output: 10
    weight_filler {
      type: ""constant""
      value: 0.001
    }
    bias_filler {
      type: ""constant""
      value: 0.001
    }
  }
}
layers {
  name: ""accuracy""
  type: ACCURACY
  bottom: ""ip1""
  bottom: ""label""
  top: ""accuracy""
  include: { phase: TEST }
}
layers {
  name: ""loss""
  type: SOFTMAX_LOSS
  bottom: ""ip1""
  bottom: ""label""
  top: ""loss""
## }
"
caffe,4416,"Hello. New caffe-user here. 

I have been having this particularly annoying bug that I can't seem to solve.

I have a CNN that takes two images (one of them is the GT). When I start the training, the loss starts at 0 and stays there. 

I tried to play on the parameters of the solver to no avil. I also tried to change the parameters. 

The model is defined here: http://pastebin.com/t9Q7i0iZ 

An image of the model: http://imgur.com/RBBlvfb

The solver: http://pastebin.com/sUjg8mpe

And the command I used was:



And lastly the log: http://pastebin.com/B5DhJYVg

NOTE: I have been having this problem for a week and I did ask this question on stackoverflow, and on the google group (twice). I hope it is okay to ask it here.
",1,Loss stuck at 0,"Loss stuck at 0 Hello. New caffe-user here. 

I have been having this particularly annoying bug that I can't seem to solve.

I have a CNN that takes two images (one of them is the GT). When I start the training, the loss starts at 0 and stays there. 

I tried to play on the parameters of the solver to no avil. I also tried to change the parameters. 

The model is defined here: http://pastebin.com/t9Q7i0iZ 

An image of the model: http://imgur.com/RBBlvfb

The solver: http://pastebin.com/sUjg8mpe

And the command I used was:



And lastly the log: http://pastebin.com/B5DhJYVg

NOTE: I have been having this problem for a week and I did ask this question on stackoverflow, and on the google group (twice). I hope it is okay to ask it here.
"
caffe,3907,"I built caffe windows successfully.
However, this is so slow compared with happynear's caffe-windows.
I did nothing with default props file except cudaArchitecture, means compiled with GPU enable and useCudnn.
My GPU is gtx 750 and it's cc is 50, so i changed cudaArchitecture like below <CudaArchitecture>compute_35,sm_35;compute_50,sm_50</CudaArchitecture>
I embedded cudnnv4. what's wrong?
Thanks in advance.
",1,Caffe windows is slow.,"Caffe windows is slow. I built caffe windows successfully.
However, this is so slow compared with happynear's caffe-windows.
I did nothing with default props file except cudaArchitecture, means compiled with GPU enable and useCudnn.
My GPU is gtx 750 and it's cc is 50, so i changed cudaArchitecture like below <CudaArchitecture>compute_35,sm_35;compute_50,sm_50</CudaArchitecture>
I embedded cudnnv4. what's wrong?
Thanks in advance.
"
caffe,4038,"Hi everyone, I am new to caffe and I have already compiled the [Microsoft windows version](https://github.com/Microsoft/caffe) with VS2013 community, Matlab 2015a, CUDA 7.5 and cuDNN v4. My PC is Win10 with GTX 765M. The compilation is successful without error.

The problem is that when I try to run the mnist example, I can get reasonable accuracy with CPU training (0.97 accuracy after 500 iterations). However, when I turn to GPU mode, it only gives 0.11 accuracy even after 10000 iterations. This is the same whether I use bat and run caffe.exe or run the corresponding matlab wrapper.

In both Cpu and Gpu mode, the solver and network structure is just the lenet in /example/mnist/. The log file for gpu training is attched.

[log_gpu.txt](https://github.com/BVLC/caffe/files/233292/log_gpu.txt)

If anyone has encountered similar problem, please help me out. Thank you!
",1,Low accuracy (11%) on mnist with gpu,"Low accuracy (11%) on mnist with gpu Hi everyone, I am new to caffe and I have already compiled the [Microsoft windows version](https://github.com/Microsoft/caffe) with VS2013 community, Matlab 2015a, CUDA 7.5 and cuDNN v4. My PC is Win10 with GTX 765M. The compilation is successful without error.

The problem is that when I try to run the mnist example, I can get reasonable accuracy with CPU training (0.97 accuracy after 500 iterations). However, when I turn to GPU mode, it only gives 0.11 accuracy even after 10000 iterations. This is the same whether I use bat and run caffe.exe or run the corresponding matlab wrapper.

In both Cpu and Gpu mode, the solver and network structure is just the lenet in /example/mnist/. The log file for gpu training is attched.

[log_gpu.txt](https://github.com/BVLC/caffe/files/233292/log_gpu.txt)

If anyone has encountered similar problem, please help me out. Thank you!
"
caffe,2895,"I was debugging a network with two loss layers and wanted to disable one of them (a SoftmaxWithLossLayer), as such I set the loss_weight to 0. However, this does not do what I expected at all. The clearest way to explain this is probably using an example on how to reproduce it.

To reproduce one can take the examples/mnist/lenet_train_test.prototxt and add a second loss layer, with weight 0:



and then run this python script:



The diff for the split belonging to the SoftmaxWithLoss with loss_weight 0 will contain 64 (batchsize) values equal to the loss (NOT the gradient) for that input, and all the other elements will be 0. The other split will correctly contain all the diff values (64*10) for the loss with weight 1.

However, these two splits still get combined, creating the diff for 'ip2' for which the first 64 values are not comparable to the last 576. Am I wrong in how I tried to use the loss_weight or is this a bug? (It doesn't seem to be specific to SoftmaxWithLoss, though its most clear for this layer).
",1,Incorrect gradient from a SoftmaxWithLossLayer with loss_weight 0,"Incorrect gradient from a SoftmaxWithLossLayer with loss_weight 0 I was debugging a network with two loss layers and wanted to disable one of them (a SoftmaxWithLossLayer), as such I set the loss_weight to 0. However, this does not do what I expected at all. The clearest way to explain this is probably using an example on how to reproduce it.

To reproduce one can take the examples/mnist/lenet_train_test.prototxt and add a second loss layer, with weight 0:



and then run this python script:



The diff for the split belonging to the SoftmaxWithLoss with loss_weight 0 will contain 64 (batchsize) values equal to the loss (NOT the gradient) for that input, and all the other elements will be 0. The other split will correctly contain all the diff values (64*10) for the loss with weight 1.

However, these two splits still get combined, creating the diff for 'ip2' for which the first 64 values are not comparable to the last 576. Am I wrong in how I tried to use the loss_weight or is this a bug? (It doesn't seem to be specific to SoftmaxWithLoss, though its most clear for this layer).
"
caffe,2515,"I recently started to use matcaffe and it seems the output value is not between [0 and 1] as the python caffe (after softmax). The output score of matcaffe could be minus or larger than one. 

I just used the default googlenet. 

Anyone experience the same issue?
",1,Matcaffe: output score value is not between [0 and 1],"Matcaffe: output score value is not between [0 and 1] I recently started to use matcaffe and it seems the output value is not between [0 and 1] as the python caffe (after softmax). The output score of matcaffe could be minus or larger than one. 

I just used the default googlenet. 

Anyone experience the same issue?
"
caffe,5845,"Hello everyone.
I've trained CNN to classify 7 type of classes. But during tests I noticed that for some images classification results via DIGITS GUI and via python interface are different. Can anyone please help me how can i get exact same results which produce by DIGITS.? Which is the best code to load DIGITS trained model and classify image.",1,Prediction results different from DIGITS,"Prediction results different from DIGITS Hello everyone.
I've trained CNN to classify 7 type of classes. But during tests I noticed that for some images classification results via DIGITS GUI and via python interface are different. Can anyone please help me how can i get exact same results which produce by DIGITS.? Which is the best code to load DIGITS trained model and classify image."
caffe,4950,"### Issue summary
*Tried to ask this on googlegroup, didn't get a response that I had to post it here!*
I have a 50x50 image generated from a 1x2500 vector and I then generated HDF5 file from the images and feed it to caffeNet. My labels range from 0 to 255, so I am expecting 256 different classes. Below is my network prototxt and my solver. 
layer {
name: ""data""
type: ""HDF5Data""
top: ""X""
top: ""y""
hdf5_data_param{
source:""/Path/to/trainh5list.txt""
batch_size: 1
}
include{phase: TRAIN}
}
layer {
name: ""data""
type: ""HDF5Data""
top: ""X""
top: ""y""
hdf5_data_param{
source:""/Path/to/testh5list.txt""
batch_size: 1
}
include{phase: TEST}
}
....

layer {
name: ""accuracy""
type: ""Accuracy""
bottom: ""fc8""
bottom: ""y""
top: ""accuracy""
include {
phase: TEST
}
}
layer {
name: ""loss""
type: ""SoftmaxWithLoss""
bottom: ""fc8""
bottom: ""y""
top: ""loss""
}

Solver: 
net: ""/path/to/train_ntk.prototxt""
test_iter: 20
test_interval: 1000
base_lr: 0.001
lr_policy: ""step""
gamma: 0.1
stepsize: 10000
display: 1000
max_iter: 80000
momentum: 0.9
weight_decay: 0.0005
snapshot: 40000
snapshot_prefix: ""/path/to/model_""
solver_mode: GPU
#debug_info: true

While training in debug mode, 1. The accuracy layer has zero output all the time. 2. The fully connected layers seems to have the same result for all the inputs. 3. The loss doesn't decrease as the training goes on, it swings in some random value. I tried different batch size and learning rate, I don't seem to get it fixed. I also tried changing the data layer to ImageData layer, it didn't solve the issue. What can I do different to fix this issue?

Here is the training log: 
I1103 12:01:41.822055 108615 solver.cpp:337] Iteration 0, Testing net (#0)
I1103 12:01:41.849742 108615 solver.cpp:404]     Test net output #0: accuracy = 0
I1103 12:01:41.849761 108615 solver.cpp:404]     Test net output #1: loss = 6.02617 (* 1 = 6.02617 loss)
I1103 12:01:41.869380 108615 solver.cpp:228] Iteration 0, loss = 6.05644
I1103 12:01:41.869398 108615 solver.cpp:244]     Train net output #0: loss = 6.05644 (* 1 = 6.05644 loss)
I1103 12:01:41.869413 108615 sgd_solver.cpp:106] Iteration 0, lr = 0.1
I1103 12:01:47.624855 108615 solver.cpp:228] Iteration 500, loss = 87.3365
I1103 12:01:47.624876 108615 solver.cpp:244]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I1103 12:01:47.624882 108615 sgd_solver.cpp:106] Iteration 500, lr = 0.1
I1103 12:01:53.290213 108615 solver.cpp:337] Iteration 1000, Testing net (#0)
I1103 12:01:53.299310 108615 solver.cpp:404]     Test net output #0: accuracy = 0
I1103 12:01:53.299327 108615 solver.cpp:404]     Test net output #1: loss = 87.3365 (* 1 = 87.3365 loss)
I1103 12:01:53.314584 108615 solver.cpp:228] Iteration 1000, loss = 87.3365
I1103 12:01:53.314615 108615 solver.cpp:244]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I1103 12:01:53.314621 108615 sgd_solver.cpp:106] Iteration 1000, lr = 0.01
I1103 12:01:58.991268 108615 solver.cpp:228] Iteration 1500, loss = 87.3365
I1103 12:01:58.991315 108615 solver.cpp:244]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I1103 12:01:58.991322 108615 sgd_solver.cpp:106] Iteration 1500, lr = 0.01
I1103 12:02:04.664419 108615 solver.cpp:337] Iteration 2000, Testing net (#0)
I1103 12:02:04.673518 108615 solver.cpp:404]     Test net output #0: accuracy = 0
I1103 12:02:04.673537 108615 solver.cpp:404]     Test net output #1: loss = 87.3365 (* 1 = 87.3365 loss)
I1103 12:02:04.690434 108615 solver.cpp:228] Iteration 2000, loss = 87.3365
I1103 12:02:04.690469 108615 solver.cpp:244]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I1103 12:02:04.690481 108615 sgd_solver.cpp:106] Iteration 2000, lr = 0.001
I1103 12:02:10.373788 108615 solver.cpp:228] Iteration 2500, loss = 87.3365
I1103 12:02:10.373852 108615 solver.cpp:244]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I1103 12:02:10.373859 108615 sgd_solver.cpp:106] Iteration 2500, lr = 0.001
I1103 12:02:16.047372 108615 solver.cpp:337] Iteration 3000, Testing net (#0)
I1103 12:02:16.056390 108615 solver.cpp:404]     Test net output #0: accuracy = 0
I1103 12:02:16.056407 108615 solver.cpp:404]     Test net output #1: loss = 87.3365 (* 1 = 87.3365 loss)
I1103 12:02:16.070235 108615 solver.cpp:228] Iteration 3000, loss = 87.3365
I1103 12:02:16.070261 108615 solver.cpp:244]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I1103 12:02:16.070267 108615 sgd_solver.cpp:106] Iteration 3000, lr = 0.0001
I1103 12:02:21.755348 108615 solver.cpp:228] Iteration 3500, loss = 87.3365
I1103 12:02:21.755369 108615 solver.cpp:244]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
 I1103 12:02:21.755375 108615 sgd_solver.cpp:106] Iteration 3500, lr = 0.0001
",1,"Zero accuracy, loss not decreasing and layers have the same value for different inputs","Zero accuracy, loss not decreasing and layers have the same value for different inputs ### Issue summary
*Tried to ask this on googlegroup, didn't get a response that I had to post it here!*
I have a 50x50 image generated from a 1x2500 vector and I then generated HDF5 file from the images and feed it to caffeNet. My labels range from 0 to 255, so I am expecting 256 different classes. Below is my network prototxt and my solver. 
layer {
name: ""data""
type: ""HDF5Data""
top: ""X""
top: ""y""
hdf5_data_param{
source:""/Path/to/trainh5list.txt""
batch_size: 1
}
include{phase: TRAIN}
}
layer {
name: ""data""
type: ""HDF5Data""
top: ""X""
top: ""y""
hdf5_data_param{
source:""/Path/to/testh5list.txt""
batch_size: 1
}
include{phase: TEST}
}
....

layer {
name: ""accuracy""
type: ""Accuracy""
bottom: ""fc8""
bottom: ""y""
top: ""accuracy""
include {
phase: TEST
}
}
layer {
name: ""loss""
type: ""SoftmaxWithLoss""
bottom: ""fc8""
bottom: ""y""
top: ""loss""
}

Solver: 
net: ""/path/to/train_ntk.prototxt""
test_iter: 20
test_interval: 1000
base_lr: 0.001
lr_policy: ""step""
gamma: 0.1
stepsize: 10000
display: 1000
max_iter: 80000
momentum: 0.9
weight_decay: 0.0005
snapshot: 40000
snapshot_prefix: ""/path/to/model_""
solver_mode: GPU
#debug_info: true

While training in debug mode, 1. The accuracy layer has zero output all the time. 2. The fully connected layers seems to have the same result for all the inputs. 3. The loss doesn't decrease as the training goes on, it swings in some random value. I tried different batch size and learning rate, I don't seem to get it fixed. I also tried changing the data layer to ImageData layer, it didn't solve the issue. What can I do different to fix this issue?

Here is the training log: 
I1103 12:01:41.822055 108615 solver.cpp:337] Iteration 0, Testing net (#0)
I1103 12:01:41.849742 108615 solver.cpp:404]     Test net output #0: accuracy = 0
I1103 12:01:41.849761 108615 solver.cpp:404]     Test net output #1: loss = 6.02617 (* 1 = 6.02617 loss)
I1103 12:01:41.869380 108615 solver.cpp:228] Iteration 0, loss = 6.05644
I1103 12:01:41.869398 108615 solver.cpp:244]     Train net output #0: loss = 6.05644 (* 1 = 6.05644 loss)
I1103 12:01:41.869413 108615 sgd_solver.cpp:106] Iteration 0, lr = 0.1
I1103 12:01:47.624855 108615 solver.cpp:228] Iteration 500, loss = 87.3365
I1103 12:01:47.624876 108615 solver.cpp:244]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I1103 12:01:47.624882 108615 sgd_solver.cpp:106] Iteration 500, lr = 0.1
I1103 12:01:53.290213 108615 solver.cpp:337] Iteration 1000, Testing net (#0)
I1103 12:01:53.299310 108615 solver.cpp:404]     Test net output #0: accuracy = 0
I1103 12:01:53.299327 108615 solver.cpp:404]     Test net output #1: loss = 87.3365 (* 1 = 87.3365 loss)
I1103 12:01:53.314584 108615 solver.cpp:228] Iteration 1000, loss = 87.3365
I1103 12:01:53.314615 108615 solver.cpp:244]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I1103 12:01:53.314621 108615 sgd_solver.cpp:106] Iteration 1000, lr = 0.01
I1103 12:01:58.991268 108615 solver.cpp:228] Iteration 1500, loss = 87.3365
I1103 12:01:58.991315 108615 solver.cpp:244]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I1103 12:01:58.991322 108615 sgd_solver.cpp:106] Iteration 1500, lr = 0.01
I1103 12:02:04.664419 108615 solver.cpp:337] Iteration 2000, Testing net (#0)
I1103 12:02:04.673518 108615 solver.cpp:404]     Test net output #0: accuracy = 0
I1103 12:02:04.673537 108615 solver.cpp:404]     Test net output #1: loss = 87.3365 (* 1 = 87.3365 loss)
I1103 12:02:04.690434 108615 solver.cpp:228] Iteration 2000, loss = 87.3365
I1103 12:02:04.690469 108615 solver.cpp:244]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I1103 12:02:04.690481 108615 sgd_solver.cpp:106] Iteration 2000, lr = 0.001
I1103 12:02:10.373788 108615 solver.cpp:228] Iteration 2500, loss = 87.3365
I1103 12:02:10.373852 108615 solver.cpp:244]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I1103 12:02:10.373859 108615 sgd_solver.cpp:106] Iteration 2500, lr = 0.001
I1103 12:02:16.047372 108615 solver.cpp:337] Iteration 3000, Testing net (#0)
I1103 12:02:16.056390 108615 solver.cpp:404]     Test net output #0: accuracy = 0
I1103 12:02:16.056407 108615 solver.cpp:404]     Test net output #1: loss = 87.3365 (* 1 = 87.3365 loss)
I1103 12:02:16.070235 108615 solver.cpp:228] Iteration 3000, loss = 87.3365
I1103 12:02:16.070261 108615 solver.cpp:244]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
I1103 12:02:16.070267 108615 sgd_solver.cpp:106] Iteration 3000, lr = 0.0001
I1103 12:02:21.755348 108615 solver.cpp:228] Iteration 3500, loss = 87.3365
I1103 12:02:21.755369 108615 solver.cpp:244]     Train net output #0: loss = 87.3365 (* 1 = 87.3365 loss)
 I1103 12:02:21.755375 108615 sgd_solver.cpp:106] Iteration 3500, lr = 0.0001
"
caffe,5212,"I follow all the steps of http://caffe.berkeleyvision.org/gathered/examples/imagenet.html.
the training loss is always 6.9 and accuracy is 0.001. 
I searched and found this issue https://github.com/BVLC/caffe/issues/4482
So I tried to download the hosted mean file and trained again. It's ok! I got accuracy of 58% after 450000 iteration.
it seems the problem is my calculated mean file(I tried many times with it to avoid  bad parameter initialization). What's wrong with it?  ",1,training loss always 6.9 when using my calculated mean file in ILSVRC12 data,"training loss always 6.9 when using my calculated mean file in ILSVRC12 data I follow all the steps of http://caffe.berkeleyvision.org/gathered/examples/imagenet.html.
the training loss is always 6.9 and accuracy is 0.001. 
I searched and found this issue https://github.com/BVLC/caffe/issues/4482
So I tried to download the hosted mean file and trained again. It's ok! I got accuracy of 58% after 450000 iteration.
it seems the problem is my calculated mean file(I tried many times with it to avoid  bad parameter initialization). What's wrong with it?  "
caffe,4700,"I am trying to run CaffeNet training based on the image_data_layer. I am seeing the following behaviour. The loss value is 7.5 in Iteration 0 and then goes to 0 immediately. Is this expected ?

I0909 20:42:33.842559 136906 solver.cpp:279] Solving CaffeNet
I0909 20:42:33.842563 136906 solver.cpp:280] Learning Rate Policy: step
I0909 20:42:34.010679 136906 solver.cpp:228] Iteration 0, loss = 7.56662
I0909 20:42:34.010716 136906 solver.cpp:244]     Train net output #0: loss = 7.56662 (\* 1 = 7.56662 loss)
I0909 20:42:34.010766 136906 sgd_solver.cpp:106] Iteration 0, lr = 0.01
I0909 20:42:34.430747 136906 solver.cpp:228] Iteration 1, loss = 0
I0909 20:42:34.430763 136906 solver.cpp:244]     Train net output #0: loss = 0 (\* 1 = 0 loss)
I0909 20:42:34.430768 136906 sgd_solver.cpp:106] Iteration 1, lr = 0.01
I0909 20:42:34.875175 136906 solver.cpp:228] Iteration 2, loss = 0
I0909 20:42:34.875191 136906 solver.cpp:244]     Train net output #0: loss = 0 (\* 1 = 0 loss)
I0909 20:42:34.875196 136906 sgd_solver.cpp:106] Iteration 2, lr = 0.01
I0909 20:42:34.875416 136906 blocking_queue.cpp:50] Data layer prefetch queue empty
I0909 20:42:35.293489 136906 solver.cpp:228] Iteration 3, loss = 0
I0909 20:42:35.293507 136906 solver.cpp:244]     Train net output #0: loss = 0 (\* 1 = 0 loss)
I0909 20:42:35.293512 136906 sgd_solver.cpp:106] Iteration 3, lr = 0.01
I0909 20:42:35.796902 136906 solver.cpp:228] Iteration 4, loss = 0
I0909 20:42:35.796919 136906 solver.cpp:244]     Train net output #0: loss = 0 (\* 1 = 0 loss)
I0909 20:42:35.796924 136906 sgd_solver.cpp:106] Iteration 4, lr = 0.01
I0909 20:42:36.378119 136906 solver.cpp:228] Iteration 5, loss = 0

The head of my prototxt file is:

name: ""CaffeNet""
layer {
  name: ""data""
  type: ""ImageData""
  top: ""data""
  top: ""label""
  include {
    phase: TRAIN
  }
  transform_param {
    mirror: true
    crop_size: 227
    mean_file: ""/users/xyz/projects/caffe/nvcaffe/caffe/data/ilsvrc12/imagenet_mean.binaryproto""
  }

  image_data_param {
    source: ""/users/xyz/projects/caffe/nvcaffe/caffe/train.txt""
    batch_size: 128
    new_height: 256
    new_width: 256
  }
}
layer {
  name: ""data""
  type: ""Data""
  top: ""data""
  top: ""label""
  include {
    phase: TEST
  }
  transform_param {
    mirror: false
    crop_size: 227
    mean_file: ""/users/xyz/projects/caffe/nvcaffe/caffe/data/ilsvrc12/imagenet_mean.binaryproto""
  }
  data_param {
    source: ""/lus/scratch/xyz/imagenet-on-lustre/examples/imagenet/ilsvrc12_val_lmdb""
    batch_size: 50
    backend: LMDB
  }
}
",1,Loss goes to 0 for image data layer,"Loss goes to 0 for image data layer I am trying to run CaffeNet training based on the image_data_layer. I am seeing the following behaviour. The loss value is 7.5 in Iteration 0 and then goes to 0 immediately. Is this expected ?

I0909 20:42:33.842559 136906 solver.cpp:279] Solving CaffeNet
I0909 20:42:33.842563 136906 solver.cpp:280] Learning Rate Policy: step
I0909 20:42:34.010679 136906 solver.cpp:228] Iteration 0, loss = 7.56662
I0909 20:42:34.010716 136906 solver.cpp:244]     Train net output #0: loss = 7.56662 (\* 1 = 7.56662 loss)
I0909 20:42:34.010766 136906 sgd_solver.cpp:106] Iteration 0, lr = 0.01
I0909 20:42:34.430747 136906 solver.cpp:228] Iteration 1, loss = 0
I0909 20:42:34.430763 136906 solver.cpp:244]     Train net output #0: loss = 0 (\* 1 = 0 loss)
I0909 20:42:34.430768 136906 sgd_solver.cpp:106] Iteration 1, lr = 0.01
I0909 20:42:34.875175 136906 solver.cpp:228] Iteration 2, loss = 0
I0909 20:42:34.875191 136906 solver.cpp:244]     Train net output #0: loss = 0 (\* 1 = 0 loss)
I0909 20:42:34.875196 136906 sgd_solver.cpp:106] Iteration 2, lr = 0.01
I0909 20:42:34.875416 136906 blocking_queue.cpp:50] Data layer prefetch queue empty
I0909 20:42:35.293489 136906 solver.cpp:228] Iteration 3, loss = 0
I0909 20:42:35.293507 136906 solver.cpp:244]     Train net output #0: loss = 0 (\* 1 = 0 loss)
I0909 20:42:35.293512 136906 sgd_solver.cpp:106] Iteration 3, lr = 0.01
I0909 20:42:35.796902 136906 solver.cpp:228] Iteration 4, loss = 0
I0909 20:42:35.796919 136906 solver.cpp:244]     Train net output #0: loss = 0 (\* 1 = 0 loss)
I0909 20:42:35.796924 136906 sgd_solver.cpp:106] Iteration 4, lr = 0.01
I0909 20:42:36.378119 136906 solver.cpp:228] Iteration 5, loss = 0

The head of my prototxt file is:

name: ""CaffeNet""
layer {
  name: ""data""
  type: ""ImageData""
  top: ""data""
  top: ""label""
  include {
    phase: TRAIN
  }
  transform_param {
    mirror: true
    crop_size: 227
    mean_file: ""/users/xyz/projects/caffe/nvcaffe/caffe/data/ilsvrc12/imagenet_mean.binaryproto""
  }

  image_data_param {
    source: ""/users/xyz/projects/caffe/nvcaffe/caffe/train.txt""
    batch_size: 128
    new_height: 256
    new_width: 256
  }
}
layer {
  name: ""data""
  type: ""Data""
  top: ""data""
  top: ""label""
  include {
    phase: TEST
  }
  transform_param {
    mirror: false
    crop_size: 227
    mean_file: ""/users/xyz/projects/caffe/nvcaffe/caffe/data/ilsvrc12/imagenet_mean.binaryproto""
  }
  data_param {
    source: ""/lus/scratch/xyz/imagenet-on-lustre/examples/imagenet/ilsvrc12_val_lmdb""
    batch_size: 50
    backend: LMDB
  }
}
"
caffe,2783,"Hi I am using **Caffe** on **Ubuntu 14.04
CUDA version 7.0 
cudnn version 2
GPU : NVIDIA GT 730**

In caffe first I get the initialization done and then I load the imagenet model (Alexnet). I also initialize the gpu using **set_mode_gpu()**
After that I take an image. Lets call the image as x.
I copy this image onto the caffe source blob. Then I perform a forward pass for this image by using : **net.forward(end='fc7')**
Then I extract the 4096 dimensional fc7 output.(the activation features of the fc7 layer)

The problem I am facing is that when I run the same code multiple times, everytime I obtain a different result. That is, in GPU mode, everytime the activation features are different for the same image. When I am using forward pass, the function of the network is supposed to be deterministic right ? So I should get the same output everytime for the same image. 
On the other hand, when I run caffe on cpu by using **set_mode_cpu()** everything works perfectly, i.e, I get the same output each time
The code used and the outputs obtained are shown below. I am not able to understand what the problem is. Is it that the problem is caused due to GPU rounding off ? But the errors are very large. Or is it due to some issues with the latest CUDNN version ? Or is it something else altogether ?

Following is the **CODE**
#1) IMPORT libraries


#2) IMPORT Caffe Models and define utility functions


#3) LOADING Image and setting constants


#4) Setting the source image and making the forward pass to obtain fc7 activation features



**FOLLOWING is the output that I obtained for 'print dst.data' when I ran the above code multiple times**
# output on 1st execution of code


# output on 2nd execution of code


# output on 3rd execution of code


# output on 4th execution of code


",1,Caffe - inconsistency in the activation feature values - GPU mode,"Caffe - inconsistency in the activation feature values - GPU mode Hi I am using **Caffe** on **Ubuntu 14.04
CUDA version 7.0 
cudnn version 2
GPU : NVIDIA GT 730**

In caffe first I get the initialization done and then I load the imagenet model (Alexnet). I also initialize the gpu using **set_mode_gpu()**
After that I take an image. Lets call the image as x.
I copy this image onto the caffe source blob. Then I perform a forward pass for this image by using : **net.forward(end='fc7')**
Then I extract the 4096 dimensional fc7 output.(the activation features of the fc7 layer)

The problem I am facing is that when I run the same code multiple times, everytime I obtain a different result. That is, in GPU mode, everytime the activation features are different for the same image. When I am using forward pass, the function of the network is supposed to be deterministic right ? So I should get the same output everytime for the same image. 
On the other hand, when I run caffe on cpu by using **set_mode_cpu()** everything works perfectly, i.e, I get the same output each time
The code used and the outputs obtained are shown below. I am not able to understand what the problem is. Is it that the problem is caused due to GPU rounding off ? But the errors are very large. Or is it due to some issues with the latest CUDNN version ? Or is it something else altogether ?

Following is the **CODE**
#1) IMPORT libraries


#2) IMPORT Caffe Models and define utility functions


#3) LOADING Image and setting constants


#4) Setting the source image and making the forward pass to obtain fc7 activation features



**FOLLOWING is the output that I obtained for 'print dst.data' when I ran the above code multiple times**
# output on 1st execution of code


# output on 2nd execution of code


# output on 3rd execution of code


# output on 4th execution of code


"
caffe,2406,"Did some benchmarking and found that Prelu is slower than conv layer in backpropagation

I0502 15:58:03.235862 19023 caffe.cpp:276]      conv1    forward: 23.0175 ms.
I0502 15:58:03.235872 19023 caffe.cpp:279]      conv1    backward: 26.5506 ms.
I0502 15:58:03.235882 19023 caffe.cpp:276]     prelu1    forward: 5.75406 ms.
I0502 15:58:03.235893 19023 caffe.cpp:279]     prelu1    backward: 111.537 ms.

https://groups.google.com/forum/#!topic/caffe-users/-xj7EaMrMGI

A small change in the prelu_layer.cu speeds up prelu 3-4 times

Change: CAFFE_GET_BLOCKS(count)



To: CAFFE_GET_BLOCKS(cdim)



I ran the caffe test after this and it passed.

Other than this I think the rest of the code can be optimized further because we do backpropagation 1 output at a time. Is someone working on that?
",1,PReLU is slow,"PReLU is slow Did some benchmarking and found that Prelu is slower than conv layer in backpropagation

I0502 15:58:03.235862 19023 caffe.cpp:276]      conv1    forward: 23.0175 ms.
I0502 15:58:03.235872 19023 caffe.cpp:279]      conv1    backward: 26.5506 ms.
I0502 15:58:03.235882 19023 caffe.cpp:276]     prelu1    forward: 5.75406 ms.
I0502 15:58:03.235893 19023 caffe.cpp:279]     prelu1    backward: 111.537 ms.

https://groups.google.com/forum/#!topic/caffe-users/-xj7EaMrMGI

A small change in the prelu_layer.cu speeds up prelu 3-4 times

Change: CAFFE_GET_BLOCKS(count)



To: CAFFE_GET_BLOCKS(cdim)



I ran the caffe test after this and it passed.

Other than this I think the rest of the code can be optimized further because we do backpropagation 1 output at a time. Is someone working on that?
"
caffe,4072,"I did the fine tunning but the results are not promising. I am only getting the accuracy of 50% after 1000 iterations. My training dataset has 3k images validation has 1k images and evaluation has 1k images. I set the base_lr to 0.001 and max_itr to 10000. Is this accuracy normal or did I do something wrong? I can also share what changes I did in the files, if needed. 
",1,Fine Tunning of GoogLeNet Model,"Fine Tunning of GoogLeNet Model I did the fine tunning but the results are not promising. I am only getting the accuracy of 50% after 1000 iterations. My training dataset has 3k images validation has 1k images and evaluation has 1k images. I set the base_lr to 0.001 and max_itr to 10000. Is this accuracy normal or did I do something wrong? I can also share what changes I did in the files, if needed. 
"
caffe,5611,"Please use the [caffe-users list](https://groups.google.com/forum/#!forum/caffe-users) for usage, installation, or modeling questions, or other requests for help.
_Do not post such requests to Issues._ Doing so interferes with the development of Caffe.

Please read the [guidelines for contributing](https://github.com/BVLC/caffe/blob/master/CONTRIBUTING.md) before submitting this issue.

### Issue summary
Hi,
I am trying to fine tuning FCN32s network. However, I've inspected that my accuracies(overall accuracies, mean accuracies and mean IUs fluctuate in certain range and the values does not happy to increase at all).

In the paper, it described that ""Final layer deconvolutional filters are fixed to bilinear interpolation , while intermediate upsampling layers are intialised to bilinear upsampling, and then learned"", but in the train.prototxt the learning rate was set to 0 and there is only one deconvolution layer. I am a little bit confused here... 

It would be greatly appreciated if someone provide me with any insight of this issue.

Hanyi 

@shelhamer 

### Steps to reproduce

If you are having difficulty building Caffe or training a model, please ask the caffe-users mailing list. If you are reporting a build error that seems to be due to a bug in Caffe, please attach your build configuration (either Makefile.config or CMakeCache.txt) and the output of the make (or cmake) command.

### Your system configuration
Operating system:
Compiler:
CUDA version (if applicable):
CUDNN version (if applicable):
BLAS:
Python or MATLAB version (for pycaffe and matcaffe respectively):
",1,Deconvolution initialisation,"Deconvolution initialisation Please use the [caffe-users list](https://groups.google.com/forum/#!forum/caffe-users) for usage, installation, or modeling questions, or other requests for help.
_Do not post such requests to Issues._ Doing so interferes with the development of Caffe.

Please read the [guidelines for contributing](https://github.com/BVLC/caffe/blob/master/CONTRIBUTING.md) before submitting this issue.

### Issue summary
Hi,
I am trying to fine tuning FCN32s network. However, I've inspected that my accuracies(overall accuracies, mean accuracies and mean IUs fluctuate in certain range and the values does not happy to increase at all).

In the paper, it described that ""Final layer deconvolutional filters are fixed to bilinear interpolation , while intermediate upsampling layers are intialised to bilinear upsampling, and then learned"", but in the train.prototxt the learning rate was set to 0 and there is only one deconvolution layer. I am a little bit confused here... 

It would be greatly appreciated if someone provide me with any insight of this issue.

Hanyi 

@shelhamer 

### Steps to reproduce

If you are having difficulty building Caffe or training a model, please ask the caffe-users mailing list. If you are reporting a build error that seems to be due to a bug in Caffe, please attach your build configuration (either Makefile.config or CMakeCache.txt) and the output of the make (or cmake) command.

### Your system configuration
Operating system:
Compiler:
CUDA version (if applicable):
CUDNN version (if applicable):
BLAS:
Python or MATLAB version (for pycaffe and matcaffe respectively):
"
caffe,6256,"Hi,
When I use models/bvlc_googlenet/train_val.prototxt and solver.prototxt to train googlenet from scratch on imagenet, I only got accuracy of 59% at top-1 and 83% at top-5 at max_iteration (10000000) . However, googlenet is claimed to achieve 68.9% and 89.04%. What accuracy did you get? Should I do some modifications to the prototxt files?
Thanks!
",1,training Googlenet from scratch got low accuracy,"training Googlenet from scratch got low accuracy Hi,
When I use models/bvlc_googlenet/train_val.prototxt and solver.prototxt to train googlenet from scratch on imagenet, I only got accuracy of 59% at top-1 and 83% at top-5 at max_iteration (10000000) . However, googlenet is claimed to achieve 68.9% and 89.04%. What accuracy did you get? Should I do some modifications to the prototxt files?
Thanks!
"
caffe,2681,"Hi,

I realize it is very slow to convert data to lmdb using caffe toolkit convert_imageset. It tooks about 10 hours by converting 2 millions images with the size of 95*95. My friend's caffe is much faster (6 millions in 4 hours). He has similar computer configuration with me (CPU and hard drive). The weird thing is when I am converting, the tool does not use much memory and CPU. The only difference with my friend's is the version of caffe (I use the latest one he has a older version),and he use ATLAS and I use OpenBlas. Is there anyone know how to solve this problem?
",1,"convert_imageset is very slow, not normal?","convert_imageset is very slow, not normal? Hi,

I realize it is very slow to convert data to lmdb using caffe toolkit convert_imageset. It tooks about 10 hours by converting 2 millions images with the size of 95*95. My friend's caffe is much faster (6 millions in 4 hours). He has similar computer configuration with me (CPU and hard drive). The weird thing is when I am converting, the tool does not use much memory and CPU. The only difference with my friend's is the version of caffe (I use the latest one he has a older version),and he use ATLAS and I use OpenBlas. Is there anyone know how to solve this problem?
"
caffe,4730,"I'm attempting use caffe and python to do real-time image classification. I'm using OpenCV to stream from my webcam in one process, and in a separate process, using caffe to perform image classification on the frames pulled from the webcam. Then I'm passing the result of the classification back to the main thread to caption the webcam stream.

The problem is that even though I have an NVIDIA GPU and am performing the caffe predictions on the GPU, the main thread gets slown down. Normally without doing any predictions, my webcam stream runs at 30 fps; however, with the predictions, my webcam stream gets at best 15 fps. 

I've verified that caffe is indeed using the GPU when performing the predictions, and that my GPU or GPU memory is not maxing out. I've also verified that my CPU cores are not getting maxed out at any point during the program. I'm wondering if I am doing something wrong or if there is no way to keep these 2 processes truly separate. Or if in fact this is a bug in caffe GPU mode. Any advice is appreciated. Here is my code for reference



I am pretty sure it is the caffe prediction slowing everything down, because when I comment out the prediction and pass dummy text back and forth between the processes, I get 30 fps again.



    def __init__(self, task_queue, result_queue):
        multiprocessing.Process.__init__(self)
        self.task_queue = task_queue
        self.result_queue = result_queue
        #other initialization stuff

    def run(self):
        caffe.set_mode_gpu()
        caffe.set_device(0)
        #Load caffe net -- code omitted
        while True:
            image = self.task_queue.get()
            #crop image -- code omitted
            #text = net.predict(image)
            text = ""dummy text""
            self.result_queue.put(text)

        return

import cv2
import caffe
import multiprocessing
import Queue 

tasks = multiprocessing.Queue()
results = multiprocessing.Queue()
consumer = Consumer(tasks,results)
consumer.start()

#Creating window and starting video capturer from camera
cv2.namedWindow(""preview"")
vc = cv2.VideoCapture(0)
#Try to get the first frame
if vc.isOpened():
    rval, frame = vc.read()
else:
    rval = False
frame_copy[:] = frame

while rval:
    if tasks.empty():
       tasks.put(frame_copy)
    else:
       text = tasks.get()
       #Add text to frame
       cv2.putText(frame,text)

    #Showing the frame with all the applied modifications
    cv2.imshow(""preview"", frame)

    #Getting next frame from camera
    rval, frame = vc.read()
    frame_copy[:] = frame
    #Getting keyboard input 
    key = cv2.waitKey(1)
    #exit on ESC
    if key == 27:
        break

`
",1,Pycaffe slowing down CPU in GPU mode ,"Pycaffe slowing down CPU in GPU mode  I'm attempting use caffe and python to do real-time image classification. I'm using OpenCV to stream from my webcam in one process, and in a separate process, using caffe to perform image classification on the frames pulled from the webcam. Then I'm passing the result of the classification back to the main thread to caption the webcam stream.

The problem is that even though I have an NVIDIA GPU and am performing the caffe predictions on the GPU, the main thread gets slown down. Normally without doing any predictions, my webcam stream runs at 30 fps; however, with the predictions, my webcam stream gets at best 15 fps. 

I've verified that caffe is indeed using the GPU when performing the predictions, and that my GPU or GPU memory is not maxing out. I've also verified that my CPU cores are not getting maxed out at any point during the program. I'm wondering if I am doing something wrong or if there is no way to keep these 2 processes truly separate. Or if in fact this is a bug in caffe GPU mode. Any advice is appreciated. Here is my code for reference



I am pretty sure it is the caffe prediction slowing everything down, because when I comment out the prediction and pass dummy text back and forth between the processes, I get 30 fps again.



    def __init__(self, task_queue, result_queue):
        multiprocessing.Process.__init__(self)
        self.task_queue = task_queue
        self.result_queue = result_queue
        #other initialization stuff

    def run(self):
        caffe.set_mode_gpu()
        caffe.set_device(0)
        #Load caffe net -- code omitted
        while True:
            image = self.task_queue.get()
            #crop image -- code omitted
            #text = net.predict(image)
            text = ""dummy text""
            self.result_queue.put(text)

        return

import cv2
import caffe
import multiprocessing
import Queue 

tasks = multiprocessing.Queue()
results = multiprocessing.Queue()
consumer = Consumer(tasks,results)
consumer.start()

#Creating window and starting video capturer from camera
cv2.namedWindow(""preview"")
vc = cv2.VideoCapture(0)
#Try to get the first frame
if vc.isOpened():
    rval, frame = vc.read()
else:
    rval = False
frame_copy[:] = frame

while rval:
    if tasks.empty():
       tasks.put(frame_copy)
    else:
       text = tasks.get()
       #Add text to frame
       cv2.putText(frame,text)

    #Showing the frame with all the applied modifications
    cv2.imshow(""preview"", frame)

    #Getting next frame from camera
    rval, frame = vc.read()
    frame_copy[:] = frame
    #Getting keyboard input 
    key = cv2.waitKey(1)
    #exit on ESC
    if key == 27:
        break

`
"
caffe,6780,"Issue Summary 
![caffe_issue image](https://user-images.githubusercontent.com/34826390/58903355-dc5b5680-86ca-11e9-8651-ae10cb77e1c4.PNG)

I am not able to train ssd 300x300 in kitti dataset. The detection eval is zero while training.

",1,getting loss of zero while training ssd 300x300 in kitti dataset,"getting loss of zero while training ssd 300x300 in kitti dataset Issue Summary 
![caffe_issue image](https://user-images.githubusercontent.com/34826390/58903355-dc5b5680-86ca-11e9-8651-ae10cb77e1c4.PNG)

I am not able to train ssd 300x300 in kitti dataset. The detection eval is zero while training.

"
caffe,3965,"I created a simplest net to learn the division ""/"" function (input is A and B, label is A/B). However, when I try to run the trainer, it hang forever. If I do , I see that it's waiting for . Searched around and it was mentioned (didn't note down the source) that it might be caused by the training and testing phase sharing the same lmdb. So I copied the same data to separate training and testing folders, but the problem persists.

**Wondering why the hang, and how I should debug this problem?**

Here is the console output:



Here is my :



Here is my :



Here is how I generated the training and label data:


",1,Caffe hang when creating data layer,"Caffe hang when creating data layer I created a simplest net to learn the division ""/"" function (input is A and B, label is A/B). However, when I try to run the trainer, it hang forever. If I do , I see that it's waiting for . Searched around and it was mentioned (didn't note down the source) that it might be caused by the training and testing phase sharing the same lmdb. So I copied the same data to separate training and testing folders, but the problem persists.

**Wondering why the hang, and how I should debug this problem?**

Here is the console output:



Here is my :



Here is my :



Here is how I generated the training and label data:


"
caffe,1856,"Are we interested in [GSOC 2015](https://www.google-melange.com/gsoc/homepage/google/gsoc2015)? Organization submission deadline is 20 Feb 2015.

/cc @shelhamer @longjon @jeffdonahue @sguada
",0,Gsoc 2015,"Gsoc 2015 Are we interested in [GSOC 2015](https://www.google-melange.com/gsoc/homepage/google/gsoc2015)? Organization submission deadline is 20 Feb 2015.

/cc @shelhamer @longjon @jeffdonahue @sguada
"
caffe,1843,"As I have understood it the code for a stochastic pooling layer uses random sampling to select elements. Another approach would be to sample using a multinomial distribution with probabilities derived from the activation values. (normalized appropriately of course)

Reference: http://www.matthewzeiler.com/pubs/iclr2013/iclr2013.pdf

1)Am I right in about the pooling layer using random sampling?
2) Is there an existing way to do multinomial sampling?
2) If not this would be a good addition?
",0,Stochastic pooling layer: using a multinomial distribution based on activation values instead of a uniform distribution,"Stochastic pooling layer: using a multinomial distribution based on activation values instead of a uniform distribution As I have understood it the code for a stochastic pooling layer uses random sampling to select elements. Another approach would be to sample using a multinomial distribution with probabilities derived from the activation values. (normalized appropriately of course)

Reference: http://www.matthewzeiler.com/pubs/iclr2013/iclr2013.pdf

1)Am I right in about the pooling layer using random sampling?
2) Is there an existing way to do multinomial sampling?
2) If not this would be a good addition?
"
caffe,5156,"### Issue summary

Specifying --gpu=2,4,5,6 does not have the intended effect.  In the latest master, the code simply calls Caffe::SetDevice(gpus[0]) when testing, where the vector gpus = {2,4,5,6} and gpus[0] = 2, so only the first GPU in the list is used.  When training, the code also calls Caffe::set_solver_count(gpus.size()) which apparently allows the solver to create one worker per GPU, but the actual GPU device IDs are not propagated to the workers.

### Steps to reproduce

Run Caffe on a machine with at least 3 GPUs, and specify --gpu=N,M where M-N > 1 (i.e. do not specify consecutive GPUs).

### Your system configuration
Operating system: Ubuntu 14.04
Compiler: GCC 4.8.4
CUDA version (if applicable): 7.0, 7.5 and 8.0 tested
CUDNN version (if applicable): 4.0.7 and 5.1 tested
BLAS: Atlas and MKL tested
Python or MATLAB version (for pycaffe and matcaffe respectively): N/A
",0,GPU Device IDs Not Honored,"GPU Device IDs Not Honored ### Issue summary

Specifying --gpu=2,4,5,6 does not have the intended effect.  In the latest master, the code simply calls Caffe::SetDevice(gpus[0]) when testing, where the vector gpus = {2,4,5,6} and gpus[0] = 2, so only the first GPU in the list is used.  When training, the code also calls Caffe::set_solver_count(gpus.size()) which apparently allows the solver to create one worker per GPU, but the actual GPU device IDs are not propagated to the workers.

### Steps to reproduce

Run Caffe on a machine with at least 3 GPUs, and specify --gpu=N,M where M-N > 1 (i.e. do not specify consecutive GPUs).

### Your system configuration
Operating system: Ubuntu 14.04
Compiler: GCC 4.8.4
CUDA version (if applicable): 7.0, 7.5 and 8.0 tested
CUDNN version (if applicable): 4.0.7 and 5.1 tested
BLAS: Atlas and MKL tested
Python or MATLAB version (for pycaffe and matcaffe respectively): N/A
"
caffe,2233,"In io.cpp, ReadProtoFromBinaryFile, the line::



should be:



Should I make a pull request or is this enough?
",0,"missing O_BINARY in ReadProtoFromBinaryFile(), io.cpp","missing O_BINARY in ReadProtoFromBinaryFile(), io.cpp In io.cpp, ReadProtoFromBinaryFile, the line::



should be:



Should I make a pull request or is this enough?
"
caffe,423,"Hi all,

I'm new in this kind of deep learning algorithms and I would like to know other hardware specs beyond a powerful GPU? Specifically a powerful machine for training big models like the imagenet challenge and many others.
minimum RAM ?
minimum number of cores ?
minimum hdd ?

Thank you for sharing this valuable piece of code and create a active community!!!
",0,Hardware Recommendation and Choosing CPU Cluster or GPUs,"Hardware Recommendation and Choosing CPU Cluster or GPUs Hi all,

I'm new in this kind of deep learning algorithms and I would like to know other hardware specs beyond a powerful GPU? Specifically a powerful machine for training big models like the imagenet challenge and many others.
minimum RAM ?
minimum number of cores ?
minimum hdd ?

Thank you for sharing this valuable piece of code and create a active community!!!
"
caffe,6350,"### Issue summary

I need to run  in MATLAB but this error appear:

GLIBCXX_3.4.21'
not found (required by /home/vicom/caffe/matlab/+caffe/private/caffe_.mexa64).

Error in caffe.reset_all (line 5)
caffe_('reset');

Error in saveFilters (line 1)
caffe.reset_all();
`

### System configuration

* Operating system: Ubuntu 16.06
* MATLAB: R2016b
",0,Error saveFilter.m in MEX,"Error saveFilter.m in MEX ### Issue summary

I need to run  in MATLAB but this error appear:

GLIBCXX_3.4.21'
not found (required by /home/vicom/caffe/matlab/+caffe/private/caffe_.mexa64).

Error in caffe.reset_all (line 5)
caffe_('reset');

Error in saveFilters (line 1)
caffe.reset_all();
`

### System configuration

* Operating system: Ubuntu 16.06
* MATLAB: R2016b
"
caffe,5790,"Tried compiling the latest code with 
atlas on Centos machine  x86_64 GNU/Linux

But ""make all"" failed at 
/usr/bin/ld: cannot find -lcblas
/usr/bin/ld: cannot find -latlas

Makefile.config 
BLAS_LIB = /usr/lib64/atlas

The content of /usr/lib64/atlas
total 21304
lrwxrwxrwx 1 root root       17 Jul 21 16:28 libsatlas.so -> libsatlas.so.3.10
lrwxrwxrwx 1 root root       17 Oct  8  2016 libsatlas.so.3 -> libsatlas.so.3.10
-rwxr-xr-x 1 root root 10852104 Nov 20  2015 libsatlas.so.3.10
lrwxrwxrwx 1 root root       17 Jul 21 16:28 libtatlas.so -> libtatlas.so.3.10
lrwxrwxrwx 1 root root       17 Oct  8  2016 libtatlas.so.3 -> libtatlas.so.3.10
-rwxr-xr-x 1 root root 10959464 Nov 20  2015 libtatlas.so.3.10

However, then I used openblas and it just worked fine. 

http://caffe.berkeleyvision.org/install_yum.html has no instructions on ""cblas"" but when I see Makefile I see 
 ifeq ($(LINUX), 1)
                ifeq ($(BLAS), atlas)
                        # Linux simply has cblas and atlas
                        LIBRARIES += cblas atlas

Not sure 
",0,LD issue compiling the latest code,"LD issue compiling the latest code Tried compiling the latest code with 
atlas on Centos machine  x86_64 GNU/Linux

But ""make all"" failed at 
/usr/bin/ld: cannot find -lcblas
/usr/bin/ld: cannot find -latlas

Makefile.config 
BLAS_LIB = /usr/lib64/atlas

The content of /usr/lib64/atlas
total 21304
lrwxrwxrwx 1 root root       17 Jul 21 16:28 libsatlas.so -> libsatlas.so.3.10
lrwxrwxrwx 1 root root       17 Oct  8  2016 libsatlas.so.3 -> libsatlas.so.3.10
-rwxr-xr-x 1 root root 10852104 Nov 20  2015 libsatlas.so.3.10
lrwxrwxrwx 1 root root       17 Jul 21 16:28 libtatlas.so -> libtatlas.so.3.10
lrwxrwxrwx 1 root root       17 Oct  8  2016 libtatlas.so.3 -> libtatlas.so.3.10
-rwxr-xr-x 1 root root 10959464 Nov 20  2015 libtatlas.so.3.10

However, then I used openblas and it just worked fine. 

http://caffe.berkeleyvision.org/install_yum.html has no instructions on ""cblas"" but when I see Makefile I see 
 ifeq ($(LINUX), 1)
                ifeq ($(BLAS), atlas)
                        # Linux simply has cblas and atlas
                        LIBRARIES += cblas atlas

Not sure 
"
caffe,2985,"Hello,

My Matlab crashes after calling ""caffe.reset_all()"". Here is a snapshot of the crash:

![screenshot from 2015-08-27 05 24 09](https://cloud.githubusercontent.com/assets/2586855/9517271/c7cbe446-4c7c-11e5-9293-daf33a3c5df6.png)

I use Ubuntu 15.04 and my version of Matlab is R2014b. This issue happens every time I call ""caffe.reset_all()"" after I load and use the net but does not happen by calling ""caffe.reset_all()"" before loading and using the net. I did not have this problem in an older version of caffe from a few months ago calling caffe('reset'). Please help!

Thanks,
Ehsan
",0,"Matlab crashes upon calling ""caffe.reset_all()""","Matlab crashes upon calling ""caffe.reset_all()"" Hello,

My Matlab crashes after calling ""caffe.reset_all()"". Here is a snapshot of the crash:

![screenshot from 2015-08-27 05 24 09](https://cloud.githubusercontent.com/assets/2586855/9517271/c7cbe446-4c7c-11e5-9293-daf33a3c5df6.png)

I use Ubuntu 15.04 and my version of Matlab is R2014b. This issue happens every time I call ""caffe.reset_all()"" after I load and use the net but does not happen by calling ""caffe.reset_all()"" before loading and using the net. I did not have this problem in an older version of caffe from a few months ago calling caffe('reset'). Please help!

Thanks,
Ehsan
"
caffe,4394,"I got the following error building caffe and I was not able to fix it:

[ 78%] Linking CXX executable finetune_net
CMakeFiles/finetune_net.dir/finetune_net.cpp.o: In function google::LogMessageFatal::LogMessageFatal(char const_, int)'
/global/homes/m/mwdmaas/caffe/caffe_original/caffe/tools/finetune_net.cpp:(.text+0x47): undefined reference to google::LogMessageFatal::~LogMessageFatal()'
/global/homes/m/mwdmaas/caffe/caffe_original/caffe/tools/finetune_net.cpp:(.text+0x68): undefined reference to **sti**$E':
/global/homes/m/mwdmaas/caffe/caffe_original/caffe/tools/finetune_net.cpp:(.text+0x98): undefined reference to boost::system::generic_category()'
/global/homes/m/mwdmaas/caffe/caffe_original/caffe/tools/finetune_net.cpp:(.text+0xb0): undefined reference to finetune_net'
make[2]: *_\* [tools/finetune_net] Error 1
make[1]: **\* [tools/CMakeFiles/finetune_net.dir/all] Error 2
make: **\* [all] Error 2

I will add that the Makefile was created with:
cmake -DCPU_ONLY=ON -DBLAS=open -DUSE_LEVELDB=OFF -DUSE_OPENCV=OFF ..
",0,finetuning glog linking error,"finetuning glog linking error I got the following error building caffe and I was not able to fix it:

[ 78%] Linking CXX executable finetune_net
CMakeFiles/finetune_net.dir/finetune_net.cpp.o: In function google::LogMessageFatal::LogMessageFatal(char const_, int)'
/global/homes/m/mwdmaas/caffe/caffe_original/caffe/tools/finetune_net.cpp:(.text+0x47): undefined reference to google::LogMessageFatal::~LogMessageFatal()'
/global/homes/m/mwdmaas/caffe/caffe_original/caffe/tools/finetune_net.cpp:(.text+0x68): undefined reference to **sti**$E':
/global/homes/m/mwdmaas/caffe/caffe_original/caffe/tools/finetune_net.cpp:(.text+0x98): undefined reference to boost::system::generic_category()'
/global/homes/m/mwdmaas/caffe/caffe_original/caffe/tools/finetune_net.cpp:(.text+0xb0): undefined reference to finetune_net'
make[2]: *_\* [tools/finetune_net] Error 1
make[1]: **\* [tools/CMakeFiles/finetune_net.dir/all] Error 2
make: **\* [all] Error 2

I will add that the Makefile was created with:
cmake -DCPU_ONLY=ON -DBLAS=open -DUSE_LEVELDB=OFF -DUSE_OPENCV=OFF ..
"
caffe,5258,"**issue 1** Can build caffe as static library ? If positive, how to do it, and also how to set it in test single image program using c++ and cmake?

**issue 2** If I just build a program to predicting a image using c++, should it must build GPU version caffe?
Have anyone try above explorings, wish receive some help :)",0,Is it possible build caffe static when just build a caffe-based predict single image program?,"Is it possible build caffe static when just build a caffe-based predict single image program? **issue 1** Can build caffe as static library ? If positive, how to do it, and also how to set it in test single image program using c++ and cmake?

**issue 2** If I just build a program to predicting a image using c++, should it must build GPU version caffe?
Have anyone try above explorings, wish receive some help :)"
caffe,1727,"Hello, every author who contrbute to Caffe!  First I want to say thanks very much for your work, now I had a problem about the MemoryDataLayer. I can test a model in MemoryDataLayer, but when I train a model, it failed(I want to testing a model after training), I don't know why this happen, the reason why I post my question here is that I am not sure whether we need a new PR to solve this problem.  Thanks in advance!
",0,Why MemoryDataLayer can't be used in the TEST phase of solver?,"Why MemoryDataLayer can't be used in the TEST phase of solver? Hello, every author who contrbute to Caffe!  First I want to say thanks very much for your work, now I had a problem about the MemoryDataLayer. I can test a model in MemoryDataLayer, but when I train a model, it failed(I want to testing a model after training), I don't know why this happen, the reason why I post my question here is that I am not sure whether we need a new PR to solve this problem.  Thanks in advance!
"
caffe,3039,"I tried to execute the first cell of **detection.ipynb**. It has an error. It seems to be an simple error, but I do not know how to rectify it. Hope to see a patch soon.


",0,errors in detection ipython notebook,"errors in detection ipython notebook I tried to execute the first cell of **detection.ipynb**. It has an error. It seems to be an simple error, but I do not know how to rectify it. Hope to see a patch soon.


"
caffe,6402,"I have changed ""WITH_PYTHON_LAYER := 1"",make pycaffe, abd add python path to /etc/profile so that I can import caffe successfully. 
But when I add new python layer like this:

it occured the error as follows:

I am really confused of this, and I found no solution to solve it on the Internet. 
Could you please tell me how to deal with it? Thanks in advance!",0,pycaffe error: 'module' object has no attribute 'Layer',"pycaffe error: 'module' object has no attribute 'Layer' I have changed ""WITH_PYTHON_LAYER := 1"",make pycaffe, abd add python path to /etc/profile so that I can import caffe successfully. 
But when I add new python layer like this:

it occured the error as follows:

I am really confused of this, and I found no solution to solve it on the Internet. 
Could you please tell me how to deal with it? Thanks in advance!"
caffe,949,"Hi, 
I want to create a cumtom layer, that has number of output units n^2, where n is the number of input variables. unit(i,j)=f(inp(i),inp(j)). I want the unit(i,j) produces output according to input present at position i and j only. How can i create such a layer?
",0,How to develop a new layer?,"How to develop a new layer? Hi, 
I want to create a cumtom layer, that has number of output units n^2, where n is the number of input variables. unit(i,j)=f(inp(i),inp(j)). I want the unit(i,j) produces output according to input present at position i and j only. How can i create such a layer?
"
caffe,1499,"I compiled the dev branch without any error message and then tried the finetuning tutorial on ""flicker style"" data following the instructions provided on the Caffe website. The images were successfully downloaded, but when I run the finetuning, it crashes with the following error message:

F1128 13:34:59.766399 27222 net.cpp:712] Check failed: target_blobs[j]->channels() == source_layer.blobs(j).channels() (1 vs. 3) 
**\* Check failure stack trace: ***
    @     0x7ffe8c713b5d  google::LogMessage::Fail()
    @     0x7ffe8c717b77  google::LogMessage::SendToLog()
    @     0x7ffe8c7159f9  google::LogMessage::Flush()
    @     0x7ffe8c715cfd  google::LogMessageFatal::~LogMessageFatal()
    @           0x50f68c  caffe::Net<>::CopyTrainedLayersFrom()
    @           0x50f90a  caffe::Net<>::CopyTrainedLayersFrom()
    @           0x425f99  train()
    @           0x425b96  main
    @       0x308ba1ed5d  (unknown)
    @           0x422f19  (unknown)
Aborted

There are also some other error messages before this crash (edited to mask out my file system structure):

E1128 13:34:55.158053 27229 io.cpp:75] Could not open or find file /xxxxx/xxxxxx/xxxx/caffe-dev/data/flickr_style/images/9296163038_3009f6b805.jpg

But this file is indeed there (I used the script to download the files):

$ ls -lh /xxxxx/xxxxxx/xxxx/caffe-dev/data/flickr_style/images/9296163038_3009f6b805.jpg
-rw-r--r-- 1 abcd efg 121K Nov 28 13:21 /xxxxx/xxxxxx/xxxx/caffe-dev/data/flickr_style/images/9296163038_3009f6b805.jpg

BTW, I did the same finetuning tutorial on the master branch with no problem.  
",0,Crash when try the finetune tutorial on flicker data,"Crash when try the finetune tutorial on flicker data I compiled the dev branch without any error message and then tried the finetuning tutorial on ""flicker style"" data following the instructions provided on the Caffe website. The images were successfully downloaded, but when I run the finetuning, it crashes with the following error message:

F1128 13:34:59.766399 27222 net.cpp:712] Check failed: target_blobs[j]->channels() == source_layer.blobs(j).channels() (1 vs. 3) 
**\* Check failure stack trace: ***
    @     0x7ffe8c713b5d  google::LogMessage::Fail()
    @     0x7ffe8c717b77  google::LogMessage::SendToLog()
    @     0x7ffe8c7159f9  google::LogMessage::Flush()
    @     0x7ffe8c715cfd  google::LogMessageFatal::~LogMessageFatal()
    @           0x50f68c  caffe::Net<>::CopyTrainedLayersFrom()
    @           0x50f90a  caffe::Net<>::CopyTrainedLayersFrom()
    @           0x425f99  train()
    @           0x425b96  main
    @       0x308ba1ed5d  (unknown)
    @           0x422f19  (unknown)
Aborted

There are also some other error messages before this crash (edited to mask out my file system structure):

E1128 13:34:55.158053 27229 io.cpp:75] Could not open or find file /xxxxx/xxxxxx/xxxx/caffe-dev/data/flickr_style/images/9296163038_3009f6b805.jpg

But this file is indeed there (I used the script to download the files):

$ ls -lh /xxxxx/xxxxxx/xxxx/caffe-dev/data/flickr_style/images/9296163038_3009f6b805.jpg
-rw-r--r-- 1 abcd efg 121K Nov 28 13:21 /xxxxx/xxxxxx/xxxx/caffe-dev/data/flickr_style/images/9296163038_3009f6b805.jpg

BTW, I did the same finetuning tutorial on the master branch with no problem.  
"
caffe,5839,"Please use the [caffe-users list](https://groups.google.com/forum/#!forum/caffe-users) for usage, installation, or modeling questions, or other requests for help.
_Do not post such requests to Issues._ Doing so interferes with the development of Caffe.

Please read the [guidelines for contributing](https://github.com/BVLC/caffe/blob/master/CONTRIBUTING.md) before submitting this issue.

### Issue summary


### Steps to reproduce

If you are having difficulty building Caffe or training a model, please ask the caffe-users mailing list. If you are reporting a build error that seems to be due to a bug in Caffe, please attach your build configuration (either Makefile.config or CMakeCache.txt) and the output of the make (or cmake) command.

### Your system configuration
Operating system:
Compiler:
CUDA version (if applicable):
CUDNN version (if applicable):
BLAS:
Python or MATLAB version (for pycaffe and matcaffe respectively):
",0,cd,"cd Please use the [caffe-users list](https://groups.google.com/forum/#!forum/caffe-users) for usage, installation, or modeling questions, or other requests for help.
_Do not post such requests to Issues._ Doing so interferes with the development of Caffe.

Please read the [guidelines for contributing](https://github.com/BVLC/caffe/blob/master/CONTRIBUTING.md) before submitting this issue.

### Issue summary


### Steps to reproduce

If you are having difficulty building Caffe or training a model, please ask the caffe-users mailing list. If you are reporting a build error that seems to be due to a bug in Caffe, please attach your build configuration (either Makefile.config or CMakeCache.txt) and the output of the make (or cmake) command.

### Your system configuration
Operating system:
Compiler:
CUDA version (if applicable):
CUDNN version (if applicable):
BLAS:
Python or MATLAB version (for pycaffe and matcaffe respectively):
"
caffe,1859,"I'm pretty new to Caffe and deep learning. I have to questions about this library:
- is it possible to use Caffe for classification of data that is not image (generic vectors in n-dimensional space)? Are there examples?
- is Caffe compatible with hadoop or similar?
",0,how to classify generic n-dimensional samples,"how to classify generic n-dimensional samples I'm pretty new to Caffe and deep learning. I have to questions about this library:
- is it possible to use Caffe for classification of data that is not image (generic vectors in n-dimensional space)? Are there examples?
- is Caffe compatible with hadoop or similar?
"
caffe,390,"I need a network with just two output classes A and B. I have tonnes of training data of images which either have A or B and are appropriately labelled. Once I have trained such a network, I wish to pass some images to test the network. Lets say I pass an image which contains neither A nor B. I understand that since the last layer is SOFTMAX it will still classify this image as either A or B with sum of probabilities of A and B equal to 1. Which means even if image does not have A it may still have a probability > 0.5 of carrying A according to the network.

Am I correct in my understanding? If yes, what kind of last layer should I use so that I can rely on a probability threshold to be able to reject such images which contain neither A not B.
",0,Two classes output only!,"Two classes output only! I need a network with just two output classes A and B. I have tonnes of training data of images which either have A or B and are appropriately labelled. Once I have trained such a network, I wish to pass some images to test the network. Lets say I pass an image which contains neither A nor B. I understand that since the last layer is SOFTMAX it will still classify this image as either A or B with sum of probabilities of A and B equal to 1. Which means even if image does not have A it may still have a probability > 0.5 of carrying A according to the network.

Am I correct in my understanding? If yes, what kind of last layer should I use so that I can rely on a probability threshold to be able to reject such images which contain neither A not B.
"
caffe,5384,"F0311 02:01:35.163677  6921 benchmark.cpp:92] Check failed: error == cudaSuccess (74 vs. 0)  misaligned address
*** Check failure stack trace: ***
    @     0x7f5f4aa0a5cd  google::LogMessage::Fail()
    @     0x7f5f4aa0c433  google::LogMessage::SendToLog()
    @     0x7f5f4aa0a15b  google::LogMessage::Flush()
    @     0x7f5f4aa0ce1e  google::LogMessageFatal::~LogMessageFatal()
    @     0x7f5f4b09108a  caffe::Timer::MilliSeconds()
    @     0x7f5f4b0905ea  caffe::Timer::Seconds()
    @     0x7f5f4b018aed  caffe::Solver<>::Step()
    @     0x7f5f4b0194fa  caffe::Solver<>::Solve()
    @           0x40aeb4  train()
    @           0x4075a8  main
    @     0x7f5f491a1830  __libc_start_main
    @           0x407e79  _start
    @              (nil)  (unknown)
Aborted (core dumped)
",0,Can I know what does the following error message mean?,"Can I know what does the following error message mean? F0311 02:01:35.163677  6921 benchmark.cpp:92] Check failed: error == cudaSuccess (74 vs. 0)  misaligned address
*** Check failure stack trace: ***
    @     0x7f5f4aa0a5cd  google::LogMessage::Fail()
    @     0x7f5f4aa0c433  google::LogMessage::SendToLog()
    @     0x7f5f4aa0a15b  google::LogMessage::Flush()
    @     0x7f5f4aa0ce1e  google::LogMessageFatal::~LogMessageFatal()
    @     0x7f5f4b09108a  caffe::Timer::MilliSeconds()
    @     0x7f5f4b0905ea  caffe::Timer::Seconds()
    @     0x7f5f4b018aed  caffe::Solver<>::Step()
    @     0x7f5f4b0194fa  caffe::Solver<>::Solve()
    @           0x40aeb4  train()
    @           0x4075a8  main
    @     0x7f5f491a1830  __libc_start_main
    @           0x407e79  _start
    @              (nil)  (unknown)
Aborted (core dumped)
"
caffe,3030,"I've used gunplot to plot the accuracy of a solver over its epochs and was wondering whether this would be functionality Caffe would like to have built-in? I suppose it would add another optional dependancy, but I've found having a .png plot of my training really assists in gaining a higher-level perspective of how my model is performing, and gives greater insight into how the change of hyper parameters effects the curve of training.

Let me know if it would be considered a useful tool or not.
",0,Solver Progress Plotting,"Solver Progress Plotting I've used gunplot to plot the accuracy of a solver over its epochs and was wondering whether this would be functionality Caffe would like to have built-in? I suppose it would add another optional dependancy, but I've found having a .png plot of my training really assists in gaining a higher-level perspective of how my model is performing, and gives greater insight into how the change of hyper parameters effects the curve of training.

Let me know if it would be considered a useful tool or not.
"
caffe,3242,"I have noticed that gradient w.r.t convolution layer parameters are summed over all pixel multiplications of previous layer feature image and next layer gradient image. However, feature image sizes (hidden layer images) change from layer to layer. 

So the gradient at each step is a sum of all gradients in the gradient image, without normalization. So, effectively, convolution layer weight-gradients used in stochastic-gradient-solver is using varying learning rates. Therefore, I think the backward methods require a normalization depending on the number of pixels in the gradient.
",0,Normalization for convolution layer weights,"Normalization for convolution layer weights I have noticed that gradient w.r.t convolution layer parameters are summed over all pixel multiplications of previous layer feature image and next layer gradient image. However, feature image sizes (hidden layer images) change from layer to layer. 

So the gradient at each step is a sum of all gradients in the gradient image, without normalization. So, effectively, convolution layer weight-gradients used in stochastic-gradient-solver is using varying learning rates. Therefore, I think the backward methods require a normalization depending on the number of pixels in the gradient.
"
caffe,4009,"I got that warning message when I loaded the python caffe module. I got it just loading the module....

I'm using the last version of caffe (I updated yesterday) and python 2.7.3.  Thanks in advance!  


",0,RuntimeWarning when loading the Python Caffe module ,"RuntimeWarning when loading the Python Caffe module  I got that warning message when I loaded the python caffe module. I got it just loading the module....

I'm using the last version of caffe (I updated yesterday) and python 2.7.3.  Thanks in advance!  


"
caffe,3816,"Hi all!
As we know we need to clear layer's parameters' diff and bottom/top's diff at each step when training a net. And I found that in  it has  to clear layer's parameters' diff, but when does caffe clear layers' bottom/top's diff?
",0,When dose caffe clear layers' bottom/top's diff?,"When dose caffe clear layers' bottom/top's diff? Hi all!
As we know we need to clear layer's parameters' diff and bottom/top's diff at each step when training a net. And I found that in  it has  to clear layer's parameters' diff, but when does caffe clear layers' bottom/top's diff?
"
caffe,460,"Lots of visual recognition problems have a multi-label nature. That means an entry (Datum) will be assigned to multiple labels. Currently the DataLayer and LevelDB converter assume there is a single label assigned to every entry. 

Has anybody worked on that yet?

My suggestion is that we make an alternative ""label array"" \in {-1,0,1}^k where k is the number of possible labels and 

-1: label does not exist in the entry image
0: undefined/unknown/irrelevant label for the image
1: label exist in the entry image
",0,Feature Request: Multi-Label DataLayer and LevelDB converter,"Feature Request: Multi-Label DataLayer and LevelDB converter Lots of visual recognition problems have a multi-label nature. That means an entry (Datum) will be assigned to multiple labels. Currently the DataLayer and LevelDB converter assume there is a single label assigned to every entry. 

Has anybody worked on that yet?

My suggestion is that we make an alternative ""label array"" \in {-1,0,1}^k where k is the number of possible labels and 

-1: label does not exist in the entry image
0: undefined/unknown/irrelevant label for the image
1: label exist in the entry image
"
caffe,2272,"I have written a java web service that call the ""classify.py"" in java using .getRuntime().exec. command

The problem is, every time I call the web service it will initialize the net model.

Would you please suggest me a way to go over this prob., 

Thanks 
",0,"Java web service Call ""classify.py"" python script","Java web service Call ""classify.py"" python script I have written a java web service that call the ""classify.py"" in java using .getRuntime().exec. command

The problem is, every time I call the web service it will initialize the net model.

Would you please suggest me a way to go over this prob., 

Thanks 
"
caffe,4401,"hi,

I found that caffe support Octave interface https://github.com/BVLC/caffe/blob/master/matlab/CMakeLists.txt

I installed  and  and uncomment  in .

but it still said that it required matlab binary



Is that Octave interface still an ongoing feature?
",0,matcaffe with octave,"matcaffe with octave hi,

I found that caffe support Octave interface https://github.com/BVLC/caffe/blob/master/matlab/CMakeLists.txt

I installed  and  and uncomment  in .

but it still said that it required matlab binary



Is that Octave interface still an ongoing feature?
"
caffe,1822,"I get following errors: 

g++ .build_release/tools/dump_network.o .build_release/lib/libcaffe.a -o .build_release/tools/dump_network.bin -fPIC -DNDEBUG -O2 -DUSE_CUDNN -I/usr/include/python2.7 -I/usr/lib/python2.7/dist-packages/numpy/core/include -I/usr/local/include -I.build_release/src -I./src -I./include -I/usr/local/cuda/include -Wall -Wno-sign-compare -L/usr/lib -L/usr/local/lib -L/usr/lib -L/usr/local/cuda/lib64 -L/usr/local/cuda/lib -lcudart -lcublas -lcurand -lglog -lgflags -lprotobuf -lleveldb -lsnappy -llmdb -lboost_system -lhdf5_hl -lhdf5 -lopencv_core -lopencv_highgui -lopencv_imgproc -lpthread -lboost_thread -lcudnn -lcblas -latlas
/usr/lib/x86_64-linux-gnu/libgflags.so: undefined reference to `std::__throw_out_of_range_fmt(char const_, ...)@GLIBCXX_3.4.20'
collect2: error: ld returned 1 exit status
Makefile:494: recipe for target '.build_release/tools/dump_network.bin' failed
make: *_\* [.build_release/tools/dump_network.bin] Error 1

I am a beginner for the Caffe and Linux. I have no any idea. If anyone know how to solve this error, please help me! Thanks very much!
",0, error: When build the Caffe on Ubuntu 14.04 with GPU," error: When build the Caffe on Ubuntu 14.04 with GPU I get following errors: 

g++ .build_release/tools/dump_network.o .build_release/lib/libcaffe.a -o .build_release/tools/dump_network.bin -fPIC -DNDEBUG -O2 -DUSE_CUDNN -I/usr/include/python2.7 -I/usr/lib/python2.7/dist-packages/numpy/core/include -I/usr/local/include -I.build_release/src -I./src -I./include -I/usr/local/cuda/include -Wall -Wno-sign-compare -L/usr/lib -L/usr/local/lib -L/usr/lib -L/usr/local/cuda/lib64 -L/usr/local/cuda/lib -lcudart -lcublas -lcurand -lglog -lgflags -lprotobuf -lleveldb -lsnappy -llmdb -lboost_system -lhdf5_hl -lhdf5 -lopencv_core -lopencv_highgui -lopencv_imgproc -lpthread -lboost_thread -lcudnn -lcblas -latlas
/usr/lib/x86_64-linux-gnu/libgflags.so: undefined reference to `std::__throw_out_of_range_fmt(char const_, ...)@GLIBCXX_3.4.20'
collect2: error: ld returned 1 exit status
Makefile:494: recipe for target '.build_release/tools/dump_network.bin' failed
make: *_\* [.build_release/tools/dump_network.bin] Error 1

I am a beginner for the Caffe and Linux. I have no any idea. If anyone know how to solve this error, please help me! Thanks very much!
"
caffe,1005,"make runtest

.build_release/test/test_all.testbin 0 --gtest_shuffle 
Cuda number of devices: 1
Setting to use device 0
Current device id: 0
Note: Randomizing tests' orders with a seed of 42484 .
[==========] Running 718 tests from 154 test cases.
[----------] Global test environment set-up.
[----------] 6 tests from SliceLayerTest/2, where TypeParam = caffe::FloatGPU
[ RUN      ] SliceLayerTest/2.TestGradientAcrossNum
F0829 15:41:50.183418  3050 math_functions.cu:81] Check failed: error == cudaSuccess (11 vs. 0)  invalid argument
**\* Check failure stack trace: ***
    @     0x2b96b5faadaa  (unknown)
    @     0x2b96b5faace4  (unknown)
    @     0x2b96b5faa6e6  (unknown)
    @     0x2b96b5fad687  (unknown)
    @           0x7374d2  caffe::caffe_gpu_memcpy()
    @           0x69eaea  caffe::SyncedMemory::mutable_gpu_data()
    @           0x72a9f1  caffe::Blob<>::mutable_gpu_data()
    @           0x754c8b  caffe::SliceLayer<>::Forward_gpu()
    @           0x4257bb  caffe::GradientChecker<>::CheckGradientSingle()
    @           0x46ddc9  caffe::GradientChecker<>::CheckGradientExhaustive()
    @           0x5d99e2  caffe::SliceLayerTest_TestGradientAcrossNum_Test<>::TestBody()
    @           0x6593bd  testing::internal::HandleExceptionsInMethodIfSupported<>()
    @           0x651191  testing::Test::Run()
    @           0x651276  testing::TestInfo::Run()
    @           0x6513b7  testing::TestCase::Run()
    @           0x65170e  testing::internal::UnitTestImpl::RunAllTests()
    @           0x658f3d  testing::internal::HandleExceptionsInMethodIfSupported<>()
    @           0x6507ee  testing::UnitTest::Run()
    @           0x4170cd  main
    @     0x2b96b8c48ec5  (unknown)
    @           0x41c891  (unknown)
    @              (nil)  (unknown)
make: **\* [runtest] Aborted (core dumped)

I run ""make runtest"" many times. Everytime it's core dumped at ""math_functions.cu""
I get this error on (Ubuntu14.04 x86_64) + (NVIDIA Driver Version: 340.32) + (CUDA: 6.0).
I suppose it may caused by cuda. Then I reinstall cuda6.5 to replace cuda6.0. But I still get this error.
",0,make runtest core dumped,"make runtest core dumped make runtest

.build_release/test/test_all.testbin 0 --gtest_shuffle 
Cuda number of devices: 1
Setting to use device 0
Current device id: 0
Note: Randomizing tests' orders with a seed of 42484 .
[==========] Running 718 tests from 154 test cases.
[----------] Global test environment set-up.
[----------] 6 tests from SliceLayerTest/2, where TypeParam = caffe::FloatGPU
[ RUN      ] SliceLayerTest/2.TestGradientAcrossNum
F0829 15:41:50.183418  3050 math_functions.cu:81] Check failed: error == cudaSuccess (11 vs. 0)  invalid argument
**\* Check failure stack trace: ***
    @     0x2b96b5faadaa  (unknown)
    @     0x2b96b5faace4  (unknown)
    @     0x2b96b5faa6e6  (unknown)
    @     0x2b96b5fad687  (unknown)
    @           0x7374d2  caffe::caffe_gpu_memcpy()
    @           0x69eaea  caffe::SyncedMemory::mutable_gpu_data()
    @           0x72a9f1  caffe::Blob<>::mutable_gpu_data()
    @           0x754c8b  caffe::SliceLayer<>::Forward_gpu()
    @           0x4257bb  caffe::GradientChecker<>::CheckGradientSingle()
    @           0x46ddc9  caffe::GradientChecker<>::CheckGradientExhaustive()
    @           0x5d99e2  caffe::SliceLayerTest_TestGradientAcrossNum_Test<>::TestBody()
    @           0x6593bd  testing::internal::HandleExceptionsInMethodIfSupported<>()
    @           0x651191  testing::Test::Run()
    @           0x651276  testing::TestInfo::Run()
    @           0x6513b7  testing::TestCase::Run()
    @           0x65170e  testing::internal::UnitTestImpl::RunAllTests()
    @           0x658f3d  testing::internal::HandleExceptionsInMethodIfSupported<>()
    @           0x6507ee  testing::UnitTest::Run()
    @           0x4170cd  main
    @     0x2b96b8c48ec5  (unknown)
    @           0x41c891  (unknown)
    @              (nil)  (unknown)
make: **\* [runtest] Aborted (core dumped)

I run ""make runtest"" many times. Everytime it's core dumped at ""math_functions.cu""
I get this error on (Ubuntu14.04 x86_64) + (NVIDIA Driver Version: 340.32) + (CUDA: 6.0).
I suppose it may caused by cuda. Then I reinstall cuda6.5 to replace cuda6.0. But I still get this error.
"
caffe,4680,"Hi. I compile caffe and hav not any warning  , but in make runtest i have this problem
anyone can help ?
Nvidia GTX 1070
Cuda-8.0
ubuntu 16.04.1
.
.
[ RUN      ] ImageDataLayerTest/0.TestShuffle
[       OK ] ImageDataLayerTest/0.TestShuffle (61 ms)
[ RUN      ] ImageDataLayerTest/0.TestSpace
[       OK ] ImageDataLayerTest/0.TestSpace (21 ms)
[ RUN      ] ImageDataLayerTest/0.TestResize
**\* Aborted at 1472932722 (unix time) try ""date -d @1472932722"" if you are using GNU date ***
PC: @     0x7f79eb212d84 __GI___pthread_mutex_lock
**\* SIGSEGV (@0x3038) received by PID 10525 (TID 0x7f79f497c740) from PID 12344; stack trace: ***
    @     0x7f79eb21a3d0 (unknown)
    @     0x7f79eb212d84 __GI___pthread_mutex_lock
    @     0x7f79c27add98 (unknown)
    @     0x7f79c2863c41 (unknown)
    @     0x7f79c2863db5 (unknown)
    @     0x7f79c27b3ad4 (unknown)
    @     0x7f79c27b5327 (unknown)
    @     0x7f79c27894f6 (unknown)
    @     0x7f79c2688d7d (unknown)
    @     0x7f79c2688d18 (unknown)
    @     0x7f79d0142022 (unknown)
    @     0x7f79d0143d42 (unknown)
    @     0x7f79d01434d0 clGetPlatformIDs
    @     0x7f79ee8b98d5 (anonymous namespace)::opencl_fn3<>::switch_fn()
    @     0x7f79ee99ceea cv::ocl::haveOpenCL()
    @     0x7f79ee9ac288 cv::ocl::useOpenCL()
    @     0x7f79ece56a6c cv::resize()
    @     0x7f79ebaffb47 caffe::ReadImageToCVMat()
    @     0x7f79ebb8f12e caffe::ImageDataLayer<>::DataLayerSetUp()
    @     0x7f79ebbaa203 caffe::BasePrefetchingDataLayer<>::LayerSetUp()
    @           0x48ab2f caffe::Layer<>::SetUp()
    @           0x707fe7 caffe::ImageDataLayerTest_TestResize_Test<>::TestBody()
    @           0x917183 testing::internal::HandleExceptionsInMethodIfSupported<>()
    @           0x91079a testing::Test::Run()
    @           0x9108e8 testing::TestInfo::Run()
    @           0x9109c5 testing::TestCase::Run()
    @           0x911c9f testing::internal::UnitTestImpl::RunAllTests()
    @           0x911fc3 testing::UnitTest::Run()
    @           0x46d47d main
    @     0x7f79eae60830 __libc_start_main
    @           0x474ee9 _start
    @                0x0 (unknown)
Makefile:526: recipe for target 'runtest' failed
make: **\* [runtest] Segmentation fault (core dumped)
",0,Crash with cv::resize() and OpenCL,"Crash with cv::resize() and OpenCL Hi. I compile caffe and hav not any warning  , but in make runtest i have this problem
anyone can help ?
Nvidia GTX 1070
Cuda-8.0
ubuntu 16.04.1
.
.
[ RUN      ] ImageDataLayerTest/0.TestShuffle
[       OK ] ImageDataLayerTest/0.TestShuffle (61 ms)
[ RUN      ] ImageDataLayerTest/0.TestSpace
[       OK ] ImageDataLayerTest/0.TestSpace (21 ms)
[ RUN      ] ImageDataLayerTest/0.TestResize
**\* Aborted at 1472932722 (unix time) try ""date -d @1472932722"" if you are using GNU date ***
PC: @     0x7f79eb212d84 __GI___pthread_mutex_lock
**\* SIGSEGV (@0x3038) received by PID 10525 (TID 0x7f79f497c740) from PID 12344; stack trace: ***
    @     0x7f79eb21a3d0 (unknown)
    @     0x7f79eb212d84 __GI___pthread_mutex_lock
    @     0x7f79c27add98 (unknown)
    @     0x7f79c2863c41 (unknown)
    @     0x7f79c2863db5 (unknown)
    @     0x7f79c27b3ad4 (unknown)
    @     0x7f79c27b5327 (unknown)
    @     0x7f79c27894f6 (unknown)
    @     0x7f79c2688d7d (unknown)
    @     0x7f79c2688d18 (unknown)
    @     0x7f79d0142022 (unknown)
    @     0x7f79d0143d42 (unknown)
    @     0x7f79d01434d0 clGetPlatformIDs
    @     0x7f79ee8b98d5 (anonymous namespace)::opencl_fn3<>::switch_fn()
    @     0x7f79ee99ceea cv::ocl::haveOpenCL()
    @     0x7f79ee9ac288 cv::ocl::useOpenCL()
    @     0x7f79ece56a6c cv::resize()
    @     0x7f79ebaffb47 caffe::ReadImageToCVMat()
    @     0x7f79ebb8f12e caffe::ImageDataLayer<>::DataLayerSetUp()
    @     0x7f79ebbaa203 caffe::BasePrefetchingDataLayer<>::LayerSetUp()
    @           0x48ab2f caffe::Layer<>::SetUp()
    @           0x707fe7 caffe::ImageDataLayerTest_TestResize_Test<>::TestBody()
    @           0x917183 testing::internal::HandleExceptionsInMethodIfSupported<>()
    @           0x91079a testing::Test::Run()
    @           0x9108e8 testing::TestInfo::Run()
    @           0x9109c5 testing::TestCase::Run()
    @           0x911c9f testing::internal::UnitTestImpl::RunAllTests()
    @           0x911fc3 testing::UnitTest::Run()
    @           0x46d47d main
    @     0x7f79eae60830 __libc_start_main
    @           0x474ee9 _start
    @                0x0 (unknown)
Makefile:526: recipe for target 'runtest' failed
make: **\* [runtest] Segmentation fault (core dumped)
"
caffe,4869,"I'm trying to incorporate a custom python data layer to a network in . I have built  with  and have added the caffe python modules and the custom python data layer module to . However, when I try to run it, it gives the following error:
I1020 16:13:35.672911 24362 layer_factory.hpp:74]  
    ImportError: Import by filename is not supported.
    terminate called after throwing an instance of boost::python::error_already_set'
   **\* Aborted at 1476960215 (unix time) try ""date -d @1476960215"" if you are using GNU date ***
   PC: @     0x7fea94390c37 (unknown)     **\* SIGABRT (@0x3e800005f2a) received by PID 24362 (TID 0x7fea95db7a40)  from PID 24362; stack trace: ***
        @     0x7fea94390cb0 (unknown)
        @     0x7fea94390c37 (unknown)
        @     0x7fea94394028 (unknown)
        @     0x7fea94995535 (unknown)
        @     0x7fea949936d6 (unknown)
        @     0x7fea94993703 (unknown)
        @     0x7fea94993976 (unknown)
        @     0x7fea955ffe22 caffe::GetPythonLayer<>()
        @     0x7fea95616326 caffe::LayerRegistry<>::CreateLayer()
        @     0x7fea9561b4cc caffe::Net<>::Init()
        @     0x7fea9561d262 caffe::Net<>::Net()
        @     0x7fea955f9139 caffe::Solver<>::InitTrainNet()
        @     0x7fea955fa2f3 caffe::Solver<>::Init()
        @     0x7fea955fa4c6 caffe::Solver<>::Solver()
        @           0x40d030 caffe::GetSolver<>()
        @           0x407143 train()
        @           0x4056e1 main
        @     0x7fea9437bf45 (unknown)
        @           0x405c8d (unknown)
        @                0x0 (unknown)
        Aborted (core dumped)

Any idea why I'm running into this problem? Thanks in advance.
",0,ImportError when implementing Python Layer in Caffe,"ImportError when implementing Python Layer in Caffe I'm trying to incorporate a custom python data layer to a network in . I have built  with  and have added the caffe python modules and the custom python data layer module to . However, when I try to run it, it gives the following error:
I1020 16:13:35.672911 24362 layer_factory.hpp:74]  
    ImportError: Import by filename is not supported.
    terminate called after throwing an instance of boost::python::error_already_set'
   **\* Aborted at 1476960215 (unix time) try ""date -d @1476960215"" if you are using GNU date ***
   PC: @     0x7fea94390c37 (unknown)     **\* SIGABRT (@0x3e800005f2a) received by PID 24362 (TID 0x7fea95db7a40)  from PID 24362; stack trace: ***
        @     0x7fea94390cb0 (unknown)
        @     0x7fea94390c37 (unknown)
        @     0x7fea94394028 (unknown)
        @     0x7fea94995535 (unknown)
        @     0x7fea949936d6 (unknown)
        @     0x7fea94993703 (unknown)
        @     0x7fea94993976 (unknown)
        @     0x7fea955ffe22 caffe::GetPythonLayer<>()
        @     0x7fea95616326 caffe::LayerRegistry<>::CreateLayer()
        @     0x7fea9561b4cc caffe::Net<>::Init()
        @     0x7fea9561d262 caffe::Net<>::Net()
        @     0x7fea955f9139 caffe::Solver<>::InitTrainNet()
        @     0x7fea955fa2f3 caffe::Solver<>::Init()
        @     0x7fea955fa4c6 caffe::Solver<>::Solver()
        @           0x40d030 caffe::GetSolver<>()
        @           0x407143 train()
        @           0x4056e1 main
        @     0x7fea9437bf45 (unknown)
        @           0x405c8d (unknown)
        @                0x0 (unknown)
        Aborted (core dumped)

Any idea why I'm running into this problem? Thanks in advance.
"
caffe,3374,"For example, in   
Function implementation:  
part of code is:



Backward pass is only present in the training stage. So, is this  necessary ? I think removing the conditional sentence still makes sense.
",0,Is `if (this->phase_ == TRAIN) `  necessary in the backward computation of  dropout_layer implementation ?,"Is `if (this->phase_ == TRAIN) `  necessary in the backward computation of  dropout_layer implementation ? For example, in   
Function implementation:  
part of code is:



Backward pass is only present in the training stage. So, is this  necessary ? I think removing the conditional sentence still makes sense.
"
caffe,4864,"I finally install caffe on my Mac, without error for 'make pycaffe' and 'make distribute', so I got to \<caffe-root\>/python, and run python -c ""import caffe"", but I got some errors here, It seems it's the problem of protocol buffers, I am not sure:


I think that maybe gcc expected gcc code and it found clang code instead, and that the compiler is reporting not having found the gcc symbol, but mac use clang to compiler?

Because when I install the requirement.txt for pycaffe, I got some errors for gcc.

To install caffe is really headache, can someone give me a hand to tell me what happended???
Thanks in advance
",0,Mac import caffe error,"Mac import caffe error I finally install caffe on my Mac, without error for 'make pycaffe' and 'make distribute', so I got to \<caffe-root\>/python, and run python -c ""import caffe"", but I got some errors here, It seems it's the problem of protocol buffers, I am not sure:


I think that maybe gcc expected gcc code and it found clang code instead, and that the compiler is reporting not having found the gcc symbol, but mac use clang to compiler?

Because when I install the requirement.txt for pycaffe, I got some errors for gcc.

To install caffe is really headache, can someone give me a hand to tell me what happended???
Thanks in advance
"
caffe,3453,"Seems like CuDNN v4 is out in RC
https://developer.nvidia.com/cudnn

And it added some wonderful new features, 2D tiled FFT-conv, BN.
I think FFT-Conv is a great new feature.
So I wonder if there are any plans from the core dev team?

If not, may be we can make some contributions.
",0,Any official plans of sync with cuDNN v4?,"Any official plans of sync with cuDNN v4? Seems like CuDNN v4 is out in RC
https://developer.nvidia.com/cudnn

And it added some wonderful new features, 2D tiled FFT-conv, BN.
I think FFT-Conv is a great new feature.
So I wonder if there are any plans from the core dev team?

If not, may be we can make some contributions.
"
caffe,6875,"when making runtest, I met the following problem, does anyone know how to solve it? 
![image](https://user-images.githubusercontent.com/56579909/70400771-83002580-1a67-11ea-8327-15f490632472.png)
",0,Check failed: error == cudaSuccess (63 vs. 0)  OS call failed or operation not supported on this OS,"Check failed: error == cudaSuccess (63 vs. 0)  OS call failed or operation not supported on this OS when making runtest, I met the following problem, does anyone know how to solve it? 
![image](https://user-images.githubusercontent.com/56579909/70400771-83002580-1a67-11ea-8327-15f490632472.png)
"
caffe,4868,"Currently the pieces for caffe.Net will not accept unicode literals for the first and second arguments and generate cryptic messages about call signature for the underlying c++. I discovered this because I am in the habit of writing python2/3 compliant code so I always  in my code to get the really basic stuff out of the way. This however breaks the init method in the c++ code behind the Net class.

I have already gotten around this but it should be addressed in a further release.
",0,No support for unicode literals on caffe.Net,"No support for unicode literals on caffe.Net Currently the pieces for caffe.Net will not accept unicode literals for the first and second arguments and generate cryptic messages about call signature for the underlying c++. I discovered this because I am in the habit of writing python2/3 compliant code so I always  in my code to get the really basic stuff out of the way. This however breaks the init method in the c++ code behind the Net class.

I have already gotten around this but it should be addressed in a further release.
"
caffe,5867,"Ive compiled the OpenCL caffe branch on windows using the scripts/build_win.cmd. When running the classification example I get a crash when caffe tries to build a cl program:
e:\Work\Projects\VisualStudio\Viennacl-dev\viennacl-dev\viennacl\ocl\context.hpp:438
    if (!temp)
    {
      temp = clCreateProgramWithSource(h_.get(), 1, (const char **)&source_text, &source_size, &err);
      VIENNACL_ERR_CHECK(err);
    }
const char * options = build_options_.c_str();
->err = clBuildProgram(temp, 0, NULL, options, NULL, NULL);

Variables:
source_size	337011	unsigned __int64
source_text 	[source_text.cl.txt](https://github.com/BVLC/caffe/files/1244442/source_text.cl.txt)

Callstack:
>	classification-d.exe!viennacl::ocl::context::add_program(const std::basic_string<char,std::char_traits<char>,std::allocator<char> > & source, const std::basic_string<char,std::char_traits<char>,std::allocator<char> > & prog_name) Line 438	C++
 	classification-d.exe!caffe::RegisterKernels(viennacl::ocl::context * ctx) Line 5262	C++
 	classification-d.exe!caffe::device::SetProgram() Line 286	C++
 	classification-d.exe!caffe::device::Init() Line 67	C++
 	classification-d.exe!caffe::Caffe::SetDevices(std::vector<int,std::allocator<int> > device_ids) Line 531	C++
 	classification-d.exe!Classifier::Classifier(const std::basic_string<char,std::char_traits<char>,std::allocator<char> > & model_file, const std::basic_string<char,std::char_traits<char>,std::allocator<char> > & trained_file, const std::basic_string<char,std::char_traits<char>,std::allocator<char> > & mean_file, const std::basic_string<char,std::char_traits<char>,std::allocator<char> > & label_file) Line 72	C++
 	classification-d.exe!main(int argc, char * * argv) Line 269	C++

I am trying to run caffe on my AMD R9 270X GPU using opencl but it crashes in the C:\Windows\System32\amdocl64.dll. If I switch to using my other opencl device (Intel CPU) it compiles successfully. To be honest Im not sure if this should be reported here or to AMD support?

My machine specs:
Windows 10 64-bit
AMD Radeon R9 200 Series with latest drivers (17.7.2)
OpenCL Version: 22.19.662.4

Reproducible:
Ive also reproduced this issue by loading the generated cl program into the AMD APP SDK 2.9.1 HelloWorld.cpp example. 

I've made a few local modifications but I don't think these will cause any change in this behaviour:

diff --git a/scripts/build_win.cmd b/scripts/build_win.cmd
index f90306e3..49176291 100755
--- a/scripts/build_win.cmd
+++ b/scripts/build_win.cmd
@@ -68,7 +68,7 @@ if DEFINED APPVEYOR (
 ) else (
:: Change the settings here to match your setup
:: Change MSVC_VERSION to 12 to use VS 2013
-if NOT DEFINED MSVC_VERSION set MSVC_VERSION=14
+if NOT DEFINED MSVC_VERSION set MSVC_VERSION=12
:: Change to 1 to use Ninja generator (builds much faster)
if NOT DEFINED WITH_NINJA set WITH_NINJA=0
:: Change to 1 to build caffe without CUDA support
@@ -85,8 +85,8 @@ if DEFINED APPVEYOR (
:: Change to 3 if using python 3.5 (only 2.7 and 3.5 are supported)
if NOT DEFINED PYTHON_VERSION set PYTHON_VERSION=2
:: Change these options for your needs.
-if NOT DEFINED BUILD_PYTHON set BUILD_PYTHON=1
-if NOT DEFINED BUILD_PYTHON_LAYER set BUILD_PYTHON_LAYER=1
+if NOT DEFINED BUILD_PYTHON set BUILD_PYTHON=0
+if NOT DEFINED BUILD_PYTHON_LAYER set BUILD_PYTHON_LAYER=0
if NOT DEFINED BUILD_MATLAB set BUILD_MATLAB=0
:: If python is on your path leave this alone
if NOT DEFINED PYTHON_EXE set PYTHON_EXE=python

diff --git a/src/caffe/layers/libdnn_conv_layer.cpp b/src/caffe/layers/libdnn_conv_layer.cpp
index 451f1ef8..600bb6a8 100644
--- a/src/caffe/layers/libdnn_conv_layer.cpp
+++ b/src/caffe/layers/libdnn_conv_layer.cpp
@@ -63,7 +63,7 @@ void LibDNNConvolutionLayer<Dtype>::Reshape(
     config.bias_term = this->bias_term_;
     config.fast_unsafe_math = true;
     config.weights_backward = this->param_propagate_down_[0];
-config.bias_backward = this->param_propagate_down_[1];
+config.bias_backward = this->bias_term_ ? this->param_propagate_down_[1] : false;

if ((std::is_same<Dtype, float>::value

diff --git a/src/caffe/layers/libdnn_deconv_layer.cpp b/src/caffe/layers/libdnn_deconv_layer.cpp
index aacde554..0bc49ca7 100644
--- a/src/caffe/layers/libdnn_deconv_layer.cpp
+++ b/src/caffe/layers/libdnn_deconv_layer.cpp
@@ -59,7 +59,7 @@ void LibDNNDeconvolutionLayer<Dtype>::Reshape(
     config.bias_term = this->bias_term_;
     config.fast_unsafe_math = true;
     config.weights_backward = this->param_propagate_down_[0];
-config.bias_backward = this->param_propagate_down_[1];
+config.bias_backward = this->bias_term_ ? this->param_propagate_down_[1] : false;

// Atomic algorithm requirements:
// - Float & 32 bit atomics available

",0,Crash in amdocl64.dll when Caffe calls clBuildProgram,"Crash in amdocl64.dll when Caffe calls clBuildProgram Ive compiled the OpenCL caffe branch on windows using the scripts/build_win.cmd. When running the classification example I get a crash when caffe tries to build a cl program:
e:\Work\Projects\VisualStudio\Viennacl-dev\viennacl-dev\viennacl\ocl\context.hpp:438
    if (!temp)
    {
      temp = clCreateProgramWithSource(h_.get(), 1, (const char **)&source_text, &source_size, &err);
      VIENNACL_ERR_CHECK(err);
    }
const char * options = build_options_.c_str();
->err = clBuildProgram(temp, 0, NULL, options, NULL, NULL);

Variables:
source_size	337011	unsigned __int64
source_text 	[source_text.cl.txt](https://github.com/BVLC/caffe/files/1244442/source_text.cl.txt)

Callstack:
>	classification-d.exe!viennacl::ocl::context::add_program(const std::basic_string<char,std::char_traits<char>,std::allocator<char> > & source, const std::basic_string<char,std::char_traits<char>,std::allocator<char> > & prog_name) Line 438	C++
 	classification-d.exe!caffe::RegisterKernels(viennacl::ocl::context * ctx) Line 5262	C++
 	classification-d.exe!caffe::device::SetProgram() Line 286	C++
 	classification-d.exe!caffe::device::Init() Line 67	C++
 	classification-d.exe!caffe::Caffe::SetDevices(std::vector<int,std::allocator<int> > device_ids) Line 531	C++
 	classification-d.exe!Classifier::Classifier(const std::basic_string<char,std::char_traits<char>,std::allocator<char> > & model_file, const std::basic_string<char,std::char_traits<char>,std::allocator<char> > & trained_file, const std::basic_string<char,std::char_traits<char>,std::allocator<char> > & mean_file, const std::basic_string<char,std::char_traits<char>,std::allocator<char> > & label_file) Line 72	C++
 	classification-d.exe!main(int argc, char * * argv) Line 269	C++

I am trying to run caffe on my AMD R9 270X GPU using opencl but it crashes in the C:\Windows\System32\amdocl64.dll. If I switch to using my other opencl device (Intel CPU) it compiles successfully. To be honest Im not sure if this should be reported here or to AMD support?

My machine specs:
Windows 10 64-bit
AMD Radeon R9 200 Series with latest drivers (17.7.2)
OpenCL Version: 22.19.662.4

Reproducible:
Ive also reproduced this issue by loading the generated cl program into the AMD APP SDK 2.9.1 HelloWorld.cpp example. 

I've made a few local modifications but I don't think these will cause any change in this behaviour:

diff --git a/scripts/build_win.cmd b/scripts/build_win.cmd
index f90306e3..49176291 100755
--- a/scripts/build_win.cmd
+++ b/scripts/build_win.cmd
@@ -68,7 +68,7 @@ if DEFINED APPVEYOR (
 ) else (
:: Change the settings here to match your setup
:: Change MSVC_VERSION to 12 to use VS 2013
-if NOT DEFINED MSVC_VERSION set MSVC_VERSION=14
+if NOT DEFINED MSVC_VERSION set MSVC_VERSION=12
:: Change to 1 to use Ninja generator (builds much faster)
if NOT DEFINED WITH_NINJA set WITH_NINJA=0
:: Change to 1 to build caffe without CUDA support
@@ -85,8 +85,8 @@ if DEFINED APPVEYOR (
:: Change to 3 if using python 3.5 (only 2.7 and 3.5 are supported)
if NOT DEFINED PYTHON_VERSION set PYTHON_VERSION=2
:: Change these options for your needs.
-if NOT DEFINED BUILD_PYTHON set BUILD_PYTHON=1
-if NOT DEFINED BUILD_PYTHON_LAYER set BUILD_PYTHON_LAYER=1
+if NOT DEFINED BUILD_PYTHON set BUILD_PYTHON=0
+if NOT DEFINED BUILD_PYTHON_LAYER set BUILD_PYTHON_LAYER=0
if NOT DEFINED BUILD_MATLAB set BUILD_MATLAB=0
:: If python is on your path leave this alone
if NOT DEFINED PYTHON_EXE set PYTHON_EXE=python

diff --git a/src/caffe/layers/libdnn_conv_layer.cpp b/src/caffe/layers/libdnn_conv_layer.cpp
index 451f1ef8..600bb6a8 100644
--- a/src/caffe/layers/libdnn_conv_layer.cpp
+++ b/src/caffe/layers/libdnn_conv_layer.cpp
@@ -63,7 +63,7 @@ void LibDNNConvolutionLayer<Dtype>::Reshape(
     config.bias_term = this->bias_term_;
     config.fast_unsafe_math = true;
     config.weights_backward = this->param_propagate_down_[0];
-config.bias_backward = this->param_propagate_down_[1];
+config.bias_backward = this->bias_term_ ? this->param_propagate_down_[1] : false;

if ((std::is_same<Dtype, float>::value

diff --git a/src/caffe/layers/libdnn_deconv_layer.cpp b/src/caffe/layers/libdnn_deconv_layer.cpp
index aacde554..0bc49ca7 100644
--- a/src/caffe/layers/libdnn_deconv_layer.cpp
+++ b/src/caffe/layers/libdnn_deconv_layer.cpp
@@ -59,7 +59,7 @@ void LibDNNDeconvolutionLayer<Dtype>::Reshape(
     config.bias_term = this->bias_term_;
     config.fast_unsafe_math = true;
     config.weights_backward = this->param_propagate_down_[0];
-config.bias_backward = this->param_propagate_down_[1];
+config.bias_backward = this->bias_term_ ? this->param_propagate_down_[1] : false;

// Atomic algorithm requirements:
// - Float & 32 bit atomics available

"
caffe,4304,"Hardware: SKL i5-6600U - Intel GPU
OS: Ubuntu 16.04

Testing AlexNet (batch 128), as defined by soumith's convnet benchmarks. Also tried 32


",0,OpenCL spatial convolution bug ,"OpenCL spatial convolution bug  Hardware: SKL i5-6600U - Intel GPU
OS: Ubuntu 16.04

Testing AlexNet (batch 128), as defined by soumith's convnet benchmarks. Also tried 32


"
caffe,3385,"I was recently faced with a weird compilation error on my university's system, while trying to compile caffe on my home directory. The source of the error was that during the linking phase all files under the ""tools"" directory tried to link against an old caffe library installed system-wide in /usr/local/lib. To solve this issue I had to manually add ./build/lib to my Makefile.config's LIBRARY_DIRS variable, and make sure it was placed before /usr/local/lib.

While the above scenario might be very specific to my university, it still seems to be more correct to link against the local library of caffe under build/lib by default, either by changing Makefile.config.example or by changing the Makefile itself. I think the latter is the preferred option.
",0,Always link to the local libcaffe.so under ./build/lib,"Always link to the local libcaffe.so under ./build/lib I was recently faced with a weird compilation error on my university's system, while trying to compile caffe on my home directory. The source of the error was that during the linking phase all files under the ""tools"" directory tried to link against an old caffe library installed system-wide in /usr/local/lib. To solve this issue I had to manually add ./build/lib to my Makefile.config's LIBRARY_DIRS variable, and make sure it was placed before /usr/local/lib.

While the above scenario might be very specific to my university, it still seems to be more correct to link against the local library of caffe under build/lib by default, either by changing Makefile.config.example or by changing the Makefile itself. I think the latter is the preferred option.
"
caffe,5429,"Please use the [caffe-users list](https://groups.google.com/forum/#!forum/caffe-users) for usage, installation, or modeling questions, or other requests for help.
_Do not post such requests to Issues._ Doing so interferes with the development of Caffe.

Please read the [guidelines for contributing](https://github.com/BVLC/caffe/blob/master/CONTRIBUTING.md) before submitting this issue.

### Issue summary


### Steps to reproduce

If you are having difficulty building Caffe or training a model, please ask the caffe-users mailing list. If you are reporting a build error that seems to be due to a bug in Caffe, please attach your build configuration (either Makefile.config or CMakeCache.txt) and the output of the make (or cmake) command.

### Your system configuration
Operating system:
Compiler:
CUDA version (if applicable):
CUDNN version (if applicable):
BLAS:
Python or MATLAB version (for pycaffe and matcaffe respectively):
",0,make pycaffee error,"make pycaffee error Please use the [caffe-users list](https://groups.google.com/forum/#!forum/caffe-users) for usage, installation, or modeling questions, or other requests for help.
_Do not post such requests to Issues._ Doing so interferes with the development of Caffe.

Please read the [guidelines for contributing](https://github.com/BVLC/caffe/blob/master/CONTRIBUTING.md) before submitting this issue.

### Issue summary


### Steps to reproduce

If you are having difficulty building Caffe or training a model, please ask the caffe-users mailing list. If you are reporting a build error that seems to be due to a bug in Caffe, please attach your build configuration (either Makefile.config or CMakeCache.txt) and the output of the make (or cmake) command.

### Your system configuration
Operating system:
Compiler:
CUDA version (if applicable):
CUDNN version (if applicable):
BLAS:
Python or MATLAB version (for pycaffe and matcaffe respectively):
"
caffe,2307,"MacOS 10.9, Cuda 6.5, Opencv 3.0, Anaconda
In Opencv 2.4.11 I was facing similar problem, solution was including library opencv_imgcodecs. But this was available only in opencv 3.0. But even after installing opencv 3.0.. I am getting same error, although in a different part. Any help appreciated.

It might seem there is a problem with leveldb, but all the environment variables in leveldb have -stdlib=libstdc++. So i can't figure out the problem.

PROTOC src/caffe/proto/caffe.proto
CXX src/caffe/blob.cpp
CXX src/caffe/common.cpp
CXX src/caffe/data_transformer.cpp
CXX src/caffe/internal_thread.cpp
CXX src/caffe/layer_factory.cpp
CXX src/caffe/layers/absval_layer.cpp
CXX src/caffe/layers/accuracy_layer.cpp
CXX src/caffe/layers/argmax_layer.cpp
CXX src/caffe/layers/base_conv_layer.cpp
CXX src/caffe/layers/base_data_layer.cpp
CXX src/caffe/layers/bnll_layer.cpp
CXX src/caffe/layers/concat_layer.cpp
CXX src/caffe/layers/contrastive_loss_layer.cpp
CXX src/caffe/layers/conv_layer.cpp
CXX src/caffe/layers/cudnn_conv_layer.cpp
CXX src/caffe/layers/cudnn_pooling_layer.cpp
CXX src/caffe/layers/cudnn_relu_layer.cpp
CXX src/caffe/layers/cudnn_sigmoid_layer.cpp
CXX src/caffe/layers/cudnn_softmax_layer.cpp
CXX src/caffe/layers/cudnn_tanh_layer.cpp
CXX src/caffe/layers/data_layer.cpp
CXX src/caffe/layers/deconv_layer.cpp
CXX src/caffe/layers/dropout_layer.cpp
CXX src/caffe/layers/dummy_data_layer.cpp
CXX src/caffe/layers/eltwise_layer.cpp
CXX src/caffe/layers/euclidean_loss_layer.cpp
CXX src/caffe/layers/exp_layer.cpp
CXX src/caffe/layers/flatten_layer.cpp
CXX src/caffe/layers/hdf5_data_layer.cpp
CXX src/caffe/layers/hdf5_output_layer.cpp
CXX src/caffe/layers/hinge_loss_layer.cpp
CXX src/caffe/layers/im2col_layer.cpp
CXX src/caffe/layers/image_data_layer.cpp
CXX src/caffe/layers/infogain_loss_layer.cpp
CXX src/caffe/layers/inner_product_layer.cpp
CXX src/caffe/layers/loss_layer.cpp
CXX src/caffe/layers/lrn_layer.cpp
CXX src/caffe/layers/memory_data_layer.cpp
CXX src/caffe/layers/multinomial_logistic_loss_layer.cpp
CXX src/caffe/layers/mvn_layer.cpp
CXX src/caffe/layers/neuron_layer.cpp
CXX src/caffe/layers/pooling_layer.cpp
CXX src/caffe/layers/power_layer.cpp
CXX src/caffe/layers/prelu_layer.cpp
CXX src/caffe/layers/relu_layer.cpp
CXX src/caffe/layers/sigmoid_cross_entropy_loss_layer.cpp
CXX src/caffe/layers/sigmoid_layer.cpp
CXX src/caffe/layers/silence_layer.cpp
CXX src/caffe/layers/slice_layer.cpp
CXX src/caffe/layers/softmax_layer.cpp
CXX src/caffe/layers/softmax_loss_layer.cpp
CXX src/caffe/layers/split_layer.cpp
CXX src/caffe/layers/tanh_layer.cpp
CXX src/caffe/layers/threshold_layer.cpp
CXX src/caffe/layers/window_data_layer.cpp
CXX src/caffe/net.cpp
CXX src/caffe/solver.cpp
CXX src/caffe/syncedmem.cpp
CXX src/caffe/util/benchmark.cpp
CXX src/caffe/util/cudnn.cpp
CXX src/caffe/util/db.cpp
CXX src/caffe/util/im2col.cpp
CXX src/caffe/util/insert_splits.cpp
CXX src/caffe/util/io.cpp
CXX src/caffe/util/math_functions.cpp
CXX src/caffe/util/upgrade_proto.cpp
NVCC src/caffe/layers/absval_layer.cu
NVCC src/caffe/layers/base_data_layer.cu
NVCC src/caffe/layers/bnll_layer.cu
NVCC src/caffe/layers/concat_layer.cu
NVCC src/caffe/layers/contrastive_loss_layer.cu
NVCC src/caffe/layers/conv_layer.cu
NVCC src/caffe/layers/cudnn_conv_layer.cu
NVCC src/caffe/layers/cudnn_pooling_layer.cu
NVCC src/caffe/layers/cudnn_relu_layer.cu
NVCC src/caffe/layers/cudnn_sigmoid_layer.cu
NVCC src/caffe/layers/cudnn_softmax_layer.cu
NVCC src/caffe/layers/cudnn_tanh_layer.cu
NVCC src/caffe/layers/deconv_layer.cu
NVCC src/caffe/layers/dropout_layer.cu
NVCC src/caffe/layers/eltwise_layer.cu
NVCC src/caffe/layers/euclidean_loss_layer.cu
NVCC src/caffe/layers/exp_layer.cu
NVCC src/caffe/layers/hdf5_data_layer.cu
NVCC src/caffe/layers/hdf5_output_layer.cu
NVCC src/caffe/layers/im2col_layer.cu
NVCC src/caffe/layers/inner_product_layer.cu
NVCC src/caffe/layers/lrn_layer.cu
NVCC src/caffe/layers/mvn_layer.cu
NVCC src/caffe/layers/pooling_layer.cu
NVCC src/caffe/layers/power_layer.cu
NVCC src/caffe/layers/prelu_layer.cu
NVCC src/caffe/layers/relu_layer.cu
NVCC src/caffe/layers/sigmoid_cross_entropy_loss_layer.cu
NVCC src/caffe/layers/sigmoid_layer.cu
NVCC src/caffe/layers/silence_layer.cu
NVCC src/caffe/layers/slice_layer.cu
NVCC src/caffe/layers/softmax_layer.cu
NVCC src/caffe/layers/softmax_loss_layer.cu
NVCC src/caffe/layers/split_layer.cu
NVCC src/caffe/layers/tanh_layer.cu
NVCC src/caffe/layers/threshold_layer.cu
NVCC src/caffe/util/im2col.cu
NVCC src/caffe/util/math_functions.cu
CXX tools/caffe.cpp
CXX tools/compute_image_mean.cpp
CXX tools/convert_imageset.cpp
CXX tools/device_query.cpp
CXX tools/extract_features.cpp
CXX tools/finetune_net.cpp
CXX tools/net_speed_benchmark.cpp
CXX tools/test_net.cpp
CXX tools/train_net.cpp
CXX tools/upgrade_net_proto_binary.cpp
CXX tools/upgrade_net_proto_text.cpp
CXX examples/cifar10/convert_cifar_data.cpp
CXX examples/mnist/convert_mnist_data.cpp
CXX examples/siamese/convert_mnist_siamese_data.cpp
CXX .build_release/src/caffe/proto/caffe.pb.cc
AR -o .build_release/lib/libcaffe.a
LD -o .build_release/lib/libcaffe.so
clang: warning: argument unused during compilation: '-pthread'
ld: warning: directory not found for option '-L/opt/intel/mkl/lib/intel64'
Undefined symbols for architecture x86_64:
  ""leveldb::DB::Open(leveldb::Options const&, std::string const&, leveldb::DB*_)"", referenced from:
      caffe::db::LevelDB::Open(std::string const&, caffe::db::Mode) in db.o
ld: symbol(s) not found for architecture x86_64
clang: error: linker command failed with exit code 1 (use -v to see invocation)
make: *_\* [.build_release/lib/libcaffe.so] Error 1
",0,Unable to make caffe on MacOS 10.9,"Unable to make caffe on MacOS 10.9 MacOS 10.9, Cuda 6.5, Opencv 3.0, Anaconda
In Opencv 2.4.11 I was facing similar problem, solution was including library opencv_imgcodecs. But this was available only in opencv 3.0. But even after installing opencv 3.0.. I am getting same error, although in a different part. Any help appreciated.

It might seem there is a problem with leveldb, but all the environment variables in leveldb have -stdlib=libstdc++. So i can't figure out the problem.

PROTOC src/caffe/proto/caffe.proto
CXX src/caffe/blob.cpp
CXX src/caffe/common.cpp
CXX src/caffe/data_transformer.cpp
CXX src/caffe/internal_thread.cpp
CXX src/caffe/layer_factory.cpp
CXX src/caffe/layers/absval_layer.cpp
CXX src/caffe/layers/accuracy_layer.cpp
CXX src/caffe/layers/argmax_layer.cpp
CXX src/caffe/layers/base_conv_layer.cpp
CXX src/caffe/layers/base_data_layer.cpp
CXX src/caffe/layers/bnll_layer.cpp
CXX src/caffe/layers/concat_layer.cpp
CXX src/caffe/layers/contrastive_loss_layer.cpp
CXX src/caffe/layers/conv_layer.cpp
CXX src/caffe/layers/cudnn_conv_layer.cpp
CXX src/caffe/layers/cudnn_pooling_layer.cpp
CXX src/caffe/layers/cudnn_relu_layer.cpp
CXX src/caffe/layers/cudnn_sigmoid_layer.cpp
CXX src/caffe/layers/cudnn_softmax_layer.cpp
CXX src/caffe/layers/cudnn_tanh_layer.cpp
CXX src/caffe/layers/data_layer.cpp
CXX src/caffe/layers/deconv_layer.cpp
CXX src/caffe/layers/dropout_layer.cpp
CXX src/caffe/layers/dummy_data_layer.cpp
CXX src/caffe/layers/eltwise_layer.cpp
CXX src/caffe/layers/euclidean_loss_layer.cpp
CXX src/caffe/layers/exp_layer.cpp
CXX src/caffe/layers/flatten_layer.cpp
CXX src/caffe/layers/hdf5_data_layer.cpp
CXX src/caffe/layers/hdf5_output_layer.cpp
CXX src/caffe/layers/hinge_loss_layer.cpp
CXX src/caffe/layers/im2col_layer.cpp
CXX src/caffe/layers/image_data_layer.cpp
CXX src/caffe/layers/infogain_loss_layer.cpp
CXX src/caffe/layers/inner_product_layer.cpp
CXX src/caffe/layers/loss_layer.cpp
CXX src/caffe/layers/lrn_layer.cpp
CXX src/caffe/layers/memory_data_layer.cpp
CXX src/caffe/layers/multinomial_logistic_loss_layer.cpp
CXX src/caffe/layers/mvn_layer.cpp
CXX src/caffe/layers/neuron_layer.cpp
CXX src/caffe/layers/pooling_layer.cpp
CXX src/caffe/layers/power_layer.cpp
CXX src/caffe/layers/prelu_layer.cpp
CXX src/caffe/layers/relu_layer.cpp
CXX src/caffe/layers/sigmoid_cross_entropy_loss_layer.cpp
CXX src/caffe/layers/sigmoid_layer.cpp
CXX src/caffe/layers/silence_layer.cpp
CXX src/caffe/layers/slice_layer.cpp
CXX src/caffe/layers/softmax_layer.cpp
CXX src/caffe/layers/softmax_loss_layer.cpp
CXX src/caffe/layers/split_layer.cpp
CXX src/caffe/layers/tanh_layer.cpp
CXX src/caffe/layers/threshold_layer.cpp
CXX src/caffe/layers/window_data_layer.cpp
CXX src/caffe/net.cpp
CXX src/caffe/solver.cpp
CXX src/caffe/syncedmem.cpp
CXX src/caffe/util/benchmark.cpp
CXX src/caffe/util/cudnn.cpp
CXX src/caffe/util/db.cpp
CXX src/caffe/util/im2col.cpp
CXX src/caffe/util/insert_splits.cpp
CXX src/caffe/util/io.cpp
CXX src/caffe/util/math_functions.cpp
CXX src/caffe/util/upgrade_proto.cpp
NVCC src/caffe/layers/absval_layer.cu
NVCC src/caffe/layers/base_data_layer.cu
NVCC src/caffe/layers/bnll_layer.cu
NVCC src/caffe/layers/concat_layer.cu
NVCC src/caffe/layers/contrastive_loss_layer.cu
NVCC src/caffe/layers/conv_layer.cu
NVCC src/caffe/layers/cudnn_conv_layer.cu
NVCC src/caffe/layers/cudnn_pooling_layer.cu
NVCC src/caffe/layers/cudnn_relu_layer.cu
NVCC src/caffe/layers/cudnn_sigmoid_layer.cu
NVCC src/caffe/layers/cudnn_softmax_layer.cu
NVCC src/caffe/layers/cudnn_tanh_layer.cu
NVCC src/caffe/layers/deconv_layer.cu
NVCC src/caffe/layers/dropout_layer.cu
NVCC src/caffe/layers/eltwise_layer.cu
NVCC src/caffe/layers/euclidean_loss_layer.cu
NVCC src/caffe/layers/exp_layer.cu
NVCC src/caffe/layers/hdf5_data_layer.cu
NVCC src/caffe/layers/hdf5_output_layer.cu
NVCC src/caffe/layers/im2col_layer.cu
NVCC src/caffe/layers/inner_product_layer.cu
NVCC src/caffe/layers/lrn_layer.cu
NVCC src/caffe/layers/mvn_layer.cu
NVCC src/caffe/layers/pooling_layer.cu
NVCC src/caffe/layers/power_layer.cu
NVCC src/caffe/layers/prelu_layer.cu
NVCC src/caffe/layers/relu_layer.cu
NVCC src/caffe/layers/sigmoid_cross_entropy_loss_layer.cu
NVCC src/caffe/layers/sigmoid_layer.cu
NVCC src/caffe/layers/silence_layer.cu
NVCC src/caffe/layers/slice_layer.cu
NVCC src/caffe/layers/softmax_layer.cu
NVCC src/caffe/layers/softmax_loss_layer.cu
NVCC src/caffe/layers/split_layer.cu
NVCC src/caffe/layers/tanh_layer.cu
NVCC src/caffe/layers/threshold_layer.cu
NVCC src/caffe/util/im2col.cu
NVCC src/caffe/util/math_functions.cu
CXX tools/caffe.cpp
CXX tools/compute_image_mean.cpp
CXX tools/convert_imageset.cpp
CXX tools/device_query.cpp
CXX tools/extract_features.cpp
CXX tools/finetune_net.cpp
CXX tools/net_speed_benchmark.cpp
CXX tools/test_net.cpp
CXX tools/train_net.cpp
CXX tools/upgrade_net_proto_binary.cpp
CXX tools/upgrade_net_proto_text.cpp
CXX examples/cifar10/convert_cifar_data.cpp
CXX examples/mnist/convert_mnist_data.cpp
CXX examples/siamese/convert_mnist_siamese_data.cpp
CXX .build_release/src/caffe/proto/caffe.pb.cc
AR -o .build_release/lib/libcaffe.a
LD -o .build_release/lib/libcaffe.so
clang: warning: argument unused during compilation: '-pthread'
ld: warning: directory not found for option '-L/opt/intel/mkl/lib/intel64'
Undefined symbols for architecture x86_64:
  ""leveldb::DB::Open(leveldb::Options const&, std::string const&, leveldb::DB*_)"", referenced from:
      caffe::db::LevelDB::Open(std::string const&, caffe::db::Mode) in db.o
ld: symbol(s) not found for architecture x86_64
clang: error: linker command failed with exit code 1 (use -v to see invocation)
make: *_\* [.build_release/lib/libcaffe.so] Error 1
"
caffe,351,"I don't have MKL so need to check the dev brach...
""Make"" gives this error:
/usr/bin/g++ src/caffe/common.cpp -pthread -fPIC -DNDEBUG -O2 -I/usr/local/include/python2.7 -I/usr/local/lib/python2.7/dist-packages/numpy/core/include -I/usr/local/include -Ibuild/src -I./src -I./include -I/usr/local/cuda/include -c -o build/src/caffe/common.o
src/caffe/common.cpp: In function const char\* caffe::curandGetErrorString(curandStatus_t):
src/caffe/common.cpp:178:8: error: CURAND_STATUS_DOUBLE_PRECISION_REQUIRED was not declared in this scope
make: **\* [build/src/caffe/common.o] Error 1

Anyone has a clue?
Thanks and regards
",0,compling error of common.cpp in dev branch,"compling error of common.cpp in dev branch I don't have MKL so need to check the dev brach...
""Make"" gives this error:
/usr/bin/g++ src/caffe/common.cpp -pthread -fPIC -DNDEBUG -O2 -I/usr/local/include/python2.7 -I/usr/local/lib/python2.7/dist-packages/numpy/core/include -I/usr/local/include -Ibuild/src -I./src -I./include -I/usr/local/cuda/include -c -o build/src/caffe/common.o
src/caffe/common.cpp: In function const char\* caffe::curandGetErrorString(curandStatus_t):
src/caffe/common.cpp:178:8: error: CURAND_STATUS_DOUBLE_PRECISION_REQUIRED was not declared in this scope
make: **\* [build/src/caffe/common.o] Error 1

Anyone has a clue?
Thanks and regards
"
caffe,58,"I  run with the caffe for training my dataset. But after the iteration 1620, the program crushed in the cublasSgemm. The log is listed as following, Can you give some advices for fixing this error? 

I0127 14:31:22.608165 19425 solver.cpp:204] Iteration 1580, lr = 0.01
I0127 14:31:22.609833 19425 solver.cpp:66] Iteration 1580, loss = 0.0217456
I0127 14:31:49.345432 19425 solver.cpp:204] Iteration 1600, lr = 0.01
I0127 14:31:49.347100 19425 solver.cpp:66] Iteration 1600, loss = 0.0122987
I0127 14:32:16.079083 19425 solver.cpp:204] Iteration 1620, lr = 0.01
I0127 14:32:16.080762 19425 solver.cpp:66] Iteration 1620, loss = 1.67767
F0127 14:32:39.484519 19425 math_functions.cpp:45] Check failed: (cublasSgemm_v2(Caffe::cublas_handle(), cuTransB, cuTransA, N, M, K, &alpha, B, ldb, A, lda, &beta, C, N)) == CUBLAS_STATUS_SUCCESS (14 vs. 0) 
**\* Check failure stack trace: ***
    @     0x7fa69de70b7d  google::LogMessage::Fail()
    @     0x7fa69de72c7f  google::LogMessage::SendToLog()
    @     0x7fa69de7076c  google::LogMessage::Flush()
    @     0x7fa69de7351d  google::LogMessageFatal::~LogMessageFatal()
    @           0x42ee79  caffe::caffe_gpu_gemm<>()
    @           0x45f7e2  caffe::ConvolutionLayer<>::Backward_gpu()
    @           0x42770b  caffe::Net<>::Backward()
    @           0x421278  caffe::Solver<>::Solve()
    @           0x40d055  main
",0,"Crash after the iteration 1620. Check failed,cublasSgemm","Crash after the iteration 1620. Check failed,cublasSgemm I  run with the caffe for training my dataset. But after the iteration 1620, the program crushed in the cublasSgemm. The log is listed as following, Can you give some advices for fixing this error? 

I0127 14:31:22.608165 19425 solver.cpp:204] Iteration 1580, lr = 0.01
I0127 14:31:22.609833 19425 solver.cpp:66] Iteration 1580, loss = 0.0217456
I0127 14:31:49.345432 19425 solver.cpp:204] Iteration 1600, lr = 0.01
I0127 14:31:49.347100 19425 solver.cpp:66] Iteration 1600, loss = 0.0122987
I0127 14:32:16.079083 19425 solver.cpp:204] Iteration 1620, lr = 0.01
I0127 14:32:16.080762 19425 solver.cpp:66] Iteration 1620, loss = 1.67767
F0127 14:32:39.484519 19425 math_functions.cpp:45] Check failed: (cublasSgemm_v2(Caffe::cublas_handle(), cuTransB, cuTransA, N, M, K, &alpha, B, ldb, A, lda, &beta, C, N)) == CUBLAS_STATUS_SUCCESS (14 vs. 0) 
**\* Check failure stack trace: ***
    @     0x7fa69de70b7d  google::LogMessage::Fail()
    @     0x7fa69de72c7f  google::LogMessage::SendToLog()
    @     0x7fa69de7076c  google::LogMessage::Flush()
    @     0x7fa69de7351d  google::LogMessageFatal::~LogMessageFatal()
    @           0x42ee79  caffe::caffe_gpu_gemm<>()
    @           0x45f7e2  caffe::ConvolutionLayer<>::Backward_gpu()
    @           0x42770b  caffe::Net<>::Backward()
    @           0x421278  caffe::Solver<>::Solve()
    @           0x40d055  main
"
caffe,54,"Currently, the algorithm codes are quite aware of the memory layout of the underlying data. Adding a Matrix class in-between helps separate concerns of different modules which is a good practice in software engineering. 

The biggest benefit is to simplify coding and improve the development productivity. It will also ease understanding of the existing and future algorithms. As a result, we will see accelerated development and adoption progresses.

The Matrix class is intended to be a view of 2D array contained in a Blob. Its main functionality is to provide high level wrappers of the common operations.



So that we can write like codes like the following snippets.
The convolution:



The fully connected layer:



The ReLU activation:



The Softmax activation



As you can see, the API is highly inspired by MATLAB which also motivates ArrayFire C++. But of course the snippets are only rough sketches. Many more details need to be considered. For example, if the performance price of boost move operations is too high, it could be replaced by shared_ptr which would complicate the user codes a little. Another question is should we pass in the shared_ptr of the result matrix instead of returning it. More importantly, the GPU codes may greatly differ from the CPU codes depending on whether CUDA can play well with the proposed API syntax. 

Therefore, this issue's scope is limited to the implementation of the Matrix classes for both kinds of devices. Porting algorithms should be put into independent issues until benchmark results show no performance gap between the low level API and the proposed high level API.

Welcome efforts to refine the API and help implement it.
",0,Implement Matrix class to abstract algorithms away from data storage details,"Implement Matrix class to abstract algorithms away from data storage details Currently, the algorithm codes are quite aware of the memory layout of the underlying data. Adding a Matrix class in-between helps separate concerns of different modules which is a good practice in software engineering. 

The biggest benefit is to simplify coding and improve the development productivity. It will also ease understanding of the existing and future algorithms. As a result, we will see accelerated development and adoption progresses.

The Matrix class is intended to be a view of 2D array contained in a Blob. Its main functionality is to provide high level wrappers of the common operations.



So that we can write like codes like the following snippets.
The convolution:



The fully connected layer:



The ReLU activation:



The Softmax activation



As you can see, the API is highly inspired by MATLAB which also motivates ArrayFire C++. But of course the snippets are only rough sketches. Many more details need to be considered. For example, if the performance price of boost move operations is too high, it could be replaced by shared_ptr which would complicate the user codes a little. Another question is should we pass in the shared_ptr of the result matrix instead of returning it. More importantly, the GPU codes may greatly differ from the CPU codes depending on whether CUDA can play well with the proposed API syntax. 

Therefore, this issue's scope is limited to the implementation of the Matrix classes for both kinds of devices. Porting algorithms should be put into independent issues until benchmark results show no performance gap between the low level API and the proposed high level API.

Welcome efforts to refine the API and help implement it.
"
caffe,6325,"
Please read the [guidelines for contributing](https://github.com/BVLC/caffe/blob/master/CONTRIBUTING.md) before submitting this issue.

### Issue summary
Error after: sudo make all -j8


###  System configuration
Operating system: Linux Mint (based on Ubuntu version 16.04)
Compiler:
CUDA version (if applicable): 8.0
CUDNN version (if applicable): 7.0",0,"ERROR: Make all, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD was not declared in this scope","ERROR: Make all, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD was not declared in this scope 
Please read the [guidelines for contributing](https://github.com/BVLC/caffe/blob/master/CONTRIBUTING.md) before submitting this issue.

### Issue summary
Error after: sudo make all -j8


###  System configuration
Operating system: Linux Mint (based on Ubuntu version 16.04)
Compiler:
CUDA version (if applicable): 8.0
CUDNN version (if applicable): 7.0"
caffe,6859,"## Important - read before submitting

*Please read the [guidelines for contributing](https://github.com/BVLC/caffe/blob/master/CONTRIBUTING.md) before submitting this issue!*

*Please do not post installation, build, usage, or modeling questions, or other requests for help to Issues.*
Use the [caffe-users list](https://groups.google.com/forum/#!forum/caffe-users) instead.
This helps developers maintain a clear, uncluttered, and efficient view of the state of Caffe.

### Issue summary


### Steps to reproduce


### Tried solutions


### System configuration

* Operating system: 
* Compiler: 
* CUDA version (if applicable): 
* CUDNN version (if applicable): 
* BLAS: 
* Python version (if using pycaffe): 
* MATLAB version (if using matcaffe): 

### Issue checklist

- [ ] read the guidelines and removed the first paragraph
- [ ] written a short summary and detailed steps to reproduce
- [ ] explained how solutions to related problems failed (tick if found none)
- [ ] filled system configuration
- [ ] attached relevant logs/config files (tick if not applicable)
",0,How to make LMDB using regression data (e.g.   0001.jpg 0.865),"How to make LMDB using regression data (e.g.   0001.jpg 0.865) ## Important - read before submitting

*Please read the [guidelines for contributing](https://github.com/BVLC/caffe/blob/master/CONTRIBUTING.md) before submitting this issue!*

*Please do not post installation, build, usage, or modeling questions, or other requests for help to Issues.*
Use the [caffe-users list](https://groups.google.com/forum/#!forum/caffe-users) instead.
This helps developers maintain a clear, uncluttered, and efficient view of the state of Caffe.

### Issue summary


### Steps to reproduce


### Tried solutions


### System configuration

* Operating system: 
* Compiler: 
* CUDA version (if applicable): 
* CUDNN version (if applicable): 
* BLAS: 
* Python version (if using pycaffe): 
* MATLAB version (if using matcaffe): 

### Issue checklist

- [ ] read the guidelines and removed the first paragraph
- [ ] written a short summary and detailed steps to reproduce
- [ ] explained how solutions to related problems failed (tick if found none)
- [ ] filled system configuration
- [ ] attached relevant logs/config files (tick if not applicable)
"
caffe,6832,"When I run the code from GitHubwhich is Face_Alignment_Two_Stage_Re-initialization,I got the following error:
I0911 08:08:29.921020 12228 layer_factory.hpp:88] Creating layer input
I0911 08:08:29.922020 12228 net.cpp:100] Creating Layer input
I0911 08:08:29.922020 12228 net.cpp:418] input -> data
I0911 08:08:29.923020 12228 net.cpp:150] Setting up input
I0911 08:08:29.924021 12228 net.cpp:157] Top shape: 1 3 448 448 (602112)
I0911 08:08:29.927022 12228 net.cpp:165] Memory required for data: 2408448
I0911 08:08:29.927022 12228 layer_factory.hpp:88] Creating layer data_input_0_split
I0911 08:08:29.928025 12228 net.cpp:100] Creating Layer data_input_0_split
I0911 08:08:29.928025 12228 net.cpp:444] data_input_0_split <- data
I0911 08:08:29.929024 12228 net.cpp:418] data_input_0_split -> data_input_0_split_0
I0911 08:08:29.930024 12228 net.cpp:418] data_input_0_split -> data_input_0_split_1
I0911 08:08:29.930024 12228 net.cpp:150] Setting up data_input_0_split
I0911 08:08:29.930024 12228 net.cpp:157] Top shape: 1 3 448 448 (602112)
I0911 08:08:29.932025 12228 net.cpp:157] Top shape: 1 3 448 448 (602112)
I0911 08:08:29.932025 12228 net.cpp:165] Memory required for data: 7225344
I0911 08:08:29.932025 12228 layer_factory.hpp:88] Creating layer downsample_data
I0911 08:08:29.932025 12228 net.cpp:100] Creating Layer downsample_data
I0911 08:08:29.932025 12228 net.cpp:444] downsample_data <- data_input_0_split_0
I0911 08:08:29.932025 12228 net.cpp:418] downsample_data -> downsample_data
I0911 08:08:29.932025 12228 net.cpp:150] Setting up downsample_data
I0911 08:08:29.932025 12228 net.cpp:157] Top shape: (0)
I0911 08:08:29.932025 12228 net.cpp:165] Memory required for data: 7225344
I0911 08:08:29.933027 12228 layer_factory.hpp:88] Creating layer net1_conv1
I0911 08:08:29.933027 12228 net.cpp:100] Creating Layer net1_conv1
I0911 08:08:29.933027 12228 net.cpp:444] net1_conv1 <- downsample_data
I0911 08:08:29.933027 12228 net.cpp:418] net1_conv1 -> net1_conv1
F0911 08:08:29.933027 12228 blob.hpp:122] Check failed: axis_index < num_axes() (1 vs. 0) axis 1 out of range for 0-D Blob with shape (0)
*** Check failure stack trace: ***

The part of the deploy is as follows.

name: ""facial_point_net""
input: ""data""
input_dim: 1
input_dim: 3
input_dim: 448
input_dim: 448

layer {
  name: ""downsample_data""
  type: ""SubsamplePooling""
  bottom: ""data""
  top: ""downsample_data""
  subsample_pooling_param {
    output_H: 60
    output_W: 60
  }
}
##########################################
layer {
  name: ""net1_conv1""
  type: ""Convolution""
  bottom: ""downsample_data""
  top: ""net1_conv1""
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 20
    pad: 0
    kernel_size: 5
    stride: 1
    weight_filler {
      type: ""gaussian""
      std: 0.01
    }
    bias_filler {
      type: ""constant""
      value: 0
    }
  }
}
layer {
  name: ""net1_PReLU1""
  type: ""PReLU""
  bottom: ""net1_conv1""
  top: ""net1_conv1""
}

layer {
  name: ""net1_pool1""
  type: ""Pooling""
  bottom: ""net1_conv1""
  top: ""net1_pool1""
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}

Does anyone know what the problem is ?",0,Check failed: axis_index < num_axes() (1 vs. 0) axis 1 out of range for 0-D Blob with shape (0),"Check failed: axis_index < num_axes() (1 vs. 0) axis 1 out of range for 0-D Blob with shape (0) When I run the code from GitHubwhich is Face_Alignment_Two_Stage_Re-initialization,I got the following error:
I0911 08:08:29.921020 12228 layer_factory.hpp:88] Creating layer input
I0911 08:08:29.922020 12228 net.cpp:100] Creating Layer input
I0911 08:08:29.922020 12228 net.cpp:418] input -> data
I0911 08:08:29.923020 12228 net.cpp:150] Setting up input
I0911 08:08:29.924021 12228 net.cpp:157] Top shape: 1 3 448 448 (602112)
I0911 08:08:29.927022 12228 net.cpp:165] Memory required for data: 2408448
I0911 08:08:29.927022 12228 layer_factory.hpp:88] Creating layer data_input_0_split
I0911 08:08:29.928025 12228 net.cpp:100] Creating Layer data_input_0_split
I0911 08:08:29.928025 12228 net.cpp:444] data_input_0_split <- data
I0911 08:08:29.929024 12228 net.cpp:418] data_input_0_split -> data_input_0_split_0
I0911 08:08:29.930024 12228 net.cpp:418] data_input_0_split -> data_input_0_split_1
I0911 08:08:29.930024 12228 net.cpp:150] Setting up data_input_0_split
I0911 08:08:29.930024 12228 net.cpp:157] Top shape: 1 3 448 448 (602112)
I0911 08:08:29.932025 12228 net.cpp:157] Top shape: 1 3 448 448 (602112)
I0911 08:08:29.932025 12228 net.cpp:165] Memory required for data: 7225344
I0911 08:08:29.932025 12228 layer_factory.hpp:88] Creating layer downsample_data
I0911 08:08:29.932025 12228 net.cpp:100] Creating Layer downsample_data
I0911 08:08:29.932025 12228 net.cpp:444] downsample_data <- data_input_0_split_0
I0911 08:08:29.932025 12228 net.cpp:418] downsample_data -> downsample_data
I0911 08:08:29.932025 12228 net.cpp:150] Setting up downsample_data
I0911 08:08:29.932025 12228 net.cpp:157] Top shape: (0)
I0911 08:08:29.932025 12228 net.cpp:165] Memory required for data: 7225344
I0911 08:08:29.933027 12228 layer_factory.hpp:88] Creating layer net1_conv1
I0911 08:08:29.933027 12228 net.cpp:100] Creating Layer net1_conv1
I0911 08:08:29.933027 12228 net.cpp:444] net1_conv1 <- downsample_data
I0911 08:08:29.933027 12228 net.cpp:418] net1_conv1 -> net1_conv1
F0911 08:08:29.933027 12228 blob.hpp:122] Check failed: axis_index < num_axes() (1 vs. 0) axis 1 out of range for 0-D Blob with shape (0)
*** Check failure stack trace: ***

The part of the deploy is as follows.

name: ""facial_point_net""
input: ""data""
input_dim: 1
input_dim: 3
input_dim: 448
input_dim: 448

layer {
  name: ""downsample_data""
  type: ""SubsamplePooling""
  bottom: ""data""
  top: ""downsample_data""
  subsample_pooling_param {
    output_H: 60
    output_W: 60
  }
}
##########################################
layer {
  name: ""net1_conv1""
  type: ""Convolution""
  bottom: ""downsample_data""
  top: ""net1_conv1""
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 20
    pad: 0
    kernel_size: 5
    stride: 1
    weight_filler {
      type: ""gaussian""
      std: 0.01
    }
    bias_filler {
      type: ""constant""
      value: 0
    }
  }
}
layer {
  name: ""net1_PReLU1""
  type: ""PReLU""
  bottom: ""net1_conv1""
  top: ""net1_conv1""
}

layer {
  name: ""net1_pool1""
  type: ""Pooling""
  bottom: ""net1_conv1""
  top: ""net1_pool1""
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}

Does anyone know what the problem is ?"
caffe,5589,"I try to train alexnet over ImageNet. I read its train_val.prototxt, its transform_param is as below:

transform_param {
    mirror: true
    crop_size: 227
    mean_file: ""data/ilsvrc12/imagenet_mean.binaryproto""
 }

But in the example of MNIST tutorial, there is a scale parameter to normalize the pixel value from [0, 255] to [0, 1], listed as below:

transform_param {
    scale: 0.00390625
 }

And I tried to add ""scale: 0.00390625"" in the transform_param in my case, the accuracy becomes very poor.

Is the lmdb data converted by ""build/tools/convert_imageset"" already normalized? I tried to trace the codes in ""tools/convert_imageset"" and I found the image is loaded by function ""ReadImageToDatum"". Then ReadFileToDatum is called in ReadImageToDatum. Finally in ReadFileToDatum, I still don's see any codes normalizing the pixel value.

Why scale: 0.00390625 is only available in MNIST example but not in other examples? If the lmdb data is normalized, when and where has the normalization been done? thanks",0,why scale within transform_param of data layer used in MNIST example is not applied in ImageNet example?,"why scale within transform_param of data layer used in MNIST example is not applied in ImageNet example? I try to train alexnet over ImageNet. I read its train_val.prototxt, its transform_param is as below:

transform_param {
    mirror: true
    crop_size: 227
    mean_file: ""data/ilsvrc12/imagenet_mean.binaryproto""
 }

But in the example of MNIST tutorial, there is a scale parameter to normalize the pixel value from [0, 255] to [0, 1], listed as below:

transform_param {
    scale: 0.00390625
 }

And I tried to add ""scale: 0.00390625"" in the transform_param in my case, the accuracy becomes very poor.

Is the lmdb data converted by ""build/tools/convert_imageset"" already normalized? I tried to trace the codes in ""tools/convert_imageset"" and I found the image is loaded by function ""ReadImageToDatum"". Then ReadFileToDatum is called in ReadImageToDatum. Finally in ReadFileToDatum, I still don's see any codes normalizing the pixel value.

Why scale: 0.00390625 is only available in MNIST example but not in other examples? If the lmdb data is normalized, when and where has the normalization been done? thanks"
caffe,745,"I am currently creating several million artificial data images in python, all of which I would like stored into LevelDB to be fed through caffe. 

At the moment I'm saving all of the images directly to file, and then using 'create_leveldb.sh' to create the LevelDB directories. This creates a problem as I  am having to save a couple million images to the HDD.

What I am trying to do is have python directly save the artificial images into LevelDB, and do so without having to save the image to file. Currently, my code is trying to emulate what happens in 'ReadImageToDatum' from io.cpp. 

The LevelDB created from my code matches the size (number of leveldb files) of the LevelDB created from  convert_imageset.bin; however, when I train caffe on my leveldb directory, both Test#1 and Test#2 get worse over time.

What I am suspecting is that I have missed something when converting the image into a string format, but I may have missed something completely different.


",0,Creating LevelDB in Python,"Creating LevelDB in Python I am currently creating several million artificial data images in python, all of which I would like stored into LevelDB to be fed through caffe. 

At the moment I'm saving all of the images directly to file, and then using 'create_leveldb.sh' to create the LevelDB directories. This creates a problem as I  am having to save a couple million images to the HDD.

What I am trying to do is have python directly save the artificial images into LevelDB, and do so without having to save the image to file. Currently, my code is trying to emulate what happens in 'ReadImageToDatum' from io.cpp. 

The LevelDB created from my code matches the size (number of leveldb files) of the LevelDB created from  convert_imageset.bin; however, when I train caffe on my leveldb directory, both Test#1 and Test#2 get worse over time.

What I am suspecting is that I have missed something when converting the image into a string format, but I may have missed something completely different.


"
caffe,2259,"I am having trouble compiling caffe on the Jetson TK1 board. I already have Cudnn v2 set up, but I am following the instructions to install caffe and when I get to the 'make -j 4 all' step, I execute that step, but during compilation, I get several errors about 'functions not being declared in this scope' or several other errors that terminate compilation. I know that caffe and cudnn v2 don't play well together (yet), but this version of caffe should. Any advice? Thanks!
",0,Cudnn v2 with caffe on Jetson TK1,"Cudnn v2 with caffe on Jetson TK1 I am having trouble compiling caffe on the Jetson TK1 board. I already have Cudnn v2 set up, but I am following the instructions to install caffe and when I get to the 'make -j 4 all' step, I execute that step, but during compilation, I get several errors about 'functions not being declared in this scope' or several other errors that terminate compilation. I know that caffe and cudnn v2 don't play well together (yet), but this version of caffe should. Any advice? Thanks!
"
caffe,6622,"when i want to create a new type of  layer which need PCL , and add dependencies in cmake/Dependencies.cmake only with find_package line like this:

compile error with MPI xxxxxxx happend. if delete this line, everything's OK. 

building output as follow:
[  0%] Running C++/Python protocol buffer compiler on /home/dongxufu/Dev/caffe/src/caffe/proto/caffe.proto
Scanning dependencies of target caffeproto
[  1%] Building CXX object src/caffe/CMakeFiles/caffeproto.dir/__/__/include/caffe/proto/caffe.pb.cc.o
[  1%] Linking CXX static library ../../lib/libcaffeproto.a
[  1%] Built target caffeproto
Scanning dependencies of target caffe
[  1%] Building CXX object src/caffe/CMakeFiles/caffe.dir/solver.cpp.o
[  1%] Building CXX object src/caffe/CMakeFiles/caffe.dir/blob.cpp.o
[  3%] Building CXX object src/caffe/CMakeFiles/caffe.dir/data_transformer.cpp.o
[  3%] Building CXX object src/caffe/CMakeFiles/caffe.dir/syncedmem.cpp.o
[  4%] Building CXX object src/caffe/CMakeFiles/caffe.dir/parallel.cpp.o
[  4%] Building CXX object src/caffe/CMakeFiles/caffe.dir/layer_factory.cpp.o
[  6%] Building CXX object src/caffe/CMakeFiles/caffe.dir/layer.cpp.o
[  6%] Building CXX object src/caffe/CMakeFiles/caffe.dir/solvers/nesterov_solver.cpp.o
[  7%] Building CXX object src/caffe/CMakeFiles/caffe.dir/solvers/adagrad_solver.cpp.o
[  7%] Building CXX object src/caffe/CMakeFiles/caffe.dir/solvers/sgd_solver.cpp.o
[  9%] Building CXX object src/caffe/CMakeFiles/caffe.dir/solvers/adam_solver.cpp.o
[  9%] Building CXX object src/caffe/CMakeFiles/caffe.dir/solvers/rmsprop_solver.cpp.o
[ 10%] Building CXX object src/caffe/CMakeFiles/caffe.dir/solvers/adadelta_solver.cpp.o
[ 10%] Building CXX object src/caffe/CMakeFiles/caffe.dir/layers/cudnn_relu_layer.cpp.o
[ 12%] Building CXX object src/caffe/CMakeFiles/caffe.dir/layers/cudnn_softmax_layer.cpp.o
[ 12%] Building CXX object src/caffe/CMakeFiles/caffe.dir/layers/rnn_layer.cpp.o
[ 14%] Building CXX object src/caffe/CMakeFiles/caffe.dir/layers/lstm_layer.cpp.o
[ 14%] Building CXX object src/caffe/CMakeFiles/caffe.dir/layers/cudnn_sigmoid_layer.cpp.o
[ 15%] Building CXX object src/caffe/CMakeFiles/caffe.dir/layers/concat_layer.cpp.o
[ 15%] Building CXX object src/caffe/CMakeFiles/caffe.dir/layers/mvn_layer.cpp.o
[ 17%] Building CXX object src/caffe/CMakeFiles/caffe.dir/layers/cudnn_lrn_layer.cpp.o
[ 17%] Building CXX object src/caffe/CMakeFiles/caffe.dir/layers/cudnn_conv_layer.cpp.o
[ 18%] Building CXX object src/caffe/CMakeFiles/caffe.dir/layers/absval_layer.cpp.o
[ 18%] Building CXX object src/caffe/CMakeFiles/caffe.dir/layers/input_layer.cpp.o
[ 20%] Building CXX object src/caffe/CMakeFiles/caffe.dir/layers/silence_layer.cpp.o
[ 20%] Building CXX object src/caffe/CMakeFiles/caffe.dir/layers/cudnn_tanh_layer.cpp.o
[ 21%] Building CXX object src/caffe/CMakeFiles/caffe.dir/layers/recurrent_layer.cpp.o
[ 21%] Building CXX object src/caffe/CMakeFiles/caffe.dir/layers/image_data_layer.cpp.o
[ 23%] Building CXX object src/caffe/CMakeFiles/caffe.dir/layers/window_data_layer.cpp.o
[ 23%] Building CXX object src/caffe/CMakeFiles/caffe.dir/layers/batch_reindex_layer.cpp.o
[ 25%] Building CXX object src/caffe/CMakeFiles/caffe.dir/layers/accuracy_layer.cpp.o
[ 25%] Building CXX object src/caffe/CMakeFiles/caffe.dir/layers/inner_product_layer.cpp.o
[ 26%] Building CXX object src/caffe/CMakeFiles/caffe.dir/layers/argmax_layer.cpp.o
[ 26%] Building CXX object src/caffe/CMakeFiles/caffe.dir/layers/relu_layer.cpp.o
[ 28%] Building CXX object src/caffe/CMakeFiles/caffe.dir/layers/clip_layer.cpp.o
[ 28%] Building CXX object src/caffe/CMakeFiles/caffe.dir/layers/neuron_layer.cpp.o
[ 29%] Building CXX object src/caffe/CMakeFiles/caffe.dir/layers/spp_layer.cpp.o
[ 29%] Building CXX object src/caffe/CMakeFiles/caffe.dir/layers/slice_layer.cpp.o
[ 31%] Building CXX object src/caffe/CMakeFiles/caffe.dir/layers/flatten_layer.cpp.o
[ 31%] Building CXX object src/caffe/CMakeFiles/caffe.dir/layers/lstm_unit_layer.cpp.o
[ 32%] Building CXX object src/caffe/CMakeFiles/caffe.dir/layers/log_layer.cpp.o
[ 32%] Building CXX object src/caffe/CMakeFiles/caffe.dir/layers/softmax_layer.cpp.o
[ 34%] Building CXX object src/caffe/CMakeFiles/caffe.dir/layers/base_data_layer.cpp.o
[ 34%] Building CXX object src/caffe/CMakeFiles/caffe.dir/layers/euclidean_loss_layer.cpp.o
[ 35%] Building CXX object src/caffe/CMakeFiles/caffe.dir/layers/sigmoid_layer.cpp.o
[ 35%] Building CXX object src/caffe/CMakeFiles/caffe.dir/layers/base_conv_layer.cpp.o
[ 37%] Building CXX object src/caffe/CMakeFiles/caffe.dir/layers/loss_layer.cpp.o
[ 37%] Building CXX object src/caffe/CMakeFiles/caffe.dir/layers/power_layer.cpp.o
[ 39%] Building CXX object src/caffe/CMakeFiles/caffe.dir/layers/hdf5_output_layer.cpp.o
[ 39%] Building CXX object src/caffe/CMakeFiles/caffe.dir/layers/crop_layer.cpp.o
[ 40%] Building CXX object src/caffe/CMakeFiles/caffe.dir/layers/dropout_layer.cpp.o
[ 40%] Building CXX object src/caffe/CMakeFiles/caffe.dir/layers/bias_layer.cpp.o
[ 42%] Building CXX object src/caffe/CMakeFiles/caffe.dir/layers/conv_layer.cpp.o
[ 42%] Building CXX object src/caffe/CMakeFiles/caffe.dir/layers/parameter_layer.cpp.o
[ 43%] Building CXX object src/caffe/CMakeFiles/caffe.dir/layers/reduction_layer.cpp.o
[ 43%] Building CXX object src/caffe/CMakeFiles/caffe.dir/layers/tanh_layer.cpp.o
[ 45%] Building CXX object src/caffe/CMakeFiles/caffe.dir/layers/pooling_layer.cpp.o
[ 45%] Building CXX object src/caffe/CMakeFiles/caffe.dir/layers/im2col_layer.cpp.o
[ 46%] Building CXX object src/caffe/CMakeFiles/caffe.dir/layers/hinge_loss_layer.cpp.o
[ 46%] Building CXX object src/caffe/CMakeFiles/caffe.dir/layers/sigmoid_cross_entropy_loss_layer.cpp.o
[ 48%] Building CXX object src/caffe/CMakeFiles/caffe.dir/layers/prelu_layer.cpp.o
[ 48%] Building CXX object src/caffe/CMakeFiles/caffe.dir/layers/contrastive_loss_layer.cpp.o
[ 50%] Building CXX object src/caffe/CMakeFiles/caffe.dir/layers/cudnn_deconv_layer.cpp.o
[ 50%] Building CXX object src/caffe/CMakeFiles/caffe.dir/layers/split_layer.cpp.o
[ 51%] Building CXX object src/caffe/CMakeFiles/caffe.dir/layers/hdf5_data_layer.cpp.o
[ 51%] Building CXX object src/caffe/CMakeFiles/caffe.dir/layers/threshold_layer.cpp.o
[ 53%] Building CXX object src/caffe/CMakeFiles/caffe.dir/layers/cudnn_lcn_layer.cpp.o
[ 53%] Building CXX object src/caffe/CMakeFiles/caffe.dir/layers/bnll_layer.cpp.o
[ 53%] Building CXX object src/caffe/CMakeFiles/caffe.dir/layers/elu_layer.cpp.o
[ 54%] Building CXX object src/caffe/CMakeFiles/caffe.dir/layers/cudnn_pooling_layer.cpp.o
[ 54%] Building CXX object src/caffe/CMakeFiles/caffe.dir/layers/infogain_loss_layer.cpp.o
[ 56%] Building CXX object src/caffe/CMakeFiles/caffe.dir/layers/deconv_layer.cpp.o
[ 56%] Building CXX object src/caffe/CMakeFiles/caffe.dir/layers/eltwise_layer.cpp.o
[ 57%] Building CXX object src/caffe/CMakeFiles/caffe.dir/layers/scale_layer.cpp.o
[ 57%] Building CXX object src/caffe/CMakeFiles/caffe.dir/layers/softmax_loss_layer.cpp.o
[ 59%] Building CXX object src/caffe/CMakeFiles/caffe.dir/layers/filter_layer.cpp.o
[ 59%] Building CXX object src/caffe/CMakeFiles/caffe.dir/layers/dummy_data_layer.cpp.o
[ 60%] Building CXX object src/caffe/CMakeFiles/caffe.dir/layers/lrn_layer.cpp.o
[ 60%] Building CXX object src/caffe/CMakeFiles/caffe.dir/layers/tile_layer.cpp.o
[ 62%] Building CXX object src/caffe/CMakeFiles/caffe.dir/layers/exp_layer.cpp.o
[ 62%] Building CXX object src/caffe/CMakeFiles/caffe.dir/layers/data_layer.cpp.o
[ 64%] Building CXX object src/caffe/CMakeFiles/caffe.dir/layers/memory_data_layer.cpp.o
[ 64%] Building CXX object src/caffe/CMakeFiles/caffe.dir/layers/multinomial_logistic_loss_layer.cpp.o
[ 65%] Building CXX object src/caffe/CMakeFiles/caffe.dir/layers/reshape_layer.cpp.o
[ 65%] Building CXX object src/caffe/CMakeFiles/caffe.dir/layers/embed_layer.cpp.o
[ 67%] Building CXX object src/caffe/CMakeFiles/caffe.dir/layers/swish_layer.cpp.o
[ 67%] Building CXX object src/caffe/CMakeFiles/caffe.dir/layers/batch_norm_layer.cpp.o
[ 68%] Building CXX object src/caffe/CMakeFiles/caffe.dir/util/im2col.cpp.o
[ 68%] Building CXX object src/caffe/CMakeFiles/caffe.dir/util/db_lmdb.cpp.o
[ 70%] Building CXX object src/caffe/CMakeFiles/caffe.dir/util/io.cpp.o
[ 70%] Building CXX object src/caffe/CMakeFiles/caffe.dir/util/signal_handler.cpp.o
[ 71%] Building CXX object src/caffe/CMakeFiles/caffe.dir/util/math_functions.cpp.o
[ 71%] Building CXX object src/caffe/CMakeFiles/caffe.dir/util/benchmark.cpp.o
[ 73%] Building CXX object src/caffe/CMakeFiles/caffe.dir/util/cudnn.cpp.o
[ 73%] Building CXX object src/caffe/CMakeFiles/caffe.dir/util/upgrade_proto.cpp.o
[ 75%] Building CXX object src/caffe/CMakeFiles/caffe.dir/util/insert_splits.cpp.o
[ 75%] Building CXX object src/caffe/CMakeFiles/caffe.dir/util/blocking_queue.cpp.o
[ 76%] Building CXX object src/caffe/CMakeFiles/caffe.dir/util/hdf5.cpp.o
[ 76%] Building CXX object src/caffe/CMakeFiles/caffe.dir/util/db.cpp.o
[ 78%] Building CXX object src/caffe/CMakeFiles/caffe.dir/util/db_leveldb.cpp.o
[ 78%] Building CXX object src/caffe/CMakeFiles/caffe.dir/net.cpp.o
[ 79%] Building CXX object src/caffe/CMakeFiles/caffe.dir/common.cpp.o
[ 79%] Building CXX object src/caffe/CMakeFiles/caffe.dir/internal_thread.cpp.o
[ 81%] Linking CXX shared library ../../lib/libcaffe.so
[ 81%] Built target caffe
Scanning dependencies of target upgrade_net_proto_text
[ 81%] Building CXX object tools/CMakeFiles/upgrade_net_proto_text.dir/upgrade_net_proto_text.cpp.o
[ 82%] Linking CXX executable upgrade_net_proto_text
../lib/libcaffe.so.1.0.0: undefined reference to ompi_mpi_comm_null'
../lib/libcaffe.so.1.0.0: undefined reference to MPI_Topo_test'
../lib/libcaffe.so.1.0.0: undefined reference to MPI_Type_create_subarray'
../lib/libcaffe.so.1.0.0: undefined reference to MPI_Comm_dup'
../lib/libcaffe.so.1.0.0: undefined reference to MPI_Get_count'
../lib/libcaffe.so.1.0.0: undefined reference to MPI_Graph_neighbors'
../lib/libcaffe.so.1.0.0: undefined reference to MPI_Comm_create'
../lib/libcaffe.so.1.0.0: undefined reference to MPI_Scan'
../lib/libcaffe.so.1.0.0: undefined reference to MPI_Test_cancelled'
../lib/libcaffe.so.1.0.0: undefined reference to MPI_Errhandler_free'
../lib/libcaffe.so.1.0.0: undefined reference to MPI_Group_incl'
../lib/libcaffe.so.1.0.0: undefined reference to MPI_Group_range_excl'
../lib/libcaffe.so.1.0.0: undefined reference to MPI_Comm_get_attr'
../lib/libcaffe.so.1.0.0: undefined reference to MPI_Type_get_contents'
../lib/libcaffe.so.1.0.0: undefined reference to MPI_Irsend'
../lib/libcaffe.so.1.0.0: undefined reference to MPI_Pack_external_size'
../lib/libcaffe.so.1.0.0: undefined reference to MPI_Sendrecv_replace'
../lib/libcaffe.so.1.0.0: undefined reference to MPI_Comm_remote_group'
../lib/libcaffe.so.1.0.0: undefined reference to MPI_Ssend_init'
../lib/libcaffe.so.1.0.0: undefined reference to MPI_Win_set_errhandler'
../lib/libcaffe.so.1.0.0: undefined reference to MPI_Win_start'
../lib/libcaffe.so.1.0.0: undefined reference to MPI_Comm_set_name'
../lib/libcaffe.so.1.0.0: undefined reference to MPI_Accumulate'
../lib/libcaffe.so.1.0.0: undefined reference to MPI_Initialized'
../lib/libcaffe.so.1.0.0: undefined reference to MPI_Graph_neighbors_count'
../lib/libcaffe.so.1.0.0: undefined reference to MPI_Probe'
../lib/libcaffe.so.1.0.0: undefined reference to MPI_Allgatherv'
../lib/libcaffe.so.1.0.0: undefined reference to MPI_Comm_disconnect'
../lib/libcaffe.so.1.0.0: undefined reference to MPI_Info_get'
../lib/libcaffe.so.1.0.0: undefined reference to MPI_Cancel'
../lib/libcaffe.so.1.0.0: undefined reference to MPI_Unpack_external'
../lib/libcaffe.so.1.0.0: undefined reference to MPI_Win_wait'
../lib/libcaffe.so.1.0.0: undefined reference to MPI_Send'
../lib/libcaffe.so.1.0.0: undefined reference to MPI_Type_size'
../lib/libcaffe.so.1.0.0: undefined reference to MPI_Allreduce'
../lib/libcaffe.so.1.0.0: undefined reference to MPI_Reduce_local'
../lib/libcaffe.so.1.0.0: undefined reference to MPI_Info_delete'
../lib/libcaffe.so.1.0.0: undefined reference to MPI_Exscan'
../lib/libcaffe.so.1.0.0: undefined reference to MPI_Status_set_cancelled'
../lib/libcaffe.so.1.0.0: undefined reference to MPI_Win_get_group'
../lib/libcaffe.so.1.0.0: undefined reference to MPI_Bsend'
../lib/libcaffe.so.1.0.0: undefined reference to MPI_Cartdim_get'
../lib/libcaffe.so.1.0.0: undefined reference to MPI_Win_lock'
../lib/libcaffe.so.1.0.0: undefined reference to MPI_Cart_coords'
../lib/libcaffe.so.1.0.0: undefined reference to MPI_Type_create_indexed_block'
../lib/libcaffe.so.1.0.0: undefined reference to MPI_Comm_free'
../lib/libcaffe.so.1.0.0: undefined reference to MPI_Type_dup'
../lib/libcaffe.so.1.0.0: undefined reference to MPI_Reduce'
../lib/libcaffe.so.1.0.0: undefined reference to MPI_Cart_rank'
../lib/libcaffe.so.1.0.0: undefined reference to MPI_Comm_delete_attr'
../lib/libcaffe.so.1.0.0: undefined reference to MPI_Wait'
../lib/libcaffe.so.1.0.0: undefined reference to MPI_Ibsend'
../lib/libcaffe.so.1.0.0: undefined reference to MPI_Type_create_resized'
../lib/libcaffe.so.1.0.0: undefined reference to MPI_Get_elements'
../lib/libcaffe.so.1.0.0: undefined reference to MPI_Op_create'
../lib/libcaffe.so.1.0.0: undefined reference to ompi_op_set_cxx_callback'
../lib/libcaffe.so.1.0.0: undefined reference to MPI_Pack_size'
../lib/libcaffe.so.1.0.0: undefined reference to MPI_Ssend'
../lib/libcaffe.so.1.0.0: undefined reference to MPI_Test'
../lib/libcaffe.so.1.0.0: undefined reference to MPI_Comm_spawn_multiple'
../lib/libcaffe.so.1.0.0: undefined reference to MPI_Win_delete_attr'
../lib/libcaffe.so.1.0.0: undefined reference to MPI_Type_set_attr'
../lib/libcaffe.so.1.0.0: undefined reference to MPI::Comm::Comm()'
../lib/libcaffe.so.1.0.0: undefined reference to MPI_Rsend_init'
../lib/libcaffe.so.1.0.0: undefined reference to MPI_Comm_test_inter'
../lib/libcaffe.so.1.0.0: undefined reference to MPI_Type_get_name'
../lib/libcaffe.so.1.0.0: undefined reference to MPI_Comm_spawn'
../lib/libcaffe.so.1.0.0: undefined reference to MPI_Win_set_name'
../lib/libcaffe.so.1.0.0: undefined reference to MPI_Cart_shift'
../lib/libcaffe.so.1.0.0: undefined reference to MPI_Status_set_elements'
../lib/libcaffe.so.1.0.0: undefined reference to MPI_Win_call_errhandler'
../lib/libcaffe.so.1.0.0: undefined reference to MPI_Win_get_errhandler'
../lib/libcaffe.so.1.0.0: undefined reference to MPI_Intercomm_merge'
../lib/libcaffe.so.1.0.0: undefined reference to MPI_Group_range_incl'
../lib/libcaffe.so.1.0.0: undefined reference to MPI_Group_free'
../lib/libcaffe.so.1.0.0: undefined reference to MPI_Cart_get'
../lib/libcaffe.so.1.0.0: undefined reference to MPI_Pack_external'
../lib/libcaffe.so.1.0.0: undefined reference to MPI::Win::Free()'
../lib/libcaffe.so.1.0.0: undefined reference to MPI_Win_unlock'
../lib/libcaffe.so.1.0.0: undefined reference to MPI_Alltoallw'
collect2: error: ld returned 1 exit status
tools/CMakeFiles/upgrade_net_proto_text.dir/build.make:132: recipe for target 'tools/upgrade_net_proto_text' failed
make[2]: *** [tools/upgrade_net_proto_text] Error 1
CMakeFiles/Makefile2:457: recipe for target 'tools/CMakeFiles/upgrade_net_proto_text.dir/all' failed
make[1]: *** [tools/CMakeFiles/upgrade_net_proto_text.dir/all] Error 2
Makefile:127: recipe for target 'all' failed
make: *** [all] Error 2

anybody knows why?",0,../lib/libcaffe.so.1.0.0: undefined reference to `MPI_Type_get_envelope',"../lib/libcaffe.so.1.0.0: undefined reference to `MPI_Type_get_envelope' when i want to create a new type of  layer which need PCL , and add dependencies in cmake/Dependencies.cmake only with find_package line like this:

compile error with MPI xxxxxxx happend. if delete this line, everything's OK. 

building output as follow:
[  0%] Running C++/Python protocol buffer compiler on /home/dongxufu/Dev/caffe/src/caffe/proto/caffe.proto
Scanning dependencies of target caffeproto
[  1%] Building CXX object src/caffe/CMakeFiles/caffeproto.dir/__/__/include/caffe/proto/caffe.pb.cc.o
[  1%] Linking CXX static library ../../lib/libcaffeproto.a
[  1%] Built target caffeproto
Scanning dependencies of target caffe
[  1%] Building CXX object src/caffe/CMakeFiles/caffe.dir/solver.cpp.o
[  1%] Building CXX object src/caffe/CMakeFiles/caffe.dir/blob.cpp.o
[  3%] Building CXX object src/caffe/CMakeFiles/caffe.dir/data_transformer.cpp.o
[  3%] Building CXX object src/caffe/CMakeFiles/caffe.dir/syncedmem.cpp.o
[  4%] Building CXX object src/caffe/CMakeFiles/caffe.dir/parallel.cpp.o
[  4%] Building CXX object src/caffe/CMakeFiles/caffe.dir/layer_factory.cpp.o
[  6%] Building CXX object src/caffe/CMakeFiles/caffe.dir/layer.cpp.o
[  6%] Building CXX object src/caffe/CMakeFiles/caffe.dir/solvers/nesterov_solver.cpp.o
[  7%] Building CXX object src/caffe/CMakeFiles/caffe.dir/solvers/adagrad_solver.cpp.o
[  7%] Building CXX object src/caffe/CMakeFiles/caffe.dir/solvers/sgd_solver.cpp.o
[  9%] Building CXX object src/caffe/CMakeFiles/caffe.dir/solvers/adam_solver.cpp.o
[  9%] Building CXX object src/caffe/CMakeFiles/caffe.dir/solvers/rmsprop_solver.cpp.o
[ 10%] Building CXX object src/caffe/CMakeFiles/caffe.dir/solvers/adadelta_solver.cpp.o
[ 10%] Building CXX object src/caffe/CMakeFiles/caffe.dir/layers/cudnn_relu_layer.cpp.o
[ 12%] Building CXX object src/caffe/CMakeFiles/caffe.dir/layers/cudnn_softmax_layer.cpp.o
[ 12%] Building CXX object src/caffe/CMakeFiles/caffe.dir/layers/rnn_layer.cpp.o
[ 14%] Building CXX object src/caffe/CMakeFiles/caffe.dir/layers/lstm_layer.cpp.o
[ 14%] Building CXX object src/caffe/CMakeFiles/caffe.dir/layers/cudnn_sigmoid_layer.cpp.o
[ 15%] Building CXX object src/caffe/CMakeFiles/caffe.dir/layers/concat_layer.cpp.o
[ 15%] Building CXX object src/caffe/CMakeFiles/caffe.dir/layers/mvn_layer.cpp.o
[ 17%] Building CXX object src/caffe/CMakeFiles/caffe.dir/layers/cudnn_lrn_layer.cpp.o
[ 17%] Building CXX object src/caffe/CMakeFiles/caffe.dir/layers/cudnn_conv_layer.cpp.o
[ 18%] Building CXX object src/caffe/CMakeFiles/caffe.dir/layers/absval_layer.cpp.o
[ 18%] Building CXX object src/caffe/CMakeFiles/caffe.dir/layers/input_layer.cpp.o
[ 20%] Building CXX object src/caffe/CMakeFiles/caffe.dir/layers/silence_layer.cpp.o
[ 20%] Building CXX object src/caffe/CMakeFiles/caffe.dir/layers/cudnn_tanh_layer.cpp.o
[ 21%] Building CXX object src/caffe/CMakeFiles/caffe.dir/layers/recurrent_layer.cpp.o
[ 21%] Building CXX object src/caffe/CMakeFiles/caffe.dir/layers/image_data_layer.cpp.o
[ 23%] Building CXX object src/caffe/CMakeFiles/caffe.dir/layers/window_data_layer.cpp.o
[ 23%] Building CXX object src/caffe/CMakeFiles/caffe.dir/layers/batch_reindex_layer.cpp.o
[ 25%] Building CXX object src/caffe/CMakeFiles/caffe.dir/layers/accuracy_layer.cpp.o
[ 25%] Building CXX object src/caffe/CMakeFiles/caffe.dir/layers/inner_product_layer.cpp.o
[ 26%] Building CXX object src/caffe/CMakeFiles/caffe.dir/layers/argmax_layer.cpp.o
[ 26%] Building CXX object src/caffe/CMakeFiles/caffe.dir/layers/relu_layer.cpp.o
[ 28%] Building CXX object src/caffe/CMakeFiles/caffe.dir/layers/clip_layer.cpp.o
[ 28%] Building CXX object src/caffe/CMakeFiles/caffe.dir/layers/neuron_layer.cpp.o
[ 29%] Building CXX object src/caffe/CMakeFiles/caffe.dir/layers/spp_layer.cpp.o
[ 29%] Building CXX object src/caffe/CMakeFiles/caffe.dir/layers/slice_layer.cpp.o
[ 31%] Building CXX object src/caffe/CMakeFiles/caffe.dir/layers/flatten_layer.cpp.o
[ 31%] Building CXX object src/caffe/CMakeFiles/caffe.dir/layers/lstm_unit_layer.cpp.o
[ 32%] Building CXX object src/caffe/CMakeFiles/caffe.dir/layers/log_layer.cpp.o
[ 32%] Building CXX object src/caffe/CMakeFiles/caffe.dir/layers/softmax_layer.cpp.o
[ 34%] Building CXX object src/caffe/CMakeFiles/caffe.dir/layers/base_data_layer.cpp.o
[ 34%] Building CXX object src/caffe/CMakeFiles/caffe.dir/layers/euclidean_loss_layer.cpp.o
[ 35%] Building CXX object src/caffe/CMakeFiles/caffe.dir/layers/sigmoid_layer.cpp.o
[ 35%] Building CXX object src/caffe/CMakeFiles/caffe.dir/layers/base_conv_layer.cpp.o
[ 37%] Building CXX object src/caffe/CMakeFiles/caffe.dir/layers/loss_layer.cpp.o
[ 37%] Building CXX object src/caffe/CMakeFiles/caffe.dir/layers/power_layer.cpp.o
[ 39%] Building CXX object src/caffe/CMakeFiles/caffe.dir/layers/hdf5_output_layer.cpp.o
[ 39%] Building CXX object src/caffe/CMakeFiles/caffe.dir/layers/crop_layer.cpp.o
[ 40%] Building CXX object src/caffe/CMakeFiles/caffe.dir/layers/dropout_layer.cpp.o
[ 40%] Building CXX object src/caffe/CMakeFiles/caffe.dir/layers/bias_layer.cpp.o
[ 42%] Building CXX object src/caffe/CMakeFiles/caffe.dir/layers/conv_layer.cpp.o
[ 42%] Building CXX object src/caffe/CMakeFiles/caffe.dir/layers/parameter_layer.cpp.o
[ 43%] Building CXX object src/caffe/CMakeFiles/caffe.dir/layers/reduction_layer.cpp.o
[ 43%] Building CXX object src/caffe/CMakeFiles/caffe.dir/layers/tanh_layer.cpp.o
[ 45%] Building CXX object src/caffe/CMakeFiles/caffe.dir/layers/pooling_layer.cpp.o
[ 45%] Building CXX object src/caffe/CMakeFiles/caffe.dir/layers/im2col_layer.cpp.o
[ 46%] Building CXX object src/caffe/CMakeFiles/caffe.dir/layers/hinge_loss_layer.cpp.o
[ 46%] Building CXX object src/caffe/CMakeFiles/caffe.dir/layers/sigmoid_cross_entropy_loss_layer.cpp.o
[ 48%] Building CXX object src/caffe/CMakeFiles/caffe.dir/layers/prelu_layer.cpp.o
[ 48%] Building CXX object src/caffe/CMakeFiles/caffe.dir/layers/contrastive_loss_layer.cpp.o
[ 50%] Building CXX object src/caffe/CMakeFiles/caffe.dir/layers/cudnn_deconv_layer.cpp.o
[ 50%] Building CXX object src/caffe/CMakeFiles/caffe.dir/layers/split_layer.cpp.o
[ 51%] Building CXX object src/caffe/CMakeFiles/caffe.dir/layers/hdf5_data_layer.cpp.o
[ 51%] Building CXX object src/caffe/CMakeFiles/caffe.dir/layers/threshold_layer.cpp.o
[ 53%] Building CXX object src/caffe/CMakeFiles/caffe.dir/layers/cudnn_lcn_layer.cpp.o
[ 53%] Building CXX object src/caffe/CMakeFiles/caffe.dir/layers/bnll_layer.cpp.o
[ 53%] Building CXX object src/caffe/CMakeFiles/caffe.dir/layers/elu_layer.cpp.o
[ 54%] Building CXX object src/caffe/CMakeFiles/caffe.dir/layers/cudnn_pooling_layer.cpp.o
[ 54%] Building CXX object src/caffe/CMakeFiles/caffe.dir/layers/infogain_loss_layer.cpp.o
[ 56%] Building CXX object src/caffe/CMakeFiles/caffe.dir/layers/deconv_layer.cpp.o
[ 56%] Building CXX object src/caffe/CMakeFiles/caffe.dir/layers/eltwise_layer.cpp.o
[ 57%] Building CXX object src/caffe/CMakeFiles/caffe.dir/layers/scale_layer.cpp.o
[ 57%] Building CXX object src/caffe/CMakeFiles/caffe.dir/layers/softmax_loss_layer.cpp.o
[ 59%] Building CXX object src/caffe/CMakeFiles/caffe.dir/layers/filter_layer.cpp.o
[ 59%] Building CXX object src/caffe/CMakeFiles/caffe.dir/layers/dummy_data_layer.cpp.o
[ 60%] Building CXX object src/caffe/CMakeFiles/caffe.dir/layers/lrn_layer.cpp.o
[ 60%] Building CXX object src/caffe/CMakeFiles/caffe.dir/layers/tile_layer.cpp.o
[ 62%] Building CXX object src/caffe/CMakeFiles/caffe.dir/layers/exp_layer.cpp.o
[ 62%] Building CXX object src/caffe/CMakeFiles/caffe.dir/layers/data_layer.cpp.o
[ 64%] Building CXX object src/caffe/CMakeFiles/caffe.dir/layers/memory_data_layer.cpp.o
[ 64%] Building CXX object src/caffe/CMakeFiles/caffe.dir/layers/multinomial_logistic_loss_layer.cpp.o
[ 65%] Building CXX object src/caffe/CMakeFiles/caffe.dir/layers/reshape_layer.cpp.o
[ 65%] Building CXX object src/caffe/CMakeFiles/caffe.dir/layers/embed_layer.cpp.o
[ 67%] Building CXX object src/caffe/CMakeFiles/caffe.dir/layers/swish_layer.cpp.o
[ 67%] Building CXX object src/caffe/CMakeFiles/caffe.dir/layers/batch_norm_layer.cpp.o
[ 68%] Building CXX object src/caffe/CMakeFiles/caffe.dir/util/im2col.cpp.o
[ 68%] Building CXX object src/caffe/CMakeFiles/caffe.dir/util/db_lmdb.cpp.o
[ 70%] Building CXX object src/caffe/CMakeFiles/caffe.dir/util/io.cpp.o
[ 70%] Building CXX object src/caffe/CMakeFiles/caffe.dir/util/signal_handler.cpp.o
[ 71%] Building CXX object src/caffe/CMakeFiles/caffe.dir/util/math_functions.cpp.o
[ 71%] Building CXX object src/caffe/CMakeFiles/caffe.dir/util/benchmark.cpp.o
[ 73%] Building CXX object src/caffe/CMakeFiles/caffe.dir/util/cudnn.cpp.o
[ 73%] Building CXX object src/caffe/CMakeFiles/caffe.dir/util/upgrade_proto.cpp.o
[ 75%] Building CXX object src/caffe/CMakeFiles/caffe.dir/util/insert_splits.cpp.o
[ 75%] Building CXX object src/caffe/CMakeFiles/caffe.dir/util/blocking_queue.cpp.o
[ 76%] Building CXX object src/caffe/CMakeFiles/caffe.dir/util/hdf5.cpp.o
[ 76%] Building CXX object src/caffe/CMakeFiles/caffe.dir/util/db.cpp.o
[ 78%] Building CXX object src/caffe/CMakeFiles/caffe.dir/util/db_leveldb.cpp.o
[ 78%] Building CXX object src/caffe/CMakeFiles/caffe.dir/net.cpp.o
[ 79%] Building CXX object src/caffe/CMakeFiles/caffe.dir/common.cpp.o
[ 79%] Building CXX object src/caffe/CMakeFiles/caffe.dir/internal_thread.cpp.o
[ 81%] Linking CXX shared library ../../lib/libcaffe.so
[ 81%] Built target caffe
Scanning dependencies of target upgrade_net_proto_text
[ 81%] Building CXX object tools/CMakeFiles/upgrade_net_proto_text.dir/upgrade_net_proto_text.cpp.o
[ 82%] Linking CXX executable upgrade_net_proto_text
../lib/libcaffe.so.1.0.0: undefined reference to ompi_mpi_comm_null'
../lib/libcaffe.so.1.0.0: undefined reference to MPI_Topo_test'
../lib/libcaffe.so.1.0.0: undefined reference to MPI_Type_create_subarray'
../lib/libcaffe.so.1.0.0: undefined reference to MPI_Comm_dup'
../lib/libcaffe.so.1.0.0: undefined reference to MPI_Get_count'
../lib/libcaffe.so.1.0.0: undefined reference to MPI_Graph_neighbors'
../lib/libcaffe.so.1.0.0: undefined reference to MPI_Comm_create'
../lib/libcaffe.so.1.0.0: undefined reference to MPI_Scan'
../lib/libcaffe.so.1.0.0: undefined reference to MPI_Test_cancelled'
../lib/libcaffe.so.1.0.0: undefined reference to MPI_Errhandler_free'
../lib/libcaffe.so.1.0.0: undefined reference to MPI_Group_incl'
../lib/libcaffe.so.1.0.0: undefined reference to MPI_Group_range_excl'
../lib/libcaffe.so.1.0.0: undefined reference to MPI_Comm_get_attr'
../lib/libcaffe.so.1.0.0: undefined reference to MPI_Type_get_contents'
../lib/libcaffe.so.1.0.0: undefined reference to MPI_Irsend'
../lib/libcaffe.so.1.0.0: undefined reference to MPI_Pack_external_size'
../lib/libcaffe.so.1.0.0: undefined reference to MPI_Sendrecv_replace'
../lib/libcaffe.so.1.0.0: undefined reference to MPI_Comm_remote_group'
../lib/libcaffe.so.1.0.0: undefined reference to MPI_Ssend_init'
../lib/libcaffe.so.1.0.0: undefined reference to MPI_Win_set_errhandler'
../lib/libcaffe.so.1.0.0: undefined reference to MPI_Win_start'
../lib/libcaffe.so.1.0.0: undefined reference to MPI_Comm_set_name'
../lib/libcaffe.so.1.0.0: undefined reference to MPI_Accumulate'
../lib/libcaffe.so.1.0.0: undefined reference to MPI_Initialized'
../lib/libcaffe.so.1.0.0: undefined reference to MPI_Graph_neighbors_count'
../lib/libcaffe.so.1.0.0: undefined reference to MPI_Probe'
../lib/libcaffe.so.1.0.0: undefined reference to MPI_Allgatherv'
../lib/libcaffe.so.1.0.0: undefined reference to MPI_Comm_disconnect'
../lib/libcaffe.so.1.0.0: undefined reference to MPI_Info_get'
../lib/libcaffe.so.1.0.0: undefined reference to MPI_Cancel'
../lib/libcaffe.so.1.0.0: undefined reference to MPI_Unpack_external'
../lib/libcaffe.so.1.0.0: undefined reference to MPI_Win_wait'
../lib/libcaffe.so.1.0.0: undefined reference to MPI_Send'
../lib/libcaffe.so.1.0.0: undefined reference to MPI_Type_size'
../lib/libcaffe.so.1.0.0: undefined reference to MPI_Allreduce'
../lib/libcaffe.so.1.0.0: undefined reference to MPI_Reduce_local'
../lib/libcaffe.so.1.0.0: undefined reference to MPI_Info_delete'
../lib/libcaffe.so.1.0.0: undefined reference to MPI_Exscan'
../lib/libcaffe.so.1.0.0: undefined reference to MPI_Status_set_cancelled'
../lib/libcaffe.so.1.0.0: undefined reference to MPI_Win_get_group'
../lib/libcaffe.so.1.0.0: undefined reference to MPI_Bsend'
../lib/libcaffe.so.1.0.0: undefined reference to MPI_Cartdim_get'
../lib/libcaffe.so.1.0.0: undefined reference to MPI_Win_lock'
../lib/libcaffe.so.1.0.0: undefined reference to MPI_Cart_coords'
../lib/libcaffe.so.1.0.0: undefined reference to MPI_Type_create_indexed_block'
../lib/libcaffe.so.1.0.0: undefined reference to MPI_Comm_free'
../lib/libcaffe.so.1.0.0: undefined reference to MPI_Type_dup'
../lib/libcaffe.so.1.0.0: undefined reference to MPI_Reduce'
../lib/libcaffe.so.1.0.0: undefined reference to MPI_Cart_rank'
../lib/libcaffe.so.1.0.0: undefined reference to MPI_Comm_delete_attr'
../lib/libcaffe.so.1.0.0: undefined reference to MPI_Wait'
../lib/libcaffe.so.1.0.0: undefined reference to MPI_Ibsend'
../lib/libcaffe.so.1.0.0: undefined reference to MPI_Type_create_resized'
../lib/libcaffe.so.1.0.0: undefined reference to MPI_Get_elements'
../lib/libcaffe.so.1.0.0: undefined reference to MPI_Op_create'
../lib/libcaffe.so.1.0.0: undefined reference to ompi_op_set_cxx_callback'
../lib/libcaffe.so.1.0.0: undefined reference to MPI_Pack_size'
../lib/libcaffe.so.1.0.0: undefined reference to MPI_Ssend'
../lib/libcaffe.so.1.0.0: undefined reference to MPI_Test'
../lib/libcaffe.so.1.0.0: undefined reference to MPI_Comm_spawn_multiple'
../lib/libcaffe.so.1.0.0: undefined reference to MPI_Win_delete_attr'
../lib/libcaffe.so.1.0.0: undefined reference to MPI_Type_set_attr'
../lib/libcaffe.so.1.0.0: undefined reference to MPI::Comm::Comm()'
../lib/libcaffe.so.1.0.0: undefined reference to MPI_Rsend_init'
../lib/libcaffe.so.1.0.0: undefined reference to MPI_Comm_test_inter'
../lib/libcaffe.so.1.0.0: undefined reference to MPI_Type_get_name'
../lib/libcaffe.so.1.0.0: undefined reference to MPI_Comm_spawn'
../lib/libcaffe.so.1.0.0: undefined reference to MPI_Win_set_name'
../lib/libcaffe.so.1.0.0: undefined reference to MPI_Cart_shift'
../lib/libcaffe.so.1.0.0: undefined reference to MPI_Status_set_elements'
../lib/libcaffe.so.1.0.0: undefined reference to MPI_Win_call_errhandler'
../lib/libcaffe.so.1.0.0: undefined reference to MPI_Win_get_errhandler'
../lib/libcaffe.so.1.0.0: undefined reference to MPI_Intercomm_merge'
../lib/libcaffe.so.1.0.0: undefined reference to MPI_Group_range_incl'
../lib/libcaffe.so.1.0.0: undefined reference to MPI_Group_free'
../lib/libcaffe.so.1.0.0: undefined reference to MPI_Cart_get'
../lib/libcaffe.so.1.0.0: undefined reference to MPI_Pack_external'
../lib/libcaffe.so.1.0.0: undefined reference to MPI::Win::Free()'
../lib/libcaffe.so.1.0.0: undefined reference to MPI_Win_unlock'
../lib/libcaffe.so.1.0.0: undefined reference to MPI_Alltoallw'
collect2: error: ld returned 1 exit status
tools/CMakeFiles/upgrade_net_proto_text.dir/build.make:132: recipe for target 'tools/upgrade_net_proto_text' failed
make[2]: *** [tools/upgrade_net_proto_text] Error 1
CMakeFiles/Makefile2:457: recipe for target 'tools/CMakeFiles/upgrade_net_proto_text.dir/all' failed
make[1]: *** [tools/CMakeFiles/upgrade_net_proto_text.dir/all] Error 2
Makefile:127: recipe for target 'all' failed
make: *** [all] Error 2

anybody knows why?"
caffe,797,"I checked out the latest dev, but always seem to stuck on this test:



I'm running on a mid-2012 MBPr (compute_capability=3.0) so double precision GPU computation shouldn't be an issue here, could anyone offer any idea on this?

Thanks in advance!
",0,Timer test always fails?,"Timer test always fails? I checked out the latest dev, but always seem to stuck on this test:



I'm running on a mid-2012 MBPr (compute_capability=3.0) so double precision GPU computation shouldn't be an issue here, could anyone offer any idea on this?

Thanks in advance!
"
caffe,1623,"In this issue i want to try to collect feedback and policy proposal by core developers and community contributors (we don't have a developer group like caffe-user). Please try to do it in a constructive and scalable prospection. The objective, with the proliferation of deep learning toolkit and research momentum, is to try attract more contributors in Caffe and to retain developers that already opened good  PR.
",0,Community issues and policy feedbacks,"Community issues and policy feedbacks In this issue i want to try to collect feedback and policy proposal by core developers and community contributors (we don't have a developer group like caffe-user). Please try to do it in a constructive and scalable prospection. The objective, with the proliferation of deep learning toolkit and research momentum, is to try attract more contributors in Caffe and to retain developers that already opened good  PR.
"
caffe,5892,"make clean
make all
![screenshot from 2017-09-04 17-41-59](https://user-images.githubusercontent.com/11031352/30026018-c033518e-9198-11e7-8e74-4d4b85ceb0d6.png)


AR -o .build_release/lib/libcaffe.a
LD -o .build_release/lib/libcaffe.so.1.0.0
/usr/bin/ld: cannot find -l -llmdb
collect2: error: ld returned 1 exit status
Makefile:575: recipe for target '.build_release/lib/libcaffe.so.1.0.0' failed
make: *** [.build_release/lib/libcaffe.so.1.0.0] Error 1
any one have solution thank you",0,/usr/bin/ld: cannot find -l -llmdb,"/usr/bin/ld: cannot find -l -llmdb make clean
make all
![screenshot from 2017-09-04 17-41-59](https://user-images.githubusercontent.com/11031352/30026018-c033518e-9198-11e7-8e74-4d4b85ceb0d6.png)


AR -o .build_release/lib/libcaffe.a
LD -o .build_release/lib/libcaffe.so.1.0.0
/usr/bin/ld: cannot find -l -llmdb
collect2: error: ld returned 1 exit status
Makefile:575: recipe for target '.build_release/lib/libcaffe.so.1.0.0' failed
make: *** [.build_release/lib/libcaffe.so.1.0.0] Error 1
any one have solution thank you"
caffe,3053,"In my case, when caffe model finishs predicting an image. There is the below error:

F0910 15:09:52.590445 21913 syncedmem.hpp:30] Check failed: error == cudaSuccess (11 vs. 0)  invalid argument
**\* Check failure stack trace: ***
Aborted

It comes from CUDA_CHECK(cudaFreeHost(ptr));  in syncedmem.hpp.
Everything else works well. Any ideas for fixing this? I use latest code in the master.

I'm using Ubuntu 14.04, NVIDIA driver 352.41, Cuda 7.5 and CuDNN v2.
",0,Error with cudaFreeHost(ptr) in syncedmem.hpp:30,"Error with cudaFreeHost(ptr) in syncedmem.hpp:30 In my case, when caffe model finishs predicting an image. There is the below error:

F0910 15:09:52.590445 21913 syncedmem.hpp:30] Check failed: error == cudaSuccess (11 vs. 0)  invalid argument
**\* Check failure stack trace: ***
Aborted

It comes from CUDA_CHECK(cudaFreeHost(ptr));  in syncedmem.hpp.
Everything else works well. Any ideas for fixing this? I use latest code in the master.

I'm using Ubuntu 14.04, NVIDIA driver 352.41, Cuda 7.5 and CuDNN v2.
"
caffe,6740,"
### Issue summary
PROTOC src/caffe/proto/caffe.proto
CXX .build_debug/src/caffe/proto/caffe.pb.cc
CXX src/caffe/util/upgrade_proto.cpp
CXX src/caffe/util/signal_handler.cpp
CXX src/caffe/util/math_functions.cpp
CXX src/caffe/util/io.cpp
CXX src/caffe/util/insert_splits.cpp
CXX src/caffe/util/im2col.cpp
CXX src/caffe/util/hdf5.cpp
CXX src/caffe/util/db_lmdb.cpp
CXX src/caffe/util/db_leveldb.cpp
CXX src/caffe/util/db.cpp
CXX src/caffe/util/cudnn.cpp
CXX src/caffe/util/blocking_queue.cpp
CXX src/caffe/util/benchmark.cpp
CXX src/caffe/solvers/sgd_solver.cpp
CXX src/caffe/solvers/rmsprop_solver.cpp
CXX src/caffe/solvers/nesterov_solver.cpp
CXX src/caffe/solvers/adam_solver.cpp
CXX src/caffe/solvers/adagrad_solver.cpp
CXX src/caffe/solvers/adadelta_solver.cpp
CXX src/caffe/layers/window_data_layer.cpp
CXX src/caffe/layers/tile_layer.cpp
CXX src/caffe/layers/threshold_layer.cpp
CXX src/caffe/layers/tanh_layer.cpp
CXX src/caffe/layers/swish_layer.cpp
CXX src/caffe/layers/spp_layer.cpp
CXX src/caffe/layers/split_layer.cpp
CXX src/caffe/layers/softmax_loss_layer.cpp
CXX src/caffe/layers/softmax_layer.cpp
CXX src/caffe/layers/slice_layer.cpp
CXX src/caffe/layers/silence_layer.cpp
CXX src/caffe/layers/sigmoid_layer.cpp
CXX src/caffe/layers/sigmoid_cross_entropy_loss_layer.cpp
CXX src/caffe/layers/scale_layer.cpp
CXX src/caffe/layers/rnn_layer.cpp
CXX src/caffe/layers/reshape_layer.cpp
CXX src/caffe/layers/relu_layer.cpp
CXX src/caffe/layers/reduction_layer.cpp
CXX src/caffe/layers/recurrent_layer.cpp
CXX src/caffe/layers/prelu_layer.cpp
CXX src/caffe/layers/power_layer.cpp
CXX src/caffe/layers/pooling_layer.cpp
CXX src/caffe/layers/parameter_layer.cpp
CXX src/caffe/layers/neuron_layer.cpp
CXX src/caffe/layers/mvn_layer.cpp
CXX src/caffe/layers/multinomial_logistic_loss_layer.cpp
CXX src/caffe/layers/memory_data_layer.cpp
CXX src/caffe/layers/lstm_unit_layer.cpp
CXX src/caffe/layers/lstm_layer.cpp
CXX src/caffe/layers/lrn_layer.cpp
CXX src/caffe/layers/loss_layer.cpp
CXX src/caffe/layers/log_layer.cpp
CXX src/caffe/layers/input_layer.cpp
CXX src/caffe/layers/inner_product_layer.cpp
CXX src/caffe/layers/infogain_loss_layer.cpp
CXX src/caffe/layers/image_data_layer.cpp
CXX src/caffe/layers/im2col_layer.cpp
CXX src/caffe/layers/hinge_loss_layer.cpp
CXX src/caffe/layers/hdf5_output_layer.cpp
CXX src/caffe/layers/hdf5_data_layer.cpp
CXX src/caffe/layers/flatten_layer.cpp
CXX src/caffe/layers/filter_layer.cpp
CXX src/caffe/layers/exp_layer.cpp
CXX src/caffe/layers/euclidean_loss_layer.cpp
CXX src/caffe/layers/embed_layer.cpp
CXX src/caffe/layers/elu_layer.cpp
CXX src/caffe/layers/eltwise_layer.cpp
CXX src/caffe/layers/dummy_data_layer.cpp
CXX src/caffe/layers/dropout_layer.cpp
CXX src/caffe/layers/deconv_layer.cpp
CXX src/caffe/layers/data_layer.cpp
CXX src/caffe/layers/cudnn_tanh_layer.cpp
CXX src/caffe/layers/cudnn_softmax_layer.cpp
CXX src/caffe/layers/cudnn_sigmoid_layer.cpp
CXX src/caffe/layers/cudnn_relu_layer.cpp
CXX src/caffe/layers/cudnn_pooling_layer.cpp
CXX src/caffe/layers/cudnn_lrn_layer.cpp
CXX src/caffe/layers/cudnn_lcn_layer.cpp
CXX src/caffe/layers/cudnn_deconv_layer.cpp
CXX src/caffe/layers/cudnn_conv_layer.cpp
CXX src/caffe/layers/crop_layer.cpp
CXX src/caffe/layers/conv_layer.cpp
CXX src/caffe/layers/contrastive_loss_layer.cpp
CXX src/caffe/layers/concat_layer.cpp
CXX src/caffe/layers/clip_layer.cpp
CXX src/caffe/layers/bnll_layer.cpp
CXX src/caffe/layers/bias_layer.cpp
CXX src/caffe/layers/batch_reindex_layer.cpp
CXX src/caffe/layers/batch_norm_layer.cpp
CXX src/caffe/layers/base_data_layer.cpp
CXX src/caffe/layers/base_conv_layer.cpp
CXX src/caffe/layers/argmax_layer.cpp
CXX src/caffe/layers/accuracy_layer.cpp
CXX src/caffe/layers/absval_layer.cpp
CXX src/caffe/syncedmem.cpp
CXX src/caffe/solver.cpp
CXX src/caffe/parallel.cpp
CXX src/caffe/net.cpp
CXX src/caffe/layer_factory.cpp
CXX src/caffe/layer.cpp
CXX src/caffe/internal_thread.cpp
CXX src/caffe/data_transformer.cpp
CXX src/caffe/common.cpp
CXX src/caffe/blob.cpp
NVCC src/caffe/util/math_functions.cu
nvcc fatal   : A single input file is required for a non-link phase when an outputfile is specified
make: *** [.build_debug/cuda/src/caffe/util/math_functions.o] Error 1

### System configuration

* Operating system: Linux
* Compiler: gcc 4.8.5
* CUDA version (if applicable):  CUDA 10 / CUDA 9 all tried
* CUDNN version (if applicable):  7.4.1
* BLAS:  open
* Python version (if using pycaffe):  2.7

### Tried solutions
I tried use CUDA9 and 10 but all failed. It's strange there is only nvcc fatal ... but no more information. I don't know how to continue. Can anyone help me? Thank you !!!

",0,nvcc fatal   : A single input file is required for a non-link phase when an outputfile is specified,"nvcc fatal   : A single input file is required for a non-link phase when an outputfile is specified 
### Issue summary
PROTOC src/caffe/proto/caffe.proto
CXX .build_debug/src/caffe/proto/caffe.pb.cc
CXX src/caffe/util/upgrade_proto.cpp
CXX src/caffe/util/signal_handler.cpp
CXX src/caffe/util/math_functions.cpp
CXX src/caffe/util/io.cpp
CXX src/caffe/util/insert_splits.cpp
CXX src/caffe/util/im2col.cpp
CXX src/caffe/util/hdf5.cpp
CXX src/caffe/util/db_lmdb.cpp
CXX src/caffe/util/db_leveldb.cpp
CXX src/caffe/util/db.cpp
CXX src/caffe/util/cudnn.cpp
CXX src/caffe/util/blocking_queue.cpp
CXX src/caffe/util/benchmark.cpp
CXX src/caffe/solvers/sgd_solver.cpp
CXX src/caffe/solvers/rmsprop_solver.cpp
CXX src/caffe/solvers/nesterov_solver.cpp
CXX src/caffe/solvers/adam_solver.cpp
CXX src/caffe/solvers/adagrad_solver.cpp
CXX src/caffe/solvers/adadelta_solver.cpp
CXX src/caffe/layers/window_data_layer.cpp
CXX src/caffe/layers/tile_layer.cpp
CXX src/caffe/layers/threshold_layer.cpp
CXX src/caffe/layers/tanh_layer.cpp
CXX src/caffe/layers/swish_layer.cpp
CXX src/caffe/layers/spp_layer.cpp
CXX src/caffe/layers/split_layer.cpp
CXX src/caffe/layers/softmax_loss_layer.cpp
CXX src/caffe/layers/softmax_layer.cpp
CXX src/caffe/layers/slice_layer.cpp
CXX src/caffe/layers/silence_layer.cpp
CXX src/caffe/layers/sigmoid_layer.cpp
CXX src/caffe/layers/sigmoid_cross_entropy_loss_layer.cpp
CXX src/caffe/layers/scale_layer.cpp
CXX src/caffe/layers/rnn_layer.cpp
CXX src/caffe/layers/reshape_layer.cpp
CXX src/caffe/layers/relu_layer.cpp
CXX src/caffe/layers/reduction_layer.cpp
CXX src/caffe/layers/recurrent_layer.cpp
CXX src/caffe/layers/prelu_layer.cpp
CXX src/caffe/layers/power_layer.cpp
CXX src/caffe/layers/pooling_layer.cpp
CXX src/caffe/layers/parameter_layer.cpp
CXX src/caffe/layers/neuron_layer.cpp
CXX src/caffe/layers/mvn_layer.cpp
CXX src/caffe/layers/multinomial_logistic_loss_layer.cpp
CXX src/caffe/layers/memory_data_layer.cpp
CXX src/caffe/layers/lstm_unit_layer.cpp
CXX src/caffe/layers/lstm_layer.cpp
CXX src/caffe/layers/lrn_layer.cpp
CXX src/caffe/layers/loss_layer.cpp
CXX src/caffe/layers/log_layer.cpp
CXX src/caffe/layers/input_layer.cpp
CXX src/caffe/layers/inner_product_layer.cpp
CXX src/caffe/layers/infogain_loss_layer.cpp
CXX src/caffe/layers/image_data_layer.cpp
CXX src/caffe/layers/im2col_layer.cpp
CXX src/caffe/layers/hinge_loss_layer.cpp
CXX src/caffe/layers/hdf5_output_layer.cpp
CXX src/caffe/layers/hdf5_data_layer.cpp
CXX src/caffe/layers/flatten_layer.cpp
CXX src/caffe/layers/filter_layer.cpp
CXX src/caffe/layers/exp_layer.cpp
CXX src/caffe/layers/euclidean_loss_layer.cpp
CXX src/caffe/layers/embed_layer.cpp
CXX src/caffe/layers/elu_layer.cpp
CXX src/caffe/layers/eltwise_layer.cpp
CXX src/caffe/layers/dummy_data_layer.cpp
CXX src/caffe/layers/dropout_layer.cpp
CXX src/caffe/layers/deconv_layer.cpp
CXX src/caffe/layers/data_layer.cpp
CXX src/caffe/layers/cudnn_tanh_layer.cpp
CXX src/caffe/layers/cudnn_softmax_layer.cpp
CXX src/caffe/layers/cudnn_sigmoid_layer.cpp
CXX src/caffe/layers/cudnn_relu_layer.cpp
CXX src/caffe/layers/cudnn_pooling_layer.cpp
CXX src/caffe/layers/cudnn_lrn_layer.cpp
CXX src/caffe/layers/cudnn_lcn_layer.cpp
CXX src/caffe/layers/cudnn_deconv_layer.cpp
CXX src/caffe/layers/cudnn_conv_layer.cpp
CXX src/caffe/layers/crop_layer.cpp
CXX src/caffe/layers/conv_layer.cpp
CXX src/caffe/layers/contrastive_loss_layer.cpp
CXX src/caffe/layers/concat_layer.cpp
CXX src/caffe/layers/clip_layer.cpp
CXX src/caffe/layers/bnll_layer.cpp
CXX src/caffe/layers/bias_layer.cpp
CXX src/caffe/layers/batch_reindex_layer.cpp
CXX src/caffe/layers/batch_norm_layer.cpp
CXX src/caffe/layers/base_data_layer.cpp
CXX src/caffe/layers/base_conv_layer.cpp
CXX src/caffe/layers/argmax_layer.cpp
CXX src/caffe/layers/accuracy_layer.cpp
CXX src/caffe/layers/absval_layer.cpp
CXX src/caffe/syncedmem.cpp
CXX src/caffe/solver.cpp
CXX src/caffe/parallel.cpp
CXX src/caffe/net.cpp
CXX src/caffe/layer_factory.cpp
CXX src/caffe/layer.cpp
CXX src/caffe/internal_thread.cpp
CXX src/caffe/data_transformer.cpp
CXX src/caffe/common.cpp
CXX src/caffe/blob.cpp
NVCC src/caffe/util/math_functions.cu
nvcc fatal   : A single input file is required for a non-link phase when an outputfile is specified
make: *** [.build_debug/cuda/src/caffe/util/math_functions.o] Error 1

### System configuration

* Operating system: Linux
* Compiler: gcc 4.8.5
* CUDA version (if applicable):  CUDA 10 / CUDA 9 all tried
* CUDNN version (if applicable):  7.4.1
* BLAS:  open
* Python version (if using pycaffe):  2.7

### Tried solutions
I tried use CUDA9 and 10 but all failed. It's strange there is only nvcc fatal ... but no more information. I don't know how to continue. Can anyone help me? Thank you !!!

"
caffe,920,"When training the imagenet set (1.2M images, 1K classes) using an ImageDataLayer, I get approximately the same (<5% diff) performance as when using leveldb+DataLayer. What is the use of the leveldb then?

fwiw: 36 secs / 20 iterations (5,120 images). Testing: 123 secs / validation set (50,000 images) on a GTX760/4GB and i5-4590
",0,Why use leveldb and DataLayer i.s.o. ImageDataLayer ?,"Why use leveldb and DataLayer i.s.o. ImageDataLayer ? When training the imagenet set (1.2M images, 1K classes) using an ImageDataLayer, I get approximately the same (<5% diff) performance as when using leveldb+DataLayer. What is the use of the leveldb then?

fwiw: 36 secs / 20 iterations (5,120 images). Testing: 123 secs / validation set (50,000 images) on a GTX760/4GB and i5-4590
"
caffe,6153,"Hi:

I am trying to build **matcaffe** with matlab 2015a with gcc-4.7.4. (Ubuntu 16.04)

And I have succeed in compilng caffe for python already.

However, I met this ""caffe_.cpp"" issue and tried several solutions online does not know the best way to fix this. Could anyone help me on this?

Please use the [caffe-users list](https://groups.google.com/forum/#!forum/caffe-users) for usage, installation, or modeling questions, or other requests for help.
_Do not post such requests to Issues._ Doing so interferes with the development of Caffe.

Please read the [guidelines for contributing](https://github.com/BVLC/caffe/blob/master/CONTRIBUTING.md) before submitting this issue.

### Issue summary


### Steps to reproduce

If you are having difficulty building Caffe or training a model, please ask the caffe-users mailing list. If you are reporting a build error that seems to be due to a bug in Caffe, please attach your build configuration (either Makefile.config or CMakeCache.txt) and the output of the make (or cmake) command.

### Your system configuration
Operating system:
Compiler:
CUDA version (if applicable):
CUDNN version (if applicable):
BLAS:
Python or MATLAB version (for pycaffe and matcaffe respectively):
",0,"mex: compile of ' ""matlab/+caffe/private/caffe_.cpp""' failed.","mex: compile of ' ""matlab/+caffe/private/caffe_.cpp""' failed. Hi:

I am trying to build **matcaffe** with matlab 2015a with gcc-4.7.4. (Ubuntu 16.04)

And I have succeed in compilng caffe for python already.

However, I met this ""caffe_.cpp"" issue and tried several solutions online does not know the best way to fix this. Could anyone help me on this?

Please use the [caffe-users list](https://groups.google.com/forum/#!forum/caffe-users) for usage, installation, or modeling questions, or other requests for help.
_Do not post such requests to Issues._ Doing so interferes with the development of Caffe.

Please read the [guidelines for contributing](https://github.com/BVLC/caffe/blob/master/CONTRIBUTING.md) before submitting this issue.

### Issue summary


### Steps to reproduce

If you are having difficulty building Caffe or training a model, please ask the caffe-users mailing list. If you are reporting a build error that seems to be due to a bug in Caffe, please attach your build configuration (either Makefile.config or CMakeCache.txt) and the output of the make (or cmake) command.

### Your system configuration
Operating system:
Compiler:
CUDA version (if applicable):
CUDNN version (if applicable):
BLAS:
Python or MATLAB version (for pycaffe and matcaffe respectively):
"
caffe,5639,"### Issue summary
I ran ""make -j8 runtest"" and noticed that a single test out of 2000 or so fails. This is a CPU test. There seems to be a slight difference in the numerical output, by eye it seems less than <0.001. The error persists if I use openBlas instead of Atlas (using update-alternatives).

### Steps to reproduce
 

Final output:


### Your system configuration
Operating system: linux mint (similar to Ubuntu 16.04 with custom kernel 4.10)
Compiler: gcc version 5.4.0 20160609 (Ubuntu 5.4.0-6ubuntu1~16.04.4) 
CUDA version (if applicable): no
CUDNN version (if applicable): no
BLAS: ATLAS 
Python or MATLAB version (for pycaffe and matcaffe respectively):

I suppose it shouldn't change anything, but the CPU is a Ryzen 1700X and my compilation flags are:
CFLAGS=-O2 -mprefer-avx128 -mavx2 -mcx16 -mmovbe -mf16c -mpopcnt -mbmi -mbmi2 -mclflushopt -fomit-frame-pointer
CXXFLAGS= the same

",0,OpenCL branch: runtest fails NetTest/0 on CPUDevice<float>,"OpenCL branch: runtest fails NetTest/0 on CPUDevice<float> ### Issue summary
I ran ""make -j8 runtest"" and noticed that a single test out of 2000 or so fails. This is a CPU test. There seems to be a slight difference in the numerical output, by eye it seems less than <0.001. The error persists if I use openBlas instead of Atlas (using update-alternatives).

### Steps to reproduce
 

Final output:


### Your system configuration
Operating system: linux mint (similar to Ubuntu 16.04 with custom kernel 4.10)
Compiler: gcc version 5.4.0 20160609 (Ubuntu 5.4.0-6ubuntu1~16.04.4) 
CUDA version (if applicable): no
CUDNN version (if applicable): no
BLAS: ATLAS 
Python or MATLAB version (for pycaffe and matcaffe respectively):

I suppose it shouldn't change anything, but the CPU is a Ryzen 1700X and my compilation flags are:
CFLAGS=-O2 -mprefer-avx128 -mavx2 -mcx16 -mmovbe -mf16c -mpopcnt -mbmi -mbmi2 -mclflushopt -fomit-frame-pointer
CXXFLAGS= the same

"
caffe,2357,"Hi

I am trying to do some regression for audio signals. My training signal is a big spectrogram which I am feeding in as data of size 513 x num_frames. I want my loss to be computed based on this signal and another spectrogram of the same dimensions. I am using 2d convolution for this and am feeding in the data to the first data blob in HDF5 format as 1 1 513 num_frames. I adjusted the padding according to the kernel size at each layer to keep this dimensionality consistent. At the loss layer (using eucledian distance), I have 1 1 513 num_frames but caffe doesn't let me feed in the label as more than 2d (though my data is 4d as mentioned). It gives me an error: Check failed: ndims <= max_dim (4 vs. 2) 

I tried feeding in an extra input to the first data layer of 1 1 513 num_frames as well along with data and label. But I get the error: ExactNumTopBlobs() == top.size() (2 vs. 3) HDF5_DATA Layer produces 2 top blob(s) as output.

I need my labels to be in the format 1 1 513 num_frames. Could you help me with this?
",0,Regression for a 2d signal in caffe,"Regression for a 2d signal in caffe Hi

I am trying to do some regression for audio signals. My training signal is a big spectrogram which I am feeding in as data of size 513 x num_frames. I want my loss to be computed based on this signal and another spectrogram of the same dimensions. I am using 2d convolution for this and am feeding in the data to the first data blob in HDF5 format as 1 1 513 num_frames. I adjusted the padding according to the kernel size at each layer to keep this dimensionality consistent. At the loss layer (using eucledian distance), I have 1 1 513 num_frames but caffe doesn't let me feed in the label as more than 2d (though my data is 4d as mentioned). It gives me an error: Check failed: ndims <= max_dim (4 vs. 2) 

I tried feeding in an extra input to the first data layer of 1 1 513 num_frames as well along with data and label. But I get the error: ExactNumTopBlobs() == top.size() (2 vs. 3) HDF5_DATA Layer produces 2 top blob(s) as output.

I need my labels to be in the format 1 1 513 num_frames. Could you help me with this?
"
caffe,393,"Hi everyone,
I'm trying to run the CIFAR10  example(train_full.sh), which seems to work on the GPU (at least here I don't get the same issue, I'm not sure yet to what value it converges). In any case the problem I'm having is that after the first forward-backward pass somewhere in the pipeline there's a _nan_ introduced. This is not an issue related to the learning rate being to big, as I can even set the lr to 0 and have the same problem.
I tried to figure out what the problem was, but without any success yet. 

First of all, I wanted to ask if anyone else is having the same issues. It would be great if someone could confirm, that something is going wrong here.

What I found out so far is, that this might be related to the LRN layer, because after removal the issue disappears. However it might also be due to the interaction between some of the other layers, I'm not sure. If anyone else has some insight in this, it would be greatly appreciated.
",0,nan issue with CIFAR10 example when running on CPU only,"nan issue with CIFAR10 example when running on CPU only Hi everyone,
I'm trying to run the CIFAR10  example(train_full.sh), which seems to work on the GPU (at least here I don't get the same issue, I'm not sure yet to what value it converges). In any case the problem I'm having is that after the first forward-backward pass somewhere in the pipeline there's a _nan_ introduced. This is not an issue related to the learning rate being to big, as I can even set the lr to 0 and have the same problem.
I tried to figure out what the problem was, but without any success yet. 

First of all, I wanted to ask if anyone else is having the same issues. It would be great if someone could confirm, that something is going wrong here.

What I found out so far is, that this might be related to the LRN layer, because after removal the issue disappears. However it might also be due to the interaction between some of the other layers, I'm not sure. If anyone else has some insight in this, it would be greatly appreciated.
"
caffe,5197,"I am trying to make caffe on CentOS 7. I am pretty sure all the required depedencies are installed.

This is Makefile.config:


the command  gives me the error:


I am sure I have snappy installed. _libsnappy.so.1_ and _libsnappy.so.1.1.4_ are in _/usr/lib64_.

What is the problem? Any kind of help is appreciated.
",0,Unable to compile caffe on CentOS 7 (cannot find -lsnappy),"Unable to compile caffe on CentOS 7 (cannot find -lsnappy) I am trying to make caffe on CentOS 7. I am pretty sure all the required depedencies are installed.

This is Makefile.config:


the command  gives me the error:


I am sure I have snappy installed. _libsnappy.so.1_ and _libsnappy.so.1.1.4_ are in _/usr/lib64_.

What is the problem? Any kind of help is appreciated.
"
caffe,6131,"### Issue summary
I wondered, what is the use of mean_file param under data_param. If it is deprecated, I didn't found any comments in the proto file indicating that. Apparently, it didn't work like its literal meaning. 

### Steps to reproduce

I was using googlenet to do classifications. I got high validation accuracy  during training. Unfortunately, I got weird low test accuracy during testing. I tried to many things like adjusting train sets, modifying learning rate, gamma and so on, I got totally different results even if the modifications were very subtle.

At last, I found that I used the 'mean_file' param under 'data_param' in data layer instead of in 'transform_param'. And it was the cause of weird output of testing. In fact, during initialization of network, it didn't load the mean_file param if it was under the data_param.

And I looked into the caffe.proto file and indeed there is a mean_file param in data_param. That's why no  error raised. But still it seems that it is of no use.

### Your system configuration
Operating system:
Compiler:
CUDA version (if applicable):
CUDNN version (if applicable):
BLAS:
Python or MATLAB version (for pycaffe and matcaffe respectively):
",0,DataParameter field 'mean_file' should be deprecated?,"DataParameter field 'mean_file' should be deprecated? ### Issue summary
I wondered, what is the use of mean_file param under data_param. If it is deprecated, I didn't found any comments in the proto file indicating that. Apparently, it didn't work like its literal meaning. 

### Steps to reproduce

I was using googlenet to do classifications. I got high validation accuracy  during training. Unfortunately, I got weird low test accuracy during testing. I tried to many things like adjusting train sets, modifying learning rate, gamma and so on, I got totally different results even if the modifications were very subtle.

At last, I found that I used the 'mean_file' param under 'data_param' in data layer instead of in 'transform_param'. And it was the cause of weird output of testing. In fact, during initialization of network, it didn't load the mean_file param if it was under the data_param.

And I looked into the caffe.proto file and indeed there is a mean_file param in data_param. That's why no  error raised. But still it seems that it is of no use.

### Your system configuration
Operating system:
Compiler:
CUDA version (if applicable):
CUDNN version (if applicable):
BLAS:
Python or MATLAB version (for pycaffe and matcaffe respectively):
"
caffe,6744,"why i can not open the fold""Windows caffe /Prebuilt binaries""?",0,"why i can not open the fold""Windows caffe /Prebuilt binaries""?","why i can not open the fold""Windows caffe /Prebuilt binaries""? why i can not open the fold""Windows caffe /Prebuilt binaries""?"
caffe,6811,"Run-flownet. py when I run flownet's program: usage: run-flownet.py [-h] [--gpu gpu] [--verbose]
Caffemodel deployproto img0 img1 out
Run-flownet. py: error: too few arguments
An exception has occurred, use % TB to see the full traceback.

SystemExit: 2
What should I do?
Anybody know what's going on? Can some bodies help me to solve it for me",0,About flownet codes` problem,"About flownet codes` problem Run-flownet. py when I run flownet's program: usage: run-flownet.py [-h] [--gpu gpu] [--verbose]
Caffemodel deployproto img0 img1 out
Run-flownet. py: error: too few arguments
An exception has occurred, use % TB to see the full traceback.

SystemExit: 2
What should I do?
Anybody know what's going on? Can some bodies help me to solve it for me"
caffe,5944,"Please have a look at this 
https://pastebin.com/vCNr36dh
make runtest fails with the following error:


Please suggest fixes.",0,make: *** [runtest] Segmentation fault CentOS ,"make: *** [runtest] Segmentation fault CentOS  Please have a look at this 
https://pastebin.com/vCNr36dh
make runtest fails with the following error:


Please suggest fixes."
caffe,6758,"In win10 operating system , how can I solve this problem? Thank you. ",0,sudo rm -rf .nv/,"sudo rm -rf .nv/ In win10 operating system , how can I solve this problem? Thank you. "
caffe,6731,"### Issue summary
I got the latest version caffe and set WITH_PYTHON_LAYER := 1 in makefile.config. However, I still get the following error:

------
F0325 16:49:50.756420  6840 layer_factory.hpp:81] Check failed: registry.count(type) == 1 (0 vs. 1) Unknown layer type: Warping (known types: AbsVal, Accuracy, ArgMax, BNLL, BatchNorm, BatchReindex, Bias, Clip, Concat, ContrastiveLoss, Convolution, Crop, Data, Deconvolution, Dropout, DummyData, ELU, Eltwise, Embed, EuclideanLoss, Exp, Filter, Flatten, HDF5Data, HDF5Output, HingeLoss, Im2col, ImageData, InfogainLoss, InnerProduct, Input, LRN, LSTM, LSTMUnit, Log, MVN, MemoryData, MultinomialLogisticLoss, PReLU, Parameter, Pooling, Power, Python, RNN, ReLU, Reduction, Reshape, SPP, Scale, Sigmoid, SigmoidCrossEntropyLoss, Silence, Slice, Softmax, SoftmaxWithLoss, Split, Swish, TanH, Threshold, Tile, WindowData)
*** Check failure stack trace: ***
    @     0x7fb0d2d7f5cd  google::LogMessage::Fail()
    @     0x7fb0d2d81433  google::LogMessage::SendToLog()
    @     0x7fb0d2d7f15b  google::LogMessage::Flush()
    @     0x7fb0d2d81e1e  google::LogMessageFatal::~LogMessageFatal()
    @     0x7fb0d33cf46c  caffe::Net<>::Init()
    @     0x7fb0d33d0b7e  caffe::Net<>::Net()
    @     0x7fb0d352f22a  caffe::Solver<>::InitTrainNet()
    @     0x7fb0d35306f5  caffe::Solver<>::Init()
    @     0x7fb0d3530a0f  caffe::Solver<>::Solver()
    @     0x7fb0d33a1b21  caffe::Creator_AdamSolver<>()
    @           0x40a778  train()
    @           0x407568  main
    @     0x7fb0d1522830  __libc_start_main
    @           0x407e39  _start
    @              (nil)  (unknown)
train_resnet_50by2_pool_tvl1.sh: line 8:  6840 Aborted                 (core dumped) $TOOLS/caffe train --solver=solver_resnet50by2_pooladam_tvl1.prototxt --gpu=0,1,2,3 --log_dir=logs/
------
Is there anything I missed? Thanks.",0,Unknown layer type: Warping,"Unknown layer type: Warping ### Issue summary
I got the latest version caffe and set WITH_PYTHON_LAYER := 1 in makefile.config. However, I still get the following error:

------
F0325 16:49:50.756420  6840 layer_factory.hpp:81] Check failed: registry.count(type) == 1 (0 vs. 1) Unknown layer type: Warping (known types: AbsVal, Accuracy, ArgMax, BNLL, BatchNorm, BatchReindex, Bias, Clip, Concat, ContrastiveLoss, Convolution, Crop, Data, Deconvolution, Dropout, DummyData, ELU, Eltwise, Embed, EuclideanLoss, Exp, Filter, Flatten, HDF5Data, HDF5Output, HingeLoss, Im2col, ImageData, InfogainLoss, InnerProduct, Input, LRN, LSTM, LSTMUnit, Log, MVN, MemoryData, MultinomialLogisticLoss, PReLU, Parameter, Pooling, Power, Python, RNN, ReLU, Reduction, Reshape, SPP, Scale, Sigmoid, SigmoidCrossEntropyLoss, Silence, Slice, Softmax, SoftmaxWithLoss, Split, Swish, TanH, Threshold, Tile, WindowData)
*** Check failure stack trace: ***
    @     0x7fb0d2d7f5cd  google::LogMessage::Fail()
    @     0x7fb0d2d81433  google::LogMessage::SendToLog()
    @     0x7fb0d2d7f15b  google::LogMessage::Flush()
    @     0x7fb0d2d81e1e  google::LogMessageFatal::~LogMessageFatal()
    @     0x7fb0d33cf46c  caffe::Net<>::Init()
    @     0x7fb0d33d0b7e  caffe::Net<>::Net()
    @     0x7fb0d352f22a  caffe::Solver<>::InitTrainNet()
    @     0x7fb0d35306f5  caffe::Solver<>::Init()
    @     0x7fb0d3530a0f  caffe::Solver<>::Solver()
    @     0x7fb0d33a1b21  caffe::Creator_AdamSolver<>()
    @           0x40a778  train()
    @           0x407568  main
    @     0x7fb0d1522830  __libc_start_main
    @           0x407e39  _start
    @              (nil)  (unknown)
train_resnet_50by2_pool_tvl1.sh: line 8:  6840 Aborted                 (core dumped) $TOOLS/caffe train --solver=solver_resnet50by2_pooladam_tvl1.prototxt --gpu=0,1,2,3 --log_dir=logs/
------
Is there anything I missed? Thanks."
caffe,2495,"When building applications around caffe, it's critical to know whether caffe was built with CUDA or in CPU-only mode. It's not acceptable to just try running it with the  flag and detecting whether an error was thrown or not:

> Cannot use GPU in CPU-only Caffe: check mode

Can we add a command like  or  to actually be able to check the mode, as suggested? I'd like to know about:
- CUDA
- cuDNN
- OpenCL (#2195)

---

~~Relatedly, can someone explain to me (or point me to an explanation) why you can't run caffe on the CPU if it was built with CUDA? It seems to me that I should be able to build caffe with CUDA support, and still choose later whether I want to use it or not. More generally,~~

I'd like to be able to build caffe on one machine with support for CUDA, cuDNN and OpenCL, then distribute that binary to another system that may or may not have them installed. Then there would be three states for each of the acceleration libraries:


",0,Provide a summary of available backends/engines/devices,"Provide a summary of available backends/engines/devices When building applications around caffe, it's critical to know whether caffe was built with CUDA or in CPU-only mode. It's not acceptable to just try running it with the  flag and detecting whether an error was thrown or not:

> Cannot use GPU in CPU-only Caffe: check mode

Can we add a command like  or  to actually be able to check the mode, as suggested? I'd like to know about:
- CUDA
- cuDNN
- OpenCL (#2195)

---

~~Relatedly, can someone explain to me (or point me to an explanation) why you can't run caffe on the CPU if it was built with CUDA? It seems to me that I should be able to build caffe with CUDA support, and still choose later whether I want to use it or not. More generally,~~

I'd like to be able to build caffe on one machine with support for CUDA, cuDNN and OpenCL, then distribute that binary to another system that may or may not have them installed. Then there would be three states for each of the acceleration libraries:


"
caffe,5632,"
### Issue summary
I was trying to run a FCN for Semantic Segmentation on caffe, but then i encounter with this problem. Could anyone know how to solve it ?

F0519 13:47:44.378706 22962 math_functions.cu:79] Check failed: error == cudaSuccess (4 vs. 0)  unspecified launch failure
*** Check failure stack trace: ***
Aborted (core dumped)

I was running this with GPU TiTAN X with CUDA 8.0 on Ubuntu 16.04.
",0,cuda error when running caffe ,"cuda error when running caffe  
### Issue summary
I was trying to run a FCN for Semantic Segmentation on caffe, but then i encounter with this problem. Could anyone know how to solve it ?

F0519 13:47:44.378706 22962 math_functions.cu:79] Check failed: error == cudaSuccess (4 vs. 0)  unspecified launch failure
*** Check failure stack trace: ***
Aborted (core dumped)

I was running this with GPU TiTAN X with CUDA 8.0 on Ubuntu 16.04.
"
caffe,6535,"## Important - read before submitting

*Please read the [guidelines for contributing](https://github.com/BVLC/caffe/blob/master/CONTRIBUTING.md) before submitting this issue!*

*Please do not post installation, build, usage, or modeling questions, or other requests for help to Issues.*
Use the [caffe-users list](https://groups.google.com/forum/#!forum/caffe-users) instead.
This helps developers maintain a clear, uncluttered, and efficient view of the state of Caffe.

### Issue summary


### Steps to reproduce


### Tried solutions


### System configuration

* Operating system: 
* Compiler: 
* CUDA version (if applicable): 
* CUDNN version (if applicable): 
* BLAS: 
* Python version (if using pycaffe): 
* MATLAB version (if using matcaffe): 

### Issue checklist

- [ ] read the guidelines and removed the first paragraph
- [ ] written a short summary and detailed steps to reproduce
- [ ] explained how solutions to related problems failed (tick if found none)
- [ ] filled system configuration
- [ ] attached relevant logs/config files (tick if not applicable)
",0,PoolingParameter_RoundMode does not name a type,"PoolingParameter_RoundMode does not name a type ## Important - read before submitting

*Please read the [guidelines for contributing](https://github.com/BVLC/caffe/blob/master/CONTRIBUTING.md) before submitting this issue!*

*Please do not post installation, build, usage, or modeling questions, or other requests for help to Issues.*
Use the [caffe-users list](https://groups.google.com/forum/#!forum/caffe-users) instead.
This helps developers maintain a clear, uncluttered, and efficient view of the state of Caffe.

### Issue summary


### Steps to reproduce


### Tried solutions


### System configuration

* Operating system: 
* Compiler: 
* CUDA version (if applicable): 
* CUDNN version (if applicable): 
* BLAS: 
* Python version (if using pycaffe): 
* MATLAB version (if using matcaffe): 

### Issue checklist

- [ ] read the guidelines and removed the first paragraph
- [ ] written a short summary and detailed steps to reproduce
- [ ] explained how solutions to related problems failed (tick if found none)
- [ ] filled system configuration
- [ ] attached relevant logs/config files (tick if not applicable)
"
caffe,3420,"Say the input shape is 10 \* 192 \* 32 \* 32.
Then:



There isn't bottom[0]->offset(11) .

The last one in a batch is in offset(9) because we start at zero. So,  I think there shouldn't be offset(10). 
If we read bottom[0]->cpu_data()[1966080], the code will exit without any error prompt.

In the file ,  offset() is defined as:



is   better?  I'm not sure if what I've found is a problem.
",0,Is this a BUG in blob->offset(n) ?,"Is this a BUG in blob->offset(n) ? Say the input shape is 10 \* 192 \* 32 \* 32.
Then:



There isn't bottom[0]->offset(11) .

The last one in a batch is in offset(9) because we start at zero. So,  I think there shouldn't be offset(10). 
If we read bottom[0]->cpu_data()[1966080], the code will exit without any error prompt.

In the file ,  offset() is defined as:



is   better?  I'm not sure if what I've found is a problem.
"
caffe,6654,"C:\Program Files (x86)\MSBuild\Microsoft.Cpp\v4.0\V140\Microsoft.CppCommon.targets(171,5): error MSB6006: cmd.exe 
 1 [D:\projects\caffe\build\src\caffe\caffe.vcxproj]
D:\projects\caffe\build\src\caffe\caffe.vcxproj() - 

D:\projects\caffe\build\ALL_BUILD.vcxproj() - 




D:\projects\caffe\build\ALL_BUILD.vcxproj() (1) ->
D:\projects\caffe\build\src\caffe\caffe.vcxproj() (3) ->
(CustomBuild ) ->
  C:\Program Files (x86)\MSBuild\Microsoft.Cpp\v4.0\V140\Microsoft.CppCommon.targets(171,5): error MSB6006: cmd.exe 
 1 [D:\projects\caffe\build\src\caffe\caffe.vcxproj]

    0 
    1 
",0,"when i order build_win.cmd in cmd, there is an error MSB6006","when i order build_win.cmd in cmd, there is an error MSB6006 C:\Program Files (x86)\MSBuild\Microsoft.Cpp\v4.0\V140\Microsoft.CppCommon.targets(171,5): error MSB6006: cmd.exe 
 1 [D:\projects\caffe\build\src\caffe\caffe.vcxproj]
D:\projects\caffe\build\src\caffe\caffe.vcxproj() - 

D:\projects\caffe\build\ALL_BUILD.vcxproj() - 




D:\projects\caffe\build\ALL_BUILD.vcxproj() (1) ->
D:\projects\caffe\build\src\caffe\caffe.vcxproj() (3) ->
(CustomBuild ) ->
  C:\Program Files (x86)\MSBuild\Microsoft.Cpp\v4.0\V140\Microsoft.CppCommon.targets(171,5): error MSB6006: cmd.exe 
 1 [D:\projects\caffe\build\src\caffe\caffe.vcxproj]

    0 
    1 
"
caffe,6534,"Sorry, I can't log in google, But the question has always disturb me. So I come here for help!
When I compile caffe on my centos6.5 machine, I got the following error, I don't know what's the meaning by such info, please help me, thanks!

the error info is:
CXX/LD -o .build_release/examples/siamese/convert_mnist_siamese_data.bin
/usr/local/bin/ld: .build_release/examples/mnist/convert_mnist_data.o: undefined reference to symbol 'strlen@@GLIBC_2.2.5'
/lib64/libc.so.6: error adding symbols: DSO missing from command line",0,compile caffe on centos6.5 got DSO error,"compile caffe on centos6.5 got DSO error Sorry, I can't log in google, But the question has always disturb me. So I come here for help!
When I compile caffe on my centos6.5 machine, I got the following error, I don't know what's the meaning by such info, please help me, thanks!

the error info is:
CXX/LD -o .build_release/examples/siamese/convert_mnist_siamese_data.bin
/usr/local/bin/ld: .build_release/examples/mnist/convert_mnist_data.o: undefined reference to symbol 'strlen@@GLIBC_2.2.5'
/lib64/libc.so.6: error adding symbols: DSO missing from command line"
caffe,2015,"For instance, MVNLayer reads data from its top blob during the backward pass, under the assumption that this data is exactly the same as the output it created.  If it's been modified by a later layer that does in-place computation, the gradient will be computed incorrectly.

In general, caffe should not rely on the user to know under what circumstances a layer can safely be done in-place.  
",0,In-place computation can break gradient computation,"In-place computation can break gradient computation For instance, MVNLayer reads data from its top blob during the backward pass, under the assumption that this data is exactly the same as the output it created.  If it's been modified by a later layer that does in-place computation, the gradient will be computed incorrectly.

In general, caffe should not rely on the user to know under what circumstances a layer can safely be done in-place.  
"
caffe,1918,"It currently doesn't seem to be possible to define layers with  and  of different data types. I noticed @jeffdonahue's  (#1872) which works around this by casting floating point inputs to . Specifically, I was hoping to implement a layer that encodes DNA sequence in 1-of-n encoding using  as  and  as . This doesn't seem possible with the current architecture so I'll probably have to work around it by pre-encoding all my data. However, maybe it will be possible to make the architecture flexible enough to allow this sort of thing.
",0,Layers with heterogeneous data types,"Layers with heterogeneous data types It currently doesn't seem to be possible to define layers with  and  of different data types. I noticed @jeffdonahue's  (#1872) which works around this by casting floating point inputs to . Specifically, I was hoping to implement a layer that encodes DNA sequence in 1-of-n encoding using  as  and  as . This doesn't seem possible with the current architecture so I'll probably have to work around it by pre-encoding all my data. However, maybe it will be possible to make the architecture flexible enough to allow this sort of thing.
"
caffe,4288,"On a current build of Caffe against Cuda 7.5 and CuDNN 4 on Visual Studio 2013, all tests pass except those for the timer class - where 4 tests of  test_benchmark.cpp fail (more details including output can be found in closed issue BenchmarkTest fails #4280). Inspection shows that the tests use  
which has been deprecated since 1.53 and withdrawn after 1.56. 
a suitable replacement could be:


However making such a replacement does not in itself fix the errors completely. Is the underlying timer used in the timer class updated when the thread is asleep? The timer reporting the times of the tests seems accurate!
",0,test_benchmark.cpp failed and uses boost::this_thread::sleep which was deprecated in 1.53.0 for removal in 1.56.0,"test_benchmark.cpp failed and uses boost::this_thread::sleep which was deprecated in 1.53.0 for removal in 1.56.0 On a current build of Caffe against Cuda 7.5 and CuDNN 4 on Visual Studio 2013, all tests pass except those for the timer class - where 4 tests of  test_benchmark.cpp fail (more details including output can be found in closed issue BenchmarkTest fails #4280). Inspection shows that the tests use  
which has been deprecated since 1.53 and withdrawn after 1.56. 
a suitable replacement could be:


However making such a replacement does not in itself fix the errors completely. Is the underlying timer used in the timer class updated when the thread is asleep? The timer reporting the times of the tests seems accurate!
"
caffe,6686,"### Issue summary
I passed make -j8 all and make -j8 test but make -j8 runtest gives me the following error : 

### Tested solutions 

Tried adding -G to NVCC in Debugging section of  Makefile then Make clean Make -j8 all make -j8 test make -j8 runtest. Get the same error.

### System configuration
Python 3.6
* Operating system: Ubuntu 18.04
* Compiler: gcc 7.3
* CUDA version (if applicable): 9.1
* CUDNN version (if applicable): 7.1
* BLAS: atlas
* Python version (if using pycaffe): 3.6
* MATLAB version (if using matcaffe): 

",0,Make runtest fails ubuntu 18.04 cuda 9.1 ,"Make runtest fails ubuntu 18.04 cuda 9.1  ### Issue summary
I passed make -j8 all and make -j8 test but make -j8 runtest gives me the following error : 

### Tested solutions 

Tried adding -G to NVCC in Debugging section of  Makefile then Make clean Make -j8 all make -j8 test make -j8 runtest. Get the same error.

### System configuration
Python 3.6
* Operating system: Ubuntu 18.04
* Compiler: gcc 7.3
* CUDA version (if applicable): 9.1
* CUDNN version (if applicable): 7.1
* BLAS: atlas
* Python version (if using pycaffe): 3.6
* MATLAB version (if using matcaffe): 

"
caffe,6175,"Hi,

Few days ago I found myself on a quest to get a working minimal caffe build on windows/msys2. I thought it was going to be easy (haha). It ended up a lot more hellish than I thought. Long story short, there are some broken packages out there and a few hard-to-dig answers on stackoverflow, and it can take a really long time for a newcomer to put it all together, especially when you don't know much about msys.

Somehow managed to work through the issues as far as my requirements are concerned and came up with this build: [mingw-caffe](https://github.com/lemonsqueeze/mingw-caffe).

I don't want to go through this again, and really don't wish it to anyone else either.
I understand msys is not in the officially supported platforms and that's perfectly fine, but a version of me from the past and i'm sure many others in a similar situation would appreciate a pointer in the [installation](http://caffe.berkeleyvision.org/installation.html) section (also right now there's only one entry for Windows and it points to the windows branch, which is deliciously confusing).

I'm sure this can be improved, but it's much better than nothing.
If someone tells me this has already been done i kill him.",0,Some installation instructions for msys2,"Some installation instructions for msys2 Hi,

Few days ago I found myself on a quest to get a working minimal caffe build on windows/msys2. I thought it was going to be easy (haha). It ended up a lot more hellish than I thought. Long story short, there are some broken packages out there and a few hard-to-dig answers on stackoverflow, and it can take a really long time for a newcomer to put it all together, especially when you don't know much about msys.

Somehow managed to work through the issues as far as my requirements are concerned and came up with this build: [mingw-caffe](https://github.com/lemonsqueeze/mingw-caffe).

I don't want to go through this again, and really don't wish it to anyone else either.
I understand msys is not in the officially supported platforms and that's perfectly fine, but a version of me from the past and i'm sure many others in a similar situation would appreciate a pointer in the [installation](http://caffe.berkeleyvision.org/installation.html) section (also right now there's only one entry for Windows and it points to the windows branch, which is deliciously confusing).

I'm sure this can be improved, but it's much better than nothing.
If someone tells me this has already been done i kill him."
caffe,6745,"I used the command ""sudo apt install caffe-cuda"" to complete the installation of Caffe on Ubuntu 18.04, which can be used directly. Now I need to modify the source code or add a layer. But I can only find the usr/bin/caffe shared library through ""which caffe, where is caffe"", and I can't find the source code. I can't thank you enough for your advice.ubuntu18.04sudo apt install caffe-cudacaffewhich caffewhereis caffeusr/bin/caffe",0,Source file directory not found--,"Source file directory not found-- I used the command ""sudo apt install caffe-cuda"" to complete the installation of Caffe on Ubuntu 18.04, which can be used directly. Now I need to modify the source code or add a layer. But I can only find the usr/bin/caffe shared library through ""which caffe, where is caffe"", and I can't find the source code. I can't thank you enough for your advice.ubuntu18.04sudo apt install caffe-cudacaffewhich caffewhereis caffeusr/bin/caffe"
caffe,6724,"I make caffe with cmake. However I can't find _caffe.so  in my python directory.So when I import caffe, I get the error:_No module named _caffe.so. How can I fix it?",0,_No module named _caffe.so,"_No module named _caffe.so I make caffe with cmake. However I can't find _caffe.so  in my python directory.So when I import caffe, I get the error:_No module named _caffe.so. How can I fix it?"
caffe,6881,"@csukuangfj  @Noiredd
### Issue summary
A bug in pooling_layer.cpp?
### **original code:**
  if (pad_h_ || pad_w_) {
    // If we have padding, ensure that the last pooling starts strictly
    // inside the image (instead of at the padding); otherwise clip the last.`
    if ((pooled_height_ - 1) * stride_h_ >= height_ + pad_h_) {
      --pooled_height_;
    }
    if ((pooled_width_ - 1) * stride_w_ >= width_ + pad_w_) {
      --pooled_width_;
    }
    CHECK_LT((pooled_height_ - 1) * stride_h_, height_ + pad_h_);
    CHECK_LT((pooled_width_ - 1) * stride_w_, width_ + pad_w_);
  }
    **I think that if (pad_h_ || pad_w_)  should be removed**

   For example
       Input feature map = hi*wi= 6*6kernel=1stride=2pad=0round_mode=CEIL;
       ho=ceil_div((6+2*0-1) ,2)+1=4
       (ho-1)*stride=(4-1)*2=6=hi+pad=6+0
   In this case, the 4th output point is out side of the input map, ho/wo must be decreased by 1!

   So, if (pad_h_ || pad_w_)  should be removed

",0,A bug in pooling_layer.cpp?,"A bug in pooling_layer.cpp? @csukuangfj  @Noiredd
### Issue summary
A bug in pooling_layer.cpp?
### **original code:**
  if (pad_h_ || pad_w_) {
    // If we have padding, ensure that the last pooling starts strictly
    // inside the image (instead of at the padding); otherwise clip the last.`
    if ((pooled_height_ - 1) * stride_h_ >= height_ + pad_h_) {
      --pooled_height_;
    }
    if ((pooled_width_ - 1) * stride_w_ >= width_ + pad_w_) {
      --pooled_width_;
    }
    CHECK_LT((pooled_height_ - 1) * stride_h_, height_ + pad_h_);
    CHECK_LT((pooled_width_ - 1) * stride_w_, width_ + pad_w_);
  }
    **I think that if (pad_h_ || pad_w_)  should be removed**

   For example
       Input feature map = hi*wi= 6*6kernel=1stride=2pad=0round_mode=CEIL;
       ho=ceil_div((6+2*0-1) ,2)+1=4
       (ho-1)*stride=(4-1)*2=6=hi+pad=6+0
   In this case, the 4th output point is out side of the input map, ho/wo must be decreased by 1!

   So, if (pad_h_ || pad_w_)  should be removed

"
caffe,6691,"hello
I've installed caffe on windows 
I followed up instructions in [README.md](https://github.com/BVLC/caffe/tree/windows) and there was no error in running build-win.cmd

in demo.py  in [here](https://github.com/suyogduttjain/pixelobjectness) I have to mention the path of caffe.bin in caffe_binary = './deeplab-public/distribute/bin/caffe.bin'  but I don't have such file neither in that path nor any other paths where caffe is installed.

I appreciate any help.",0,there is no caffe.bin,"there is no caffe.bin hello
I've installed caffe on windows 
I followed up instructions in [README.md](https://github.com/BVLC/caffe/tree/windows) and there was no error in running build-win.cmd

in demo.py  in [here](https://github.com/suyogduttjain/pixelobjectness) I have to mention the path of caffe.bin in caffe_binary = './deeplab-public/distribute/bin/caffe.bin'  but I don't have such file neither in that path nor any other paths where caffe is installed.

I appreciate any help."
caffe,4607,"I get thrown out of Jupyter after importing any model, even after calling set_mode_cpu(), because caffe fails to find any CUDA device (which is shouldn't).

here's my input:



This results in the following error message:



The kernel is not dead yet, now I try this:



This is the response I get:



followed by the text in 
followed by:



I'm running on a laptop with no GPU, so it makes sense it doesn't find any CUDA-capable device, but why is it ignoring my request to use the CPU?

My docker has CUDA drivers installed on it (for when I do want to run in GPU mode) - is that a problem?
Here's my Dockerfile:

[Dockerfile.txt](https://github.com/BVLC/caffe/files/426713/Dockerfile.txt)
",0,"loading any network results in Jupyter kernel crash because of lack of CUDA device, set_mode_cpu fails as well.","loading any network results in Jupyter kernel crash because of lack of CUDA device, set_mode_cpu fails as well. I get thrown out of Jupyter after importing any model, even after calling set_mode_cpu(), because caffe fails to find any CUDA device (which is shouldn't).

here's my input:



This results in the following error message:



The kernel is not dead yet, now I try this:



This is the response I get:



followed by the text in 
followed by:



I'm running on a laptop with no GPU, so it makes sense it doesn't find any CUDA-capable device, but why is it ignoring my request to use the CPU?

My docker has CUDA drivers installed on it (for when I do want to run in GPU mode) - is that a problem?
Here's my Dockerfile:

[Dockerfile.txt](https://github.com/BVLC/caffe/files/426713/Dockerfile.txt)
"
caffe,5695,"sometimes, we only need to detect objects inside 1-2 ROIs in the whole image, but we have to do the conv/pool etc on the whole image, for example, if we use vgg on a HD image, the performance is really bad. my question is, do you have plan to add ROI settings for Conv settings(list of rectangles)?",0,could caffe add ROI settings for convolution?,"could caffe add ROI settings for convolution? sometimes, we only need to detect objects inside 1-2 ROIs in the whole image, but we have to do the conv/pool etc on the whole image, for example, if we use vgg on a HD image, the performance is really bad. my question is, do you have plan to add ROI settings for Conv settings(list of rectangles)?"
caffe,4706,"Sry if this is something I missed to read in the documentation but I'm facing this issue. Any help is really appreciated. Thanks in advance.

I followed the installation and complication steps. 

ran make test with following pass results: 



Verified all followed exist as mentioned in the web demo setup steps but start web app failed with import caffe error:


",0,Facing issue with bringing up the flash server (web demo),"Facing issue with bringing up the flash server (web demo) Sry if this is something I missed to read in the documentation but I'm facing this issue. Any help is really appreciated. Thanks in advance.

I followed the installation and complication steps. 

ran make test with following pass results: 



Verified all followed exist as mentioned in the web demo setup steps but start web app failed with import caffe error:


"
caffe,6816,"## Important - read before submitting

*Please read the [guidelines for contributing](https://github.com/BVLC/caffe/blob/master/CONTRIBUTING.md) before submitting this issue!*

*Please do not post installation, build, usage, or modeling questions, or other requests for help to Issues.*
Use the [caffe-users list](https://groups.google.com/forum/#!forum/caffe-users) instead.
This helps developers maintain a clear, uncluttered, and efficient view of the state of Caffe.

### Issue summary



### Steps to reproduce


### Tried solutions


### System configuration

* Operating system: 
* Compiler: 
* CUDA version (if applicable): 
* CUDNN version (if applicable): 
* BLAS: 
* Python version (if using pycaffe): 
* MATLAB version (if using matcaffe): 

### Issue checklist

- [ ] read the guidelines and removed the first paragraph
- [ ] written a short summary and detailed steps to reproduce
- [ ] explained how solutions to related problems failed (tick if found none)
- [ ] filled system configuration
- [ ] attached relevant logs/config files (tick if not applicable)

Hi 

Is this repo giving face recognition on images or terminal. Like in Davidberg Repo, Face recognition is done on terminal rather than on images. I want on images. will it do? If yes, I will pursue.

Thanks
",0,Recognition on Image itself,"Recognition on Image itself ## Important - read before submitting

*Please read the [guidelines for contributing](https://github.com/BVLC/caffe/blob/master/CONTRIBUTING.md) before submitting this issue!*

*Please do not post installation, build, usage, or modeling questions, or other requests for help to Issues.*
Use the [caffe-users list](https://groups.google.com/forum/#!forum/caffe-users) instead.
This helps developers maintain a clear, uncluttered, and efficient view of the state of Caffe.

### Issue summary



### Steps to reproduce


### Tried solutions


### System configuration

* Operating system: 
* Compiler: 
* CUDA version (if applicable): 
* CUDNN version (if applicable): 
* BLAS: 
* Python version (if using pycaffe): 
* MATLAB version (if using matcaffe): 

### Issue checklist

- [ ] read the guidelines and removed the first paragraph
- [ ] written a short summary and detailed steps to reproduce
- [ ] explained how solutions to related problems failed (tick if found none)
- [ ] filled system configuration
- [ ] attached relevant logs/config files (tick if not applicable)

Hi 

Is this repo giving face recognition on images or terminal. Like in Davidberg Repo, Face recognition is done on terminal rather than on images. I want on images. will it do? If yes, I will pursue.

Thanks
"
caffe,6375,"### Issue summary
I installed caffe on ubuntu16.04 with cuda8.0 and cudnn5.0. Installation is very normal, and when using  single GPU caffe can run very smoothly, but when i use the command -gpu all to use multiple GPU, the terminal just keep unchanged as you can seehowever it doesn't report a mistake. I have to use 'sudo kill -9 PID' to stop the process. what's the problem? anyone can help me?

### Steps to reproduce
![image](https://user-images.githubusercontent.com/35474700/39393837-9b94f9ea-4afc-11e8-99ef-cbfe08844fe8.png)
![image](https://user-images.githubusercontent.com/35474700/39393848-bcf8a26c-4afc-11e8-93d8-773671ebb935.png)


### Tried solutions
reinstall NCCL and recompile caffe but it doesn't help 

### System configuration

* Operating system: ubuntu
* Compiler: gcc 5.4.0
* CUDA version (if applicable): cuda_8.0.44_linux
* CUDNN version (if applicable): cudnn-8.0-linux-x64-v5.1
* BLAS: OpenBLAS-0.2.20
* Python version (if using pycaffe): python2.7
* MATLAB version (if using matcaffe): 

### Issue checklist
",0,problem encountered when train with multiple GPUs in caffe,"problem encountered when train with multiple GPUs in caffe ### Issue summary
I installed caffe on ubuntu16.04 with cuda8.0 and cudnn5.0. Installation is very normal, and when using  single GPU caffe can run very smoothly, but when i use the command -gpu all to use multiple GPU, the terminal just keep unchanged as you can seehowever it doesn't report a mistake. I have to use 'sudo kill -9 PID' to stop the process. what's the problem? anyone can help me?

### Steps to reproduce
![image](https://user-images.githubusercontent.com/35474700/39393837-9b94f9ea-4afc-11e8-99ef-cbfe08844fe8.png)
![image](https://user-images.githubusercontent.com/35474700/39393848-bcf8a26c-4afc-11e8-93d8-773671ebb935.png)


### Tried solutions
reinstall NCCL and recompile caffe but it doesn't help 

### System configuration

* Operating system: ubuntu
* Compiler: gcc 5.4.0
* CUDA version (if applicable): cuda_8.0.44_linux
* CUDNN version (if applicable): cudnn-8.0-linux-x64-v5.1
* BLAS: OpenBLAS-0.2.20
* Python version (if using pycaffe): python2.7
* MATLAB version (if using matcaffe): 

### Issue checklist
"
caffe,6377,"I try to train a ResNet to do image classification on multiple GPU card. But I encountered error at the training/test beginning:

My environment is Tesla M40 x 4, Ubuntu 16.04, CUDA 9.1, CUDNN v7.1, NCCL 2.1.15.
And I test many times this network can be successfully trained on one GPU card.",0,Caffe+NCCL : Check failed: result == ncclSuccess (13 vs. 0) invalid data type,"Caffe+NCCL : Check failed: result == ncclSuccess (13 vs. 0) invalid data type I try to train a ResNet to do image classification on multiple GPU card. But I encountered error at the training/test beginning:

My environment is Tesla M40 x 4, Ubuntu 16.04, CUDA 9.1, CUDNN v7.1, NCCL 2.1.15.
And I test many times this network can be successfully trained on one GPU card."
caffe,6687,"When i train my image use this Code,there is a problem about convergence: Training results shows that the 'total loss' is fluctuate around 2 ,and did not decrease anymore; Second, the 'loss_box' always decay to zero and it will rise a little only; Third, the training is always cut off, no reason; Fourth, the candidate boxes are generally small than the fact; Fifth, when i run the demo there are no object has been detected. So, what happen to my project, please reply to me as soon as possible,thank U!",0,There is a problem about convergence of the networks,"There is a problem about convergence of the networks When i train my image use this Code,there is a problem about convergence: Training results shows that the 'total loss' is fluctuate around 2 ,and did not decrease anymore; Second, the 'loss_box' always decay to zero and it will rise a little only; Third, the training is always cut off, no reason; Fourth, the candidate boxes are generally small than the fact; Fifth, when i run the demo there are no object has been detected. So, what happen to my project, please reply to me as soon as possible,thank U!"
caffe,4772,"I'm not sure what might be causing this, but here's what I'm seeing when I run  on a checkout of master. I'm running on Debian Jessie, with GCC 4.9 and CUDA 8 RC. The only interesting thing about this machine is that it has 4x GTX 1080.



EDIT: I'm compiling with CuDNN enabled, but turning it off doesn't seem to make a difference.
",0,make runtest segfaulting,"make runtest segfaulting I'm not sure what might be causing this, but here's what I'm seeing when I run  on a checkout of master. I'm running on Debian Jessie, with GCC 4.9 and CUDA 8 RC. The only interesting thing about this machine is that it has 4x GTX 1080.



EDIT: I'm compiling with CuDNN enabled, but turning it off doesn't seem to make a difference.
"
caffe,6822,"So, I want to understand is caffe losing the detail of my high-resolution image while resizing to 300x300?

There could be two cases while resizing the image to 300x300 my 1200x990 image:
1) Cropping
2) Squishing 

In the first case i.e. cropping it's loosing details of my actual labelled image

In the second case i.e. Squishing my original high-resolution image is squished to a small size which also means it's a waste to pass high-resolution image

Now, I saw the source and ***train.prototxt***

    transform_param {
        resize_param {
              resize_mode: WARP
              height: 300
              width: 300
        }
    }

then I saw CPP code for wrap & found

Link to cpp code [here][1]



and the actual logic



now I understand Cplusplus but what I don't understand is what is bbox? and what's the logic for WRAP mode?

Can someone please explain to me what's happening with my 1200x900 image. Thanks




  [1]: https://github.com/intel/caffe/blob/master/src/caffe/util/im_transforms.cpp",0,How is caffe resizing my images for training in transform_param,"How is caffe resizing my images for training in transform_param So, I want to understand is caffe losing the detail of my high-resolution image while resizing to 300x300?

There could be two cases while resizing the image to 300x300 my 1200x990 image:
1) Cropping
2) Squishing 

In the first case i.e. cropping it's loosing details of my actual labelled image

In the second case i.e. Squishing my original high-resolution image is squished to a small size which also means it's a waste to pass high-resolution image

Now, I saw the source and ***train.prototxt***

    transform_param {
        resize_param {
              resize_mode: WARP
              height: 300
              width: 300
        }
    }

then I saw CPP code for wrap & found

Link to cpp code [here][1]



and the actual logic



now I understand Cplusplus but what I don't understand is what is bbox? and what's the logic for WRAP mode?

Can someone please explain to me what's happening with my 1200x900 image. Thanks




  [1]: https://github.com/intel/caffe/blob/master/src/caffe/util/im_transforms.cpp"
caffe,4255,"I have a forloop in which I load different data into the layer and runs forward. It crashes my Nvidia driver. I found that this net can only do a forward once, and on the second  it crashes, giving unspecified launch failure. But if I load the net again in the forloop, it works fine.

I did not have this problem before, not sure why this shows up now. I tried different versions of Nvidia driver and it's the same.


",0,Cannot do a second net.forward() with Input layer,"Cannot do a second net.forward() with Input layer I have a forloop in which I load different data into the layer and runs forward. It crashes my Nvidia driver. I found that this net can only do a forward once, and on the second  it crashes, giving unspecified launch failure. But if I load the net again in the forloop, it works fine.

I did not have this problem before, not sure why this shows up now. I tried different versions of Nvidia driver and it's the same.


"
caffe,6882,"Unsupported gpu architecture 'compute_481'
py3.5
gtx1660ti
cuda10.0",0,Unsupported gpu architecture 'compute_481',"Unsupported gpu architecture 'compute_481' Unsupported gpu architecture 'compute_481'
py3.5
gtx1660ti
cuda10.0"
caffe,6642,"


*** Error in `.build_release/tools/caffe': free(): invalid pointer: 0x0000000001f342c0 ***
======= Backtrace: =========
/lib/x86_64-linux-gnu/libc.so.6(+0x777e5)[0x7f6d7e4387e5]
/lib/x86_64-linux-gnu/libc.so.6(+0x8037a)[0x7f6d7e44137a]
/lib/x86_64-linux-gnu/libc.so.6(cfree+0x4c)[0x7f6d7e44553c]
/usr/lib/x86_64-linux-gnu/libprotobuf.so.9(_ZN6google8protobuf8internal28DestroyDefaultRepeatedFieldsEv+0x1f)[0x7f6d7f74f8af]
/usr/lib/x86_64-linux-gnu/libprotobuf.so.9(_ZN6google8protobuf23ShutdownProtobufLibraryEv+0x8b)[0x7f6d7f74eb3b]
/usr/lib/x86_64-linux-gnu/libmirprotobuf.so.3(+0x233b9)[0x7f6d4bb8d3b9]
/lib64/ld-linux-x86-64.so.2(+0x10de7)[0x7f6d80ef3de7]
/lib/x86_64-linux-gnu/libc.so.6(+0x39ff8)[0x7f6d7e3faff8]
/lib/x86_64-linux-gnu/libc.so.6(+0x3a045)[0x7f6d7e3fb045]
/lib/x86_64-linux-gnu/libc.so.6(__libc_start_main+0xf7)[0x7f6d7e3e1837]
.build_release/tools/caffe[0x4080b9]
======= Memory map: ========
00400000-00418000 r-xp 00000000 08:18 659239                             /home/csy/caffe/.build_release/tools/caffe.bin
00618000-00619000 r--p 00018000 08:18 659239                             /home/csy/caffe/.build_release/tools/caffe.bin
00619000-0061a000 rw-p 00019000 08:18 659239                             /home/csy/caffe/.build_release/tools/caffe.bin
01f22000-02191000 rw-p 00000000 00:00 0                                  [heap]
7f6d38000000-7f6d38021000 rw-p 00000000 00:00 0 
7f6d38021000-7f6d3c000000 ---p 00000000 00:00 0 
7f6d3fb87000-7f6d3fb88000 ---p 00000000 00:00 0 
7f6d3fb88000-7f6d40388000 rw-p 00000000 00:00 0 
7f6d46388000-7f6d46389000 ---p 00000000 00:00 0 
7f6d46389000-7f6d46b89000 rw-p 00000000 00:00 0 
7f6d46b89000-7f6d46b8a000 ---p 00000000 00:00 0 
7f6d46b8a000-7f6d4738a000 rw-p 00000000 00:00 0 
7f6d4738a000-7f6d4738b000 ---p 00000000 00:00 0 
7f6d4738b000-7f6d47b8b000 rw-p 00000000 00:00 0 
7f6d47b8b000-7f6d47b8e000 r-xp 00000000 08:16 1447018                    /lib/x86_64-linux-gnu/libkeyutils.so.1.5
7f6d47b8e000-7f6d47d8d000 ---p 00003000 08:16 1447018                    /lib/x86_64-linux-gnu/libkeyutils.so.1.5
7f6d47d8d000-7f6d47d8e000 r--p 00002000 08:16 1447018                    /lib/x86_64-linux-gnu/libkeyutils.so.1.5
7f6d47d8e000-7f6d47d8f000 rw-p 00003000 08:16 1447018                    /lib/x86_64-linux-gnu/libkeyutils.so.1.5
7f6d47d8f000-7f6d47d99000 r-xp 00000000 08:16 3417915                    /usr/lib/x86_64-linux-gnu/libkrb5support.so.0.1
7f6d47d99000-7f6d47f98000 ---p 0000a000 08:16 3417915                    /usr/lib/x86_64-linux-gnu/libkrb5support.so.0.1
7f6d47f98000-7f6d47f99000 r--p 00009000 08:16 3417915                    /usr/lib/x86_64-linux-gnu/libkrb5support.so.0.1
7f6d47f99000-7f6d47f9a000 rw-p 0000a000 08:16 3417915                    /usr/lib/x86_64-linux-gnu/libkrb5support.so.0.1
7f6d47f9a000-7f6d47f9d000 r-xp 00000000 08:16 1446966                    /lib/x86_64-linux-gnu/libcom_err.so.2.1
7f6d47f9d000-7f6d4819c000 ---p 00003000 08:16 1446966                    /lib/x86_64-linux-gnu/libcom_err.so.2.1
7f6d4819c000-7f6d4819d000 r--p 00002000 08:16 1446966                    /lib/x86_64-linux-gnu/libcom_err.so.2.1
7f6d4819d000-7f6d4819e000 rw-p 00003000 08:16 1446966                    /lib/x86_64-linux-gnu/libcom_err.so.2.1
7f6d4819e000-7f6d481ca000 r-xp 00000000 08:16 3417905                    /usr/lib/x86_64-linux-gnu/libk5crypto.so.3.1
7f6d481ca000-7f6d483c9000 ---p 0002c000 08:16 3417905                    /usr/lib/x86_64-linux-gnu/libk5crypto.so.3.1
7f6d483c9000-7f6d483cb000 r--p 0002b000 08:16 3417905                    /usr/lib/x86_64-linux-gnu/libk5crypto.so.3.1
7f6d483cb000-7f6d483cc000 rw-p 0002d000 08:16 3417905                    /usr/lib/x86_64-linux-gnu/libk5crypto.so.3.1
7f6d483cc000-7f6d483cd000 rw-p 00000000 00:00 0 
7f6d483cd000-7f6d48490000 r-xp 00000000 08:16 3417913                    /usr/lib/x86_64-linux-gnu/libkrb5.so.3.3
7f6d48490000-7f6d48690000 ---p 000c3000 08:16 3417913                    /usr/lib/x86_64-linux-gnu/libkrb5.so.3.3
7f6d48690000-7f6d4869d000 r--p 000c3000 08:16 3417913                    /usr/lib/x86_64-linux-gnu/libkrb5.so.3.3
7f6d4869d000-7f6d4869f000 rw-p 000d0000 08:16 3417913                    /usr/lib/x86_64-linux-gnu/libkrb5.so.3.3
7f6d4869f000-7f6d486b1000 r-xp 00000000 08:16 1447001                    /lib/x86_64-linux-gnu/libgpg-error.so.0.17.0
7f6d486b1000-7f6d488b1000 ---p 00012000 08:16 1447001                    /lib/x86_64-linux-gnu/libgpg-error.so.0.17.0
7f6d488b1000-7f6d488b2000 r--p 00012000 08:16 1447001                    /lib/x86_64-linux-gnu/libgpg-error.so.0.17.0
7f6d488b2000-7f6d488b3000 rw-p 00013000 08:16 1447001                    /lib/x86_64-linux-gnu/libgpg-error.so.0.17.0
7f6d488b3000-7f6d488d4000 r-xp 00000000 08:16 3417698                    /usr/lib/x86_64-linux-gnu/libgomp.so.1.0.0
7f6d488d4000-7f6d48ad3000 ---p 00021000 08:16 3417698                    /usr/lib/x86_64-linux-gnu/libgomp.so.1.0.0
7f6d48ad3000-7f6d48ad4000 r--p 00020000 08:16 3417698                    /usr/lib/x86_64-linux-gnu/libgomp.so.1.0.0
7f6d48ad4000-7f6d48ad5000 rw-p 00021000 08:16 3417698                    /usr/lib/x86_64-linux-gnu/libgomp.so.1.0.0
7f6d48ad5000-7f6d48afc000 r-xp 00000000 08:16 3408053                    /usr/lib/x86_64-linux-gnu/libkj-0.5.3.so
7f6d48afc000-7f6d48cfc000 ---p 00027000 08:16 3408053                    /usr/lib/x86_64-linux-gnu/libkj-0.5.3.so
7f6d48cfc000-7f6d48cfd000 r--p 00027000 08:16 3408053                    /usr/lib/x86_64-linux-gnu/libkj-0.5.3.so
7f6d48cfd000-7f6d48cfe000 rw-p 00028000 08:16 3408053                    /usr/lib/x86_64-linux-gnu/libkj-0.5.3.so
7f6d48cfe000-7f6d48d04000 r-xp 00000000 08:16 3417408                    /usr/lib/x86_64-linux-gnu/libdatrie.so.1.3.3
7f6d48d04000-7f6d48f04000 ---p 00006000 08:16 3417408                    /usr/lib/x86_64-linux-gnu/libdatrie.so.1.3.3
7f6d48f04000-7f6d48f05000 r--p 00006000 08:16 3417408                    /usr/lib/x86_64-linux-gnu/libdatrie.so.1.3.3
7f6d48f05000-7f6d48f06000 rw-p 00007000 08:16 3417408                    /usr/lib/x86_64-linux-gnu/libdatrie.so.1.3.3
7f6d48f06000-7f6d48f2a000 r-xp 00000000 08:16 3417714                    /usr/lib/x86_64-linux-gnu/libgraphite2.so.3.0.1
7f6d48f2a000-7f6d49129000 ---p 00024000 08:16 3417714                    /usr/lib/x86_64-linux-gnu/libgraphite2.so.3.0.1
7f6d49129000-7f6d4912b000 r--p 00023000 08:16 3417714                    /usr/lib/x86_64-linux-gnu/libgraphite2.so.3.0.1
7f6d4912b000-7f6d4912c000 rw-p 00025000 08:16 3417714                    /usr/lib/x86_64-linux-gnu/libgraphite2.so.3.0.1
7f6d4912c000-7f6d4913d000 r-xp 00000000 08:16 3418331                    /usr/lib/x86_64-linux-gnu/libtasn1.so.6.5.1
7f6d4913d000-7f6d4933d000 ---p 00011000 08:16 3418331                    /usr/lib/x86_64-linux-gnu/libtasn1.so.6.5.1
7f6d4933d000-7f6d4933e000 r--p 00011000 08:16 3418331                    /usr/lib/x86_64-linux-gnu/libtasn1.so.6.5.1
.....
@1544933246"" if you are using GNU date ***
PC: @     0x7f28a54fa428 gsignal
*** SIGABRT (@0x426f) received by PID 17007 (TID 0x7f28a80f7b00) from PID 17007; stack trace: ***
    @     0x7f28a54fa4b0 (unknown)
    @     0x7f28a54fa428 gsignal
    @     0x7f28a54fc02a abort
    @     0x7f28a553c7ea (unknown)
    @     0x7f28a554537a (unknown)
    @     0x7f28a554953c cfree
    @     0x7f28a68538af google::protobuf::internal::DestroyDefaultRepeatedFields()
    @     0x7f28a6852b3b google::protobuf::ShutdownProtobufLibrary()
    @     0x7f2872c913b9 (unknown)
    @     0x7f28a7ff7de7 (unknown)
    @     0x7f28a54feff8 (unknown)
    @     0x7f28a54ff045 exit
    @     0x7f28a54e5837 __libc_start_main
    @           0x4080b9 _start
    @                0x0 (unknown)
Makefile:526: recipe for target 'runtest' failed
",0,make runtest error,"make runtest error 


*** Error in `.build_release/tools/caffe': free(): invalid pointer: 0x0000000001f342c0 ***
======= Backtrace: =========
/lib/x86_64-linux-gnu/libc.so.6(+0x777e5)[0x7f6d7e4387e5]
/lib/x86_64-linux-gnu/libc.so.6(+0x8037a)[0x7f6d7e44137a]
/lib/x86_64-linux-gnu/libc.so.6(cfree+0x4c)[0x7f6d7e44553c]
/usr/lib/x86_64-linux-gnu/libprotobuf.so.9(_ZN6google8protobuf8internal28DestroyDefaultRepeatedFieldsEv+0x1f)[0x7f6d7f74f8af]
/usr/lib/x86_64-linux-gnu/libprotobuf.so.9(_ZN6google8protobuf23ShutdownProtobufLibraryEv+0x8b)[0x7f6d7f74eb3b]
/usr/lib/x86_64-linux-gnu/libmirprotobuf.so.3(+0x233b9)[0x7f6d4bb8d3b9]
/lib64/ld-linux-x86-64.so.2(+0x10de7)[0x7f6d80ef3de7]
/lib/x86_64-linux-gnu/libc.so.6(+0x39ff8)[0x7f6d7e3faff8]
/lib/x86_64-linux-gnu/libc.so.6(+0x3a045)[0x7f6d7e3fb045]
/lib/x86_64-linux-gnu/libc.so.6(__libc_start_main+0xf7)[0x7f6d7e3e1837]
.build_release/tools/caffe[0x4080b9]
======= Memory map: ========
00400000-00418000 r-xp 00000000 08:18 659239                             /home/csy/caffe/.build_release/tools/caffe.bin
00618000-00619000 r--p 00018000 08:18 659239                             /home/csy/caffe/.build_release/tools/caffe.bin
00619000-0061a000 rw-p 00019000 08:18 659239                             /home/csy/caffe/.build_release/tools/caffe.bin
01f22000-02191000 rw-p 00000000 00:00 0                                  [heap]
7f6d38000000-7f6d38021000 rw-p 00000000 00:00 0 
7f6d38021000-7f6d3c000000 ---p 00000000 00:00 0 
7f6d3fb87000-7f6d3fb88000 ---p 00000000 00:00 0 
7f6d3fb88000-7f6d40388000 rw-p 00000000 00:00 0 
7f6d46388000-7f6d46389000 ---p 00000000 00:00 0 
7f6d46389000-7f6d46b89000 rw-p 00000000 00:00 0 
7f6d46b89000-7f6d46b8a000 ---p 00000000 00:00 0 
7f6d46b8a000-7f6d4738a000 rw-p 00000000 00:00 0 
7f6d4738a000-7f6d4738b000 ---p 00000000 00:00 0 
7f6d4738b000-7f6d47b8b000 rw-p 00000000 00:00 0 
7f6d47b8b000-7f6d47b8e000 r-xp 00000000 08:16 1447018                    /lib/x86_64-linux-gnu/libkeyutils.so.1.5
7f6d47b8e000-7f6d47d8d000 ---p 00003000 08:16 1447018                    /lib/x86_64-linux-gnu/libkeyutils.so.1.5
7f6d47d8d000-7f6d47d8e000 r--p 00002000 08:16 1447018                    /lib/x86_64-linux-gnu/libkeyutils.so.1.5
7f6d47d8e000-7f6d47d8f000 rw-p 00003000 08:16 1447018                    /lib/x86_64-linux-gnu/libkeyutils.so.1.5
7f6d47d8f000-7f6d47d99000 r-xp 00000000 08:16 3417915                    /usr/lib/x86_64-linux-gnu/libkrb5support.so.0.1
7f6d47d99000-7f6d47f98000 ---p 0000a000 08:16 3417915                    /usr/lib/x86_64-linux-gnu/libkrb5support.so.0.1
7f6d47f98000-7f6d47f99000 r--p 00009000 08:16 3417915                    /usr/lib/x86_64-linux-gnu/libkrb5support.so.0.1
7f6d47f99000-7f6d47f9a000 rw-p 0000a000 08:16 3417915                    /usr/lib/x86_64-linux-gnu/libkrb5support.so.0.1
7f6d47f9a000-7f6d47f9d000 r-xp 00000000 08:16 1446966                    /lib/x86_64-linux-gnu/libcom_err.so.2.1
7f6d47f9d000-7f6d4819c000 ---p 00003000 08:16 1446966                    /lib/x86_64-linux-gnu/libcom_err.so.2.1
7f6d4819c000-7f6d4819d000 r--p 00002000 08:16 1446966                    /lib/x86_64-linux-gnu/libcom_err.so.2.1
7f6d4819d000-7f6d4819e000 rw-p 00003000 08:16 1446966                    /lib/x86_64-linux-gnu/libcom_err.so.2.1
7f6d4819e000-7f6d481ca000 r-xp 00000000 08:16 3417905                    /usr/lib/x86_64-linux-gnu/libk5crypto.so.3.1
7f6d481ca000-7f6d483c9000 ---p 0002c000 08:16 3417905                    /usr/lib/x86_64-linux-gnu/libk5crypto.so.3.1
7f6d483c9000-7f6d483cb000 r--p 0002b000 08:16 3417905                    /usr/lib/x86_64-linux-gnu/libk5crypto.so.3.1
7f6d483cb000-7f6d483cc000 rw-p 0002d000 08:16 3417905                    /usr/lib/x86_64-linux-gnu/libk5crypto.so.3.1
7f6d483cc000-7f6d483cd000 rw-p 00000000 00:00 0 
7f6d483cd000-7f6d48490000 r-xp 00000000 08:16 3417913                    /usr/lib/x86_64-linux-gnu/libkrb5.so.3.3
7f6d48490000-7f6d48690000 ---p 000c3000 08:16 3417913                    /usr/lib/x86_64-linux-gnu/libkrb5.so.3.3
7f6d48690000-7f6d4869d000 r--p 000c3000 08:16 3417913                    /usr/lib/x86_64-linux-gnu/libkrb5.so.3.3
7f6d4869d000-7f6d4869f000 rw-p 000d0000 08:16 3417913                    /usr/lib/x86_64-linux-gnu/libkrb5.so.3.3
7f6d4869f000-7f6d486b1000 r-xp 00000000 08:16 1447001                    /lib/x86_64-linux-gnu/libgpg-error.so.0.17.0
7f6d486b1000-7f6d488b1000 ---p 00012000 08:16 1447001                    /lib/x86_64-linux-gnu/libgpg-error.so.0.17.0
7f6d488b1000-7f6d488b2000 r--p 00012000 08:16 1447001                    /lib/x86_64-linux-gnu/libgpg-error.so.0.17.0
7f6d488b2000-7f6d488b3000 rw-p 00013000 08:16 1447001                    /lib/x86_64-linux-gnu/libgpg-error.so.0.17.0
7f6d488b3000-7f6d488d4000 r-xp 00000000 08:16 3417698                    /usr/lib/x86_64-linux-gnu/libgomp.so.1.0.0
7f6d488d4000-7f6d48ad3000 ---p 00021000 08:16 3417698                    /usr/lib/x86_64-linux-gnu/libgomp.so.1.0.0
7f6d48ad3000-7f6d48ad4000 r--p 00020000 08:16 3417698                    /usr/lib/x86_64-linux-gnu/libgomp.so.1.0.0
7f6d48ad4000-7f6d48ad5000 rw-p 00021000 08:16 3417698                    /usr/lib/x86_64-linux-gnu/libgomp.so.1.0.0
7f6d48ad5000-7f6d48afc000 r-xp 00000000 08:16 3408053                    /usr/lib/x86_64-linux-gnu/libkj-0.5.3.so
7f6d48afc000-7f6d48cfc000 ---p 00027000 08:16 3408053                    /usr/lib/x86_64-linux-gnu/libkj-0.5.3.so
7f6d48cfc000-7f6d48cfd000 r--p 00027000 08:16 3408053                    /usr/lib/x86_64-linux-gnu/libkj-0.5.3.so
7f6d48cfd000-7f6d48cfe000 rw-p 00028000 08:16 3408053                    /usr/lib/x86_64-linux-gnu/libkj-0.5.3.so
7f6d48cfe000-7f6d48d04000 r-xp 00000000 08:16 3417408                    /usr/lib/x86_64-linux-gnu/libdatrie.so.1.3.3
7f6d48d04000-7f6d48f04000 ---p 00006000 08:16 3417408                    /usr/lib/x86_64-linux-gnu/libdatrie.so.1.3.3
7f6d48f04000-7f6d48f05000 r--p 00006000 08:16 3417408                    /usr/lib/x86_64-linux-gnu/libdatrie.so.1.3.3
7f6d48f05000-7f6d48f06000 rw-p 00007000 08:16 3417408                    /usr/lib/x86_64-linux-gnu/libdatrie.so.1.3.3
7f6d48f06000-7f6d48f2a000 r-xp 00000000 08:16 3417714                    /usr/lib/x86_64-linux-gnu/libgraphite2.so.3.0.1
7f6d48f2a000-7f6d49129000 ---p 00024000 08:16 3417714                    /usr/lib/x86_64-linux-gnu/libgraphite2.so.3.0.1
7f6d49129000-7f6d4912b000 r--p 00023000 08:16 3417714                    /usr/lib/x86_64-linux-gnu/libgraphite2.so.3.0.1
7f6d4912b000-7f6d4912c000 rw-p 00025000 08:16 3417714                    /usr/lib/x86_64-linux-gnu/libgraphite2.so.3.0.1
7f6d4912c000-7f6d4913d000 r-xp 00000000 08:16 3418331                    /usr/lib/x86_64-linux-gnu/libtasn1.so.6.5.1
7f6d4913d000-7f6d4933d000 ---p 00011000 08:16 3418331                    /usr/lib/x86_64-linux-gnu/libtasn1.so.6.5.1
7f6d4933d000-7f6d4933e000 r--p 00011000 08:16 3418331                    /usr/lib/x86_64-linux-gnu/libtasn1.so.6.5.1
.....
@1544933246"" if you are using GNU date ***
PC: @     0x7f28a54fa428 gsignal
*** SIGABRT (@0x426f) received by PID 17007 (TID 0x7f28a80f7b00) from PID 17007; stack trace: ***
    @     0x7f28a54fa4b0 (unknown)
    @     0x7f28a54fa428 gsignal
    @     0x7f28a54fc02a abort
    @     0x7f28a553c7ea (unknown)
    @     0x7f28a554537a (unknown)
    @     0x7f28a554953c cfree
    @     0x7f28a68538af google::protobuf::internal::DestroyDefaultRepeatedFields()
    @     0x7f28a6852b3b google::protobuf::ShutdownProtobufLibrary()
    @     0x7f2872c913b9 (unknown)
    @     0x7f28a7ff7de7 (unknown)
    @     0x7f28a54feff8 (unknown)
    @     0x7f28a54ff045 exit
    @     0x7f28a54e5837 __libc_start_main
    @           0x4080b9 _start
    @                0x0 (unknown)
Makefile:526: recipe for target 'runtest' failed
"
caffe,5661,"Using code in line 19 of the notebook example in [http://nbviewer.jupyter.org/github/BVLC/caffe/blob/master/examples/01-learning-lenet.ipynb](url), i created a python code to create and run caffe model.

But, when i call method by IPython, even if i specified which gpu it want to use, caffe occupy all gpus avaliables.

My code is:



### Your system configuration
Operating system: Ubuntu 14.04
caffe version: 0.15.13
Python: 2.7.6 ",0,pycaffe occupy all gpus avaliables,"pycaffe occupy all gpus avaliables Using code in line 19 of the notebook example in [http://nbviewer.jupyter.org/github/BVLC/caffe/blob/master/examples/01-learning-lenet.ipynb](url), i created a python code to create and run caffe model.

But, when i call method by IPython, even if i specified which gpu it want to use, caffe occupy all gpus avaliables.

My code is:



### Your system configuration
Operating system: Ubuntu 14.04
caffe version: 0.15.13
Python: 2.7.6 "
caffe,5641,"https://github.com/BVLC/caffe/search?utf8=%E2%9C%93&q=VERION&type=

should be VERSION, shouldn't it?",0,VERION,"VERION https://github.com/BVLC/caffe/search?utf8=%E2%9C%93&q=VERION&type=

should be VERSION, shouldn't it?"
caffe,5422,"#Iters Seconds TrainingLoss LearningRate
0      60.772177    60.7705s/100  0.0001
100    96.870289    36.0956s/100  0.0001
200    133.719327   36.8478s/100  0.0001
300    170.667324   36.946s/100   0.0001
400    207.608621   36.9394s/100  0.0001
500    244.500362   36.89s/100    0.0001
600    281.548364   37.0463s/100  0.0001
700    318.434129   36.8841s/100  0.0001
800    355.580382   37.1446s/100  0.0001
900    392.528603   36.9466s/100  0.0001
1000   429.473415   36.9432s/100  0.0001
1100   466.495543   37.0206s/100  0.0001
1200   503.244692   36.7477s/100  0.0001
1300   540.270175   37.0241s/100  0.0001
1400   577.266514   36.995s/100   0.0001
1500   614.206808   36.9389s/100  0.0001
1600   651.201857   36.9936s/100  0.0001
1700   688.026594   36.8234s/100  0.0001
1800   725.065849   37.0379s/100  0.0001
1900   761.979795   36.9126s/100  0.0001
2000   859.710876   97.7279s/100  0.0001
2100   896.690779   36.9773s/100  0.0001
2200   933.594652   36.9026s/100  0.0001
2300   970.522537   36.9266s/100  0.0001
2400   1007.543336  37.0196s/100  0.0001
2500   1044.460583  36.916s/100   0.0001
2600   1081.500929  37.0391s/100  0.0001
2700   1118.330184  36.8281s/100  0.0001
2800   1155.352403  37.021s/100   0.0001
2900   1192.409225  37.0556s/100  0.0001
3000   1229.340137  36.9297s/100  0.0001
3100   1266.362365  37.0211s/100  0.0001
3200   1303.306361  36.9428s/100  0.0001
3300   1340.233099  36.9256s/100  0.0001
...

The training loss here is time per 100 iteration.",0,parse_log.sh and plot_training_log.py broken by log format,"parse_log.sh and plot_training_log.py broken by log format #Iters Seconds TrainingLoss LearningRate
0      60.772177    60.7705s/100  0.0001
100    96.870289    36.0956s/100  0.0001
200    133.719327   36.8478s/100  0.0001
300    170.667324   36.946s/100   0.0001
400    207.608621   36.9394s/100  0.0001
500    244.500362   36.89s/100    0.0001
600    281.548364   37.0463s/100  0.0001
700    318.434129   36.8841s/100  0.0001
800    355.580382   37.1446s/100  0.0001
900    392.528603   36.9466s/100  0.0001
1000   429.473415   36.9432s/100  0.0001
1100   466.495543   37.0206s/100  0.0001
1200   503.244692   36.7477s/100  0.0001
1300   540.270175   37.0241s/100  0.0001
1400   577.266514   36.995s/100   0.0001
1500   614.206808   36.9389s/100  0.0001
1600   651.201857   36.9936s/100  0.0001
1700   688.026594   36.8234s/100  0.0001
1800   725.065849   37.0379s/100  0.0001
1900   761.979795   36.9126s/100  0.0001
2000   859.710876   97.7279s/100  0.0001
2100   896.690779   36.9773s/100  0.0001
2200   933.594652   36.9026s/100  0.0001
2300   970.522537   36.9266s/100  0.0001
2400   1007.543336  37.0196s/100  0.0001
2500   1044.460583  36.916s/100   0.0001
2600   1081.500929  37.0391s/100  0.0001
2700   1118.330184  36.8281s/100  0.0001
2800   1155.352403  37.021s/100   0.0001
2900   1192.409225  37.0556s/100  0.0001
3000   1229.340137  36.9297s/100  0.0001
3100   1266.362365  37.0211s/100  0.0001
3200   1303.306361  36.9428s/100  0.0001
3300   1340.233099  36.9256s/100  0.0001
...

The training loss here is time per 100 iteration."
caffe,4309,"will there be some examples for the new lstm layer?
",0,will there be some examples for the new lstm layers?,"will there be some examples for the new lstm layers? will there be some examples for the new lstm layer?
"
caffe,5566,"Seems I googled around for a while and found no interface to reset the network to its status as newly loaded in test phase.

I mean the memory used can only increase but not decrease.

The problem is that say, I have 3 separate alg worker process that are in the algorithm chain.

Suppose A/B/C have similar memory usage above, and my GPU is 8GB.
It's clear where the problem appears. It will say  while forwarding in .

One embarrass solution is copy output from alg and then  and  the net to keep ~400MB memory usage. And continue with the next alg.
Under production, this is a stupid solution that will waste a lot of time on  and  (file reading IO).
So, can we supply an interface to free all the appropriate variables that will lead the  fresh new as just loaded?

I found the most possible API in [ApolloCaffe](https://github.com/Russell91/apollocaffe). But the author seemed not maintaining the repo. @Russell91 


Code from google snapshot of http://apollocaffe.com/",0,Any interface to clear to intermediate variables generated by network forward in test phase?,"Any interface to clear to intermediate variables generated by network forward in test phase? Seems I googled around for a while and found no interface to reset the network to its status as newly loaded in test phase.

I mean the memory used can only increase but not decrease.

The problem is that say, I have 3 separate alg worker process that are in the algorithm chain.

Suppose A/B/C have similar memory usage above, and my GPU is 8GB.
It's clear where the problem appears. It will say  while forwarding in .

One embarrass solution is copy output from alg and then  and  the net to keep ~400MB memory usage. And continue with the next alg.
Under production, this is a stupid solution that will waste a lot of time on  and  (file reading IO).
So, can we supply an interface to free all the appropriate variables that will lead the  fresh new as just loaded?

I found the most possible API in [ApolloCaffe](https://github.com/Russell91/apollocaffe). But the author seemed not maintaining the repo. @Russell91 


Code from google snapshot of http://apollocaffe.com/"
caffe,6685,"## Important - read before submitting

*Please read the [guidelines for contributing](https://github.com/BVLC/caffe/blob/master/CONTRIBUTING.md) before submitting this issue!*

*Please do not post installation, build, usage, or modeling questions, or other requests for help to Issues.*
Use the [caffe-users list](https://groups.google.com/forum/#!forum/caffe-users) instead.
This helps developers maintain a clear, uncluttered, and efficient view of the state of Caffe.

### Issue summary


### Steps to reproduce


### Tried solutions


### System configuration

* Operating system: 
* Compiler: 
* CUDA version (if applicable): 
* CUDNN version (if applicable): 
* BLAS: 
* Python version (if using pycaffe): 
* MATLAB version (if using matcaffe): 

### Issue checklist

- [ ] read the guidelines and removed the first paragraph
- [ ] written a short summary and detailed steps to reproduce
- [ ] explained how solutions to related problems failed (tick if found none)
- [ ] filled system configuration
- [ ] attached relevant logs/config files (tick if not applicable)
",0, boost," boost ## Important - read before submitting

*Please read the [guidelines for contributing](https://github.com/BVLC/caffe/blob/master/CONTRIBUTING.md) before submitting this issue!*

*Please do not post installation, build, usage, or modeling questions, or other requests for help to Issues.*
Use the [caffe-users list](https://groups.google.com/forum/#!forum/caffe-users) instead.
This helps developers maintain a clear, uncluttered, and efficient view of the state of Caffe.

### Issue summary


### Steps to reproduce


### Tried solutions


### System configuration

* Operating system: 
* Compiler: 
* CUDA version (if applicable): 
* CUDNN version (if applicable): 
* BLAS: 
* Python version (if using pycaffe): 
* MATLAB version (if using matcaffe): 

### Issue checklist

- [ ] read the guidelines and removed the first paragraph
- [ ] written a short summary and detailed steps to reproduce
- [ ] explained how solutions to related problems failed (tick if found none)
- [ ] filled system configuration
- [ ] attached relevant logs/config files (tick if not applicable)
"
caffe,3331,"cccp5
top data[max:105.422 min:-123.697 mean:-1.9735]

bottom data[max:96.133 min:-0 mean:0.000809771]

top diff[max1.35333e-08 min:-1.33265e-08 mean:-1.96833e-16]

bottom diff[max:0.000116085 min:-0.000235484 mean:-2.54426e-16]

weight diff[max:1.17238e+06 min:-0.770284 mean:10107.5]

weight[max:0.177044 min:-0.202771 mean:-0.000405207]

i print the information of a conv layer,the top diff and bottom data is small,why weight diff became so big???  since we know weight diff depends on the value of top diff and bottom data.

code:
if (this->param_propagate_down_[0]) {
          this->weight_cpu_gemm(bottom_data + n \* this->bottom_dim_,
              top_diff + n \* this->top_dim_, weight_diff);
}
",0,how  could weight_diff of a conv layer become so large?,"how  could weight_diff of a conv layer become so large? cccp5
top data[max:105.422 min:-123.697 mean:-1.9735]

bottom data[max:96.133 min:-0 mean:0.000809771]

top diff[max1.35333e-08 min:-1.33265e-08 mean:-1.96833e-16]

bottom diff[max:0.000116085 min:-0.000235484 mean:-2.54426e-16]

weight diff[max:1.17238e+06 min:-0.770284 mean:10107.5]

weight[max:0.177044 min:-0.202771 mean:-0.000405207]

i print the information of a conv layer,the top diff and bottom data is small,why weight diff became so big???  since we know weight diff depends on the value of top diff and bottom data.

code:
if (this->param_propagate_down_[0]) {
          this->weight_cpu_gemm(bottom_data + n \* this->bottom_dim_,
              top_diff + n \* this->top_dim_, weight_diff);
}
"
caffe,5954,"Please use the [caffe-users list](https://groups.google.com/forum/#!forum/caffe-users) for usage, installation, or modeling questions, or other requests for help.
_Do not post such requests to Issues._ Doing so interferes with the development of Caffe.

Please read the [guidelines for contributing](https://github.com/BVLC/caffe/blob/master/CONTRIBUTING.md) before submitting this issue.

### Issue summary
I'm training imagenet dataset using inception v3. Everything was fine in the first two days, but then caffe was stuck since yesterday.  The log hasn't been updated for 24 hours, but when I check cpu and gpu, it seems that caffe in still runing. Does anyone know what's going on? Thanks a lot!


### nvidia-smi output:

### log
",0,Caffe got stuck halfway when training imagenet dataset,"Caffe got stuck halfway when training imagenet dataset Please use the [caffe-users list](https://groups.google.com/forum/#!forum/caffe-users) for usage, installation, or modeling questions, or other requests for help.
_Do not post such requests to Issues._ Doing so interferes with the development of Caffe.

Please read the [guidelines for contributing](https://github.com/BVLC/caffe/blob/master/CONTRIBUTING.md) before submitting this issue.

### Issue summary
I'm training imagenet dataset using inception v3. Everything was fine in the first two days, but then caffe was stuck since yesterday.  The log hasn't been updated for 24 hours, but when I check cpu and gpu, it seems that caffe in still runing. Does anyone know what's going on? Thanks a lot!


### nvidia-smi output:

### log
"
caffe,86,,0,Add python script to plot the training log,
caffe,703,"When I try to import caffe from the terminal in Ubuntu by the command python -c ""import caffe"", I get the following error :

GLIBC_2.15' not found (required by /usr/lib/x86_64-linux-gnu/libvorbis.so.0)
`

Could anyone assist here ?
",0,Problem while importing caffe in Python,"Problem while importing caffe in Python When I try to import caffe from the terminal in Ubuntu by the command python -c ""import caffe"", I get the following error :

GLIBC_2.15' not found (required by /usr/lib/x86_64-linux-gnu/libvorbis.so.0)
`

Could anyone assist here ?
"
caffe,101,"This is a request for comments on the Caffe development model and contribution protocol. These aren't draconian laws so much as guidelines so we can all happily brew Caffe. Establishing protocols is only a side effect of having such thriving development, so it's a lovely problem we have.

Development workflow proposal:
-  is golden.
- work is done in feature branches. These should be rebased to the tip of  to avoid driftand this is required for merge.
-  is the branching point for features, and the target of pull requests for merge.
- contributions are shepherded from  to   by the project maintainers after integration and testing via merge.
- the history of  is not rewritten. accidents are fixed by reverts.
- ""releases"" are marked with tags

Contributions/Issues/PR proposal: see https://github.com/BVLC/caffe/issues/101#issuecomment-35349256.

The closer your contribution follows these suggestions, the less friction for merging.
",0,Establish Development and Contribution Guidelines,"Establish Development and Contribution Guidelines This is a request for comments on the Caffe development model and contribution protocol. These aren't draconian laws so much as guidelines so we can all happily brew Caffe. Establishing protocols is only a side effect of having such thriving development, so it's a lovely problem we have.

Development workflow proposal:
-  is golden.
- work is done in feature branches. These should be rebased to the tip of  to avoid driftand this is required for merge.
-  is the branching point for features, and the target of pull requests for merge.
- contributions are shepherded from  to   by the project maintainers after integration and testing via merge.
- the history of  is not rewritten. accidents are fixed by reverts.
- ""releases"" are marked with tags

Contributions/Issues/PR proposal: see https://github.com/BVLC/caffe/issues/101#issuecomment-35349256.

The closer your contribution follows these suggestions, the less friction for merging.
"
caffe,626,"Hi,

I have installed and compiled caffe successfully (I mean I ran 'make all' and 'make test' without any error). While running 'make runtest' I'm getting an ""invalid device function error"". The full log is given below. I'm using cuda 6.0 in Ubuntu 14.04 LTS. The gcc/g++ version is 4.6 and I installed and changed the make files as described in 'https://github.com/BVLC/caffe/issues/337' by weinman. I had to install gcc/g++ 4.6 as I was having an error as described in the above link while using gcc/g++ 4.8
Any help will be highly appreciated

The log is below:::
[ RUN      ] StochasticPoolingLayerTest/1.TestGradientGPU
F0705 07:26:44.199472 14804 pooling_layer.cu:186] Check failed: error == cudaSuccess (8 vs. 0)  invalid device function 
**\* Check failure stack trace: ***
    @     0x2ad77f5559fd  google::LogMessage::Fail()
    @     0x2ad77f55789d  google::LogMessage::SendToLog()
    @     0x2ad77f5555ec  google::LogMessage::Flush()
    @     0x2ad77f5581be  google::LogMessageFatal::~LogMessageFatal()
    @           0x610fba  caffe::PoolingLayer<>::Forward_gpu()
    @           0x431c58  caffe::GradientChecker<>::CheckGradientSingle()
    @           0x474124  caffe::StochasticPoolingLayerTest_TestGradientGPU_Test<>::TestBody()
    @           0x55b30d  testing::internal::HandleExceptionsInMethodIfSupported<>()
    @           0x553131  testing::Test::Run()
    @           0x553216  testing::TestInfo::Run()
    @           0x553357  testing::TestCase::Run()
    @           0x5536ae  testing::internal::UnitTestImpl::RunAllTests()
    @           0x55ae8d  testing::internal::HandleExceptionsInMethodIfSupported<>()
    @           0x55278e  testing::UnitTest::Run()
    @           0x4120dd  main
    @     0x2ad7818ffec5  (unknown)
    @           0x416e57  (unknown)
make: **\* [runtest] Aborted (core dumped)

Thanks,
Abir
",0,Getting invalid device function error while trying to make runtest in Ubuntu 14.04 with Cuda 6.0,"Getting invalid device function error while trying to make runtest in Ubuntu 14.04 with Cuda 6.0 Hi,

I have installed and compiled caffe successfully (I mean I ran 'make all' and 'make test' without any error). While running 'make runtest' I'm getting an ""invalid device function error"". The full log is given below. I'm using cuda 6.0 in Ubuntu 14.04 LTS. The gcc/g++ version is 4.6 and I installed and changed the make files as described in 'https://github.com/BVLC/caffe/issues/337' by weinman. I had to install gcc/g++ 4.6 as I was having an error as described in the above link while using gcc/g++ 4.8
Any help will be highly appreciated

The log is below:::
[ RUN      ] StochasticPoolingLayerTest/1.TestGradientGPU
F0705 07:26:44.199472 14804 pooling_layer.cu:186] Check failed: error == cudaSuccess (8 vs. 0)  invalid device function 
**\* Check failure stack trace: ***
    @     0x2ad77f5559fd  google::LogMessage::Fail()
    @     0x2ad77f55789d  google::LogMessage::SendToLog()
    @     0x2ad77f5555ec  google::LogMessage::Flush()
    @     0x2ad77f5581be  google::LogMessageFatal::~LogMessageFatal()
    @           0x610fba  caffe::PoolingLayer<>::Forward_gpu()
    @           0x431c58  caffe::GradientChecker<>::CheckGradientSingle()
    @           0x474124  caffe::StochasticPoolingLayerTest_TestGradientGPU_Test<>::TestBody()
    @           0x55b30d  testing::internal::HandleExceptionsInMethodIfSupported<>()
    @           0x553131  testing::Test::Run()
    @           0x553216  testing::TestInfo::Run()
    @           0x553357  testing::TestCase::Run()
    @           0x5536ae  testing::internal::UnitTestImpl::RunAllTests()
    @           0x55ae8d  testing::internal::HandleExceptionsInMethodIfSupported<>()
    @           0x55278e  testing::UnitTest::Run()
    @           0x4120dd  main
    @     0x2ad7818ffec5  (unknown)
    @           0x416e57  (unknown)
make: **\* [runtest] Aborted (core dumped)

Thanks,
Abir
"
caffe,5606,"### Issue summary
I tried to compile caffe and matcaffe but i get the following error


### Steps to reproduce
Build commands



### system configuration
Operating system: Arch Linux
CUDA version : 8.0.61
CUDNN version : 6.0.21
BLAS: OpenBlas
Matlab : R2017a
",0,Cmake build with malab,"Cmake build with malab ### Issue summary
I tried to compile caffe and matcaffe but i get the following error


### Steps to reproduce
Build commands



### system configuration
Operating system: Arch Linux
CUDA version : 8.0.61
CUDNN version : 6.0.21
BLAS: OpenBlas
Matlab : R2017a
"
caffe,4289,"Hi all. I am trying to run the classification_demo. I have completed the steps stated at https://github.com/BVLC/caffe/tree/windows. I only have Matlab enabled to true. My code is extremely simple

im = imread('cat.jpg');
[score, class] = classification_demo(im, 0);

But when I run it I get the following error. Undefined function 'caffe_' for input arguments of type 'char'
Can anyone help me on this ?
",0,Undefined function 'caffe_' for input arguments of type 'char',"Undefined function 'caffe_' for input arguments of type 'char' Hi all. I am trying to run the classification_demo. I have completed the steps stated at https://github.com/BVLC/caffe/tree/windows. I only have Matlab enabled to true. My code is extremely simple

im = imread('cat.jpg');
[score, class] = classification_demo(im, 0);

But when I run it I get the following error. Undefined function 'caffe_' for input arguments of type 'char'
Can anyone help me on this ?
"
caffe,4577,"Hi

I see ""Undefined symbols for architecture x86_64:""  error when i try to build Caffe on Mac which has OS X version 10.11.6 running on it. 
I did follow the instructions mentioned here : http://caffe.berkeleyvision.org/install_osx.html, however i still see the error. I even downgraded the version of Xcode from 7.3 to 7.0 as was suggested in one of the blogposts i encountered.

I have the following dependencies installed and I have Cuda version 7.5 installed.
snappy-1.1.3 
leveldb-1.18 
gflags-2.1.2 
glog-0.3.4 
szip-2.1 
lmdb-0.9.14 
homebrew/science/opencv-2.4.13 
protobuf-2.6.1

_I have attached the Makefile and Makefile.config with this email._

> brew install --build-from-source --fresh -vd boost boost-python 
> /usr/local/Library/Homebrew/brew.rb (Formulary::FormulaLoader): loading /usr/local/Library/Taps/homebrew/homebrew-core/Formula/boost.rb
> /usr/local/Library/Homebrew/brew.rb (Formulary::FormulaLoader): loading /usr/local/Library/Taps/homebrew/homebrew-core/Formula/boost-python.rb
> Warning: boost-1.61.0 already installed
> Warning: boost-python-1.61.0 already installed

====================================== make all output ============================================
MacBook-Pro-3:caffe praveenbodigutla$ make all
CXX .build_release/src/caffe/proto/caffe.pb.cc
CXX src/caffe/blob.cpp
CXX src/caffe/common.cpp
CXX src/caffe/data_reader.cpp
CXX src/caffe/data_transformer.cpp
CXX src/caffe/internal_thread.cpp
CXX src/caffe/layer.cpp
CXX src/caffe/layer_factory.cpp
CXX src/caffe/layers/absval_layer.cpp
CXX src/caffe/layers/accuracy_layer.cpp
CXX src/caffe/layers/argmax_layer.cpp
CXX src/caffe/layers/base_conv_layer.cpp
CXX src/caffe/layers/base_data_layer.cpp
CXX src/caffe/layers/batch_norm_layer.cpp
CXX src/caffe/layers/batch_reindex_layer.cpp
CXX src/caffe/layers/bias_layer.cpp
CXX src/caffe/layers/bnll_layer.cpp
CXX src/caffe/layers/concat_layer.cpp
CXX src/caffe/layers/contrastive_loss_layer.cpp
CXX src/caffe/layers/conv_layer.cpp
CXX src/caffe/layers/crop_layer.cpp
In file included from src/caffe/layers/crop_layer.cpp:10:
./include/caffe/net.hpp:42:5: warning: unused typedef 'INVALID_REQUESTED_LOG_SEVERITY' [-Wunused-local-typedef]
    LOG_EVERY_N(WARNING, 1000) << ""DEPRECATED: ForwardPrefilled() ""
    ^
/usr/local/include/glog/logging.h:917:30: note: expanded from macro 'LOG_EVERY_N'
                             INVALID_REQUESTED_LOG_SEVERITY);           \
                             ^
/usr/local/include/glog/logging.h:912:73: note: expanded from macro 'GOOGLE_GLOG_COMPILE_ASSERT'
  typedef google::glog_internal_namespace_::CompileAssert<(bool(expr))> msg[bool(expr) ? 1 : -1]
                                                                        ^
1 warning generated.
CXX src/caffe/layers/cudnn_conv_layer.cpp
CXX src/caffe/layers/cudnn_lcn_layer.cpp
CXX src/caffe/layers/cudnn_lrn_layer.cpp
CXX src/caffe/layers/cudnn_pooling_layer.cpp
CXX src/caffe/layers/cudnn_relu_layer.cpp
CXX src/caffe/layers/cudnn_sigmoid_layer.cpp
CXX src/caffe/layers/cudnn_softmax_layer.cpp
CXX src/caffe/layers/cudnn_tanh_layer.cpp
CXX src/caffe/layers/data_layer.cpp
CXX src/caffe/layers/deconv_layer.cpp
CXX src/caffe/layers/dropout_layer.cpp
CXX src/caffe/layers/dummy_data_layer.cpp
CXX src/caffe/layers/eltwise_layer.cpp
CXX src/caffe/layers/elu_layer.cpp
CXX src/caffe/layers/embed_layer.cpp
CXX src/caffe/layers/euclidean_loss_layer.cpp
CXX src/caffe/layers/exp_layer.cpp
CXX src/caffe/layers/filter_layer.cpp
CXX src/caffe/layers/flatten_layer.cpp
CXX src/caffe/layers/hdf5_data_layer.cpp
CXX src/caffe/layers/hdf5_output_layer.cpp
CXX src/caffe/layers/hinge_loss_layer.cpp
CXX src/caffe/layers/im2col_layer.cpp
CXX src/caffe/layers/image_data_layer.cpp
CXX src/caffe/layers/infogain_loss_layer.cpp
CXX src/caffe/layers/inner_product_layer.cpp
CXX src/caffe/layers/input_layer.cpp
CXX src/caffe/layers/log_layer.cpp
CXX src/caffe/layers/loss_layer.cpp
CXX src/caffe/layers/lrn_layer.cpp
CXX src/caffe/layers/lstm_layer.cpp
In file included from src/caffe/layers/lstm_layer.cpp:8:
In file included from ./include/caffe/layers/lstm_layer.hpp:11:
In file included from ./include/caffe/layers/recurrent_layer.hpp:11:
./include/caffe/net.hpp:42:5: warning: unused typedef 'INVALID_REQUESTED_LOG_SEVERITY' [-Wunused-local-typedef]
    LOG_EVERY_N(WARNING, 1000) << ""DEPRECATED: ForwardPrefilled() ""
    ^
/usr/local/include/glog/logging.h:917:30: note: expanded from macro 'LOG_EVERY_N'
                             INVALID_REQUESTED_LOG_SEVERITY);           \
                             ^
/usr/local/include/glog/logging.h:912:73: note: expanded from macro 'GOOGLE_GLOG_COMPILE_ASSERT'
  typedef google::glog_internal_namespace_::CompileAssert<(bool(expr))> msg[bool(expr) ? 1 : -1]
                                                                        ^
1 warning generated.
CXX src/caffe/layers/lstm_unit_layer.cpp
In file included from src/caffe/layers/lstm_unit_layer.cpp:6:
In file included from ./include/caffe/layers/lstm_layer.hpp:11:
In file included from ./include/caffe/layers/recurrent_layer.hpp:11:
./include/caffe/net.hpp:42:5: warning: unused typedef 'INVALID_REQUESTED_LOG_SEVERITY' [-Wunused-local-typedef]
    LOG_EVERY_N(WARNING, 1000) << ""DEPRECATED: ForwardPrefilled() ""
    ^
/usr/local/include/glog/logging.h:917:30: note: expanded from macro 'LOG_EVERY_N'
                             INVALID_REQUESTED_LOG_SEVERITY);           \
                             ^
/usr/local/include/glog/logging.h:912:73: note: expanded from macro 'GOOGLE_GLOG_COMPILE_ASSERT'
  typedef google::glog_internal_namespace_::CompileAssert<(bool(expr))> msg[bool(expr) ? 1 : -1]
                                                                        ^
1 warning generated.
CXX src/caffe/layers/memory_data_layer.cpp
CXX src/caffe/layers/multinomial_logistic_loss_layer.cpp
CXX src/caffe/layers/mvn_layer.cpp
CXX src/caffe/layers/neuron_layer.cpp
CXX src/caffe/layers/parameter_layer.cpp
CXX src/caffe/layers/pooling_layer.cpp
CXX src/caffe/layers/power_layer.cpp
CXX src/caffe/layers/prelu_layer.cpp
CXX src/caffe/layers/recurrent_layer.cpp
In file included from src/caffe/layers/recurrent_layer.cpp:8:
In file included from ./include/caffe/layers/recurrent_layer.hpp:11:
./include/caffe/net.hpp:42:5: warning: unused typedef 'INVALID_REQUESTED_LOG_SEVERITY' [-Wunused-local-typedef]
    LOG_EVERY_N(WARNING, 1000) << ""DEPRECATED: ForwardPrefilled() ""
    ^
/usr/local/include/glog/logging.h:917:30: note: expanded from macro 'LOG_EVERY_N'
                             INVALID_REQUESTED_LOG_SEVERITY);           \
                             ^
/usr/local/include/glog/logging.h:912:73: note: expanded from macro 'GOOGLE_GLOG_COMPILE_ASSERT'
  typedef google::glog_internal_namespace_::CompileAssert<(bool(expr))> msg[bool(expr) ? 1 : -1]
                                                                        ^
1 warning generated.
CXX src/caffe/layers/reduction_layer.cpp
CXX src/caffe/layers/relu_layer.cpp
CXX src/caffe/layers/reshape_layer.cpp
CXX src/caffe/layers/rnn_layer.cpp
In file included from src/caffe/layers/rnn_layer.cpp:8:
In file included from ./include/caffe/layers/rnn_layer.hpp:11:
In file included from ./include/caffe/layers/recurrent_layer.hpp:11:
./include/caffe/net.hpp:42:5: warning: unused typedef 'INVALID_REQUESTED_LOG_SEVERITY' [-Wunused-local-typedef]
    LOG_EVERY_N(WARNING, 1000) << ""DEPRECATED: ForwardPrefilled() ""
    ^
/usr/local/include/glog/logging.h:917:30: note: expanded from macro 'LOG_EVERY_N'
                             INVALID_REQUESTED_LOG_SEVERITY);           \
                             ^
/usr/local/include/glog/logging.h:912:73: note: expanded from macro 'GOOGLE_GLOG_COMPILE_ASSERT'
  typedef google::glog_internal_namespace_::CompileAssert<(bool(expr))> msg[bool(expr) ? 1 : -1]
                                                                        ^
1 warning generated.
CXX src/caffe/layers/scale_layer.cpp
CXX src/caffe/layers/sigmoid_cross_entropy_loss_layer.cpp
CXX src/caffe/layers/sigmoid_layer.cpp
CXX src/caffe/layers/silence_layer.cpp
CXX src/caffe/layers/slice_layer.cpp
CXX src/caffe/layers/softmax_layer.cpp
CXX src/caffe/layers/softmax_loss_layer.cpp
CXX src/caffe/layers/split_layer.cpp
CXX src/caffe/layers/spp_layer.cpp
CXX src/caffe/layers/tanh_layer.cpp
CXX src/caffe/layers/threshold_layer.cpp
CXX src/caffe/layers/tile_layer.cpp
CXX src/caffe/layers/window_data_layer.cpp
CXX src/caffe/net.cpp
In file included from src/caffe/net.cpp:12:
./include/caffe/net.hpp:42:5: warning: unused typedef 'INVALID_REQUESTED_LOG_SEVERITY' [-Wunused-local-typedef]
    LOG_EVERY_N(WARNING, 1000) << ""DEPRECATED: ForwardPrefilled() ""
    ^
/usr/local/include/glog/logging.h:917:30: note: expanded from macro 'LOG_EVERY_N'
                             INVALID_REQUESTED_LOG_SEVERITY);           \
                             ^
/usr/local/include/glog/logging.h:912:73: note: expanded from macro 'GOOGLE_GLOG_COMPILE_ASSERT'
  typedef google::glog_internal_namespace_::CompileAssert<(bool(expr))> msg[bool(expr) ? 1 : -1]
src/caffe/net.cpp:580:3: warning: unused typedef 'INVALID_REQUESTED_LOG_SEVERITY' [-Wunused-local-typedef]
  LOG_EVERY_N(WARNING, 1000) << ""DEPRECATED: Forward(bottom, loss) ""
  ^
/usr/local/include/glog/logging.h:917:30: note: expanded from macro 'LOG_EVERY_N'
                             INVALID_REQUESTED_LOG_SEVERITY);           \
                             ^
/usr/local/include/glog/logging.h:912:73: note: expanded from macro 'GOOGLE_GLOG_COMPILE_ASSERT'
  typedef google::glog_internal_namespace_::CompileAssert<(bool(expr))> msg[bool(expr) ? 1 : -1]
                                                                        ^
2 warnings generated.
CXX src/caffe/parallel.cpp
In file included from src/caffe/parallel.cpp:12:
In file included from ./include/caffe/caffe.hpp:12:
./include/caffe/net.hpp:42:5: warning: unused typedef 'INVALID_REQUESTED_LOG_SEVERITY' [-Wunused-local-typedef]
    LOG_EVERY_N(WARNING, 1000) << ""DEPRECATED: ForwardPrefilled() ""
    ^
/usr/local/include/glog/logging.h:917:30: note: expanded from macro 'LOG_EVERY_N'
                             INVALID_REQUESTED_LOG_SEVERITY);           \
                             ^
/usr/local/include/glog/logging.h:912:73: note: expanded from macro 'GOOGLE_GLOG_COMPILE_ASSERT'
  typedef google::glog_internal_namespace_::CompileAssert<(bool(expr))> msg[bool(expr) ? 1 : -1]
                                                                        ^
1 warning generated.
CXX src/caffe/solver.cpp
In file included from src/caffe/solver.cpp:6:
In file included from ./include/caffe/solver.hpp:7:
./include/caffe/net.hpp:42:5: warning: unused typedef 'INVALID_REQUESTED_LOG_SEVERITY' [-Wunused-local-typedef]
    LOG_EVERY_N(WARNING, 1000) << ""DEPRECATED: ForwardPrefilled() ""
    ^
/usr/local/include/glog/logging.h:917:30: note: expanded from macro 'LOG_EVERY_N'
                             INVALID_REQUESTED_LOG_SEVERITY);           \
                             ^
/usr/local/include/glog/logging.h:912:73: note: expanded from macro 'GOOGLE_GLOG_COMPILE_ASSERT'
  typedef google::glog_internal_namespace_::CompileAssert<(bool(expr))> msg[bool(expr) ? 1 : -1]
                                                                        ^
1 warning generated.
CXX src/caffe/solvers/adadelta_solver.cpp
In file included from src/caffe/solvers/adadelta_solver.cpp:3:
In file included from ./include/caffe/sgd_solvers.hpp:7:
In file included from ./include/caffe/solver.hpp:7:
./include/caffe/net.hpp:42:5: warning: unused typedef 'INVALID_REQUESTED_LOG_SEVERITY' [-Wunused-local-typedef]
    LOG_EVERY_N(WARNING, 1000) << ""DEPRECATED: ForwardPrefilled() ""
    ^
/usr/local/include/glog/logging.h:917:30: note: expanded from macro 'LOG_EVERY_N'
                             INVALID_REQUESTED_LOG_SEVERITY);           \
                             ^
/usr/local/include/glog/logging.h:912:73: note: expanded from macro 'GOOGLE_GLOG_COMPILE_ASSERT'
  typedef google::glog_internal_namespace_::CompileAssert<(bool(expr))> msg[bool(expr) ? 1 : -1]
                                                                        ^
1 warning generated.
CXX src/caffe/solvers/adagrad_solver.cpp
In file included from src/caffe/solvers/adagrad_solver.cpp:3:
In file included from ./include/caffe/sgd_solvers.hpp:7:
In file included from ./include/caffe/solver.hpp:7:
./include/caffe/net.hpp:42:5: warning: unused typedef 'INVALID_REQUESTED_LOG_SEVERITY' [-Wunused-local-typedef]
    LOG_EVERY_N(WARNING, 1000) << ""DEPRECATED: ForwardPrefilled() ""
    ^
/usr/local/include/glog/logging.h:917:30: note: expanded from macro 'LOG_EVERY_N'
                             INVALID_REQUESTED_LOG_SEVERITY);           \
                             ^
/usr/local/include/glog/logging.h:912:73: note: expanded from macro 'GOOGLE_GLOG_COMPILE_ASSERT'
  typedef google::glog_internal_namespace_::CompileAssert<(bool(expr))> msg[bool(expr) ? 1 : -1]
                                                                        ^
1 warning generated.
CXX src/caffe/solvers/adam_solver.cpp
In file included from src/caffe/solvers/adam_solver.cpp:3:
In file included from ./include/caffe/sgd_solvers.hpp:7:
In file included from ./include/caffe/solver.hpp:7:
./include/caffe/net.hpp:42:5: warning: unused typedef 'INVALID_REQUESTED_LOG_SEVERITY' [-Wunused-local-typedef]
    LOG_EVERY_N(WARNING, 1000) << ""DEPRECATED: ForwardPrefilled() ""
    ^
/usr/local/include/glog/logging.h:917:30: note: expanded from macro 'LOG_EVERY_N'
                             INVALID_REQUESTED_LOG_SEVERITY);           \
                             ^
/usr/local/include/glog/logging.h:912:73: note: expanded from macro 'GOOGLE_GLOG_COMPILE_ASSERT'
  typedef google::glog_internal_namespace_::CompileAssert<(bool(expr))> msg[bool(expr) ? 1 : -1]
                                                                        ^
1 warning generated.
CXX src/caffe/solvers/nesterov_solver.cpp
In file included from src/caffe/solvers/nesterov_solver.cpp:3:
In file included from ./include/caffe/sgd_solvers.hpp:7:
In file included from ./include/caffe/solver.hpp:7:
./include/caffe/net.hpp:42:5: warning: unused typedef 'INVALID_REQUESTED_LOG_SEVERITY' [-Wunused-local-typedef]
    LOG_EVERY_N(WARNING, 1000) << ""DEPRECATED: ForwardPrefilled() ""
    ^
/usr/local/include/glog/logging.h:917:30: note: expanded from macro 'LOG_EVERY_N'
                             INVALID_REQUESTED_LOG_SEVERITY);           \
                             ^
/usr/local/include/glog/logging.h:912:73: note: expanded from macro 'GOOGLE_GLOG_COMPILE_ASSERT'
  typedef google::glog_internal_namespace_::CompileAssert<(bool(expr))> msg[bool(expr) ? 1 : -1]
                                                                        ^
1 warning generated.
CXX src/caffe/solvers/rmsprop_solver.cpp
In file included from src/caffe/solvers/rmsprop_solver.cpp:3:
In file included from ./include/caffe/sgd_solvers.hpp:7:
In file included from ./include/caffe/solver.hpp:7:
./include/caffe/net.hpp:42:5: warning: unused typedef 'INVALID_REQUESTED_LOG_SEVERITY' [-Wunused-local-typedef]
    LOG_EVERY_N(WARNING, 1000) << ""DEPRECATED: ForwardPrefilled() ""
    ^
/usr/local/include/glog/logging.h:917:30: note: expanded from macro 'LOG_EVERY_N'
                             INVALID_REQUESTED_LOG_SEVERITY);           \
                             ^
/usr/local/include/glog/logging.h:912:73: note: expanded from macro 'GOOGLE_GLOG_COMPILE_ASSERT'
  typedef google::glog_internal_namespace_::CompileAssert<(bool(expr))> msg[bool(expr) ? 1 : -1]
                                                                        ^
1 warning generated.
CXX src/caffe/solvers/sgd_solver.cpp
In file included from src/caffe/solvers/sgd_solver.cpp:4:
In file included from ./include/caffe/sgd_solvers.hpp:7:
In file included from ./include/caffe/solver.hpp:7:
./include/caffe/net.hpp:42:5: warning: unused typedef 'INVALID_REQUESTED_LOG_SEVERITY' [-Wunused-local-typedef]
    LOG_EVERY_N(WARNING, 1000) << ""DEPRECATED: ForwardPrefilled() ""
    ^
/usr/local/include/glog/logging.h:917:30: note: expanded from macro 'LOG_EVERY_N'
                             INVALID_REQUESTED_LOG_SEVERITY);           \
                             ^
/usr/local/include/glog/logging.h:912:73: note: expanded from macro 'GOOGLE_GLOG_COMPILE_ASSERT'
  typedef google::glog_internal_namespace_::CompileAssert<(bool(expr))> msg[bool(expr) ? 1 : -1]
                                                                        ^

1 warning generated.
CXX src/caffe/syncedmem.cpp
CXX src/caffe/util/benchmark.cpp
CXX src/caffe/util/blocking_queue.cpp
In file included from src/caffe/util/blocking_queue.cpp:6:
In file included from ./include/caffe/parallel.hpp:13:
In file included from ./include/caffe/solver.hpp:7:
./include/caffe/net.hpp:42:5: warning: unused typedef 'INVALID_REQUESTED_LOG_SEVERITY' [-Wunused-local-typedef]
    LOG_EVERY_N(WARNING, 1000) << ""DEPRECATED: ForwardPrefilled() ""
    ^
/usr/local/include/glog/logging.h:917:30: note: expanded from macro 'LOG_EVERY_N'
                             INVALID_REQUESTED_LOG_SEVERITY);           \
                             ^
/usr/local/include/glog/logging.h:912:73: note: expanded from macro 'GOOGLE_GLOG_COMPILE_ASSERT'
  typedef google::glog_internal_namespace_::CompileAssert<(bool(expr))> msg[bool(expr) ? 1 : -1]
                                                                        ^
src/caffe/util/blocking_queue.cpp:50:7: warning: unused typedef 'INVALID_REQUESTED_LOG_SEVERITY' [-Wunused-local-typedef]
      LOG_EVERY_N(INFO, 1000)<< log_on_wait;
      ^
/usr/local/include/glog/logging.h:917:30: note: expanded from macro 'LOG_EVERY_N'
                             INVALID_REQUESTED_LOG_SEVERITY);           \
                             ^
/usr/local/include/glog/logging.h:912:73: note: expanded from macro 'GOOGLE_GLOG_COMPILE_ASSERT'
  typedef google::glog_internal_namespace_::CompileAssert<(bool(expr))> msg[bool(expr) ? 1 : -1]
                                                                        ^
2 warnings generated.
CXX src/caffe/util/cudnn.cpp
CXX src/caffe/util/db.cpp
CXX src/caffe/util/db_leveldb.cpp
CXX src/caffe/util/db_lmdb.cpp
CXX src/caffe/util/hdf5.cpp
CXX src/caffe/util/im2col.cpp
CXX src/caffe/util/insert_splits.cpp
CXX src/caffe/util/io.cpp
CXX src/caffe/util/math_functions.cpp
CXX src/caffe/util/signal_handler.cpp
In file included from src/caffe/util/signal_handler.cpp:7:
In file included from ./include/caffe/util/signal_handler.h:5:
In file included from ./include/caffe/solver.hpp:7:
./include/caffe/net.hpp:42:5: warning: unused typedef 'INVALID_REQUESTED_LOG_SEVERITY' [-Wunused-local-typedef]
    LOG_EVERY_N(WARNING, 1000) << ""DEPRECATED: ForwardPrefilled() ""
    ^
/usr/local/include/glog/logging.h:917:30: note: expanded from macro 'LOG_EVERY_N'
                             INVALID_REQUESTED_LOG_SEVERITY);           \
                             ^
/usr/local/include/glog/logging.h:912:73: note: expanded from macro 'GOOGLE_GLOG_COMPILE_ASSERT'
  typedef google::glog_internal_namespace_::CompileAssert<(bool(expr))> msg[bool(expr) ? 1 : -1]
                                                                        ^
1 warning generated.
CXX src/caffe/util/upgrade_proto.cpp
AR -o .build_release/lib/libcaffe.a
/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/ranlib: file: .build_release/lib/libcaffe.a(cudnn_conv_layer.o) has no symbols
/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/ranlib: file: .build_release/lib/libcaffe.a(cudnn_lcn_layer.o) has no symbols
/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/ranlib: file: .build_release/lib/libcaffe.a(cudnn_lrn_layer.o) has no symbols
/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/ranlib: file: .build_release/lib/libcaffe.a(cudnn_pooling_layer.o) has no symbols
/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/ranlib: file: .build_release/lib/libcaffe.a(cudnn_relu_layer.o) has no symbols
/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/ranlib: file: .build_release/lib/libcaffe.a(cudnn_sigmoid_layer.o) has no symbols
/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/ranlib: file: .build_release/lib/libcaffe.a(cudnn_softmax_layer.o) has no symbols
/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/ranlib: file: .build_release/lib/libcaffe.a(cudnn_tanh_layer.o) has no symbols
/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/ranlib: file: .build_release/lib/libcaffe.a(cudnn.o) has no symbols
LD -o .build_release/lib/libcaffe.so.1.0.0-rc3
clang: warning: argument unused during compilation: '-pthread'
CXX tools/caffe.cpp
In file included from tools/caffe.cpp:15:
In file included from ./include/caffe/caffe.hpp:12:
./include/caffe/net.hpp:42:5: warning: unused typedef 'INVALID_REQUESTED_LOG_SEVERITY' [-Wunused-local-typedef]
    LOG_EVERY_N(WARNING, 1000) << ""DEPRECATED: ForwardPrefilled() ""
    ^
/usr/local/include/glog/logging.h:917:30: note: expanded from macro 'LOG_EVERY_N'
                             INVALID_REQUESTED_LOG_SEVERITY);           \
                             ^
/usr/local/include/glog/logging.h:912:73: note: expanded from macro 'GOOGLE_GLOG_COMPILE_ASSERT'
  typedef google::glog_internal_namespace_::CompileAssert<(bool(expr))> msg[bool(expr) ? 1 : -1]
                                                                        ^
1 warning generated.
CXX/LD -o .build_release/tools/caffe.bin
clang: warning: argument unused during compilation: '-pthread'
Undefined symbols for architecture x86_64:
  ""caffe::Net<float>::Forward(float_)"", referenced from:
      test() in caffe.o
      time() in caffe.o
  ""caffe::Net<float>::Net(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, caffe::Phase, int, std::__1::vector<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, std::__1::allocator<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > > > const_, caffe::Net<float> const_)"", referenced from:
      test() in caffe.o
      time() in caffe.o
  ""caffe::P2PSync<float>::Run(std::__1::vector<int, std::__1::allocator<int> > const&)"", referenced from:
      train() in caffe.o
ld: symbol(s) not found for architecture x86_64
clang: error: linker command failed with exit code 1 (use -v to see invocation)
make: *_\* [.build_release/tools/caffe.bin] Error 1

_Even when i tried using cmake i get the following error:_

In file included from /Volumes/Data/caffe/src/caffe/util/signal_handler.cpp:7:
In file included from /Volumes/Data/caffe/include/caffe/util/signal_handler.h:5:
In file included from /Volumes/Data/caffe/include/caffe/solver.hpp:7:
/Volumes/Data/caffe/include/caffe/net.hpp:42:5: warning: unused typedef 'INVALID_REQUESTED_LOG_SEVERITY' [-Wunused-local-typedef]
    LOG_EVERY_N(WARNING, 1000) << ""DEPRECATED: ForwardPrefilled() ""
    ^
/usr/local/include/glog/logging.h:917:30: note: expanded from macro 'LOG_EVERY_N'
                             INVALID_REQUESTED_LOG_SEVERITY);           \
                             ^
/usr/local/include/glog/logging.h:912:73: note: expanded from macro 'GOOGLE_GLOG_COMPILE_ASSERT'
  typedef google::glog_internal_namespace_::CompileAssert<(bool(expr))> msg[bool(expr) ? 1 : -1]
                                                                        ^
1 warning generated.
[ 82%] Building CXX object src/caffe/CMakeFiles/caffe.dir/util/upgrade_proto.cpp.o
[ 82%] Linking CXX shared library ../../lib/libcaffe.dylib
ld: framework not found vecLib
clang: error: linker command failed with exit code 1 (use -v to see invocation)
make[2]: **\* [lib/libcaffe.1.0.0-rc3.dylib] Error 1
make[1]: **\* [src/caffe/CMakeFiles/caffe.dir/all] Error 2
make: **\* [all] Error 2

At this point I am not sure how to proceed or debug the issue from here. Thanks for any help.

Regards
Praveen
",0,Caffe installation failing on OS X 10.11 - Undefined symbols for architecture x86_64:,"Caffe installation failing on OS X 10.11 - Undefined symbols for architecture x86_64: Hi

I see ""Undefined symbols for architecture x86_64:""  error when i try to build Caffe on Mac which has OS X version 10.11.6 running on it. 
I did follow the instructions mentioned here : http://caffe.berkeleyvision.org/install_osx.html, however i still see the error. I even downgraded the version of Xcode from 7.3 to 7.0 as was suggested in one of the blogposts i encountered.

I have the following dependencies installed and I have Cuda version 7.5 installed.
snappy-1.1.3 
leveldb-1.18 
gflags-2.1.2 
glog-0.3.4 
szip-2.1 
lmdb-0.9.14 
homebrew/science/opencv-2.4.13 
protobuf-2.6.1

_I have attached the Makefile and Makefile.config with this email._

> brew install --build-from-source --fresh -vd boost boost-python 
> /usr/local/Library/Homebrew/brew.rb (Formulary::FormulaLoader): loading /usr/local/Library/Taps/homebrew/homebrew-core/Formula/boost.rb
> /usr/local/Library/Homebrew/brew.rb (Formulary::FormulaLoader): loading /usr/local/Library/Taps/homebrew/homebrew-core/Formula/boost-python.rb
> Warning: boost-1.61.0 already installed
> Warning: boost-python-1.61.0 already installed

====================================== make all output ============================================
MacBook-Pro-3:caffe praveenbodigutla$ make all
CXX .build_release/src/caffe/proto/caffe.pb.cc
CXX src/caffe/blob.cpp
CXX src/caffe/common.cpp
CXX src/caffe/data_reader.cpp
CXX src/caffe/data_transformer.cpp
CXX src/caffe/internal_thread.cpp
CXX src/caffe/layer.cpp
CXX src/caffe/layer_factory.cpp
CXX src/caffe/layers/absval_layer.cpp
CXX src/caffe/layers/accuracy_layer.cpp
CXX src/caffe/layers/argmax_layer.cpp
CXX src/caffe/layers/base_conv_layer.cpp
CXX src/caffe/layers/base_data_layer.cpp
CXX src/caffe/layers/batch_norm_layer.cpp
CXX src/caffe/layers/batch_reindex_layer.cpp
CXX src/caffe/layers/bias_layer.cpp
CXX src/caffe/layers/bnll_layer.cpp
CXX src/caffe/layers/concat_layer.cpp
CXX src/caffe/layers/contrastive_loss_layer.cpp
CXX src/caffe/layers/conv_layer.cpp
CXX src/caffe/layers/crop_layer.cpp
In file included from src/caffe/layers/crop_layer.cpp:10:
./include/caffe/net.hpp:42:5: warning: unused typedef 'INVALID_REQUESTED_LOG_SEVERITY' [-Wunused-local-typedef]
    LOG_EVERY_N(WARNING, 1000) << ""DEPRECATED: ForwardPrefilled() ""
    ^
/usr/local/include/glog/logging.h:917:30: note: expanded from macro 'LOG_EVERY_N'
                             INVALID_REQUESTED_LOG_SEVERITY);           \
                             ^
/usr/local/include/glog/logging.h:912:73: note: expanded from macro 'GOOGLE_GLOG_COMPILE_ASSERT'
  typedef google::glog_internal_namespace_::CompileAssert<(bool(expr))> msg[bool(expr) ? 1 : -1]
                                                                        ^
1 warning generated.
CXX src/caffe/layers/cudnn_conv_layer.cpp
CXX src/caffe/layers/cudnn_lcn_layer.cpp
CXX src/caffe/layers/cudnn_lrn_layer.cpp
CXX src/caffe/layers/cudnn_pooling_layer.cpp
CXX src/caffe/layers/cudnn_relu_layer.cpp
CXX src/caffe/layers/cudnn_sigmoid_layer.cpp
CXX src/caffe/layers/cudnn_softmax_layer.cpp
CXX src/caffe/layers/cudnn_tanh_layer.cpp
CXX src/caffe/layers/data_layer.cpp
CXX src/caffe/layers/deconv_layer.cpp
CXX src/caffe/layers/dropout_layer.cpp
CXX src/caffe/layers/dummy_data_layer.cpp
CXX src/caffe/layers/eltwise_layer.cpp
CXX src/caffe/layers/elu_layer.cpp
CXX src/caffe/layers/embed_layer.cpp
CXX src/caffe/layers/euclidean_loss_layer.cpp
CXX src/caffe/layers/exp_layer.cpp
CXX src/caffe/layers/filter_layer.cpp
CXX src/caffe/layers/flatten_layer.cpp
CXX src/caffe/layers/hdf5_data_layer.cpp
CXX src/caffe/layers/hdf5_output_layer.cpp
CXX src/caffe/layers/hinge_loss_layer.cpp
CXX src/caffe/layers/im2col_layer.cpp
CXX src/caffe/layers/image_data_layer.cpp
CXX src/caffe/layers/infogain_loss_layer.cpp
CXX src/caffe/layers/inner_product_layer.cpp
CXX src/caffe/layers/input_layer.cpp
CXX src/caffe/layers/log_layer.cpp
CXX src/caffe/layers/loss_layer.cpp
CXX src/caffe/layers/lrn_layer.cpp
CXX src/caffe/layers/lstm_layer.cpp
In file included from src/caffe/layers/lstm_layer.cpp:8:
In file included from ./include/caffe/layers/lstm_layer.hpp:11:
In file included from ./include/caffe/layers/recurrent_layer.hpp:11:
./include/caffe/net.hpp:42:5: warning: unused typedef 'INVALID_REQUESTED_LOG_SEVERITY' [-Wunused-local-typedef]
    LOG_EVERY_N(WARNING, 1000) << ""DEPRECATED: ForwardPrefilled() ""
    ^
/usr/local/include/glog/logging.h:917:30: note: expanded from macro 'LOG_EVERY_N'
                             INVALID_REQUESTED_LOG_SEVERITY);           \
                             ^
/usr/local/include/glog/logging.h:912:73: note: expanded from macro 'GOOGLE_GLOG_COMPILE_ASSERT'
  typedef google::glog_internal_namespace_::CompileAssert<(bool(expr))> msg[bool(expr) ? 1 : -1]
                                                                        ^
1 warning generated.
CXX src/caffe/layers/lstm_unit_layer.cpp
In file included from src/caffe/layers/lstm_unit_layer.cpp:6:
In file included from ./include/caffe/layers/lstm_layer.hpp:11:
In file included from ./include/caffe/layers/recurrent_layer.hpp:11:
./include/caffe/net.hpp:42:5: warning: unused typedef 'INVALID_REQUESTED_LOG_SEVERITY' [-Wunused-local-typedef]
    LOG_EVERY_N(WARNING, 1000) << ""DEPRECATED: ForwardPrefilled() ""
    ^
/usr/local/include/glog/logging.h:917:30: note: expanded from macro 'LOG_EVERY_N'
                             INVALID_REQUESTED_LOG_SEVERITY);           \
                             ^
/usr/local/include/glog/logging.h:912:73: note: expanded from macro 'GOOGLE_GLOG_COMPILE_ASSERT'
  typedef google::glog_internal_namespace_::CompileAssert<(bool(expr))> msg[bool(expr) ? 1 : -1]
                                                                        ^
1 warning generated.
CXX src/caffe/layers/memory_data_layer.cpp
CXX src/caffe/layers/multinomial_logistic_loss_layer.cpp
CXX src/caffe/layers/mvn_layer.cpp
CXX src/caffe/layers/neuron_layer.cpp
CXX src/caffe/layers/parameter_layer.cpp
CXX src/caffe/layers/pooling_layer.cpp
CXX src/caffe/layers/power_layer.cpp
CXX src/caffe/layers/prelu_layer.cpp
CXX src/caffe/layers/recurrent_layer.cpp
In file included from src/caffe/layers/recurrent_layer.cpp:8:
In file included from ./include/caffe/layers/recurrent_layer.hpp:11:
./include/caffe/net.hpp:42:5: warning: unused typedef 'INVALID_REQUESTED_LOG_SEVERITY' [-Wunused-local-typedef]
    LOG_EVERY_N(WARNING, 1000) << ""DEPRECATED: ForwardPrefilled() ""
    ^
/usr/local/include/glog/logging.h:917:30: note: expanded from macro 'LOG_EVERY_N'
                             INVALID_REQUESTED_LOG_SEVERITY);           \
                             ^
/usr/local/include/glog/logging.h:912:73: note: expanded from macro 'GOOGLE_GLOG_COMPILE_ASSERT'
  typedef google::glog_internal_namespace_::CompileAssert<(bool(expr))> msg[bool(expr) ? 1 : -1]
                                                                        ^
1 warning generated.
CXX src/caffe/layers/reduction_layer.cpp
CXX src/caffe/layers/relu_layer.cpp
CXX src/caffe/layers/reshape_layer.cpp
CXX src/caffe/layers/rnn_layer.cpp
In file included from src/caffe/layers/rnn_layer.cpp:8:
In file included from ./include/caffe/layers/rnn_layer.hpp:11:
In file included from ./include/caffe/layers/recurrent_layer.hpp:11:
./include/caffe/net.hpp:42:5: warning: unused typedef 'INVALID_REQUESTED_LOG_SEVERITY' [-Wunused-local-typedef]
    LOG_EVERY_N(WARNING, 1000) << ""DEPRECATED: ForwardPrefilled() ""
    ^
/usr/local/include/glog/logging.h:917:30: note: expanded from macro 'LOG_EVERY_N'
                             INVALID_REQUESTED_LOG_SEVERITY);           \
                             ^
/usr/local/include/glog/logging.h:912:73: note: expanded from macro 'GOOGLE_GLOG_COMPILE_ASSERT'
  typedef google::glog_internal_namespace_::CompileAssert<(bool(expr))> msg[bool(expr) ? 1 : -1]
                                                                        ^
1 warning generated.
CXX src/caffe/layers/scale_layer.cpp
CXX src/caffe/layers/sigmoid_cross_entropy_loss_layer.cpp
CXX src/caffe/layers/sigmoid_layer.cpp
CXX src/caffe/layers/silence_layer.cpp
CXX src/caffe/layers/slice_layer.cpp
CXX src/caffe/layers/softmax_layer.cpp
CXX src/caffe/layers/softmax_loss_layer.cpp
CXX src/caffe/layers/split_layer.cpp
CXX src/caffe/layers/spp_layer.cpp
CXX src/caffe/layers/tanh_layer.cpp
CXX src/caffe/layers/threshold_layer.cpp
CXX src/caffe/layers/tile_layer.cpp
CXX src/caffe/layers/window_data_layer.cpp
CXX src/caffe/net.cpp
In file included from src/caffe/net.cpp:12:
./include/caffe/net.hpp:42:5: warning: unused typedef 'INVALID_REQUESTED_LOG_SEVERITY' [-Wunused-local-typedef]
    LOG_EVERY_N(WARNING, 1000) << ""DEPRECATED: ForwardPrefilled() ""
    ^
/usr/local/include/glog/logging.h:917:30: note: expanded from macro 'LOG_EVERY_N'
                             INVALID_REQUESTED_LOG_SEVERITY);           \
                             ^
/usr/local/include/glog/logging.h:912:73: note: expanded from macro 'GOOGLE_GLOG_COMPILE_ASSERT'
  typedef google::glog_internal_namespace_::CompileAssert<(bool(expr))> msg[bool(expr) ? 1 : -1]
src/caffe/net.cpp:580:3: warning: unused typedef 'INVALID_REQUESTED_LOG_SEVERITY' [-Wunused-local-typedef]
  LOG_EVERY_N(WARNING, 1000) << ""DEPRECATED: Forward(bottom, loss) ""
  ^
/usr/local/include/glog/logging.h:917:30: note: expanded from macro 'LOG_EVERY_N'
                             INVALID_REQUESTED_LOG_SEVERITY);           \
                             ^
/usr/local/include/glog/logging.h:912:73: note: expanded from macro 'GOOGLE_GLOG_COMPILE_ASSERT'
  typedef google::glog_internal_namespace_::CompileAssert<(bool(expr))> msg[bool(expr) ? 1 : -1]
                                                                        ^
2 warnings generated.
CXX src/caffe/parallel.cpp
In file included from src/caffe/parallel.cpp:12:
In file included from ./include/caffe/caffe.hpp:12:
./include/caffe/net.hpp:42:5: warning: unused typedef 'INVALID_REQUESTED_LOG_SEVERITY' [-Wunused-local-typedef]
    LOG_EVERY_N(WARNING, 1000) << ""DEPRECATED: ForwardPrefilled() ""
    ^
/usr/local/include/glog/logging.h:917:30: note: expanded from macro 'LOG_EVERY_N'
                             INVALID_REQUESTED_LOG_SEVERITY);           \
                             ^
/usr/local/include/glog/logging.h:912:73: note: expanded from macro 'GOOGLE_GLOG_COMPILE_ASSERT'
  typedef google::glog_internal_namespace_::CompileAssert<(bool(expr))> msg[bool(expr) ? 1 : -1]
                                                                        ^
1 warning generated.
CXX src/caffe/solver.cpp
In file included from src/caffe/solver.cpp:6:
In file included from ./include/caffe/solver.hpp:7:
./include/caffe/net.hpp:42:5: warning: unused typedef 'INVALID_REQUESTED_LOG_SEVERITY' [-Wunused-local-typedef]
    LOG_EVERY_N(WARNING, 1000) << ""DEPRECATED: ForwardPrefilled() ""
    ^
/usr/local/include/glog/logging.h:917:30: note: expanded from macro 'LOG_EVERY_N'
                             INVALID_REQUESTED_LOG_SEVERITY);           \
                             ^
/usr/local/include/glog/logging.h:912:73: note: expanded from macro 'GOOGLE_GLOG_COMPILE_ASSERT'
  typedef google::glog_internal_namespace_::CompileAssert<(bool(expr))> msg[bool(expr) ? 1 : -1]
                                                                        ^
1 warning generated.
CXX src/caffe/solvers/adadelta_solver.cpp
In file included from src/caffe/solvers/adadelta_solver.cpp:3:
In file included from ./include/caffe/sgd_solvers.hpp:7:
In file included from ./include/caffe/solver.hpp:7:
./include/caffe/net.hpp:42:5: warning: unused typedef 'INVALID_REQUESTED_LOG_SEVERITY' [-Wunused-local-typedef]
    LOG_EVERY_N(WARNING, 1000) << ""DEPRECATED: ForwardPrefilled() ""
    ^
/usr/local/include/glog/logging.h:917:30: note: expanded from macro 'LOG_EVERY_N'
                             INVALID_REQUESTED_LOG_SEVERITY);           \
                             ^
/usr/local/include/glog/logging.h:912:73: note: expanded from macro 'GOOGLE_GLOG_COMPILE_ASSERT'
  typedef google::glog_internal_namespace_::CompileAssert<(bool(expr))> msg[bool(expr) ? 1 : -1]
                                                                        ^
1 warning generated.
CXX src/caffe/solvers/adagrad_solver.cpp
In file included from src/caffe/solvers/adagrad_solver.cpp:3:
In file included from ./include/caffe/sgd_solvers.hpp:7:
In file included from ./include/caffe/solver.hpp:7:
./include/caffe/net.hpp:42:5: warning: unused typedef 'INVALID_REQUESTED_LOG_SEVERITY' [-Wunused-local-typedef]
    LOG_EVERY_N(WARNING, 1000) << ""DEPRECATED: ForwardPrefilled() ""
    ^
/usr/local/include/glog/logging.h:917:30: note: expanded from macro 'LOG_EVERY_N'
                             INVALID_REQUESTED_LOG_SEVERITY);           \
                             ^
/usr/local/include/glog/logging.h:912:73: note: expanded from macro 'GOOGLE_GLOG_COMPILE_ASSERT'
  typedef google::glog_internal_namespace_::CompileAssert<(bool(expr))> msg[bool(expr) ? 1 : -1]
                                                                        ^
1 warning generated.
CXX src/caffe/solvers/adam_solver.cpp
In file included from src/caffe/solvers/adam_solver.cpp:3:
In file included from ./include/caffe/sgd_solvers.hpp:7:
In file included from ./include/caffe/solver.hpp:7:
./include/caffe/net.hpp:42:5: warning: unused typedef 'INVALID_REQUESTED_LOG_SEVERITY' [-Wunused-local-typedef]
    LOG_EVERY_N(WARNING, 1000) << ""DEPRECATED: ForwardPrefilled() ""
    ^
/usr/local/include/glog/logging.h:917:30: note: expanded from macro 'LOG_EVERY_N'
                             INVALID_REQUESTED_LOG_SEVERITY);           \
                             ^
/usr/local/include/glog/logging.h:912:73: note: expanded from macro 'GOOGLE_GLOG_COMPILE_ASSERT'
  typedef google::glog_internal_namespace_::CompileAssert<(bool(expr))> msg[bool(expr) ? 1 : -1]
                                                                        ^
1 warning generated.
CXX src/caffe/solvers/nesterov_solver.cpp
In file included from src/caffe/solvers/nesterov_solver.cpp:3:
In file included from ./include/caffe/sgd_solvers.hpp:7:
In file included from ./include/caffe/solver.hpp:7:
./include/caffe/net.hpp:42:5: warning: unused typedef 'INVALID_REQUESTED_LOG_SEVERITY' [-Wunused-local-typedef]
    LOG_EVERY_N(WARNING, 1000) << ""DEPRECATED: ForwardPrefilled() ""
    ^
/usr/local/include/glog/logging.h:917:30: note: expanded from macro 'LOG_EVERY_N'
                             INVALID_REQUESTED_LOG_SEVERITY);           \
                             ^
/usr/local/include/glog/logging.h:912:73: note: expanded from macro 'GOOGLE_GLOG_COMPILE_ASSERT'
  typedef google::glog_internal_namespace_::CompileAssert<(bool(expr))> msg[bool(expr) ? 1 : -1]
                                                                        ^
1 warning generated.
CXX src/caffe/solvers/rmsprop_solver.cpp
In file included from src/caffe/solvers/rmsprop_solver.cpp:3:
In file included from ./include/caffe/sgd_solvers.hpp:7:
In file included from ./include/caffe/solver.hpp:7:
./include/caffe/net.hpp:42:5: warning: unused typedef 'INVALID_REQUESTED_LOG_SEVERITY' [-Wunused-local-typedef]
    LOG_EVERY_N(WARNING, 1000) << ""DEPRECATED: ForwardPrefilled() ""
    ^
/usr/local/include/glog/logging.h:917:30: note: expanded from macro 'LOG_EVERY_N'
                             INVALID_REQUESTED_LOG_SEVERITY);           \
                             ^
/usr/local/include/glog/logging.h:912:73: note: expanded from macro 'GOOGLE_GLOG_COMPILE_ASSERT'
  typedef google::glog_internal_namespace_::CompileAssert<(bool(expr))> msg[bool(expr) ? 1 : -1]
                                                                        ^
1 warning generated.
CXX src/caffe/solvers/sgd_solver.cpp
In file included from src/caffe/solvers/sgd_solver.cpp:4:
In file included from ./include/caffe/sgd_solvers.hpp:7:
In file included from ./include/caffe/solver.hpp:7:
./include/caffe/net.hpp:42:5: warning: unused typedef 'INVALID_REQUESTED_LOG_SEVERITY' [-Wunused-local-typedef]
    LOG_EVERY_N(WARNING, 1000) << ""DEPRECATED: ForwardPrefilled() ""
    ^
/usr/local/include/glog/logging.h:917:30: note: expanded from macro 'LOG_EVERY_N'
                             INVALID_REQUESTED_LOG_SEVERITY);           \
                             ^
/usr/local/include/glog/logging.h:912:73: note: expanded from macro 'GOOGLE_GLOG_COMPILE_ASSERT'
  typedef google::glog_internal_namespace_::CompileAssert<(bool(expr))> msg[bool(expr) ? 1 : -1]
                                                                        ^

1 warning generated.
CXX src/caffe/syncedmem.cpp
CXX src/caffe/util/benchmark.cpp
CXX src/caffe/util/blocking_queue.cpp
In file included from src/caffe/util/blocking_queue.cpp:6:
In file included from ./include/caffe/parallel.hpp:13:
In file included from ./include/caffe/solver.hpp:7:
./include/caffe/net.hpp:42:5: warning: unused typedef 'INVALID_REQUESTED_LOG_SEVERITY' [-Wunused-local-typedef]
    LOG_EVERY_N(WARNING, 1000) << ""DEPRECATED: ForwardPrefilled() ""
    ^
/usr/local/include/glog/logging.h:917:30: note: expanded from macro 'LOG_EVERY_N'
                             INVALID_REQUESTED_LOG_SEVERITY);           \
                             ^
/usr/local/include/glog/logging.h:912:73: note: expanded from macro 'GOOGLE_GLOG_COMPILE_ASSERT'
  typedef google::glog_internal_namespace_::CompileAssert<(bool(expr))> msg[bool(expr) ? 1 : -1]
                                                                        ^
src/caffe/util/blocking_queue.cpp:50:7: warning: unused typedef 'INVALID_REQUESTED_LOG_SEVERITY' [-Wunused-local-typedef]
      LOG_EVERY_N(INFO, 1000)<< log_on_wait;
      ^
/usr/local/include/glog/logging.h:917:30: note: expanded from macro 'LOG_EVERY_N'
                             INVALID_REQUESTED_LOG_SEVERITY);           \
                             ^
/usr/local/include/glog/logging.h:912:73: note: expanded from macro 'GOOGLE_GLOG_COMPILE_ASSERT'
  typedef google::glog_internal_namespace_::CompileAssert<(bool(expr))> msg[bool(expr) ? 1 : -1]
                                                                        ^
2 warnings generated.
CXX src/caffe/util/cudnn.cpp
CXX src/caffe/util/db.cpp
CXX src/caffe/util/db_leveldb.cpp
CXX src/caffe/util/db_lmdb.cpp
CXX src/caffe/util/hdf5.cpp
CXX src/caffe/util/im2col.cpp
CXX src/caffe/util/insert_splits.cpp
CXX src/caffe/util/io.cpp
CXX src/caffe/util/math_functions.cpp
CXX src/caffe/util/signal_handler.cpp
In file included from src/caffe/util/signal_handler.cpp:7:
In file included from ./include/caffe/util/signal_handler.h:5:
In file included from ./include/caffe/solver.hpp:7:
./include/caffe/net.hpp:42:5: warning: unused typedef 'INVALID_REQUESTED_LOG_SEVERITY' [-Wunused-local-typedef]
    LOG_EVERY_N(WARNING, 1000) << ""DEPRECATED: ForwardPrefilled() ""
    ^
/usr/local/include/glog/logging.h:917:30: note: expanded from macro 'LOG_EVERY_N'
                             INVALID_REQUESTED_LOG_SEVERITY);           \
                             ^
/usr/local/include/glog/logging.h:912:73: note: expanded from macro 'GOOGLE_GLOG_COMPILE_ASSERT'
  typedef google::glog_internal_namespace_::CompileAssert<(bool(expr))> msg[bool(expr) ? 1 : -1]
                                                                        ^
1 warning generated.
CXX src/caffe/util/upgrade_proto.cpp
AR -o .build_release/lib/libcaffe.a
/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/ranlib: file: .build_release/lib/libcaffe.a(cudnn_conv_layer.o) has no symbols
/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/ranlib: file: .build_release/lib/libcaffe.a(cudnn_lcn_layer.o) has no symbols
/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/ranlib: file: .build_release/lib/libcaffe.a(cudnn_lrn_layer.o) has no symbols
/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/ranlib: file: .build_release/lib/libcaffe.a(cudnn_pooling_layer.o) has no symbols
/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/ranlib: file: .build_release/lib/libcaffe.a(cudnn_relu_layer.o) has no symbols
/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/ranlib: file: .build_release/lib/libcaffe.a(cudnn_sigmoid_layer.o) has no symbols
/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/ranlib: file: .build_release/lib/libcaffe.a(cudnn_softmax_layer.o) has no symbols
/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/ranlib: file: .build_release/lib/libcaffe.a(cudnn_tanh_layer.o) has no symbols
/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/ranlib: file: .build_release/lib/libcaffe.a(cudnn.o) has no symbols
LD -o .build_release/lib/libcaffe.so.1.0.0-rc3
clang: warning: argument unused during compilation: '-pthread'
CXX tools/caffe.cpp
In file included from tools/caffe.cpp:15:
In file included from ./include/caffe/caffe.hpp:12:
./include/caffe/net.hpp:42:5: warning: unused typedef 'INVALID_REQUESTED_LOG_SEVERITY' [-Wunused-local-typedef]
    LOG_EVERY_N(WARNING, 1000) << ""DEPRECATED: ForwardPrefilled() ""
    ^
/usr/local/include/glog/logging.h:917:30: note: expanded from macro 'LOG_EVERY_N'
                             INVALID_REQUESTED_LOG_SEVERITY);           \
                             ^
/usr/local/include/glog/logging.h:912:73: note: expanded from macro 'GOOGLE_GLOG_COMPILE_ASSERT'
  typedef google::glog_internal_namespace_::CompileAssert<(bool(expr))> msg[bool(expr) ? 1 : -1]
                                                                        ^
1 warning generated.
CXX/LD -o .build_release/tools/caffe.bin
clang: warning: argument unused during compilation: '-pthread'
Undefined symbols for architecture x86_64:
  ""caffe::Net<float>::Forward(float_)"", referenced from:
      test() in caffe.o
      time() in caffe.o
  ""caffe::Net<float>::Net(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, caffe::Phase, int, std::__1::vector<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, std::__1::allocator<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > > > const_, caffe::Net<float> const_)"", referenced from:
      test() in caffe.o
      time() in caffe.o
  ""caffe::P2PSync<float>::Run(std::__1::vector<int, std::__1::allocator<int> > const&)"", referenced from:
      train() in caffe.o
ld: symbol(s) not found for architecture x86_64
clang: error: linker command failed with exit code 1 (use -v to see invocation)
make: *_\* [.build_release/tools/caffe.bin] Error 1

_Even when i tried using cmake i get the following error:_

In file included from /Volumes/Data/caffe/src/caffe/util/signal_handler.cpp:7:
In file included from /Volumes/Data/caffe/include/caffe/util/signal_handler.h:5:
In file included from /Volumes/Data/caffe/include/caffe/solver.hpp:7:
/Volumes/Data/caffe/include/caffe/net.hpp:42:5: warning: unused typedef 'INVALID_REQUESTED_LOG_SEVERITY' [-Wunused-local-typedef]
    LOG_EVERY_N(WARNING, 1000) << ""DEPRECATED: ForwardPrefilled() ""
    ^
/usr/local/include/glog/logging.h:917:30: note: expanded from macro 'LOG_EVERY_N'
                             INVALID_REQUESTED_LOG_SEVERITY);           \
                             ^
/usr/local/include/glog/logging.h:912:73: note: expanded from macro 'GOOGLE_GLOG_COMPILE_ASSERT'
  typedef google::glog_internal_namespace_::CompileAssert<(bool(expr))> msg[bool(expr) ? 1 : -1]
                                                                        ^
1 warning generated.
[ 82%] Building CXX object src/caffe/CMakeFiles/caffe.dir/util/upgrade_proto.cpp.o
[ 82%] Linking CXX shared library ../../lib/libcaffe.dylib
ld: framework not found vecLib
clang: error: linker command failed with exit code 1 (use -v to see invocation)
make[2]: **\* [lib/libcaffe.1.0.0-rc3.dylib] Error 1
make[1]: **\* [src/caffe/CMakeFiles/caffe.dir/all] Error 2
make: **\* [all] Error 2

At this point I am not sure how to proceed or debug the issue from here. Thanks for any help.

Regards
Praveen
"
caffe,151,"It's frustrating that to write docs, one needs to switch into gh-pages branch (or have a separate filesystem folder with that branch checked out). We propose to have all docs in Markdown format in new docs/ directory, with a script to push the contents to gh-pages.

After this is done, we will disable the github wiki.
",0,Assemble all docs and examples from gh-pages and wiki into docs/ directory,"Assemble all docs and examples from gh-pages and wiki into docs/ directory It's frustrating that to write docs, one needs to switch into gh-pages branch (or have a separate filesystem folder with that branch checked out). We propose to have all docs in Markdown format in new docs/ directory, with a script to push the contents to gh-pages.

After this is done, we will disable the github wiki.
"
caffe,4164,"I am trying caffe on Centos Server. 

I wrote 
# glog

wget https://google-glog.googlecode.com/files/glog-0.3.3.tar.gz
tar zxvf glog-0.3.3.tar.gz
cd glog-0.3.3
./configure
make && make install
# gflags

wget https://github.com/schuhschuh/gflags/archive/master.zip
unzip master.zip
cd gflags-master
mkdir build && cd build
export CXXFLAGS=""-fPIC"" && cmake .. && make VERBOSE=1
make && make install
.
I received cafe_intsall.caffe 36 error.

What is problem. Could you please say a solution
",0,cafe_intsall.caffe 36 error,"cafe_intsall.caffe 36 error I am trying caffe on Centos Server. 

I wrote 
# glog

wget https://google-glog.googlecode.com/files/glog-0.3.3.tar.gz
tar zxvf glog-0.3.3.tar.gz
cd glog-0.3.3
./configure
make && make install
# gflags

wget https://github.com/schuhschuh/gflags/archive/master.zip
unzip master.zip
cd gflags-master
mkdir build && cd build
export CXXFLAGS=""-fPIC"" && cmake .. && make VERBOSE=1
make && make install
.
I received cafe_intsall.caffe 36 error.

What is problem. Could you please say a solution
"
caffe,2487,"Hi,

I want to contribute a simple C++ prediction example to the codebase, is this something you are interested in adding? If yes, what kind of features would you like to see in this sample app?

From my point of view, I think the sample application should take as input the following files: deploy file, trained weights, labels, mean file and input image to classify.
It would be nice to be able to use file  as-is, without requiring to add an extra layer for preprocessing (like cropping, resizing, mean subtraction), so I think the processing should be done from the sample app after probing the network for the dimensions of its input layer. This way, the app should hopefully work with caffenet and MNIST by just changing the input files (and those files are either already present in the repo, or are downloadable through a provided script).

What do you think? Any other feature request? I'm already excluding features like image batching or oversampling from the list, for the sake of readability.
",0,Simple C++ prediction example,"Simple C++ prediction example Hi,

I want to contribute a simple C++ prediction example to the codebase, is this something you are interested in adding? If yes, what kind of features would you like to see in this sample app?

From my point of view, I think the sample application should take as input the following files: deploy file, trained weights, labels, mean file and input image to classify.
It would be nice to be able to use file  as-is, without requiring to add an extra layer for preprocessing (like cropping, resizing, mean subtraction), so I think the processing should be done from the sample app after probing the network for the dimensions of its input layer. This way, the app should hopefully work with caffenet and MNIST by just changing the input files (and those files are either already present in the repo, or are downloadable through a provided script).

What do you think? Any other feature request? I'm already excluding features like image batching or oversampling from the list, for the sake of readability.
"
caffe,4508,,0,the error is CXX/LD -o .build_release/test/test_all.testbin src/caffe/test/test_caffe_main.cpp .build_release/lib/libcaffe.so: undefined reference to `fLS::FLAGS_step' collect2: error: ld returned 1 exit statuswhat should i do,
caffe,1967,"Hi, all,

I had followed the install steps in page, the caffe library can run successfully, but fpycaffeor  , I came across the following issue. Did someone come across this issue before and how to fix it?  THX!
My system is  ubuntu 12.04 64bit, I had installed anacorda, and set $PYTHONPATH as:
export PYTHONPATH=""$CAFFE_ROOT/python:$PYTHONPATH""
# 

root@milton-Desktop:~# python ~/Downloads/caffe-public-bvlc_googlenet/python/classify.py
Traceback (most recent call last):
  File ""/root/Downloads/caffe-public-bvlc_googlenet/python/classify.py"", line 14, in <module>
    import caffe
  File ""/root/Downloads/caffe-public-bvlc_googlenet/python/caffe/**init**.py"", line 1, in <module>
    from .pycaffe import Net, SGDSolver
  File ""/root/Downloads/caffe-public-bvlc_googlenet/python/caffe/pycaffe.py"", line 10, in <module>
    from ._caffe import Net, SGDSolver
ImportError: /root/anaconda/bin/../lib/libm.so.6: version `GLIBC_2.15' not found (required by /usr/lib/libopencv_core.so.2.3)
root@milton-Desktop:~# 
# 
",0,issue in pycaffe,"issue in pycaffe Hi, all,

I had followed the install steps in page, the caffe library can run successfully, but fpycaffeor  , I came across the following issue. Did someone come across this issue before and how to fix it?  THX!
My system is  ubuntu 12.04 64bit, I had installed anacorda, and set $PYTHONPATH as:
export PYTHONPATH=""$CAFFE_ROOT/python:$PYTHONPATH""
# 

root@milton-Desktop:~# python ~/Downloads/caffe-public-bvlc_googlenet/python/classify.py
Traceback (most recent call last):
  File ""/root/Downloads/caffe-public-bvlc_googlenet/python/classify.py"", line 14, in <module>
    import caffe
  File ""/root/Downloads/caffe-public-bvlc_googlenet/python/caffe/**init**.py"", line 1, in <module>
    from .pycaffe import Net, SGDSolver
  File ""/root/Downloads/caffe-public-bvlc_googlenet/python/caffe/pycaffe.py"", line 10, in <module>
    from ._caffe import Net, SGDSolver
ImportError: /root/anaconda/bin/../lib/libm.so.6: version `GLIBC_2.15' not found (required by /usr/lib/libopencv_core.so.2.3)
root@milton-Desktop:~# 
# 
"
caffe,1294,"Hi, I am trying execute ""make runtest"" and met some problems.
The caffe is successfully compiled on Ubuntu12.04 with CPU only.
When executing the testcases, it shows like this:

src/caffe/test/test_math_functions.cpp:109: Failure
Value of: x[i] < 0 ? 1 : 0
  Actual: 1
Expected: signbits[i]
Which is: 512
..................
[  PASSED  ] 450 tests.
[  FAILED  ] 7 tests, listed below:
[  FAILED  ] MathFunctionsTest/0.TestSgnbitCPU, where TypeParam = float
[  FAILED  ] MathFunctionsTest/1.TestSgnbitCPU, where TypeParam = double
[  FAILED  ] MathFunctionsTest/1.TestHammingDistanceCPU, where TypeParam = double
[  FAILED  ] EltwiseLayerTest/0.TestProd, where TypeParam = caffe::FloatCPU
[  FAILED  ] EltwiseLayerTest/0.TestSum, where TypeParam = caffe::FloatCPU
[  FAILED  ] EltwiseLayerTest/1.TestProd, where TypeParam = caffe::DoubleCPU
[  FAILED  ] EltwiseLayerTest/1.TestSum, where TypeParam = caffe::DoubleCPU

I am a new learner of linux and caffe. Great thanks for any help.
",0,Testcase failed during make runtest,"Testcase failed during make runtest Hi, I am trying execute ""make runtest"" and met some problems.
The caffe is successfully compiled on Ubuntu12.04 with CPU only.
When executing the testcases, it shows like this:

src/caffe/test/test_math_functions.cpp:109: Failure
Value of: x[i] < 0 ? 1 : 0
  Actual: 1
Expected: signbits[i]
Which is: 512
..................
[  PASSED  ] 450 tests.
[  FAILED  ] 7 tests, listed below:
[  FAILED  ] MathFunctionsTest/0.TestSgnbitCPU, where TypeParam = float
[  FAILED  ] MathFunctionsTest/1.TestSgnbitCPU, where TypeParam = double
[  FAILED  ] MathFunctionsTest/1.TestHammingDistanceCPU, where TypeParam = double
[  FAILED  ] EltwiseLayerTest/0.TestProd, where TypeParam = caffe::FloatCPU
[  FAILED  ] EltwiseLayerTest/0.TestSum, where TypeParam = caffe::FloatCPU
[  FAILED  ] EltwiseLayerTest/1.TestProd, where TypeParam = caffe::DoubleCPU
[  FAILED  ] EltwiseLayerTest/1.TestSum, where TypeParam = caffe::DoubleCPU

I am a new learner of linux and caffe. Great thanks for any help.
"
caffe,3522,"first I use the imagenet framework to train my dataset, the data have 11 classes, so I change the output_num and the corresponding path , the result of training-processing is done with none error. 
Then I use the  caffenet_train_iter_6000.caffemodelmean.binaryprotodeploy.prototxt to initial the 
caffe.There is a error come out after ""Network initialization done. Memory required data:62433600. ""
The error is :
""Check failed: target_blobs[j]->num() == source_layer.blobs(j).num() (96 vs. 0)""

If I use the ""bvlc_reference_caffenet.caffemodel"" and change the output_num , the classify is success!

How can I do with the error?
",0,"matlab interface use my train_caffemodel for image classification!""Check failed: target_blobs[j]->num() == source_layer.blobs(j).num() (96 vs. 0)""","matlab interface use my train_caffemodel for image classification!""Check failed: target_blobs[j]->num() == source_layer.blobs(j).num() (96 vs. 0)"" first I use the imagenet framework to train my dataset, the data have 11 classes, so I change the output_num and the corresponding path , the result of training-processing is done with none error. 
Then I use the  caffenet_train_iter_6000.caffemodelmean.binaryprotodeploy.prototxt to initial the 
caffe.There is a error come out after ""Network initialization done. Memory required data:62433600. ""
The error is :
""Check failed: target_blobs[j]->num() == source_layer.blobs(j).num() (96 vs. 0)""

If I use the ""bvlc_reference_caffenet.caffemodel"" and change the output_num , the classify is success!

How can I do with the error?
"
caffe,1513,"As I try to define a autoencoder by using my custom data loaded in to HDF5, I see that we must define top: ""label"" in any way so it forces you to define a dummy set of label values for instances. Although, it does not cause any problem in optimization, it causes very disturbing set of console outputs as 

Train net output #1: label=0
Train net output #2: label=0
Train net output #3: label=0
Train net output #4: label=0
... (goes to number of your data dimension)

I solved this by only inserting this to data_layers.hpp line 184

virtual inline bool AutoTopBlobs() const { return true; }

I don't know whether it is acceptable solution but in practice it is pretty fine. If it is not a convenient solution please make HDF5DataLayer able to accept no label configuration.
",0,Allow no label setting for HDF5DataLayer,"Allow no label setting for HDF5DataLayer As I try to define a autoencoder by using my custom data loaded in to HDF5, I see that we must define top: ""label"" in any way so it forces you to define a dummy set of label values for instances. Although, it does not cause any problem in optimization, it causes very disturbing set of console outputs as 

Train net output #1: label=0
Train net output #2: label=0
Train net output #3: label=0
Train net output #4: label=0
... (goes to number of your data dimension)

I solved this by only inserting this to data_layers.hpp line 184

virtual inline bool AutoTopBlobs() const { return true; }

I don't know whether it is acceptable solution but in practice it is pretty fine. If it is not a convenient solution please make HDF5DataLayer able to accept no label configuration.
"
caffe,2549,"In the training examples provided with Caffe (imagenet or cifar), is there a way to stop the training based on some preset convergence criteria like value of the loss function or accuracy on validation data ? How and where to specify these threshold in the prototxt files ?
",0,Preset Convergence Criteria,"Preset Convergence Criteria In the training examples provided with Caffe (imagenet or cifar), is there a way to stop the training based on some preset convergence criteria like value of the loss function or accuracy on validation data ? How and where to specify these threshold in the prototxt files ?
"
caffe,3269,"I have a question:
Assume my dataset has 10000 images and I set the value of 'batch_size' as 100. Then after 100 iterations, all images in the dataset has been used to update net weights. But the value of 'max_iter' is set as a bigger number, e.g. max_iter = 200. I want to know how does the solver choose images to update weights during iteration from 101 to 200.
Is anyone knowing about this?
thx~~
",0,how does the solver execute iteration?,"how does the solver execute iteration? I have a question:
Assume my dataset has 10000 images and I set the value of 'batch_size' as 100. Then after 100 iterations, all images in the dataset has been used to update net weights. But the value of 'max_iter' is set as a bigger number, e.g. max_iter = 200. I want to know how does the solver choose images to update weights during iteration from 101 to 200.
Is anyone knowing about this?
thx~~
"
caffe,4181,"i run a training for about 40k iterations.
than i stopped it and tried to train again (with lower lr) from snapshot of iteration 17k.

command :


i used the same solver.prototxt i used in the first training phase, except for changing base_lr from 0.001 to 0.0001 (lr_policy: ""fixed"").

I get the following error :


my suspect is that caffe can't find the "".solverstate"" file, although it's there right next to the "".caffemodel"" snapshot file.
",0,failed to start training from snapshot,"failed to start training from snapshot i run a training for about 40k iterations.
than i stopped it and tried to train again (with lower lr) from snapshot of iteration 17k.

command :


i used the same solver.prototxt i used in the first training phase, except for changing base_lr from 0.001 to 0.0001 (lr_policy: ""fixed"").

I get the following error :


my suspect is that caffe can't find the "".solverstate"" file, although it's there right next to the "".caffemodel"" snapshot file.
"
caffe,896,"I am trying to use classify.py to classify a test image. However, irrespective of the image or model used, the predictions output always has max. prob for class index 111. The probability values keep changing but out of 1000 classes, the prediction always seem to contain max. prob for array index 111. I am not sure why this is happening. Has anyone come across an issue like this before?
",0,Unexpected results while running classify.py,"Unexpected results while running classify.py I am trying to use classify.py to classify a test image. However, irrespective of the image or model used, the predictions output always has max. prob for class index 111. The probability values keep changing but out of 1000 classes, the prediction always seem to contain max. prob for array index 111. I am not sure why this is happening. Has anyone come across an issue like this before?
"
caffe,1908,"Hi all, I just want to train the [googleNet](https://github.com/BVLC/caffe/tree/master/models/bvlc_googlenet) and using the image data as input data type (giving the path of the training and validation txt file). I failed to train it because I encountered the following problem: the loss becomes 'nan' from the 80th iteration.



Here is the data layer



Then I turned to Leveldb, everything is OK.  



Did I make any mistakes?
",0,Can I use IMAGE_DATA as input when training GoogleNet?,"Can I use IMAGE_DATA as input when training GoogleNet? Hi all, I just want to train the [googleNet](https://github.com/BVLC/caffe/tree/master/models/bvlc_googlenet) and using the image data as input data type (giving the path of the training and validation txt file). I failed to train it because I encountered the following problem: the loss becomes 'nan' from the 80th iteration.



Here is the data layer



Then I turned to Leveldb, everything is OK.  



Did I make any mistakes?
"
caffe,6137,"Please use the [caffe-users list](https://groups.google.com/forum/#!forum/caffe-users) for usage, installation, or modeling questions, or other requests for help.
_Do not post such requests to Issues._ Doing so interferes with the development of Caffe.

Please read the [guidelines for contributing](https://github.com/BVLC/caffe/blob/master/CONTRIBUTING.md) before submitting this issue.

### Issue summary


### Steps to reproduce

If you are having difficulty building Caffe or training a model, please ask the caffe-users mailing list. If you are reporting a build error that seems to be due to a bug in Caffe, please attach your build configuration (either Makefile.config or CMakeCache.txt) and the output of the make (or cmake) command.

### Your system configuration
Operating system:
Compiler:
CUDA version (if applicable):
CUDNN version (if applicable):
BLAS:
Python or MATLAB version (for pycaffe and matcaffe respectively):
",0,"m,,,,..//","m,,,,..// Please use the [caffe-users list](https://groups.google.com/forum/#!forum/caffe-users) for usage, installation, or modeling questions, or other requests for help.
_Do not post such requests to Issues._ Doing so interferes with the development of Caffe.

Please read the [guidelines for contributing](https://github.com/BVLC/caffe/blob/master/CONTRIBUTING.md) before submitting this issue.

### Issue summary


### Steps to reproduce

If you are having difficulty building Caffe or training a model, please ask the caffe-users mailing list. If you are reporting a build error that seems to be due to a bug in Caffe, please attach your build configuration (either Makefile.config or CMakeCache.txt) and the output of the make (or cmake) command.

### Your system configuration
Operating system:
Compiler:
CUDA version (if applicable):
CUDNN version (if applicable):
BLAS:
Python or MATLAB version (for pycaffe and matcaffe respectively):
"
caffe,3509,"I have a big net with many layers. I add a new full-connection layer in the net and want to do a fine-tuning. However, it's so difficult to set lr_mult=0 in every layer except the new one, since there are many layers in the net. If there is a good way to solve these problem?

Thanks.
",0,How to set learning rate in a big net for fine-tuning?,"How to set learning rate in a big net for fine-tuning? I have a big net with many layers. I add a new full-connection layer in the net and want to do a fine-tuning. However, it's so difficult to set lr_mult=0 in every layer except the new one, since there are many layers in the net. If there is a good way to solve these problem?

Thanks.
"
caffe,3092,"On MacOS 10.10.3 

I had to change 6 for 7 in : 



since pkgutil --pkg-info=com.apple.pkg.CLTools_Executables was giving : 

package-id: com.apple.pkg.CLTools_Executables
version: 7.0.0.0.1.1441394355
volume: /
location: /
install-time: 1442705061
groups: com.apple.FindSystemFiles.pkg-group com.apple.DevToolsBoth.pkg-group com.apple.DevToolsNonRelocatableShared.pkg-group 
",0,Caffe Makefile Command line tools version 7,"Caffe Makefile Command line tools version 7 On MacOS 10.10.3 

I had to change 6 for 7 in : 



since pkgutil --pkg-info=com.apple.pkg.CLTools_Executables was giving : 

package-id: com.apple.pkg.CLTools_Executables
version: 7.0.0.0.1.1441394355
volume: /
location: /
install-time: 1442705061
groups: com.apple.FindSystemFiles.pkg-group com.apple.DevToolsBoth.pkg-group com.apple.DevToolsNonRelocatableShared.pkg-group 
"
caffe,6368,"### I use anaconda3 to install caffe by comand: conda install caffe in virtual env named : myenv with python 3.6.5
When i try  import caffe  in pytnon interpreter i have error: 

>  import caffe
> Traceback (most recent call last):
>   File ""<stdin>"", line 1, in <module>
>   File ""/opt/DL/caffe-ibm/python/caffe/__init__.py"", line 1, in <module>
>     from .pycaffe import Net, SGDSolver, NesterovSolver, AdaGradSolver, RMSPropS olver, AdaDeltaSolver, AdamSolver, NCCL, Timer
>   File ""/opt/DL/caffe-ibm/python/caffe/pycaffe.py"", line 13, in <module>
>     from _caffe import Net, SGDSolver, NesterovSolver, AdaGradSolver, \
> ModuleNotFoundError: No module named '_caffe
> 

my ~/.bashrc file looks like :
#.bashrc
#Source global definitions
rt PATH=""/home/prztszeliga/anaconda3/bin:$PATH""
if [ -f /etc/bashrc ]; then
        . /etc/bashrc
fi

#Uncomment the following line if you don't like systemctl's auto-paging feature:
#export SYSTEMD_PAGER=
#User specific aliases and functions
#added by Anaconda3 installer
export PATH=""/home/prztszeliga/anaconda3/bin:$PATH""
export CAFFE_ROOT=""/home/prztszeliga/anaconda3/envs/myenv/lib/python3.6/site-packages/caffe""
export PYTHONPATH=""anaconda3/envs/myenv/lib/python3.6""",0,ModuleNotFoundError: No module named '_caffe' ,"ModuleNotFoundError: No module named '_caffe'  ### I use anaconda3 to install caffe by comand: conda install caffe in virtual env named : myenv with python 3.6.5
When i try  import caffe  in pytnon interpreter i have error: 

>  import caffe
> Traceback (most recent call last):
>   File ""<stdin>"", line 1, in <module>
>   File ""/opt/DL/caffe-ibm/python/caffe/__init__.py"", line 1, in <module>
>     from .pycaffe import Net, SGDSolver, NesterovSolver, AdaGradSolver, RMSPropS olver, AdaDeltaSolver, AdamSolver, NCCL, Timer
>   File ""/opt/DL/caffe-ibm/python/caffe/pycaffe.py"", line 13, in <module>
>     from _caffe import Net, SGDSolver, NesterovSolver, AdaGradSolver, \
> ModuleNotFoundError: No module named '_caffe
> 

my ~/.bashrc file looks like :
#.bashrc
#Source global definitions
rt PATH=""/home/prztszeliga/anaconda3/bin:$PATH""
if [ -f /etc/bashrc ]; then
        . /etc/bashrc
fi

#Uncomment the following line if you don't like systemctl's auto-paging feature:
#export SYSTEMD_PAGER=
#User specific aliases and functions
#added by Anaconda3 installer
export PATH=""/home/prztszeliga/anaconda3/bin:$PATH""
export CAFFE_ROOT=""/home/prztszeliga/anaconda3/envs/myenv/lib/python3.6/site-packages/caffe""
export PYTHONPATH=""anaconda3/envs/myenv/lib/python3.6"""
caffe,115,"Caffe is a really good and simple deep neural network framework. I think its config file can support other type neural network, such as deep feed-forward neural network. 

Can anyone give me some examples for that? for example. I want to build a deep feed-forward neural network. It's structure is data input - 2000 - 1000 - 500 - 128 - 64 output, the inner hidden layers are sigmoid or relu. 

Thanks for your help :)
",0,network structure definition,"network structure definition Caffe is a really good and simple deep neural network framework. I think its config file can support other type neural network, such as deep feed-forward neural network. 

Can anyone give me some examples for that? for example. I want to build a deep feed-forward neural network. It's structure is data input - 2000 - 1000 - 500 - 128 - 64 output, the inner hidden layers are sigmoid or relu. 

Thanks for your help :)
"
caffe,1485,"Suppose I have 1 GPU and light network.
Is it possible to process several pictures in parallel in test phase on one GPU?
",0,Can Caffe run several parallel processing task on one GPU?,"Can Caffe run several parallel processing task on one GPU? Suppose I have 1 GPU and light network.
Is it possible to process several pictures in parallel in test phase on one GPU?
"
caffe,2050,"Caffe just outputs these results on the screen, but I don't know how to save these results into files for further use.
",0,"How to save training loss, training accuracy and test accuracy into .txt files?","How to save training loss, training accuracy and test accuracy into .txt files? Caffe just outputs these results on the screen, but I don't know how to save these results into files for further use.
"
caffe,5175,"### Issue summary

The shared object seems to be not correctly linked or does not find a symbol after it has been compiled successfully.

### Steps to reproduce

compile it using cmake:

copy the python code into its include path so it is visible to applications


### Your system configuration
Operating system: CentOS 6
Compiler: gcc (GCC) 4.4.7 20120313 (Red Hat 4.4.7-17)
",0,Build succeed but cannot run because of an issue with the shared object,"Build succeed but cannot run because of an issue with the shared object ### Issue summary

The shared object seems to be not correctly linked or does not find a symbol after it has been compiled successfully.

### Steps to reproduce

compile it using cmake:

copy the python code into its include path so it is visible to applications


### Your system configuration
Operating system: CentOS 6
Compiler: gcc (GCC) 4.4.7 20120313 (Red Hat 4.4.7-17)
"
caffe,2525,"Hi All,
We are trying to implement the Cross Input Neighbourhood difference layer as described in http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Ahmed_An_Improved_Deep_2015_CVPR_paper.pdf in caffe.

Two input blobs (features from two different images generated by conv and pooling) are used to compute this feature. A 5x5 neighbourhood of one pixel (f) channel (for each pixel) is subtracted from  the corresponding channel from the other blob (g). To subtract the 5x5 pixels form f the corresponding g pixel value is repeated in a 5x5 patch. This outputs a 5x5 patch for each input pixel. Thereafter the role of f and g are reversed and the operation is repeated and concatenated in the channel dimension. Hence for two WxHxC images the output is of the size 5Wx5Hx2C, where W, H and C are width, height and channels respectively. 

For simplicity the following is the corresponding matlab code for the operation
[inputs : f, g each of which are of size MxMxC, for simplicity C=1]
image_size=size(f);
kernel_size=5;
pad_size=floor((kernel_size-1)/2);

G=padarray(g,[pad_size,pad_size],'replicate');
F=padarray(f,[pad_size,pad_size],'replicate');
%%%%%%%%%%%%%%%%%%%%%%%%%%
%im2col to get neighbouring kernels into a matrix
%%%%%%%%%%%%%%%%%%%%%%%%%%

fcol=im2col(F,[kernel_size,kernel_size],'sliding');
gcol=im2col(G,[kernel_size,kernel_size],'sliding');

%%%%%%%%%%%%%%%%%%%%%%%%%%
%Each pixel is repeated to the size of the Kernel 
%%%%%%%%%%%%%%%%%%%%%%%%%%

fres=repmat(reshape(f,[1,image_size_image_size]),[kernel_size_kernel_size,1]);
gres=repmat(reshape(g,[1,image_size_image_size]),[kernel_size_kernel_size,1]);

%%%%%%%%%%%%%%%%%%%%
%Subtract g neighbourhoods from f
%%%%%%%%%%%%%%%%%%%%

c1=fres-gcol;
Res2(:,:,1)=col2im(c1,[kernel_size,kernel_size],[image_size_kernel_size,image_size_kernel_size],'distinct');

%%%%%%%%%%%%%%%%%%%%
%Subtract f neighbourhoods from g
%%%%%%%%%%%%%%%%%%%%

c2=gres-fcol;
Res2(:,:,2)=col2im(c2,[kernel_size,kernel_size],[image_size_kernel_size,image_size_kernel_size],'distinct');

%Res2 is the output

While this is straigtforward in matlab, we were wondering how to implement the repmat in caffe. Furthermore since the layer has no parameters, it will not have any diffs. However for the backward pass how do we propagate the gradients from the layers that follow to the layers that preceed it. This layer has proven to be very useful for person reidentification and it might be used in future for new face verification networks as well. 
",0,Implement Cross Input Neighbourhood differences Layer for Reidentification,"Implement Cross Input Neighbourhood differences Layer for Reidentification Hi All,
We are trying to implement the Cross Input Neighbourhood difference layer as described in http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Ahmed_An_Improved_Deep_2015_CVPR_paper.pdf in caffe.

Two input blobs (features from two different images generated by conv and pooling) are used to compute this feature. A 5x5 neighbourhood of one pixel (f) channel (for each pixel) is subtracted from  the corresponding channel from the other blob (g). To subtract the 5x5 pixels form f the corresponding g pixel value is repeated in a 5x5 patch. This outputs a 5x5 patch for each input pixel. Thereafter the role of f and g are reversed and the operation is repeated and concatenated in the channel dimension. Hence for two WxHxC images the output is of the size 5Wx5Hx2C, where W, H and C are width, height and channels respectively. 

For simplicity the following is the corresponding matlab code for the operation
[inputs : f, g each of which are of size MxMxC, for simplicity C=1]
image_size=size(f);
kernel_size=5;
pad_size=floor((kernel_size-1)/2);

G=padarray(g,[pad_size,pad_size],'replicate');
F=padarray(f,[pad_size,pad_size],'replicate');
%%%%%%%%%%%%%%%%%%%%%%%%%%
%im2col to get neighbouring kernels into a matrix
%%%%%%%%%%%%%%%%%%%%%%%%%%

fcol=im2col(F,[kernel_size,kernel_size],'sliding');
gcol=im2col(G,[kernel_size,kernel_size],'sliding');

%%%%%%%%%%%%%%%%%%%%%%%%%%
%Each pixel is repeated to the size of the Kernel 
%%%%%%%%%%%%%%%%%%%%%%%%%%

fres=repmat(reshape(f,[1,image_size_image_size]),[kernel_size_kernel_size,1]);
gres=repmat(reshape(g,[1,image_size_image_size]),[kernel_size_kernel_size,1]);

%%%%%%%%%%%%%%%%%%%%
%Subtract g neighbourhoods from f
%%%%%%%%%%%%%%%%%%%%

c1=fres-gcol;
Res2(:,:,1)=col2im(c1,[kernel_size,kernel_size],[image_size_kernel_size,image_size_kernel_size],'distinct');

%%%%%%%%%%%%%%%%%%%%
%Subtract f neighbourhoods from g
%%%%%%%%%%%%%%%%%%%%

c2=gres-fcol;
Res2(:,:,2)=col2im(c2,[kernel_size,kernel_size],[image_size_kernel_size,image_size_kernel_size],'distinct');

%Res2 is the output

While this is straigtforward in matlab, we were wondering how to implement the repmat in caffe. Furthermore since the layer has no parameters, it will not have any diffs. However for the backward pass how do we propagate the gradients from the layers that follow to the layers that preceed it. This layer has proven to be very useful for person reidentification and it might be used in future for new face verification networks as well. 
"
caffe,4598,"I'm using Visual Studio 2013 Community to build caffe-windows, I'm not using MATLAB support, Python support, cuDNN. The building process just stuck with the Nuget Package Manager.
I've installed CUDA-7.5 properly.
",0,Nuget doesn't work,"Nuget doesn't work I'm using Visual Studio 2013 Community to build caffe-windows, I'm not using MATLAB support, Python support, cuDNN. The building process just stuck with the Nuget Package Manager.
I've installed CUDA-7.5 properly.
"
caffe,3340,"Compiling with R2015b and Apple LLVM version 7.0.0 (clang-700.1.76) gives the following error during linking, anyone else facing the same issue?


",0,Issue in compiling Matlab wrapper,"Issue in compiling Matlab wrapper Compiling with R2015b and Apple LLVM version 7.0.0 (clang-700.1.76) gives the following error during linking, anyone else facing the same issue?


"
caffe,938,"Hello,

Thanks for the great library. I have trying to run filter_visualization.py however importing caffe module has been raising problems. Boost has been built, pycaffe compiled properly and all other examples are running smoothly. However, I am getting the following error related to boostpython:

ImportError: ../python/caffe/_caffe.so: undefined symbol: _ZN5boost6python6detail12gcc_demangleEPKc

Thanks
Nate
",0,Boost Python Problem on import caffe,"Boost Python Problem on import caffe Hello,

Thanks for the great library. I have trying to run filter_visualization.py however importing caffe module has been raising problems. Boost has been built, pycaffe compiled properly and all other examples are running smoothly. However, I am getting the following error related to boostpython:

ImportError: ../python/caffe/_caffe.so: undefined symbol: _ZN5boost6python6detail12gcc_demangleEPKc

Thanks
Nate
"
caffe,962,"May i know if it is possible to use customized loss function in caffe? Also, if I want to change the formula of weight update in the backpropagation, is this allowed in caffe? Could you please give some hints about this? Thanks a lot! 
",0,how to use customized loss function and modify trainning procedure in caffe,"how to use customized loss function and modify trainning procedure in caffe May i know if it is possible to use customized loss function in caffe? Also, if I want to change the formula of weight update in the backpropagation, is this allowed in caffe? Could you please give some hints about this? Thanks a lot! 
"
caffe,3110,"Hello everyone,

I have saved some features in Caffe::DB (database type does not matter) for a number of images, and features for each image are saved with a different key. While retrieving the features from database, I am able to get the data sequentially, but I was wondering if there is a method to randomly access the data corresponding to a specific key. I looked in to the source code for Caffe::DB and Caffe::DB::Cursor, but couldn't find any method for random access of database entries. The cursor can only be placed either to the first entry of database, or to the next. Should not there be a method where one can access database entries with any key??

Thanks! 
",0,Data accessing based on key value from Caffe::DB,"Data accessing based on key value from Caffe::DB Hello everyone,

I have saved some features in Caffe::DB (database type does not matter) for a number of images, and features for each image are saved with a different key. While retrieving the features from database, I am able to get the data sequentially, but I was wondering if there is a method to randomly access the data corresponding to a specific key. I looked in to the source code for Caffe::DB and Caffe::DB::Cursor, but couldn't find any method for random access of database entries. The cursor can only be placed either to the first entry of database, or to the next. Should not there be a method where one can access database entries with any key??

Thanks! 
"
caffe,5096,"### Issue summary
I am just running pycaffe lenet example code (https://github.com/BVLC/caffe/blob/master/examples/01-learning-lenet.ipynb) , and getting 

**[AttributeError: 'LayerParameter' object has no attribute 'weight_filter']**

My code is exactly same with _2. Creating the net - in[4]_ for net part, slightly different for 1. Setup part because of caffe path.

I've found few similar issues and tried with ""convolution_param=dict(weight_filter...)"" thing (someone said it is a quick fix) then I got another error, [AttributeError: 'ConvolutionParameter' object has no attribute 'weight_filter']


_Trace as follows:_
$ python ex_classification.py 
Traceback (most recent call last):
  File ""ex_classification.py"", line 37, in <module>
    lenet('mnist/mnist_train_lmdb', 64)
  File ""ex_classification.py"", line 35, in lenet
    return n.to_proto()
  File ""/home/sunyou/Downloads/caffe-master/python/caffe/net_spec.py"", line 189, in to_proto
    top._to_proto(layers, names, autonames)
  File ""/home/sunyou/Downloads/caffe-master/python/caffe/net_spec.py"", line 97, in _to_proto
    return self.fn._to_proto(layers, names, autonames)
  File ""/home/sunyou/Downloads/caffe-master/python/caffe/net_spec.py"", line 158, in _to_proto
    assign_proto(layer, k, v)
  File ""/home/sunyou/Downloads/caffe-master/python/caffe/net_spec.py"", line 64, in assign_proto
    is_repeated_field = hasattr(getattr(proto, name), 'extend')
AttributeError: 'LayerParameter' object has no attribute 'weight_filter'

running on Ubuntu 14.04 LTS with Python 2.7.6.
caffe-master from git repo, libprotobuf-dev from apt-get (2.5.0-9ubuntu1)",0,getting AttributeError: 'LayerParameter' object has no attribute 'weight_filter',"getting AttributeError: 'LayerParameter' object has no attribute 'weight_filter' ### Issue summary
I am just running pycaffe lenet example code (https://github.com/BVLC/caffe/blob/master/examples/01-learning-lenet.ipynb) , and getting 

**[AttributeError: 'LayerParameter' object has no attribute 'weight_filter']**

My code is exactly same with _2. Creating the net - in[4]_ for net part, slightly different for 1. Setup part because of caffe path.

I've found few similar issues and tried with ""convolution_param=dict(weight_filter...)"" thing (someone said it is a quick fix) then I got another error, [AttributeError: 'ConvolutionParameter' object has no attribute 'weight_filter']


_Trace as follows:_
$ python ex_classification.py 
Traceback (most recent call last):
  File ""ex_classification.py"", line 37, in <module>
    lenet('mnist/mnist_train_lmdb', 64)
  File ""ex_classification.py"", line 35, in lenet
    return n.to_proto()
  File ""/home/sunyou/Downloads/caffe-master/python/caffe/net_spec.py"", line 189, in to_proto
    top._to_proto(layers, names, autonames)
  File ""/home/sunyou/Downloads/caffe-master/python/caffe/net_spec.py"", line 97, in _to_proto
    return self.fn._to_proto(layers, names, autonames)
  File ""/home/sunyou/Downloads/caffe-master/python/caffe/net_spec.py"", line 158, in _to_proto
    assign_proto(layer, k, v)
  File ""/home/sunyou/Downloads/caffe-master/python/caffe/net_spec.py"", line 64, in assign_proto
    is_repeated_field = hasattr(getattr(proto, name), 'extend')
AttributeError: 'LayerParameter' object has no attribute 'weight_filter'

running on Ubuntu 14.04 LTS with Python 2.7.6.
caffe-master from git repo, libprotobuf-dev from apt-get (2.5.0-9ubuntu1)"
caffe,5379,"Please use the [caffe-users list](https://groups.google.com/forum/#!forum/caffe-users) for usage, installation, or modeling questions, or other requests for help.
_Do not post such requests to Issues._ Doing so interferes with the development of Caffe.

Please read the [guidelines for contributing](https://github.com/BVLC/caffe/blob/master/CONTRIBUTING.md) before submitting this issue.

### Issue summary
In our .cpp file, we invoked the feature_extraction_net(proto, caffe::Phase::TEST) in which the proto is the full path of the proto file. However, it ends up with an error as following:
I0303 20:38:16.966768 11556 net.cpp:110] Creating Layer volleyball_level2
I0303 20:38:16.966792 11556 net.cpp:433] volleyball_level2 -> data
I0303 20:38:16.966830 11556 net.cpp:433] volleyball_level2 -> label
F0303 20:38:16.970520 11581 db_leveldb.cpp:16] Check failed: status.ok() Failed to open leveldb examples/deep-activity-rec/ibrahim16-cvpr/p4-network2/trainval-leveldb
Invalid argument: examples/deep-activity-rec/ibrahim16-cvpr/p4-network2/trainval-leveldb: does not exist (create_if_missing is false)
*** Check failure stack trace: ***
    @     0x7f4c1d646daa  (unknown)
    @     0x7f4c1d646ce4  (unknown)
    @     0x7f4c1d6466e6  (unknown)
    @     0x7f4c1d649687  (unknown)
    @     0x7f4c1dd62d07  caffe::db::LevelDB::Open()
    @     0x7f4c1dd16463  caffe::DataReader::Body::InternalThreadEntry()
    @     0x7f4c1dd7fcc5  caffe::InternalThread::entry()
    @     0x7f4c118b7a4a  (unknown)
    @     0x7f4c0fb87184  start_thread
    @     0x7f4c1c47137d  (unknown)
    @              (nil)  (unknown)
Aborted (core dumped)

We notice that there were others asked a similar question before (Feature Extractor Error #3505).

### Steps to reproduce

If you are having difficulty building Caffe or training a model, please ask the caffe-users mailing list. If you are reporting a build error that seems to be due to a bug in Caffe, please attach your build configuration (either Makefile.config or CMakeCache.txt) and the output of the make (or cmake) command.

### Your system configuration
Operating system: Ubuntu 14.04 (64bit)
Compiler: gcc
CUDA version (if applicable): cuda8.0
CUDNN version (if applicable):
BLAS: atlas
Python or MATLAB version (for pycaffe and matcaffe respectively): python 2.7
",0,feature_extraction_net (core dumped) Error,"feature_extraction_net (core dumped) Error Please use the [caffe-users list](https://groups.google.com/forum/#!forum/caffe-users) for usage, installation, or modeling questions, or other requests for help.
_Do not post such requests to Issues._ Doing so interferes with the development of Caffe.

Please read the [guidelines for contributing](https://github.com/BVLC/caffe/blob/master/CONTRIBUTING.md) before submitting this issue.

### Issue summary
In our .cpp file, we invoked the feature_extraction_net(proto, caffe::Phase::TEST) in which the proto is the full path of the proto file. However, it ends up with an error as following:
I0303 20:38:16.966768 11556 net.cpp:110] Creating Layer volleyball_level2
I0303 20:38:16.966792 11556 net.cpp:433] volleyball_level2 -> data
I0303 20:38:16.966830 11556 net.cpp:433] volleyball_level2 -> label
F0303 20:38:16.970520 11581 db_leveldb.cpp:16] Check failed: status.ok() Failed to open leveldb examples/deep-activity-rec/ibrahim16-cvpr/p4-network2/trainval-leveldb
Invalid argument: examples/deep-activity-rec/ibrahim16-cvpr/p4-network2/trainval-leveldb: does not exist (create_if_missing is false)
*** Check failure stack trace: ***
    @     0x7f4c1d646daa  (unknown)
    @     0x7f4c1d646ce4  (unknown)
    @     0x7f4c1d6466e6  (unknown)
    @     0x7f4c1d649687  (unknown)
    @     0x7f4c1dd62d07  caffe::db::LevelDB::Open()
    @     0x7f4c1dd16463  caffe::DataReader::Body::InternalThreadEntry()
    @     0x7f4c1dd7fcc5  caffe::InternalThread::entry()
    @     0x7f4c118b7a4a  (unknown)
    @     0x7f4c0fb87184  start_thread
    @     0x7f4c1c47137d  (unknown)
    @              (nil)  (unknown)
Aborted (core dumped)

We notice that there were others asked a similar question before (Feature Extractor Error #3505).

### Steps to reproduce

If you are having difficulty building Caffe or training a model, please ask the caffe-users mailing list. If you are reporting a build error that seems to be due to a bug in Caffe, please attach your build configuration (either Makefile.config or CMakeCache.txt) and the output of the make (or cmake) command.

### Your system configuration
Operating system: Ubuntu 14.04 (64bit)
Compiler: gcc
CUDA version (if applicable): cuda8.0
CUDNN version (if applicable):
BLAS: atlas
Python or MATLAB version (for pycaffe and matcaffe respectively): python 2.7
"
caffe,1125,"In #995, the data layers are re-arranged into a hierarchical structure. After this modification, the WindowDataLayer has conflicting parameters in TransformationParameter and in WindowDataParameter:



and        



Parameters such as  and  appear twice, which cause serious problems and break down the finetune-pascal example. Currently WindowDataParameter is using  from TransformationParameter, ,  and  from WindowDataParameter. Especially, the  in TransformationParameter is used, while it is specified in WindowDataParameter in finetune-pascal example and in the R. Girshick's R-CNN repo).

I found my experimental result these days so weird and I later found that the WindowDataLayer is not working at all due to this issue. I am going to make a pull request to resolve it.

We should also add unit test for WindowDataLayer.
",0,Conflicting Parameters in WindowDataLayer,"Conflicting Parameters in WindowDataLayer In #995, the data layers are re-arranged into a hierarchical structure. After this modification, the WindowDataLayer has conflicting parameters in TransformationParameter and in WindowDataParameter:



and        



Parameters such as  and  appear twice, which cause serious problems and break down the finetune-pascal example. Currently WindowDataParameter is using  from TransformationParameter, ,  and  from WindowDataParameter. Especially, the  in TransformationParameter is used, while it is specified in WindowDataParameter in finetune-pascal example and in the R. Girshick's R-CNN repo).

I found my experimental result these days so weird and I later found that the WindowDataLayer is not working at all due to this issue. I am going to make a pull request to resolve it.

We should also add unit test for WindowDataLayer.
"
caffe,3803,"Hi,

I succeed at compiling Caffe with Matlab but I get this error when I try to call any function of the caffe api inside Matlab.

I guess everything is okay since the ""caffe_.mexa64"" is generated.
This is my CMake output:

-- Boost version: 1.55.0
-- Found the following Boost libraries:
--   system
--   thread
--   filesystem
-- Found gflags  (include: /usr/include, library: /usr/lib/x86_64-linux-gnu/libgflags.so)
-- Found glog    (include: /usr/include, library: /usr/lib/x86_64-linux-gnu/libglog.so)
-- Found PROTOBUF Compiler: /usr/bin/protoc
-- Found lmdb    (include: /usr/include, library: /usr/lib/x86_64-linux-gnu/liblmdb.so)
-- Found LevelDB (include: /usr/include, library: /usr/lib/x86_64-linux-gnu/libleveldb.so)
-- Found Snappy  (include: /usr/include, library: /usr/lib/libsnappy.so)
-- CUDA detected: 7.5
-- Found cuDNN: ver. 4.0.7 found (include: /usr/local/include, library: /usr/local/lib/libcudnn.so)
-- Added CUDA NVCC flags for: sm_52 sm_50
-- Cuda + Boost 1.55: Applying noinline work around
-- OpenCV found (/usr/local/share/OpenCV)
-- Found Atlas (include: /usr/include, library: /usr/lib/libatlas.so)
-- NumPy ver. 1.8.2 found (include: /usr/lib/python2.7/dist-packages/numpy/core/include)
-- Boost version: 1.55.0
-- Found the following Boost libraries:
--   python
## -- Could NOT find Doxygen (missing:  DOXYGEN_EXECUTABLE) 

-- ******************\* Caffe Configuration Summary *******************
-- General:
--   Version           :   1.0.0-rc3
--   Git               :   rc2-878-gbe163be-dirty
--   System            :   Linux
--   C++ compiler      :   /usr/bin/c++
--   Release CXX flags :   -O3 -DNDEBUG -fPIC -Wall -Wno-sign-compare -Wno-uninitialized
--   Debug CXX flags   :   -g -fPIC -Wall -Wno-sign-compare -Wno-uninitialized
## --   Build type        :   Release

--   BUILD_SHARED_LIBS :   ON
--   BUILD_python      :   ON
--   BUILD_matlab      :   ON
--   BUILD_docs        :   ON
--   CPU_ONLY          :   OFF
--   USE_OPENCV        :   ON
--   USE_LEVELDB       :   ON
--   USE_LMDB          :   ON
## --   ALLOW_LMDB_NOLOCK :   OFF

-- Dependencies:
--   BLAS              :   Yes (Atlas)
--   Boost             :   Yes (ver. 1.55)
--   glog              :   Yes
--   gflags            :   Yes
--   protobuf          :   Yes (ver. 2.5.0)
--   lmdb              :   Yes (ver. 0.9.10)
--   LevelDB           :   Yes (ver. 1.15)
--   Snappy            :   Yes (ver. 1.1.0)
--   OpenCV            :   Yes (ver. 3.1.0)
## --   CUDA              :   Yes (ver. 7.5)

-- NVIDIA CUDA:
--   Target GPU(s)     :   Auto
--   GPU arch(s)       :   sm_52 sm_50
## --   cuDNN             :   Yes (ver. 4.0.7)

-- Python:
--   Interpreter       :   /usr/bin/python2.7 (ver. 2.7.6)
--   Libraries         :   /usr/lib/x86_64-linux-gnu/libpython2.7.so (ver 2.7.6)
## --   NumPy             :   /usr/lib/python2.7/dist-packages/numpy/core/include (ver 1.8.2)

-- Matlab:
--   Matlab            :   Yes (/usr/local/MATLAB/R2014b/bin/mex, /usr/local/MATLAB/R2014b/bin/mexext
## --   Octave            :   No

-- Documentaion:
--   Doxygen           :   No
## --   config_file       :   

-- Install:
## --   Install path      :   /home/eriba/software/caffe/build/install

-- Configuring done
-- Generating done
-- Build files have been written to: /home/eriba/software/caffe/build
",0,make: *** No rule to make target `matcaffe'.  Stop.,"make: *** No rule to make target `matcaffe'.  Stop. Hi,

I succeed at compiling Caffe with Matlab but I get this error when I try to call any function of the caffe api inside Matlab.

I guess everything is okay since the ""caffe_.mexa64"" is generated.
This is my CMake output:

-- Boost version: 1.55.0
-- Found the following Boost libraries:
--   system
--   thread
--   filesystem
-- Found gflags  (include: /usr/include, library: /usr/lib/x86_64-linux-gnu/libgflags.so)
-- Found glog    (include: /usr/include, library: /usr/lib/x86_64-linux-gnu/libglog.so)
-- Found PROTOBUF Compiler: /usr/bin/protoc
-- Found lmdb    (include: /usr/include, library: /usr/lib/x86_64-linux-gnu/liblmdb.so)
-- Found LevelDB (include: /usr/include, library: /usr/lib/x86_64-linux-gnu/libleveldb.so)
-- Found Snappy  (include: /usr/include, library: /usr/lib/libsnappy.so)
-- CUDA detected: 7.5
-- Found cuDNN: ver. 4.0.7 found (include: /usr/local/include, library: /usr/local/lib/libcudnn.so)
-- Added CUDA NVCC flags for: sm_52 sm_50
-- Cuda + Boost 1.55: Applying noinline work around
-- OpenCV found (/usr/local/share/OpenCV)
-- Found Atlas (include: /usr/include, library: /usr/lib/libatlas.so)
-- NumPy ver. 1.8.2 found (include: /usr/lib/python2.7/dist-packages/numpy/core/include)
-- Boost version: 1.55.0
-- Found the following Boost libraries:
--   python
## -- Could NOT find Doxygen (missing:  DOXYGEN_EXECUTABLE) 

-- ******************\* Caffe Configuration Summary *******************
-- General:
--   Version           :   1.0.0-rc3
--   Git               :   rc2-878-gbe163be-dirty
--   System            :   Linux
--   C++ compiler      :   /usr/bin/c++
--   Release CXX flags :   -O3 -DNDEBUG -fPIC -Wall -Wno-sign-compare -Wno-uninitialized
--   Debug CXX flags   :   -g -fPIC -Wall -Wno-sign-compare -Wno-uninitialized
## --   Build type        :   Release

--   BUILD_SHARED_LIBS :   ON
--   BUILD_python      :   ON
--   BUILD_matlab      :   ON
--   BUILD_docs        :   ON
--   CPU_ONLY          :   OFF
--   USE_OPENCV        :   ON
--   USE_LEVELDB       :   ON
--   USE_LMDB          :   ON
## --   ALLOW_LMDB_NOLOCK :   OFF

-- Dependencies:
--   BLAS              :   Yes (Atlas)
--   Boost             :   Yes (ver. 1.55)
--   glog              :   Yes
--   gflags            :   Yes
--   protobuf          :   Yes (ver. 2.5.0)
--   lmdb              :   Yes (ver. 0.9.10)
--   LevelDB           :   Yes (ver. 1.15)
--   Snappy            :   Yes (ver. 1.1.0)
--   OpenCV            :   Yes (ver. 3.1.0)
## --   CUDA              :   Yes (ver. 7.5)

-- NVIDIA CUDA:
--   Target GPU(s)     :   Auto
--   GPU arch(s)       :   sm_52 sm_50
## --   cuDNN             :   Yes (ver. 4.0.7)

-- Python:
--   Interpreter       :   /usr/bin/python2.7 (ver. 2.7.6)
--   Libraries         :   /usr/lib/x86_64-linux-gnu/libpython2.7.so (ver 2.7.6)
## --   NumPy             :   /usr/lib/python2.7/dist-packages/numpy/core/include (ver 1.8.2)

-- Matlab:
--   Matlab            :   Yes (/usr/local/MATLAB/R2014b/bin/mex, /usr/local/MATLAB/R2014b/bin/mexext
## --   Octave            :   No

-- Documentaion:
--   Doxygen           :   No
## --   config_file       :   

-- Install:
## --   Install path      :   /home/eriba/software/caffe/build/install

-- Configuring done
-- Generating done
-- Build files have been written to: /home/eriba/software/caffe/build
"
caffe,3141,"ubuntu 12.04 
caffe
cudnn-v2

when I run ""sudo make all -j8"" in caffe got error logs like this:


",0,make all errors,"make all errors ubuntu 12.04 
caffe
cudnn-v2

when I run ""sudo make all -j8"" in caffe got error logs like this:


"
caffe,1602,"I copied the solver.prototxt and train_val.prototxt files from models/bvlc_reference_caffenet, modified the latter to use IMAGE_DATA layers, and then attempted to run caffe, but my train.prototxt contained an error -- the source file specified (file_list.txt) did not exist. It crashes with an uncaught segfault instead of properly reporting the error. The only hint was the ""A total of 0 images."" message.

Below are machine details, gdb backtrace, and command and log output:

Machine: MacBook Pro Retina Late 2013, Mac OS X 10.10.1, 16GB RAM




",0,Non-existent IMAGE_DATA source file causes segmentation fault in caffe::ImageDataLayer<float>::DataLayerSetUp,"Non-existent IMAGE_DATA source file causes segmentation fault in caffe::ImageDataLayer<float>::DataLayerSetUp I copied the solver.prototxt and train_val.prototxt files from models/bvlc_reference_caffenet, modified the latter to use IMAGE_DATA layers, and then attempted to run caffe, but my train.prototxt contained an error -- the source file specified (file_list.txt) did not exist. It crashes with an uncaught segfault instead of properly reporting the error. The only hint was the ""A total of 0 images."" message.

Below are machine details, gdb backtrace, and command and log output:

Machine: MacBook Pro Retina Late 2013, Mac OS X 10.10.1, 16GB RAM




"
caffe,5254," when I want to make pyest error after already successfully done with the all, test, runtest and pycaffe, and something went wrong as below:
cd python; python -m unittest discover -s caffe/test
EEEEEEEE
======================================================================
ERROR: test_coord_map (unittest.loader.ModuleImportFailure)
----------------------------------------------------------------------
ImportError: Failed to import test module: test_coord_map
Traceback (most recent call last):
  File ""/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/unittest/loader.py"", line 254, in _find_tests
    module = self._get_module_from_name(name)
  File ""/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/unittest/loader.py"", line 232, in _get_module_from_name
    __import__(name)
  File ""/Users/yolanda/caffe/python/caffe/test/test_coord_map.py"", line 6, in <module>
    import caffe
  File ""caffe/__init__.py"", line 1, in <module>
    from .pycaffe import Net, SGDSolver, NesterovSolver, AdaGradSolver, RMSPropSolver, AdaDeltaSolver, AdamSolver, NCCL, Timer
  File ""caffe/pycaffe.py"", line 13, in <module>
    from ._caffe import Net, SGDSolver, NesterovSolver, AdaGradSolver, \
ImportError: dlopen(caffe/_caffe.so, 2): Library not loaded: @rpath/libopencv_imgcodecs.3.2.dylib
  Referenced from: /Users/yolanda/caffe/python/caffe/_caffe.so
  Reason: image not found


======================================================================
ERROR: test_io (unittest.loader.ModuleImportFailure)
----------------------------------------------------------------------
ImportError: Failed to import test module: test_io
Traceback (most recent call last):
  File ""/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/unittest/loader.py"", line 254, in _find_tests
    module = self._get_module_from_name(name)
  File ""/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/unittest/loader.py"", line 232, in _get_module_from_name
    __import__(name)
  File ""/Users/yolanda/caffe/python/caffe/test/test_io.py"", line 4, in <module>
    import caffe
  File ""caffe/__init__.py"", line 1, in <module>
    from .pycaffe import Net, SGDSolver, NesterovSolver, AdaGradSolver, RMSPropSolver, AdaDeltaSolver, AdamSolver, NCCL, Timer
  File ""caffe/pycaffe.py"", line 13, in <module>
    from ._caffe import Net, SGDSolver, NesterovSolver, AdaGradSolver, \
ImportError: dlopen(caffe/_caffe.so, 2): Library not loaded: @rpath/libopencv_imgcodecs.3.2.dylib
  Referenced from: /Users/yolanda/caffe/python/caffe/_caffe.so
  Reason: image not found


======================================================================
ERROR: test_layer_type_list (unittest.loader.ModuleImportFailure)
----------------------------------------------------------------------
ImportError: Failed to import test module: test_layer_type_list
Traceback (most recent call last):
  File ""/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/unittest/loader.py"", line 254, in _find_tests
    module = self._get_module_from_name(name)
  File ""/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/unittest/loader.py"", line 232, in _get_module_from_name
    __import__(name)
  File ""/Users/yolanda/caffe/python/caffe/test/test_layer_type_list.py"", line 3, in <module>
    import caffe
  File ""caffe/__init__.py"", line 1, in <module>
    from .pycaffe import Net, SGDSolver, NesterovSolver, AdaGradSolver, RMSPropSolver, AdaDeltaSolver, AdamSolver, NCCL, Timer
  File ""caffe/pycaffe.py"", line 13, in <module>
    from ._caffe import Net, SGDSolver, NesterovSolver, AdaGradSolver, \
ImportError: dlopen(caffe/_caffe.so, 2): Library not loaded: @rpath/libopencv_imgcodecs.3.2.dylib
  Referenced from: /Users/yolanda/caffe/python/caffe/_caffe.so
  Reason: image not found


======================================================================
ERROR: test_net (unittest.loader.ModuleImportFailure)
----------------------------------------------------------------------
ImportError: Failed to import test module: test_net
Traceback (most recent call last):
  File ""/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/unittest/loader.py"", line 254, in _find_tests
    module = self._get_module_from_name(name)
  File ""/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/unittest/loader.py"", line 232, in _get_module_from_name
    __import__(name)
  File ""/Users/yolanda/caffe/python/caffe/test/test_net.py"", line 8, in <module>
    import caffe
  File ""caffe/__init__.py"", line 1, in <module>
    from .pycaffe import Net, SGDSolver, NesterovSolver, AdaGradSolver, RMSPropSolver, AdaDeltaSolver, AdamSolver, NCCL, Timer
  File ""caffe/pycaffe.py"", line 13, in <module>
    from ._caffe import Net, SGDSolver, NesterovSolver, AdaGradSolver, \
ImportError: dlopen(caffe/_caffe.so, 2): Library not loaded: @rpath/libopencv_imgcodecs.3.2.dylib
  Referenced from: /Users/yolanda/caffe/python/caffe/_caffe.so
  Reason: image not found


======================================================================
ERROR: test_net_spec (unittest.loader.ModuleImportFailure)
----------------------------------------------------------------------
ImportError: Failed to import test module: test_net_spec
Traceback (most recent call last):
  File ""/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/unittest/loader.py"", line 254, in _find_tests
    module = self._get_module_from_name(name)
  File ""/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/unittest/loader.py"", line 232, in _get_module_from_name
    __import__(name)
  File ""/Users/yolanda/caffe/python/caffe/test/test_net_spec.py"", line 3, in <module>
    import caffe
  File ""caffe/__init__.py"", line 1, in <module>
    from .pycaffe import Net, SGDSolver, NesterovSolver, AdaGradSolver, RMSPropSolver, AdaDeltaSolver, AdamSolver, NCCL, Timer
  File ""caffe/pycaffe.py"", line 13, in <module>
    from ._caffe import Net, SGDSolver, NesterovSolver, AdaGradSolver, \
ImportError: dlopen(caffe/_caffe.so, 2): Library not loaded: @rpath/libopencv_imgcodecs.3.2.dylib
  Referenced from: /Users/yolanda/caffe/python/caffe/_caffe.so
  Reason: image not found


======================================================================
ERROR: test_python_layer (unittest.loader.ModuleImportFailure)
----------------------------------------------------------------------
ImportError: Failed to import test module: test_python_layer
Traceback (most recent call last):
  File ""/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/unittest/loader.py"", line 254, in _find_tests
    module = self._get_module_from_name(name)
  File ""/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/unittest/loader.py"", line 232, in _get_module_from_name
    __import__(name)
  File ""/Users/yolanda/caffe/python/caffe/test/test_python_layer.py"", line 6, in <module>
    import caffe
  File ""caffe/__init__.py"", line 1, in <module>
    from .pycaffe import Net, SGDSolver, NesterovSolver, AdaGradSolver, RMSPropSolver, AdaDeltaSolver, AdamSolver, NCCL, Timer
  File ""caffe/pycaffe.py"", line 13, in <module>
    from ._caffe import Net, SGDSolver, NesterovSolver, AdaGradSolver, \
ImportError: dlopen(caffe/_caffe.so, 2): Library not loaded: @rpath/libopencv_imgcodecs.3.2.dylib
  Referenced from: /Users/yolanda/caffe/python/caffe/_caffe.so
  Reason: image not found


======================================================================
ERROR: test_python_layer_with_param_str (unittest.loader.ModuleImportFailure)
----------------------------------------------------------------------
ImportError: Failed to import test module: test_python_layer_with_param_str
Traceback (most recent call last):
  File ""/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/unittest/loader.py"", line 254, in _find_tests
    module = self._get_module_from_name(name)
  File ""/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/unittest/loader.py"", line 232, in _get_module_from_name
    __import__(name)
  File ""/Users/yolanda/caffe/python/caffe/test/test_python_layer_with_param_str.py"", line 6, in <module>
    import caffe
  File ""caffe/__init__.py"", line 1, in <module>
    from .pycaffe import Net, SGDSolver, NesterovSolver, AdaGradSolver, RMSPropSolver, AdaDeltaSolver, AdamSolver, NCCL, Timer
  File ""caffe/pycaffe.py"", line 13, in <module>
    from ._caffe import Net, SGDSolver, NesterovSolver, AdaGradSolver, \
ImportError: dlopen(caffe/_caffe.so, 2): Library not loaded: @rpath/libopencv_imgcodecs.3.2.dylib
  Referenced from: /Users/yolanda/caffe/python/caffe/_caffe.so
  Reason: image not found


======================================================================
ERROR: test_solver (unittest.loader.ModuleImportFailure)
----------------------------------------------------------------------
ImportError: Failed to import test module: test_solver
Traceback (most recent call last):
  File ""/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/unittest/loader.py"", line 254, in _find_tests
    module = self._get_module_from_name(name)
  File ""/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/unittest/loader.py"", line 232, in _get_module_from_name
    __import__(name)
  File ""/Users/yolanda/caffe/python/caffe/test/test_solver.py"", line 7, in <module>
    import caffe
  File ""caffe/__init__.py"", line 1, in <module>
    from .pycaffe import Net, SGDSolver, NesterovSolver, AdaGradSolver, RMSPropSolver, AdaDeltaSolver, AdamSolver, NCCL, Timer
  File ""caffe/pycaffe.py"", line 13, in <module>
    from ._caffe import Net, SGDSolver, NesterovSolver, AdaGradSolver, \
ImportError: dlopen(caffe/_caffe.so, 2): Library not loaded: @rpath/libopencv_imgcodecs.3.2.dylib
  Referenced from: /Users/yolanda/caffe/python/caffe/_caffe.so
  Reason: image not found

and when I try importing caffe in shell I got the error as below:
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""caffe/__init__.py"", line 1, in <module>
    from .pycaffe import Net, SGDSolver, NesterovSolver, AdaGradSolver, RMSPropSolver, AdaDeltaSolver, AdamSolver, NCCL, Timer
  File ""caffe/pycaffe.py"", line 13, in <module>
    from ._caffe import Net, SGDSolver, NesterovSolver, AdaGradSolver, \
ImportError: dlopen(caffe/_caffe.so, 2): Library not loaded: @rpath/libopencv_imgcodecs.3.2.dylib
  Referenced from: /Users/yolanda/caffe/python/caffe/_caffe.so
  Reason: image not found

but when I use 'otool -L python/caffe/_caffe.so' it shows as below which contains libopencv_imgcodecs:
python/caffe/_caffe.so:
	python/caffe/_caffe.so (compatibility version 0.0.0, current version 0.0.0)
	@rpath/libcaffe.so.1.0.0-rc3 (compatibility version 0.0.0, current version 0.0.0)
	/usr/local/opt/glog/lib/libglog.0.dylib (compatibility version 1.0.0, current version 1.0.0)
	/usr/local/opt/gflags/lib/libgflags.2.2.dylib (compatibility version 2.2.0, current version 2.2.0)
	/usr/local/opt/protobuf/lib/libprotobuf.12.dylib (compatibility version 13.0.0, current version 13.0.0)
	/usr/local/opt/leveldb/lib/libleveldb.1.dylib (compatibility version 0.0.0, current version 0.0.0)
	/usr/local/opt/snappy/lib/libsnappy.1.dylib (compatibility version 5.0.0, current version 5.1.0)
	/usr/local/opt/lmdb/lib/liblmdb.dylib (compatibility version 0.0.0, current version 0.0.0)
	/usr/local/opt/boost/lib/libboost_system.dylib (compatibility version 0.0.0, current version 0.0.0)
	/usr/local/opt/hdf5/lib/libhdf5_hl.10.dylib (compatibility version 12.0.0, current version 12.1.0)
	/usr/local/opt/hdf5/lib/libhdf5.10.dylib (compatibility version 13.0.0, current version 13.1.0)
	@rpath/libopencv_imgcodecs.3.2.dylib (compatibility version 3.2.0, current version 3.2.0)
	@rpath/libopencv_highgui.3.2.dylib (compatibility version 3.2.0, current version 3.2.0)
	@rpath/libopencv_imgproc.3.2.dylib (compatibility version 3.2.0, current version 3.2.0)
	@rpath/libopencv_core.3.2.dylib (compatibility version 3.2.0, current version 3.2.0)
	/usr/lib/libSystem.B.dylib (compatibility version 1.0.0, current version 1238.0.0)
	/usr/local/opt/boost/lib/libboost_filesystem.dylib (compatibility version 0.0.0, current version 0.0.0)
	/usr/local/opt/boost/lib/libboost_thread-mt.dylib (compatibility version 0.0.0, current version 0.0.0)
	/System/Library/Frameworks/Accelerate.framework/Versions/A/Frameworks/vecLib.framework/Versions/A/libBLAS.dylib (compatibility version 1.0.0, current version 1.0.0)
	/usr/local/opt/boost-python/lib/libboost_python.dylib (compatibility version 0.0.0, current version 0.0.0)
	/System/Library/Frameworks/Python.framework/Versions/2.7/Python (compatibility version 2.7.0, current version 2.7.10)
	/usr/lib/libc++.1.dylib (compatibility version 1.0.0, current version 307.4.0)

when I check the opencv lib there doesn't exist this library.
Do I need to download that and install path?



### Your system configuration
Operating system: Mac OS sierra 10.12.3
[CMakeLists.txt](https://github.com/BVLC/caffe/files/752340/CMakeLists.txt)",0,error make pytest,"error make pytest  when I want to make pyest error after already successfully done with the all, test, runtest and pycaffe, and something went wrong as below:
cd python; python -m unittest discover -s caffe/test
EEEEEEEE
======================================================================
ERROR: test_coord_map (unittest.loader.ModuleImportFailure)
----------------------------------------------------------------------
ImportError: Failed to import test module: test_coord_map
Traceback (most recent call last):
  File ""/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/unittest/loader.py"", line 254, in _find_tests
    module = self._get_module_from_name(name)
  File ""/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/unittest/loader.py"", line 232, in _get_module_from_name
    __import__(name)
  File ""/Users/yolanda/caffe/python/caffe/test/test_coord_map.py"", line 6, in <module>
    import caffe
  File ""caffe/__init__.py"", line 1, in <module>
    from .pycaffe import Net, SGDSolver, NesterovSolver, AdaGradSolver, RMSPropSolver, AdaDeltaSolver, AdamSolver, NCCL, Timer
  File ""caffe/pycaffe.py"", line 13, in <module>
    from ._caffe import Net, SGDSolver, NesterovSolver, AdaGradSolver, \
ImportError: dlopen(caffe/_caffe.so, 2): Library not loaded: @rpath/libopencv_imgcodecs.3.2.dylib
  Referenced from: /Users/yolanda/caffe/python/caffe/_caffe.so
  Reason: image not found


======================================================================
ERROR: test_io (unittest.loader.ModuleImportFailure)
----------------------------------------------------------------------
ImportError: Failed to import test module: test_io
Traceback (most recent call last):
  File ""/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/unittest/loader.py"", line 254, in _find_tests
    module = self._get_module_from_name(name)
  File ""/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/unittest/loader.py"", line 232, in _get_module_from_name
    __import__(name)
  File ""/Users/yolanda/caffe/python/caffe/test/test_io.py"", line 4, in <module>
    import caffe
  File ""caffe/__init__.py"", line 1, in <module>
    from .pycaffe import Net, SGDSolver, NesterovSolver, AdaGradSolver, RMSPropSolver, AdaDeltaSolver, AdamSolver, NCCL, Timer
  File ""caffe/pycaffe.py"", line 13, in <module>
    from ._caffe import Net, SGDSolver, NesterovSolver, AdaGradSolver, \
ImportError: dlopen(caffe/_caffe.so, 2): Library not loaded: @rpath/libopencv_imgcodecs.3.2.dylib
  Referenced from: /Users/yolanda/caffe/python/caffe/_caffe.so
  Reason: image not found


======================================================================
ERROR: test_layer_type_list (unittest.loader.ModuleImportFailure)
----------------------------------------------------------------------
ImportError: Failed to import test module: test_layer_type_list
Traceback (most recent call last):
  File ""/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/unittest/loader.py"", line 254, in _find_tests
    module = self._get_module_from_name(name)
  File ""/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/unittest/loader.py"", line 232, in _get_module_from_name
    __import__(name)
  File ""/Users/yolanda/caffe/python/caffe/test/test_layer_type_list.py"", line 3, in <module>
    import caffe
  File ""caffe/__init__.py"", line 1, in <module>
    from .pycaffe import Net, SGDSolver, NesterovSolver, AdaGradSolver, RMSPropSolver, AdaDeltaSolver, AdamSolver, NCCL, Timer
  File ""caffe/pycaffe.py"", line 13, in <module>
    from ._caffe import Net, SGDSolver, NesterovSolver, AdaGradSolver, \
ImportError: dlopen(caffe/_caffe.so, 2): Library not loaded: @rpath/libopencv_imgcodecs.3.2.dylib
  Referenced from: /Users/yolanda/caffe/python/caffe/_caffe.so
  Reason: image not found


======================================================================
ERROR: test_net (unittest.loader.ModuleImportFailure)
----------------------------------------------------------------------
ImportError: Failed to import test module: test_net
Traceback (most recent call last):
  File ""/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/unittest/loader.py"", line 254, in _find_tests
    module = self._get_module_from_name(name)
  File ""/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/unittest/loader.py"", line 232, in _get_module_from_name
    __import__(name)
  File ""/Users/yolanda/caffe/python/caffe/test/test_net.py"", line 8, in <module>
    import caffe
  File ""caffe/__init__.py"", line 1, in <module>
    from .pycaffe import Net, SGDSolver, NesterovSolver, AdaGradSolver, RMSPropSolver, AdaDeltaSolver, AdamSolver, NCCL, Timer
  File ""caffe/pycaffe.py"", line 13, in <module>
    from ._caffe import Net, SGDSolver, NesterovSolver, AdaGradSolver, \
ImportError: dlopen(caffe/_caffe.so, 2): Library not loaded: @rpath/libopencv_imgcodecs.3.2.dylib
  Referenced from: /Users/yolanda/caffe/python/caffe/_caffe.so
  Reason: image not found


======================================================================
ERROR: test_net_spec (unittest.loader.ModuleImportFailure)
----------------------------------------------------------------------
ImportError: Failed to import test module: test_net_spec
Traceback (most recent call last):
  File ""/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/unittest/loader.py"", line 254, in _find_tests
    module = self._get_module_from_name(name)
  File ""/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/unittest/loader.py"", line 232, in _get_module_from_name
    __import__(name)
  File ""/Users/yolanda/caffe/python/caffe/test/test_net_spec.py"", line 3, in <module>
    import caffe
  File ""caffe/__init__.py"", line 1, in <module>
    from .pycaffe import Net, SGDSolver, NesterovSolver, AdaGradSolver, RMSPropSolver, AdaDeltaSolver, AdamSolver, NCCL, Timer
  File ""caffe/pycaffe.py"", line 13, in <module>
    from ._caffe import Net, SGDSolver, NesterovSolver, AdaGradSolver, \
ImportError: dlopen(caffe/_caffe.so, 2): Library not loaded: @rpath/libopencv_imgcodecs.3.2.dylib
  Referenced from: /Users/yolanda/caffe/python/caffe/_caffe.so
  Reason: image not found


======================================================================
ERROR: test_python_layer (unittest.loader.ModuleImportFailure)
----------------------------------------------------------------------
ImportError: Failed to import test module: test_python_layer
Traceback (most recent call last):
  File ""/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/unittest/loader.py"", line 254, in _find_tests
    module = self._get_module_from_name(name)
  File ""/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/unittest/loader.py"", line 232, in _get_module_from_name
    __import__(name)
  File ""/Users/yolanda/caffe/python/caffe/test/test_python_layer.py"", line 6, in <module>
    import caffe
  File ""caffe/__init__.py"", line 1, in <module>
    from .pycaffe import Net, SGDSolver, NesterovSolver, AdaGradSolver, RMSPropSolver, AdaDeltaSolver, AdamSolver, NCCL, Timer
  File ""caffe/pycaffe.py"", line 13, in <module>
    from ._caffe import Net, SGDSolver, NesterovSolver, AdaGradSolver, \
ImportError: dlopen(caffe/_caffe.so, 2): Library not loaded: @rpath/libopencv_imgcodecs.3.2.dylib
  Referenced from: /Users/yolanda/caffe/python/caffe/_caffe.so
  Reason: image not found


======================================================================
ERROR: test_python_layer_with_param_str (unittest.loader.ModuleImportFailure)
----------------------------------------------------------------------
ImportError: Failed to import test module: test_python_layer_with_param_str
Traceback (most recent call last):
  File ""/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/unittest/loader.py"", line 254, in _find_tests
    module = self._get_module_from_name(name)
  File ""/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/unittest/loader.py"", line 232, in _get_module_from_name
    __import__(name)
  File ""/Users/yolanda/caffe/python/caffe/test/test_python_layer_with_param_str.py"", line 6, in <module>
    import caffe
  File ""caffe/__init__.py"", line 1, in <module>
    from .pycaffe import Net, SGDSolver, NesterovSolver, AdaGradSolver, RMSPropSolver, AdaDeltaSolver, AdamSolver, NCCL, Timer
  File ""caffe/pycaffe.py"", line 13, in <module>
    from ._caffe import Net, SGDSolver, NesterovSolver, AdaGradSolver, \
ImportError: dlopen(caffe/_caffe.so, 2): Library not loaded: @rpath/libopencv_imgcodecs.3.2.dylib
  Referenced from: /Users/yolanda/caffe/python/caffe/_caffe.so
  Reason: image not found


======================================================================
ERROR: test_solver (unittest.loader.ModuleImportFailure)
----------------------------------------------------------------------
ImportError: Failed to import test module: test_solver
Traceback (most recent call last):
  File ""/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/unittest/loader.py"", line 254, in _find_tests
    module = self._get_module_from_name(name)
  File ""/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/unittest/loader.py"", line 232, in _get_module_from_name
    __import__(name)
  File ""/Users/yolanda/caffe/python/caffe/test/test_solver.py"", line 7, in <module>
    import caffe
  File ""caffe/__init__.py"", line 1, in <module>
    from .pycaffe import Net, SGDSolver, NesterovSolver, AdaGradSolver, RMSPropSolver, AdaDeltaSolver, AdamSolver, NCCL, Timer
  File ""caffe/pycaffe.py"", line 13, in <module>
    from ._caffe import Net, SGDSolver, NesterovSolver, AdaGradSolver, \
ImportError: dlopen(caffe/_caffe.so, 2): Library not loaded: @rpath/libopencv_imgcodecs.3.2.dylib
  Referenced from: /Users/yolanda/caffe/python/caffe/_caffe.so
  Reason: image not found

and when I try importing caffe in shell I got the error as below:
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""caffe/__init__.py"", line 1, in <module>
    from .pycaffe import Net, SGDSolver, NesterovSolver, AdaGradSolver, RMSPropSolver, AdaDeltaSolver, AdamSolver, NCCL, Timer
  File ""caffe/pycaffe.py"", line 13, in <module>
    from ._caffe import Net, SGDSolver, NesterovSolver, AdaGradSolver, \
ImportError: dlopen(caffe/_caffe.so, 2): Library not loaded: @rpath/libopencv_imgcodecs.3.2.dylib
  Referenced from: /Users/yolanda/caffe/python/caffe/_caffe.so
  Reason: image not found

but when I use 'otool -L python/caffe/_caffe.so' it shows as below which contains libopencv_imgcodecs:
python/caffe/_caffe.so:
	python/caffe/_caffe.so (compatibility version 0.0.0, current version 0.0.0)
	@rpath/libcaffe.so.1.0.0-rc3 (compatibility version 0.0.0, current version 0.0.0)
	/usr/local/opt/glog/lib/libglog.0.dylib (compatibility version 1.0.0, current version 1.0.0)
	/usr/local/opt/gflags/lib/libgflags.2.2.dylib (compatibility version 2.2.0, current version 2.2.0)
	/usr/local/opt/protobuf/lib/libprotobuf.12.dylib (compatibility version 13.0.0, current version 13.0.0)
	/usr/local/opt/leveldb/lib/libleveldb.1.dylib (compatibility version 0.0.0, current version 0.0.0)
	/usr/local/opt/snappy/lib/libsnappy.1.dylib (compatibility version 5.0.0, current version 5.1.0)
	/usr/local/opt/lmdb/lib/liblmdb.dylib (compatibility version 0.0.0, current version 0.0.0)
	/usr/local/opt/boost/lib/libboost_system.dylib (compatibility version 0.0.0, current version 0.0.0)
	/usr/local/opt/hdf5/lib/libhdf5_hl.10.dylib (compatibility version 12.0.0, current version 12.1.0)
	/usr/local/opt/hdf5/lib/libhdf5.10.dylib (compatibility version 13.0.0, current version 13.1.0)
	@rpath/libopencv_imgcodecs.3.2.dylib (compatibility version 3.2.0, current version 3.2.0)
	@rpath/libopencv_highgui.3.2.dylib (compatibility version 3.2.0, current version 3.2.0)
	@rpath/libopencv_imgproc.3.2.dylib (compatibility version 3.2.0, current version 3.2.0)
	@rpath/libopencv_core.3.2.dylib (compatibility version 3.2.0, current version 3.2.0)
	/usr/lib/libSystem.B.dylib (compatibility version 1.0.0, current version 1238.0.0)
	/usr/local/opt/boost/lib/libboost_filesystem.dylib (compatibility version 0.0.0, current version 0.0.0)
	/usr/local/opt/boost/lib/libboost_thread-mt.dylib (compatibility version 0.0.0, current version 0.0.0)
	/System/Library/Frameworks/Accelerate.framework/Versions/A/Frameworks/vecLib.framework/Versions/A/libBLAS.dylib (compatibility version 1.0.0, current version 1.0.0)
	/usr/local/opt/boost-python/lib/libboost_python.dylib (compatibility version 0.0.0, current version 0.0.0)
	/System/Library/Frameworks/Python.framework/Versions/2.7/Python (compatibility version 2.7.0, current version 2.7.10)
	/usr/lib/libc++.1.dylib (compatibility version 1.0.0, current version 307.4.0)

when I check the opencv lib there doesn't exist this library.
Do I need to download that and install path?



### Your system configuration
Operating system: Mac OS sierra 10.12.3
[CMakeLists.txt](https://github.com/BVLC/caffe/files/752340/CMakeLists.txt)"
caffe,2455,"I am trying to install and build Caffe along with its dependencies. After going through all the prerequisite when I run 'make all' I get this:
CXX src/caffe/net.cpp
In file included from src/caffe/net.cpp:8:0:
./include/caffe/common.hpp:5:27: fatal error: gflags/gflags.h: No such file or directory
 #include <gflags/gflags.h>
                           ^
compilation terminated.
make: **\* [.build_release/src/caffe/net.o] Error 1

Please help in resolving it.
",0,Compilation error when using caffe on Ubuntu 14.04,"Compilation error when using caffe on Ubuntu 14.04 I am trying to install and build Caffe along with its dependencies. After going through all the prerequisite when I run 'make all' I get this:
CXX src/caffe/net.cpp
In file included from src/caffe/net.cpp:8:0:
./include/caffe/common.hpp:5:27: fatal error: gflags/gflags.h: No such file or directory
 #include <gflags/gflags.h>
                           ^
compilation terminated.
make: **\* [.build_release/src/caffe/net.o] Error 1

Please help in resolving it.
"
caffe,3530,"It runs until

CXX src/caffe/solvers/sgd_solver.cpp

then fails with the following error:

In file included from src/caffe/solvers/sgd_solver.cpp:5:0:
./include/caffe/util/hdf5.hpp:6:18: fatal error: hdf5.h: No such file or directory
compilation terminated.
Makefile:552: recipe for target '.build_release/src/caffe/solvers/sgd_solver.o' failed
make: **\* [.build_release/src/caffe/solvers/sgd_solver.o] Error 1

Any ideas?
",0,error during make pycaffe,"error during make pycaffe It runs until

CXX src/caffe/solvers/sgd_solver.cpp

then fails with the following error:

In file included from src/caffe/solvers/sgd_solver.cpp:5:0:
./include/caffe/util/hdf5.hpp:6:18: fatal error: hdf5.h: No such file or directory
compilation terminated.
Makefile:552: recipe for target '.build_release/src/caffe/solvers/sgd_solver.o' failed
make: **\* [.build_release/src/caffe/solvers/sgd_solver.o] Error 1

Any ideas?
"
caffe,312,"Hi. I just train my model. It seems everything goes well. But when testing the net. it outputs Segmentation fault. I don't know what's the problem. Thanks.

I0410 19:44:32.296176 26997 solver.cpp:36] Solver scaffolding done.
I0410 19:44:32.296200 26997 solver.cpp:47] Solving CaffeNet
I0410 19:49:34.169067 26997 solver.cpp:208] Iteration 500, lr = 0.01
I0410 19:49:34.172401 26997 solver.cpp:65] Iteration 500, loss = 0
I0410 19:54:43.192100 26997 solver.cpp:208] Iteration 1000, lr = 0.01
I0410 19:54:43.195472 26997 solver.cpp:65] Iteration 1000, loss = 0
I0410 19:54:43.195485 26997 solver.cpp:87] Iteration 1000, Testing net
Segmentation fault (core dumped)
",0,Segmentation fault (core dumped) when training and CommonTest Failed,"Segmentation fault (core dumped) when training and CommonTest Failed Hi. I just train my model. It seems everything goes well. But when testing the net. it outputs Segmentation fault. I don't know what's the problem. Thanks.

I0410 19:44:32.296176 26997 solver.cpp:36] Solver scaffolding done.
I0410 19:44:32.296200 26997 solver.cpp:47] Solving CaffeNet
I0410 19:49:34.169067 26997 solver.cpp:208] Iteration 500, lr = 0.01
I0410 19:49:34.172401 26997 solver.cpp:65] Iteration 500, loss = 0
I0410 19:54:43.192100 26997 solver.cpp:208] Iteration 1000, lr = 0.01
I0410 19:54:43.195472 26997 solver.cpp:65] Iteration 1000, loss = 0
I0410 19:54:43.195485 26997 solver.cpp:87] Iteration 1000, Testing net
Segmentation fault (core dumped)
"
caffe,3078,"Hello,

I need to call the convert_imageset function for converting a database into LMDB format from inside Matlab. 

I am using Linux, and I have created a shell (.sh) script with the needed parameters for running the conversion. Here is an example of how does my shell file look like:

> GLOG_logtostderr=1 /usr/local/caffe-master2/build/tools/convert_imageset -resize_height=256 -resize_width=256 images_folder data_split/train.txt data_split/dataCNN_train_lmdb

When I simply run my script from the terminal like this:

> ./example_shell.sh

it works without any problem. 
But when I try to do it from Matlab using the system() function:

> system('./example_shell.sh')

it seems it is not able to open/find my files, rising the following error for each image in train.txt:

> I0917 18:15:13.637830  8605 convert_imageset.cpp:82] A total of 68175 images.
> I0917 18:15:13.638947  8605 db.cpp:34] Opened lmdb data_split/dataCNN_train_lmdb
> E0917 18:15:13.639143  8605 io.cpp:77] Could not open or find file ...
> E0917 18:15:13.639143  8605 io.cpp:77] Could not open or find file ...
> E0917 18:15:13.639143  8605 io.cpp:77] Could not open or find file ...

Thank you,
Marc
",0,Problem creating LMDB from Matlab,"Problem creating LMDB from Matlab Hello,

I need to call the convert_imageset function for converting a database into LMDB format from inside Matlab. 

I am using Linux, and I have created a shell (.sh) script with the needed parameters for running the conversion. Here is an example of how does my shell file look like:

> GLOG_logtostderr=1 /usr/local/caffe-master2/build/tools/convert_imageset -resize_height=256 -resize_width=256 images_folder data_split/train.txt data_split/dataCNN_train_lmdb

When I simply run my script from the terminal like this:

> ./example_shell.sh

it works without any problem. 
But when I try to do it from Matlab using the system() function:

> system('./example_shell.sh')

it seems it is not able to open/find my files, rising the following error for each image in train.txt:

> I0917 18:15:13.637830  8605 convert_imageset.cpp:82] A total of 68175 images.
> I0917 18:15:13.638947  8605 db.cpp:34] Opened lmdb data_split/dataCNN_train_lmdb
> E0917 18:15:13.639143  8605 io.cpp:77] Could not open or find file ...
> E0917 18:15:13.639143  8605 io.cpp:77] Could not open or find file ...
> E0917 18:15:13.639143  8605 io.cpp:77] Could not open or find file ...

Thank you,
Marc
"
caffe,1160,"Hello!, every body.  I had tried to compile the parallel version of caffe, but I found that it can make all and make test, but can't run the make runtest. The error info is that error while loading shared libraries:libcudart.so.6.5: cannot open shared object file: No shuch file or directory. I had installed the cuDNN version caffe and common caffe, they are both ok and not appear this error. Can any expert help me? Thanks in advance!
",0,Can't run the tests in caffe-parallel version,"Can't run the tests in caffe-parallel version Hello!, every body.  I had tried to compile the parallel version of caffe, but I found that it can make all and make test, but can't run the make runtest. The error info is that error while loading shared libraries:libcudart.so.6.5: cannot open shared object file: No shuch file or directory. I had installed the cuDNN version caffe and common caffe, they are both ok and not appear this error. Can any expert help me? Thanks in advance!
"
caffe,6279,"please delete
",0,please delete,"please delete please delete
"
caffe,4376,"GTX970 Cuda7.0 cudnn4.0 opencv2.4.12
I don't know how to solve this problem!
**\* Aborted at 1467078048 (unix time) try ""date -d @1467078048"" if you are using GNU date ***
PC: @     0x2b3690e4b897 cudnnAddTensor
**\* SIGSEGV (@0x703105204) received by PID 31656 (TID 0x2b3682bce3c0) from PID 51401220; stack trace: ***
    @     0x2b3689a1f340 (unknown)
    @     0x2b3690e4b897 cudnnAddTensor
    @     0x2b3688de8463 caffe::CuDNNConvolutionLayer<>::Forward_gpu()
    @           0x48df76 caffe::Layer<>::Forward()
    @           0x93bd0f caffe::CuDNNConvolutionLayerTest_TestSimpleConvolutionCuDNN_Test<>::TestBody()
    @           0x9a4e53 testing::internal::HandleExceptionsInMethodIfSupported<>()
    @           0x99bb37 testing::Test::Run()
    @           0x99bbde testing::TestInfo::Run()
    @           0x99bce5 testing::TestCase::Run()
    @           0x99f028 testing::internal::UnitTestImpl::RunAllTests()
    @           0x99f2b7 testing::UnitTest::Run()
    @           0x473c7f main
    @     0x2b3689c4eec5 __libc_start_main
    @           0x47ca69 (unknown)
make: **\* [runtest] Segmentation fault (core dumped)
",0,make runtest error,"make runtest error GTX970 Cuda7.0 cudnn4.0 opencv2.4.12
I don't know how to solve this problem!
**\* Aborted at 1467078048 (unix time) try ""date -d @1467078048"" if you are using GNU date ***
PC: @     0x2b3690e4b897 cudnnAddTensor
**\* SIGSEGV (@0x703105204) received by PID 31656 (TID 0x2b3682bce3c0) from PID 51401220; stack trace: ***
    @     0x2b3689a1f340 (unknown)
    @     0x2b3690e4b897 cudnnAddTensor
    @     0x2b3688de8463 caffe::CuDNNConvolutionLayer<>::Forward_gpu()
    @           0x48df76 caffe::Layer<>::Forward()
    @           0x93bd0f caffe::CuDNNConvolutionLayerTest_TestSimpleConvolutionCuDNN_Test<>::TestBody()
    @           0x9a4e53 testing::internal::HandleExceptionsInMethodIfSupported<>()
    @           0x99bb37 testing::Test::Run()
    @           0x99bbde testing::TestInfo::Run()
    @           0x99bce5 testing::TestCase::Run()
    @           0x99f028 testing::internal::UnitTestImpl::RunAllTests()
    @           0x99f2b7 testing::UnitTest::Run()
    @           0x473c7f main
    @     0x2b3689c4eec5 __libc_start_main
    @           0x47ca69 (unknown)
make: **\* [runtest] Segmentation fault (core dumped)
"
caffe,1143,"I am a recruit, so I am sorry to my foolish question.
When I use the cuDNN to the Caffe-master, I get the same result without cuDNN.

My PC is i7-4770k and GTX 770, CUDA 6.5
The running time on MNIST is all 150s, and the cifar10_full is 62s / 1000 Iteration.

I set "" USE_CUDNN :=1"", and copy ""cudnn.h"" to ""/usr/local/include"", copy ""libcudnn.so, libcudnn.so.6.5, libcudnn.so.6.5.18, libcudnn_static.a"" to ""/usr/local/lib"".

Am I right? 
It seems not work.
Help me, thank you!
",0,Am I right ? when I use the cuDNN....,"Am I right ? when I use the cuDNN.... I am a recruit, so I am sorry to my foolish question.
When I use the cuDNN to the Caffe-master, I get the same result without cuDNN.

My PC is i7-4770k and GTX 770, CUDA 6.5
The running time on MNIST is all 150s, and the cifar10_full is 62s / 1000 Iteration.

I set "" USE_CUDNN :=1"", and copy ""cudnn.h"" to ""/usr/local/include"", copy ""libcudnn.so, libcudnn.so.6.5, libcudnn.so.6.5.18, libcudnn_static.a"" to ""/usr/local/lib"".

Am I right? 
It seems not work.
Help me, thank you!
"
caffe,4205,"Hi,

I am trying to build caffe with python 3.4 on ubuntu 14.04 but I keep having a issue with python : 

I installed : 
- OpenCV 3 with python 3 support (it worked nicely)
- Protobuf v3 beta

But still : 



and : 



I tried adding in ~/.bashrc : 



But it didn't work any better

Here is the full cmake log : 


",0,Cmake regex error in dependencies with python 3,"Cmake regex error in dependencies with python 3 Hi,

I am trying to build caffe with python 3.4 on ubuntu 14.04 but I keep having a issue with python : 

I installed : 
- OpenCV 3 with python 3 support (it worked nicely)
- Protobuf v3 beta

But still : 



and : 



I tried adding in ~/.bashrc : 



But it didn't work any better

Here is the full cmake log : 


"
caffe,5910,"Hello there! I was trying to run the cifar10 caffar10 example, the implementation of create_cifar10.sh regardless of the caffe directory or in the caffe / examples / cifar10 directory implementation, the system is always error: ./ build / examples / cifar10 / convert_cifar_data. bin: not found. Please help the big god to help doubts.
",0,create_cifar10.sh: 13: create_cifar10.sh: ./build/examples/cifar10/convert_cifar_data.bin: not found,"create_cifar10.sh: 13: create_cifar10.sh: ./build/examples/cifar10/convert_cifar_data.bin: not found Hello there! I was trying to run the cifar10 caffar10 example, the implementation of create_cifar10.sh regardless of the caffe directory or in the caffe / examples / cifar10 directory implementation, the system is always error: ./ build / examples / cifar10 / convert_cifar_data. bin: not found. Please help the big god to help doubts.
"
caffe,6311,"
### Your system configuration
Operating system:ubuntu16.04
Compiler:
CUDA version (if applicable):8.0
CUDNN version (if applicable):7.0.5
BLAS:
Python or MATLAB version (for pycaffe and matcaffe respectively):2.7(anaconda3)


Hope somebody could help me.Thank you!",0,failed to make all,"failed to make all 
### Your system configuration
Operating system:ubuntu16.04
Compiler:
CUDA version (if applicable):8.0
CUDNN version (if applicable):7.0.5
BLAS:
Python or MATLAB version (for pycaffe and matcaffe respectively):2.7(anaconda3)


Hope somebody could help me.Thank you!"
caffe,3546,"Sorry false issue. 
",0,Caffe Crashes when HDF5 data used on multi GPU system.,"Caffe Crashes when HDF5 data used on multi GPU system. Sorry false issue. 
"
caffe,3667,"Is there a difference in the way the predictions work in the case of caffe.Classifier() and caffe.Net()? I have an image (linked below) that when run in the caffe-example (classification.ipynb) gives a prediction of class 287 (net.predict([img]).argmax()). However, if I use this image in the another example (filter_visualization.ipynb) the class probabilites (net.blobs['prob'].argmax()) comes out to be 2! Is there something trivial that I am missing or this really shouldn't be happening? Which of these is the correct classification if at all one is correct (I have tweaked the cat image a little bit to see what are the effects on classification)?

I am using the bvlc_reference_caffenet.caffemodel and the corresponding deploy.prototxt with the imagenet mean image provided in caffe as default (ilsvrc12). 

Sample cat image : ![advr_out](https://cloud.githubusercontent.com/assets/5744373/13016016/419b72b0-d1e2-11e5-9019-2a1791c56ad4.jpg)
",0,caffe.Classifier() and caffe.Net() have different class predictions for same image?,"caffe.Classifier() and caffe.Net() have different class predictions for same image? Is there a difference in the way the predictions work in the case of caffe.Classifier() and caffe.Net()? I have an image (linked below) that when run in the caffe-example (classification.ipynb) gives a prediction of class 287 (net.predict([img]).argmax()). However, if I use this image in the another example (filter_visualization.ipynb) the class probabilites (net.blobs['prob'].argmax()) comes out to be 2! Is there something trivial that I am missing or this really shouldn't be happening? Which of these is the correct classification if at all one is correct (I have tweaked the cat image a little bit to see what are the effects on classification)?

I am using the bvlc_reference_caffenet.caffemodel and the corresponding deploy.prototxt with the imagenet mean image provided in caffe as default (ilsvrc12). 

Sample cat image : ![advr_out](https://cloud.githubusercontent.com/assets/5744373/13016016/419b72b0-d1e2-11e5-9019-2a1791c56ad4.jpg)
"
caffe,2566,"I get the following errors when doing make all in Caffe. I have these packages installed already.
/usr/bin/ld: cannot find -lleveldb
/usr/bin/ld: cannot find -lsnappy
/usr/bin/ld: cannot find -lboost_system
Any suggestions will be helpful,
Thanks
LM
",0,caffe make error,"caffe make error I get the following errors when doing make all in Caffe. I have these packages installed already.
/usr/bin/ld: cannot find -lleveldb
/usr/bin/ld: cannot find -lsnappy
/usr/bin/ld: cannot find -lboost_system
Any suggestions will be helpful,
Thanks
LM
"
caffe,3246,"Hi,

I am new to CAFFE. For training phase, it makes sense that only one type is supported. Because we mostly pre-process the training images. But for testing phase, sometimes we want to validate ImageNet accuracy, and this is suitable for ""data"" input. Sometimes, I also want to use the same prototxt model to predict one single raw image, and this needs the input type is ""imagedata"". 

I know Pycaffe can easily handle this, which is shown in the 00-example. I don't know whether CPP interface already supports this. Otherwise, I would like to modify caffe.cpp to support this. My idea is to add another input TEST layer in the prototxt model.  And Only one input layer is selected by command line.Or maybe you guys have better solutions? :) 

Thanks a lot in advance.  
",0,[test] support several types of TEST input at one prototxt model,"[test] support several types of TEST input at one prototxt model Hi,

I am new to CAFFE. For training phase, it makes sense that only one type is supported. Because we mostly pre-process the training images. But for testing phase, sometimes we want to validate ImageNet accuracy, and this is suitable for ""data"" input. Sometimes, I also want to use the same prototxt model to predict one single raw image, and this needs the input type is ""imagedata"". 

I know Pycaffe can easily handle this, which is shown in the 00-example. I don't know whether CPP interface already supports this. Otherwise, I would like to modify caffe.cpp to support this. My idea is to add another input TEST layer in the prototxt model.  And Only one input layer is selected by command line.Or maybe you guys have better solutions? :) 

Thanks a lot in advance.  
"
caffe,3833,"Hi there,

I don't whether I make a mistake or not, I found that the bias in the 1x1 conv layer is not learnt (which is exactly the initialization value) in the latest version of caffe, even if I double check that the second mult_lr is set as 2.0 in my prototxt. Could anyone help to load their google net and check the bias value of some 1x1 layers, such as ""conv2/3x3_reduce""? Apologize if I make something wrong. :)
",0,The bias is not learnt in the 1x1 conv layer,"The bias is not learnt in the 1x1 conv layer Hi there,

I don't whether I make a mistake or not, I found that the bias in the 1x1 conv layer is not learnt (which is exactly the initialization value) in the latest version of caffe, even if I double check that the second mult_lr is set as 2.0 in my prototxt. Could anyone help to load their google net and check the bias value of some 1x1 layers, such as ""conv2/3x3_reduce""? Apologize if I make something wrong. :)
"
caffe,2661,"Is there a way to specify a specific layer to run on the cpu rather than the gpu, but still let the other layers run on the gpu? I'm trying to run SoftmaxWithLossLayer on the cpu since we are getting issues with running it on the gpu. It would be a nice feature to have if it doesn't exist already.
",0,Specify Per Layer CPU or GPU,"Specify Per Layer CPU or GPU Is there a way to specify a specific layer to run on the cpu rather than the gpu, but still let the other layers run on the gpu? I'm trying to run SoftmaxWithLossLayer on the cpu since we are getting issues with running it on the gpu. It would be a nice feature to have if it doesn't exist already.
"
caffe,2506,"Hello guys, I wanna ask can I use caffe to pretrain a model, and then use the papameters of this model to initialize another model ?
If it can, can anyone teach me how to do it? Thank you.
",0,Pretrain a model using caffe,"Pretrain a model using caffe Hello guys, I wanna ask can I use caffe to pretrain a model, and then use the papameters of this model to initialize another model ?
If it can, can anyone teach me how to do it? Thank you.
"
caffe,2362,"Hi,

I have an older  MacBook Pro (2009) with:
NVIDIA GeForce 9600M GT
NVIDIA GeForce 9400M
CUDA Driver Version: 6.5.14

Mathematica, Matlab,  and Theano can't use the GPU.
I've read that cuDNN requires CUDA 3.0 (or greater).
Can Caffe run with CUDA 1.1?

Thanks in advance!
",0,Caffe and CUDA 1.1,"Caffe and CUDA 1.1 Hi,

I have an older  MacBook Pro (2009) with:
NVIDIA GeForce 9600M GT
NVIDIA GeForce 9400M
CUDA Driver Version: 6.5.14

Mathematica, Matlab,  and Theano can't use the GPU.
I've read that cuDNN requires CUDA 3.0 (or greater).
Can Caffe run with CUDA 1.1?

Thanks in advance!
"
caffe,2584,"Running



in  makes no difference.
",0,Unit-Tests do not use custom parameters,"Unit-Tests do not use custom parameters Running



in  makes no difference.
"
caffe,849,"I wanna know how to resume training ImageNet model with ""caffe_reference_imagenet_model"". As far as I am concert, to resume training needs the *.solverstate file not a model file.
Could I download the  ""caffe_reference_imagenet_model.solverstate"" somewhere?
Or how to transform "" caffe_reference_imagenet_model"" to  "" caffe_reference_imagenet_model.solverstate""?
",0,How to resume training ImageNet model?,"How to resume training ImageNet model? I wanna know how to resume training ImageNet model with ""caffe_reference_imagenet_model"". As far as I am concert, to resume training needs the *.solverstate file not a model file.
Could I download the  ""caffe_reference_imagenet_model.solverstate"" somewhere?
Or how to transform "" caffe_reference_imagenet_model"" to  "" caffe_reference_imagenet_model.solverstate""?
"
caffe,284,"Has this system been tested on inputs that have unequal width and height?  If I use the provided DataLayer with both mirroring and cropping disabled on portrait inputs, the first convolution layer quickly tends towards (usually within the first 500 iterations) a set of filters with horizontal bands.  Sometimes the loss goes to nan (and many of the parameters in the first convolution layer have gone to nan).

I wrote a separate data layer that replaces cropsize with crop_height and crop_width so that I could manually specify a rectangular crop.  I've tried setting crop_height == crop_width and so far the issue hasn't shown up, but I haven't spent much time training (but still much longer than the rectangular cases).  If I set the aspect ratio to something other than 1, then the bands do appear and their width seems to depend on the aspect ratio of the crop.

I'm using the imagenet architecture applied to a different dataset.  The inputs from leveldb are 264x105x3 and the cropped inputs to the network are 220x88x3.

The issue is reproducible with both solver mode 0 and 1.
",0,Non-square inputs - tested and supported?,"Non-square inputs - tested and supported? Has this system been tested on inputs that have unequal width and height?  If I use the provided DataLayer with both mirroring and cropping disabled on portrait inputs, the first convolution layer quickly tends towards (usually within the first 500 iterations) a set of filters with horizontal bands.  Sometimes the loss goes to nan (and many of the parameters in the first convolution layer have gone to nan).

I wrote a separate data layer that replaces cropsize with crop_height and crop_width so that I could manually specify a rectangular crop.  I've tried setting crop_height == crop_width and so far the issue hasn't shown up, but I haven't spent much time training (but still much longer than the rectangular cases).  If I set the aspect ratio to something other than 1, then the bands do appear and their width seems to depend on the aspect ratio of the crop.

I'm using the imagenet architecture applied to a different dataset.  The inputs from leveldb are 264x105x3 and the cropped inputs to the network are 220x88x3.

The issue is reproducible with both solver mode 0 and 1.
"
caffe,4067,"The new release of multiGPU Caffe may not correct. But it is quite different from NVCaffe.
",0,Does the new release verify the MultiGPU Caffe?,"Does the new release verify the MultiGPU Caffe? The new release of multiGPU Caffe may not correct. But it is quite different from NVCaffe.
"
caffe,3727,"Could it be that there is a mistake in the MNIST example: https://github.com/BVLC/caffe/blob/master/examples/mnist/lenet_train_test.prototxt . 

The conv layers are not followed by any non-linearities. In the original paper they were followed by sigmoids. 

In this tutorial here http://caffe.berkeleyvision.org/gathered/examples/mnist.html, it is written that the sigmoids are replaced by ReLUs. However, there is only one ReLU after the FC layer. 

What is even more confusing to me, is that the network without the non-linearities still seems to perform extremely well on the classification... Any idea why that is? 
",0,No non-linearity after conv layers in MNIST example,"No non-linearity after conv layers in MNIST example Could it be that there is a mistake in the MNIST example: https://github.com/BVLC/caffe/blob/master/examples/mnist/lenet_train_test.prototxt . 

The conv layers are not followed by any non-linearities. In the original paper they were followed by sigmoids. 

In this tutorial here http://caffe.berkeleyvision.org/gathered/examples/mnist.html, it is written that the sigmoids are replaced by ReLUs. However, there is only one ReLU after the FC layer. 

What is even more confusing to me, is that the network without the non-linearities still seems to perform extremely well on the classification... Any idea why that is? 
"
caffe,5680,"After hundreds iterations,it crashed.And the nvidia-smi reveal ""The GPU is lost.Reboot the system to recover the GPU"",and this situation just happened when I use the hdf5 data to train model.
",0,Problem in training process,"Problem in training process After hundreds iterations,it crashed.And the nvidia-smi reveal ""The GPU is lost.Reboot the system to recover the GPU"",and this situation just happened when I use the hdf5 data to train model.
"
caffe,1127,"Hi, good job, I'm a big fan.

I'm using the pretrained CaffeNet model. **At test time** I don't want the model to do the 10 different cropping and reflections. I only want the model to scale my image to 224 x 224 and compute the result. (I will then pickup the  features.)

Is there a way to do this without retraining?
",0,pretrained imagnet model: not doing cropping and reflections at test time,"pretrained imagnet model: not doing cropping and reflections at test time Hi, good job, I'm a big fan.

I'm using the pretrained CaffeNet model. **At test time** I don't want the model to do the 10 different cropping and reflections. I only want the model to scale my image to 224 x 224 and compute the result. (I will then pickup the  features.)

Is there a way to do this without retraining?
"
caffe,3142,,0,src/caffe/data_transformer.cpp:229: error: invalid use of incomplete type const struct cv::Mat,
caffe,4462,"When I run my network with a simple dummy Python layer it terminates after some hundred cycles. The displayed errors always point into cuda code. This error occured after i changed the label size from a large size (1x480x640) to (1x1x1).

Some system information:













[reproduce.zip](https://github.com/BVLC/caffe/files/364050/reproduce.zip)
[fail.txt](https://github.com/BVLC/caffe/files/364052/fail.txt)
",0,Nondeterministic crashes with PythonLayer,"Nondeterministic crashes with PythonLayer When I run my network with a simple dummy Python layer it terminates after some hundred cycles. The displayed errors always point into cuda code. This error occured after i changed the label size from a large size (1x480x640) to (1x1x1).

Some system information:













[reproduce.zip](https://github.com/BVLC/caffe/files/364050/reproduce.zip)
[fail.txt](https://github.com/BVLC/caffe/files/364052/fail.txt)
"
caffe,4634,,0,A demo question about gpu,
caffe,4276,"i meet a strange question:
when i defines a net using caffe.Net, there is an error:
AttributeError: 'module' object has no attribute 'Net'

> > > import caffe
> > > net=caffe.Net()
> > > Traceback (most recent call last):
> > >   File ""<stdin>"", line 1, in <module>
> > > AttributeError: 'module' object has no attribute 'Net'

And there is another one:
python /././caffe/python/classify.py --model ./bvlc_reference_caffenet/deploy.prototxt --weights ./bvlc_reference_caffenet/bvlc-reference_caffenet.caffemodel --gpu

Failed to include caffe_pb2, things might go wrong!
Traceback (most recent call last):
  File ""/usr/local/lib/python3.4/dist-packages/google/protobuf/internal/python_message.py"", line 1091, in MergeFromString
    if self._InternalParse(serialized, 0, length) != length:
  File ""/usr/local/lib/python3.4/dist-packages/google/protobuf/internal/python_message.py"", line 1113, in InternalParse
    (tag_bytes, new_pos) = local_ReadTag(buffer, pos)
  File ""/usr/local/lib/python3.4/dist-packages/google/protobuf/internal/decoder.py"", line 181, in ReadTag
    while six.indexbytes(buffer, pos) & 0x80:
TypeError: unsupported operand type(s) for &: 'str' and 'int'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/home/ndscbigdata/caffe/python/classify.py"", line 14, in <module>
    import caffe
  File ""/home/ndscbigdata/caffe/python/caffe/**init**.py"", line 4, in <module>
    from .proto.caffe_pb2 import TRAIN, TEST
  File ""/home/ndscbigdata/caffe/python/caffe/proto/caffe_pb2.py"", line 799, in <module>
    options=_descriptor._ParseOptions(descriptor_pb2.FieldOptions(), '\020\001')),
  File ""/usr/local/lib/python3.4/dist-packages/google/protobuf/descriptor.py"", line 845, in _ParseOptions
    message.ParseFromString(string)
  File ""/usr/local/lib/python3.4/dist-packages/google/protobuf/message.py"", line 185, in ParseFromString
    self.MergeFromString(serialized)
  File ""/usr/local/lib/python3.4/dist-packages/google/protobuf/internal/python_message.py"", line 1097, in MergeFromString
    raise message_mod.DecodeError('Truncated message.')
google.protobuf.message.DecodeError: Truncated message.
",0,AttributeError: 'module' object has no attribute 'Net',"AttributeError: 'module' object has no attribute 'Net' i meet a strange question:
when i defines a net using caffe.Net, there is an error:
AttributeError: 'module' object has no attribute 'Net'

> > > import caffe
> > > net=caffe.Net()
> > > Traceback (most recent call last):
> > >   File ""<stdin>"", line 1, in <module>
> > > AttributeError: 'module' object has no attribute 'Net'

And there is another one:
python /././caffe/python/classify.py --model ./bvlc_reference_caffenet/deploy.prototxt --weights ./bvlc_reference_caffenet/bvlc-reference_caffenet.caffemodel --gpu

Failed to include caffe_pb2, things might go wrong!
Traceback (most recent call last):
  File ""/usr/local/lib/python3.4/dist-packages/google/protobuf/internal/python_message.py"", line 1091, in MergeFromString
    if self._InternalParse(serialized, 0, length) != length:
  File ""/usr/local/lib/python3.4/dist-packages/google/protobuf/internal/python_message.py"", line 1113, in InternalParse
    (tag_bytes, new_pos) = local_ReadTag(buffer, pos)
  File ""/usr/local/lib/python3.4/dist-packages/google/protobuf/internal/decoder.py"", line 181, in ReadTag
    while six.indexbytes(buffer, pos) & 0x80:
TypeError: unsupported operand type(s) for &: 'str' and 'int'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/home/ndscbigdata/caffe/python/classify.py"", line 14, in <module>
    import caffe
  File ""/home/ndscbigdata/caffe/python/caffe/**init**.py"", line 4, in <module>
    from .proto.caffe_pb2 import TRAIN, TEST
  File ""/home/ndscbigdata/caffe/python/caffe/proto/caffe_pb2.py"", line 799, in <module>
    options=_descriptor._ParseOptions(descriptor_pb2.FieldOptions(), '\020\001')),
  File ""/usr/local/lib/python3.4/dist-packages/google/protobuf/descriptor.py"", line 845, in _ParseOptions
    message.ParseFromString(string)
  File ""/usr/local/lib/python3.4/dist-packages/google/protobuf/message.py"", line 185, in ParseFromString
    self.MergeFromString(serialized)
  File ""/usr/local/lib/python3.4/dist-packages/google/protobuf/internal/python_message.py"", line 1097, in MergeFromString
    raise message_mod.DecodeError('Truncated message.')
google.protobuf.message.DecodeError: Truncated message.
"
caffe,1993,"Hi,

When I tried to train the model of bvlc_reference_caffenet by my own data set, I have a problem: can anyone tell me how to fix it?

![screen shot 2015-02-26 at 11 18 01 pm](https://cloud.githubusercontent.com/assets/11225386/6409029/76ee1e34-be12-11e4-8e3c-4be0f42a528c.png)
",0,Error: (unix time) try if you are using GNU date,"Error: (unix time) try if you are using GNU date Hi,

When I tried to train the model of bvlc_reference_caffenet by my own data set, I have a problem: can anyone tell me how to fix it?

![screen shot 2015-02-26 at 11 18 01 pm](https://cloud.githubusercontent.com/assets/11225386/6409029/76ee1e34-be12-11e4-8e3c-4be0f42a528c.png)
"
caffe,5173,"the step make all, the following error

build_release/lib/libcaffe.sojpeg_CreateCompress
.build_release/lib/libcaffe.sogoogle::protobuf::DescriptorPool::FindFileByName(std::string const&) const
.build_release/lib/libcaffe.soImf_2_2::OutputFile::~OutputFile()
.build_release/lib/libcaffe.sogoogle::protobuf::internal::WireFormatLite::ReadBytes(google::protobuf::io::CodedInputStream*, std::string*)   (  undefine ref  )
.build_release/lib/libcaffe.sopng_create_info_struct
.build_release/lib/libcaffe.sojpeg_std_error
.build_release/lib/libcaffe.sogoogle::protobuf::Message::InitializationErrorString() const
.build_release/lib/libcaffe.sopng_set_compression_level
collect2: error: ld returned 1 exit status
Makefile:620: recipe for target '.build_release/tools/compute_image_mean.bin' failed
make: *** [.build_release/tools/compute_image_mean.bin] Error 1


",0,vmware ubuntu16 undefined issue,"vmware ubuntu16 undefined issue the step make all, the following error

build_release/lib/libcaffe.sojpeg_CreateCompress
.build_release/lib/libcaffe.sogoogle::protobuf::DescriptorPool::FindFileByName(std::string const&) const
.build_release/lib/libcaffe.soImf_2_2::OutputFile::~OutputFile()
.build_release/lib/libcaffe.sogoogle::protobuf::internal::WireFormatLite::ReadBytes(google::protobuf::io::CodedInputStream*, std::string*)   (  undefine ref  )
.build_release/lib/libcaffe.sopng_create_info_struct
.build_release/lib/libcaffe.sojpeg_std_error
.build_release/lib/libcaffe.sogoogle::protobuf::Message::InitializationErrorString() const
.build_release/lib/libcaffe.sopng_set_compression_level
collect2: error: ld returned 1 exit status
Makefile:620: recipe for target '.build_release/tools/compute_image_mean.bin' failed
make: *** [.build_release/tools/compute_image_mean.bin] Error 1


"
caffe,2078,"Hello, I suspect that this might be a bug introduced after the commit 3d30510 (PR #2059). 

The environment with which I've tested is: Mac OS X 10.10. I haven't tested with Ubuntu, since I don't have any Linux machine with matlab installed as of now.



However, building matcaffe fails with the message .



Let's see the command line argument passed (absolute paths were replaced by ).
There are python-related link flags,  and , which were added from 



The reason why  fails might be that the ordering of link flags ( and ) has changed in the internal invocation, and thus it was not able to link the  dynamic module.



This build step with cmake, succeeds prior to 3d30510.

As I am doubtful these flags (e.g. ) are necessary to build matlab targets, these flags could be removed when building matlab bindings so that the build could be successful out-of-box. Currently, I tried a workaround setting  to , but after inspection to find better solution, I think I could submit a PR to fix this. Thanks.
",0,cmake with -DBUILD_matlab=ON fails,"cmake with -DBUILD_matlab=ON fails Hello, I suspect that this might be a bug introduced after the commit 3d30510 (PR #2059). 

The environment with which I've tested is: Mac OS X 10.10. I haven't tested with Ubuntu, since I don't have any Linux machine with matlab installed as of now.



However, building matcaffe fails with the message .



Let's see the command line argument passed (absolute paths were replaced by ).
There are python-related link flags,  and , which were added from 



The reason why  fails might be that the ordering of link flags ( and ) has changed in the internal invocation, and thus it was not able to link the  dynamic module.



This build step with cmake, succeeds prior to 3d30510.

As I am doubtful these flags (e.g. ) are necessary to build matlab targets, these flags could be removed when building matlab bindings so that the build could be successful out-of-box. Currently, I tried a workaround setting  to , but after inspection to find better solution, I think I could submit a PR to fix this. Thanks.
"
caffe,2792,"I am installing Caffe on a Mac Book. I am able to get it to compile and run in CPU only mode, but cannot get it to compile for the GPU. I have followed the [installation instructions](http://caffe.berkeleyvision.org/installation.html) and am not sure what to do next.

This is a Mac Book Pro running OS X 10.10.4. The graphics card is an Intel Iris Pro GPU. I am using Python from the Anaconda distribution.

I installed dependencies as outlined on the [""OS X Installation"" page](http://caffe.berkeleyvision.org/install_osx.html) via Homebrew.  I installed CUDA 7 for Mac OS from the [CUDA website](https://developer.nvidia.com/cuda-downloads).  The CUDA tools are installed beneath /usr/local/cuda.

I cloned the Caffe distribution from https://github.com/BVLC/caffe. I am on commit 247d6d66 from July 15, 2015.

In Makefile.config I uncommented the Python Anaconda include lines but otherwise left things the same.

make clean;make all builds a lot of .cpp files but fails the first time it tries to use the NVCC compiler



sp_counted_base_clang.hpp looks fine to me. Line 27 is the first line that is neither an include or namespace statement. The only two includes preceding it are



Maybe there is something wrong with my Boost configuration, although I was able to install Boost via Homebrew without a problem.

If I uncomment ""CPU_ONLY := 1"" in Makefile.config and do another clean build, I do not get any compiler errors.

(When I try to run the tests I have the problem described in issue  https://github.com/BVLC/caffe/issues/1737. I work around this by soft linking my libhdf58 libraries to libhdf9 and specifying the hdf5 headers from /usr/local/include/ in the COMMON_FLAGS in Makefile. I do not think this this is related to my CUDA compilation issue.)

Do you have any ideas why the CUDA code is not compiling for me?
",0,CUDA Compilation issue on MacBook,"CUDA Compilation issue on MacBook I am installing Caffe on a Mac Book. I am able to get it to compile and run in CPU only mode, but cannot get it to compile for the GPU. I have followed the [installation instructions](http://caffe.berkeleyvision.org/installation.html) and am not sure what to do next.

This is a Mac Book Pro running OS X 10.10.4. The graphics card is an Intel Iris Pro GPU. I am using Python from the Anaconda distribution.

I installed dependencies as outlined on the [""OS X Installation"" page](http://caffe.berkeleyvision.org/install_osx.html) via Homebrew.  I installed CUDA 7 for Mac OS from the [CUDA website](https://developer.nvidia.com/cuda-downloads).  The CUDA tools are installed beneath /usr/local/cuda.

I cloned the Caffe distribution from https://github.com/BVLC/caffe. I am on commit 247d6d66 from July 15, 2015.

In Makefile.config I uncommented the Python Anaconda include lines but otherwise left things the same.

make clean;make all builds a lot of .cpp files but fails the first time it tries to use the NVCC compiler



sp_counted_base_clang.hpp looks fine to me. Line 27 is the first line that is neither an include or namespace statement. The only two includes preceding it are



Maybe there is something wrong with my Boost configuration, although I was able to install Boost via Homebrew without a problem.

If I uncomment ""CPU_ONLY := 1"" in Makefile.config and do another clean build, I do not get any compiler errors.

(When I try to run the tests I have the problem described in issue  https://github.com/BVLC/caffe/issues/1737. I work around this by soft linking my libhdf58 libraries to libhdf9 and specifying the hdf5 headers from /usr/local/include/ in the COMMON_FLAGS in Makefile. I do not think this this is related to my CUDA compilation issue.)

Do you have any ideas why the CUDA code is not compiling for me?
"
caffe,5783,"[----------] 8 tests from SliceLayerTest/2 (465 ms total)

[----------] Global test environment tear-down
[==========] 2101 tests from 277 test cases ran. (279573 ms total)
[  PASSED  ] 2097 tests.
[  FAILED  ] 4 tests, listed below:
[  FAILED  ] LayerFactoryTest/0.TestCreateLayer, where TypeParam = caffe::CPUDevice<float>
[  FAILED  ] LayerFactoryTest/1.TestCreateLayer, where TypeParam = caffe::CPUDevice<double>
[  FAILED  ] LayerFactoryTest/2.TestCreateLayer, where TypeParam = caffe::GPUDevice<float>
[  FAILED  ] LayerFactoryTest/3.TestCreateLayer, where TypeParam = caffe::GPUDevice<double>

 4 FAILED TESTS
Makefile:534: recipe for target 'runtest' failed
make: *** [runtest] Error 1

",0,error while make runtest  about layerfactorytest,"error while make runtest  about layerfactorytest [----------] 8 tests from SliceLayerTest/2 (465 ms total)

[----------] Global test environment tear-down
[==========] 2101 tests from 277 test cases ran. (279573 ms total)
[  PASSED  ] 2097 tests.
[  FAILED  ] 4 tests, listed below:
[  FAILED  ] LayerFactoryTest/0.TestCreateLayer, where TypeParam = caffe::CPUDevice<float>
[  FAILED  ] LayerFactoryTest/1.TestCreateLayer, where TypeParam = caffe::CPUDevice<double>
[  FAILED  ] LayerFactoryTest/2.TestCreateLayer, where TypeParam = caffe::GPUDevice<float>
[  FAILED  ] LayerFactoryTest/3.TestCreateLayer, where TypeParam = caffe::GPUDevice<double>

 4 FAILED TESTS
Makefile:534: recipe for target 'runtest' failed
make: *** [runtest] Error 1

"
caffe,3467,"I have been trying to train a siamese CNN where each branch is VGG, and transfer learn it from VGG-16.. the problem is when I try to save the network after copying the W,b over at each layer, the network.save('.caffemodel') Seg faults and does not save the model correctly. I believe it is because the network is too big? Is there any way to get around this? Here is the gist: 

https://gist.github.com/Fchaubard/b40ccfe7b76043060e2f

and if you simply do..
model_path = '/some_path/siamese_vgg.prototxt'
network = caffe.Net(model_path, caffe.TEST)
network.save('/some_path/siamese_vgg.caffemodel')
you will get a ""Segmentation fault (core dumped)""

I removed the weight sharing in each layer to ensure it was the size and not something involving the shared params. I also tried doing a forward backward to see if it was a lazy instantiation issue. To no avail.  

-- UPDATE --
I just attempted on a different server and it seemed to work. Trying to track down the reason... the .caffemodel is 1.1 GB. And I have more than that available on the original server so I am not sure what the reason for this discrepancy in functionality.
",0,network.save() of very large network Seg Faults,"network.save() of very large network Seg Faults I have been trying to train a siamese CNN where each branch is VGG, and transfer learn it from VGG-16.. the problem is when I try to save the network after copying the W,b over at each layer, the network.save('.caffemodel') Seg faults and does not save the model correctly. I believe it is because the network is too big? Is there any way to get around this? Here is the gist: 

https://gist.github.com/Fchaubard/b40ccfe7b76043060e2f

and if you simply do..
model_path = '/some_path/siamese_vgg.prototxt'
network = caffe.Net(model_path, caffe.TEST)
network.save('/some_path/siamese_vgg.caffemodel')
you will get a ""Segmentation fault (core dumped)""

I removed the weight sharing in each layer to ensure it was the size and not something involving the shared params. I also tried doing a forward backward to see if it was a lazy instantiation issue. To no avail.  

-- UPDATE --
I just attempted on a different server and it seemed to work. Trying to track down the reason... the .caffemodel is 1.1 GB. And I have more than that available on the original server so I am not sure what the reason for this discrepancy in functionality.
"
caffe,4060,"/usr/lib/x86_64-linux-gnu/libunwind.so.8: undefined reference to lzma_index_size@XZ_5.0'
/usr/lib/x86_64-linux-gnu/libunwind.so.8: undefined reference to lzma_stream_footer_decode@XZ_5.0'
/usr/lib/x86_64-linux-gnu/libunwind.so.8: undefined reference to lzma_index_uncompressed_size@XZ_5.0'
collect2: error: ld returned 1 exit status
tools/CMakeFiles/finetune_net.dir/build.make:133: recipe for target 'tools/finetune_net' failed
make[2]: **\* [tools/finetune_net] Error 1
CMakeFiles/Makefile2:435: recipe for target 'tools/CMakeFiles/finetune_net.dir/all' failed
make[1]: **\* [tools/CMakeFiles/finetune_net.dir/all] Error 2
/usr/lib/x86_64-linux-gnu/libunwind.so.8: undefined reference to lzma_index_size@XZ_5.0'
/usr/lib/x86_64-linux-gnu/libunwind.so.8: undefined reference to lzma_stream_footer_decode@XZ_5.0'
/usr/lib/x86_64-linux-gnu/libunwind.so.8: undefined reference to lzma_index_uncompressed_size@XZ_5.0'
collect2: error: ld returned 1 exit status
tools/CMakeFiles/net_speed_benchmark.dir/build.make:133: recipe for target 'tools/net_speed_benchmark' failed
make[2]: **\* [tools/net_speed_benchmark] Error 1
CMakeFiles/Makefile2:625: recipe for target 'tools/CMakeFiles/net_speed_benchmark.dir/all' failed
make[1]: **\* [tools/CMakeFiles/net_speed_benchmark.dir/all] Error 2
/usr/lib/x86_64-linux-gnu/libunwind.so.8: undefined reference to lzma_index_size@XZ_5.0'
/usr/lib/x86_64-linux-gnu/libunwind.so.8: undefined reference to lzma_stream_footer_decode@XZ_5.0'
/usr/lib/x86_64-linux-gnu/libunwind.so.8: undefined reference to lzma_index_uncompressed_size@XZ_5.0'
collect2: error: ld returned 1 exit status
tools/CMakeFiles/train_net.dir/build.make:133: recipe for target 'tools/train_net' failed
make[2]: **\* [tools/train_net] Error 1
CMakeFiles/Makefile2:511: recipe for target 'tools/CMakeFiles/train_net.dir/all' failed
make[1]: **\* [tools/CMakeFiles/train_net.dir/all] Error 2
/usr/lib/x86_64-linux-gnu/libunwind.so.8: undefined reference to lzma_index_size@XZ_5.0'
/usr/lib/x86_64-linux-gnu/libunwind.so.8: undefined reference to lzma_stream_footer_decode@XZ_5.0'
/usr/lib/x86_64-linux-gnu/libunwind.so.8: undefined reference to lzma_index_uncompressed_size@XZ_5.0'
collect2: error: ld returned 1 exit status
tools/CMakeFiles/test_net.dir/build.make:133: recipe for target 'tools/test_net' failed
make[2]: **\* [tools/test_net] Error 1
CMakeFiles/Makefile2:549: recipe for target 'tools/CMakeFiles/test_net.dir/all' failed
make[1]: **\* [tools/CMakeFiles/test_net.dir/all] Error 2
[ 90%] Linking CXX executable extract_features
/usr/lib/x86_64-linux-gnu/libunwind.so.8: undefined reference to lzma_stream_footer_decode@XZ_5.0'
/usr/lib/x86_64-linux-gnu/libunwind.so.8: undefined reference to lzma_stream_buffer_decode@XZ_5.0'
/usr/lib/x86_64-linux-gnu/libunwind.so.8: undefined reference to lzma_index_end@XZ_5.0'
collect2: error: ld returned 1 exit status
tools/CMakeFiles/extract_features.dir/build.make:133: recipe for target 'tools/extract_features' failed
make[2]: **\* [tools/extract_features] Error 1
CMakeFiles/Makefile2:701: recipe for target 'tools/CMakeFiles/extract_features.dir/all' failed
make[1]: **\* [tools/CMakeFiles/extract_features.dir/all] Error 2
[ 90%] Linking CXX executable caffe
/usr/lib/x86_64-linux-gnu/libunwind.so.8: undefined reference to lzma_stream_footer_decode@XZ_5.0'
/usr/lib/x86_64-linux-gnu/libunwind.so.8: undefined reference to lzma_stream_buffer_decode@XZ_5.0'
/usr/lib/x86_64-linux-gnu/libunwind.so.8: undefined reference to lzma_index_end@XZ_5.0'
collect2: error: ld returned 1 exit status
tools/CMakeFiles/caffe.bin.dir/build.make:133: recipe for target 'tools/caffe' failed
make[2]: **\* [tools/caffe] Error 1
CMakeFiles/Makefile2:587: recipe for target 'tools/CMakeFiles/caffe.bin.dir/all' failed
make[1]: **\* [tools/CMakeFiles/caffe.bin.dir/all] Error 2
Makefile:127: recipe for target 'all' failed
make: **\* [all] Error 2
",0, undefined reference to `lzma_index_end@XZ_5.0'," undefined reference to `lzma_index_end@XZ_5.0' /usr/lib/x86_64-linux-gnu/libunwind.so.8: undefined reference to lzma_index_size@XZ_5.0'
/usr/lib/x86_64-linux-gnu/libunwind.so.8: undefined reference to lzma_stream_footer_decode@XZ_5.0'
/usr/lib/x86_64-linux-gnu/libunwind.so.8: undefined reference to lzma_index_uncompressed_size@XZ_5.0'
collect2: error: ld returned 1 exit status
tools/CMakeFiles/finetune_net.dir/build.make:133: recipe for target 'tools/finetune_net' failed
make[2]: **\* [tools/finetune_net] Error 1
CMakeFiles/Makefile2:435: recipe for target 'tools/CMakeFiles/finetune_net.dir/all' failed
make[1]: **\* [tools/CMakeFiles/finetune_net.dir/all] Error 2
/usr/lib/x86_64-linux-gnu/libunwind.so.8: undefined reference to lzma_index_size@XZ_5.0'
/usr/lib/x86_64-linux-gnu/libunwind.so.8: undefined reference to lzma_stream_footer_decode@XZ_5.0'
/usr/lib/x86_64-linux-gnu/libunwind.so.8: undefined reference to lzma_index_uncompressed_size@XZ_5.0'
collect2: error: ld returned 1 exit status
tools/CMakeFiles/net_speed_benchmark.dir/build.make:133: recipe for target 'tools/net_speed_benchmark' failed
make[2]: **\* [tools/net_speed_benchmark] Error 1
CMakeFiles/Makefile2:625: recipe for target 'tools/CMakeFiles/net_speed_benchmark.dir/all' failed
make[1]: **\* [tools/CMakeFiles/net_speed_benchmark.dir/all] Error 2
/usr/lib/x86_64-linux-gnu/libunwind.so.8: undefined reference to lzma_index_size@XZ_5.0'
/usr/lib/x86_64-linux-gnu/libunwind.so.8: undefined reference to lzma_stream_footer_decode@XZ_5.0'
/usr/lib/x86_64-linux-gnu/libunwind.so.8: undefined reference to lzma_index_uncompressed_size@XZ_5.0'
collect2: error: ld returned 1 exit status
tools/CMakeFiles/train_net.dir/build.make:133: recipe for target 'tools/train_net' failed
make[2]: **\* [tools/train_net] Error 1
CMakeFiles/Makefile2:511: recipe for target 'tools/CMakeFiles/train_net.dir/all' failed
make[1]: **\* [tools/CMakeFiles/train_net.dir/all] Error 2
/usr/lib/x86_64-linux-gnu/libunwind.so.8: undefined reference to lzma_index_size@XZ_5.0'
/usr/lib/x86_64-linux-gnu/libunwind.so.8: undefined reference to lzma_stream_footer_decode@XZ_5.0'
/usr/lib/x86_64-linux-gnu/libunwind.so.8: undefined reference to lzma_index_uncompressed_size@XZ_5.0'
collect2: error: ld returned 1 exit status
tools/CMakeFiles/test_net.dir/build.make:133: recipe for target 'tools/test_net' failed
make[2]: **\* [tools/test_net] Error 1
CMakeFiles/Makefile2:549: recipe for target 'tools/CMakeFiles/test_net.dir/all' failed
make[1]: **\* [tools/CMakeFiles/test_net.dir/all] Error 2
[ 90%] Linking CXX executable extract_features
/usr/lib/x86_64-linux-gnu/libunwind.so.8: undefined reference to lzma_stream_footer_decode@XZ_5.0'
/usr/lib/x86_64-linux-gnu/libunwind.so.8: undefined reference to lzma_stream_buffer_decode@XZ_5.0'
/usr/lib/x86_64-linux-gnu/libunwind.so.8: undefined reference to lzma_index_end@XZ_5.0'
collect2: error: ld returned 1 exit status
tools/CMakeFiles/extract_features.dir/build.make:133: recipe for target 'tools/extract_features' failed
make[2]: **\* [tools/extract_features] Error 1
CMakeFiles/Makefile2:701: recipe for target 'tools/CMakeFiles/extract_features.dir/all' failed
make[1]: **\* [tools/CMakeFiles/extract_features.dir/all] Error 2
[ 90%] Linking CXX executable caffe
/usr/lib/x86_64-linux-gnu/libunwind.so.8: undefined reference to lzma_stream_footer_decode@XZ_5.0'
/usr/lib/x86_64-linux-gnu/libunwind.so.8: undefined reference to lzma_stream_buffer_decode@XZ_5.0'
/usr/lib/x86_64-linux-gnu/libunwind.so.8: undefined reference to lzma_index_end@XZ_5.0'
collect2: error: ld returned 1 exit status
tools/CMakeFiles/caffe.bin.dir/build.make:133: recipe for target 'tools/caffe' failed
make[2]: **\* [tools/caffe] Error 1
CMakeFiles/Makefile2:587: recipe for target 'tools/CMakeFiles/caffe.bin.dir/all' failed
make[1]: **\* [tools/CMakeFiles/caffe.bin.dir/all] Error 2
Makefile:127: recipe for target 'all' failed
make: **\* [all] Error 2
"
caffe,1862,"Hi :) I wrote a code to shuffle the order of images when caffe use DataLayer (esp. LMDB). 

This set of codes are very simple, so I think there are many people who have their own code for them. However, I think this implementation will be helpful to others who are not familiar with caffe implementation. 

I'm not familiar with how I can share the code in this caffe repository. I would appreciate if you let me know how to share it.

Thanks :) 
",0,Want to share codes shuffling the order of images in training/testing for DataLayer (with LMDB) ,"Want to share codes shuffling the order of images in training/testing for DataLayer (with LMDB)  Hi :) I wrote a code to shuffle the order of images when caffe use DataLayer (esp. LMDB). 

This set of codes are very simple, so I think there are many people who have their own code for them. However, I think this implementation will be helpful to others who are not familiar with caffe implementation. 

I'm not familiar with how I can share the code in this caffe repository. I would appreciate if you let me know how to share it.

Thanks :) 
"
caffe,188,"e.g. auto-encoder, predictive sparse decomposition.
They would be very insightful for a newbie.
Thanks. 
",0,Examples of unsupervised feature learning?,"Examples of unsupervised feature learning? e.g. auto-encoder, predictive sparse decomposition.
They would be very insightful for a newbie.
Thanks. 
"
caffe,824,"I have CUDA 5.5 and CUDA 6.0 installed in /usr/local/, and the LD_LIBRARY_PATH to libcudart.so.6.0 is correct.
I run ""make runtest"" and it passed, but failed to run /caffe.mexa64.
Actually when I run sudo ldd ./caffe.mexa64, it said libcudart.so.6.0 not found.
Should I uninstall CUDA 5.5 first? Any suggestion?
FYI, after I mv /usr/local/cuda-5.5 away, I can't make caffe successfully because libopencv_core.so need the libcudart.so.5.5
",0,Invalid MEX-file '/path/to/rcnn/external/caffe/matlab/caffe/caffe.mexa64': libcudart.so.6.0: cannot open shared object file: No such file or directory,"Invalid MEX-file '/path/to/rcnn/external/caffe/matlab/caffe/caffe.mexa64': libcudart.so.6.0: cannot open shared object file: No such file or directory I have CUDA 5.5 and CUDA 6.0 installed in /usr/local/, and the LD_LIBRARY_PATH to libcudart.so.6.0 is correct.
I run ""make runtest"" and it passed, but failed to run /caffe.mexa64.
Actually when I run sudo ldd ./caffe.mexa64, it said libcudart.so.6.0 not found.
Should I uninstall CUDA 5.5 first? Any suggestion?
FYI, after I mv /usr/local/cuda-5.5 away, I can't make caffe successfully because libopencv_core.so need the libcudart.so.5.5
"
caffe,3405,"Hi guys,

I'm trying do cross-validation in caffe because I've a small set of images 3K, I'm do it this way:

split my train images in 10 subsets, then I generate the lmdb files, after this I train in subsets 1 to 9 and validation in 10, after this I finetuning this trained network in subsets 2 to 10 an validate in subset 1, and subsequently until pass for all subsets.

The problem is which the network suffers of overfiting because when I do finetuning my subset of validation already exposed to training phase.

So what I'm doing wrong? Is this way which do cross-validation in caffe?

I read about this in some places like github and here in caffe user too, but isn't clear.

If anyone help me I really thanks, I've lost a week on this and the problem persist.

Thanks guys. 
",0,Cross-Validation is a Bad idea in Deep Learning?,"Cross-Validation is a Bad idea in Deep Learning? Hi guys,

I'm trying do cross-validation in caffe because I've a small set of images 3K, I'm do it this way:

split my train images in 10 subsets, then I generate the lmdb files, after this I train in subsets 1 to 9 and validation in 10, after this I finetuning this trained network in subsets 2 to 10 an validate in subset 1, and subsequently until pass for all subsets.

The problem is which the network suffers of overfiting because when I do finetuning my subset of validation already exposed to training phase.

So what I'm doing wrong? Is this way which do cross-validation in caffe?

I read about this in some places like github and here in caffe user too, but isn't clear.

If anyone help me I really thanks, I've lost a week on this and the problem persist.

Thanks guys. 
"
caffe,6094,"### Issue summary
The function below in io.cpp, while function open() return -1, that means given a bad filename parameter, it will not return false but just give a log message. Will that cause crash? 

bool ReadProtoFromTextFile(const char* filename, Message* proto) {
  int fd = open(filename, O_RDONLY);
  CHECK_NE(fd, -1) << ""File not found: "" << filename;
  FileInputStream* input = new FileInputStream(fd);
  bool success = google::protobuf::TextFormat::Parse(input, proto);
  delete input;
  close(fd);
  return success;
}

bool ReadProtoFromBinaryFile(const char* filename, Message* proto) {
  int fd = open(filename, O_RDONLY);
  CHECK_NE(fd, -1) << ""File not found: "" << filename;
  ZeroCopyInputStream* raw_input = new FileInputStream(fd);
  CodedInputStream* coded_input = new CodedInputStream(raw_input);
  coded_input->SetTotalBytesLimit(kProtoReadBytesLimit, 536870912);

  bool success = proto->ParseFromCodedStream(coded_input);

  delete coded_input;
  delete raw_input;
  close(fd);
  return success;
}

",0,"The function ""bool ReadProtoFromTextFile(const char* filename, Message* proto)"" in util/io.cpp, what if given a bad filename parameter, the function will not return false?","The function ""bool ReadProtoFromTextFile(const char* filename, Message* proto)"" in util/io.cpp, what if given a bad filename parameter, the function will not return false? ### Issue summary
The function below in io.cpp, while function open() return -1, that means given a bad filename parameter, it will not return false but just give a log message. Will that cause crash? 

bool ReadProtoFromTextFile(const char* filename, Message* proto) {
  int fd = open(filename, O_RDONLY);
  CHECK_NE(fd, -1) << ""File not found: "" << filename;
  FileInputStream* input = new FileInputStream(fd);
  bool success = google::protobuf::TextFormat::Parse(input, proto);
  delete input;
  close(fd);
  return success;
}

bool ReadProtoFromBinaryFile(const char* filename, Message* proto) {
  int fd = open(filename, O_RDONLY);
  CHECK_NE(fd, -1) << ""File not found: "" << filename;
  ZeroCopyInputStream* raw_input = new FileInputStream(fd);
  CodedInputStream* coded_input = new CodedInputStream(raw_input);
  coded_input->SetTotalBytesLimit(kProtoReadBytesLimit, 536870912);

  bool success = proto->ParseFromCodedStream(coded_input);

  delete coded_input;
  delete raw_input;
  close(fd);
  return success;
}

"
caffe,1983,"in MemoryDataLayer
  top[0]->Reshape(batch_size_, channels_, height_, width_);
  top[1]->Reshape(batch_size_, 1, 1, 1);
  added_data_.Reshape(batch_size_, channels_, height_, width_);
  added_label_.Reshape(batch_size_, 1, 1, 1);

in ImageDataLayer
  if (crop_size > 0) {
    top[0]->Reshape(batch_size, channels, crop_size, crop_size);
    this->prefetch_data_.Reshape(batch_size, channels, crop_size, crop_size);
    this->transformed_data_.Reshape(1, channels, crop_size, crop_size);
  } else {
    top[0]->Reshape(batch_size, channels, height, width);
    this->prefetch_data_.Reshape(batch_size, channels, height, width);
    this->transformed_data_.Reshape(1, channels, height, width);
  }
When crop is used in MemoryDataLayer the result is wrong
",0,MemoryDataLayer performs transformations inconsistently,"MemoryDataLayer performs transformations inconsistently in MemoryDataLayer
  top[0]->Reshape(batch_size_, channels_, height_, width_);
  top[1]->Reshape(batch_size_, 1, 1, 1);
  added_data_.Reshape(batch_size_, channels_, height_, width_);
  added_label_.Reshape(batch_size_, 1, 1, 1);

in ImageDataLayer
  if (crop_size > 0) {
    top[0]->Reshape(batch_size, channels, crop_size, crop_size);
    this->prefetch_data_.Reshape(batch_size, channels, crop_size, crop_size);
    this->transformed_data_.Reshape(1, channels, crop_size, crop_size);
  } else {
    top[0]->Reshape(batch_size, channels, height, width);
    this->prefetch_data_.Reshape(batch_size, channels, height, width);
    this->transformed_data_.Reshape(1, channels, height, width);
  }
When crop is used in MemoryDataLayer the result is wrong
"
caffe,1917,"After switching to the current caffe version today, dynamic loading of several libraries, which contain different classifiers for specific tasks, does not work anymore. We've got the following error message:

[libprotobuf ERROR google/protobuf/descriptor_database.cc:57] File already exists in database: caffe.proto

Furthermore we are using Ubuntu 14.04 and the following version of libprotobuf:

 /usr/lib/x86_64-linux-gnu/libprotobuf.so.8
/usr/lib/x86_64-linux-gnu/libprotobuf.so.8.0.0

Before updating caffe everything was fine and allows for loading of more than one library containing a caffe net. Seems as if the part of building the caffe specific proto header and cc-file has changed. Is there any conncetion to our problem? And how can we fix our problem?

Best, Tom
",0,Problem with dynamic loading and caffe.proto,"Problem with dynamic loading and caffe.proto After switching to the current caffe version today, dynamic loading of several libraries, which contain different classifiers for specific tasks, does not work anymore. We've got the following error message:

[libprotobuf ERROR google/protobuf/descriptor_database.cc:57] File already exists in database: caffe.proto

Furthermore we are using Ubuntu 14.04 and the following version of libprotobuf:

 /usr/lib/x86_64-linux-gnu/libprotobuf.so.8
/usr/lib/x86_64-linux-gnu/libprotobuf.so.8.0.0

Before updating caffe everything was fine and allows for loading of more than one library containing a caffe net. Seems as if the part of building the caffe specific proto header and cc-file has changed. Is there any conncetion to our problem? And how can we fix our problem?

Best, Tom
"
caffe,1861,"Got an error during runtest
F0213 10:25:10.921640  9387 db.hpp:109] Check failed: mdb_status == 0 (-30792 vs. 0) MDB_MAP_FULL: Environment mapsize limit reached
**\* Check failure stack trace: ***
    @ 0x44a85060  (unknown)
    @ 0x44a84f5c  (unknown)
    @ 0x44a84b78  (unknown)
    @ 0x44a86f98  (unknown)
    @   0x265ec2  caffe::db::LMDBTransaction::Put()
    @   0x21b932  caffe::DBTest_TestWrite_Test<>::TestBody()
    @   0x230568  testing::internal::HandleExceptionsInMethodIfSupported<>()
    @   0x230568  testing::internal::HandleExceptionsInMethodIfSupported<>()
    @   0x230568  testing::internal::HandleExceptionsInMethodIfSupported<>()
    @   0x230568  testing::internal::HandleExceptionsInMethodIfSupported<>()
    @   0x230568  testing::internal::HandleExceptionsInMethodIfSupported<>()
    @   0x230568  testing::internal::HandleExceptionsInMethodIfSupported<>()
    @   0x230568  testing::internal::HandleExceptionsInMethodIfSupported<>()
    @   0x230568  testing::internal::HandleExceptionsInMethodIfSupported<>()
    @   0x230568  testing::internal::HandleExceptionsInMethodIfSupported<>()
    @   0x230568  testing::internal::HandleExceptionsInMethodIfSupported<>()
    @   0x230568  testing::internal::HandleExceptionsInMethodIfSupported<>()
    @   0x230568  testing::internal::HandleExceptionsInMethodIfSupported<>()
    @   0x230568  testing::internal::HandleExceptionsInMethodIfSupported<>()
    @   0x230568  testing::internal::HandleExceptionsInMethodIfSupported<>()
    @   0x230568  testing::internal::HandleExceptionsInMethodIfSupported<>()
    @   0x230568  testing::internal::HandleExceptionsInMethodIfSupported<>()
    @   0x230568  testing::internal::HandleExceptionsInMethodIfSupported<>()
    @   0x230568  testing::internal::HandleExceptionsInMethodIfSupported<>()
    @   0x230568  testing::internal::HandleExceptionsInMethodIfSupported<>()
    @   0x230568  testing::internal::HandleExceptionsInMethodIfSupported<>()
    @   0x230568  testing::internal::HandleExceptionsInMethodIfSupported<>()
    @   0x230568  testing::internal::HandleExceptionsInMethodIfSupported<>()
    @   0x230568  testing::internal::HandleExceptionsInMethodIfSupported<>()
    @   0x230568  testing::internal::HandleExceptionsInMethodIfSupported<>()
    @   0x230568  testing::internal::HandleExceptionsInMethodIfSupported<>()



Aborted
make: **\* [runtest] Error 134
I followed the install instruction mostly from http://petewarden.com/2014/10/25/how-to-run-the-caffe-deep-learning-vision-library-on-nvidias-jetson-mobile-gpu-board/ , except for that I installed some newer version stuff.
Anyone know how to solve this?
Great THX in advance!
",0,caffe on jetson tk1,"caffe on jetson tk1 Got an error during runtest
F0213 10:25:10.921640  9387 db.hpp:109] Check failed: mdb_status == 0 (-30792 vs. 0) MDB_MAP_FULL: Environment mapsize limit reached
**\* Check failure stack trace: ***
    @ 0x44a85060  (unknown)
    @ 0x44a84f5c  (unknown)
    @ 0x44a84b78  (unknown)
    @ 0x44a86f98  (unknown)
    @   0x265ec2  caffe::db::LMDBTransaction::Put()
    @   0x21b932  caffe::DBTest_TestWrite_Test<>::TestBody()
    @   0x230568  testing::internal::HandleExceptionsInMethodIfSupported<>()
    @   0x230568  testing::internal::HandleExceptionsInMethodIfSupported<>()
    @   0x230568  testing::internal::HandleExceptionsInMethodIfSupported<>()
    @   0x230568  testing::internal::HandleExceptionsInMethodIfSupported<>()
    @   0x230568  testing::internal::HandleExceptionsInMethodIfSupported<>()
    @   0x230568  testing::internal::HandleExceptionsInMethodIfSupported<>()
    @   0x230568  testing::internal::HandleExceptionsInMethodIfSupported<>()
    @   0x230568  testing::internal::HandleExceptionsInMethodIfSupported<>()
    @   0x230568  testing::internal::HandleExceptionsInMethodIfSupported<>()
    @   0x230568  testing::internal::HandleExceptionsInMethodIfSupported<>()
    @   0x230568  testing::internal::HandleExceptionsInMethodIfSupported<>()
    @   0x230568  testing::internal::HandleExceptionsInMethodIfSupported<>()
    @   0x230568  testing::internal::HandleExceptionsInMethodIfSupported<>()
    @   0x230568  testing::internal::HandleExceptionsInMethodIfSupported<>()
    @   0x230568  testing::internal::HandleExceptionsInMethodIfSupported<>()
    @   0x230568  testing::internal::HandleExceptionsInMethodIfSupported<>()
    @   0x230568  testing::internal::HandleExceptionsInMethodIfSupported<>()
    @   0x230568  testing::internal::HandleExceptionsInMethodIfSupported<>()
    @   0x230568  testing::internal::HandleExceptionsInMethodIfSupported<>()
    @   0x230568  testing::internal::HandleExceptionsInMethodIfSupported<>()
    @   0x230568  testing::internal::HandleExceptionsInMethodIfSupported<>()
    @   0x230568  testing::internal::HandleExceptionsInMethodIfSupported<>()
    @   0x230568  testing::internal::HandleExceptionsInMethodIfSupported<>()
    @   0x230568  testing::internal::HandleExceptionsInMethodIfSupported<>()
    @   0x230568  testing::internal::HandleExceptionsInMethodIfSupported<>()



Aborted
make: **\* [runtest] Error 134
I followed the install instruction mostly from http://petewarden.com/2014/10/25/how-to-run-the-caffe-deep-learning-vision-library-on-nvidias-jetson-mobile-gpu-board/ , except for that I installed some newer version stuff.
Anyone know how to solve this?
Great THX in advance!
"
caffe,1169,"Does anyone see any value in a new layer type that is itself a network? This would allow a recursive inclusion of networks within networks. 

The inception module used in GoogLeNet is an example of a network-in-network. I provided an implementation of GoogLeNet's Inception ""module"" in [this](https://github.com/BVLC/caffe/issues/1106). With Caffe as it is, if you want to build GoogLeNet, you would have to copy all those Inception module layers 9 times thereby duplicating the same thing many times, changing the names of layers and top and bottom blobs to be unique within the net. By contrast, if we have a Network layer type, then each Inception module  would appear as a single layer instance of type NETWORK (perhaps). It would refer to the network's prototxt somehow, and specify values for the the things that are distinctive about each instance of the Inception module (such as the number of output channels of each of the convolutions and the pool).  And also the bottom and top blob names. 

Am I on my own if I want this implemented? Anyone already working something like this? I wouldn't want to duplicate effort.
",0,"Modular model definitions: network layer, layer modules, and protobuf generation","Modular model definitions: network layer, layer modules, and protobuf generation Does anyone see any value in a new layer type that is itself a network? This would allow a recursive inclusion of networks within networks. 

The inception module used in GoogLeNet is an example of a network-in-network. I provided an implementation of GoogLeNet's Inception ""module"" in [this](https://github.com/BVLC/caffe/issues/1106). With Caffe as it is, if you want to build GoogLeNet, you would have to copy all those Inception module layers 9 times thereby duplicating the same thing many times, changing the names of layers and top and bottom blobs to be unique within the net. By contrast, if we have a Network layer type, then each Inception module  would appear as a single layer instance of type NETWORK (perhaps). It would refer to the network's prototxt somehow, and specify values for the the things that are distinctive about each instance of the Inception module (such as the number of output channels of each of the convolutions and the pool).  And also the bottom and top blob names. 

Am I on my own if I want this implemented? Anyone already working something like this? I wouldn't want to duplicate effort.
"
caffe,5077,"I have the following error while running the 'make runtest'. I have tried with cpu flag enabled and disabled both, same error.

",0,make runtest failed,"make runtest failed I have the following error while running the 'make runtest'. I have tried with cpu flag enabled and disabled both, same error.

"
caffe,3841,"Two intertwined issues

related to #3092

The caffe makefile produces

No receipt for 'com.apple.pkg.CLTools_Executables' found

when following the Atlas BLAS path on OS X 10.11

This can be fixed by changing : 

pkgutil --pkg-info=com.apple.pkg.CLTools_Executables

to 

pkgutil --pkg-info=com.apple.pkg.Xcode

Once that is taken care of, on machines with virgin installs of El Capitan, the /System/Library/Frameworks tree no-longer contains the Accelerate/veclib headers, and these have instead moved to a SDK tree under the Xcode app itself.

As a result, the line for BLAS_INCLUDE needs to be changes from:

BLAS_INCLUDE ?= /System/Library/Frameworks/Accelerate.framework/Versions/Current/Frameworks/vecLib.framework/Headers/

to

BLAS_INCLUDE ?= /Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.11.sdk/System/Library/Frameworks/Accelerate.framework//Vers\
ions/A/Frameworks/vecLib.framework/Versions/A/Headers/
",0,"Caffe Makefile Command line tools, and paths in El Capitan","Caffe Makefile Command line tools, and paths in El Capitan Two intertwined issues

related to #3092

The caffe makefile produces

No receipt for 'com.apple.pkg.CLTools_Executables' found

when following the Atlas BLAS path on OS X 10.11

This can be fixed by changing : 

pkgutil --pkg-info=com.apple.pkg.CLTools_Executables

to 

pkgutil --pkg-info=com.apple.pkg.Xcode

Once that is taken care of, on machines with virgin installs of El Capitan, the /System/Library/Frameworks tree no-longer contains the Accelerate/veclib headers, and these have instead moved to a SDK tree under the Xcode app itself.

As a result, the line for BLAS_INCLUDE needs to be changes from:

BLAS_INCLUDE ?= /System/Library/Frameworks/Accelerate.framework/Versions/Current/Frameworks/vecLib.framework/Headers/

to

BLAS_INCLUDE ?= /Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.11.sdk/System/Library/Frameworks/Accelerate.framework//Vers\
ions/A/Frameworks/vecLib.framework/Versions/A/Headers/
"
caffe,1549,"I am trying to make [caffe](http://caffe.berkeleyvision.org/) running on my machine equipped with Ubuntu 12.04LTS. 
After finishing all the steps on the [Installation page](http://caffe.berkeleyvision.org/installation.html), I trained the LeNet model successfully and tried to use it as the tutorial from [here](http://radar.oreilly.com/2014/07/how-to-build-and-run-your-first-deep-learning-network.html). Then I got the following error:



I set the  in  file before I did the above.
What is the problem? Could anyone give some hint? I am really confused. 
",0,ImportError cannot import name BytesIO when import caffe on ubuntu,"ImportError cannot import name BytesIO when import caffe on ubuntu I am trying to make [caffe](http://caffe.berkeleyvision.org/) running on my machine equipped with Ubuntu 12.04LTS. 
After finishing all the steps on the [Installation page](http://caffe.berkeleyvision.org/installation.html), I trained the LeNet model successfully and tried to use it as the tutorial from [here](http://radar.oreilly.com/2014/07/how-to-build-and-run-your-first-deep-learning-network.html). Then I got the following error:



I set the  in  file before I did the above.
What is the problem? Could anyone give some hint? I am really confused. 
"
caffe,1676,"Hi all,

The current example of siamese network uses two gray images as a channel of the data blob. How one should create a data blob for a pair of RGB images ?

Best regards
",0,Siamese network with RGB image,"Siamese network with RGB image Hi all,

The current example of siamese network uses two gray images as a channel of the data blob. How one should create a data blob for a pair of RGB images ?

Best regards
"
caffe,296,"I don't think it's explicitly stated anywhere that the ImageNet example is supposed to be an exact reimplementation of the Krizhevsky 2012 architecture, but if it is, then the order of the LRN and max pool layers in Caffe's implementation seems to be backwards.

This network uses conv -> max pool -> LRN.
https://github.com/BVLC/caffe/blob/master/examples/imagenet/imagenet_train.prototxt#L48

This text suggests that he used conv -> LRN -> max pool.
""Response-normalization layers follow the rst and second convolutional layers. Max-pooling layers, of the kind described in Section 3.4, follow both response-normalization layers as well as the fth convolutional layer.""

Either ordering seems to get good results, but for people reimplementing papers that say Krizhevsky's architecture was used, then it might be worthwhile to make sure your implementation matches his paper.
",0,ImageNet LRN/MaxPool ordering,"ImageNet LRN/MaxPool ordering I don't think it's explicitly stated anywhere that the ImageNet example is supposed to be an exact reimplementation of the Krizhevsky 2012 architecture, but if it is, then the order of the LRN and max pool layers in Caffe's implementation seems to be backwards.

This network uses conv -> max pool -> LRN.
https://github.com/BVLC/caffe/blob/master/examples/imagenet/imagenet_train.prototxt#L48

This text suggests that he used conv -> LRN -> max pool.
""Response-normalization layers follow the rst and second convolutional layers. Max-pooling layers, of the kind described in Section 3.4, follow both response-normalization layers as well as the fth convolutional layer.""

Either ordering seems to get good results, but for people reimplementing papers that say Krizhevsky's architecture was used, then it might be worthwhile to make sure your implementation matches his paper.
"
caffe,261,"Hi,

I gave up on wrapper.py as there seemed to be a bug where it was sending the wrong type to Pycaffe (which attempts to call .size on a list and gets 0). 

However, a short (the simplest possible?) prediction script causes the same error:

F0326 09:37:00.348891 2102817152 pycaffe.cpp:127] Check failed: len(bottom) == input_blobs.size() (1 vs. 0) 

This is the script:



Is it me?

Any help gratefully received.

Best regards
John
",0,Dimension mismatch training with my own model / why does training give the same prediction for all inputs?,"Dimension mismatch training with my own model / why does training give the same prediction for all inputs? Hi,

I gave up on wrapper.py as there seemed to be a bug where it was sending the wrong type to Pycaffe (which attempts to call .size on a list and gets 0). 

However, a short (the simplest possible?) prediction script causes the same error:

F0326 09:37:00.348891 2102817152 pycaffe.cpp:127] Check failed: len(bottom) == input_blobs.size() (1 vs. 0) 

This is the script:



Is it me?

Any help gratefully received.

Best regards
John
"
caffe,5881,"In python/reqirements.txt, there is a dependency of python-dateutil (python-dateutil>=1.4,<2), but when I use pycaffe (python3.5), I get an error:
         . 
The error is gone when I update python-dateutil to 2.6.1

",0,pycaffe (python3.5) conflicts with python-dateutil<2 ,"pycaffe (python3.5) conflicts with python-dateutil<2  In python/reqirements.txt, there is a dependency of python-dateutil (python-dateutil>=1.4,<2), but when I use pycaffe (python3.5), I get an error:
         . 
The error is gone when I update python-dateutil to 2.6.1

"
caffe,2309,"Hi all,
I want to know how can i only extract filters and biases from my own caffemodel.
I read the tutorial of "" filter visualization example"", but i dont know how to do when using my own caffemodel.(I dont know which part i should modify in that reference code)
I want to extract filters and bisaes and then save into .mat file for MATLAB.
can anyone help me? please and thanks.
",0,Extract filters and bias ,"Extract filters and bias  Hi all,
I want to know how can i only extract filters and biases from my own caffemodel.
I read the tutorial of "" filter visualization example"", but i dont know how to do when using my own caffemodel.(I dont know which part i should modify in that reference code)
I want to extract filters and bisaes and then save into .mat file for MATLAB.
can anyone help me? please and thanks.
"
caffe,5284,"When I install caffe in my macbook pro ,there is a problem.

Can anyone meet similar question?
HOW CAN I FIX IT?
Want HELP!!! 
### System configuration
Operating system:MACOS 10.12.3
Compiler:clang
CUDA version (if applicable):no (cpu only)
CUDNN version (if applicable):no
BLAS:
Python or MATLAB version (for pycaffe and matcaffe respectively):python
",0,ERROR:Undefined symbols for architecture x86_64,"ERROR:Undefined symbols for architecture x86_64 When I install caffe in my macbook pro ,there is a problem.

Can anyone meet similar question?
HOW CAN I FIX IT?
Want HELP!!! 
### System configuration
Operating system:MACOS 10.12.3
Compiler:clang
CUDA version (if applicable):no (cpu only)
CUDNN version (if applicable):no
BLAS:
Python or MATLAB version (for pycaffe and matcaffe respectively):python
"
caffe,5459," I want  to upgrade a train file for eailer version of caffe. But got some errors. anyone could help me  to solve it?

     upgrade_net_proto_text-d.exe D:/net_train_val.prototxt b.prototxt
     [libprotobuf ERROR C:\Users\guillaume\work\caffe-  builder\build_v140_x64\packages\protobuf\protobuf_download- prefix\src\protobuf_download\src\google\protobuf\text_format.cc:298] Error parsing text-format  caffe.NetParameter: 16:15: Message type ""caffe.TransformationParameter"" has no field named ""fixed_crop"".
    E0328 23:01:38.529631 10972 upgrade_net_proto_text.cpp:30] Failed to parse input text file as  NetParameter: D:/net_train_val.prototxt",0,Error when upgrade a train_val.prototxt file for the new version.,"Error when upgrade a train_val.prototxt file for the new version.  I want  to upgrade a train file for eailer version of caffe. But got some errors. anyone could help me  to solve it?

     upgrade_net_proto_text-d.exe D:/net_train_val.prototxt b.prototxt
     [libprotobuf ERROR C:\Users\guillaume\work\caffe-  builder\build_v140_x64\packages\protobuf\protobuf_download- prefix\src\protobuf_download\src\google\protobuf\text_format.cc:298] Error parsing text-format  caffe.NetParameter: 16:15: Message type ""caffe.TransformationParameter"" has no field named ""fixed_crop"".
    E0328 23:01:38.529631 10972 upgrade_net_proto_text.cpp:30] Failed to parse input text file as  NetParameter: D:/net_train_val.prototxt"
caffe,1888,"I am trying to use Caffe to predict a digit in MNIST dataset. I run a solver to pretrain a net as follow:



I just use LeNet prototxt file from examples to do this. Ok, this stage is crystal clear. Then I found a script which converts mnist dataset (lmdb) to a set of grayscale images (.jpg) each of them is a representation of a handwritten digit. I added Image-Data layer as mention here (https://github.com/BVLC/caffe/issues/499). Therefore, my .prototxt file is (my_lenet_test.prototxt):



Then I construct my net:



and load pre-trained layers from the solver



But on the last stage I got SIGABRT signal from my IDE and got the error:



I would highly appreciate for any help. I am stuck and have no idea what to do... Thank you!
",0,How to predict a single digit using Caffe and C++,"How to predict a single digit using Caffe and C++ I am trying to use Caffe to predict a digit in MNIST dataset. I run a solver to pretrain a net as follow:



I just use LeNet prototxt file from examples to do this. Ok, this stage is crystal clear. Then I found a script which converts mnist dataset (lmdb) to a set of grayscale images (.jpg) each of them is a representation of a handwritten digit. I added Image-Data layer as mention here (https://github.com/BVLC/caffe/issues/499). Therefore, my .prototxt file is (my_lenet_test.prototxt):



Then I construct my net:



and load pre-trained layers from the solver



But on the last stage I got SIGABRT signal from my IDE and got the error:



I would highly appreciate for any help. I am stuck and have no idea what to do... Thank you!
"
caffe,5159,The main Caffe site is up but the auxiliary sites for the demo and downloads (including reference models) are down during maintenance to campus infrastructure at Berkeley. These sites will return on Jan. 9th.,0,site outages: dl.caffe.berkeleyvision.org and demo.caffe.berkeleyvision.org,site outages: dl.caffe.berkeleyvision.org and demo.caffe.berkeleyvision.org The main Caffe site is up but the auxiliary sites for the demo and downloads (including reference models) are down during maintenance to campus infrastructure at Berkeley. These sites will return on Jan. 9th.
caffe,6024,"I try to run a caffe.NetParameter for a example and find **Error parsing text-format caffe.NetParameter**
the logging like this:
[libprotobuf ERROR google/protobuf/text_format.cc:245] Error parsing text-format caffe.NetParameter: 52:15: Message type ""caffe.BatchNormParameter"" has no field named ""scale_bias"".
WARNING: Logging before InitGoogleLogging() is written to STDERR
F1102 09:40:47.271351 37064 upgrade_proto.cpp:122] Check failed: ReadProtoFromTextFile(param_file, param) Failed to parse NetParameter file: /home/zhezhan2/pymodel/train_val_gpu.prototxt
*** Check failure stack trace: ***
run_gpu.sh: line 2: 37064 Aborted                 (core dumped) python compare_two_model.py --tool_1 caffe --file_1 /home/zhezhan2/pymodel/train_val_gpu.prototxt --tool_2 caffe --file_2 /home/zhezhan2/pymodel/train_val_mkl.prototxt
",0,"""BatchNormParameter"" has no field named ""scale_bias""","""BatchNormParameter"" has no field named ""scale_bias"" I try to run a caffe.NetParameter for a example and find **Error parsing text-format caffe.NetParameter**
the logging like this:
[libprotobuf ERROR google/protobuf/text_format.cc:245] Error parsing text-format caffe.NetParameter: 52:15: Message type ""caffe.BatchNormParameter"" has no field named ""scale_bias"".
WARNING: Logging before InitGoogleLogging() is written to STDERR
F1102 09:40:47.271351 37064 upgrade_proto.cpp:122] Check failed: ReadProtoFromTextFile(param_file, param) Failed to parse NetParameter file: /home/zhezhan2/pymodel/train_val_gpu.prototxt
*** Check failure stack trace: ***
run_gpu.sh: line 2: 37064 Aborted                 (core dumped) python compare_two_model.py --tool_1 caffe --file_1 /home/zhezhan2/pymodel/train_val_gpu.prototxt --tool_2 caffe --file_2 /home/zhezhan2/pymodel/train_val_mkl.prototxt
"
caffe,715,"I know the official installation instruction recommends to install caffe on OS X by homebrew. But, I have installed Macports on my laptop for a quite long time (one year...), in which a lot of softwares were installed.

I find a lot of libraries provided by homebrew are also able to be downloaded by macports. So, is there a way to install caffe by macports? Thanks!
",0,How to install caffe on OS X 10.9 with Macports?,"How to install caffe on OS X 10.9 with Macports? I know the official installation instruction recommends to install caffe on OS X by homebrew. But, I have installed Macports on my laptop for a quite long time (one year...), in which a lot of softwares were installed.

I find a lot of libraries provided by homebrew are also able to be downloaded by macports. So, is there a way to install caffe by macports? Thanks!
"
caffe,4922,"Amenglars-MacBook-Pro:caffe mac$ make runtest
.build_release/tools/caffe
dyld: Library not loaded: @rpath/libmkl_rt.dylib
  Referenced from: /Users/mac/workspace/caffe/caffe/.build_release/tools/caffe
  Reason: image not found
make: **\* [runtest] Abort trap: 6
",0,"I'm new here, thanks for your help!","I'm new here, thanks for your help! Amenglars-MacBook-Pro:caffe mac$ make runtest
.build_release/tools/caffe
dyld: Library not loaded: @rpath/libmkl_rt.dylib
  Referenced from: /Users/mac/workspace/caffe/caffe/.build_release/tools/caffe
  Reason: image not found
make: **\* [runtest] Abort trap: 6
"
caffe,6122,"Although I declare different training and validation txt files (that contains a list of hdf5 files), the 
Net initializes the TEST phase with the path I give for the TRAIN phase. 

For both TRAIN and TEST phase initialization I get:

**_Creating training net_** from net file: /home/caffe/models/det_alexnet/train_val.prototxt
... hdf5_data_layer.cpp:80] Loading list of HDF5 filenames from: /home/caffe/examples/det/**_train_h5_list.txt_**
... hdf5_data_layer.cpp:94] **_Number of HDF5 files: 17_**


**_Creating test net (#0)_** specified by net file: /home/caffe/models/det_alexnet/train_val.prototxt
... hdf5_data_layer.cpp:80] Loading list of HDF5 filenames from: /home/caffe/examples/det_**/train_h5_list.txt**_
... hdf5_data_layer.cpp:94] **_Number of HDF5 files: 17_**

Although my prototxt (train_val.prototxt) clearly provides different txt files for TRAIN and TEST phases:

layer {
  name: ""data""
  type: ""HDF5Data""
  top: ""imgdata""
  top: ""ph""
  top: ""sm""
  include {
    **_phase: TRAIN_**
  }
   hdf5_data_param {
      source: ""/home/caffe/examples/det/train_h5_list.txt""
      batch_size: 256
      }
}

layer {
  name: ""data""
  type: ""HDF5Data""
  top: ""imgdata""
  top: ""ph""
  top: ""sm""
  include {
    **_phase: TEST_**
  }
   hdf5_data_param {
      source: ""/home/caffe/examples/det/eval_h5_list.txt""
      batch_size: 256
      }
}

And I get this error:


 **_Check failed: data__** 
*** Check failure stack trace: ***
    @     0x7fa264a335cd  google::LogMessage::Fail()
    @     0x7fa264a35433  google::LogMessage::SendToLog()
    @     0x7fa264a3315b  google::LogMessage::Flush()
    @     0x7fa264a35e1e  google::LogMessageFatal::~LogMessageFatal()
    @     0x7fa2651d018b  caffe::Blob<>::mutable_cpu_data()
    @     0x7fa2651ed4a1  caffe::hdf5_load_nd_dataset<>()
    @     0x7fa2650c3f73  caffe::HDF5DataLayer<>::LoadHDF5FileData()
    @     0x7fa2650c21fd  caffe::HDF5DataLayer<>::Next()
    @     0x7fa265214a93  caffe::HDF5DataLayer<>::Forward_gpu()
    @     0x7fa265048a21  caffe::Net<>::ForwardFromTo()
    @     0x7fa265048b27  caffe::Net<>::Forward()
    @     0x7fa2651c7332  caffe::Solver<>::Test()
    @     0x7fa2651c7d4e  caffe::Solver<>::TestAll()
    @     0x7fa2651cb2c7  caffe::Solver<>::Step()
    @     0x7fa2651cb58a  caffe::Solver<>::Solve()
    @           0x40aba4  train()
    @           0x407390  main
    @     0x7fa2639a3830  __libc_start_main
    @           0x407bb9  _start
    @              (nil)  (unknown)
Aborted (core dumped)

 PS: I work on only one GPU (Titan X)",0,Bug? Caffe doesn't read the HDF5 files for TEST phase (instead reads the path given in TRAIN phase twice),"Bug? Caffe doesn't read the HDF5 files for TEST phase (instead reads the path given in TRAIN phase twice) Although I declare different training and validation txt files (that contains a list of hdf5 files), the 
Net initializes the TEST phase with the path I give for the TRAIN phase. 

For both TRAIN and TEST phase initialization I get:

**_Creating training net_** from net file: /home/caffe/models/det_alexnet/train_val.prototxt
... hdf5_data_layer.cpp:80] Loading list of HDF5 filenames from: /home/caffe/examples/det/**_train_h5_list.txt_**
... hdf5_data_layer.cpp:94] **_Number of HDF5 files: 17_**


**_Creating test net (#0)_** specified by net file: /home/caffe/models/det_alexnet/train_val.prototxt
... hdf5_data_layer.cpp:80] Loading list of HDF5 filenames from: /home/caffe/examples/det_**/train_h5_list.txt**_
... hdf5_data_layer.cpp:94] **_Number of HDF5 files: 17_**

Although my prototxt (train_val.prototxt) clearly provides different txt files for TRAIN and TEST phases:

layer {
  name: ""data""
  type: ""HDF5Data""
  top: ""imgdata""
  top: ""ph""
  top: ""sm""
  include {
    **_phase: TRAIN_**
  }
   hdf5_data_param {
      source: ""/home/caffe/examples/det/train_h5_list.txt""
      batch_size: 256
      }
}

layer {
  name: ""data""
  type: ""HDF5Data""
  top: ""imgdata""
  top: ""ph""
  top: ""sm""
  include {
    **_phase: TEST_**
  }
   hdf5_data_param {
      source: ""/home/caffe/examples/det/eval_h5_list.txt""
      batch_size: 256
      }
}

And I get this error:


 **_Check failed: data__** 
*** Check failure stack trace: ***
    @     0x7fa264a335cd  google::LogMessage::Fail()
    @     0x7fa264a35433  google::LogMessage::SendToLog()
    @     0x7fa264a3315b  google::LogMessage::Flush()
    @     0x7fa264a35e1e  google::LogMessageFatal::~LogMessageFatal()
    @     0x7fa2651d018b  caffe::Blob<>::mutable_cpu_data()
    @     0x7fa2651ed4a1  caffe::hdf5_load_nd_dataset<>()
    @     0x7fa2650c3f73  caffe::HDF5DataLayer<>::LoadHDF5FileData()
    @     0x7fa2650c21fd  caffe::HDF5DataLayer<>::Next()
    @     0x7fa265214a93  caffe::HDF5DataLayer<>::Forward_gpu()
    @     0x7fa265048a21  caffe::Net<>::ForwardFromTo()
    @     0x7fa265048b27  caffe::Net<>::Forward()
    @     0x7fa2651c7332  caffe::Solver<>::Test()
    @     0x7fa2651c7d4e  caffe::Solver<>::TestAll()
    @     0x7fa2651cb2c7  caffe::Solver<>::Step()
    @     0x7fa2651cb58a  caffe::Solver<>::Solve()
    @           0x40aba4  train()
    @           0x407390  main
    @     0x7fa2639a3830  __libc_start_main
    @           0x407bb9  _start
    @              (nil)  (unknown)
Aborted (core dumped)

 PS: I work on only one GPU (Titan X)"
caffe,1768,"Code [here](https://github.com/BVLC/caffe/blob/master/src/caffe/layers/pooling_layer.cu#L31).

During every loop iteration, the amount  changes differ.
",0,`pooling_layer.cu` incorrect computation of index,"`pooling_layer.cu` incorrect computation of index Code [here](https://github.com/BVLC/caffe/blob/master/src/caffe/layers/pooling_layer.cu#L31).

During every loop iteration, the amount  changes differ.
"
caffe,6288,"Makefile:6: *** Makefile.config not found. See Makefile.config.example..  Stop.
zhouyihan@DESKTOP-NHP04OU:/mnt/c/Users//Desktop/caffe/caffe-master/caffe-master$ make all
PROTOC src/caffe/proto/caffe.proto
CXX .build_release/src/caffe/proto/caffe.pb.cc
CXX src/caffe/blob.cpp
CXX src/caffe/common.cpp
CXX src/caffe/data_transformer.cpp
CXX src/caffe/internal_thread.cpp
CXX src/caffe/layer.cpp
CXX src/caffe/layers/absval_layer.cpp
CXX src/caffe/layers/accuracy_layer.cpp
CXX src/caffe/layers/argmax_layer.cpp
CXX src/caffe/layers/base_conv_layer.cpp
CXX src/caffe/layers/base_data_layer.cpp
CXX src/caffe/layers/batch_norm_layer.cpp
CXX src/caffe/layers/batch_reindex_layer.cpp
CXX src/caffe/layers/bias_layer.cpp
CXX src/caffe/layers/bnll_layer.cpp
CXX src/caffe/layers/concat_layer.cpp
CXX src/caffe/layers/contrastive_loss_layer.cpp
CXX src/caffe/layers/conv_layer.cpp
CXX src/caffe/layers/crop_layer.cpp
CXX src/caffe/layers/cudnn_conv_layer.cpp
CXX src/caffe/layers/cudnn_deconv_layer.cpp
CXX src/caffe/layers/cudnn_lcn_layer.cpp
CXX src/caffe/layers/cudnn_lrn_layer.cpp
CXX src/caffe/layers/cudnn_pooling_layer.cpp
CXX src/caffe/layers/cudnn_relu_layer.cpp
CXX src/caffe/layers/cudnn_sigmoid_layer.cpp
CXX src/caffe/layers/cudnn_softmax_layer.cpp
CXX src/caffe/layers/cudnn_tanh_layer.cpp
CXX src/caffe/layers/data_layer.cpp
CXX src/caffe/layers/deconv_layer.cpp
CXX src/caffe/layers/dropout_layer.cpp
CXX src/caffe/layers/dummy_data_layer.cpp
CXX src/caffe/layers/eltwise_layer.cpp
CXX src/caffe/layers/elu_layer.cpp
CXX src/caffe/layers/embed_layer.cpp
CXX src/caffe/layers/euclidean_loss_layer.cpp
CXX src/caffe/layers/exp_layer.cpp
CXX src/caffe/layers/filter_layer.cpp
CXX src/caffe/layers/flatten_layer.cpp
CXX src/caffe/layers/hdf5_data_layer.cpp
src/caffe/layers/hdf5_data_layer.cpp:13:18: fatal error: hdf5.h: No such file or directory
compilation terminated.
Makefile:581: recipe for target '.build_release/src/caffe/layers/hdf5_data_layer.o' failed
make: *** [.build_release/src/caffe/layers/hdf5_data_layer.o] Error 1
",0,failed to make all ,"failed to make all  Makefile:6: *** Makefile.config not found. See Makefile.config.example..  Stop.
zhouyihan@DESKTOP-NHP04OU:/mnt/c/Users//Desktop/caffe/caffe-master/caffe-master$ make all
PROTOC src/caffe/proto/caffe.proto
CXX .build_release/src/caffe/proto/caffe.pb.cc
CXX src/caffe/blob.cpp
CXX src/caffe/common.cpp
CXX src/caffe/data_transformer.cpp
CXX src/caffe/internal_thread.cpp
CXX src/caffe/layer.cpp
CXX src/caffe/layers/absval_layer.cpp
CXX src/caffe/layers/accuracy_layer.cpp
CXX src/caffe/layers/argmax_layer.cpp
CXX src/caffe/layers/base_conv_layer.cpp
CXX src/caffe/layers/base_data_layer.cpp
CXX src/caffe/layers/batch_norm_layer.cpp
CXX src/caffe/layers/batch_reindex_layer.cpp
CXX src/caffe/layers/bias_layer.cpp
CXX src/caffe/layers/bnll_layer.cpp
CXX src/caffe/layers/concat_layer.cpp
CXX src/caffe/layers/contrastive_loss_layer.cpp
CXX src/caffe/layers/conv_layer.cpp
CXX src/caffe/layers/crop_layer.cpp
CXX src/caffe/layers/cudnn_conv_layer.cpp
CXX src/caffe/layers/cudnn_deconv_layer.cpp
CXX src/caffe/layers/cudnn_lcn_layer.cpp
CXX src/caffe/layers/cudnn_lrn_layer.cpp
CXX src/caffe/layers/cudnn_pooling_layer.cpp
CXX src/caffe/layers/cudnn_relu_layer.cpp
CXX src/caffe/layers/cudnn_sigmoid_layer.cpp
CXX src/caffe/layers/cudnn_softmax_layer.cpp
CXX src/caffe/layers/cudnn_tanh_layer.cpp
CXX src/caffe/layers/data_layer.cpp
CXX src/caffe/layers/deconv_layer.cpp
CXX src/caffe/layers/dropout_layer.cpp
CXX src/caffe/layers/dummy_data_layer.cpp
CXX src/caffe/layers/eltwise_layer.cpp
CXX src/caffe/layers/elu_layer.cpp
CXX src/caffe/layers/embed_layer.cpp
CXX src/caffe/layers/euclidean_loss_layer.cpp
CXX src/caffe/layers/exp_layer.cpp
CXX src/caffe/layers/filter_layer.cpp
CXX src/caffe/layers/flatten_layer.cpp
CXX src/caffe/layers/hdf5_data_layer.cpp
src/caffe/layers/hdf5_data_layer.cpp:13:18: fatal error: hdf5.h: No such file or directory
compilation terminated.
Makefile:581: recipe for target '.build_release/src/caffe/layers/hdf5_data_layer.o' failed
make: *** [.build_release/src/caffe/layers/hdf5_data_layer.o] Error 1
"
caffe,4113,"Hey,

I'm relatively new to caffe and am trying to run solver.prototxt on my own data (./build/tools/caffe train --solver=path/to/solver.prototxt ) and am currently getting this issue: 

data_transformer.cpp:63] Check failed: datum_height == data_mean_.height() (760 vs. 256) 

So the dimensions of something are wrong but I'm not sure how to fix it. It's probably after the layer conv1. 

Here's my train_val.prototxt and solver.prototxt--slightly modified from what's here. 
My images are 256x256 -- some things are admittedly pretty weird in the files because my dataset is stupid levels of small at this point (I'm mainly using it to familiarize myself with using caffe for my own datasets).
[solver.txt](https://github.com/BVLC/caffe/files/253862/solver.txt)
[train_val.txt](https://github.com/BVLC/caffe/files/253863/train_val.txt)

Any help would be great!
",0,datum_height == data_mean_.height() when running solver.prototxt?,"datum_height == data_mean_.height() when running solver.prototxt? Hey,

I'm relatively new to caffe and am trying to run solver.prototxt on my own data (./build/tools/caffe train --solver=path/to/solver.prototxt ) and am currently getting this issue: 

data_transformer.cpp:63] Check failed: datum_height == data_mean_.height() (760 vs. 256) 

So the dimensions of something are wrong but I'm not sure how to fix it. It's probably after the layer conv1. 

Here's my train_val.prototxt and solver.prototxt--slightly modified from what's here. 
My images are 256x256 -- some things are admittedly pretty weird in the files because my dataset is stupid levels of small at this point (I'm mainly using it to familiarize myself with using caffe for my own datasets).
[solver.txt](https://github.com/BVLC/caffe/files/253862/solver.txt)
[train_val.txt](https://github.com/BVLC/caffe/files/253863/train_val.txt)

Any help would be great!
"
caffe,651,"Where is LayerParameter_LayerType_ELTWISE and  EltwiseParameter_EltwiseOp in file caffe\include\caffe\vision_layers.hpp

Find it in proto.
",0,Where is EltwiseParameter_EltwiseOp?,"Where is EltwiseParameter_EltwiseOp? Where is LayerParameter_LayerType_ELTWISE and  EltwiseParameter_EltwiseOp in file caffe\include\caffe\vision_layers.hpp

Find it in proto.
"
caffe,3486,"Hi everyone! I am trying to run imageNet with caffe on multi-GPU today with 8 Titan X and CentOS. I also set --gpu=all, hoping all the 8 GPUs would run in parallel. But, it turned out that only four GPUs could work for my task. Does not caffe support 8-GPUs framework? Or I have to implement some other things in order to make it work? I attached two log file pictures below to help you to better understand my problem. Anyone's kind advice is appreciated and eagerly needed. Thanks.
",0,Problem with Caffe running on multi-GPUs (8 Titan X),"Problem with Caffe running on multi-GPUs (8 Titan X) Hi everyone! I am trying to run imageNet with caffe on multi-GPU today with 8 Titan X and CentOS. I also set --gpu=all, hoping all the 8 GPUs would run in parallel. But, it turned out that only four GPUs could work for my task. Does not caffe support 8-GPUs framework? Or I have to implement some other things in order to make it work? I attached two log file pictures below to help you to better understand my problem. Anyone's kind advice is appreciated and eagerly needed. Thanks.
"
caffe,2190,"Hi, all:

I am trying to use the python to build a model with existing prototxt and caffemodel file. However, I found that I could not successfully build one even with the deploy file and model offered on github unless I delete all the weigth_filler and bias_filler content. 
I run my code on python command and it shows ""Segmentation fault (core dumped)""
Can anyone figure out what's wrong?
",0,python deploy prototxt problem,"python deploy prototxt problem Hi, all:

I am trying to use the python to build a model with existing prototxt and caffemodel file. However, I found that I could not successfully build one even with the deploy file and model offered on github unless I delete all the weigth_filler and bias_filler content. 
I run my code on python command and it shows ""Segmentation fault (core dumped)""
Can anyone figure out what's wrong?
"
caffe,149,"Start with reference [1].

[1] Deep Convolutional Ranking for Multilabel Image Annotation
Yunchao Gong, Yangqing Jia, Sergey Ioffe, Alexander Toshev, Thomas Leung
http://arxiv.org/abs/1312.4894
",0,Implement MultiLabel losses and data input,"Implement MultiLabel losses and data input Start with reference [1].

[1] Deep Convolutional Ranking for Multilabel Image Annotation
Yunchao Gong, Yangqing Jia, Sergey Ioffe, Alexander Toshev, Thomas Leung
http://arxiv.org/abs/1312.4894
"
caffe,67,"It will be nice to be able to set the GPU devide_id to be used in the solver.prototxt instead of being hard_coded. Add to #57 


",0,Add set_device_id to solver prototxt,"Add set_device_id to solver prototxt It will be nice to be able to set the GPU devide_id to be used in the solver.prototxt instead of being hard_coded. Add to #57 


"
caffe,6510,"## Important - read before submitting

*Please read the [guidelines for contributing](https://github.com/BVLC/caffe/blob/master/CONTRIBUTING.md) before submitting this issue!*

*Please do not post installation, build, usage, or modeling questions, or other requests for help to Issues.*
Use the [caffe-users list](https://groups.google.com/forum/#!forum/caffe-users) instead.
This helps developers maintain a clear, uncluttered, and efficient view of the state of Caffe.

### Issue summary


### Steps to reproduce


### Tried solutions


### System configuration

* Operating system: ubuntu16.04
* Compiler: 
* CUDA version (if applicable): 8.0
* CUDNN version (if applicable): 7.1
* BLAS: 
* Python version (if using pycaffe): 3.6
* MATLAB version (if using matcaffe): 

### Issue checklist

- [ ] read the guidelines and removed the first paragraph
- [ ] written a short summary and detailed steps to reproduce
- [ ] explained how solutions to related problems failed (tick if found none)
- [ ] filled system configuration
- [ ] attached relevant logs/config files (tick if not applicable)

CXX .build_release/src/caffe/proto/caffe.pb.cc
CXX src/caffe/syncedmem.cpp
CXX src/caffe/common.cpp
CXX src/caffe/layers/spp_layer.cpp
CXX src/caffe/layers/cudnn_deconv_layer.cpp
CXX src/caffe/layers/cudnn_tanh_layer.cpp
CXX src/caffe/layers/cudnn_lrn_layer.cpp
CXX src/caffe/layers/cudnn_relu_layer.cpp
In file included from ./include/caffe/util/math_functions.hpp:11:0,
                 from ./include/caffe/layer.hpp:12,
                 from src/caffe/layers/spp_layer.cpp:4:
./include/caffe/util/mkl_alternate.hpp:14:19: fatal error: cblas.h: No such file or directory
compilation terminated.
In file included from ./include/caffe/util/math_functions.hpp:11:0,
                 from ./include/caffe/layer.hpp:12,
                 from ./include/caffe/layers/cudnn_deconv_layer.hpp:7,
                 from src/caffe/layers/cudnn_deconv_layer.cpp:5:
./include/caffe/util/mkl_alternate.hpp:14:19: fatal error: cblas.h: No such file or directory
compilation terminated.
Makefile:591: recipe for target '.build_release/src/caffe/layers/spp_layer.o' failed
make: *** [.build_release/src/caffe/layers/spp_layer.o] Error 1
make: *** Waiting for unfinished jobs....
Makefile:591: recipe for target '.build_release/src/caffe/layers/cudnn_deconv_layer.o' failed
make: *** [.build_release/src/caffe/layers/cudnn_deconv_layer.o] Error 1
In file included from ./include/caffe/util/math_functions.hpp:11:0,
                 from ./include/caffe/layer.hpp:12,
                 from ./include/caffe/layers/cudnn_lrn_layer.hpp:7,
                 from src/caffe/layers/cudnn_lrn_layer.cpp:4:
./include/caffe/util/mkl_alternate.hpp:14:19: fatal error: cblas.h: No such file or directory
compilation terminated.
In file included from ./include/caffe/util/math_functions.hpp:11:0,
                 from src/caffe/syncedmem.cpp:3:
./include/caffe/util/mkl_alternate.hpp:14:19: fatal error: cblas.h: No such file or directory
compilation terminated.
In file included from ./include/caffe/util/math_functions.hpp:11:0,
                 from ./include/caffe/layer.hpp:12,
                 from ./include/caffe/layers/cudnn_tanh_layer.hpp:7,
                 from src/caffe/layers/cudnn_tanh_layer.cpp:4:
./include/caffe/util/mkl_alternate.hpp:14:19: fatal error: cblas.h: No such file or directory
compilation terminated.
Makefile:591: recipe for target '.build_release/src/caffe/layers/cudnn_lrn_layer.o' failed
make: *** [.build_release/src/caffe/layers/cudnn_lrn_layer.o] Error 1
Makefile:591: recipe for target '.build_release/src/caffe/syncedmem.o' failed
make: *** [.build_release/src/caffe/syncedmem.o] Error 1
Makefile:591: recipe for target '.build_release/src/caffe/layers/cudnn_tanh_layer.o' failed
make: *** [.build_release/src/caffe/layers/cudnn_tanh_layer.o] Error 1
In file included from ./include/caffe/util/math_functions.hpp:11:0,
                 from ./include/caffe/layer.hpp:12,
                 from ./include/caffe/layers/cudnn_relu_layer.hpp:7,
                 from src/caffe/layers/cudnn_relu_layer.cpp:4:
./include/caffe/util/mkl_alternate.hpp:14:19: fatal error: cblas.h: No such file or directory
compilation terminated.
Makefile:591: recipe for target '.build_release/src/caffe/layers/cudnn_relu_layer.o' failed
make: *** [.build_release/src/caffe/layers/cudnn_relu_layer.o] Error 1",0,Makefile:591: recipe for target '.build_release/src/caffe/layers/cudnn_relu_layer.o' failed,"Makefile:591: recipe for target '.build_release/src/caffe/layers/cudnn_relu_layer.o' failed ## Important - read before submitting

*Please read the [guidelines for contributing](https://github.com/BVLC/caffe/blob/master/CONTRIBUTING.md) before submitting this issue!*

*Please do not post installation, build, usage, or modeling questions, or other requests for help to Issues.*
Use the [caffe-users list](https://groups.google.com/forum/#!forum/caffe-users) instead.
This helps developers maintain a clear, uncluttered, and efficient view of the state of Caffe.

### Issue summary


### Steps to reproduce


### Tried solutions


### System configuration

* Operating system: ubuntu16.04
* Compiler: 
* CUDA version (if applicable): 8.0
* CUDNN version (if applicable): 7.1
* BLAS: 
* Python version (if using pycaffe): 3.6
* MATLAB version (if using matcaffe): 

### Issue checklist

- [ ] read the guidelines and removed the first paragraph
- [ ] written a short summary and detailed steps to reproduce
- [ ] explained how solutions to related problems failed (tick if found none)
- [ ] filled system configuration
- [ ] attached relevant logs/config files (tick if not applicable)

CXX .build_release/src/caffe/proto/caffe.pb.cc
CXX src/caffe/syncedmem.cpp
CXX src/caffe/common.cpp
CXX src/caffe/layers/spp_layer.cpp
CXX src/caffe/layers/cudnn_deconv_layer.cpp
CXX src/caffe/layers/cudnn_tanh_layer.cpp
CXX src/caffe/layers/cudnn_lrn_layer.cpp
CXX src/caffe/layers/cudnn_relu_layer.cpp
In file included from ./include/caffe/util/math_functions.hpp:11:0,
                 from ./include/caffe/layer.hpp:12,
                 from src/caffe/layers/spp_layer.cpp:4:
./include/caffe/util/mkl_alternate.hpp:14:19: fatal error: cblas.h: No such file or directory
compilation terminated.
In file included from ./include/caffe/util/math_functions.hpp:11:0,
                 from ./include/caffe/layer.hpp:12,
                 from ./include/caffe/layers/cudnn_deconv_layer.hpp:7,
                 from src/caffe/layers/cudnn_deconv_layer.cpp:5:
./include/caffe/util/mkl_alternate.hpp:14:19: fatal error: cblas.h: No such file or directory
compilation terminated.
Makefile:591: recipe for target '.build_release/src/caffe/layers/spp_layer.o' failed
make: *** [.build_release/src/caffe/layers/spp_layer.o] Error 1
make: *** Waiting for unfinished jobs....
Makefile:591: recipe for target '.build_release/src/caffe/layers/cudnn_deconv_layer.o' failed
make: *** [.build_release/src/caffe/layers/cudnn_deconv_layer.o] Error 1
In file included from ./include/caffe/util/math_functions.hpp:11:0,
                 from ./include/caffe/layer.hpp:12,
                 from ./include/caffe/layers/cudnn_lrn_layer.hpp:7,
                 from src/caffe/layers/cudnn_lrn_layer.cpp:4:
./include/caffe/util/mkl_alternate.hpp:14:19: fatal error: cblas.h: No such file or directory
compilation terminated.
In file included from ./include/caffe/util/math_functions.hpp:11:0,
                 from src/caffe/syncedmem.cpp:3:
./include/caffe/util/mkl_alternate.hpp:14:19: fatal error: cblas.h: No such file or directory
compilation terminated.
In file included from ./include/caffe/util/math_functions.hpp:11:0,
                 from ./include/caffe/layer.hpp:12,
                 from ./include/caffe/layers/cudnn_tanh_layer.hpp:7,
                 from src/caffe/layers/cudnn_tanh_layer.cpp:4:
./include/caffe/util/mkl_alternate.hpp:14:19: fatal error: cblas.h: No such file or directory
compilation terminated.
Makefile:591: recipe for target '.build_release/src/caffe/layers/cudnn_lrn_layer.o' failed
make: *** [.build_release/src/caffe/layers/cudnn_lrn_layer.o] Error 1
Makefile:591: recipe for target '.build_release/src/caffe/syncedmem.o' failed
make: *** [.build_release/src/caffe/syncedmem.o] Error 1
Makefile:591: recipe for target '.build_release/src/caffe/layers/cudnn_tanh_layer.o' failed
make: *** [.build_release/src/caffe/layers/cudnn_tanh_layer.o] Error 1
In file included from ./include/caffe/util/math_functions.hpp:11:0,
                 from ./include/caffe/layer.hpp:12,
                 from ./include/caffe/layers/cudnn_relu_layer.hpp:7,
                 from src/caffe/layers/cudnn_relu_layer.cpp:4:
./include/caffe/util/mkl_alternate.hpp:14:19: fatal error: cblas.h: No such file or directory
compilation terminated.
Makefile:591: recipe for target '.build_release/src/caffe/layers/cudnn_relu_layer.o' failed
make: *** [.build_release/src/caffe/layers/cudnn_relu_layer.o] Error 1"
caffe,2060,"it seems that accuracy layer make some changes.And when testing we must go through a argmax layer and then go to accuracy layer.
Is that true?
",0,accuracy layer questions,"accuracy layer questions it seems that accuracy layer make some changes.And when testing we must go through a argmax layer and then go to accuracy layer.
Is that true?
"
caffe,3830,"Suppose I have 1056 x 1056 image. The image contains multiple objects like car, house, tree etc but I want only car as a target object in this case how shall I prepare train.txt file? Should I crop only car from original image and prepare two class which have car denoted by 1 and which dose not have car denote by 0.

or 

should I consider original image(size 1056x1056) having car 1 and doesn't have car 0.

Please explain.
",0,Should I need to crop original image to prepare train.txt or val.txt file?,"Should I need to crop original image to prepare train.txt or val.txt file? Suppose I have 1056 x 1056 image. The image contains multiple objects like car, house, tree etc but I want only car as a target object in this case how shall I prepare train.txt file? Should I crop only car from original image and prepare two class which have car denoted by 1 and which dose not have car denote by 0.

or 

should I consider original image(size 1056x1056) having car 1 and doesn't have car 0.

Please explain.
"
caffe,4080,"Could we replace open by fopen ? I am ready to do it.

Why? faster, more portable, ...
take a look here: http://stackoverflow.com/questions/1658476/c-fopen-vs-open
",0,replace open by fopen in io.cpp,"replace open by fopen in io.cpp Could we replace open by fopen ? I am ready to do it.

Why? faster, more portable, ...
take a look here: http://stackoverflow.com/questions/1658476/c-fopen-vs-open
"
caffe,349,"when I make the newest version of caffe, I get an error returned below:

build/src/caffe/proto/caffe.pb.o: file not recognized: File format not recognized
what can I do to solve this error
",0,caffe.pb.o File format not recognized after update,"caffe.pb.o File format not recognized after update when I make the newest version of caffe, I get an error returned below:

build/src/caffe/proto/caffe.pb.o: file not recognized: File format not recognized
what can I do to solve this error
"
caffe,2819,"In the tutorial it is written that ""The module dir caffe/python/caffe should be installed in your PYTHONPATH for import caffe"". However, when this path is added, a number of errors appear during the IPython initialization due to the confusion with internal io.py file. It seems like the actual path should be ""caffe/python"", in this case the initialization works well, and the ""caffe"" module can be correctly found.
",0,PYTHONPATH to import caffe module,"PYTHONPATH to import caffe module In the tutorial it is written that ""The module dir caffe/python/caffe should be installed in your PYTHONPATH for import caffe"". However, when this path is added, a number of errors appear during the IPython initialization due to the confusion with internal io.py file. It seems like the actual path should be ""caffe/python"", in this case the initialization works well, and the ""caffe"" module can be correctly found.
"
caffe,526,"I have some C++ code that uses weights as an array of double, and I would like to interface it with caffe, and transfer the weights back and forth between my code and caffe.
What is the simplest way of doing that ? It doesn't need to be blazing fast.
",0,How to manually set the weights of a network ?,"How to manually set the weights of a network ? I have some C++ code that uses weights as an array of double, and I would like to interface it with caffe, and transfer the weights back and forth between my code and caffe.
What is the simplest way of doing that ? It doesn't need to be blazing fast.
"
caffe,454,"The homebrew install of leveldb seems to not properly package the dylib name or else it's missing a symlink. Caffe compiles properly, but crashes instantly:



The solution is to make a libleveldb.dylib.1 symlink to libleveldb.1.15.dylib in /usr/local/opt/leveldb/lib either manually or by appending



to the install method of the leveldb formula by .

An upstream fix is probably best, but I'm merely documenting it here for lack of time.
",0,homebrew leveldb dyld: Library not loaded (image not found),"homebrew leveldb dyld: Library not loaded (image not found) The homebrew install of leveldb seems to not properly package the dylib name or else it's missing a symlink. Caffe compiles properly, but crashes instantly:



The solution is to make a libleveldb.dylib.1 symlink to libleveldb.1.15.dylib in /usr/local/opt/leveldb/lib either manually or by appending



to the install method of the leveldb formula by .

An upstream fix is probably best, but I'm merely documenting it here for lack of time.
"
caffe,4439,"During the Caffe installation process, I managed to gone through the ""make all"" and ""make test"" without any error. I got an error as shown below during the ""make runtest"". Would like to seek for some guidance.
![cudnn_fail](https://cloud.githubusercontent.com/assets/20381221/16713251/f94710c0-46d3-11e6-8724-e4163ba28fda.png)

I have already installed CUDA8.0 RC and cuDNN5. My Nvidia GPU is GTX960 should be able to meet the compute capability for cuDNN.
![nvidia_driver](https://cloud.githubusercontent.com/assets/20381221/16713253/fea34480-46d3-11e6-9c8f-3bec37b1975a.png)

My caffe version is as shown below:
![caffe_version](https://cloud.githubusercontent.com/assets/20381221/16713255/017c9f8a-46d4-11e6-8201-fe6d45f2620c.png)

I have already included the cuDNN 5 into the Makefile.config ==> verified by using ""cmake.."" at the /build directory.
![nvidia_cuda](https://cloud.githubusercontent.com/assets/20381221/16713256/062d21b2-46d4-11e6-8b86-f97370d3bb11.png)
",0,Caffe Make Installation cuDNN status fail,"Caffe Make Installation cuDNN status fail During the Caffe installation process, I managed to gone through the ""make all"" and ""make test"" without any error. I got an error as shown below during the ""make runtest"". Would like to seek for some guidance.
![cudnn_fail](https://cloud.githubusercontent.com/assets/20381221/16713251/f94710c0-46d3-11e6-8724-e4163ba28fda.png)

I have already installed CUDA8.0 RC and cuDNN5. My Nvidia GPU is GTX960 should be able to meet the compute capability for cuDNN.
![nvidia_driver](https://cloud.githubusercontent.com/assets/20381221/16713253/fea34480-46d3-11e6-9c8f-3bec37b1975a.png)

My caffe version is as shown below:
![caffe_version](https://cloud.githubusercontent.com/assets/20381221/16713255/017c9f8a-46d4-11e6-8201-fe6d45f2620c.png)

I have already included the cuDNN 5 into the Makefile.config ==> verified by using ""cmake.."" at the /build directory.
![nvidia_cuda](https://cloud.githubusercontent.com/assets/20381221/16713256/062d21b2-46d4-11e6-8b86-f97370d3bb11.png)
"
caffe,608,"Hi,
I am wondering that does caffe use the stochastic gradient descent? That means update the weights for each batch, so that every iteration will have batch_size times updates.
Or it just updates the weights for each iteration, and all the batches in this iteration will share the same parameters?

I assumed the caffe uses the stochastic gradient descent, and tried to find the code about that part in the .cpp but got nothing. 
I increased and decreased batch_size, and expected different performance under the same iterations, since small batch_size means more updates, but I got all similar results!

Could someone tell me about it?
",0,Does caffe use stochastic gradient descent,"Does caffe use stochastic gradient descent Hi,
I am wondering that does caffe use the stochastic gradient descent? That means update the weights for each batch, so that every iteration will have batch_size times updates.
Or it just updates the weights for each iteration, and all the batches in this iteration will share the same parameters?

I assumed the caffe uses the stochastic gradient descent, and tried to find the code about that part in the .cpp but got nothing. 
I increased and decreased batch_size, and expected different performance under the same iterations, since small batch_size means more updates, but I got all similar results!

Could someone tell me about it?
"
caffe,3203,"In the eltwise_layer.cpp you do the following check in the Reshape member function



Meanwhile in other layers like euclidean_loss_layer.cpp you do this:



I think the above mentioned for loop should look something like this:



So if I have to following structure (just a small part):



This will result an error even if I have another memory data layer at the beginning which has batch_size: 10 which then goes thorough some convolutional layer and ends up in the inner inner-product-layer. I assume here you are checking 10*3 == 3 instead of checking 3==3.

I might be wrong and please let me know If I am happened to be wrong and I shouldn't change the checking in my version of caffe for some reason.

Thank you

Tams
",0,ELTWISE layer suspected bug,"ELTWISE layer suspected bug In the eltwise_layer.cpp you do the following check in the Reshape member function



Meanwhile in other layers like euclidean_loss_layer.cpp you do this:



I think the above mentioned for loop should look something like this:



So if I have to following structure (just a small part):



This will result an error even if I have another memory data layer at the beginning which has batch_size: 10 which then goes thorough some convolutional layer and ends up in the inner inner-product-layer. I assume here you are checking 10*3 == 3 instead of checking 3==3.

I might be wrong and please let me know If I am happened to be wrong and I shouldn't change the checking in my version of caffe for some reason.

Thank you

Tams
"
caffe,4130,"Hi, I'm using the Python interface. There seems to be no way of returning test accuracy during training when test net is specified. The accuracies can only be printed out. Is this correct?
",0,Return test accuracy during training in Python,"Return test accuracy during training in Python Hi, I'm using the Python interface. There seems to be no way of returning test accuracy during training when test net is specified. The accuracies can only be printed out. Is this correct?
"
caffe,5061,"### Issue summary
When run the caffe model ,it seems the fps will decrease .And sometimes the fps would not increase untill restart the computer. I try to reinstall ubuntu14.04 and caffe  ,but it can not help .

### Steps to reproduce


### Your system configuration
Operating system:utuntu14.04
Compiler:
CUDA version (if applicable):8.0.44
CUDNN version (if applicable):non
BLAS:
Python or MATLAB version (for pycaffe and matcaffe respectively):MATLAB

I am sorry for I do not know how to find out what the compiler and BLAS is.",0,It looks like the fps would decrease when run caffe model even the mnist,"It looks like the fps would decrease when run caffe model even the mnist ### Issue summary
When run the caffe model ,it seems the fps will decrease .And sometimes the fps would not increase untill restart the computer. I try to reinstall ubuntu14.04 and caffe  ,but it can not help .

### Steps to reproduce


### Your system configuration
Operating system:utuntu14.04
Compiler:
CUDA version (if applicable):8.0.44
CUDNN version (if applicable):non
BLAS:
Python or MATLAB version (for pycaffe and matcaffe respectively):MATLAB

I am sorry for I do not know how to find out what the compiler and BLAS is."
caffe,264,"I managed to successfully compile caffe on my computer. But one of the unit tests seem to fail. (all the other ones pass). Here's the error:

src/caffe/test/test_common.cpp:34: Failure
Value of: Caffe::TRAIN
  Actual: 0
Expected: Caffe::phase()
Which is: 1
[  FAILED  ] CommonTest.TestPhase

What do you think has caused this error?
",0,CommonTest.TestPhase unit test failure,"CommonTest.TestPhase unit test failure I managed to successfully compile caffe on my computer. But one of the unit tests seem to fail. (all the other ones pass). Here's the error:

src/caffe/test/test_common.cpp:34: Failure
Value of: Caffe::TRAIN
  Actual: 0
Expected: Caffe::phase()
Which is: 1
[  FAILED  ] CommonTest.TestPhase

What do you think has caused this error?
"
caffe,3406,"datalayer1    data:x11  label: label1
datalayer2    data:y11 label: label2
The data in two datalayer has relationship of their  order
x11 ---> y11
x12---->y12
when train in SGD , can they shuffle in the same order >? ??
",0,the relationship of 2 data layer,"the relationship of 2 data layer datalayer1    data:x11  label: label1
datalayer2    data:y11 label: label2
The data in two datalayer has relationship of their  order
x11 ---> y11
x12---->y12
when train in SGD , can they shuffle in the same order >? ??
"
caffe,4667,"After finishing all the steps, finally i run the demo.py under tools/,
Instead I get thsi problem:
...
faster-rcnn/tools/../lib/fast_rcnn/nms_wrapper.py"", line 10, in <module>
    from nms.cpu_nms import cpu_nms
ImportError: No module named cpu_nms

I did everything correctly..
Any help will be appreciate
",0,Problems with CPU-ONLY,"Problems with CPU-ONLY After finishing all the steps, finally i run the demo.py under tools/,
Instead I get thsi problem:
...
faster-rcnn/tools/../lib/fast_rcnn/nms_wrapper.py"", line 10, in <module>
    from nms.cpu_nms import cpu_nms
ImportError: No module named cpu_nms

I did everything correctly..
Any help will be appreciate
"
caffe,2593,"Did anyone try to reproduce first steps of SPP-Net? What i would like to try is to run first half of alexnet (data -> conv5), define several levels of pyramid with custom maxpool layer in python to output blobs of different sizes, & then run second half (fc6-conv -> fc8-conv) for all the different levels in the pyramid. is there an example somewhere on how to define layers without the proto txt file?
",0,spp-net first steps,"spp-net first steps Did anyone try to reproduce first steps of SPP-Net? What i would like to try is to run first half of alexnet (data -> conv5), define several levels of pyramid with custom maxpool layer in python to output blobs of different sizes, & then run second half (fc6-conv -> fc8-conv) for all the different levels in the pyramid. is there an example somewhere on how to define layers without the proto txt file?
"
caffe,1765,"i am extracting 30 facial keypoints (x,y) from an input image as per kaggle facialkeypoints competition.

How do i setup caffe to run a regression and produce 30 dimensional output??. 



How do i setup caffe accordingly?. I am using EUCLIDEAN_LOSS (sum of squares) to get the regressed output. Here is a simple logistic regressor model using caffe but it is not working. Looks accuracy layer cannot handle multi-label output.



Here is the layer file:



I am have seen this topic but really cant get a grasp of it. I see that stable version of caffe can handle only 1 or 2 outputs.
",0,Multi label regression in Caffe,"Multi label regression in Caffe i am extracting 30 facial keypoints (x,y) from an input image as per kaggle facialkeypoints competition.

How do i setup caffe to run a regression and produce 30 dimensional output??. 



How do i setup caffe accordingly?. I am using EUCLIDEAN_LOSS (sum of squares) to get the regressed output. Here is a simple logistic regressor model using caffe but it is not working. Looks accuracy layer cannot handle multi-label output.



Here is the layer file:



I am have seen this topic but really cant get a grasp of it. I see that stable version of caffe can handle only 1 or 2 outputs.
"
caffe,4468,"Hi, does anybody know how to transform .caffemodel to .mat.
My current model was trained using caffe, and the model it saved are .caffemodel type. Now, I hope to do fine tuning using matconvnet, and it requires the model in "".mat"" type. 
",0,"Hi, anybody know how to transform .caffemodel to .mat?","Hi, anybody know how to transform .caffemodel to .mat? Hi, does anybody know how to transform .caffemodel to .mat.
My current model was trained using caffe, and the model it saved are .caffemodel type. Now, I hope to do fine tuning using matconvnet, and it requires the model in "".mat"" type. 
"
caffe,1498,"when the cross_channel = true
",0,why the mvn_layer after inner_product_layer return nan?,"why the mvn_layer after inner_product_layer return nan? when the cross_channel = true
"
caffe,2159,"I linked my code against 'libcaffe.a', and the code below:

will generate runtime error message:  


If I switched to 'libcaffe.so', no error message generated. But in previous Caffe library, I can link against static library without any problem.

I cannot understand what difference between the static and dynamic versions of libcaffe.

Could somebody here elucidate this phenomenon?
Thanks.
",0,Difference between static linkage and dynamic linkage,"Difference between static linkage and dynamic linkage I linked my code against 'libcaffe.a', and the code below:

will generate runtime error message:  


If I switched to 'libcaffe.so', no error message generated. But in previous Caffe library, I can link against static library without any problem.

I cannot understand what difference between the static and dynamic versions of libcaffe.

Could somebody here elucidate this phenomenon?
Thanks.
"
caffe,2564,"When compiling on osx Yosemite the absval_layer raises a warning stating:
""warning: absolute value function 'abs' given an argument of type 'long long' but has parameter of type 'int' which may cause truncation of value""

I resolved it for me by putting the following code directly after ""namespace caffe {""


",0,osx: abs not defined absval_layer,"osx: abs not defined absval_layer When compiling on osx Yosemite the absval_layer raises a warning stating:
""warning: absolute value function 'abs' given an argument of type 'long long' but has parameter of type 'int' which may cause truncation of value""

I resolved it for me by putting the following code directly after ""namespace caffe {""


"
caffe,6095,"The command is 

> make all -j$64 && make distribute -j$64

PROTOC src/caffe/proto/caffe.proto
CXX src/caffe/util/cudnn.cpp
CXX src/caffe/util/hdf5.cpp
CXX src/caffe/util/db.cpp
CXX src/caffe/util/db_lmdb.cpp
CXX src/caffe/util/insert_splits.cpp
CXX src/caffe/util/blocking_queue.cpp
CXX src/caffe/util/math_functions.cpp
CXX src/caffe/util/db_leveldb.cpp
CXX src/caffe/util/benchmark.cpp
CXX src/caffe/util/upgrade_proto.cpp
CXX src/caffe/util/im2col.cpp
CXX src/caffe/util/io.cpp
CXX src/caffe/util/signal_handler.cpp
CXX src/caffe/syncedmem.cpp
CXX src/caffe/layer.cpp
CXX src/caffe/internal_thread.cpp
CXX src/caffe/blob.cpp
CXX src/caffe/layer_factory.cpp
CXX src/caffe/solver.cpp
CXX src/caffe/solvers/rmsprop_solver.cpp
CXX src/caffe/solvers/adadelta_solver.cpp
CXX src/caffe/solvers/sgd_solver.cpp
CXX src/caffe/solvers/nesterov_solver.cpp
CXX src/caffe/solvers/adam_solver.cpp
CXX src/caffe/solvers/adagrad_solver.cpp
CXX src/caffe/parallel.cpp
CXX src/caffe/layers/log_layer.cpp
CXX src/caffe/layers/bias_layer.cpp
CXX src/caffe/layers/filter_layer.cpp
CXX src/caffe/layers/loss_layer.cpp
CXX src/caffe/layers/hinge_loss_layer.cpp
CXX src/caffe/layers/cudnn_sigmoid_layer.cpp
CXX src/caffe/layers/cudnn_pooling_layer.cpp
CXX src/caffe/layers/sigmoid_cross_entropy_loss_layer.cpp
CXX src/caffe/layers/input_layer.cpp
CXX src/caffe/layers/elu_layer.cpp
CXX src/caffe/layers/cudnn_lrn_layer.cpp
CXX src/caffe/layers/base_conv_layer.cpp
CXX src/caffe/layers/multinomial_logistic_loss_layer.cpp
CXX src/caffe/layers/pooling_layer.cpp
CXX src/caffe/layers/crop_layer.cpp
CXX src/caffe/layers/relu_layer.cpp
CXX src/caffe/layers/threshold_layer.cpp
CXX src/caffe/layers/cudnn_lcn_layer.cpp
CXX src/caffe/layers/scale_layer.cpp
CXX src/caffe/layers/concat_layer.cpp
CXX src/caffe/layers/spp_layer.cpp
CXX src/caffe/layers/contrastive_loss_layer.cpp
CXX src/caffe/layers/lrn_layer.cpp
CXX src/caffe/layers/split_layer.cpp
CXX src/caffe/layers/inner_product_layer.cpp
CXX src/caffe/layers/batch_reindex_layer.cpp
CXX src/caffe/layers/mvn_layer.cpp
CXX src/caffe/layers/softmax_loss_layer.cpp
CXX src/caffe/layers/tanh_layer.cpp
CXX src/caffe/layers/power_layer.cpp
CXX src/caffe/layers/image_data_layer.cpp
CXX src/caffe/layers/cudnn_conv_layer.cpp
CXX src/caffe/layers/absval_layer.cpp
CXX src/caffe/layers/embed_layer.cpp
CXX src/caffe/layers/window_data_layer.cpp
CXX src/caffe/layers/bnll_layer.cpp
CXX src/caffe/layers/euclidean_loss_layer.cpp
CXX src/caffe/layers/im2col_layer.cpp
CXX src/caffe/layers/parameter_layer.cpp
CXX src/caffe/layers/memory_data_layer.cpp
CXX src/caffe/layers/argmax_layer.cpp
CXX src/caffe/layers/reduction_layer.cpp
CXX src/caffe/layers/data_layer.cpp
CXX src/caffe/layers/dummy_data_layer.cpp
CXX src/caffe/layers/silence_layer.cpp
CXX src/caffe/layers/batch_norm_layer.cpp
CXX src/caffe/layers/conv_layer.cpp
CXX src/caffe/layers/prelu_layer.cpp
CXX src/caffe/layers/slice_layer.cpp
CXX src/caffe/layers/lstm_unit_layer.cpp
CXX src/caffe/layers/hdf5_data_layer.cpp
CXX src/caffe/layers/reshape_layer.cpp
CXX src/caffe/layers/cudnn_tanh_layer.cpp
CXX src/caffe/layers/cudnn_softmax_layer.cpp
CXX src/caffe/layers/infogain_loss_layer.cpp
CXX src/caffe/layers/base_data_layer.cpp
CXX src/caffe/layers/eltwise_layer.cpp
CXX src/caffe/layers/accuracy_layer.cpp
CXX src/caffe/layers/sigmoid_layer.cpp
CXX src/caffe/layers/dropout_layer.cpp
CXX src/caffe/layers/neuron_layer.cpp
CXX src/caffe/layers/recurrent_layer.cpp
CXX src/caffe/layers/tile_layer.cpp
CXX src/caffe/layers/hdf5_output_layer.cpp
CXX src/caffe/layers/softmax_layer.cpp
CXX src/caffe/layers/flatten_layer.cpp
CXX src/caffe/layers/rnn_layer.cpp
CXX src/caffe/layers/exp_layer.cpp
CXX src/caffe/layers/lstm_layer.cpp
CXX src/caffe/layers/cudnn_relu_layer.cpp
CXX src/caffe/layers/deconv_layer.cpp
CXX src/caffe/common.cpp
CXX src/caffe/data_transformer.cpp
CXX src/caffe/net.cpp
NVCC src/caffe/util/math_functions.cu
NVCC src/caffe/util/im2col.cu
NVCC src/caffe/solvers/adagrad_solver.cu
NVCC src/caffe/solvers/sgd_solver.cu
NVCC src/caffe/solvers/adam_solver.cu
NVCC src/caffe/solvers/adadelta_solver.cu
NVCC src/caffe/solvers/nesterov_solver.cu
NVCC src/caffe/solvers/rmsprop_solver.cu
NVCC src/caffe/layers/accuracy_layer.cu
NVCC src/caffe/layers/batch_norm_layer.cu
NVCC src/caffe/layers/softmax_layer.cu
NVCC src/caffe/layers/im2col_layer.cu
NVCC src/caffe/layers/softmax_loss_layer.cu
NVCC src/caffe/layers/base_data_layer.cu
NVCC src/caffe/layers/sigmoid_cross_entropy_loss_layer.cu
NVCC src/caffe/layers/threshold_layer.cu
NVCC src/caffe/layers/log_layer.cu
NVCC src/caffe/layers/cudnn_lcn_layer.cu
NVCC src/caffe/layers/hdf5_data_layer.cu
NVCC src/caffe/layers/crop_layer.cu
NVCC src/caffe/layers/relu_layer.cu
NVCC src/caffe/layers/slice_layer.cu
NVCC src/caffe/layers/lrn_layer.cu
NVCC src/caffe/layers/batch_reindex_layer.cu
NVCC src/caffe/layers/hdf5_output_layer.cu
NVCC src/caffe/layers/elu_layer.cu
NVCC src/caffe/layers/mvn_layer.cu
NVCC src/caffe/layers/lstm_unit_layer.cu
NVCC src/caffe/layers/tanh_layer.cu
NVCC src/caffe/layers/tile_layer.cu
NVCC src/caffe/layers/dropout_layer.cu
NVCC src/caffe/layers/filter_layer.cu
NVCC src/caffe/layers/cudnn_sigmoid_layer.cu
NVCC src/caffe/layers/absval_layer.cu
NVCC src/caffe/layers/bias_layer.cu
NVCC src/caffe/layers/sigmoid_layer.cu
NVCC src/caffe/layers/pooling_layer.cu
NVCC src/caffe/layers/concat_layer.cu
NVCC src/caffe/layers/cudnn_pooling_layer.cu
NVCC src/caffe/layers/power_layer.cu
NVCC src/caffe/layers/exp_layer.cu
NVCC src/caffe/layers/split_layer.cu
NVCC src/caffe/layers/eltwise_layer.cu
NVCC src/caffe/layers/cudnn_softmax_layer.cu
NVCC src/caffe/layers/deconv_layer.cu
NVCC src/caffe/layers/bnll_layer.cu
NVCC src/caffe/layers/reduction_layer.cu
NVCC src/caffe/layers/cudnn_lrn_layer.cu
NVCC src/caffe/layers/scale_layer.cu
NVCC src/caffe/layers/cudnn_relu_layer.cu
NVCC src/caffe/layers/recurrent_layer.cu
NVCC src/caffe/layers/cudnn_tanh_layer.cu
NVCC src/caffe/layers/euclidean_loss_layer.cu
NVCC src/caffe/layers/inner_product_layer.cu
NVCC src/caffe/layers/embed_layer.cu
NVCC src/caffe/layers/prelu_layer.cu
NVCC src/caffe/layers/conv_layer.cu
NVCC src/caffe/layers/silence_layer.cu
NVCC src/caffe/layers/contrastive_loss_layer.cu
NVCC src/caffe/layers/cudnn_conv_layer.cu
CXX tools/upgrade_net_proto_text.cpp
CXX tools/device_query.cpp
CXX tools/caffe.cpp
CXX tools/extract_features.cpp
CXX tools/compute_image_mean.cpp
CXX tools/test_net.cpp
CXX tools/train_net.cpp
CXX tools/convert_imageset.cpp
CXX tools/finetune_net.cpp
CXX tools/upgrade_solver_proto_text.cpp
CXX tools/net_speed_benchmark.cpp
CXX tools/upgrade_net_proto_binary.cpp
CXX examples/cifar10/convert_cifar_data.cpp
CXX examples/siamese/convert_mnist_siamese_data.cpp
CXX examples/cpp_classification/classification.cpp
CXX examples/mnist/convert_mnist_data.cpp
CXX .build_release/src/caffe/proto/caffe.pb.cc
AR -o .build_release/lib/libcaffe.a
LD -o .build_release/lib/libcaffe.so.1.0.0
/usr/bin/ld: cannot find -lopencv_core
collect2: error: ld returned 1 exit status
make: *** [.build_release/lib/libcaffe.so.1.0.0] Error 1
make: *** Waiting for unfinished jobs....
",0,cannot find -lopencv_core,"cannot find -lopencv_core The command is 

> make all -j$64 && make distribute -j$64

PROTOC src/caffe/proto/caffe.proto
CXX src/caffe/util/cudnn.cpp
CXX src/caffe/util/hdf5.cpp
CXX src/caffe/util/db.cpp
CXX src/caffe/util/db_lmdb.cpp
CXX src/caffe/util/insert_splits.cpp
CXX src/caffe/util/blocking_queue.cpp
CXX src/caffe/util/math_functions.cpp
CXX src/caffe/util/db_leveldb.cpp
CXX src/caffe/util/benchmark.cpp
CXX src/caffe/util/upgrade_proto.cpp
CXX src/caffe/util/im2col.cpp
CXX src/caffe/util/io.cpp
CXX src/caffe/util/signal_handler.cpp
CXX src/caffe/syncedmem.cpp
CXX src/caffe/layer.cpp
CXX src/caffe/internal_thread.cpp
CXX src/caffe/blob.cpp
CXX src/caffe/layer_factory.cpp
CXX src/caffe/solver.cpp
CXX src/caffe/solvers/rmsprop_solver.cpp
CXX src/caffe/solvers/adadelta_solver.cpp
CXX src/caffe/solvers/sgd_solver.cpp
CXX src/caffe/solvers/nesterov_solver.cpp
CXX src/caffe/solvers/adam_solver.cpp
CXX src/caffe/solvers/adagrad_solver.cpp
CXX src/caffe/parallel.cpp
CXX src/caffe/layers/log_layer.cpp
CXX src/caffe/layers/bias_layer.cpp
CXX src/caffe/layers/filter_layer.cpp
CXX src/caffe/layers/loss_layer.cpp
CXX src/caffe/layers/hinge_loss_layer.cpp
CXX src/caffe/layers/cudnn_sigmoid_layer.cpp
CXX src/caffe/layers/cudnn_pooling_layer.cpp
CXX src/caffe/layers/sigmoid_cross_entropy_loss_layer.cpp
CXX src/caffe/layers/input_layer.cpp
CXX src/caffe/layers/elu_layer.cpp
CXX src/caffe/layers/cudnn_lrn_layer.cpp
CXX src/caffe/layers/base_conv_layer.cpp
CXX src/caffe/layers/multinomial_logistic_loss_layer.cpp
CXX src/caffe/layers/pooling_layer.cpp
CXX src/caffe/layers/crop_layer.cpp
CXX src/caffe/layers/relu_layer.cpp
CXX src/caffe/layers/threshold_layer.cpp
CXX src/caffe/layers/cudnn_lcn_layer.cpp
CXX src/caffe/layers/scale_layer.cpp
CXX src/caffe/layers/concat_layer.cpp
CXX src/caffe/layers/spp_layer.cpp
CXX src/caffe/layers/contrastive_loss_layer.cpp
CXX src/caffe/layers/lrn_layer.cpp
CXX src/caffe/layers/split_layer.cpp
CXX src/caffe/layers/inner_product_layer.cpp
CXX src/caffe/layers/batch_reindex_layer.cpp
CXX src/caffe/layers/mvn_layer.cpp
CXX src/caffe/layers/softmax_loss_layer.cpp
CXX src/caffe/layers/tanh_layer.cpp
CXX src/caffe/layers/power_layer.cpp
CXX src/caffe/layers/image_data_layer.cpp
CXX src/caffe/layers/cudnn_conv_layer.cpp
CXX src/caffe/layers/absval_layer.cpp
CXX src/caffe/layers/embed_layer.cpp
CXX src/caffe/layers/window_data_layer.cpp
CXX src/caffe/layers/bnll_layer.cpp
CXX src/caffe/layers/euclidean_loss_layer.cpp
CXX src/caffe/layers/im2col_layer.cpp
CXX src/caffe/layers/parameter_layer.cpp
CXX src/caffe/layers/memory_data_layer.cpp
CXX src/caffe/layers/argmax_layer.cpp
CXX src/caffe/layers/reduction_layer.cpp
CXX src/caffe/layers/data_layer.cpp
CXX src/caffe/layers/dummy_data_layer.cpp
CXX src/caffe/layers/silence_layer.cpp
CXX src/caffe/layers/batch_norm_layer.cpp
CXX src/caffe/layers/conv_layer.cpp
CXX src/caffe/layers/prelu_layer.cpp
CXX src/caffe/layers/slice_layer.cpp
CXX src/caffe/layers/lstm_unit_layer.cpp
CXX src/caffe/layers/hdf5_data_layer.cpp
CXX src/caffe/layers/reshape_layer.cpp
CXX src/caffe/layers/cudnn_tanh_layer.cpp
CXX src/caffe/layers/cudnn_softmax_layer.cpp
CXX src/caffe/layers/infogain_loss_layer.cpp
CXX src/caffe/layers/base_data_layer.cpp
CXX src/caffe/layers/eltwise_layer.cpp
CXX src/caffe/layers/accuracy_layer.cpp
CXX src/caffe/layers/sigmoid_layer.cpp
CXX src/caffe/layers/dropout_layer.cpp
CXX src/caffe/layers/neuron_layer.cpp
CXX src/caffe/layers/recurrent_layer.cpp
CXX src/caffe/layers/tile_layer.cpp
CXX src/caffe/layers/hdf5_output_layer.cpp
CXX src/caffe/layers/softmax_layer.cpp
CXX src/caffe/layers/flatten_layer.cpp
CXX src/caffe/layers/rnn_layer.cpp
CXX src/caffe/layers/exp_layer.cpp
CXX src/caffe/layers/lstm_layer.cpp
CXX src/caffe/layers/cudnn_relu_layer.cpp
CXX src/caffe/layers/deconv_layer.cpp
CXX src/caffe/common.cpp
CXX src/caffe/data_transformer.cpp
CXX src/caffe/net.cpp
NVCC src/caffe/util/math_functions.cu
NVCC src/caffe/util/im2col.cu
NVCC src/caffe/solvers/adagrad_solver.cu
NVCC src/caffe/solvers/sgd_solver.cu
NVCC src/caffe/solvers/adam_solver.cu
NVCC src/caffe/solvers/adadelta_solver.cu
NVCC src/caffe/solvers/nesterov_solver.cu
NVCC src/caffe/solvers/rmsprop_solver.cu
NVCC src/caffe/layers/accuracy_layer.cu
NVCC src/caffe/layers/batch_norm_layer.cu
NVCC src/caffe/layers/softmax_layer.cu
NVCC src/caffe/layers/im2col_layer.cu
NVCC src/caffe/layers/softmax_loss_layer.cu
NVCC src/caffe/layers/base_data_layer.cu
NVCC src/caffe/layers/sigmoid_cross_entropy_loss_layer.cu
NVCC src/caffe/layers/threshold_layer.cu
NVCC src/caffe/layers/log_layer.cu
NVCC src/caffe/layers/cudnn_lcn_layer.cu
NVCC src/caffe/layers/hdf5_data_layer.cu
NVCC src/caffe/layers/crop_layer.cu
NVCC src/caffe/layers/relu_layer.cu
NVCC src/caffe/layers/slice_layer.cu
NVCC src/caffe/layers/lrn_layer.cu
NVCC src/caffe/layers/batch_reindex_layer.cu
NVCC src/caffe/layers/hdf5_output_layer.cu
NVCC src/caffe/layers/elu_layer.cu
NVCC src/caffe/layers/mvn_layer.cu
NVCC src/caffe/layers/lstm_unit_layer.cu
NVCC src/caffe/layers/tanh_layer.cu
NVCC src/caffe/layers/tile_layer.cu
NVCC src/caffe/layers/dropout_layer.cu
NVCC src/caffe/layers/filter_layer.cu
NVCC src/caffe/layers/cudnn_sigmoid_layer.cu
NVCC src/caffe/layers/absval_layer.cu
NVCC src/caffe/layers/bias_layer.cu
NVCC src/caffe/layers/sigmoid_layer.cu
NVCC src/caffe/layers/pooling_layer.cu
NVCC src/caffe/layers/concat_layer.cu
NVCC src/caffe/layers/cudnn_pooling_layer.cu
NVCC src/caffe/layers/power_layer.cu
NVCC src/caffe/layers/exp_layer.cu
NVCC src/caffe/layers/split_layer.cu
NVCC src/caffe/layers/eltwise_layer.cu
NVCC src/caffe/layers/cudnn_softmax_layer.cu
NVCC src/caffe/layers/deconv_layer.cu
NVCC src/caffe/layers/bnll_layer.cu
NVCC src/caffe/layers/reduction_layer.cu
NVCC src/caffe/layers/cudnn_lrn_layer.cu
NVCC src/caffe/layers/scale_layer.cu
NVCC src/caffe/layers/cudnn_relu_layer.cu
NVCC src/caffe/layers/recurrent_layer.cu
NVCC src/caffe/layers/cudnn_tanh_layer.cu
NVCC src/caffe/layers/euclidean_loss_layer.cu
NVCC src/caffe/layers/inner_product_layer.cu
NVCC src/caffe/layers/embed_layer.cu
NVCC src/caffe/layers/prelu_layer.cu
NVCC src/caffe/layers/conv_layer.cu
NVCC src/caffe/layers/silence_layer.cu
NVCC src/caffe/layers/contrastive_loss_layer.cu
NVCC src/caffe/layers/cudnn_conv_layer.cu
CXX tools/upgrade_net_proto_text.cpp
CXX tools/device_query.cpp
CXX tools/caffe.cpp
CXX tools/extract_features.cpp
CXX tools/compute_image_mean.cpp
CXX tools/test_net.cpp
CXX tools/train_net.cpp
CXX tools/convert_imageset.cpp
CXX tools/finetune_net.cpp
CXX tools/upgrade_solver_proto_text.cpp
CXX tools/net_speed_benchmark.cpp
CXX tools/upgrade_net_proto_binary.cpp
CXX examples/cifar10/convert_cifar_data.cpp
CXX examples/siamese/convert_mnist_siamese_data.cpp
CXX examples/cpp_classification/classification.cpp
CXX examples/mnist/convert_mnist_data.cpp
CXX .build_release/src/caffe/proto/caffe.pb.cc
AR -o .build_release/lib/libcaffe.a
LD -o .build_release/lib/libcaffe.so.1.0.0
/usr/bin/ld: cannot find -lopencv_core
collect2: error: ld returned 1 exit status
make: *** [.build_release/lib/libcaffe.so.1.0.0] Error 1
make: *** Waiting for unfinished jobs....
"
caffe,1832,"I want to know how to debug caffe step by step. I can not find the main function
",0,how to debug caffe?,"how to debug caffe? I want to know how to debug caffe step by step. I can not find the main function
"
caffe,3398,"Hi everyone,

I use caffe to train a multitask net which do both classification and regression.
My loss layers are:
layer {
  name: ""loss1""
  type: ""SoftmaxWithLoss""
  bottom: ""fc8_1""
  bottom: ""label1"" //size100 for classfication
  top: ""loss1""
  loss_weight: 2.0
}
layer {
  name: ""loss2""
  type: ""EuclideanLoss""
  bottom: ""fc8_2""
  bottom: ""label2"" //size4 for regression
  top: ""loss2""
}
I finetune it with Caffenet.caffemodel, and the solver.prototxt is:
net: ""/*****_/train_vol.prototxt""
test_iter: 100
test_interval: 1000
base_lr: 0.001
lr_policy: ""step""
gamma: 0.1
stepsize: 20000
display: 20
max_iter: 100000
momentum: 0.9
weight_decay: 0.0005
snapshot: 10000
snapshot_prefix: ""**_*********""
solver_mode: GPU

The error is:
Iteration 20, loss = nan
Train net output #0: loss1 = 2.58978 (\* 4 = 10.3591 loss)
Train net output #1: loss2 = nan (\* 1 = nan loss)
Only iteration 0 is without nan:
Iteration 0, loss = 14.9363
Train net output #0: loss1 = 3.10484 (\* 4 = 12.4193 loss)
Train net output #1: loss2 = 2.51693 (\* 1 = 2.51693 loss)
The other iterations are always with nan loss.
I follow https://github.com/BVLC/caffe/issues/409,but it is not helpful for me.
When I set base_lr to 0, nan is gone, but even base_lr is small as 0.0001 there is nan again.

Any advice will be appreciated!
Thanks!
artiit.
",0,Training error loss = NAN in multitask net,"Training error loss = NAN in multitask net Hi everyone,

I use caffe to train a multitask net which do both classification and regression.
My loss layers are:
layer {
  name: ""loss1""
  type: ""SoftmaxWithLoss""
  bottom: ""fc8_1""
  bottom: ""label1"" //size100 for classfication
  top: ""loss1""
  loss_weight: 2.0
}
layer {
  name: ""loss2""
  type: ""EuclideanLoss""
  bottom: ""fc8_2""
  bottom: ""label2"" //size4 for regression
  top: ""loss2""
}
I finetune it with Caffenet.caffemodel, and the solver.prototxt is:
net: ""/*****_/train_vol.prototxt""
test_iter: 100
test_interval: 1000
base_lr: 0.001
lr_policy: ""step""
gamma: 0.1
stepsize: 20000
display: 20
max_iter: 100000
momentum: 0.9
weight_decay: 0.0005
snapshot: 10000
snapshot_prefix: ""**_*********""
solver_mode: GPU

The error is:
Iteration 20, loss = nan
Train net output #0: loss1 = 2.58978 (\* 4 = 10.3591 loss)
Train net output #1: loss2 = nan (\* 1 = nan loss)
Only iteration 0 is without nan:
Iteration 0, loss = 14.9363
Train net output #0: loss1 = 3.10484 (\* 4 = 12.4193 loss)
Train net output #1: loss2 = 2.51693 (\* 1 = 2.51693 loss)
The other iterations are always with nan loss.
I follow https://github.com/BVLC/caffe/issues/409,but it is not helpful for me.
When I set base_lr to 0, nan is gone, but even base_lr is small as 0.0001 there is nan again.

Any advice will be appreciated!
Thanks!
artiit.
"
caffe,3489,"Hi All,

I am using Caffe with GPU (NVIDIA GTX 590) support. I have installed the NVIDIA Driver 352 (comes with Cuda 7.5 runfile) with no open gl libs and Caffe is working fine.

However, every time I start a new Caffe session after turning my computer on I get the following error ,

WARNING: Logging before InitGoogleLogging() is written to STDERR
E1219 10:42:22.132685 23546 common.cpp:104] Cannot create Cublas handle. Cublas won't be available.
E1219 10:42:22.159312 23546 common.cpp:111] Cannot create Curand generator. Curand won't be available.
F1219 10:42:22.176038 23546 common.cpp:142] Check failed: error == cudaSuccess (30 vs. 0)  unknown error

and have to re-make caffe every time. Do you know a permanent solution for this? It would be of great help... Thanks in advance.
",0,Has to re-make after every reboot?,"Has to re-make after every reboot? Hi All,

I am using Caffe with GPU (NVIDIA GTX 590) support. I have installed the NVIDIA Driver 352 (comes with Cuda 7.5 runfile) with no open gl libs and Caffe is working fine.

However, every time I start a new Caffe session after turning my computer on I get the following error ,

WARNING: Logging before InitGoogleLogging() is written to STDERR
E1219 10:42:22.132685 23546 common.cpp:104] Cannot create Cublas handle. Cublas won't be available.
E1219 10:42:22.159312 23546 common.cpp:111] Cannot create Curand generator. Curand won't be available.
F1219 10:42:22.176038 23546 common.cpp:142] Check failed: error == cudaSuccess (30 vs. 0)  unknown error

and have to re-make caffe every time. Do you know a permanent solution for this? It would be of great help... Thanks in advance.
"
caffe,1029,"Hi, I have a puzzle. I used the python interface of the latest release caffe, the features of different images are almostly same.  When I employ the former ""io.py"" instead of this new one, the features seem  
to be Ok. So, could you tell me reason about it?
",0,Feature extraction problem using python interface,"Feature extraction problem using python interface Hi, I have a puzzle. I used the python interface of the latest release caffe, the features of different images are almostly same.  When I employ the former ""io.py"" instead of this new one, the features seem  
to be Ok. So, could you tell me reason about it?
"
caffe,3938,"I trained a CNN. I used matcaffe to test it. My code is

data = ones(net.blobs('data').shape);
net.blobs('data').set_data(data);
net.forward_prefilled();
prob = net.blobs('prob').get_data()

the results I am getting is (2 class classification),

0.0001    0.9999    0.0001    0.0000    0.9882  
0.9999    0.0001    0.9999    1.0000    0.0118  

Why I am getting different results for the same input?
",0,Caffe matlab wrapper giving me random outputs for the same input ,"Caffe matlab wrapper giving me random outputs for the same input  I trained a CNN. I used matcaffe to test it. My code is

data = ones(net.blobs('data').shape);
net.blobs('data').set_data(data);
net.forward_prefilled();
prob = net.blobs('prob').get_data()

the results I am getting is (2 class classification),

0.0001    0.9999    0.0001    0.0000    0.9882  
0.9999    0.0001    0.9999    1.0000    0.0118  

Why I am getting different results for the same input?
"
caffe,4526,,0,fe rnn lstm,
caffe,1405,"Hi folks.

I followed [this CIFAR-10 tutorial](http://caffe.berkeleyvision.org/gathered/examples/cifar10.html) and got trained model .

In this training, caffe output accuracy on test data set was 



However, once I tried to predict test data set on ipython,  error rate was extremely high (that was 90.88 percent).  My python notebook which I used on this experiment is [here](http://nbviewer.ipython.org/github/everpeace/caffe/blob/master/examples/cifar10_test_error_is_high.ipynb).

I'm not sure what went wrong.  I'm really appreciated if someone gave me help.
",0,"In manual testing on CIFAR-10 example, prediction error of test set was 90.88%. What happened?","In manual testing on CIFAR-10 example, prediction error of test set was 90.88%. What happened? Hi folks.

I followed [this CIFAR-10 tutorial](http://caffe.berkeleyvision.org/gathered/examples/cifar10.html) and got trained model .

In this training, caffe output accuracy on test data set was 



However, once I tried to predict test data set on ipython,  error rate was extremely high (that was 90.88 percent).  My python notebook which I used on this experiment is [here](http://nbviewer.ipython.org/github/everpeace/caffe/blob/master/examples/cifar10_test_error_is_high.ipynb).

I'm not sure what went wrong.  I'm really appreciated if someone gave me help.
"
caffe,4659,"win7 install Anaconda2 64 bit,
I compile caffe branch:windows, support python, 
set environment variable PythonPath is f:\caffe-window\Build\x64\Release\pycaffe\,

open Anaconda prompt, 
<d:\Anaconda2 C:\Users\lx> python

> > > import caffe 
> > >  but long time no response,
> > > what's problem?
",0,caffe-windows   python problem,"caffe-windows   python problem win7 install Anaconda2 64 bit,
I compile caffe branch:windows, support python, 
set environment variable PythonPath is f:\caffe-window\Build\x64\Release\pycaffe\,

open Anaconda prompt, 
<d:\Anaconda2 C:\Users\lx> python

> > > import caffe 
> > >  but long time no response,
> > > what's problem?
"
caffe,3283,"Dear developer, 
I get the following error when writing (sudo) make all on MAC OS X
Password:
PROTOC src/caffe/proto/caffe.proto
make: protoc: No such file or directory
make: **\* [.build_release/src/caffe/proto/caffe.pb.cc] Error 1

i find it odd as I clearly see that the file is there..
",0,protoc no such file or directory,"protoc no such file or directory Dear developer, 
I get the following error when writing (sudo) make all on MAC OS X
Password:
PROTOC src/caffe/proto/caffe.proto
make: protoc: No such file or directory
make: **\* [.build_release/src/caffe/proto/caffe.pb.cc] Error 1

i find it odd as I clearly see that the file is there..
"
caffe,4834,"Hi,
I have been trying to compile Caffe on Windows with the new GPU Titan X Pascal. I have managed to make it compile by using Cuda 7.5, cuDNN 4, Visual studio 2013 and with Python and Matlab support. However, when I try to train a model, I keep getting the following error:

F1012 08:24:58.364405  6516 math_functions.cu:375] Check failed: status == CURAND_STATUS_SUCCESS (201 vs. 0)  CURAND_STATUS_LAUNCH_FAILURE

I tried the following and also got the same error:
1- I tried disabling cuDNN and then recompiled Caffe which was fine but when I tried to train a model, I got the same error.

2- I uninstalled Cuda, then installed installed Cuda 8.0, cuDNN 5.2. I managed to compile Caffe (with and without cuDNN) but again got the same error when I tried to train a model.

3- I tried to train the same model on a different PC which has Palit NVIDIA GTX TITAN X, Cuda 7.5, cuDNN 4, visual studio 2013, with Python support. Everything was fine and managed to train the same model and got reasonable accuracy on my problem.

4- I tried to train a model using MatConvNet using the Titan X Pascal and Cuda 8 and it worked fine.

If you have any suggestion, please advise. Thanks!

Regards,
",0,Running Caffe on Titan x Pascal - Windows,"Running Caffe on Titan x Pascal - Windows Hi,
I have been trying to compile Caffe on Windows with the new GPU Titan X Pascal. I have managed to make it compile by using Cuda 7.5, cuDNN 4, Visual studio 2013 and with Python and Matlab support. However, when I try to train a model, I keep getting the following error:

F1012 08:24:58.364405  6516 math_functions.cu:375] Check failed: status == CURAND_STATUS_SUCCESS (201 vs. 0)  CURAND_STATUS_LAUNCH_FAILURE

I tried the following and also got the same error:
1- I tried disabling cuDNN and then recompiled Caffe which was fine but when I tried to train a model, I got the same error.

2- I uninstalled Cuda, then installed installed Cuda 8.0, cuDNN 5.2. I managed to compile Caffe (with and without cuDNN) but again got the same error when I tried to train a model.

3- I tried to train the same model on a different PC which has Palit NVIDIA GTX TITAN X, Cuda 7.5, cuDNN 4, visual studio 2013, with Python support. Everything was fine and managed to train the same model and got reasonable accuracy on my problem.

4- I tried to train a model using MatConvNet using the Titan X Pascal and Cuda 8 and it worked fine.

If you have any suggestion, please advise. Thanks!

Regards,
"
caffe,1622,"CuDNN RC 2 is available for download. https://developer.nvidia.com/cuDNN
They say, it is 40% faster than original cuDNN. However, it breaks a compatibility with caffe-wrappers (more general ways to pass n-d tensors, additional coefficients, etc.) Does anyone know, NVIDIA guys going to support caffe in future, or rewriting cudnn-layers are up to us?
",0,cuDNN R2 is available,"cuDNN R2 is available CuDNN RC 2 is available for download. https://developer.nvidia.com/cuDNN
They say, it is 40% faster than original cuDNN. However, it breaks a compatibility with caffe-wrappers (more general ways to pass n-d tensors, additional coefficients, etc.) Does anyone know, NVIDIA guys going to support caffe in future, or rewriting cudnn-layers are up to us?
"
caffe,591,"I'm debuging on my local macbook. Before I tried to use the python lib ""pycaffe"" everything works well. 
While when I turn to attempt to write some python scripts relying on the ""_caffe.so"", the following error turns out:

python
import _caffe

Segmentation fault: 11

This should be some problems in compiling the python lib.

Thanks. 
",0,"import pycaffe in local Mac OS 10.9 reports ""segmentation fault: 11""","import pycaffe in local Mac OS 10.9 reports ""segmentation fault: 11"" I'm debuging on my local macbook. Before I tried to use the python lib ""pycaffe"" everything works well. 
While when I turn to attempt to write some python scripts relying on the ""_caffe.so"", the following error turns out:

python
import _caffe

Segmentation fault: 11

This should be some problems in compiling the python lib.

Thanks. 
"
caffe,3890,"@shelhamer @longjon From my naive point of view, it looks like the requirements for FCN-x from the model zoo have made it into master; is this true?

I'm eventually hoping to do some fine tuning off of that work, but in the process, I thought I'd try creating a reproducible example of one of the FCN-X's working.  Work started in a separate repo here: https://github.com/developmentseed/caffe-fcn/blob/master/src/fcn-fwd.ipynb -- before I take it further, just wanted to ask if you'd be interested in a PR adding something like this to the examples in this repo?  (If so, I might as well set up a branch and work from there.)
",0,Add FCN example for semantic segmentation,"Add FCN example for semantic segmentation @shelhamer @longjon From my naive point of view, it looks like the requirements for FCN-x from the model zoo have made it into master; is this true?

I'm eventually hoping to do some fine tuning off of that work, but in the process, I thought I'd try creating a reproducible example of one of the FCN-X's working.  Work started in a separate repo here: https://github.com/developmentseed/caffe-fcn/blob/master/src/fcn-fwd.ipynb -- before I take it further, just wanted to ask if you'd be interested in a PR adding something like this to the examples in this repo?  (If so, I might as well set up a branch and work from there.)
"
caffe,2662,"I have trained a model myself,I want to use it in a web demo,so I read the example/web_demo/app.py code,
'bet_file': (
            '{}/data/ilsvrc12/imagenet.bet.pickle'.format(REPO_DIRNAME)),

in this line code,i don't kown the 'imagenet.bet.pickle' file how to create, is it necessary,who can tell me how to solve the problem, thanks.
",0,how to create a new imagenet.bet.pickle file my own,"how to create a new imagenet.bet.pickle file my own I have trained a model myself,I want to use it in a web demo,so I read the example/web_demo/app.py code,
'bet_file': (
            '{}/data/ilsvrc12/imagenet.bet.pickle'.format(REPO_DIRNAME)),

in this line code,i don't kown the 'imagenet.bet.pickle' file how to create, is it necessary,who can tell me how to solve the problem, thanks.
"
caffe,485,"Hi,
there is a problem in the latest dev branch. In the dummy_data_layer.cpp there is a call to the non defined Forward method, I suppose it must be the the Forward_cpu method
",0,Error compiling lastest dev branch,"Error compiling lastest dev branch Hi,
there is a problem in the latest dev branch. In the dummy_data_layer.cpp there is a call to the non defined Forward method, I suppose it must be the the Forward_cpu method
"
caffe,6224,"### Issue summary

When I typed ""make all"" on terminal trying to install caffe, following information will occur. I already installed gflags. I've tried every solution I found on Google and Github, how can I fix that? Thanks in advance.

If my description is not clear enough, please tell me.

=======================================

Jays-iMac:build jay$ make all
[  1%] Running C++/Python protocol buffer compiler on /Users/jay/documents/caffe/src/caffe/proto/caffe.proto
Scanning dependencies of target caffeproto
[  1%] Building CXX object src/caffe/CMakeFiles/caffeproto.dir/__/__/include/caffe/proto/caffe.pb.cc.o
[  1%] Linking CXX static library ../../lib/libcaffeproto.a
[  1%] Built target caffeproto
Scanning dependencies of target caffe
[  1%] Building CXX object src/caffe/CMakeFiles/caffe.dir/blob.cpp.o
[  1%] Building CXX object src/caffe/CMakeFiles/caffe.dir/common.cpp.o
/Users/jay/documents/caffe/src/caffe/common.cpp:134:5: error: no member named 'gflags' in the global namespace
  ::gflags::ParseCommandLineFlags(pargc, pargv, true);
  ~~^
/Users/jay/documents/caffe/src/caffe/common.cpp:182:7: warning: field 'default_device_' will be initialized after field 'solver_count_' [-Wreorder]
      default_device_(cpu_device_.get()),
      ^
/Users/jay/documents/caffe/src/caffe/common.cpp:323:7: warning: field 'default_device_' will be initialized after field 'solver_count_' [-Wreorder]
      default_device_(cpu_device_.get()),
      ^
2 warnings and 1 error generated.
make[2]: *** [src/caffe/CMakeFiles/caffe.dir/common.cpp.o] Error 1
make[1]: *** [src/caffe/CMakeFiles/caffe.dir/all] Error 2
make: *** [all] Error 2

=======================================

[Makefile.config.txt](https://github.com/BVLC/caffe/files/1711294/Makefile.config.txt)
[CMakeCache.txt](https://github.com/BVLC/caffe/files/1711293/CMakeCache.txt)
[CMakeOutput.log](https://github.com/BVLC/caffe/files/1711300/CMakeOutput.log)



Operating system: OS X 10.12
CUDA version (if applicable): 9.0
Python or MATLAB version (for pycaffe and matcaffe respectively): python3.6
",0,Build caffe by CLion failed,"Build caffe by CLion failed ### Issue summary

When I typed ""make all"" on terminal trying to install caffe, following information will occur. I already installed gflags. I've tried every solution I found on Google and Github, how can I fix that? Thanks in advance.

If my description is not clear enough, please tell me.

=======================================

Jays-iMac:build jay$ make all
[  1%] Running C++/Python protocol buffer compiler on /Users/jay/documents/caffe/src/caffe/proto/caffe.proto
Scanning dependencies of target caffeproto
[  1%] Building CXX object src/caffe/CMakeFiles/caffeproto.dir/__/__/include/caffe/proto/caffe.pb.cc.o
[  1%] Linking CXX static library ../../lib/libcaffeproto.a
[  1%] Built target caffeproto
Scanning dependencies of target caffe
[  1%] Building CXX object src/caffe/CMakeFiles/caffe.dir/blob.cpp.o
[  1%] Building CXX object src/caffe/CMakeFiles/caffe.dir/common.cpp.o
/Users/jay/documents/caffe/src/caffe/common.cpp:134:5: error: no member named 'gflags' in the global namespace
  ::gflags::ParseCommandLineFlags(pargc, pargv, true);
  ~~^
/Users/jay/documents/caffe/src/caffe/common.cpp:182:7: warning: field 'default_device_' will be initialized after field 'solver_count_' [-Wreorder]
      default_device_(cpu_device_.get()),
      ^
/Users/jay/documents/caffe/src/caffe/common.cpp:323:7: warning: field 'default_device_' will be initialized after field 'solver_count_' [-Wreorder]
      default_device_(cpu_device_.get()),
      ^
2 warnings and 1 error generated.
make[2]: *** [src/caffe/CMakeFiles/caffe.dir/common.cpp.o] Error 1
make[1]: *** [src/caffe/CMakeFiles/caffe.dir/all] Error 2
make: *** [all] Error 2

=======================================

[Makefile.config.txt](https://github.com/BVLC/caffe/files/1711294/Makefile.config.txt)
[CMakeCache.txt](https://github.com/BVLC/caffe/files/1711293/CMakeCache.txt)
[CMakeOutput.log](https://github.com/BVLC/caffe/files/1711300/CMakeOutput.log)



Operating system: OS X 10.12
CUDA version (if applicable): 9.0
Python or MATLAB version (for pycaffe and matcaffe respectively): python3.6
"
caffe,3167,"I want to extract only one image feature without prototxt file. Not the batch feature. The image is loaded from disk. We don't need configuration file with a set of images path.Thank you!
",0,How to extract one image feature?,"How to extract one image feature? I want to extract only one image feature without prototxt file. Not the batch feature. The image is loaded from disk. We don't need configuration file with a set of images path.Thank you!
"
